{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\HH TRADERS\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13017</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_frame0.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13018</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_frame1.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13019</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_frame2.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13020</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_frame3.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13021</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_frame4.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image class\n",
       "13017  winKen_wave_u_cm_np1_ri_bad_1_frame0.jpg  wave\n",
       "13018  winKen_wave_u_cm_np1_ri_bad_1_frame1.jpg  wave\n",
       "13019  winKen_wave_u_cm_np1_ri_bad_1_frame2.jpg  wave\n",
       "13020  winKen_wave_u_cm_np1_ri_bad_1_frame3.jpg  wave\n",
       "13021  winKen_wave_u_cm_np1_ri_bad_1_frame4.jpg  wave"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "train.sort_values(by=['class', 'image'])\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13022/13022 [01:26<00:00, 151.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating an empty list\n",
    "train_image = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    # loading the image and keeping the target size as (224,224,3)\n",
    "    img = image.load_img('../data/train_frame/'+train['image'][i], target_size=(224,224,3))\n",
    "    # converting it to array\n",
    "    img = image.img_to_array(img)\n",
    "    # normalizing the pixel value\n",
    "    img = img/255\n",
    "    # appending the image to the train_image list\n",
    "    train_image.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13022, 224, 224, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the list to numpy array\n",
    "X_train = np.array(train_image,np.float16)\n",
    "\n",
    "# shape of the array\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5504</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5505</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5506</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5507</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5508</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  image class\n",
       "5504  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "5505  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "5506  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "5507  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "5508  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = pd.read_csv('../data/val.csv')\n",
    "val.sort_values(by=['class', 'image'])\n",
    "val.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5509/5509 [00:41<00:00, 134.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating an empty list\n",
    "val_image = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(val.shape[0])):\n",
    "    # loading the image and keeping the target size as (224,224,3)\n",
    "    img = image.load_img('../data/val_frame/'+val['image'][i], target_size=(224,224,3))\n",
    "    # converting it to array\n",
    "    img = image.img_to_array(img)\n",
    "    # normalizing the pixel value\n",
    "    img = img/255\n",
    "    # appending the image to the train_image list\n",
    "    val_image.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5509, 224, 224, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the list to numpy array\n",
    "X_test = np.array(val_image,np.float16)\n",
    "\n",
    "# shape of the array\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image    13022\n",
      "class       51\n",
      "dtype: int64\n",
      "image    5509\n",
      "class      51\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# separating the target\n",
    "y_train = train['class']\n",
    "y_test = val['class']\n",
    "print(train.nunique())\n",
    "print(val.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13022, 51)\n",
      "(5509, 51)\n"
     ]
    }
   ],
   "source": [
    "# creating dummies of target variable for train and validation set\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = DenseNet121(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, None, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(base_model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"densenet121\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, None, None, 3 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)             (None, None, None, 6 9408        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)   (None, None, None, 6 256         conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1/relu (Activation)         (None, None, None, 6 0           conv1/bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, None, None, 6 0           conv1/relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, None, None, 6 0           zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, None, None, 6 256         pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, None, None, 6 0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, None, None, 1 8192        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, None, None, 1 512         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, None, None, 1 0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, None, None, 3 36864       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, None, None, 9 0           pool1[0][0]                      \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, None, None, 9 384         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, None, None, 9 0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, None, None, 1 12288       conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, None, None, 1 512         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, None, None, 1 0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, None, None, 3 36864       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, None, None, 1 0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormali (None, None, None, 1 512         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation (None, None, None, 1 0           conv2_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, None, None, 1 16384       conv2_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, None, None, 1 512         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, None, None, 1 0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, None, None, 3 36864       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenat (None, None, None, 1 0           conv2_block2_concat[0][0]        \n",
      "                                                                 conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormali (None, None, None, 1 640         conv2_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation (None, None, None, 1 0           conv2_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)    (None, None, None, 1 20480       conv2_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormali (None, None, None, 1 512         conv2_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation (None, None, None, 1 0           conv2_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)    (None, None, None, 3 36864       conv2_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenat (None, None, None, 1 0           conv2_block3_concat[0][0]        \n",
      "                                                                 conv2_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormali (None, None, None, 1 768         conv2_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation (None, None, None, 1 0           conv2_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)    (None, None, None, 1 24576       conv2_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormali (None, None, None, 1 512         conv2_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation (None, None, None, 1 0           conv2_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)    (None, None, None, 3 36864       conv2_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenat (None, None, None, 2 0           conv2_block4_concat[0][0]        \n",
      "                                                                 conv2_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormali (None, None, None, 2 896         conv2_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation (None, None, None, 2 0           conv2_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)    (None, None, None, 1 28672       conv2_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormali (None, None, None, 1 512         conv2_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation (None, None, None, 1 0           conv2_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)    (None, None, None, 3 36864       conv2_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenat (None, None, None, 2 0           conv2_block5_concat[0][0]        \n",
      "                                                                 conv2_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, None, None, 2 1024        conv2_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, None, None, 2 0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)             (None, None, None, 1 32768       pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)   (None, None, None, 1 0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, None, None, 1 512         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, None, None, 1 0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, None, None, 1 16384       conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, None, None, 1 512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, None, None, 1 0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, None, None, 3 36864       conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, None, None, 1 0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, None, None, 1 640         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, None, None, 1 0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, None, None, 1 20480       conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, None, None, 1 512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, None, None, 1 0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, None, None, 3 36864       conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, None, None, 1 0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormali (None, None, None, 1 768         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation (None, None, None, 1 0           conv3_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, None, None, 1 24576       conv3_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, None, None, 1 512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, None, None, 1 0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, None, None, 3 36864       conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenat (None, None, None, 2 0           conv3_block2_concat[0][0]        \n",
      "                                                                 conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormali (None, None, None, 2 896         conv3_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation (None, None, None, 2 0           conv3_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv3_block4_1_conv (Conv2D)    (None, None, None, 1 28672       conv3_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, None, None, 1 512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, None, None, 1 0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, None, None, 3 36864       conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenat (None, None, None, 2 0           conv3_block3_concat[0][0]        \n",
      "                                                                 conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormali (None, None, None, 2 1024        conv3_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation (None, None, None, 2 0           conv3_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, None, None, 1 32768       conv3_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, None, None, 1 512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, None, None, 1 0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, None, None, 3 36864       conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenat (None, None, None, 2 0           conv3_block4_concat[0][0]        \n",
      "                                                                 conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormali (None, None, None, 2 1152        conv3_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation (None, None, None, 2 0           conv3_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, None, None, 1 36864       conv3_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, None, None, 1 512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, None, None, 1 0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, None, None, 3 36864       conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenat (None, None, None, 3 0           conv3_block5_concat[0][0]        \n",
      "                                                                 conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormali (None, None, None, 3 1280        conv3_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation (None, None, None, 3 0           conv3_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, None, None, 1 40960       conv3_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, None, None, 1 512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, None, None, 1 0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, None, None, 3 36864       conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenat (None, None, None, 3 0           conv3_block6_concat[0][0]        \n",
      "                                                                 conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormali (None, None, None, 3 1408        conv3_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation (None, None, None, 3 0           conv3_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, None, None, 1 45056       conv3_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, None, None, 1 512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, None, None, 1 0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, None, None, 3 36864       conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenat (None, None, None, 3 0           conv3_block7_concat[0][0]        \n",
      "                                                                 conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormali (None, None, None, 3 1536        conv3_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation (None, None, None, 3 0           conv3_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)    (None, None, None, 1 49152       conv3_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormali (None, None, None, 1 512         conv3_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation (None, None, None, 1 0           conv3_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)    (None, None, None, 3 36864       conv3_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenat (None, None, None, 4 0           conv3_block8_concat[0][0]        \n",
      "                                                                 conv3_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormal (None, None, None, 4 1664        conv3_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activatio (None, None, None, 4 0           conv3_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)   (None, None, None, 1 53248       conv3_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormal (None, None, None, 1 512         conv3_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activatio (None, None, None, 1 0           conv3_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)   (None, None, None, 3 36864       conv3_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatena (None, None, None, 4 0           conv3_block9_concat[0][0]        \n",
      "                                                                 conv3_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormal (None, None, None, 4 1792        conv3_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activatio (None, None, None, 4 0           conv3_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)   (None, None, None, 1 57344       conv3_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormal (None, None, None, 1 512         conv3_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activatio (None, None, None, 1 0           conv3_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)   (None, None, None, 3 36864       conv3_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatena (None, None, None, 4 0           conv3_block10_concat[0][0]       \n",
      "                                                                 conv3_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormal (None, None, None, 4 1920        conv3_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activatio (None, None, None, 4 0           conv3_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)   (None, None, None, 1 61440       conv3_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormal (None, None, None, 1 512         conv3_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activatio (None, None, None, 1 0           conv3_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)   (None, None, None, 3 36864       conv3_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatena (None, None, None, 5 0           conv3_block11_concat[0][0]       \n",
      "                                                                 conv3_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, None, None, 5 2048        conv3_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, None, None, 5 0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)             (None, None, None, 2 131072      pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)   (None, None, None, 2 0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, None, None, 2 1024        pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, None, None, 2 0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, None, None, 1 32768       conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, None, None, 1 512         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, None, None, 1 0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, None, None, 3 36864       conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, None, None, 2 0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, None, None, 2 1152        conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, None, None, 2 0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, None, None, 1 36864       conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, None, None, 1 512         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, None, None, 1 0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv4_block2_2_conv (Conv2D)    (None, None, None, 3 36864       conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, None, None, 3 0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormali (None, None, None, 3 1280        conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation (None, None, None, 3 0           conv4_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, None, None, 1 40960       conv4_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, None, None, 1 512         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, None, None, 1 0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, None, None, 3 36864       conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenat (None, None, None, 3 0           conv4_block2_concat[0][0]        \n",
      "                                                                 conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormali (None, None, None, 3 1408        conv4_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation (None, None, None, 3 0           conv4_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, None, None, 1 45056       conv4_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, None, None, 1 512         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, None, None, 1 0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, None, None, 3 36864       conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenat (None, None, None, 3 0           conv4_block3_concat[0][0]        \n",
      "                                                                 conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormali (None, None, None, 3 1536        conv4_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation (None, None, None, 3 0           conv4_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, None, None, 1 49152       conv4_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, None, None, 1 512         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, None, None, 1 0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, None, None, 3 36864       conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenat (None, None, None, 4 0           conv4_block4_concat[0][0]        \n",
      "                                                                 conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormali (None, None, None, 4 1664        conv4_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation (None, None, None, 4 0           conv4_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, None, None, 1 53248       conv4_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, None, None, 1 512         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, None, None, 1 0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, None, None, 3 36864       conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenat (None, None, None, 4 0           conv4_block5_concat[0][0]        \n",
      "                                                                 conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormali (None, None, None, 4 1792        conv4_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation (None, None, None, 4 0           conv4_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, None, None, 1 57344       conv4_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, None, None, 1 512         conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, None, None, 1 0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, None, None, 3 36864       conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenat (None, None, None, 4 0           conv4_block6_concat[0][0]        \n",
      "                                                                 conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormali (None, None, None, 4 1920        conv4_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation (None, None, None, 4 0           conv4_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, None, None, 1 61440       conv4_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, None, None, 1 512         conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, None, None, 1 0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, None, None, 3 36864       conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenat (None, None, None, 5 0           conv4_block7_concat[0][0]        \n",
      "                                                                 conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormali (None, None, None, 5 2048        conv4_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation (None, None, None, 5 0           conv4_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, None, None, 1 65536       conv4_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, None, None, 1 512         conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, None, None, 1 0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, None, None, 3 36864       conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenat (None, None, None, 5 0           conv4_block8_concat[0][0]        \n",
      "                                                                 conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormal (None, None, None, 5 2176        conv4_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activatio (None, None, None, 5 0           conv4_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, None, None, 1 69632       conv4_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, None, None, 1 512         conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, None, None, 1 0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, None, None, 3 36864       conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatena (None, None, None, 5 0           conv4_block9_concat[0][0]        \n",
      "                                                                 conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormal (None, None, None, 5 2304        conv4_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activatio (None, None, None, 5 0           conv4_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, None, None, 1 73728       conv4_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, None, None, 1 512         conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, None, None, 1 0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, None, None, 3 36864       conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatena (None, None, None, 6 0           conv4_block10_concat[0][0]       \n",
      "                                                                 conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormal (None, None, None, 6 2432        conv4_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activatio (None, None, None, 6 0           conv4_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, None, None, 1 77824       conv4_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, None, None, 1 512         conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, None, None, 1 0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, None, None, 3 36864       conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatena (None, None, None, 6 0           conv4_block11_concat[0][0]       \n",
      "                                                                 conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormal (None, None, None, 6 2560        conv4_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activatio (None, None, None, 6 0           conv4_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, None, None, 1 81920       conv4_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, None, None, 1 512         conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, None, None, 1 0           conv4_block13_1_bn[0][0]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, None, None, 3 36864       conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatena (None, None, None, 6 0           conv4_block12_concat[0][0]       \n",
      "                                                                 conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormal (None, None, None, 6 2688        conv4_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activatio (None, None, None, 6 0           conv4_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, None, None, 1 86016       conv4_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, None, None, 1 512         conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, None, None, 1 0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, None, None, 3 36864       conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatena (None, None, None, 7 0           conv4_block13_concat[0][0]       \n",
      "                                                                 conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormal (None, None, None, 7 2816        conv4_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activatio (None, None, None, 7 0           conv4_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, None, None, 1 90112       conv4_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, None, None, 1 512         conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, None, None, 1 0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, None, None, 3 36864       conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatena (None, None, None, 7 0           conv4_block14_concat[0][0]       \n",
      "                                                                 conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormal (None, None, None, 7 2944        conv4_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activatio (None, None, None, 7 0           conv4_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, None, None, 1 94208       conv4_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, None, None, 1 512         conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, None, None, 1 0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, None, None, 3 36864       conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatena (None, None, None, 7 0           conv4_block15_concat[0][0]       \n",
      "                                                                 conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormal (None, None, None, 7 3072        conv4_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activatio (None, None, None, 7 0           conv4_block17_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, None, None, 1 98304       conv4_block17_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, None, None, 1 512         conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, None, None, 1 0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, None, None, 3 36864       conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatena (None, None, None, 8 0           conv4_block16_concat[0][0]       \n",
      "                                                                 conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormal (None, None, None, 8 3200        conv4_block17_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activatio (None, None, None, 8 0           conv4_block18_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, None, None, 1 102400      conv4_block18_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, None, None, 1 512         conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, None, None, 1 0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, None, None, 3 36864       conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatena (None, None, None, 8 0           conv4_block17_concat[0][0]       \n",
      "                                                                 conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormal (None, None, None, 8 3328        conv4_block18_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activatio (None, None, None, 8 0           conv4_block19_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, None, None, 1 106496      conv4_block19_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, None, None, 1 512         conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, None, None, 1 0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, None, None, 3 36864       conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatena (None, None, None, 8 0           conv4_block18_concat[0][0]       \n",
      "                                                                 conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormal (None, None, None, 8 3456        conv4_block19_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activatio (None, None, None, 8 0           conv4_block20_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, None, None, 1 110592      conv4_block20_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, None, None, 1 512         conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, None, None, 1 0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, None, None, 3 36864       conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatena (None, None, None, 8 0           conv4_block19_concat[0][0]       \n",
      "                                                                 conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormal (None, None, None, 8 3584        conv4_block20_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activatio (None, None, None, 8 0           conv4_block21_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, None, None, 1 114688      conv4_block21_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, None, None, 1 512         conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, None, None, 1 0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, None, None, 3 36864       conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatena (None, None, None, 9 0           conv4_block20_concat[0][0]       \n",
      "                                                                 conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormal (None, None, None, 9 3712        conv4_block21_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activatio (None, None, None, 9 0           conv4_block22_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, None, None, 1 118784      conv4_block22_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, None, None, 1 512         conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, None, None, 1 0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, None, None, 3 36864       conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatena (None, None, None, 9 0           conv4_block21_concat[0][0]       \n",
      "                                                                 conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormal (None, None, None, 9 3840        conv4_block22_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activatio (None, None, None, 9 0           conv4_block23_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, None, None, 1 122880      conv4_block23_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, None, None, 1 512         conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, None, None, 1 0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, None, None, 3 36864       conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatena (None, None, None, 9 0           conv4_block22_concat[0][0]       \n",
      "                                                                 conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormal (None, None, None, 9 3968        conv4_block23_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activatio (None, None, None, 9 0           conv4_block24_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, None, None, 1 126976      conv4_block24_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, None, None, 1 512         conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv4_block24_1_relu (Activatio (None, None, None, 1 0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, None, None, 3 36864       conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatena (None, None, None, 1 0           conv4_block23_concat[0][0]       \n",
      "                                                                 conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)   (None, None, None, 1 4096        conv4_block24_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_relu (Activation)         (None, None, None, 1 0           pool4_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)             (None, None, None, 5 524288      pool4_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)   (None, None, None, 5 0           pool4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, None, None, 5 2048        pool4_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation (None, None, None, 5 0           conv5_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, None, None, 1 65536       conv5_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, None, None, 1 512         conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, None, None, 1 0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, None, None, 3 36864       conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenat (None, None, None, 5 0           pool4_pool[0][0]                 \n",
      "                                                                 conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormali (None, None, None, 5 2176        conv5_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation (None, None, None, 5 0           conv5_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, None, None, 1 69632       conv5_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, None, None, 1 512         conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, None, None, 1 0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, None, None, 3 36864       conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenat (None, None, None, 5 0           conv5_block1_concat[0][0]        \n",
      "                                                                 conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormali (None, None, None, 5 2304        conv5_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation (None, None, None, 5 0           conv5_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, None, None, 1 73728       conv5_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, None, None, 1 512         conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, None, None, 1 0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, None, None, 3 36864       conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenat (None, None, None, 6 0           conv5_block2_concat[0][0]        \n",
      "                                                                 conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormali (None, None, None, 6 2432        conv5_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation (None, None, None, 6 0           conv5_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)    (None, None, None, 1 77824       conv5_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormali (None, None, None, 1 512         conv5_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation (None, None, None, 1 0           conv5_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)    (None, None, None, 3 36864       conv5_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenat (None, None, None, 6 0           conv5_block3_concat[0][0]        \n",
      "                                                                 conv5_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormali (None, None, None, 6 2560        conv5_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation (None, None, None, 6 0           conv5_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)    (None, None, None, 1 81920       conv5_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormali (None, None, None, 1 512         conv5_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation (None, None, None, 1 0           conv5_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)    (None, None, None, 3 36864       conv5_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenat (None, None, None, 6 0           conv5_block4_concat[0][0]        \n",
      "                                                                 conv5_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormali (None, None, None, 6 2688        conv5_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation (None, None, None, 6 0           conv5_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)    (None, None, None, 1 86016       conv5_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormali (None, None, None, 1 512         conv5_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation (None, None, None, 1 0           conv5_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)    (None, None, None, 3 36864       conv5_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenat (None, None, None, 7 0           conv5_block5_concat[0][0]        \n",
      "                                                                 conv5_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormali (None, None, None, 7 2816        conv5_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation (None, None, None, 7 0           conv5_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)    (None, None, None, 1 90112       conv5_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormali (None, None, None, 1 512         conv5_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation (None, None, None, 1 0           conv5_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)    (None, None, None, 3 36864       conv5_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenat (None, None, None, 7 0           conv5_block6_concat[0][0]        \n",
      "                                                                 conv5_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormali (None, None, None, 7 2944        conv5_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation (None, None, None, 7 0           conv5_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)    (None, None, None, 1 94208       conv5_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormali (None, None, None, 1 512         conv5_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation (None, None, None, 1 0           conv5_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)    (None, None, None, 3 36864       conv5_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenat (None, None, None, 7 0           conv5_block7_concat[0][0]        \n",
      "                                                                 conv5_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormali (None, None, None, 7 3072        conv5_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation (None, None, None, 7 0           conv5_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)    (None, None, None, 1 98304       conv5_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormali (None, None, None, 1 512         conv5_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation (None, None, None, 1 0           conv5_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)    (None, None, None, 3 36864       conv5_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenat (None, None, None, 8 0           conv5_block8_concat[0][0]        \n",
      "                                                                 conv5_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormal (None, None, None, 8 3200        conv5_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activatio (None, None, None, 8 0           conv5_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)   (None, None, None, 1 102400      conv5_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormal (None, None, None, 1 512         conv5_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activatio (None, None, None, 1 0           conv5_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)   (None, None, None, 3 36864       conv5_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatena (None, None, None, 8 0           conv5_block9_concat[0][0]        \n",
      "                                                                 conv5_block10_2_conv[0][0]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormal (None, None, None, 8 3328        conv5_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activatio (None, None, None, 8 0           conv5_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)   (None, None, None, 1 106496      conv5_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormal (None, None, None, 1 512         conv5_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activatio (None, None, None, 1 0           conv5_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)   (None, None, None, 3 36864       conv5_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatena (None, None, None, 8 0           conv5_block10_concat[0][0]       \n",
      "                                                                 conv5_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormal (None, None, None, 8 3456        conv5_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activatio (None, None, None, 8 0           conv5_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)   (None, None, None, 1 110592      conv5_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormal (None, None, None, 1 512         conv5_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activatio (None, None, None, 1 0           conv5_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)   (None, None, None, 3 36864       conv5_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatena (None, None, None, 8 0           conv5_block11_concat[0][0]       \n",
      "                                                                 conv5_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormal (None, None, None, 8 3584        conv5_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activatio (None, None, None, 8 0           conv5_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)   (None, None, None, 1 114688      conv5_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormal (None, None, None, 1 512         conv5_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activatio (None, None, None, 1 0           conv5_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)   (None, None, None, 3 36864       conv5_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatena (None, None, None, 9 0           conv5_block12_concat[0][0]       \n",
      "                                                                 conv5_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormal (None, None, None, 9 3712        conv5_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activatio (None, None, None, 9 0           conv5_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)   (None, None, None, 1 118784      conv5_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormal (None, None, None, 1 512         conv5_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activatio (None, None, None, 1 0           conv5_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)   (None, None, None, 3 36864       conv5_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatena (None, None, None, 9 0           conv5_block13_concat[0][0]       \n",
      "                                                                 conv5_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormal (None, None, None, 9 3840        conv5_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activatio (None, None, None, 9 0           conv5_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)   (None, None, None, 1 122880      conv5_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormal (None, None, None, 1 512         conv5_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activatio (None, None, None, 1 0           conv5_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)   (None, None, None, 3 36864       conv5_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatena (None, None, None, 9 0           conv5_block14_concat[0][0]       \n",
      "                                                                 conv5_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormal (None, None, None, 9 3968        conv5_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activatio (None, None, None, 9 0           conv5_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)   (None, None, None, 1 126976      conv5_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormal (None, None, None, 1 512         conv5_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activatio (None, None, None, 1 0           conv5_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)   (None, None, None, 3 36864       conv5_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatena (None, None, None, 1 0           conv5_block15_concat[0][0]       \n",
      "                                                                 conv5_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, None, None, 1 4096        conv5_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "relu (Activation)               (None, None, None, 1 0           bn[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 7,037,504\n",
      "Trainable params: 6,953,856\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'densenet121',\n",
       " 'layers': [{'name': 'input_1',\n",
       "   'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, None, None, 3),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'name': 'input_1'},\n",
       "   'inbound_nodes': []},\n",
       "  {'name': 'zero_padding2d_1',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'zero_padding2d_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((3, 3), (3, 3)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['input_1', 0, 0, {}]]]},\n",
       "  {'name': 'conv1/conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv1/conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['zero_padding2d_1', 0, 0, {}]]]},\n",
       "  {'name': 'conv1/bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv1/bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv1/conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv1/relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv1/relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv1/bn', 0, 0, {}]]]},\n",
       "  {'name': 'zero_padding2d_2',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'zero_padding2d_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((1, 1), (1, 1)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['conv1/relu', 0, 0, {}]]]},\n",
       "  {'name': 'pool1',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'pool1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['zero_padding2d_2', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block1_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv2_block1_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['pool1', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block1_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv2_block1_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv2_block1_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block1_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2_block1_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block1_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block1_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv2_block1_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block1_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block1_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv2_block1_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv2_block1_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block1_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2_block1_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block1_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block1_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv2_block1_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['pool1', 0, 0, {}],\n",
       "     ['conv2_block1_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block2_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv2_block2_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block1_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block2_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv2_block2_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv2_block2_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block2_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2_block2_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block2_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block2_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv2_block2_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block2_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block2_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv2_block2_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv2_block2_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block2_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2_block2_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block2_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block2_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv2_block2_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv2_block1_concat', 0, 0, {}],\n",
       "     ['conv2_block2_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block3_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv2_block3_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block2_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block3_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv2_block3_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv2_block3_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block3_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2_block3_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block3_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block3_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv2_block3_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block3_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block3_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv2_block3_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv2_block3_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block3_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2_block3_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block3_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block3_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv2_block3_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv2_block2_concat', 0, 0, {}],\n",
       "     ['conv2_block3_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block4_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv2_block4_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block3_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block4_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv2_block4_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv2_block4_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block4_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2_block4_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block4_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block4_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv2_block4_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block4_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block4_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv2_block4_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv2_block4_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block4_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2_block4_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block4_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block4_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv2_block4_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv2_block3_concat', 0, 0, {}],\n",
       "     ['conv2_block4_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block5_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv2_block5_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block4_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block5_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv2_block5_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv2_block5_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block5_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2_block5_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block5_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block5_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv2_block5_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block5_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block5_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv2_block5_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv2_block5_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block5_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2_block5_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block5_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block5_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv2_block5_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv2_block4_concat', 0, 0, {}],\n",
       "     ['conv2_block5_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block6_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv2_block6_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block5_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block6_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv2_block6_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv2_block6_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block6_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2_block6_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block6_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block6_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv2_block6_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block6_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block6_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv2_block6_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv2_block6_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block6_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2_block6_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block6_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv2_block6_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv2_block6_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv2_block5_concat', 0, 0, {}],\n",
       "     ['conv2_block6_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'pool2_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'pool2_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2_block6_concat', 0, 0, {}]]]},\n",
       "  {'name': 'pool2_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'pool2_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['pool2_bn', 0, 0, {}]]]},\n",
       "  {'name': 'pool2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'pool2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['pool2_relu', 0, 0, {}]]]},\n",
       "  {'name': 'pool2_pool',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'pool2_pool',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['pool2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block1_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block1_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['pool2_pool', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block1_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block1_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block1_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block1_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block1_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block1_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block1_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block1_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block1_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block1_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block1_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block1_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block1_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block1_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block1_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block1_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv3_block1_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['pool2_pool', 0, 0, {}],\n",
       "     ['conv3_block1_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block2_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block2_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block1_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block2_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block2_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block2_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block2_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block2_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block2_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block2_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block2_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block2_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block2_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block2_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block2_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block2_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block2_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block2_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block2_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv3_block2_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv3_block1_concat', 0, 0, {}],\n",
       "     ['conv3_block2_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block3_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block3_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block2_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block3_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block3_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block3_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block3_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block3_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block3_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block3_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block3_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block3_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block3_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block3_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block3_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block3_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block3_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block3_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block3_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv3_block3_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv3_block2_concat', 0, 0, {}],\n",
       "     ['conv3_block3_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block4_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block4_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block3_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block4_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block4_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block4_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block4_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block4_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block4_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block4_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block4_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block4_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block4_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block4_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block4_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block4_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block4_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block4_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block4_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv3_block4_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv3_block3_concat', 0, 0, {}],\n",
       "     ['conv3_block4_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block5_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block5_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block4_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block5_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block5_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block5_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block5_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block5_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block5_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block5_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block5_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block5_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block5_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block5_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block5_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block5_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block5_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block5_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block5_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv3_block5_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv3_block4_concat', 0, 0, {}],\n",
       "     ['conv3_block5_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block6_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block6_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block5_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block6_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block6_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block6_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block6_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block6_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block6_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block6_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block6_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block6_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block6_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block6_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block6_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block6_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block6_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block6_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block6_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv3_block6_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv3_block5_concat', 0, 0, {}],\n",
       "     ['conv3_block6_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block7_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block7_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block6_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block7_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block7_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block7_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block7_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block7_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block7_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block7_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block7_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block7_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block7_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block7_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block7_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block7_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block7_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block7_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block7_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv3_block7_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv3_block6_concat', 0, 0, {}],\n",
       "     ['conv3_block7_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block8_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block8_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block7_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block8_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block8_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block8_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block8_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block8_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block8_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block8_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block8_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block8_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block8_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block8_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block8_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block8_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block8_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block8_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block8_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv3_block8_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv3_block7_concat', 0, 0, {}],\n",
       "     ['conv3_block8_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block9_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block9_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block8_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block9_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block9_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block9_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block9_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block9_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block9_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block9_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block9_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block9_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block9_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block9_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block9_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block9_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block9_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block9_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block9_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv3_block9_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv3_block8_concat', 0, 0, {}],\n",
       "     ['conv3_block9_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block10_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block10_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block9_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block10_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block10_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block10_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block10_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block10_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block10_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block10_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block10_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block10_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block10_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block10_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block10_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block10_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block10_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block10_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block10_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv3_block10_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv3_block9_concat', 0, 0, {}],\n",
       "     ['conv3_block10_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block11_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block11_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block10_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block11_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block11_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block11_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block11_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block11_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block11_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block11_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block11_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block11_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block11_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block11_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block11_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block11_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block11_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block11_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block11_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv3_block11_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv3_block10_concat', 0, 0, {}],\n",
       "     ['conv3_block11_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block12_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block12_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block11_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block12_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block12_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block12_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block12_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block12_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block12_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block12_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv3_block12_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block12_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block12_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv3_block12_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv3_block12_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block12_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv3_block12_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block12_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv3_block12_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv3_block12_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv3_block11_concat', 0, 0, {}],\n",
       "     ['conv3_block12_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'pool3_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'pool3_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv3_block12_concat', 0, 0, {}]]]},\n",
       "  {'name': 'pool3_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'pool3_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['pool3_bn', 0, 0, {}]]]},\n",
       "  {'name': 'pool3_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'pool3_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['pool3_relu', 0, 0, {}]]]},\n",
       "  {'name': 'pool3_pool',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'pool3_pool',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['pool3_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block1_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block1_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['pool3_pool', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block1_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block1_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block1_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block1_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block1_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block1_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block1_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block1_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block1_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block1_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block1_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block1_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block1_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block1_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block1_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block1_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block1_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['pool3_pool', 0, 0, {}],\n",
       "     ['conv4_block1_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block2_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block2_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block1_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block2_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block2_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block2_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block2_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block2_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block2_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block2_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block2_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block2_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block2_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block2_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block2_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block2_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block2_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block2_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block2_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block2_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block1_concat', 0, 0, {}],\n",
       "     ['conv4_block2_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block3_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block3_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block2_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block3_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block3_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block3_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block3_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block3_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block3_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block3_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block3_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block3_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block3_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block3_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block3_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block3_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block3_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block3_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block3_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block3_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block2_concat', 0, 0, {}],\n",
       "     ['conv4_block3_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block4_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block4_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block3_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block4_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block4_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block4_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block4_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block4_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block4_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block4_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block4_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block4_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block4_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block4_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block4_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block4_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block4_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block4_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block4_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block4_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block3_concat', 0, 0, {}],\n",
       "     ['conv4_block4_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block5_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block5_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block4_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block5_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block5_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block5_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block5_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block5_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block5_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block5_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block5_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block5_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block5_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block5_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block5_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block5_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block5_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block5_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block5_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block5_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block4_concat', 0, 0, {}],\n",
       "     ['conv4_block5_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block6_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block6_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block5_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block6_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block6_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block6_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block6_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block6_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block6_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block6_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block6_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block6_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block6_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block6_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block6_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block6_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block6_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block6_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block6_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block6_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block5_concat', 0, 0, {}],\n",
       "     ['conv4_block6_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block7_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block7_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block6_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block7_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block7_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block7_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block7_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block7_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block7_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block7_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block7_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block7_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block7_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block7_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block7_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block7_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block7_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block7_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block7_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block7_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block6_concat', 0, 0, {}],\n",
       "     ['conv4_block7_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block8_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block8_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block7_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block8_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block8_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block8_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block8_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block8_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block8_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block8_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block8_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block8_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block8_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block8_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block8_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block8_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block8_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block8_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block8_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block8_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block7_concat', 0, 0, {}],\n",
       "     ['conv4_block8_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block9_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block9_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block8_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block9_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block9_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block9_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block9_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block9_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block9_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block9_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block9_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block9_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block9_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block9_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block9_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block9_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block9_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block9_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block9_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block9_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block8_concat', 0, 0, {}],\n",
       "     ['conv4_block9_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block10_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block10_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block9_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block10_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block10_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block10_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block10_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block10_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block10_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block10_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block10_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block10_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block10_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block10_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block10_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block10_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block10_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block10_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block10_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block10_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block9_concat', 0, 0, {}],\n",
       "     ['conv4_block10_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block11_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block11_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block10_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block11_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block11_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block11_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block11_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block11_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block11_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block11_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block11_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block11_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block11_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block11_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block11_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block11_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block11_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block11_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block11_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block11_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block10_concat', 0, 0, {}],\n",
       "     ['conv4_block11_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block12_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block12_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block11_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block12_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block12_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block12_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block12_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block12_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block12_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block12_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block12_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block12_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block12_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block12_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block12_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block12_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block12_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block12_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block12_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block12_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block11_concat', 0, 0, {}],\n",
       "     ['conv4_block12_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block13_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block13_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block12_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block13_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block13_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block13_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block13_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block13_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block13_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block13_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block13_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block13_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block13_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block13_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block13_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block13_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block13_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block13_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block13_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block13_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block12_concat', 0, 0, {}],\n",
       "     ['conv4_block13_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block14_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block14_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block13_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block14_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block14_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block14_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block14_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block14_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block14_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block14_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block14_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block14_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block14_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block14_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block14_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block14_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block14_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block14_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block14_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block14_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block13_concat', 0, 0, {}],\n",
       "     ['conv4_block14_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block15_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block15_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block14_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block15_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block15_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block15_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block15_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block15_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block15_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block15_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block15_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block15_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block15_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block15_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block15_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block15_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block15_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block15_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block15_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block15_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block14_concat', 0, 0, {}],\n",
       "     ['conv4_block15_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block16_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block16_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block15_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block16_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block16_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block16_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block16_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block16_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block16_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block16_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block16_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block16_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block16_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block16_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block16_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block16_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block16_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block16_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block16_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block16_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block15_concat', 0, 0, {}],\n",
       "     ['conv4_block16_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block17_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block17_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block16_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block17_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block17_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block17_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block17_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block17_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block17_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block17_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block17_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block17_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block17_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block17_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block17_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block17_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block17_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block17_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block17_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block17_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block16_concat', 0, 0, {}],\n",
       "     ['conv4_block17_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block18_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block18_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block17_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block18_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block18_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block18_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block18_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block18_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block18_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block18_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block18_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block18_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block18_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block18_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block18_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block18_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block18_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block18_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block18_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block18_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block17_concat', 0, 0, {}],\n",
       "     ['conv4_block18_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block19_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block19_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block18_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block19_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block19_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block19_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block19_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block19_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block19_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block19_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block19_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block19_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block19_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block19_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block19_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block19_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block19_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block19_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block19_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block19_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block18_concat', 0, 0, {}],\n",
       "     ['conv4_block19_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block20_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block20_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block19_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block20_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block20_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block20_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block20_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block20_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block20_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block20_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block20_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block20_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block20_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block20_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block20_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block20_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block20_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block20_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block20_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block20_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block19_concat', 0, 0, {}],\n",
       "     ['conv4_block20_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block21_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block21_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block20_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block21_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block21_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block21_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block21_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block21_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block21_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block21_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block21_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block21_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block21_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block21_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block21_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block21_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block21_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block21_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block21_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block21_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block20_concat', 0, 0, {}],\n",
       "     ['conv4_block21_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block22_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block22_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block21_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block22_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block22_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block22_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block22_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block22_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block22_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block22_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block22_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block22_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block22_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block22_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block22_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block22_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block22_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block22_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block22_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block22_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block21_concat', 0, 0, {}],\n",
       "     ['conv4_block22_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block23_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block23_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block22_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block23_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block23_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block23_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block23_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block23_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block23_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block23_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block23_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block23_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block23_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block23_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block23_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block23_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block23_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block23_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block23_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block23_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block22_concat', 0, 0, {}],\n",
       "     ['conv4_block23_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block24_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block24_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block23_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block24_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block24_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block24_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block24_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block24_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block24_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block24_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv4_block24_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block24_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block24_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv4_block24_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv4_block24_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block24_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv4_block24_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block24_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv4_block24_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv4_block24_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv4_block23_concat', 0, 0, {}],\n",
       "     ['conv4_block24_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'pool4_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'pool4_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv4_block24_concat', 0, 0, {}]]]},\n",
       "  {'name': 'pool4_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'pool4_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['pool4_bn', 0, 0, {}]]]},\n",
       "  {'name': 'pool4_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'pool4_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 512,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['pool4_relu', 0, 0, {}]]]},\n",
       "  {'name': 'pool4_pool',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'pool4_pool',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['pool4_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block1_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block1_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['pool4_pool', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block1_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block1_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block1_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block1_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block1_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block1_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block1_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block1_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block1_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block1_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block1_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block1_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block1_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block1_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block1_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block1_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block1_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['pool4_pool', 0, 0, {}],\n",
       "     ['conv5_block1_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block2_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block2_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block1_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block2_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block2_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block2_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block2_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block2_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block2_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block2_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block2_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block2_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block2_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block2_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block2_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block2_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block2_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block2_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block2_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block2_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv5_block1_concat', 0, 0, {}],\n",
       "     ['conv5_block2_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block3_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block3_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block2_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block3_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block3_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block3_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block3_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block3_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block3_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block3_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block3_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block3_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block3_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block3_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block3_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block3_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block3_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block3_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block3_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block3_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv5_block2_concat', 0, 0, {}],\n",
       "     ['conv5_block3_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block4_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block4_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block3_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block4_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block4_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block4_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block4_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block4_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block4_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block4_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block4_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block4_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block4_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block4_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block4_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block4_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block4_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block4_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block4_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block4_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv5_block3_concat', 0, 0, {}],\n",
       "     ['conv5_block4_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block5_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block5_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block4_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block5_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block5_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block5_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block5_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block5_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block5_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block5_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block5_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block5_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block5_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block5_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block5_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block5_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block5_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block5_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block5_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block5_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv5_block4_concat', 0, 0, {}],\n",
       "     ['conv5_block5_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block6_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block6_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block5_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block6_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block6_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block6_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block6_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block6_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block6_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block6_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block6_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block6_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block6_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block6_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block6_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block6_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block6_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block6_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block6_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block6_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv5_block5_concat', 0, 0, {}],\n",
       "     ['conv5_block6_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block7_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block7_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block6_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block7_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block7_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block7_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block7_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block7_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block7_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block7_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block7_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block7_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block7_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block7_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block7_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block7_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block7_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block7_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block7_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block7_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv5_block6_concat', 0, 0, {}],\n",
       "     ['conv5_block7_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block8_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block8_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block7_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block8_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block8_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block8_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block8_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block8_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block8_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block8_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block8_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block8_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block8_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block8_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block8_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block8_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block8_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block8_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block8_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block8_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv5_block7_concat', 0, 0, {}],\n",
       "     ['conv5_block8_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block9_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block9_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block8_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block9_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block9_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block9_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block9_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block9_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block9_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block9_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block9_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block9_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block9_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block9_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block9_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block9_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block9_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block9_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block9_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block9_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv5_block8_concat', 0, 0, {}],\n",
       "     ['conv5_block9_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block10_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block10_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block9_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block10_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block10_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block10_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block10_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block10_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block10_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block10_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block10_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block10_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block10_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block10_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block10_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block10_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block10_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block10_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block10_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block10_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv5_block9_concat', 0, 0, {}],\n",
       "     ['conv5_block10_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block11_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block11_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block10_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block11_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block11_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block11_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block11_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block11_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block11_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block11_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block11_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block11_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block11_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block11_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block11_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block11_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block11_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block11_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block11_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block11_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv5_block10_concat', 0, 0, {}],\n",
       "     ['conv5_block11_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block12_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block12_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block11_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block12_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block12_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block12_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block12_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block12_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block12_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block12_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block12_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block12_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block12_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block12_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block12_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block12_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block12_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block12_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block12_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block12_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv5_block11_concat', 0, 0, {}],\n",
       "     ['conv5_block12_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block13_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block13_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block12_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block13_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block13_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block13_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block13_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block13_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block13_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block13_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block13_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block13_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block13_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block13_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block13_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block13_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block13_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block13_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block13_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block13_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv5_block12_concat', 0, 0, {}],\n",
       "     ['conv5_block13_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block14_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block14_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block13_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block14_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block14_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block14_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block14_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block14_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block14_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block14_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block14_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block14_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block14_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block14_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block14_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block14_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block14_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block14_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block14_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block14_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv5_block13_concat', 0, 0, {}],\n",
       "     ['conv5_block14_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block15_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block15_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block14_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block15_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block15_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block15_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block15_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block15_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block15_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block15_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block15_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block15_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block15_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block15_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block15_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block15_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block15_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block15_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block15_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block15_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv5_block14_concat', 0, 0, {}],\n",
       "     ['conv5_block15_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block16_0_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block16_0_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block15_concat', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block16_0_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block16_0_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block16_0_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block16_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block16_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block16_0_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block16_1_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv5_block16_1_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block16_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block16_1_relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv5_block16_1_relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv5_block16_1_bn', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block16_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv5_block16_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block16_1_relu', 0, 0, {}]]]},\n",
       "  {'name': 'conv5_block16_concat',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'conv5_block16_concat',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['conv5_block15_concat', 0, 0, {}],\n",
       "     ['conv5_block16_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 1.001e-05,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv5_block16_concat', 0, 0, {}]]]},\n",
       "  {'name': 'relu',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'relu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['bn', 0, 0, {}]]]}],\n",
       " 'input_layers': [['input_1', 0, 0]],\n",
       " 'output_layers': [['relu', 0, 0]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-21 20:22:51.627274\n",
      "(13022, 7, 7, 1024)\n",
      "2020-03-21 22:10:49.117849\n"
     ]
    }
   ],
   "source": [
    "t1=datetime.datetime.now()\n",
    "print(t1)\n",
    "# extracting features for training frames\n",
    "X_train = base_model.predict(X_train)\n",
    "print(X_train.shape)\n",
    "t2=datetime.datetime.now()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-21 22:10:49.152777\n",
      "(5509, 7, 7, 1024)\n",
      "2020-03-21 22:47:23.819955\n"
     ]
    }
   ],
   "source": [
    "t3=datetime.datetime.now()\n",
    "print(t3)\n",
    "# extracting features for validation frames\n",
    "X_test = base_model.predict(X_test)\n",
    "print(X_test.shape)\n",
    "t4=datetime.datetime.now()\n",
    "print(t4-t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the training as well as validation frames in single dimension\n",
    "X_train = X_train.reshape(13022, 7*7*1024)\n",
    "X_test = X_test.reshape(5509, 7*7*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the pixel values\n",
    "max = X_train.max()\n",
    "X_train = X_train/max\n",
    "X_test = X_test/max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Pickle/DenseNet121_X_test.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model as a pickle in a file \n",
    "joblib.dump(X_train, '../Pickle/DenseNet121_X_train.pkl') \n",
    "joblib.dump(X_test, '../Pickle/DenseNet121_X_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file \n",
    "X_train = joblib.load('../Pickle/DenseNet121_X_train.pkl') \n",
    "X_test = joblib.load('../Pickle/DenseNet121_X_test.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13022, 50176)\n",
      "(5509, 50176)\n",
      "(13022, 51)\n",
      "(5509, 51)\n"
     ]
    }
   ],
   "source": [
    "# shape of images\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(50176,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(51, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 51)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              51381248  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 51)                6579      \n",
      "=================================================================\n",
      "Total params: 52,076,851\n",
      "Trainable params: 52,076,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential_1',\n",
       " 'layers': [{'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_1',\n",
       "    'trainable': True,\n",
       "    'batch_input_shape': (None, 50176),\n",
       "    'dtype': 'float32',\n",
       "    'units': 1024,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 512,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 256,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 128,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 51,\n",
       "    'activation': 'softmax',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_save = ModelCheckpoint('../Models/weightDenseNet121.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-21 22:48:05.355562\n",
      "Train on 13022 samples, validate on 5509 samples\n",
      "Epoch 1/100\n",
      "13022/13022 [==============================] - ETA: 7:40 - loss: 3.9353 - accuracy: 0.03 - ETA: 4:44 - loss: 3.9533 - accuracy: 0.03 - ETA: 3:42 - loss: 3.9665 - accuracy: 0.03 - ETA: 3:11 - loss: 3.9831 - accuracy: 0.03 - ETA: 2:52 - loss: 4.0019 - accuracy: 0.03 - ETA: 2:39 - loss: 4.0050 - accuracy: 0.03 - ETA: 2:28 - loss: 4.0045 - accuracy: 0.03 - ETA: 2:21 - loss: 4.0045 - accuracy: 0.03 - ETA: 2:15 - loss: 3.9978 - accuracy: 0.03 - ETA: 2:09 - loss: 3.9993 - accuracy: 0.03 - ETA: 2:05 - loss: 3.9966 - accuracy: 0.03 - ETA: 2:01 - loss: 3.9918 - accuracy: 0.03 - ETA: 1:59 - loss: 3.9837 - accuracy: 0.03 - ETA: 1:55 - loss: 3.9798 - accuracy: 0.03 - ETA: 1:52 - loss: 3.9764 - accuracy: 0.03 - ETA: 1:49 - loss: 3.9737 - accuracy: 0.03 - ETA: 1:46 - loss: 3.9690 - accuracy: 0.03 - ETA: 1:44 - loss: 3.9629 - accuracy: 0.03 - ETA: 1:42 - loss: 3.9624 - accuracy: 0.03 - ETA: 1:39 - loss: 3.9622 - accuracy: 0.03 - ETA: 1:37 - loss: 3.9607 - accuracy: 0.03 - ETA: 1:35 - loss: 3.9591 - accuracy: 0.03 - ETA: 1:34 - loss: 3.9595 - accuracy: 0.03 - ETA: 1:32 - loss: 3.9589 - accuracy: 0.03 - ETA: 1:30 - loss: 3.9556 - accuracy: 0.03 - ETA: 1:28 - loss: 3.9520 - accuracy: 0.03 - ETA: 1:27 - loss: 3.9511 - accuracy: 0.03 - ETA: 1:25 - loss: 3.9503 - accuracy: 0.03 - ETA: 1:23 - loss: 3.9501 - accuracy: 0.03 - ETA: 1:22 - loss: 3.9473 - accuracy: 0.03 - ETA: 1:20 - loss: 3.9484 - accuracy: 0.03 - ETA: 1:19 - loss: 3.9470 - accuracy: 0.03 - ETA: 1:17 - loss: 3.9468 - accuracy: 0.03 - ETA: 1:16 - loss: 3.9452 - accuracy: 0.03 - ETA: 1:15 - loss: 3.9427 - accuracy: 0.03 - ETA: 1:13 - loss: 3.9404 - accuracy: 0.03 - ETA: 1:12 - loss: 3.9370 - accuracy: 0.03 - ETA: 1:11 - loss: 3.9343 - accuracy: 0.03 - ETA: 1:10 - loss: 3.9330 - accuracy: 0.03 - ETA: 1:08 - loss: 3.9319 - accuracy: 0.03 - ETA: 1:07 - loss: 3.9312 - accuracy: 0.03 - ETA: 1:06 - loss: 3.9293 - accuracy: 0.03 - ETA: 1:05 - loss: 3.9279 - accuracy: 0.03 - ETA: 1:03 - loss: 3.9260 - accuracy: 0.03 - ETA: 1:02 - loss: 3.9255 - accuracy: 0.03 - ETA: 1:01 - loss: 3.9243 - accuracy: 0.03 - ETA: 1:00 - loss: 3.9212 - accuracy: 0.03 - ETA: 59s - loss: 3.9193 - accuracy: 0.0387 - ETA: 58s - loss: 3.9185 - accuracy: 0.038 - ETA: 56s - loss: 3.9178 - accuracy: 0.037 - ETA: 55s - loss: 3.9155 - accuracy: 0.038 - ETA: 54s - loss: 3.9142 - accuracy: 0.038 - ETA: 53s - loss: 3.9137 - accuracy: 0.038 - ETA: 52s - loss: 3.9125 - accuracy: 0.038 - ETA: 50s - loss: 3.9105 - accuracy: 0.038 - ETA: 49s - loss: 3.9086 - accuracy: 0.038 - ETA: 48s - loss: 3.9065 - accuracy: 0.038 - ETA: 47s - loss: 3.9037 - accuracy: 0.039 - ETA: 46s - loss: 3.9029 - accuracy: 0.039 - ETA: 45s - loss: 3.9006 - accuracy: 0.039 - ETA: 44s - loss: 3.8997 - accuracy: 0.038 - ETA: 43s - loss: 3.8978 - accuracy: 0.039 - ETA: 41s - loss: 3.8966 - accuracy: 0.039 - ETA: 40s - loss: 3.8949 - accuracy: 0.039 - ETA: 39s - loss: 3.8931 - accuracy: 0.039 - ETA: 38s - loss: 3.8908 - accuracy: 0.040 - ETA: 37s - loss: 3.8902 - accuracy: 0.040 - ETA: 36s - loss: 3.8884 - accuracy: 0.040 - ETA: 35s - loss: 3.8863 - accuracy: 0.040 - ETA: 34s - loss: 3.8837 - accuracy: 0.041 - ETA: 33s - loss: 3.8812 - accuracy: 0.041 - ETA: 31s - loss: 3.8800 - accuracy: 0.041 - ETA: 30s - loss: 3.8770 - accuracy: 0.041 - ETA: 29s - loss: 3.8750 - accuracy: 0.042 - ETA: 28s - loss: 3.8734 - accuracy: 0.042 - ETA: 27s - loss: 3.8718 - accuracy: 0.043 - ETA: 26s - loss: 3.8686 - accuracy: 0.043 - ETA: 25s - loss: 3.8670 - accuracy: 0.043 - ETA: 24s - loss: 3.8650 - accuracy: 0.043 - ETA: 23s - loss: 3.8640 - accuracy: 0.043 - ETA: 22s - loss: 3.8609 - accuracy: 0.044 - ETA: 21s - loss: 3.8590 - accuracy: 0.044 - ETA: 19s - loss: 3.8577 - accuracy: 0.044 - ETA: 18s - loss: 3.8549 - accuracy: 0.045 - ETA: 17s - loss: 3.8530 - accuracy: 0.045 - ETA: 16s - loss: 3.8514 - accuracy: 0.046 - ETA: 15s - loss: 3.8484 - accuracy: 0.046 - ETA: 14s - loss: 3.8467 - accuracy: 0.046 - ETA: 13s - loss: 3.8444 - accuracy: 0.046 - ETA: 12s - loss: 3.8413 - accuracy: 0.047 - ETA: 11s - loss: 3.8387 - accuracy: 0.048 - ETA: 10s - loss: 3.8364 - accuracy: 0.048 - ETA: 9s - loss: 3.8352 - accuracy: 0.048 - ETA: 8s - loss: 3.8330 - accuracy: 0.04 - ETA: 7s - loss: 3.8311 - accuracy: 0.04 - ETA: 6s - loss: 3.8290 - accuracy: 0.04 - ETA: 5s - loss: 3.8251 - accuracy: 0.05 - ETA: 3s - loss: 3.8218 - accuracy: 0.05 - ETA: 2s - loss: 3.8176 - accuracy: 0.05 - ETA: 1s - loss: 3.8136 - accuracy: 0.05 - ETA: 0s - loss: 3.8109 - accuracy: 0.05 - 124s 9ms/step - loss: 3.8088 - accuracy: 0.0539 - val_loss: 3.8040 - val_accuracy: 0.0670\n",
      "Epoch 2/100\n",
      "13022/13022 [==============================] - ETA: 1:42 - loss: 3.6019 - accuracy: 0.09 - ETA: 1:42 - loss: 3.6079 - accuracy: 0.10 - ETA: 1:42 - loss: 3.5814 - accuracy: 0.10 - ETA: 1:39 - loss: 3.5287 - accuracy: 0.11 - ETA: 1:37 - loss: 3.5571 - accuracy: 0.10 - ETA: 1:36 - loss: 3.5558 - accuracy: 0.10 - ETA: 1:35 - loss: 3.5424 - accuracy: 0.11 - ETA: 1:35 - loss: 3.5357 - accuracy: 0.11 - ETA: 1:34 - loss: 3.5120 - accuracy: 0.11 - ETA: 1:34 - loss: 3.5194 - accuracy: 0.11 - ETA: 1:32 - loss: 3.5198 - accuracy: 0.11 - ETA: 1:31 - loss: 3.5156 - accuracy: 0.11 - ETA: 1:29 - loss: 3.5239 - accuracy: 0.11 - ETA: 1:28 - loss: 3.5192 - accuracy: 0.11 - ETA: 1:27 - loss: 3.5162 - accuracy: 0.11 - ETA: 1:26 - loss: 3.5018 - accuracy: 0.11 - ETA: 1:25 - loss: 3.5005 - accuracy: 0.11 - ETA: 1:24 - loss: 3.4899 - accuracy: 0.11 - ETA: 1:23 - loss: 3.4873 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4792 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4690 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4685 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4638 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4640 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4562 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4562 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4476 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4414 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4384 - accuracy: 0.13 - ETA: 1:11 - loss: 3.4275 - accuracy: 0.13 - ETA: 1:10 - loss: 3.4184 - accuracy: 0.13 - ETA: 1:09 - loss: 3.4155 - accuracy: 0.13 - ETA: 1:08 - loss: 3.4097 - accuracy: 0.13 - ETA: 1:07 - loss: 3.4038 - accuracy: 0.13 - ETA: 1:06 - loss: 3.4005 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3950 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3928 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3889 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3863 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3852 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3788 - accuracy: 0.14 - ETA: 59s - loss: 3.3776 - accuracy: 0.1412 - ETA: 58s - loss: 3.3743 - accuracy: 0.141 - ETA: 57s - loss: 3.3687 - accuracy: 0.142 - ETA: 56s - loss: 3.3676 - accuracy: 0.142 - ETA: 55s - loss: 3.3623 - accuracy: 0.143 - ETA: 54s - loss: 3.3610 - accuracy: 0.143 - ETA: 54s - loss: 3.3585 - accuracy: 0.144 - ETA: 53s - loss: 3.3544 - accuracy: 0.145 - ETA: 52s - loss: 3.3511 - accuracy: 0.146 - ETA: 51s - loss: 3.3504 - accuracy: 0.147 - ETA: 50s - loss: 3.3466 - accuracy: 0.148 - ETA: 49s - loss: 3.3460 - accuracy: 0.148 - ETA: 48s - loss: 3.3426 - accuracy: 0.148 - ETA: 47s - loss: 3.3401 - accuracy: 0.148 - ETA: 46s - loss: 3.3395 - accuracy: 0.148 - ETA: 45s - loss: 3.3376 - accuracy: 0.149 - ETA: 44s - loss: 3.3350 - accuracy: 0.150 - ETA: 43s - loss: 3.3313 - accuracy: 0.150 - ETA: 42s - loss: 3.3297 - accuracy: 0.150 - ETA: 41s - loss: 3.3263 - accuracy: 0.151 - ETA: 40s - loss: 3.3230 - accuracy: 0.152 - ETA: 39s - loss: 3.3217 - accuracy: 0.153 - ETA: 38s - loss: 3.3188 - accuracy: 0.153 - ETA: 37s - loss: 3.3184 - accuracy: 0.153 - ETA: 36s - loss: 3.3133 - accuracy: 0.155 - ETA: 35s - loss: 3.3096 - accuracy: 0.155 - ETA: 34s - loss: 3.3090 - accuracy: 0.155 - ETA: 33s - loss: 3.3076 - accuracy: 0.155 - ETA: 32s - loss: 3.3034 - accuracy: 0.157 - ETA: 31s - loss: 3.2991 - accuracy: 0.158 - ETA: 30s - loss: 3.2922 - accuracy: 0.159 - ETA: 29s - loss: 3.2929 - accuracy: 0.159 - ETA: 28s - loss: 3.2895 - accuracy: 0.159 - ETA: 27s - loss: 3.2863 - accuracy: 0.160 - ETA: 26s - loss: 3.2818 - accuracy: 0.161 - ETA: 25s - loss: 3.2807 - accuracy: 0.161 - ETA: 24s - loss: 3.2792 - accuracy: 0.161 - ETA: 23s - loss: 3.2764 - accuracy: 0.162 - ETA: 22s - loss: 3.2745 - accuracy: 0.162 - ETA: 21s - loss: 3.2705 - accuracy: 0.163 - ETA: 20s - loss: 3.2663 - accuracy: 0.164 - ETA: 19s - loss: 3.2646 - accuracy: 0.164 - ETA: 18s - loss: 3.2573 - accuracy: 0.165 - ETA: 17s - loss: 3.2520 - accuracy: 0.166 - ETA: 16s - loss: 3.2497 - accuracy: 0.167 - ETA: 15s - loss: 3.2487 - accuracy: 0.168 - ETA: 14s - loss: 3.2477 - accuracy: 0.167 - ETA: 12s - loss: 3.2449 - accuracy: 0.168 - ETA: 11s - loss: 3.2428 - accuracy: 0.169 - ETA: 10s - loss: 3.2413 - accuracy: 0.169 - ETA: 9s - loss: 3.2403 - accuracy: 0.169 - ETA: 8s - loss: 3.2394 - accuracy: 0.17 - ETA: 7s - loss: 3.2375 - accuracy: 0.17 - ETA: 6s - loss: 3.2374 - accuracy: 0.17 - ETA: 5s - loss: 3.2331 - accuracy: 0.17 - ETA: 4s - loss: 3.2296 - accuracy: 0.17 - ETA: 3s - loss: 3.2282 - accuracy: 0.17 - ETA: 2s - loss: 3.2259 - accuracy: 0.17 - ETA: 1s - loss: 3.2258 - accuracy: 0.17 - ETA: 0s - loss: 3.2215 - accuracy: 0.17 - 119s 9ms/step - loss: 3.2212 - accuracy: 0.1736 - val_loss: 3.5275 - val_accuracy: 0.1011\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:42 - loss: 2.8659 - accuracy: 0.24 - ETA: 1:40 - loss: 2.9646 - accuracy: 0.20 - ETA: 1:41 - loss: 2.9548 - accuracy: 0.21 - ETA: 1:41 - loss: 2.9587 - accuracy: 0.21 - ETA: 1:40 - loss: 2.9644 - accuracy: 0.22 - ETA: 1:39 - loss: 2.9573 - accuracy: 0.21 - ETA: 1:38 - loss: 2.9482 - accuracy: 0.21 - ETA: 1:36 - loss: 2.9483 - accuracy: 0.21 - ETA: 1:35 - loss: 2.9389 - accuracy: 0.21 - ETA: 1:34 - loss: 2.9317 - accuracy: 0.22 - ETA: 1:32 - loss: 2.9135 - accuracy: 0.22 - ETA: 1:31 - loss: 2.9221 - accuracy: 0.22 - ETA: 1:29 - loss: 2.9260 - accuracy: 0.22 - ETA: 1:28 - loss: 2.9307 - accuracy: 0.22 - ETA: 1:27 - loss: 2.9378 - accuracy: 0.22 - ETA: 1:26 - loss: 2.9382 - accuracy: 0.22 - ETA: 1:25 - loss: 2.9320 - accuracy: 0.22 - ETA: 1:24 - loss: 2.9224 - accuracy: 0.23 - ETA: 1:23 - loss: 2.9195 - accuracy: 0.23 - ETA: 1:22 - loss: 2.9150 - accuracy: 0.23 - ETA: 1:21 - loss: 2.8959 - accuracy: 0.23 - ETA: 1:20 - loss: 2.8913 - accuracy: 0.23 - ETA: 1:19 - loss: 2.8867 - accuracy: 0.23 - ETA: 1:18 - loss: 2.8821 - accuracy: 0.23 - ETA: 1:17 - loss: 2.8781 - accuracy: 0.23 - ETA: 1:16 - loss: 2.8765 - accuracy: 0.24 - ETA: 1:15 - loss: 2.8692 - accuracy: 0.24 - ETA: 1:14 - loss: 2.8662 - accuracy: 0.24 - ETA: 1:13 - loss: 2.8700 - accuracy: 0.24 - ETA: 1:12 - loss: 2.8675 - accuracy: 0.24 - ETA: 1:11 - loss: 2.8652 - accuracy: 0.24 - ETA: 1:10 - loss: 2.8686 - accuracy: 0.24 - ETA: 1:08 - loss: 2.8615 - accuracy: 0.24 - ETA: 1:07 - loss: 2.8570 - accuracy: 0.24 - ETA: 1:06 - loss: 2.8558 - accuracy: 0.24 - ETA: 1:05 - loss: 2.8579 - accuracy: 0.24 - ETA: 1:04 - loss: 2.8490 - accuracy: 0.24 - ETA: 1:03 - loss: 2.8426 - accuracy: 0.24 - ETA: 1:02 - loss: 2.8369 - accuracy: 0.24 - ETA: 1:02 - loss: 2.8296 - accuracy: 0.25 - ETA: 1:01 - loss: 2.8181 - accuracy: 0.25 - ETA: 1:00 - loss: 2.8166 - accuracy: 0.25 - ETA: 59s - loss: 2.8201 - accuracy: 0.2538 - ETA: 58s - loss: 2.8200 - accuracy: 0.253 - ETA: 57s - loss: 2.8099 - accuracy: 0.255 - ETA: 56s - loss: 2.8129 - accuracy: 0.255 - ETA: 55s - loss: 2.8092 - accuracy: 0.256 - ETA: 54s - loss: 2.8025 - accuracy: 0.257 - ETA: 53s - loss: 2.8026 - accuracy: 0.258 - ETA: 52s - loss: 2.8000 - accuracy: 0.258 - ETA: 51s - loss: 2.7970 - accuracy: 0.259 - ETA: 50s - loss: 2.7924 - accuracy: 0.260 - ETA: 49s - loss: 2.7906 - accuracy: 0.259 - ETA: 48s - loss: 2.7876 - accuracy: 0.261 - ETA: 47s - loss: 2.7803 - accuracy: 0.262 - ETA: 46s - loss: 2.7801 - accuracy: 0.262 - ETA: 45s - loss: 2.7754 - accuracy: 0.264 - ETA: 44s - loss: 2.7768 - accuracy: 0.263 - ETA: 43s - loss: 2.7775 - accuracy: 0.263 - ETA: 42s - loss: 2.7771 - accuracy: 0.262 - ETA: 41s - loss: 2.7724 - accuracy: 0.264 - ETA: 40s - loss: 2.7742 - accuracy: 0.263 - ETA: 39s - loss: 2.7704 - accuracy: 0.264 - ETA: 38s - loss: 2.7710 - accuracy: 0.265 - ETA: 37s - loss: 2.7696 - accuracy: 0.265 - ETA: 36s - loss: 2.7694 - accuracy: 0.265 - ETA: 35s - loss: 2.7645 - accuracy: 0.266 - ETA: 34s - loss: 2.7646 - accuracy: 0.266 - ETA: 33s - loss: 2.7630 - accuracy: 0.267 - ETA: 32s - loss: 2.7624 - accuracy: 0.267 - ETA: 31s - loss: 2.7640 - accuracy: 0.267 - ETA: 30s - loss: 2.7622 - accuracy: 0.267 - ETA: 29s - loss: 2.7595 - accuracy: 0.268 - ETA: 28s - loss: 2.7618 - accuracy: 0.267 - ETA: 27s - loss: 2.7565 - accuracy: 0.268 - ETA: 26s - loss: 2.7524 - accuracy: 0.269 - ETA: 25s - loss: 2.7480 - accuracy: 0.270 - ETA: 24s - loss: 2.7424 - accuracy: 0.272 - ETA: 23s - loss: 2.7432 - accuracy: 0.272 - ETA: 22s - loss: 2.7402 - accuracy: 0.273 - ETA: 21s - loss: 2.7364 - accuracy: 0.274 - ETA: 20s - loss: 2.7382 - accuracy: 0.273 - ETA: 19s - loss: 2.7377 - accuracy: 0.274 - ETA: 18s - loss: 2.7355 - accuracy: 0.274 - ETA: 17s - loss: 2.7329 - accuracy: 0.274 - ETA: 16s - loss: 2.7258 - accuracy: 0.276 - ETA: 15s - loss: 2.7223 - accuracy: 0.277 - ETA: 14s - loss: 2.7201 - accuracy: 0.278 - ETA: 13s - loss: 2.7155 - accuracy: 0.278 - ETA: 11s - loss: 2.7128 - accuracy: 0.279 - ETA: 10s - loss: 2.7127 - accuracy: 0.279 - ETA: 9s - loss: 2.7124 - accuracy: 0.280 - ETA: 8s - loss: 2.7087 - accuracy: 0.28 - ETA: 7s - loss: 2.7057 - accuracy: 0.28 - ETA: 6s - loss: 2.7060 - accuracy: 0.28 - ETA: 5s - loss: 2.7036 - accuracy: 0.28 - ETA: 4s - loss: 2.6996 - accuracy: 0.28 - ETA: 3s - loss: 2.6970 - accuracy: 0.28 - ETA: 2s - loss: 2.6968 - accuracy: 0.28 - ETA: 1s - loss: 2.6948 - accuracy: 0.28 - ETA: 0s - loss: 2.6926 - accuracy: 0.28 - 119s 9ms/step - loss: 2.6897 - accuracy: 0.2855 - val_loss: 3.2543 - val_accuracy: 0.1893\n",
      "Epoch 4/100\n",
      "13022/13022 [==============================] - ETA: 1:40 - loss: 2.4274 - accuracy: 0.34 - ETA: 1:41 - loss: 2.3934 - accuracy: 0.35 - ETA: 1:39 - loss: 2.3809 - accuracy: 0.35 - ETA: 1:39 - loss: 2.3763 - accuracy: 0.34 - ETA: 1:39 - loss: 2.4084 - accuracy: 0.33 - ETA: 1:37 - loss: 2.3809 - accuracy: 0.34 - ETA: 1:35 - loss: 2.3861 - accuracy: 0.34 - ETA: 1:33 - loss: 2.3836 - accuracy: 0.34 - ETA: 1:32 - loss: 2.4009 - accuracy: 0.34 - ETA: 1:31 - loss: 2.4132 - accuracy: 0.33 - ETA: 1:29 - loss: 2.4137 - accuracy: 0.33 - ETA: 1:28 - loss: 2.4064 - accuracy: 0.33 - ETA: 1:27 - loss: 2.3965 - accuracy: 0.33 - ETA: 1:26 - loss: 2.4024 - accuracy: 0.34 - ETA: 1:25 - loss: 2.3954 - accuracy: 0.34 - ETA: 1:24 - loss: 2.3807 - accuracy: 0.35 - ETA: 1:23 - loss: 2.3720 - accuracy: 0.35 - ETA: 1:22 - loss: 2.3662 - accuracy: 0.34 - ETA: 1:21 - loss: 2.3817 - accuracy: 0.34 - ETA: 1:20 - loss: 2.3648 - accuracy: 0.35 - ETA: 1:19 - loss: 2.3773 - accuracy: 0.34 - ETA: 1:18 - loss: 2.3802 - accuracy: 0.34 - ETA: 1:17 - loss: 2.3822 - accuracy: 0.34 - ETA: 1:16 - loss: 2.3786 - accuracy: 0.34 - ETA: 1:15 - loss: 2.3710 - accuracy: 0.35 - ETA: 1:14 - loss: 2.3591 - accuracy: 0.35 - ETA: 1:13 - loss: 2.3547 - accuracy: 0.35 - ETA: 1:12 - loss: 2.3590 - accuracy: 0.35 - ETA: 1:11 - loss: 2.3593 - accuracy: 0.35 - ETA: 1:10 - loss: 2.3568 - accuracy: 0.35 - ETA: 1:09 - loss: 2.3561 - accuracy: 0.35 - ETA: 1:08 - loss: 2.3550 - accuracy: 0.35 - ETA: 1:07 - loss: 2.3593 - accuracy: 0.35 - ETA: 1:06 - loss: 2.3549 - accuracy: 0.35 - ETA: 1:05 - loss: 2.3480 - accuracy: 0.35 - ETA: 1:04 - loss: 2.3455 - accuracy: 0.36 - ETA: 1:03 - loss: 2.3501 - accuracy: 0.35 - ETA: 1:02 - loss: 2.3494 - accuracy: 0.35 - ETA: 1:01 - loss: 2.3439 - accuracy: 0.36 - ETA: 1:01 - loss: 2.3406 - accuracy: 0.36 - ETA: 59s - loss: 2.3430 - accuracy: 0.3598 - ETA: 59s - loss: 2.3398 - accuracy: 0.361 - ETA: 58s - loss: 2.3354 - accuracy: 0.362 - ETA: 57s - loss: 2.3320 - accuracy: 0.363 - ETA: 56s - loss: 2.3242 - accuracy: 0.365 - ETA: 55s - loss: 2.3239 - accuracy: 0.364 - ETA: 54s - loss: 2.3274 - accuracy: 0.363 - ETA: 53s - loss: 2.3251 - accuracy: 0.364 - ETA: 52s - loss: 2.3209 - accuracy: 0.365 - ETA: 51s - loss: 2.3184 - accuracy: 0.366 - ETA: 50s - loss: 2.3214 - accuracy: 0.366 - ETA: 49s - loss: 2.3188 - accuracy: 0.366 - ETA: 48s - loss: 2.3143 - accuracy: 0.367 - ETA: 47s - loss: 2.3099 - accuracy: 0.370 - ETA: 46s - loss: 2.3050 - accuracy: 0.372 - ETA: 45s - loss: 2.3022 - accuracy: 0.372 - ETA: 44s - loss: 2.3018 - accuracy: 0.371 - ETA: 43s - loss: 2.2997 - accuracy: 0.372 - ETA: 42s - loss: 2.2974 - accuracy: 0.373 - ETA: 41s - loss: 2.2966 - accuracy: 0.374 - ETA: 40s - loss: 2.2942 - accuracy: 0.375 - ETA: 39s - loss: 2.2919 - accuracy: 0.375 - ETA: 38s - loss: 2.2922 - accuracy: 0.375 - ETA: 37s - loss: 2.2931 - accuracy: 0.375 - ETA: 36s - loss: 2.2882 - accuracy: 0.376 - ETA: 35s - loss: 2.2897 - accuracy: 0.376 - ETA: 34s - loss: 2.2843 - accuracy: 0.377 - ETA: 33s - loss: 2.2869 - accuracy: 0.376 - ETA: 32s - loss: 2.2838 - accuracy: 0.377 - ETA: 31s - loss: 2.2812 - accuracy: 0.376 - ETA: 30s - loss: 2.2779 - accuracy: 0.377 - ETA: 29s - loss: 2.2777 - accuracy: 0.377 - ETA: 28s - loss: 2.2739 - accuracy: 0.378 - ETA: 27s - loss: 2.2764 - accuracy: 0.377 - ETA: 26s - loss: 2.2742 - accuracy: 0.377 - ETA: 25s - loss: 2.2724 - accuracy: 0.378 - ETA: 24s - loss: 2.2707 - accuracy: 0.378 - ETA: 23s - loss: 2.2698 - accuracy: 0.378 - ETA: 22s - loss: 2.2690 - accuracy: 0.378 - ETA: 21s - loss: 2.2720 - accuracy: 0.377 - ETA: 20s - loss: 2.2706 - accuracy: 0.377 - ETA: 19s - loss: 2.2711 - accuracy: 0.378 - ETA: 18s - loss: 2.2661 - accuracy: 0.379 - ETA: 17s - loss: 2.2652 - accuracy: 0.380 - ETA: 16s - loss: 2.2611 - accuracy: 0.381 - ETA: 15s - loss: 2.2587 - accuracy: 0.382 - ETA: 14s - loss: 2.2555 - accuracy: 0.383 - ETA: 13s - loss: 2.2560 - accuracy: 0.383 - ETA: 12s - loss: 2.2552 - accuracy: 0.383 - ETA: 11s - loss: 2.2547 - accuracy: 0.383 - ETA: 10s - loss: 2.2533 - accuracy: 0.383 - ETA: 9s - loss: 2.2521 - accuracy: 0.383 - ETA: 8s - loss: 2.2529 - accuracy: 0.38 - ETA: 7s - loss: 2.2530 - accuracy: 0.38 - ETA: 6s - loss: 2.2496 - accuracy: 0.38 - ETA: 5s - loss: 2.2488 - accuracy: 0.38 - ETA: 4s - loss: 2.2483 - accuracy: 0.38 - ETA: 3s - loss: 2.2478 - accuracy: 0.38 - ETA: 2s - loss: 2.2448 - accuracy: 0.38 - ETA: 1s - loss: 2.2430 - accuracy: 0.38 - ETA: 0s - loss: 2.2462 - accuracy: 0.38 - 119s 9ms/step - loss: 2.2461 - accuracy: 0.3878 - val_loss: 2.9446 - val_accuracy: 0.2870\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:44 - loss: 2.2960 - accuracy: 0.35 - ETA: 1:42 - loss: 2.1642 - accuracy: 0.38 - ETA: 1:39 - loss: 2.1172 - accuracy: 0.39 - ETA: 1:37 - loss: 2.1097 - accuracy: 0.41 - ETA: 1:36 - loss: 2.0945 - accuracy: 0.41 - ETA: 1:35 - loss: 2.0752 - accuracy: 0.42 - ETA: 1:33 - loss: 2.0562 - accuracy: 0.42 - ETA: 1:33 - loss: 2.0660 - accuracy: 0.42 - ETA: 1:32 - loss: 2.0650 - accuracy: 0.42 - ETA: 1:31 - loss: 2.0626 - accuracy: 0.42 - ETA: 1:30 - loss: 2.0745 - accuracy: 0.42 - ETA: 1:29 - loss: 2.0526 - accuracy: 0.43 - ETA: 1:28 - loss: 2.0580 - accuracy: 0.42 - ETA: 1:27 - loss: 2.0574 - accuracy: 0.43 - ETA: 1:26 - loss: 2.0563 - accuracy: 0.43 - ETA: 1:25 - loss: 2.0529 - accuracy: 0.43 - ETA: 1:23 - loss: 2.0597 - accuracy: 0.43 - ETA: 1:22 - loss: 2.0534 - accuracy: 0.43 - ETA: 1:22 - loss: 2.0618 - accuracy: 0.43 - ETA: 1:20 - loss: 2.0522 - accuracy: 0.43 - ETA: 1:19 - loss: 2.0589 - accuracy: 0.43 - ETA: 1:19 - loss: 2.0688 - accuracy: 0.43 - ETA: 1:17 - loss: 2.0588 - accuracy: 0.43 - ETA: 1:16 - loss: 2.0567 - accuracy: 0.43 - ETA: 1:15 - loss: 2.0602 - accuracy: 0.43 - ETA: 1:14 - loss: 2.0523 - accuracy: 0.43 - ETA: 1:14 - loss: 2.0434 - accuracy: 0.43 - ETA: 1:13 - loss: 2.0448 - accuracy: 0.43 - ETA: 1:12 - loss: 2.0385 - accuracy: 0.43 - ETA: 1:11 - loss: 2.0367 - accuracy: 0.43 - ETA: 1:10 - loss: 2.0306 - accuracy: 0.43 - ETA: 1:09 - loss: 2.0326 - accuracy: 0.43 - ETA: 1:09 - loss: 2.0300 - accuracy: 0.43 - ETA: 1:08 - loss: 2.0185 - accuracy: 0.44 - ETA: 1:07 - loss: 2.0278 - accuracy: 0.43 - ETA: 1:06 - loss: 2.0314 - accuracy: 0.43 - ETA: 1:05 - loss: 2.0318 - accuracy: 0.43 - ETA: 1:04 - loss: 2.0292 - accuracy: 0.43 - ETA: 1:03 - loss: 2.0270 - accuracy: 0.43 - ETA: 1:02 - loss: 2.0250 - accuracy: 0.43 - ETA: 1:01 - loss: 2.0246 - accuracy: 0.44 - ETA: 1:00 - loss: 2.0265 - accuracy: 0.44 - ETA: 59s - loss: 2.0296 - accuracy: 0.4408 - ETA: 58s - loss: 2.0279 - accuracy: 0.441 - ETA: 57s - loss: 2.0325 - accuracy: 0.438 - ETA: 56s - loss: 2.0312 - accuracy: 0.438 - ETA: 55s - loss: 2.0298 - accuracy: 0.438 - ETA: 54s - loss: 2.0308 - accuracy: 0.437 - ETA: 53s - loss: 2.0300 - accuracy: 0.437 - ETA: 52s - loss: 2.0352 - accuracy: 0.435 - ETA: 51s - loss: 2.0301 - accuracy: 0.437 - ETA: 50s - loss: 2.0302 - accuracy: 0.437 - ETA: 49s - loss: 2.0278 - accuracy: 0.438 - ETA: 48s - loss: 2.0274 - accuracy: 0.438 - ETA: 47s - loss: 2.0249 - accuracy: 0.437 - ETA: 46s - loss: 2.0268 - accuracy: 0.437 - ETA: 45s - loss: 2.0274 - accuracy: 0.437 - ETA: 44s - loss: 2.0280 - accuracy: 0.437 - ETA: 43s - loss: 2.0313 - accuracy: 0.436 - ETA: 42s - loss: 2.0279 - accuracy: 0.437 - ETA: 41s - loss: 2.0257 - accuracy: 0.437 - ETA: 40s - loss: 2.0236 - accuracy: 0.438 - ETA: 39s - loss: 2.0244 - accuracy: 0.438 - ETA: 38s - loss: 2.0196 - accuracy: 0.439 - ETA: 37s - loss: 2.0191 - accuracy: 0.440 - ETA: 36s - loss: 2.0146 - accuracy: 0.441 - ETA: 35s - loss: 2.0116 - accuracy: 0.442 - ETA: 34s - loss: 2.0141 - accuracy: 0.441 - ETA: 33s - loss: 2.0121 - accuracy: 0.441 - ETA: 32s - loss: 2.0106 - accuracy: 0.441 - ETA: 31s - loss: 2.0081 - accuracy: 0.442 - ETA: 30s - loss: 2.0086 - accuracy: 0.441 - ETA: 29s - loss: 2.0097 - accuracy: 0.441 - ETA: 28s - loss: 2.0073 - accuracy: 0.441 - ETA: 27s - loss: 2.0044 - accuracy: 0.442 - ETA: 26s - loss: 2.0028 - accuracy: 0.443 - ETA: 25s - loss: 2.0033 - accuracy: 0.443 - ETA: 24s - loss: 2.0031 - accuracy: 0.443 - ETA: 23s - loss: 2.0005 - accuracy: 0.444 - ETA: 22s - loss: 1.9968 - accuracy: 0.445 - ETA: 21s - loss: 1.9962 - accuracy: 0.445 - ETA: 20s - loss: 1.9956 - accuracy: 0.445 - ETA: 19s - loss: 1.9936 - accuracy: 0.446 - ETA: 18s - loss: 1.9924 - accuracy: 0.446 - ETA: 17s - loss: 1.9922 - accuracy: 0.446 - ETA: 16s - loss: 1.9914 - accuracy: 0.446 - ETA: 15s - loss: 1.9912 - accuracy: 0.446 - ETA: 14s - loss: 1.9921 - accuracy: 0.445 - ETA: 13s - loss: 1.9906 - accuracy: 0.446 - ETA: 12s - loss: 1.9895 - accuracy: 0.446 - ETA: 10s - loss: 1.9894 - accuracy: 0.447 - ETA: 9s - loss: 1.9884 - accuracy: 0.447 - ETA: 8s - loss: 1.9874 - accuracy: 0.44 - ETA: 7s - loss: 1.9867 - accuracy: 0.44 - ETA: 6s - loss: 1.9853 - accuracy: 0.44 - ETA: 5s - loss: 1.9840 - accuracy: 0.44 - ETA: 4s - loss: 1.9829 - accuracy: 0.44 - ETA: 3s - loss: 1.9828 - accuracy: 0.44 - ETA: 2s - loss: 1.9827 - accuracy: 0.44 - ETA: 1s - loss: 1.9816 - accuracy: 0.44 - ETA: 0s - loss: 1.9800 - accuracy: 0.45 - 119s 9ms/step - loss: 1.9774 - accuracy: 0.4506 - val_loss: 2.6976 - val_accuracy: 0.3650\n",
      "Epoch 6/100\n",
      "13022/13022 [==============================] - ETA: 1:38 - loss: 1.5846 - accuracy: 0.57 - ETA: 1:36 - loss: 1.5930 - accuracy: 0.55 - ETA: 1:38 - loss: 1.7264 - accuracy: 0.51 - ETA: 1:37 - loss: 1.7652 - accuracy: 0.50 - ETA: 1:35 - loss: 1.7494 - accuracy: 0.49 - ETA: 1:35 - loss: 1.7626 - accuracy: 0.49 - ETA: 1:35 - loss: 1.7829 - accuracy: 0.48 - ETA: 1:34 - loss: 1.7882 - accuracy: 0.48 - ETA: 1:33 - loss: 1.7621 - accuracy: 0.49 - ETA: 1:32 - loss: 1.7675 - accuracy: 0.49 - ETA: 1:31 - loss: 1.7831 - accuracy: 0.49 - ETA: 1:30 - loss: 1.7578 - accuracy: 0.50 - ETA: 1:29 - loss: 1.7614 - accuracy: 0.49 - ETA: 1:28 - loss: 1.7425 - accuracy: 0.50 - ETA: 1:27 - loss: 1.7507 - accuracy: 0.50 - ETA: 1:26 - loss: 1.7651 - accuracy: 0.49 - ETA: 1:25 - loss: 1.7622 - accuracy: 0.49 - ETA: 1:24 - loss: 1.7668 - accuracy: 0.49 - ETA: 1:22 - loss: 1.7669 - accuracy: 0.49 - ETA: 1:22 - loss: 1.7743 - accuracy: 0.49 - ETA: 1:21 - loss: 1.7623 - accuracy: 0.49 - ETA: 1:20 - loss: 1.7620 - accuracy: 0.49 - ETA: 1:19 - loss: 1.7633 - accuracy: 0.49 - ETA: 1:18 - loss: 1.7707 - accuracy: 0.49 - ETA: 1:16 - loss: 1.7684 - accuracy: 0.49 - ETA: 1:15 - loss: 1.7639 - accuracy: 0.49 - ETA: 1:14 - loss: 1.7518 - accuracy: 0.49 - ETA: 1:13 - loss: 1.7653 - accuracy: 0.49 - ETA: 1:12 - loss: 1.7678 - accuracy: 0.49 - ETA: 1:11 - loss: 1.7648 - accuracy: 0.49 - ETA: 1:10 - loss: 1.7630 - accuracy: 0.49 - ETA: 1:09 - loss: 1.7710 - accuracy: 0.48 - ETA: 1:08 - loss: 1.7732 - accuracy: 0.48 - ETA: 1:07 - loss: 1.7701 - accuracy: 0.48 - ETA: 1:07 - loss: 1.7669 - accuracy: 0.48 - ETA: 1:06 - loss: 1.7640 - accuracy: 0.49 - ETA: 1:05 - loss: 1.7607 - accuracy: 0.49 - ETA: 1:04 - loss: 1.7566 - accuracy: 0.49 - ETA: 1:03 - loss: 1.7514 - accuracy: 0.49 - ETA: 1:02 - loss: 1.7477 - accuracy: 0.49 - ETA: 1:01 - loss: 1.7454 - accuracy: 0.49 - ETA: 1:00 - loss: 1.7453 - accuracy: 0.49 - ETA: 59s - loss: 1.7461 - accuracy: 0.4929 - ETA: 58s - loss: 1.7494 - accuracy: 0.492 - ETA: 57s - loss: 1.7495 - accuracy: 0.492 - ETA: 56s - loss: 1.7536 - accuracy: 0.491 - ETA: 55s - loss: 1.7534 - accuracy: 0.492 - ETA: 54s - loss: 1.7494 - accuracy: 0.493 - ETA: 53s - loss: 1.7525 - accuracy: 0.492 - ETA: 52s - loss: 1.7508 - accuracy: 0.493 - ETA: 51s - loss: 1.7489 - accuracy: 0.493 - ETA: 50s - loss: 1.7447 - accuracy: 0.495 - ETA: 49s - loss: 1.7457 - accuracy: 0.495 - ETA: 48s - loss: 1.7494 - accuracy: 0.494 - ETA: 47s - loss: 1.7472 - accuracy: 0.495 - ETA: 46s - loss: 1.7473 - accuracy: 0.496 - ETA: 45s - loss: 1.7454 - accuracy: 0.496 - ETA: 44s - loss: 1.7424 - accuracy: 0.498 - ETA: 43s - loss: 1.7415 - accuracy: 0.499 - ETA: 42s - loss: 1.7410 - accuracy: 0.499 - ETA: 41s - loss: 1.7397 - accuracy: 0.500 - ETA: 40s - loss: 1.7422 - accuracy: 0.499 - ETA: 39s - loss: 1.7423 - accuracy: 0.499 - ETA: 38s - loss: 1.7421 - accuracy: 0.499 - ETA: 37s - loss: 1.7392 - accuracy: 0.501 - ETA: 36s - loss: 1.7420 - accuracy: 0.500 - ETA: 35s - loss: 1.7402 - accuracy: 0.500 - ETA: 34s - loss: 1.7377 - accuracy: 0.501 - ETA: 33s - loss: 1.7371 - accuracy: 0.501 - ETA: 32s - loss: 1.7343 - accuracy: 0.501 - ETA: 31s - loss: 1.7360 - accuracy: 0.501 - ETA: 30s - loss: 1.7360 - accuracy: 0.502 - ETA: 29s - loss: 1.7386 - accuracy: 0.501 - ETA: 28s - loss: 1.7386 - accuracy: 0.501 - ETA: 27s - loss: 1.7371 - accuracy: 0.502 - ETA: 26s - loss: 1.7352 - accuracy: 0.503 - ETA: 25s - loss: 1.7357 - accuracy: 0.502 - ETA: 24s - loss: 1.7325 - accuracy: 0.503 - ETA: 23s - loss: 1.7304 - accuracy: 0.504 - ETA: 22s - loss: 1.7321 - accuracy: 0.504 - ETA: 21s - loss: 1.7313 - accuracy: 0.504 - ETA: 20s - loss: 1.7328 - accuracy: 0.503 - ETA: 19s - loss: 1.7312 - accuracy: 0.504 - ETA: 18s - loss: 1.7299 - accuracy: 0.504 - ETA: 17s - loss: 1.7271 - accuracy: 0.504 - ETA: 16s - loss: 1.7266 - accuracy: 0.505 - ETA: 15s - loss: 1.7244 - accuracy: 0.505 - ETA: 14s - loss: 1.7225 - accuracy: 0.506 - ETA: 12s - loss: 1.7212 - accuracy: 0.506 - ETA: 11s - loss: 1.7227 - accuracy: 0.506 - ETA: 10s - loss: 1.7225 - accuracy: 0.506 - ETA: 9s - loss: 1.7232 - accuracy: 0.506 - ETA: 8s - loss: 1.7218 - accuracy: 0.50 - ETA: 7s - loss: 1.7208 - accuracy: 0.50 - ETA: 6s - loss: 1.7204 - accuracy: 0.50 - ETA: 5s - loss: 1.7199 - accuracy: 0.50 - ETA: 4s - loss: 1.7211 - accuracy: 0.50 - ETA: 3s - loss: 1.7196 - accuracy: 0.50 - ETA: 2s - loss: 1.7210 - accuracy: 0.50 - ETA: 1s - loss: 1.7226 - accuracy: 0.50 - ETA: 0s - loss: 1.7210 - accuracy: 0.50 - 118s 9ms/step - loss: 1.7219 - accuracy: 0.5067 - val_loss: 2.5743 - val_accuracy: 0.3935\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:40 - loss: 1.7738 - accuracy: 0.46 - ETA: 1:39 - loss: 1.7565 - accuracy: 0.46 - ETA: 1:38 - loss: 1.6933 - accuracy: 0.47 - ETA: 1:37 - loss: 1.6854 - accuracy: 0.48 - ETA: 1:35 - loss: 1.6507 - accuracy: 0.50 - ETA: 1:34 - loss: 1.6368 - accuracy: 0.51 - ETA: 1:32 - loss: 1.6352 - accuracy: 0.51 - ETA: 1:31 - loss: 1.6527 - accuracy: 0.51 - ETA: 1:30 - loss: 1.6367 - accuracy: 0.52 - ETA: 1:30 - loss: 1.6307 - accuracy: 0.52 - ETA: 1:29 - loss: 1.6453 - accuracy: 0.52 - ETA: 1:28 - loss: 1.6319 - accuracy: 0.52 - ETA: 1:27 - loss: 1.6307 - accuracy: 0.52 - ETA: 1:25 - loss: 1.6097 - accuracy: 0.52 - ETA: 1:24 - loss: 1.6092 - accuracy: 0.52 - ETA: 1:23 - loss: 1.6047 - accuracy: 0.52 - ETA: 1:22 - loss: 1.5949 - accuracy: 0.53 - ETA: 1:21 - loss: 1.5981 - accuracy: 0.53 - ETA: 1:20 - loss: 1.6015 - accuracy: 0.53 - ETA: 1:19 - loss: 1.5935 - accuracy: 0.53 - ETA: 1:18 - loss: 1.5927 - accuracy: 0.53 - ETA: 1:17 - loss: 1.5851 - accuracy: 0.53 - ETA: 1:17 - loss: 1.5888 - accuracy: 0.53 - ETA: 1:16 - loss: 1.5864 - accuracy: 0.53 - ETA: 1:15 - loss: 1.5814 - accuracy: 0.53 - ETA: 1:14 - loss: 1.5864 - accuracy: 0.53 - ETA: 1:13 - loss: 1.5881 - accuracy: 0.53 - ETA: 1:12 - loss: 1.5902 - accuracy: 0.53 - ETA: 1:11 - loss: 1.5824 - accuracy: 0.53 - ETA: 1:10 - loss: 1.5771 - accuracy: 0.53 - ETA: 1:10 - loss: 1.5764 - accuracy: 0.53 - ETA: 1:09 - loss: 1.5733 - accuracy: 0.53 - ETA: 1:08 - loss: 1.5734 - accuracy: 0.53 - ETA: 1:07 - loss: 1.5737 - accuracy: 0.53 - ETA: 1:06 - loss: 1.5808 - accuracy: 0.53 - ETA: 1:05 - loss: 1.5784 - accuracy: 0.53 - ETA: 1:04 - loss: 1.5792 - accuracy: 0.53 - ETA: 1:03 - loss: 1.5774 - accuracy: 0.53 - ETA: 1:02 - loss: 1.5741 - accuracy: 0.53 - ETA: 1:01 - loss: 1.5709 - accuracy: 0.53 - ETA: 1:00 - loss: 1.5718 - accuracy: 0.53 - ETA: 59s - loss: 1.5713 - accuracy: 0.5376 - ETA: 58s - loss: 1.5735 - accuracy: 0.537 - ETA: 57s - loss: 1.5720 - accuracy: 0.538 - ETA: 56s - loss: 1.5763 - accuracy: 0.537 - ETA: 55s - loss: 1.5741 - accuracy: 0.536 - ETA: 54s - loss: 1.5770 - accuracy: 0.535 - ETA: 53s - loss: 1.5749 - accuracy: 0.536 - ETA: 52s - loss: 1.5773 - accuracy: 0.535 - ETA: 51s - loss: 1.5714 - accuracy: 0.537 - ETA: 50s - loss: 1.5736 - accuracy: 0.537 - ETA: 49s - loss: 1.5746 - accuracy: 0.537 - ETA: 48s - loss: 1.5755 - accuracy: 0.537 - ETA: 47s - loss: 1.5774 - accuracy: 0.537 - ETA: 46s - loss: 1.5779 - accuracy: 0.537 - ETA: 45s - loss: 1.5762 - accuracy: 0.538 - ETA: 44s - loss: 1.5804 - accuracy: 0.537 - ETA: 43s - loss: 1.5784 - accuracy: 0.537 - ETA: 43s - loss: 1.5771 - accuracy: 0.537 - ETA: 42s - loss: 1.5800 - accuracy: 0.536 - ETA: 40s - loss: 1.5778 - accuracy: 0.536 - ETA: 39s - loss: 1.5782 - accuracy: 0.535 - ETA: 38s - loss: 1.5785 - accuracy: 0.535 - ETA: 37s - loss: 1.5753 - accuracy: 0.537 - ETA: 36s - loss: 1.5778 - accuracy: 0.536 - ETA: 35s - loss: 1.5775 - accuracy: 0.537 - ETA: 34s - loss: 1.5804 - accuracy: 0.536 - ETA: 33s - loss: 1.5797 - accuracy: 0.536 - ETA: 32s - loss: 1.5780 - accuracy: 0.537 - ETA: 31s - loss: 1.5790 - accuracy: 0.538 - ETA: 30s - loss: 1.5772 - accuracy: 0.538 - ETA: 29s - loss: 1.5768 - accuracy: 0.538 - ETA: 28s - loss: 1.5779 - accuracy: 0.538 - ETA: 27s - loss: 1.5781 - accuracy: 0.538 - ETA: 26s - loss: 1.5820 - accuracy: 0.537 - ETA: 26s - loss: 1.5825 - accuracy: 0.536 - ETA: 25s - loss: 1.5808 - accuracy: 0.537 - ETA: 24s - loss: 1.5774 - accuracy: 0.538 - ETA: 23s - loss: 1.5787 - accuracy: 0.538 - ETA: 22s - loss: 1.5799 - accuracy: 0.538 - ETA: 21s - loss: 1.5824 - accuracy: 0.537 - ETA: 19s - loss: 1.5796 - accuracy: 0.538 - ETA: 18s - loss: 1.5801 - accuracy: 0.538 - ETA: 17s - loss: 1.5803 - accuracy: 0.538 - ETA: 16s - loss: 1.5794 - accuracy: 0.538 - ETA: 15s - loss: 1.5802 - accuracy: 0.538 - ETA: 14s - loss: 1.5815 - accuracy: 0.538 - ETA: 13s - loss: 1.5827 - accuracy: 0.537 - ETA: 12s - loss: 1.5822 - accuracy: 0.537 - ETA: 11s - loss: 1.5825 - accuracy: 0.537 - ETA: 10s - loss: 1.5857 - accuracy: 0.536 - ETA: 9s - loss: 1.5846 - accuracy: 0.537 - ETA: 8s - loss: 1.5839 - accuracy: 0.53 - ETA: 7s - loss: 1.5825 - accuracy: 0.53 - ETA: 6s - loss: 1.5824 - accuracy: 0.53 - ETA: 5s - loss: 1.5809 - accuracy: 0.53 - ETA: 4s - loss: 1.5807 - accuracy: 0.53 - ETA: 3s - loss: 1.5795 - accuracy: 0.53 - ETA: 2s - loss: 1.5827 - accuracy: 0.53 - ETA: 1s - loss: 1.5830 - accuracy: 0.53 - ETA: 0s - loss: 1.5811 - accuracy: 0.53 - 118s 9ms/step - loss: 1.5823 - accuracy: 0.5369 - val_loss: 2.5579 - val_accuracy: 0.3600\n",
      "Epoch 8/100\n",
      "13022/13022 [==============================] - ETA: 1:42 - loss: 1.4818 - accuracy: 0.57 - ETA: 1:36 - loss: 1.5728 - accuracy: 0.53 - ETA: 1:35 - loss: 1.4359 - accuracy: 0.57 - ETA: 1:36 - loss: 1.5206 - accuracy: 0.56 - ETA: 1:34 - loss: 1.5055 - accuracy: 0.56 - ETA: 1:34 - loss: 1.5045 - accuracy: 0.55 - ETA: 1:33 - loss: 1.5246 - accuracy: 0.54 - ETA: 1:32 - loss: 1.5293 - accuracy: 0.54 - ETA: 1:31 - loss: 1.5326 - accuracy: 0.53 - ETA: 1:30 - loss: 1.5251 - accuracy: 0.54 - ETA: 1:29 - loss: 1.5311 - accuracy: 0.54 - ETA: 1:28 - loss: 1.5351 - accuracy: 0.53 - ETA: 1:27 - loss: 1.5262 - accuracy: 0.53 - ETA: 1:26 - loss: 1.5259 - accuracy: 0.53 - ETA: 1:25 - loss: 1.5160 - accuracy: 0.54 - ETA: 1:25 - loss: 1.5000 - accuracy: 0.54 - ETA: 1:24 - loss: 1.5070 - accuracy: 0.54 - ETA: 1:23 - loss: 1.4983 - accuracy: 0.54 - ETA: 1:23 - loss: 1.5023 - accuracy: 0.54 - ETA: 1:22 - loss: 1.4933 - accuracy: 0.55 - ETA: 1:21 - loss: 1.4963 - accuracy: 0.55 - ETA: 1:20 - loss: 1.4986 - accuracy: 0.55 - ETA: 1:19 - loss: 1.5028 - accuracy: 0.55 - ETA: 1:18 - loss: 1.5176 - accuracy: 0.54 - ETA: 1:17 - loss: 1.5149 - accuracy: 0.54 - ETA: 1:16 - loss: 1.5235 - accuracy: 0.54 - ETA: 1:15 - loss: 1.5049 - accuracy: 0.55 - ETA: 1:14 - loss: 1.4972 - accuracy: 0.55 - ETA: 1:13 - loss: 1.4907 - accuracy: 0.55 - ETA: 1:12 - loss: 1.4984 - accuracy: 0.55 - ETA: 1:12 - loss: 1.4987 - accuracy: 0.55 - ETA: 1:11 - loss: 1.4953 - accuracy: 0.55 - ETA: 1:10 - loss: 1.4863 - accuracy: 0.55 - ETA: 1:09 - loss: 1.4788 - accuracy: 0.55 - ETA: 1:08 - loss: 1.4717 - accuracy: 0.55 - ETA: 1:07 - loss: 1.4717 - accuracy: 0.56 - ETA: 1:06 - loss: 1.4698 - accuracy: 0.56 - ETA: 1:05 - loss: 1.4684 - accuracy: 0.56 - ETA: 1:04 - loss: 1.4641 - accuracy: 0.56 - ETA: 1:03 - loss: 1.4651 - accuracy: 0.56 - ETA: 1:01 - loss: 1.4691 - accuracy: 0.56 - ETA: 1:01 - loss: 1.4652 - accuracy: 0.56 - ETA: 1:00 - loss: 1.4678 - accuracy: 0.56 - ETA: 59s - loss: 1.4683 - accuracy: 0.5613 - ETA: 57s - loss: 1.4707 - accuracy: 0.561 - ETA: 56s - loss: 1.4679 - accuracy: 0.562 - ETA: 55s - loss: 1.4713 - accuracy: 0.561 - ETA: 54s - loss: 1.4669 - accuracy: 0.563 - ETA: 54s - loss: 1.4595 - accuracy: 0.564 - ETA: 52s - loss: 1.4571 - accuracy: 0.565 - ETA: 51s - loss: 1.4597 - accuracy: 0.564 - ETA: 50s - loss: 1.4574 - accuracy: 0.564 - ETA: 49s - loss: 1.4554 - accuracy: 0.564 - ETA: 48s - loss: 1.4572 - accuracy: 0.564 - ETA: 47s - loss: 1.4555 - accuracy: 0.564 - ETA: 46s - loss: 1.4543 - accuracy: 0.565 - ETA: 45s - loss: 1.4549 - accuracy: 0.565 - ETA: 44s - loss: 1.4544 - accuracy: 0.565 - ETA: 43s - loss: 1.4551 - accuracy: 0.565 - ETA: 42s - loss: 1.4537 - accuracy: 0.566 - ETA: 41s - loss: 1.4576 - accuracy: 0.566 - ETA: 40s - loss: 1.4600 - accuracy: 0.565 - ETA: 39s - loss: 1.4622 - accuracy: 0.565 - ETA: 38s - loss: 1.4596 - accuracy: 0.565 - ETA: 37s - loss: 1.4601 - accuracy: 0.566 - ETA: 36s - loss: 1.4633 - accuracy: 0.565 - ETA: 35s - loss: 1.4620 - accuracy: 0.566 - ETA: 34s - loss: 1.4586 - accuracy: 0.567 - ETA: 33s - loss: 1.4575 - accuracy: 0.567 - ETA: 32s - loss: 1.4526 - accuracy: 0.568 - ETA: 31s - loss: 1.4523 - accuracy: 0.569 - ETA: 30s - loss: 1.4507 - accuracy: 0.568 - ETA: 29s - loss: 1.4501 - accuracy: 0.569 - ETA: 28s - loss: 1.4498 - accuracy: 0.570 - ETA: 27s - loss: 1.4500 - accuracy: 0.570 - ETA: 26s - loss: 1.4525 - accuracy: 0.569 - ETA: 25s - loss: 1.4535 - accuracy: 0.569 - ETA: 24s - loss: 1.4518 - accuracy: 0.570 - ETA: 23s - loss: 1.4516 - accuracy: 0.570 - ETA: 22s - loss: 1.4539 - accuracy: 0.569 - ETA: 21s - loss: 1.4577 - accuracy: 0.569 - ETA: 20s - loss: 1.4568 - accuracy: 0.569 - ETA: 19s - loss: 1.4566 - accuracy: 0.569 - ETA: 18s - loss: 1.4567 - accuracy: 0.569 - ETA: 17s - loss: 1.4547 - accuracy: 0.569 - ETA: 16s - loss: 1.4545 - accuracy: 0.569 - ETA: 15s - loss: 1.4540 - accuracy: 0.569 - ETA: 14s - loss: 1.4528 - accuracy: 0.569 - ETA: 13s - loss: 1.4515 - accuracy: 0.569 - ETA: 12s - loss: 1.4524 - accuracy: 0.569 - ETA: 11s - loss: 1.4516 - accuracy: 0.569 - ETA: 10s - loss: 1.4531 - accuracy: 0.569 - ETA: 9s - loss: 1.4522 - accuracy: 0.569 - ETA: 8s - loss: 1.4511 - accuracy: 0.56 - ETA: 6s - loss: 1.4496 - accuracy: 0.57 - ETA: 5s - loss: 1.4478 - accuracy: 0.57 - ETA: 4s - loss: 1.4488 - accuracy: 0.57 - ETA: 3s - loss: 1.4495 - accuracy: 0.56 - ETA: 2s - loss: 1.4478 - accuracy: 0.57 - ETA: 1s - loss: 1.4469 - accuracy: 0.57 - ETA: 0s - loss: 1.4441 - accuracy: 0.57 - 119s 9ms/step - loss: 1.4435 - accuracy: 0.5718 - val_loss: 2.5215 - val_accuracy: 0.3854\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:35 - loss: 1.3400 - accuracy: 0.60 - ETA: 1:33 - loss: 1.4025 - accuracy: 0.54 - ETA: 1:32 - loss: 1.3638 - accuracy: 0.56 - ETA: 1:33 - loss: 1.3351 - accuracy: 0.57 - ETA: 1:30 - loss: 1.2946 - accuracy: 0.58 - ETA: 1:29 - loss: 1.2772 - accuracy: 0.59 - ETA: 1:29 - loss: 1.2702 - accuracy: 0.59 - ETA: 1:28 - loss: 1.2946 - accuracy: 0.58 - ETA: 1:27 - loss: 1.2884 - accuracy: 0.59 - ETA: 1:27 - loss: 1.2971 - accuracy: 0.59 - ETA: 1:26 - loss: 1.3021 - accuracy: 0.59 - ETA: 1:25 - loss: 1.2866 - accuracy: 0.60 - ETA: 1:24 - loss: 1.2815 - accuracy: 0.60 - ETA: 1:23 - loss: 1.2872 - accuracy: 0.60 - ETA: 1:22 - loss: 1.2923 - accuracy: 0.60 - ETA: 1:22 - loss: 1.2903 - accuracy: 0.60 - ETA: 1:22 - loss: 1.2967 - accuracy: 0.60 - ETA: 1:21 - loss: 1.2878 - accuracy: 0.60 - ETA: 1:20 - loss: 1.2914 - accuracy: 0.60 - ETA: 1:19 - loss: 1.2957 - accuracy: 0.60 - ETA: 1:19 - loss: 1.2878 - accuracy: 0.60 - ETA: 1:18 - loss: 1.2959 - accuracy: 0.59 - ETA: 1:17 - loss: 1.3020 - accuracy: 0.59 - ETA: 1:16 - loss: 1.2976 - accuracy: 0.59 - ETA: 1:15 - loss: 1.3010 - accuracy: 0.59 - ETA: 1:14 - loss: 1.3009 - accuracy: 0.59 - ETA: 1:13 - loss: 1.3041 - accuracy: 0.59 - ETA: 1:13 - loss: 1.3046 - accuracy: 0.59 - ETA: 1:12 - loss: 1.3009 - accuracy: 0.59 - ETA: 1:11 - loss: 1.3050 - accuracy: 0.60 - ETA: 1:10 - loss: 1.3052 - accuracy: 0.60 - ETA: 1:09 - loss: 1.2988 - accuracy: 0.60 - ETA: 1:08 - loss: 1.3056 - accuracy: 0.60 - ETA: 1:07 - loss: 1.3057 - accuracy: 0.60 - ETA: 1:06 - loss: 1.3126 - accuracy: 0.59 - ETA: 1:05 - loss: 1.3119 - accuracy: 0.59 - ETA: 1:04 - loss: 1.3162 - accuracy: 0.59 - ETA: 1:03 - loss: 1.3136 - accuracy: 0.59 - ETA: 1:02 - loss: 1.3182 - accuracy: 0.59 - ETA: 1:01 - loss: 1.3223 - accuracy: 0.59 - ETA: 1:00 - loss: 1.3177 - accuracy: 0.59 - ETA: 59s - loss: 1.3238 - accuracy: 0.5964 - ETA: 58s - loss: 1.3228 - accuracy: 0.596 - ETA: 57s - loss: 1.3248 - accuracy: 0.596 - ETA: 56s - loss: 1.3197 - accuracy: 0.597 - ETA: 55s - loss: 1.3186 - accuracy: 0.599 - ETA: 54s - loss: 1.3181 - accuracy: 0.598 - ETA: 53s - loss: 1.3181 - accuracy: 0.599 - ETA: 52s - loss: 1.3191 - accuracy: 0.598 - ETA: 51s - loss: 1.3177 - accuracy: 0.598 - ETA: 50s - loss: 1.3110 - accuracy: 0.600 - ETA: 49s - loss: 1.3120 - accuracy: 0.600 - ETA: 48s - loss: 1.3122 - accuracy: 0.600 - ETA: 47s - loss: 1.3137 - accuracy: 0.600 - ETA: 46s - loss: 1.3152 - accuracy: 0.600 - ETA: 45s - loss: 1.3127 - accuracy: 0.601 - ETA: 44s - loss: 1.3141 - accuracy: 0.599 - ETA: 44s - loss: 1.3097 - accuracy: 0.600 - ETA: 43s - loss: 1.3087 - accuracy: 0.600 - ETA: 42s - loss: 1.3079 - accuracy: 0.600 - ETA: 41s - loss: 1.3091 - accuracy: 0.599 - ETA: 40s - loss: 1.3116 - accuracy: 0.599 - ETA: 39s - loss: 1.3168 - accuracy: 0.598 - ETA: 38s - loss: 1.3155 - accuracy: 0.599 - ETA: 37s - loss: 1.3145 - accuracy: 0.598 - ETA: 36s - loss: 1.3137 - accuracy: 0.598 - ETA: 35s - loss: 1.3147 - accuracy: 0.598 - ETA: 34s - loss: 1.3121 - accuracy: 0.600 - ETA: 33s - loss: 1.3132 - accuracy: 0.599 - ETA: 32s - loss: 1.3152 - accuracy: 0.598 - ETA: 31s - loss: 1.3153 - accuracy: 0.598 - ETA: 30s - loss: 1.3158 - accuracy: 0.597 - ETA: 29s - loss: 1.3173 - accuracy: 0.597 - ETA: 28s - loss: 1.3195 - accuracy: 0.596 - ETA: 27s - loss: 1.3159 - accuracy: 0.597 - ETA: 26s - loss: 1.3194 - accuracy: 0.596 - ETA: 25s - loss: 1.3173 - accuracy: 0.597 - ETA: 24s - loss: 1.3211 - accuracy: 0.596 - ETA: 23s - loss: 1.3234 - accuracy: 0.595 - ETA: 22s - loss: 1.3254 - accuracy: 0.595 - ETA: 21s - loss: 1.3239 - accuracy: 0.595 - ETA: 20s - loss: 1.3239 - accuracy: 0.595 - ETA: 19s - loss: 1.3270 - accuracy: 0.594 - ETA: 18s - loss: 1.3251 - accuracy: 0.595 - ETA: 17s - loss: 1.3258 - accuracy: 0.596 - ETA: 16s - loss: 1.3249 - accuracy: 0.596 - ETA: 15s - loss: 1.3253 - accuracy: 0.596 - ETA: 13s - loss: 1.3245 - accuracy: 0.596 - ETA: 12s - loss: 1.3253 - accuracy: 0.596 - ETA: 11s - loss: 1.3247 - accuracy: 0.597 - ETA: 10s - loss: 1.3255 - accuracy: 0.597 - ETA: 9s - loss: 1.3262 - accuracy: 0.597 - ETA: 8s - loss: 1.3251 - accuracy: 0.59 - ETA: 7s - loss: 1.3238 - accuracy: 0.59 - ETA: 6s - loss: 1.3233 - accuracy: 0.59 - ETA: 5s - loss: 1.3233 - accuracy: 0.59 - ETA: 4s - loss: 1.3237 - accuracy: 0.59 - ETA: 3s - loss: 1.3263 - accuracy: 0.59 - ETA: 2s - loss: 1.3254 - accuracy: 0.59 - ETA: 1s - loss: 1.3270 - accuracy: 0.59 - ETA: 0s - loss: 1.3261 - accuracy: 0.59 - 118s 9ms/step - loss: 1.3279 - accuracy: 0.5962 - val_loss: 2.4549 - val_accuracy: 0.3797\n",
      "Epoch 10/100\n",
      "13022/13022 [==============================] - ETA: 1:39 - loss: 1.2046 - accuracy: 0.65 - ETA: 1:37 - loss: 1.2413 - accuracy: 0.64 - ETA: 1:36 - loss: 1.2433 - accuracy: 0.63 - ETA: 1:36 - loss: 1.2965 - accuracy: 0.60 - ETA: 1:34 - loss: 1.2811 - accuracy: 0.61 - ETA: 1:34 - loss: 1.2920 - accuracy: 0.61 - ETA: 1:33 - loss: 1.2786 - accuracy: 0.60 - ETA: 1:32 - loss: 1.2784 - accuracy: 0.60 - ETA: 1:32 - loss: 1.2824 - accuracy: 0.61 - ETA: 1:30 - loss: 1.2725 - accuracy: 0.61 - ETA: 1:29 - loss: 1.2543 - accuracy: 0.61 - ETA: 1:29 - loss: 1.2600 - accuracy: 0.61 - ETA: 1:28 - loss: 1.2469 - accuracy: 0.62 - ETA: 1:27 - loss: 1.2653 - accuracy: 0.61 - ETA: 1:26 - loss: 1.2492 - accuracy: 0.62 - ETA: 1:26 - loss: 1.2469 - accuracy: 0.62 - ETA: 1:25 - loss: 1.2572 - accuracy: 0.61 - ETA: 1:24 - loss: 1.2544 - accuracy: 0.61 - ETA: 1:23 - loss: 1.2540 - accuracy: 0.61 - ETA: 1:22 - loss: 1.2499 - accuracy: 0.61 - ETA: 1:21 - loss: 1.2522 - accuracy: 0.61 - ETA: 1:20 - loss: 1.2542 - accuracy: 0.61 - ETA: 1:19 - loss: 1.2564 - accuracy: 0.61 - ETA: 1:18 - loss: 1.2550 - accuracy: 0.61 - ETA: 1:17 - loss: 1.2571 - accuracy: 0.61 - ETA: 1:16 - loss: 1.2601 - accuracy: 0.61 - ETA: 1:15 - loss: 1.2572 - accuracy: 0.61 - ETA: 1:14 - loss: 1.2582 - accuracy: 0.61 - ETA: 1:13 - loss: 1.2554 - accuracy: 0.61 - ETA: 1:12 - loss: 1.2606 - accuracy: 0.61 - ETA: 1:11 - loss: 1.2536 - accuracy: 0.61 - ETA: 1:10 - loss: 1.2503 - accuracy: 0.61 - ETA: 1:09 - loss: 1.2549 - accuracy: 0.61 - ETA: 1:08 - loss: 1.2503 - accuracy: 0.61 - ETA: 1:07 - loss: 1.2492 - accuracy: 0.61 - ETA: 1:06 - loss: 1.2475 - accuracy: 0.61 - ETA: 1:05 - loss: 1.2496 - accuracy: 0.61 - ETA: 1:04 - loss: 1.2471 - accuracy: 0.61 - ETA: 1:03 - loss: 1.2429 - accuracy: 0.61 - ETA: 1:02 - loss: 1.2419 - accuracy: 0.61 - ETA: 1:01 - loss: 1.2488 - accuracy: 0.61 - ETA: 1:00 - loss: 1.2551 - accuracy: 0.61 - ETA: 59s - loss: 1.2658 - accuracy: 0.6106 - ETA: 58s - loss: 1.2645 - accuracy: 0.610 - ETA: 57s - loss: 1.2655 - accuracy: 0.609 - ETA: 56s - loss: 1.2660 - accuracy: 0.609 - ETA: 55s - loss: 1.2637 - accuracy: 0.609 - ETA: 54s - loss: 1.2649 - accuracy: 0.609 - ETA: 53s - loss: 1.2670 - accuracy: 0.609 - ETA: 52s - loss: 1.2670 - accuracy: 0.610 - ETA: 51s - loss: 1.2654 - accuracy: 0.611 - ETA: 50s - loss: 1.2676 - accuracy: 0.611 - ETA: 49s - loss: 1.2728 - accuracy: 0.610 - ETA: 48s - loss: 1.2724 - accuracy: 0.610 - ETA: 47s - loss: 1.2680 - accuracy: 0.610 - ETA: 46s - loss: 1.2669 - accuracy: 0.610 - ETA: 45s - loss: 1.2643 - accuracy: 0.611 - ETA: 44s - loss: 1.2645 - accuracy: 0.611 - ETA: 43s - loss: 1.2612 - accuracy: 0.613 - ETA: 42s - loss: 1.2616 - accuracy: 0.613 - ETA: 41s - loss: 1.2609 - accuracy: 0.613 - ETA: 40s - loss: 1.2618 - accuracy: 0.613 - ETA: 39s - loss: 1.2587 - accuracy: 0.613 - ETA: 38s - loss: 1.2594 - accuracy: 0.613 - ETA: 37s - loss: 1.2593 - accuracy: 0.613 - ETA: 36s - loss: 1.2618 - accuracy: 0.613 - ETA: 35s - loss: 1.2602 - accuracy: 0.613 - ETA: 34s - loss: 1.2608 - accuracy: 0.613 - ETA: 33s - loss: 1.2605 - accuracy: 0.613 - ETA: 32s - loss: 1.2646 - accuracy: 0.613 - ETA: 31s - loss: 1.2632 - accuracy: 0.613 - ETA: 30s - loss: 1.2638 - accuracy: 0.613 - ETA: 29s - loss: 1.2646 - accuracy: 0.613 - ETA: 28s - loss: 1.2641 - accuracy: 0.613 - ETA: 27s - loss: 1.2609 - accuracy: 0.613 - ETA: 26s - loss: 1.2611 - accuracy: 0.613 - ETA: 25s - loss: 1.2644 - accuracy: 0.612 - ETA: 24s - loss: 1.2634 - accuracy: 0.613 - ETA: 23s - loss: 1.2641 - accuracy: 0.612 - ETA: 22s - loss: 1.2626 - accuracy: 0.612 - ETA: 21s - loss: 1.2625 - accuracy: 0.613 - ETA: 20s - loss: 1.2635 - accuracy: 0.612 - ETA: 19s - loss: 1.2629 - accuracy: 0.613 - ETA: 18s - loss: 1.2613 - accuracy: 0.613 - ETA: 17s - loss: 1.2593 - accuracy: 0.614 - ETA: 16s - loss: 1.2581 - accuracy: 0.614 - ETA: 15s - loss: 1.2569 - accuracy: 0.614 - ETA: 14s - loss: 1.2550 - accuracy: 0.615 - ETA: 13s - loss: 1.2546 - accuracy: 0.615 - ETA: 12s - loss: 1.2523 - accuracy: 0.615 - ETA: 11s - loss: 1.2522 - accuracy: 0.615 - ETA: 10s - loss: 1.2535 - accuracy: 0.615 - ETA: 9s - loss: 1.2535 - accuracy: 0.616 - ETA: 7s - loss: 1.2521 - accuracy: 0.61 - ETA: 6s - loss: 1.2504 - accuracy: 0.61 - ETA: 5s - loss: 1.2524 - accuracy: 0.61 - ETA: 4s - loss: 1.2498 - accuracy: 0.61 - ETA: 3s - loss: 1.2492 - accuracy: 0.61 - ETA: 2s - loss: 1.2504 - accuracy: 0.61 - ETA: 1s - loss: 1.2501 - accuracy: 0.61 - ETA: 0s - loss: 1.2513 - accuracy: 0.61 - 118s 9ms/step - loss: 1.2510 - accuracy: 0.6177 - val_loss: 2.4604 - val_accuracy: 0.3863\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:34 - loss: 1.3626 - accuracy: 0.59 - ETA: 1:35 - loss: 1.2126 - accuracy: 0.60 - ETA: 1:37 - loss: 1.1603 - accuracy: 0.62 - ETA: 1:34 - loss: 1.1214 - accuracy: 0.63 - ETA: 1:33 - loss: 1.0934 - accuracy: 0.64 - ETA: 1:32 - loss: 1.0964 - accuracy: 0.65 - ETA: 1:32 - loss: 1.1199 - accuracy: 0.65 - ETA: 1:31 - loss: 1.1377 - accuracy: 0.65 - ETA: 1:30 - loss: 1.1454 - accuracy: 0.65 - ETA: 1:29 - loss: 1.1285 - accuracy: 0.66 - ETA: 1:28 - loss: 1.1178 - accuracy: 0.66 - ETA: 1:28 - loss: 1.1351 - accuracy: 0.65 - ETA: 1:27 - loss: 1.1373 - accuracy: 0.65 - ETA: 1:26 - loss: 1.1378 - accuracy: 0.65 - ETA: 1:25 - loss: 1.1455 - accuracy: 0.65 - ETA: 1:24 - loss: 1.1383 - accuracy: 0.65 - ETA: 1:23 - loss: 1.1317 - accuracy: 0.65 - ETA: 1:22 - loss: 1.1321 - accuracy: 0.65 - ETA: 1:21 - loss: 1.1329 - accuracy: 0.65 - ETA: 1:20 - loss: 1.1449 - accuracy: 0.64 - ETA: 1:19 - loss: 1.1365 - accuracy: 0.65 - ETA: 1:18 - loss: 1.1342 - accuracy: 0.65 - ETA: 1:17 - loss: 1.1423 - accuracy: 0.64 - ETA: 1:17 - loss: 1.1428 - accuracy: 0.64 - ETA: 1:16 - loss: 1.1513 - accuracy: 0.64 - ETA: 1:15 - loss: 1.1552 - accuracy: 0.64 - ETA: 1:14 - loss: 1.1602 - accuracy: 0.64 - ETA: 1:13 - loss: 1.1583 - accuracy: 0.64 - ETA: 1:12 - loss: 1.1534 - accuracy: 0.64 - ETA: 1:11 - loss: 1.1535 - accuracy: 0.64 - ETA: 1:10 - loss: 1.1514 - accuracy: 0.64 - ETA: 1:09 - loss: 1.1507 - accuracy: 0.64 - ETA: 1:08 - loss: 1.1491 - accuracy: 0.64 - ETA: 1:07 - loss: 1.1471 - accuracy: 0.64 - ETA: 1:07 - loss: 1.1548 - accuracy: 0.64 - ETA: 1:06 - loss: 1.1507 - accuracy: 0.64 - ETA: 1:05 - loss: 1.1564 - accuracy: 0.64 - ETA: 1:04 - loss: 1.1568 - accuracy: 0.64 - ETA: 1:03 - loss: 1.1590 - accuracy: 0.64 - ETA: 1:02 - loss: 1.1629 - accuracy: 0.64 - ETA: 1:01 - loss: 1.1607 - accuracy: 0.64 - ETA: 1:00 - loss: 1.1642 - accuracy: 0.64 - ETA: 59s - loss: 1.1638 - accuracy: 0.6472 - ETA: 58s - loss: 1.1639 - accuracy: 0.647 - ETA: 57s - loss: 1.1646 - accuracy: 0.647 - ETA: 56s - loss: 1.1644 - accuracy: 0.647 - ETA: 55s - loss: 1.1685 - accuracy: 0.646 - ETA: 54s - loss: 1.1662 - accuracy: 0.647 - ETA: 53s - loss: 1.1669 - accuracy: 0.646 - ETA: 52s - loss: 1.1666 - accuracy: 0.645 - ETA: 51s - loss: 1.1656 - accuracy: 0.646 - ETA: 50s - loss: 1.1641 - accuracy: 0.646 - ETA: 49s - loss: 1.1651 - accuracy: 0.646 - ETA: 48s - loss: 1.1639 - accuracy: 0.645 - ETA: 47s - loss: 1.1632 - accuracy: 0.645 - ETA: 46s - loss: 1.1603 - accuracy: 0.648 - ETA: 45s - loss: 1.1646 - accuracy: 0.647 - ETA: 44s - loss: 1.1656 - accuracy: 0.647 - ETA: 43s - loss: 1.1646 - accuracy: 0.647 - ETA: 42s - loss: 1.1677 - accuracy: 0.646 - ETA: 41s - loss: 1.1699 - accuracy: 0.646 - ETA: 40s - loss: 1.1674 - accuracy: 0.647 - ETA: 39s - loss: 1.1705 - accuracy: 0.646 - ETA: 38s - loss: 1.1700 - accuracy: 0.646 - ETA: 37s - loss: 1.1708 - accuracy: 0.645 - ETA: 36s - loss: 1.1702 - accuracy: 0.645 - ETA: 35s - loss: 1.1657 - accuracy: 0.646 - ETA: 34s - loss: 1.1673 - accuracy: 0.646 - ETA: 33s - loss: 1.1667 - accuracy: 0.646 - ETA: 32s - loss: 1.1668 - accuracy: 0.646 - ETA: 31s - loss: 1.1695 - accuracy: 0.645 - ETA: 30s - loss: 1.1684 - accuracy: 0.646 - ETA: 29s - loss: 1.1681 - accuracy: 0.646 - ETA: 28s - loss: 1.1690 - accuracy: 0.646 - ETA: 27s - loss: 1.1707 - accuracy: 0.645 - ETA: 26s - loss: 1.1717 - accuracy: 0.645 - ETA: 25s - loss: 1.1704 - accuracy: 0.645 - ETA: 24s - loss: 1.1685 - accuracy: 0.645 - ETA: 23s - loss: 1.1705 - accuracy: 0.645 - ETA: 22s - loss: 1.1693 - accuracy: 0.645 - ETA: 21s - loss: 1.1697 - accuracy: 0.645 - ETA: 20s - loss: 1.1676 - accuracy: 0.645 - ETA: 19s - loss: 1.1659 - accuracy: 0.645 - ETA: 18s - loss: 1.1664 - accuracy: 0.645 - ETA: 17s - loss: 1.1649 - accuracy: 0.645 - ETA: 16s - loss: 1.1685 - accuracy: 0.644 - ETA: 15s - loss: 1.1686 - accuracy: 0.643 - ETA: 14s - loss: 1.1664 - accuracy: 0.644 - ETA: 13s - loss: 1.1657 - accuracy: 0.644 - ETA: 12s - loss: 1.1666 - accuracy: 0.644 - ETA: 10s - loss: 1.1678 - accuracy: 0.644 - ETA: 9s - loss: 1.1697 - accuracy: 0.644 - ETA: 8s - loss: 1.1725 - accuracy: 0.64 - ETA: 7s - loss: 1.1708 - accuracy: 0.64 - ETA: 6s - loss: 1.1700 - accuracy: 0.64 - ETA: 5s - loss: 1.1686 - accuracy: 0.64 - ETA: 4s - loss: 1.1718 - accuracy: 0.64 - ETA: 3s - loss: 1.1706 - accuracy: 0.64 - ETA: 2s - loss: 1.1699 - accuracy: 0.64 - ETA: 1s - loss: 1.1683 - accuracy: 0.64 - ETA: 0s - loss: 1.1681 - accuracy: 0.64 - 118s 9ms/step - loss: 1.1678 - accuracy: 0.6466 - val_loss: 2.4689 - val_accuracy: 0.3975\n",
      "Epoch 12/100\n",
      "13022/13022 [==============================] - ETA: 1:37 - loss: 1.0567 - accuracy: 0.64 - ETA: 1:35 - loss: 1.1147 - accuracy: 0.63 - ETA: 1:35 - loss: 1.0910 - accuracy: 0.63 - ETA: 1:34 - loss: 1.0573 - accuracy: 0.65 - ETA: 1:36 - loss: 1.0443 - accuracy: 0.66 - ETA: 1:36 - loss: 1.0399 - accuracy: 0.66 - ETA: 1:36 - loss: 1.0606 - accuracy: 0.66 - ETA: 1:35 - loss: 1.0736 - accuracy: 0.66 - ETA: 1:35 - loss: 1.0608 - accuracy: 0.66 - ETA: 1:35 - loss: 1.0572 - accuracy: 0.66 - ETA: 1:34 - loss: 1.0679 - accuracy: 0.66 - ETA: 1:33 - loss: 1.0924 - accuracy: 0.65 - ETA: 1:31 - loss: 1.0844 - accuracy: 0.66 - ETA: 1:30 - loss: 1.0971 - accuracy: 0.66 - ETA: 1:28 - loss: 1.1001 - accuracy: 0.66 - ETA: 1:27 - loss: 1.1140 - accuracy: 0.65 - ETA: 1:26 - loss: 1.1104 - accuracy: 0.65 - ETA: 1:26 - loss: 1.1177 - accuracy: 0.65 - ETA: 1:24 - loss: 1.1187 - accuracy: 0.65 - ETA: 1:23 - loss: 1.1182 - accuracy: 0.65 - ETA: 1:22 - loss: 1.1268 - accuracy: 0.65 - ETA: 1:21 - loss: 1.1236 - accuracy: 0.65 - ETA: 1:20 - loss: 1.1313 - accuracy: 0.65 - ETA: 1:19 - loss: 1.1304 - accuracy: 0.65 - ETA: 1:18 - loss: 1.1323 - accuracy: 0.65 - ETA: 1:16 - loss: 1.1308 - accuracy: 0.65 - ETA: 1:15 - loss: 1.1373 - accuracy: 0.65 - ETA: 1:14 - loss: 1.1380 - accuracy: 0.65 - ETA: 1:13 - loss: 1.1401 - accuracy: 0.65 - ETA: 1:12 - loss: 1.1351 - accuracy: 0.65 - ETA: 1:11 - loss: 1.1373 - accuracy: 0.65 - ETA: 1:10 - loss: 1.1340 - accuracy: 0.65 - ETA: 1:09 - loss: 1.1383 - accuracy: 0.65 - ETA: 1:08 - loss: 1.1373 - accuracy: 0.65 - ETA: 1:07 - loss: 1.1336 - accuracy: 0.65 - ETA: 1:06 - loss: 1.1355 - accuracy: 0.65 - ETA: 1:05 - loss: 1.1326 - accuracy: 0.65 - ETA: 1:04 - loss: 1.1285 - accuracy: 0.65 - ETA: 1:03 - loss: 1.1299 - accuracy: 0.65 - ETA: 1:02 - loss: 1.1289 - accuracy: 0.65 - ETA: 1:01 - loss: 1.1277 - accuracy: 0.65 - ETA: 1:00 - loss: 1.1288 - accuracy: 0.64 - ETA: 59s - loss: 1.1276 - accuracy: 0.6497 - ETA: 59s - loss: 1.1230 - accuracy: 0.649 - ETA: 57s - loss: 1.1213 - accuracy: 0.649 - ETA: 56s - loss: 1.1211 - accuracy: 0.648 - ETA: 55s - loss: 1.1214 - accuracy: 0.648 - ETA: 54s - loss: 1.1182 - accuracy: 0.649 - ETA: 53s - loss: 1.1172 - accuracy: 0.650 - ETA: 52s - loss: 1.1107 - accuracy: 0.652 - ETA: 51s - loss: 1.1051 - accuracy: 0.654 - ETA: 50s - loss: 1.1045 - accuracy: 0.655 - ETA: 49s - loss: 1.1071 - accuracy: 0.653 - ETA: 48s - loss: 1.1057 - accuracy: 0.654 - ETA: 47s - loss: 1.1023 - accuracy: 0.656 - ETA: 46s - loss: 1.1021 - accuracy: 0.656 - ETA: 45s - loss: 1.0999 - accuracy: 0.656 - ETA: 44s - loss: 1.1005 - accuracy: 0.656 - ETA: 43s - loss: 1.0990 - accuracy: 0.656 - ETA: 42s - loss: 1.0992 - accuracy: 0.656 - ETA: 41s - loss: 1.0982 - accuracy: 0.657 - ETA: 40s - loss: 1.0980 - accuracy: 0.657 - ETA: 39s - loss: 1.0973 - accuracy: 0.657 - ETA: 38s - loss: 1.0995 - accuracy: 0.656 - ETA: 37s - loss: 1.0967 - accuracy: 0.657 - ETA: 36s - loss: 1.0964 - accuracy: 0.657 - ETA: 35s - loss: 1.0949 - accuracy: 0.657 - ETA: 34s - loss: 1.0936 - accuracy: 0.657 - ETA: 33s - loss: 1.0980 - accuracy: 0.656 - ETA: 32s - loss: 1.0988 - accuracy: 0.656 - ETA: 31s - loss: 1.0971 - accuracy: 0.656 - ETA: 30s - loss: 1.0981 - accuracy: 0.656 - ETA: 29s - loss: 1.0975 - accuracy: 0.656 - ETA: 28s - loss: 1.0982 - accuracy: 0.656 - ETA: 27s - loss: 1.0982 - accuracy: 0.656 - ETA: 26s - loss: 1.0973 - accuracy: 0.656 - ETA: 25s - loss: 1.0959 - accuracy: 0.657 - ETA: 24s - loss: 1.0956 - accuracy: 0.656 - ETA: 23s - loss: 1.0930 - accuracy: 0.657 - ETA: 22s - loss: 1.0897 - accuracy: 0.658 - ETA: 21s - loss: 1.0882 - accuracy: 0.658 - ETA: 20s - loss: 1.0868 - accuracy: 0.658 - ETA: 19s - loss: 1.0863 - accuracy: 0.659 - ETA: 18s - loss: 1.0886 - accuracy: 0.658 - ETA: 17s - loss: 1.0892 - accuracy: 0.658 - ETA: 16s - loss: 1.0897 - accuracy: 0.658 - ETA: 15s - loss: 1.0882 - accuracy: 0.658 - ETA: 14s - loss: 1.0880 - accuracy: 0.659 - ETA: 13s - loss: 1.0866 - accuracy: 0.659 - ETA: 11s - loss: 1.0880 - accuracy: 0.660 - ETA: 10s - loss: 1.0886 - accuracy: 0.660 - ETA: 9s - loss: 1.0888 - accuracy: 0.660 - ETA: 8s - loss: 1.0878 - accuracy: 0.65 - ETA: 7s - loss: 1.0879 - accuracy: 0.66 - ETA: 6s - loss: 1.0878 - accuracy: 0.65 - ETA: 5s - loss: 1.0870 - accuracy: 0.65 - ETA: 4s - loss: 1.0878 - accuracy: 0.66 - ETA: 3s - loss: 1.0842 - accuracy: 0.66 - ETA: 2s - loss: 1.0827 - accuracy: 0.66 - ETA: 1s - loss: 1.0838 - accuracy: 0.66 - ETA: 0s - loss: 1.0870 - accuracy: 0.66 - 119s 9ms/step - loss: 1.0870 - accuracy: 0.6609 - val_loss: 2.5002 - val_accuracy: 0.3932\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:44 - loss: 1.2486 - accuracy: 0.64 - ETA: 1:43 - loss: 1.2183 - accuracy: 0.65 - ETA: 1:42 - loss: 1.1680 - accuracy: 0.64 - ETA: 1:39 - loss: 1.1518 - accuracy: 0.66 - ETA: 1:39 - loss: 1.1852 - accuracy: 0.65 - ETA: 1:38 - loss: 1.1608 - accuracy: 0.66 - ETA: 1:37 - loss: 1.1235 - accuracy: 0.67 - ETA: 1:36 - loss: 1.1362 - accuracy: 0.66 - ETA: 1:35 - loss: 1.1070 - accuracy: 0.68 - ETA: 1:35 - loss: 1.0923 - accuracy: 0.68 - ETA: 1:34 - loss: 1.0859 - accuracy: 0.68 - ETA: 1:33 - loss: 1.1056 - accuracy: 0.67 - ETA: 1:32 - loss: 1.0993 - accuracy: 0.67 - ETA: 1:31 - loss: 1.0832 - accuracy: 0.67 - ETA: 1:30 - loss: 1.0697 - accuracy: 0.67 - ETA: 1:29 - loss: 1.0616 - accuracy: 0.67 - ETA: 1:27 - loss: 1.0647 - accuracy: 0.67 - ETA: 1:26 - loss: 1.0615 - accuracy: 0.67 - ETA: 1:25 - loss: 1.0591 - accuracy: 0.67 - ETA: 1:24 - loss: 1.0647 - accuracy: 0.67 - ETA: 1:23 - loss: 1.0597 - accuracy: 0.67 - ETA: 1:22 - loss: 1.0556 - accuracy: 0.67 - ETA: 1:21 - loss: 1.0628 - accuracy: 0.67 - ETA: 1:20 - loss: 1.0599 - accuracy: 0.67 - ETA: 1:19 - loss: 1.0530 - accuracy: 0.67 - ETA: 1:17 - loss: 1.0586 - accuracy: 0.67 - ETA: 1:16 - loss: 1.0540 - accuracy: 0.67 - ETA: 1:15 - loss: 1.0517 - accuracy: 0.67 - ETA: 1:14 - loss: 1.0515 - accuracy: 0.67 - ETA: 1:13 - loss: 1.0697 - accuracy: 0.67 - ETA: 1:12 - loss: 1.0758 - accuracy: 0.67 - ETA: 1:11 - loss: 1.0740 - accuracy: 0.67 - ETA: 1:10 - loss: 1.0692 - accuracy: 0.67 - ETA: 1:09 - loss: 1.0723 - accuracy: 0.67 - ETA: 1:08 - loss: 1.0682 - accuracy: 0.67 - ETA: 1:07 - loss: 1.0666 - accuracy: 0.67 - ETA: 1:06 - loss: 1.0637 - accuracy: 0.67 - ETA: 1:04 - loss: 1.0653 - accuracy: 0.67 - ETA: 1:03 - loss: 1.0626 - accuracy: 0.67 - ETA: 1:02 - loss: 1.0593 - accuracy: 0.67 - ETA: 1:01 - loss: 1.0612 - accuracy: 0.67 - ETA: 1:00 - loss: 1.0592 - accuracy: 0.67 - ETA: 59s - loss: 1.0607 - accuracy: 0.6759 - ETA: 58s - loss: 1.0588 - accuracy: 0.676 - ETA: 57s - loss: 1.0574 - accuracy: 0.677 - ETA: 56s - loss: 1.0582 - accuracy: 0.676 - ETA: 55s - loss: 1.0576 - accuracy: 0.677 - ETA: 54s - loss: 1.0545 - accuracy: 0.678 - ETA: 53s - loss: 1.0589 - accuracy: 0.678 - ETA: 52s - loss: 1.0544 - accuracy: 0.679 - ETA: 51s - loss: 1.0551 - accuracy: 0.679 - ETA: 50s - loss: 1.0554 - accuracy: 0.680 - ETA: 49s - loss: 1.0510 - accuracy: 0.680 - ETA: 48s - loss: 1.0474 - accuracy: 0.681 - ETA: 47s - loss: 1.0491 - accuracy: 0.680 - ETA: 46s - loss: 1.0486 - accuracy: 0.680 - ETA: 45s - loss: 1.0497 - accuracy: 0.680 - ETA: 44s - loss: 1.0490 - accuracy: 0.680 - ETA: 43s - loss: 1.0496 - accuracy: 0.679 - ETA: 42s - loss: 1.0505 - accuracy: 0.679 - ETA: 41s - loss: 1.0500 - accuracy: 0.679 - ETA: 40s - loss: 1.0484 - accuracy: 0.680 - ETA: 39s - loss: 1.0459 - accuracy: 0.680 - ETA: 38s - loss: 1.0463 - accuracy: 0.679 - ETA: 37s - loss: 1.0476 - accuracy: 0.679 - ETA: 36s - loss: 1.0497 - accuracy: 0.678 - ETA: 35s - loss: 1.0494 - accuracy: 0.677 - ETA: 34s - loss: 1.0486 - accuracy: 0.677 - ETA: 33s - loss: 1.0484 - accuracy: 0.677 - ETA: 32s - loss: 1.0518 - accuracy: 0.676 - ETA: 31s - loss: 1.0533 - accuracy: 0.676 - ETA: 30s - loss: 1.0566 - accuracy: 0.675 - ETA: 29s - loss: 1.0573 - accuracy: 0.674 - ETA: 28s - loss: 1.0551 - accuracy: 0.674 - ETA: 27s - loss: 1.0526 - accuracy: 0.675 - ETA: 26s - loss: 1.0534 - accuracy: 0.674 - ETA: 25s - loss: 1.0499 - accuracy: 0.675 - ETA: 24s - loss: 1.0494 - accuracy: 0.675 - ETA: 23s - loss: 1.0476 - accuracy: 0.675 - ETA: 22s - loss: 1.0484 - accuracy: 0.675 - ETA: 21s - loss: 1.0470 - accuracy: 0.675 - ETA: 19s - loss: 1.0473 - accuracy: 0.675 - ETA: 19s - loss: 1.0468 - accuracy: 0.675 - ETA: 18s - loss: 1.0480 - accuracy: 0.674 - ETA: 16s - loss: 1.0487 - accuracy: 0.674 - ETA: 15s - loss: 1.0455 - accuracy: 0.675 - ETA: 14s - loss: 1.0469 - accuracy: 0.675 - ETA: 13s - loss: 1.0450 - accuracy: 0.676 - ETA: 12s - loss: 1.0468 - accuracy: 0.675 - ETA: 11s - loss: 1.0477 - accuracy: 0.675 - ETA: 10s - loss: 1.0506 - accuracy: 0.674 - ETA: 9s - loss: 1.0506 - accuracy: 0.675 - ETA: 8s - loss: 1.0496 - accuracy: 0.67 - ETA: 7s - loss: 1.0492 - accuracy: 0.67 - ETA: 6s - loss: 1.0492 - accuracy: 0.67 - ETA: 5s - loss: 1.0499 - accuracy: 0.67 - ETA: 4s - loss: 1.0506 - accuracy: 0.67 - ETA: 3s - loss: 1.0515 - accuracy: 0.67 - ETA: 2s - loss: 1.0506 - accuracy: 0.67 - ETA: 1s - loss: 1.0504 - accuracy: 0.67 - ETA: 0s - loss: 1.0509 - accuracy: 0.67 - 117s 9ms/step - loss: 1.0503 - accuracy: 0.6751 - val_loss: 2.5314 - val_accuracy: 0.3638\n",
      "Epoch 14/100\n",
      "13022/13022 [==============================] - ETA: 1:36 - loss: 0.9862 - accuracy: 0.66 - ETA: 1:33 - loss: 1.0969 - accuracy: 0.66 - ETA: 1:34 - loss: 1.0751 - accuracy: 0.65 - ETA: 1:34 - loss: 1.0733 - accuracy: 0.66 - ETA: 1:34 - loss: 1.0812 - accuracy: 0.66 - ETA: 1:33 - loss: 1.0811 - accuracy: 0.66 - ETA: 1:32 - loss: 1.0699 - accuracy: 0.66 - ETA: 1:31 - loss: 1.0291 - accuracy: 0.68 - ETA: 1:31 - loss: 1.0200 - accuracy: 0.68 - ETA: 1:30 - loss: 1.0272 - accuracy: 0.68 - ETA: 1:28 - loss: 1.0425 - accuracy: 0.67 - ETA: 1:27 - loss: 1.0315 - accuracy: 0.68 - ETA: 1:27 - loss: 1.0298 - accuracy: 0.68 - ETA: 1:25 - loss: 1.0204 - accuracy: 0.68 - ETA: 1:25 - loss: 1.0119 - accuracy: 0.68 - ETA: 1:23 - loss: 1.0144 - accuracy: 0.68 - ETA: 1:22 - loss: 1.0223 - accuracy: 0.68 - ETA: 1:21 - loss: 1.0255 - accuracy: 0.68 - ETA: 1:20 - loss: 1.0187 - accuracy: 0.68 - ETA: 1:19 - loss: 1.0176 - accuracy: 0.68 - ETA: 1:18 - loss: 1.0055 - accuracy: 0.68 - ETA: 1:17 - loss: 1.0072 - accuracy: 0.68 - ETA: 1:17 - loss: 1.0074 - accuracy: 0.68 - ETA: 1:16 - loss: 1.0059 - accuracy: 0.68 - ETA: 1:15 - loss: 1.0142 - accuracy: 0.68 - ETA: 1:14 - loss: 1.0066 - accuracy: 0.68 - ETA: 1:13 - loss: 1.0139 - accuracy: 0.68 - ETA: 1:12 - loss: 1.0165 - accuracy: 0.68 - ETA: 1:11 - loss: 1.0263 - accuracy: 0.68 - ETA: 1:10 - loss: 1.0279 - accuracy: 0.68 - ETA: 1:09 - loss: 1.0256 - accuracy: 0.68 - ETA: 1:08 - loss: 1.0295 - accuracy: 0.68 - ETA: 1:07 - loss: 1.0232 - accuracy: 0.68 - ETA: 1:06 - loss: 1.0157 - accuracy: 0.68 - ETA: 1:05 - loss: 1.0128 - accuracy: 0.68 - ETA: 1:04 - loss: 1.0091 - accuracy: 0.68 - ETA: 1:03 - loss: 1.0039 - accuracy: 0.68 - ETA: 1:02 - loss: 1.0012 - accuracy: 0.68 - ETA: 1:01 - loss: 1.0008 - accuracy: 0.68 - ETA: 1:00 - loss: 1.0014 - accuracy: 0.68 - ETA: 59s - loss: 0.9992 - accuracy: 0.6852 - ETA: 58s - loss: 1.0055 - accuracy: 0.684 - ETA: 57s - loss: 1.0062 - accuracy: 0.684 - ETA: 56s - loss: 1.0113 - accuracy: 0.683 - ETA: 55s - loss: 1.0134 - accuracy: 0.683 - ETA: 54s - loss: 1.0105 - accuracy: 0.683 - ETA: 53s - loss: 1.0111 - accuracy: 0.682 - ETA: 53s - loss: 1.0071 - accuracy: 0.683 - ETA: 52s - loss: 1.0046 - accuracy: 0.684 - ETA: 51s - loss: 1.0052 - accuracy: 0.684 - ETA: 50s - loss: 1.0014 - accuracy: 0.686 - ETA: 49s - loss: 1.0073 - accuracy: 0.685 - ETA: 48s - loss: 1.0107 - accuracy: 0.684 - ETA: 47s - loss: 1.0083 - accuracy: 0.685 - ETA: 46s - loss: 1.0103 - accuracy: 0.685 - ETA: 45s - loss: 1.0093 - accuracy: 0.685 - ETA: 44s - loss: 1.0125 - accuracy: 0.684 - ETA: 43s - loss: 1.0087 - accuracy: 0.685 - ETA: 42s - loss: 1.0063 - accuracy: 0.686 - ETA: 41s - loss: 1.0069 - accuracy: 0.687 - ETA: 40s - loss: 1.0057 - accuracy: 0.687 - ETA: 39s - loss: 1.0048 - accuracy: 0.687 - ETA: 38s - loss: 1.0029 - accuracy: 0.687 - ETA: 37s - loss: 1.0078 - accuracy: 0.685 - ETA: 36s - loss: 1.0089 - accuracy: 0.684 - ETA: 35s - loss: 1.0030 - accuracy: 0.686 - ETA: 34s - loss: 1.0045 - accuracy: 0.686 - ETA: 33s - loss: 1.0048 - accuracy: 0.686 - ETA: 32s - loss: 1.0051 - accuracy: 0.686 - ETA: 31s - loss: 1.0052 - accuracy: 0.686 - ETA: 30s - loss: 1.0030 - accuracy: 0.686 - ETA: 29s - loss: 1.0034 - accuracy: 0.686 - ETA: 28s - loss: 1.0024 - accuracy: 0.687 - ETA: 27s - loss: 1.0012 - accuracy: 0.687 - ETA: 26s - loss: 1.0014 - accuracy: 0.687 - ETA: 25s - loss: 1.0025 - accuracy: 0.686 - ETA: 24s - loss: 1.0031 - accuracy: 0.687 - ETA: 23s - loss: 1.0043 - accuracy: 0.686 - ETA: 22s - loss: 1.0037 - accuracy: 0.687 - ETA: 21s - loss: 1.0042 - accuracy: 0.687 - ETA: 20s - loss: 1.0005 - accuracy: 0.688 - ETA: 19s - loss: 1.0017 - accuracy: 0.688 - ETA: 18s - loss: 1.0027 - accuracy: 0.687 - ETA: 17s - loss: 1.0030 - accuracy: 0.687 - ETA: 16s - loss: 1.0040 - accuracy: 0.687 - ETA: 15s - loss: 1.0039 - accuracy: 0.687 - ETA: 14s - loss: 1.0031 - accuracy: 0.687 - ETA: 13s - loss: 1.0050 - accuracy: 0.686 - ETA: 12s - loss: 1.0048 - accuracy: 0.686 - ETA: 11s - loss: 1.0060 - accuracy: 0.687 - ETA: 10s - loss: 1.0065 - accuracy: 0.686 - ETA: 9s - loss: 1.0067 - accuracy: 0.686 - ETA: 8s - loss: 1.0056 - accuracy: 0.68 - ETA: 7s - loss: 1.0056 - accuracy: 0.68 - ETA: 6s - loss: 1.0051 - accuracy: 0.68 - ETA: 5s - loss: 1.0036 - accuracy: 0.68 - ETA: 4s - loss: 1.0050 - accuracy: 0.68 - ETA: 3s - loss: 1.0046 - accuracy: 0.68 - ETA: 2s - loss: 1.0064 - accuracy: 0.68 - ETA: 1s - loss: 1.0058 - accuracy: 0.68 - ETA: 0s - loss: 1.0049 - accuracy: 0.68 - 118s 9ms/step - loss: 1.0041 - accuracy: 0.6879 - val_loss: 2.5539 - val_accuracy: 0.3561\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:45 - loss: 1.0926 - accuracy: 0.66 - ETA: 1:40 - loss: 1.0795 - accuracy: 0.67 - ETA: 1:36 - loss: 0.9867 - accuracy: 0.67 - ETA: 1:36 - loss: 1.0446 - accuracy: 0.66 - ETA: 1:34 - loss: 1.0610 - accuracy: 0.66 - ETA: 1:32 - loss: 1.0138 - accuracy: 0.67 - ETA: 1:31 - loss: 1.0339 - accuracy: 0.67 - ETA: 1:29 - loss: 1.0145 - accuracy: 0.67 - ETA: 1:28 - loss: 1.0117 - accuracy: 0.68 - ETA: 1:27 - loss: 1.0090 - accuracy: 0.68 - ETA: 1:25 - loss: 0.9949 - accuracy: 0.68 - ETA: 1:25 - loss: 0.9840 - accuracy: 0.68 - ETA: 1:24 - loss: 0.9884 - accuracy: 0.68 - ETA: 1:23 - loss: 1.0030 - accuracy: 0.68 - ETA: 1:22 - loss: 1.0010 - accuracy: 0.68 - ETA: 1:21 - loss: 1.0124 - accuracy: 0.67 - ETA: 1:19 - loss: 1.0025 - accuracy: 0.67 - ETA: 1:18 - loss: 1.0188 - accuracy: 0.67 - ETA: 1:17 - loss: 1.0149 - accuracy: 0.67 - ETA: 1:16 - loss: 1.0080 - accuracy: 0.67 - ETA: 1:15 - loss: 1.0084 - accuracy: 0.67 - ETA: 1:14 - loss: 1.0052 - accuracy: 0.67 - ETA: 1:13 - loss: 0.9968 - accuracy: 0.67 - ETA: 1:12 - loss: 0.9946 - accuracy: 0.67 - ETA: 1:10 - loss: 1.0048 - accuracy: 0.67 - ETA: 1:10 - loss: 1.0018 - accuracy: 0.67 - ETA: 1:09 - loss: 0.9945 - accuracy: 0.67 - ETA: 1:08 - loss: 0.9931 - accuracy: 0.67 - ETA: 1:07 - loss: 0.9857 - accuracy: 0.67 - ETA: 1:07 - loss: 0.9856 - accuracy: 0.67 - ETA: 1:05 - loss: 0.9859 - accuracy: 0.67 - ETA: 1:04 - loss: 0.9907 - accuracy: 0.67 - ETA: 1:03 - loss: 0.9827 - accuracy: 0.67 - ETA: 1:02 - loss: 0.9759 - accuracy: 0.68 - ETA: 1:01 - loss: 0.9767 - accuracy: 0.68 - ETA: 1:00 - loss: 0.9811 - accuracy: 0.68 - ETA: 59s - loss: 0.9830 - accuracy: 0.6799 - ETA: 58s - loss: 0.9810 - accuracy: 0.679 - ETA: 57s - loss: 0.9814 - accuracy: 0.679 - ETA: 56s - loss: 0.9818 - accuracy: 0.679 - ETA: 55s - loss: 0.9797 - accuracy: 0.679 - ETA: 54s - loss: 0.9802 - accuracy: 0.681 - ETA: 53s - loss: 0.9800 - accuracy: 0.680 - ETA: 52s - loss: 0.9765 - accuracy: 0.681 - ETA: 51s - loss: 0.9748 - accuracy: 0.682 - ETA: 50s - loss: 0.9702 - accuracy: 0.684 - ETA: 49s - loss: 0.9704 - accuracy: 0.685 - ETA: 48s - loss: 0.9696 - accuracy: 0.685 - ETA: 47s - loss: 0.9656 - accuracy: 0.687 - ETA: 47s - loss: 0.9671 - accuracy: 0.687 - ETA: 46s - loss: 0.9708 - accuracy: 0.687 - ETA: 45s - loss: 0.9693 - accuracy: 0.687 - ETA: 44s - loss: 0.9676 - accuracy: 0.687 - ETA: 43s - loss: 0.9655 - accuracy: 0.688 - ETA: 42s - loss: 0.9631 - accuracy: 0.688 - ETA: 41s - loss: 0.9634 - accuracy: 0.689 - ETA: 40s - loss: 0.9631 - accuracy: 0.688 - ETA: 39s - loss: 0.9647 - accuracy: 0.688 - ETA: 38s - loss: 0.9644 - accuracy: 0.689 - ETA: 38s - loss: 0.9689 - accuracy: 0.688 - ETA: 36s - loss: 0.9672 - accuracy: 0.688 - ETA: 35s - loss: 0.9664 - accuracy: 0.688 - ETA: 34s - loss: 0.9680 - accuracy: 0.688 - ETA: 33s - loss: 0.9657 - accuracy: 0.688 - ETA: 32s - loss: 0.9659 - accuracy: 0.689 - ETA: 32s - loss: 0.9662 - accuracy: 0.688 - ETA: 31s - loss: 0.9670 - accuracy: 0.689 - ETA: 30s - loss: 0.9661 - accuracy: 0.689 - ETA: 29s - loss: 0.9640 - accuracy: 0.690 - ETA: 28s - loss: 0.9659 - accuracy: 0.689 - ETA: 27s - loss: 0.9665 - accuracy: 0.689 - ETA: 26s - loss: 0.9647 - accuracy: 0.690 - ETA: 25s - loss: 0.9631 - accuracy: 0.690 - ETA: 24s - loss: 0.9640 - accuracy: 0.690 - ETA: 23s - loss: 0.9655 - accuracy: 0.689 - ETA: 22s - loss: 0.9650 - accuracy: 0.690 - ETA: 22s - loss: 0.9626 - accuracy: 0.690 - ETA: 21s - loss: 0.9614 - accuracy: 0.691 - ETA: 20s - loss: 0.9613 - accuracy: 0.691 - ETA: 19s - loss: 0.9601 - accuracy: 0.692 - ETA: 18s - loss: 0.9595 - accuracy: 0.693 - ETA: 17s - loss: 0.9589 - accuracy: 0.693 - ETA: 16s - loss: 0.9608 - accuracy: 0.693 - ETA: 15s - loss: 0.9604 - accuracy: 0.693 - ETA: 14s - loss: 0.9599 - accuracy: 0.693 - ETA: 14s - loss: 0.9595 - accuracy: 0.693 - ETA: 13s - loss: 0.9602 - accuracy: 0.693 - ETA: 12s - loss: 0.9630 - accuracy: 0.692 - ETA: 11s - loss: 0.9612 - accuracy: 0.693 - ETA: 10s - loss: 0.9591 - accuracy: 0.693 - ETA: 9s - loss: 0.9583 - accuracy: 0.693 - ETA: 8s - loss: 0.9577 - accuracy: 0.69 - ETA: 7s - loss: 0.9580 - accuracy: 0.69 - ETA: 6s - loss: 0.9571 - accuracy: 0.69 - ETA: 6s - loss: 0.9561 - accuracy: 0.69 - ETA: 5s - loss: 0.9574 - accuracy: 0.69 - ETA: 4s - loss: 0.9574 - accuracy: 0.69 - ETA: 3s - loss: 0.9570 - accuracy: 0.69 - ETA: 2s - loss: 0.9579 - accuracy: 0.69 - ETA: 1s - loss: 0.9583 - accuracy: 0.69 - ETA: 0s - loss: 0.9586 - accuracy: 0.69 - 103s 8ms/step - loss: 0.9583 - accuracy: 0.6934 - val_loss: 2.5982 - val_accuracy: 0.3505\n",
      "Epoch 16/100\n",
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.7802 - accuracy: 0.71 - ETA: 1:34 - loss: 0.8166 - accuracy: 0.71 - ETA: 1:33 - loss: 0.8164 - accuracy: 0.71 - ETA: 1:34 - loss: 0.8989 - accuracy: 0.69 - ETA: 1:34 - loss: 0.9233 - accuracy: 0.68 - ETA: 1:33 - loss: 0.9670 - accuracy: 0.68 - ETA: 1:33 - loss: 0.9514 - accuracy: 0.68 - ETA: 1:32 - loss: 0.9563 - accuracy: 0.68 - ETA: 1:32 - loss: 0.9540 - accuracy: 0.68 - ETA: 1:31 - loss: 0.9526 - accuracy: 0.68 - ETA: 1:30 - loss: 0.9358 - accuracy: 0.69 - ETA: 1:30 - loss: 0.9258 - accuracy: 0.69 - ETA: 1:29 - loss: 0.9284 - accuracy: 0.70 - ETA: 1:27 - loss: 0.9351 - accuracy: 0.70 - ETA: 1:26 - loss: 0.9348 - accuracy: 0.70 - ETA: 1:25 - loss: 0.9385 - accuracy: 0.69 - ETA: 1:25 - loss: 0.9290 - accuracy: 0.70 - ETA: 1:24 - loss: 0.9299 - accuracy: 0.70 - ETA: 1:23 - loss: 0.9298 - accuracy: 0.70 - ETA: 1:22 - loss: 0.9392 - accuracy: 0.70 - ETA: 1:21 - loss: 0.9345 - accuracy: 0.70 - ETA: 1:20 - loss: 0.9309 - accuracy: 0.70 - ETA: 1:20 - loss: 0.9371 - accuracy: 0.70 - ETA: 1:19 - loss: 0.9388 - accuracy: 0.70 - ETA: 1:18 - loss: 0.9351 - accuracy: 0.70 - ETA: 1:17 - loss: 0.9330 - accuracy: 0.70 - ETA: 1:16 - loss: 0.9363 - accuracy: 0.70 - ETA: 1:15 - loss: 0.9381 - accuracy: 0.70 - ETA: 1:14 - loss: 0.9396 - accuracy: 0.70 - ETA: 1:13 - loss: 0.9383 - accuracy: 0.70 - ETA: 1:12 - loss: 0.9384 - accuracy: 0.70 - ETA: 1:11 - loss: 0.9384 - accuracy: 0.70 - ETA: 1:10 - loss: 0.9376 - accuracy: 0.70 - ETA: 1:09 - loss: 0.9321 - accuracy: 0.70 - ETA: 1:08 - loss: 0.9314 - accuracy: 0.70 - ETA: 1:07 - loss: 0.9284 - accuracy: 0.70 - ETA: 1:06 - loss: 0.9309 - accuracy: 0.70 - ETA: 1:05 - loss: 0.9309 - accuracy: 0.70 - ETA: 1:04 - loss: 0.9314 - accuracy: 0.70 - ETA: 1:03 - loss: 0.9305 - accuracy: 0.70 - ETA: 1:02 - loss: 0.9287 - accuracy: 0.70 - ETA: 1:01 - loss: 0.9259 - accuracy: 0.70 - ETA: 1:00 - loss: 0.9264 - accuracy: 0.70 - ETA: 59s - loss: 0.9238 - accuracy: 0.7097 - ETA: 58s - loss: 0.9198 - accuracy: 0.711 - ETA: 57s - loss: 0.9216 - accuracy: 0.710 - ETA: 56s - loss: 0.9176 - accuracy: 0.712 - ETA: 55s - loss: 0.9168 - accuracy: 0.713 - ETA: 54s - loss: 0.9165 - accuracy: 0.712 - ETA: 53s - loss: 0.9150 - accuracy: 0.713 - ETA: 52s - loss: 0.9179 - accuracy: 0.711 - ETA: 51s - loss: 0.9141 - accuracy: 0.713 - ETA: 50s - loss: 0.9143 - accuracy: 0.712 - ETA: 49s - loss: 0.9111 - accuracy: 0.712 - ETA: 48s - loss: 0.9119 - accuracy: 0.712 - ETA: 47s - loss: 0.9082 - accuracy: 0.713 - ETA: 46s - loss: 0.9077 - accuracy: 0.713 - ETA: 45s - loss: 0.9088 - accuracy: 0.713 - ETA: 44s - loss: 0.9027 - accuracy: 0.715 - ETA: 43s - loss: 0.9018 - accuracy: 0.715 - ETA: 42s - loss: 0.9003 - accuracy: 0.716 - ETA: 41s - loss: 0.8978 - accuracy: 0.716 - ETA: 40s - loss: 0.8959 - accuracy: 0.717 - ETA: 39s - loss: 0.8948 - accuracy: 0.718 - ETA: 38s - loss: 0.8977 - accuracy: 0.717 - ETA: 37s - loss: 0.8958 - accuracy: 0.718 - ETA: 36s - loss: 0.8981 - accuracy: 0.717 - ETA: 35s - loss: 0.8964 - accuracy: 0.717 - ETA: 34s - loss: 0.8945 - accuracy: 0.718 - ETA: 32s - loss: 0.8941 - accuracy: 0.717 - ETA: 31s - loss: 0.8945 - accuracy: 0.716 - ETA: 30s - loss: 0.8957 - accuracy: 0.716 - ETA: 29s - loss: 0.8965 - accuracy: 0.715 - ETA: 28s - loss: 0.8947 - accuracy: 0.716 - ETA: 27s - loss: 0.8968 - accuracy: 0.715 - ETA: 26s - loss: 0.8966 - accuracy: 0.715 - ETA: 25s - loss: 0.8955 - accuracy: 0.715 - ETA: 24s - loss: 0.8956 - accuracy: 0.715 - ETA: 23s - loss: 0.8966 - accuracy: 0.715 - ETA: 22s - loss: 0.8971 - accuracy: 0.715 - ETA: 21s - loss: 0.8979 - accuracy: 0.714 - ETA: 20s - loss: 0.8980 - accuracy: 0.714 - ETA: 19s - loss: 0.8978 - accuracy: 0.714 - ETA: 18s - loss: 0.9003 - accuracy: 0.714 - ETA: 17s - loss: 0.9027 - accuracy: 0.713 - ETA: 16s - loss: 0.9024 - accuracy: 0.714 - ETA: 15s - loss: 0.9007 - accuracy: 0.714 - ETA: 14s - loss: 0.9018 - accuracy: 0.713 - ETA: 13s - loss: 0.9065 - accuracy: 0.712 - ETA: 12s - loss: 0.9065 - accuracy: 0.712 - ETA: 11s - loss: 0.9045 - accuracy: 0.713 - ETA: 10s - loss: 0.9031 - accuracy: 0.713 - ETA: 9s - loss: 0.9017 - accuracy: 0.713 - ETA: 8s - loss: 0.9014 - accuracy: 0.71 - ETA: 6s - loss: 0.9020 - accuracy: 0.71 - ETA: 5s - loss: 0.9015 - accuracy: 0.71 - ETA: 4s - loss: 0.9034 - accuracy: 0.71 - ETA: 3s - loss: 0.9017 - accuracy: 0.71 - ETA: 2s - loss: 0.9031 - accuracy: 0.71 - ETA: 1s - loss: 0.9046 - accuracy: 0.71 - ETA: 0s - loss: 0.9037 - accuracy: 0.71 - 120s 9ms/step - loss: 0.9032 - accuracy: 0.7124 - val_loss: 2.4052 - val_accuracy: 0.3854\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:37 - loss: 0.7762 - accuracy: 0.75 - ETA: 1:36 - loss: 0.7974 - accuracy: 0.75 - ETA: 1:36 - loss: 0.8211 - accuracy: 0.73 - ETA: 1:37 - loss: 0.8108 - accuracy: 0.73 - ETA: 1:38 - loss: 0.8127 - accuracy: 0.73 - ETA: 1:38 - loss: 0.8089 - accuracy: 0.73 - ETA: 1:37 - loss: 0.8113 - accuracy: 0.73 - ETA: 1:36 - loss: 0.8178 - accuracy: 0.73 - ETA: 1:35 - loss: 0.8039 - accuracy: 0.74 - ETA: 1:34 - loss: 0.7912 - accuracy: 0.74 - ETA: 1:34 - loss: 0.8148 - accuracy: 0.74 - ETA: 1:32 - loss: 0.8085 - accuracy: 0.74 - ETA: 1:32 - loss: 0.8204 - accuracy: 0.73 - ETA: 1:31 - loss: 0.8226 - accuracy: 0.73 - ETA: 1:30 - loss: 0.8350 - accuracy: 0.72 - ETA: 1:29 - loss: 0.8331 - accuracy: 0.72 - ETA: 1:27 - loss: 0.8425 - accuracy: 0.72 - ETA: 1:26 - loss: 0.8498 - accuracy: 0.72 - ETA: 1:25 - loss: 0.8415 - accuracy: 0.72 - ETA: 1:24 - loss: 0.8441 - accuracy: 0.72 - ETA: 1:23 - loss: 0.8471 - accuracy: 0.72 - ETA: 1:22 - loss: 0.8508 - accuracy: 0.72 - ETA: 1:21 - loss: 0.8497 - accuracy: 0.72 - ETA: 1:20 - loss: 0.8472 - accuracy: 0.72 - ETA: 1:19 - loss: 0.8485 - accuracy: 0.72 - ETA: 1:18 - loss: 0.8521 - accuracy: 0.72 - ETA: 1:17 - loss: 0.8562 - accuracy: 0.72 - ETA: 1:16 - loss: 0.8544 - accuracy: 0.72 - ETA: 1:15 - loss: 0.8586 - accuracy: 0.72 - ETA: 1:14 - loss: 0.8616 - accuracy: 0.72 - ETA: 1:13 - loss: 0.8589 - accuracy: 0.72 - ETA: 1:12 - loss: 0.8624 - accuracy: 0.72 - ETA: 1:11 - loss: 0.8684 - accuracy: 0.72 - ETA: 1:10 - loss: 0.8671 - accuracy: 0.72 - ETA: 1:09 - loss: 0.8675 - accuracy: 0.72 - ETA: 1:08 - loss: 0.8663 - accuracy: 0.72 - ETA: 1:07 - loss: 0.8749 - accuracy: 0.71 - ETA: 1:06 - loss: 0.8730 - accuracy: 0.71 - ETA: 1:05 - loss: 0.8761 - accuracy: 0.71 - ETA: 1:04 - loss: 0.8779 - accuracy: 0.71 - ETA: 1:03 - loss: 0.8809 - accuracy: 0.71 - ETA: 1:01 - loss: 0.8812 - accuracy: 0.71 - ETA: 1:01 - loss: 0.8781 - accuracy: 0.71 - ETA: 1:00 - loss: 0.8796 - accuracy: 0.71 - ETA: 59s - loss: 0.8774 - accuracy: 0.7189 - ETA: 58s - loss: 0.8812 - accuracy: 0.718 - ETA: 56s - loss: 0.8781 - accuracy: 0.719 - ETA: 55s - loss: 0.8781 - accuracy: 0.719 - ETA: 54s - loss: 0.8758 - accuracy: 0.720 - ETA: 53s - loss: 0.8733 - accuracy: 0.721 - ETA: 52s - loss: 0.8740 - accuracy: 0.720 - ETA: 51s - loss: 0.8766 - accuracy: 0.720 - ETA: 50s - loss: 0.8724 - accuracy: 0.721 - ETA: 49s - loss: 0.8770 - accuracy: 0.720 - ETA: 48s - loss: 0.8792 - accuracy: 0.719 - ETA: 47s - loss: 0.8803 - accuracy: 0.719 - ETA: 45s - loss: 0.8781 - accuracy: 0.719 - ETA: 44s - loss: 0.8767 - accuracy: 0.720 - ETA: 43s - loss: 0.8774 - accuracy: 0.719 - ETA: 42s - loss: 0.8775 - accuracy: 0.719 - ETA: 41s - loss: 0.8743 - accuracy: 0.720 - ETA: 40s - loss: 0.8713 - accuracy: 0.722 - ETA: 39s - loss: 0.8703 - accuracy: 0.722 - ETA: 38s - loss: 0.8707 - accuracy: 0.722 - ETA: 37s - loss: 0.8699 - accuracy: 0.722 - ETA: 36s - loss: 0.8703 - accuracy: 0.722 - ETA: 34s - loss: 0.8690 - accuracy: 0.722 - ETA: 33s - loss: 0.8702 - accuracy: 0.721 - ETA: 32s - loss: 0.8711 - accuracy: 0.722 - ETA: 31s - loss: 0.8726 - accuracy: 0.721 - ETA: 30s - loss: 0.8704 - accuracy: 0.722 - ETA: 29s - loss: 0.8698 - accuracy: 0.722 - ETA: 28s - loss: 0.8669 - accuracy: 0.722 - ETA: 27s - loss: 0.8690 - accuracy: 0.722 - ETA: 26s - loss: 0.8698 - accuracy: 0.722 - ETA: 25s - loss: 0.8718 - accuracy: 0.721 - ETA: 24s - loss: 0.8703 - accuracy: 0.721 - ETA: 23s - loss: 0.8690 - accuracy: 0.721 - ETA: 22s - loss: 0.8681 - accuracy: 0.722 - ETA: 21s - loss: 0.8699 - accuracy: 0.721 - ETA: 20s - loss: 0.8714 - accuracy: 0.721 - ETA: 19s - loss: 0.8720 - accuracy: 0.721 - ETA: 18s - loss: 0.8735 - accuracy: 0.721 - ETA: 17s - loss: 0.8736 - accuracy: 0.721 - ETA: 16s - loss: 0.8735 - accuracy: 0.721 - ETA: 15s - loss: 0.8738 - accuracy: 0.721 - ETA: 14s - loss: 0.8731 - accuracy: 0.722 - ETA: 13s - loss: 0.8755 - accuracy: 0.721 - ETA: 12s - loss: 0.8740 - accuracy: 0.722 - ETA: 11s - loss: 0.8763 - accuracy: 0.721 - ETA: 10s - loss: 0.8765 - accuracy: 0.721 - ETA: 9s - loss: 0.8779 - accuracy: 0.720 - ETA: 8s - loss: 0.8767 - accuracy: 0.72 - ETA: 7s - loss: 0.8759 - accuracy: 0.72 - ETA: 6s - loss: 0.8751 - accuracy: 0.72 - ETA: 5s - loss: 0.8740 - accuracy: 0.72 - ETA: 4s - loss: 0.8753 - accuracy: 0.72 - ETA: 3s - loss: 0.8758 - accuracy: 0.72 - ETA: 2s - loss: 0.8750 - accuracy: 0.72 - ETA: 1s - loss: 0.8737 - accuracy: 0.72 - ETA: 0s - loss: 0.8723 - accuracy: 0.72 - 111s 9ms/step - loss: 0.8711 - accuracy: 0.7216 - val_loss: 2.6624 - val_accuracy: 0.3247\n",
      "Epoch 18/100\n",
      "13022/13022 [==============================] - ETA: 1:27 - loss: 0.7260 - accuracy: 0.77 - ETA: 1:27 - loss: 0.7552 - accuracy: 0.75 - ETA: 1:27 - loss: 0.7707 - accuracy: 0.76 - ETA: 1:26 - loss: 0.7811 - accuracy: 0.75 - ETA: 1:25 - loss: 0.7783 - accuracy: 0.75 - ETA: 1:24 - loss: 0.7817 - accuracy: 0.74 - ETA: 1:23 - loss: 0.7792 - accuracy: 0.75 - ETA: 1:23 - loss: 0.7750 - accuracy: 0.75 - ETA: 1:22 - loss: 0.7838 - accuracy: 0.75 - ETA: 1:22 - loss: 0.7723 - accuracy: 0.75 - ETA: 1:21 - loss: 0.7732 - accuracy: 0.75 - ETA: 1:21 - loss: 0.7865 - accuracy: 0.74 - ETA: 1:20 - loss: 0.7867 - accuracy: 0.74 - ETA: 1:19 - loss: 0.8077 - accuracy: 0.74 - ETA: 1:19 - loss: 0.8139 - accuracy: 0.74 - ETA: 1:17 - loss: 0.8126 - accuracy: 0.74 - ETA: 1:17 - loss: 0.8087 - accuracy: 0.74 - ETA: 1:16 - loss: 0.8099 - accuracy: 0.74 - ETA: 1:15 - loss: 0.8005 - accuracy: 0.74 - ETA: 1:14 - loss: 0.7873 - accuracy: 0.74 - ETA: 1:13 - loss: 0.7858 - accuracy: 0.75 - ETA: 1:12 - loss: 0.7911 - accuracy: 0.75 - ETA: 1:11 - loss: 0.7951 - accuracy: 0.74 - ETA: 1:10 - loss: 0.8033 - accuracy: 0.74 - ETA: 1:09 - loss: 0.8013 - accuracy: 0.74 - ETA: 1:08 - loss: 0.8052 - accuracy: 0.74 - ETA: 1:07 - loss: 0.8127 - accuracy: 0.74 - ETA: 1:07 - loss: 0.8182 - accuracy: 0.74 - ETA: 1:06 - loss: 0.8217 - accuracy: 0.73 - ETA: 1:05 - loss: 0.8255 - accuracy: 0.73 - ETA: 1:04 - loss: 0.8267 - accuracy: 0.73 - ETA: 1:03 - loss: 0.8208 - accuracy: 0.73 - ETA: 1:02 - loss: 0.8145 - accuracy: 0.73 - ETA: 1:01 - loss: 0.8147 - accuracy: 0.73 - ETA: 1:00 - loss: 0.8155 - accuracy: 0.73 - ETA: 59s - loss: 0.8137 - accuracy: 0.7387 - ETA: 58s - loss: 0.8178 - accuracy: 0.737 - ETA: 57s - loss: 0.8230 - accuracy: 0.736 - ETA: 56s - loss: 0.8245 - accuracy: 0.735 - ETA: 56s - loss: 0.8220 - accuracy: 0.736 - ETA: 55s - loss: 0.8208 - accuracy: 0.736 - ETA: 54s - loss: 0.8168 - accuracy: 0.737 - ETA: 53s - loss: 0.8203 - accuracy: 0.736 - ETA: 52s - loss: 0.8241 - accuracy: 0.736 - ETA: 51s - loss: 0.8248 - accuracy: 0.735 - ETA: 50s - loss: 0.8267 - accuracy: 0.736 - ETA: 49s - loss: 0.8239 - accuracy: 0.736 - ETA: 48s - loss: 0.8285 - accuracy: 0.735 - ETA: 47s - loss: 0.8303 - accuracy: 0.736 - ETA: 46s - loss: 0.8339 - accuracy: 0.734 - ETA: 46s - loss: 0.8329 - accuracy: 0.734 - ETA: 45s - loss: 0.8340 - accuracy: 0.734 - ETA: 44s - loss: 0.8361 - accuracy: 0.733 - ETA: 43s - loss: 0.8364 - accuracy: 0.733 - ETA: 42s - loss: 0.8348 - accuracy: 0.734 - ETA: 41s - loss: 0.8335 - accuracy: 0.734 - ETA: 40s - loss: 0.8353 - accuracy: 0.733 - ETA: 39s - loss: 0.8369 - accuracy: 0.732 - ETA: 38s - loss: 0.8389 - accuracy: 0.732 - ETA: 37s - loss: 0.8397 - accuracy: 0.731 - ETA: 36s - loss: 0.8384 - accuracy: 0.732 - ETA: 35s - loss: 0.8382 - accuracy: 0.732 - ETA: 35s - loss: 0.8383 - accuracy: 0.732 - ETA: 34s - loss: 0.8382 - accuracy: 0.732 - ETA: 33s - loss: 0.8378 - accuracy: 0.733 - ETA: 32s - loss: 0.8356 - accuracy: 0.733 - ETA: 31s - loss: 0.8335 - accuracy: 0.734 - ETA: 30s - loss: 0.8328 - accuracy: 0.733 - ETA: 29s - loss: 0.8356 - accuracy: 0.733 - ETA: 28s - loss: 0.8329 - accuracy: 0.733 - ETA: 27s - loss: 0.8315 - accuracy: 0.734 - ETA: 26s - loss: 0.8314 - accuracy: 0.735 - ETA: 25s - loss: 0.8341 - accuracy: 0.734 - ETA: 25s - loss: 0.8392 - accuracy: 0.733 - ETA: 24s - loss: 0.8376 - accuracy: 0.734 - ETA: 23s - loss: 0.8389 - accuracy: 0.734 - ETA: 22s - loss: 0.8371 - accuracy: 0.735 - ETA: 21s - loss: 0.8390 - accuracy: 0.734 - ETA: 20s - loss: 0.8384 - accuracy: 0.734 - ETA: 19s - loss: 0.8383 - accuracy: 0.733 - ETA: 18s - loss: 0.8393 - accuracy: 0.734 - ETA: 17s - loss: 0.8387 - accuracy: 0.734 - ETA: 16s - loss: 0.8390 - accuracy: 0.733 - ETA: 16s - loss: 0.8395 - accuracy: 0.733 - ETA: 15s - loss: 0.8399 - accuracy: 0.733 - ETA: 14s - loss: 0.8408 - accuracy: 0.733 - ETA: 13s - loss: 0.8417 - accuracy: 0.733 - ETA: 12s - loss: 0.8405 - accuracy: 0.733 - ETA: 11s - loss: 0.8424 - accuracy: 0.733 - ETA: 10s - loss: 0.8416 - accuracy: 0.732 - ETA: 9s - loss: 0.8399 - accuracy: 0.733 - ETA: 8s - loss: 0.8389 - accuracy: 0.73 - ETA: 7s - loss: 0.8391 - accuracy: 0.73 - ETA: 7s - loss: 0.8385 - accuracy: 0.73 - ETA: 6s - loss: 0.8387 - accuracy: 0.73 - ETA: 5s - loss: 0.8398 - accuracy: 0.73 - ETA: 4s - loss: 0.8394 - accuracy: 0.73 - ETA: 3s - loss: 0.8381 - accuracy: 0.73 - ETA: 2s - loss: 0.8384 - accuracy: 0.73 - ETA: 1s - loss: 0.8376 - accuracy: 0.73 - ETA: 0s - loss: 0.8364 - accuracy: 0.73 - 105s 8ms/step - loss: 0.8371 - accuracy: 0.7338 - val_loss: 2.5092 - val_accuracy: 0.3787\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:35 - loss: 0.8635 - accuracy: 0.72 - ETA: 1:33 - loss: 0.8155 - accuracy: 0.74 - ETA: 1:30 - loss: 0.8196 - accuracy: 0.74 - ETA: 1:29 - loss: 0.8219 - accuracy: 0.74 - ETA: 1:28 - loss: 0.8248 - accuracy: 0.73 - ETA: 1:28 - loss: 0.8428 - accuracy: 0.73 - ETA: 1:26 - loss: 0.8306 - accuracy: 0.74 - ETA: 1:25 - loss: 0.7876 - accuracy: 0.75 - ETA: 1:24 - loss: 0.8004 - accuracy: 0.74 - ETA: 1:23 - loss: 0.7935 - accuracy: 0.74 - ETA: 1:22 - loss: 0.7821 - accuracy: 0.75 - ETA: 1:21 - loss: 0.7717 - accuracy: 0.75 - ETA: 1:20 - loss: 0.7848 - accuracy: 0.74 - ETA: 1:18 - loss: 0.7957 - accuracy: 0.74 - ETA: 1:18 - loss: 0.8032 - accuracy: 0.74 - ETA: 1:16 - loss: 0.8051 - accuracy: 0.74 - ETA: 1:15 - loss: 0.8044 - accuracy: 0.74 - ETA: 1:15 - loss: 0.8025 - accuracy: 0.74 - ETA: 1:14 - loss: 0.8126 - accuracy: 0.73 - ETA: 1:13 - loss: 0.8083 - accuracy: 0.74 - ETA: 1:12 - loss: 0.8159 - accuracy: 0.74 - ETA: 1:11 - loss: 0.8095 - accuracy: 0.74 - ETA: 1:10 - loss: 0.8127 - accuracy: 0.74 - ETA: 1:09 - loss: 0.8139 - accuracy: 0.74 - ETA: 1:08 - loss: 0.8088 - accuracy: 0.74 - ETA: 1:07 - loss: 0.8183 - accuracy: 0.74 - ETA: 1:06 - loss: 0.8182 - accuracy: 0.74 - ETA: 1:06 - loss: 0.8101 - accuracy: 0.74 - ETA: 1:05 - loss: 0.8090 - accuracy: 0.74 - ETA: 1:04 - loss: 0.8097 - accuracy: 0.74 - ETA: 1:03 - loss: 0.8168 - accuracy: 0.74 - ETA: 1:02 - loss: 0.8158 - accuracy: 0.74 - ETA: 1:01 - loss: 0.8161 - accuracy: 0.74 - ETA: 1:00 - loss: 0.8161 - accuracy: 0.74 - ETA: 59s - loss: 0.8129 - accuracy: 0.7446 - ETA: 59s - loss: 0.8092 - accuracy: 0.744 - ETA: 58s - loss: 0.8143 - accuracy: 0.743 - ETA: 57s - loss: 0.8135 - accuracy: 0.742 - ETA: 56s - loss: 0.8148 - accuracy: 0.741 - ETA: 55s - loss: 0.8149 - accuracy: 0.740 - ETA: 54s - loss: 0.8149 - accuracy: 0.740 - ETA: 53s - loss: 0.8163 - accuracy: 0.740 - ETA: 53s - loss: 0.8198 - accuracy: 0.740 - ETA: 52s - loss: 0.8194 - accuracy: 0.740 - ETA: 51s - loss: 0.8216 - accuracy: 0.740 - ETA: 50s - loss: 0.8265 - accuracy: 0.739 - ETA: 49s - loss: 0.8310 - accuracy: 0.738 - ETA: 48s - loss: 0.8251 - accuracy: 0.739 - ETA: 47s - loss: 0.8239 - accuracy: 0.740 - ETA: 46s - loss: 0.8252 - accuracy: 0.740 - ETA: 45s - loss: 0.8296 - accuracy: 0.739 - ETA: 45s - loss: 0.8315 - accuracy: 0.737 - ETA: 44s - loss: 0.8307 - accuracy: 0.738 - ETA: 43s - loss: 0.8309 - accuracy: 0.738 - ETA: 42s - loss: 0.8309 - accuracy: 0.738 - ETA: 41s - loss: 0.8293 - accuracy: 0.739 - ETA: 40s - loss: 0.8335 - accuracy: 0.738 - ETA: 39s - loss: 0.8315 - accuracy: 0.738 - ETA: 38s - loss: 0.8312 - accuracy: 0.738 - ETA: 37s - loss: 0.8302 - accuracy: 0.739 - ETA: 37s - loss: 0.8310 - accuracy: 0.738 - ETA: 36s - loss: 0.8336 - accuracy: 0.737 - ETA: 35s - loss: 0.8341 - accuracy: 0.736 - ETA: 34s - loss: 0.8332 - accuracy: 0.736 - ETA: 33s - loss: 0.8340 - accuracy: 0.736 - ETA: 32s - loss: 0.8314 - accuracy: 0.737 - ETA: 31s - loss: 0.8301 - accuracy: 0.737 - ETA: 30s - loss: 0.8296 - accuracy: 0.737 - ETA: 29s - loss: 0.8276 - accuracy: 0.738 - ETA: 28s - loss: 0.8256 - accuracy: 0.738 - ETA: 28s - loss: 0.8257 - accuracy: 0.738 - ETA: 27s - loss: 0.8263 - accuracy: 0.738 - ETA: 26s - loss: 0.8262 - accuracy: 0.738 - ETA: 25s - loss: 0.8259 - accuracy: 0.738 - ETA: 24s - loss: 0.8259 - accuracy: 0.738 - ETA: 23s - loss: 0.8248 - accuracy: 0.738 - ETA: 22s - loss: 0.8221 - accuracy: 0.739 - ETA: 21s - loss: 0.8231 - accuracy: 0.739 - ETA: 20s - loss: 0.8245 - accuracy: 0.738 - ETA: 19s - loss: 0.8228 - accuracy: 0.739 - ETA: 18s - loss: 0.8217 - accuracy: 0.739 - ETA: 18s - loss: 0.8218 - accuracy: 0.739 - ETA: 17s - loss: 0.8204 - accuracy: 0.739 - ETA: 16s - loss: 0.8184 - accuracy: 0.740 - ETA: 15s - loss: 0.8172 - accuracy: 0.740 - ETA: 14s - loss: 0.8185 - accuracy: 0.740 - ETA: 13s - loss: 0.8182 - accuracy: 0.740 - ETA: 12s - loss: 0.8182 - accuracy: 0.739 - ETA: 11s - loss: 0.8176 - accuracy: 0.739 - ETA: 10s - loss: 0.8187 - accuracy: 0.739 - ETA: 9s - loss: 0.8183 - accuracy: 0.739 - ETA: 8s - loss: 0.8192 - accuracy: 0.73 - ETA: 7s - loss: 0.8194 - accuracy: 0.73 - ETA: 7s - loss: 0.8185 - accuracy: 0.73 - ETA: 6s - loss: 0.8170 - accuracy: 0.73 - ETA: 5s - loss: 0.8175 - accuracy: 0.73 - ETA: 4s - loss: 0.8186 - accuracy: 0.73 - ETA: 3s - loss: 0.8188 - accuracy: 0.73 - ETA: 2s - loss: 0.8181 - accuracy: 0.73 - ETA: 1s - loss: 0.8192 - accuracy: 0.73 - ETA: 0s - loss: 0.8212 - accuracy: 0.73 - 105s 8ms/step - loss: 0.8219 - accuracy: 0.7385 - val_loss: 2.5594 - val_accuracy: 0.3734\n",
      "Epoch 20/100\n",
      "13022/13022 [==============================] - ETA: 1:31 - loss: 0.7646 - accuracy: 0.73 - ETA: 1:28 - loss: 0.7366 - accuracy: 0.73 - ETA: 1:27 - loss: 0.7559 - accuracy: 0.74 - ETA: 1:27 - loss: 0.7453 - accuracy: 0.74 - ETA: 1:27 - loss: 0.7549 - accuracy: 0.74 - ETA: 1:26 - loss: 0.7494 - accuracy: 0.74 - ETA: 1:26 - loss: 0.7508 - accuracy: 0.74 - ETA: 1:25 - loss: 0.7416 - accuracy: 0.74 - ETA: 1:25 - loss: 0.7431 - accuracy: 0.74 - ETA: 1:24 - loss: 0.7493 - accuracy: 0.74 - ETA: 1:23 - loss: 0.7488 - accuracy: 0.74 - ETA: 1:22 - loss: 0.7603 - accuracy: 0.74 - ETA: 1:21 - loss: 0.7601 - accuracy: 0.74 - ETA: 1:20 - loss: 0.7568 - accuracy: 0.74 - ETA: 1:19 - loss: 0.7723 - accuracy: 0.74 - ETA: 1:18 - loss: 0.7782 - accuracy: 0.73 - ETA: 1:17 - loss: 0.7792 - accuracy: 0.74 - ETA: 1:16 - loss: 0.7763 - accuracy: 0.74 - ETA: 1:15 - loss: 0.7787 - accuracy: 0.73 - ETA: 1:14 - loss: 0.7867 - accuracy: 0.73 - ETA: 1:13 - loss: 0.7906 - accuracy: 0.73 - ETA: 1:12 - loss: 0.7931 - accuracy: 0.73 - ETA: 1:11 - loss: 0.7934 - accuracy: 0.73 - ETA: 1:10 - loss: 0.7908 - accuracy: 0.73 - ETA: 1:10 - loss: 0.7865 - accuracy: 0.73 - ETA: 1:09 - loss: 0.7830 - accuracy: 0.74 - ETA: 1:08 - loss: 0.7841 - accuracy: 0.74 - ETA: 1:07 - loss: 0.7875 - accuracy: 0.74 - ETA: 1:06 - loss: 0.7823 - accuracy: 0.74 - ETA: 1:05 - loss: 0.7812 - accuracy: 0.74 - ETA: 1:04 - loss: 0.7824 - accuracy: 0.74 - ETA: 1:03 - loss: 0.7833 - accuracy: 0.74 - ETA: 1:03 - loss: 0.7866 - accuracy: 0.74 - ETA: 1:02 - loss: 0.7926 - accuracy: 0.73 - ETA: 1:01 - loss: 0.7981 - accuracy: 0.73 - ETA: 1:00 - loss: 0.8063 - accuracy: 0.73 - ETA: 59s - loss: 0.8065 - accuracy: 0.7375 - ETA: 59s - loss: 0.8057 - accuracy: 0.738 - ETA: 58s - loss: 0.8057 - accuracy: 0.738 - ETA: 57s - loss: 0.8116 - accuracy: 0.737 - ETA: 56s - loss: 0.8109 - accuracy: 0.737 - ETA: 55s - loss: 0.8154 - accuracy: 0.736 - ETA: 55s - loss: 0.8193 - accuracy: 0.735 - ETA: 54s - loss: 0.8177 - accuracy: 0.735 - ETA: 53s - loss: 0.8180 - accuracy: 0.735 - ETA: 52s - loss: 0.8174 - accuracy: 0.735 - ETA: 51s - loss: 0.8130 - accuracy: 0.736 - ETA: 50s - loss: 0.8079 - accuracy: 0.738 - ETA: 50s - loss: 0.8104 - accuracy: 0.738 - ETA: 49s - loss: 0.8092 - accuracy: 0.738 - ETA: 48s - loss: 0.8095 - accuracy: 0.739 - ETA: 47s - loss: 0.8116 - accuracy: 0.739 - ETA: 46s - loss: 0.8115 - accuracy: 0.738 - ETA: 45s - loss: 0.8079 - accuracy: 0.740 - ETA: 44s - loss: 0.8099 - accuracy: 0.738 - ETA: 43s - loss: 0.8104 - accuracy: 0.738 - ETA: 42s - loss: 0.8097 - accuracy: 0.738 - ETA: 41s - loss: 0.8106 - accuracy: 0.738 - ETA: 40s - loss: 0.8088 - accuracy: 0.739 - ETA: 40s - loss: 0.8116 - accuracy: 0.737 - ETA: 39s - loss: 0.8092 - accuracy: 0.738 - ETA: 38s - loss: 0.8085 - accuracy: 0.738 - ETA: 37s - loss: 0.8066 - accuracy: 0.738 - ETA: 36s - loss: 0.8113 - accuracy: 0.737 - ETA: 35s - loss: 0.8074 - accuracy: 0.738 - ETA: 34s - loss: 0.8080 - accuracy: 0.738 - ETA: 33s - loss: 0.8066 - accuracy: 0.739 - ETA: 32s - loss: 0.8077 - accuracy: 0.738 - ETA: 31s - loss: 0.8068 - accuracy: 0.739 - ETA: 30s - loss: 0.8057 - accuracy: 0.739 - ETA: 29s - loss: 0.8062 - accuracy: 0.739 - ETA: 28s - loss: 0.8070 - accuracy: 0.739 - ETA: 28s - loss: 0.8088 - accuracy: 0.738 - ETA: 27s - loss: 0.8072 - accuracy: 0.738 - ETA: 26s - loss: 0.8074 - accuracy: 0.738 - ETA: 25s - loss: 0.8065 - accuracy: 0.739 - ETA: 24s - loss: 0.8057 - accuracy: 0.739 - ETA: 23s - loss: 0.8048 - accuracy: 0.740 - ETA: 22s - loss: 0.8054 - accuracy: 0.740 - ETA: 21s - loss: 0.8031 - accuracy: 0.741 - ETA: 20s - loss: 0.8047 - accuracy: 0.741 - ETA: 19s - loss: 0.8068 - accuracy: 0.741 - ETA: 18s - loss: 0.8066 - accuracy: 0.741 - ETA: 17s - loss: 0.8053 - accuracy: 0.741 - ETA: 16s - loss: 0.8054 - accuracy: 0.741 - ETA: 15s - loss: 0.8043 - accuracy: 0.742 - ETA: 14s - loss: 0.8030 - accuracy: 0.742 - ETA: 13s - loss: 0.8056 - accuracy: 0.742 - ETA: 12s - loss: 0.8066 - accuracy: 0.742 - ETA: 11s - loss: 0.8075 - accuracy: 0.742 - ETA: 10s - loss: 0.8100 - accuracy: 0.741 - ETA: 9s - loss: 0.8092 - accuracy: 0.742 - ETA: 8s - loss: 0.8084 - accuracy: 0.74 - ETA: 7s - loss: 0.8071 - accuracy: 0.74 - ETA: 6s - loss: 0.8082 - accuracy: 0.74 - ETA: 5s - loss: 0.8079 - accuracy: 0.74 - ETA: 4s - loss: 0.8106 - accuracy: 0.74 - ETA: 3s - loss: 0.8106 - accuracy: 0.74 - ETA: 2s - loss: 0.8110 - accuracy: 0.74 - ETA: 1s - loss: 0.8094 - accuracy: 0.74 - ETA: 0s - loss: 0.8094 - accuracy: 0.74 - 115s 9ms/step - loss: 0.8085 - accuracy: 0.7423 - val_loss: 2.6090 - val_accuracy: 0.3511\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:40 - loss: 0.6349 - accuracy: 0.80 - ETA: 1:36 - loss: 0.7945 - accuracy: 0.75 - ETA: 1:35 - loss: 0.7945 - accuracy: 0.74 - ETA: 1:35 - loss: 0.7723 - accuracy: 0.75 - ETA: 1:34 - loss: 0.7744 - accuracy: 0.75 - ETA: 1:34 - loss: 0.7961 - accuracy: 0.75 - ETA: 1:32 - loss: 0.8100 - accuracy: 0.75 - ETA: 1:32 - loss: 0.7676 - accuracy: 0.76 - ETA: 1:31 - loss: 0.7876 - accuracy: 0.75 - ETA: 1:31 - loss: 0.7671 - accuracy: 0.76 - ETA: 1:30 - loss: 0.7573 - accuracy: 0.76 - ETA: 1:29 - loss: 0.7610 - accuracy: 0.76 - ETA: 1:28 - loss: 0.7639 - accuracy: 0.76 - ETA: 1:27 - loss: 0.7650 - accuracy: 0.76 - ETA: 1:26 - loss: 0.7672 - accuracy: 0.76 - ETA: 1:25 - loss: 0.7626 - accuracy: 0.76 - ETA: 1:24 - loss: 0.7637 - accuracy: 0.76 - ETA: 1:23 - loss: 0.7702 - accuracy: 0.76 - ETA: 1:22 - loss: 0.7731 - accuracy: 0.75 - ETA: 1:21 - loss: 0.7642 - accuracy: 0.76 - ETA: 1:21 - loss: 0.7665 - accuracy: 0.76 - ETA: 1:20 - loss: 0.7790 - accuracy: 0.76 - ETA: 1:19 - loss: 0.7871 - accuracy: 0.75 - ETA: 1:18 - loss: 0.7848 - accuracy: 0.76 - ETA: 1:17 - loss: 0.7800 - accuracy: 0.76 - ETA: 1:16 - loss: 0.7842 - accuracy: 0.75 - ETA: 1:15 - loss: 0.7866 - accuracy: 0.75 - ETA: 1:14 - loss: 0.7896 - accuracy: 0.75 - ETA: 1:13 - loss: 0.7904 - accuracy: 0.75 - ETA: 1:12 - loss: 0.7918 - accuracy: 0.75 - ETA: 1:11 - loss: 0.7924 - accuracy: 0.75 - ETA: 1:10 - loss: 0.7951 - accuracy: 0.75 - ETA: 1:09 - loss: 0.7956 - accuracy: 0.75 - ETA: 1:08 - loss: 0.7957 - accuracy: 0.75 - ETA: 1:07 - loss: 0.7921 - accuracy: 0.75 - ETA: 1:06 - loss: 0.7884 - accuracy: 0.75 - ETA: 1:05 - loss: 0.7932 - accuracy: 0.75 - ETA: 1:03 - loss: 0.7905 - accuracy: 0.75 - ETA: 1:02 - loss: 0.7890 - accuracy: 0.75 - ETA: 1:01 - loss: 0.7900 - accuracy: 0.75 - ETA: 1:00 - loss: 0.7867 - accuracy: 0.75 - ETA: 58s - loss: 0.7871 - accuracy: 0.7520 - ETA: 57s - loss: 0.7861 - accuracy: 0.751 - ETA: 56s - loss: 0.7858 - accuracy: 0.752 - ETA: 55s - loss: 0.7864 - accuracy: 0.750 - ETA: 54s - loss: 0.7862 - accuracy: 0.750 - ETA: 53s - loss: 0.7896 - accuracy: 0.750 - ETA: 52s - loss: 0.7889 - accuracy: 0.750 - ETA: 51s - loss: 0.7913 - accuracy: 0.750 - ETA: 50s - loss: 0.7896 - accuracy: 0.750 - ETA: 49s - loss: 0.7901 - accuracy: 0.750 - ETA: 48s - loss: 0.7880 - accuracy: 0.750 - ETA: 47s - loss: 0.7864 - accuracy: 0.751 - ETA: 46s - loss: 0.7832 - accuracy: 0.751 - ETA: 45s - loss: 0.7810 - accuracy: 0.752 - ETA: 44s - loss: 0.7822 - accuracy: 0.751 - ETA: 43s - loss: 0.7826 - accuracy: 0.751 - ETA: 42s - loss: 0.7841 - accuracy: 0.750 - ETA: 41s - loss: 0.7853 - accuracy: 0.750 - ETA: 40s - loss: 0.7854 - accuracy: 0.750 - ETA: 39s - loss: 0.7869 - accuracy: 0.750 - ETA: 38s - loss: 0.7857 - accuracy: 0.751 - ETA: 37s - loss: 0.7836 - accuracy: 0.750 - ETA: 36s - loss: 0.7822 - accuracy: 0.751 - ETA: 35s - loss: 0.7826 - accuracy: 0.750 - ETA: 34s - loss: 0.7853 - accuracy: 0.749 - ETA: 33s - loss: 0.7850 - accuracy: 0.750 - ETA: 32s - loss: 0.7872 - accuracy: 0.749 - ETA: 31s - loss: 0.7837 - accuracy: 0.750 - ETA: 31s - loss: 0.7837 - accuracy: 0.750 - ETA: 30s - loss: 0.7848 - accuracy: 0.750 - ETA: 29s - loss: 0.7850 - accuracy: 0.750 - ETA: 28s - loss: 0.7854 - accuracy: 0.750 - ETA: 27s - loss: 0.7837 - accuracy: 0.751 - ETA: 26s - loss: 0.7805 - accuracy: 0.752 - ETA: 25s - loss: 0.7801 - accuracy: 0.752 - ETA: 24s - loss: 0.7802 - accuracy: 0.752 - ETA: 23s - loss: 0.7809 - accuracy: 0.752 - ETA: 22s - loss: 0.7814 - accuracy: 0.751 - ETA: 21s - loss: 0.7809 - accuracy: 0.752 - ETA: 20s - loss: 0.7822 - accuracy: 0.751 - ETA: 19s - loss: 0.7814 - accuracy: 0.752 - ETA: 18s - loss: 0.7841 - accuracy: 0.751 - ETA: 17s - loss: 0.7853 - accuracy: 0.750 - ETA: 16s - loss: 0.7873 - accuracy: 0.750 - ETA: 15s - loss: 0.7866 - accuracy: 0.750 - ETA: 14s - loss: 0.7851 - accuracy: 0.750 - ETA: 13s - loss: 0.7838 - accuracy: 0.751 - ETA: 12s - loss: 0.7832 - accuracy: 0.751 - ETA: 11s - loss: 0.7827 - accuracy: 0.751 - ETA: 10s - loss: 0.7824 - accuracy: 0.751 - ETA: 9s - loss: 0.7846 - accuracy: 0.750 - ETA: 8s - loss: 0.7831 - accuracy: 0.75 - ETA: 7s - loss: 0.7816 - accuracy: 0.75 - ETA: 6s - loss: 0.7800 - accuracy: 0.75 - ETA: 5s - loss: 0.7823 - accuracy: 0.75 - ETA: 4s - loss: 0.7802 - accuracy: 0.75 - ETA: 3s - loss: 0.7799 - accuracy: 0.75 - ETA: 2s - loss: 0.7792 - accuracy: 0.75 - ETA: 1s - loss: 0.7801 - accuracy: 0.75 - ETA: 0s - loss: 0.7796 - accuracy: 0.75 - 116s 9ms/step - loss: 0.7814 - accuracy: 0.7512 - val_loss: 2.4604 - val_accuracy: 0.3921\n",
      "Epoch 22/100\n",
      "13022/13022 [==============================] - ETA: 1:46 - loss: 0.6787 - accuracy: 0.79 - ETA: 1:41 - loss: 0.8268 - accuracy: 0.76 - ETA: 1:37 - loss: 0.8197 - accuracy: 0.75 - ETA: 1:35 - loss: 0.8101 - accuracy: 0.73 - ETA: 1:33 - loss: 0.7881 - accuracy: 0.74 - ETA: 1:32 - loss: 0.7663 - accuracy: 0.75 - ETA: 1:31 - loss: 0.7666 - accuracy: 0.74 - ETA: 1:30 - loss: 0.7634 - accuracy: 0.74 - ETA: 1:29 - loss: 0.7677 - accuracy: 0.73 - ETA: 1:28 - loss: 0.7593 - accuracy: 0.74 - ETA: 1:28 - loss: 0.7524 - accuracy: 0.75 - ETA: 1:27 - loss: 0.7511 - accuracy: 0.75 - ETA: 1:26 - loss: 0.7629 - accuracy: 0.75 - ETA: 1:25 - loss: 0.7814 - accuracy: 0.74 - ETA: 1:24 - loss: 0.7831 - accuracy: 0.74 - ETA: 1:23 - loss: 0.7843 - accuracy: 0.74 - ETA: 1:23 - loss: 0.7767 - accuracy: 0.74 - ETA: 1:22 - loss: 0.7655 - accuracy: 0.74 - ETA: 1:21 - loss: 0.7604 - accuracy: 0.74 - ETA: 1:20 - loss: 0.7781 - accuracy: 0.74 - ETA: 1:19 - loss: 0.7740 - accuracy: 0.74 - ETA: 1:18 - loss: 0.7804 - accuracy: 0.74 - ETA: 1:17 - loss: 0.7739 - accuracy: 0.74 - ETA: 1:16 - loss: 0.7718 - accuracy: 0.74 - ETA: 1:16 - loss: 0.7685 - accuracy: 0.74 - ETA: 1:15 - loss: 0.7744 - accuracy: 0.74 - ETA: 1:14 - loss: 0.7689 - accuracy: 0.74 - ETA: 1:13 - loss: 0.7670 - accuracy: 0.74 - ETA: 1:12 - loss: 0.7644 - accuracy: 0.74 - ETA: 1:11 - loss: 0.7615 - accuracy: 0.74 - ETA: 1:10 - loss: 0.7593 - accuracy: 0.75 - ETA: 1:09 - loss: 0.7612 - accuracy: 0.75 - ETA: 1:08 - loss: 0.7599 - accuracy: 0.75 - ETA: 1:07 - loss: 0.7623 - accuracy: 0.75 - ETA: 1:06 - loss: 0.7626 - accuracy: 0.75 - ETA: 1:05 - loss: 0.7675 - accuracy: 0.75 - ETA: 1:04 - loss: 0.7676 - accuracy: 0.75 - ETA: 1:03 - loss: 0.7664 - accuracy: 0.75 - ETA: 1:02 - loss: 0.7615 - accuracy: 0.75 - ETA: 1:01 - loss: 0.7701 - accuracy: 0.75 - ETA: 1:00 - loss: 0.7694 - accuracy: 0.75 - ETA: 59s - loss: 0.7695 - accuracy: 0.7530 - ETA: 58s - loss: 0.7687 - accuracy: 0.753 - ETA: 57s - loss: 0.7672 - accuracy: 0.754 - ETA: 56s - loss: 0.7652 - accuracy: 0.754 - ETA: 55s - loss: 0.7649 - accuracy: 0.755 - ETA: 54s - loss: 0.7628 - accuracy: 0.755 - ETA: 53s - loss: 0.7612 - accuracy: 0.755 - ETA: 52s - loss: 0.7632 - accuracy: 0.755 - ETA: 51s - loss: 0.7662 - accuracy: 0.754 - ETA: 51s - loss: 0.7636 - accuracy: 0.754 - ETA: 50s - loss: 0.7627 - accuracy: 0.755 - ETA: 48s - loss: 0.7608 - accuracy: 0.754 - ETA: 47s - loss: 0.7635 - accuracy: 0.754 - ETA: 46s - loss: 0.7607 - accuracy: 0.755 - ETA: 46s - loss: 0.7591 - accuracy: 0.755 - ETA: 44s - loss: 0.7604 - accuracy: 0.754 - ETA: 43s - loss: 0.7600 - accuracy: 0.755 - ETA: 42s - loss: 0.7593 - accuracy: 0.755 - ETA: 41s - loss: 0.7679 - accuracy: 0.753 - ETA: 40s - loss: 0.7686 - accuracy: 0.753 - ETA: 39s - loss: 0.7676 - accuracy: 0.753 - ETA: 38s - loss: 0.7655 - accuracy: 0.754 - ETA: 37s - loss: 0.7665 - accuracy: 0.754 - ETA: 36s - loss: 0.7644 - accuracy: 0.755 - ETA: 35s - loss: 0.7659 - accuracy: 0.754 - ETA: 34s - loss: 0.7638 - accuracy: 0.755 - ETA: 33s - loss: 0.7608 - accuracy: 0.756 - ETA: 32s - loss: 0.7595 - accuracy: 0.756 - ETA: 31s - loss: 0.7613 - accuracy: 0.755 - ETA: 30s - loss: 0.7592 - accuracy: 0.755 - ETA: 29s - loss: 0.7584 - accuracy: 0.756 - ETA: 28s - loss: 0.7577 - accuracy: 0.756 - ETA: 27s - loss: 0.7560 - accuracy: 0.757 - ETA: 26s - loss: 0.7564 - accuracy: 0.757 - ETA: 25s - loss: 0.7560 - accuracy: 0.757 - ETA: 24s - loss: 0.7540 - accuracy: 0.757 - ETA: 23s - loss: 0.7519 - accuracy: 0.758 - ETA: 22s - loss: 0.7521 - accuracy: 0.758 - ETA: 21s - loss: 0.7526 - accuracy: 0.759 - ETA: 20s - loss: 0.7539 - accuracy: 0.758 - ETA: 19s - loss: 0.7545 - accuracy: 0.759 - ETA: 18s - loss: 0.7518 - accuracy: 0.759 - ETA: 17s - loss: 0.7516 - accuracy: 0.759 - ETA: 16s - loss: 0.7534 - accuracy: 0.759 - ETA: 15s - loss: 0.7517 - accuracy: 0.759 - ETA: 14s - loss: 0.7533 - accuracy: 0.758 - ETA: 13s - loss: 0.7532 - accuracy: 0.758 - ETA: 12s - loss: 0.7529 - accuracy: 0.758 - ETA: 11s - loss: 0.7549 - accuracy: 0.758 - ETA: 10s - loss: 0.7553 - accuracy: 0.757 - ETA: 9s - loss: 0.7564 - accuracy: 0.757 - ETA: 8s - loss: 0.7561 - accuracy: 0.75 - ETA: 7s - loss: 0.7548 - accuracy: 0.75 - ETA: 6s - loss: 0.7552 - accuracy: 0.75 - ETA: 5s - loss: 0.7554 - accuracy: 0.75 - ETA: 4s - loss: 0.7550 - accuracy: 0.75 - ETA: 3s - loss: 0.7541 - accuracy: 0.75 - ETA: 2s - loss: 0.7552 - accuracy: 0.75 - ETA: 1s - loss: 0.7541 - accuracy: 0.75 - ETA: 0s - loss: 0.7532 - accuracy: 0.75 - 119s 9ms/step - loss: 0.7538 - accuracy: 0.7586 - val_loss: 2.4875 - val_accuracy: 0.3614\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:52 - loss: 0.9348 - accuracy: 0.71 - ETA: 1:45 - loss: 0.7774 - accuracy: 0.76 - ETA: 1:42 - loss: 0.6975 - accuracy: 0.77 - ETA: 1:39 - loss: 0.7136 - accuracy: 0.76 - ETA: 1:37 - loss: 0.7122 - accuracy: 0.77 - ETA: 1:36 - loss: 0.7515 - accuracy: 0.76 - ETA: 1:34 - loss: 0.7598 - accuracy: 0.76 - ETA: 1:33 - loss: 0.7423 - accuracy: 0.76 - ETA: 1:32 - loss: 0.7480 - accuracy: 0.76 - ETA: 1:31 - loss: 0.7419 - accuracy: 0.76 - ETA: 1:30 - loss: 0.7590 - accuracy: 0.76 - ETA: 1:29 - loss: 0.7613 - accuracy: 0.76 - ETA: 1:29 - loss: 0.7611 - accuracy: 0.76 - ETA: 1:28 - loss: 0.7590 - accuracy: 0.76 - ETA: 1:27 - loss: 0.7490 - accuracy: 0.76 - ETA: 1:27 - loss: 0.7511 - accuracy: 0.76 - ETA: 1:26 - loss: 0.7472 - accuracy: 0.76 - ETA: 1:25 - loss: 0.7454 - accuracy: 0.77 - ETA: 1:24 - loss: 0.7464 - accuracy: 0.76 - ETA: 1:23 - loss: 0.7417 - accuracy: 0.76 - ETA: 1:22 - loss: 0.7549 - accuracy: 0.76 - ETA: 1:21 - loss: 0.7494 - accuracy: 0.76 - ETA: 1:20 - loss: 0.7458 - accuracy: 0.76 - ETA: 1:19 - loss: 0.7405 - accuracy: 0.76 - ETA: 1:18 - loss: 0.7338 - accuracy: 0.77 - ETA: 1:17 - loss: 0.7438 - accuracy: 0.76 - ETA: 1:16 - loss: 0.7379 - accuracy: 0.76 - ETA: 1:15 - loss: 0.7395 - accuracy: 0.76 - ETA: 1:14 - loss: 0.7359 - accuracy: 0.76 - ETA: 1:13 - loss: 0.7418 - accuracy: 0.76 - ETA: 1:12 - loss: 0.7466 - accuracy: 0.76 - ETA: 1:11 - loss: 0.7469 - accuracy: 0.76 - ETA: 1:10 - loss: 0.7468 - accuracy: 0.76 - ETA: 1:09 - loss: 0.7430 - accuracy: 0.76 - ETA: 1:08 - loss: 0.7385 - accuracy: 0.76 - ETA: 1:07 - loss: 0.7356 - accuracy: 0.76 - ETA: 1:06 - loss: 0.7351 - accuracy: 0.76 - ETA: 1:05 - loss: 0.7334 - accuracy: 0.76 - ETA: 1:04 - loss: 0.7343 - accuracy: 0.76 - ETA: 1:03 - loss: 0.7334 - accuracy: 0.76 - ETA: 1:02 - loss: 0.7313 - accuracy: 0.76 - ETA: 1:01 - loss: 0.7311 - accuracy: 0.76 - ETA: 1:00 - loss: 0.7291 - accuracy: 0.76 - ETA: 59s - loss: 0.7324 - accuracy: 0.7651 - ETA: 58s - loss: 0.7361 - accuracy: 0.763 - ETA: 57s - loss: 0.7321 - accuracy: 0.765 - ETA: 56s - loss: 0.7302 - accuracy: 0.766 - ETA: 55s - loss: 0.7319 - accuracy: 0.765 - ETA: 54s - loss: 0.7332 - accuracy: 0.765 - ETA: 53s - loss: 0.7283 - accuracy: 0.766 - ETA: 52s - loss: 0.7274 - accuracy: 0.767 - ETA: 51s - loss: 0.7270 - accuracy: 0.767 - ETA: 50s - loss: 0.7280 - accuracy: 0.766 - ETA: 49s - loss: 0.7267 - accuracy: 0.766 - ETA: 48s - loss: 0.7247 - accuracy: 0.767 - ETA: 47s - loss: 0.7236 - accuracy: 0.767 - ETA: 46s - loss: 0.7238 - accuracy: 0.767 - ETA: 45s - loss: 0.7268 - accuracy: 0.766 - ETA: 44s - loss: 0.7269 - accuracy: 0.766 - ETA: 43s - loss: 0.7311 - accuracy: 0.765 - ETA: 41s - loss: 0.7310 - accuracy: 0.765 - ETA: 40s - loss: 0.7285 - accuracy: 0.766 - ETA: 39s - loss: 0.7280 - accuracy: 0.765 - ETA: 38s - loss: 0.7283 - accuracy: 0.766 - ETA: 37s - loss: 0.7265 - accuracy: 0.766 - ETA: 36s - loss: 0.7252 - accuracy: 0.766 - ETA: 35s - loss: 0.7274 - accuracy: 0.766 - ETA: 34s - loss: 0.7263 - accuracy: 0.766 - ETA: 33s - loss: 0.7253 - accuracy: 0.766 - ETA: 32s - loss: 0.7246 - accuracy: 0.766 - ETA: 31s - loss: 0.7240 - accuracy: 0.766 - ETA: 30s - loss: 0.7221 - accuracy: 0.767 - ETA: 29s - loss: 0.7213 - accuracy: 0.767 - ETA: 28s - loss: 0.7230 - accuracy: 0.766 - ETA: 27s - loss: 0.7229 - accuracy: 0.767 - ETA: 26s - loss: 0.7241 - accuracy: 0.766 - ETA: 25s - loss: 0.7243 - accuracy: 0.766 - ETA: 24s - loss: 0.7224 - accuracy: 0.767 - ETA: 23s - loss: 0.7254 - accuracy: 0.766 - ETA: 22s - loss: 0.7261 - accuracy: 0.765 - ETA: 21s - loss: 0.7235 - accuracy: 0.766 - ETA: 20s - loss: 0.7218 - accuracy: 0.767 - ETA: 19s - loss: 0.7228 - accuracy: 0.767 - ETA: 18s - loss: 0.7219 - accuracy: 0.767 - ETA: 17s - loss: 0.7211 - accuracy: 0.768 - ETA: 16s - loss: 0.7214 - accuracy: 0.768 - ETA: 15s - loss: 0.7205 - accuracy: 0.768 - ETA: 14s - loss: 0.7216 - accuracy: 0.768 - ETA: 13s - loss: 0.7231 - accuracy: 0.767 - ETA: 12s - loss: 0.7239 - accuracy: 0.767 - ETA: 11s - loss: 0.7247 - accuracy: 0.767 - ETA: 10s - loss: 0.7239 - accuracy: 0.767 - ETA: 9s - loss: 0.7241 - accuracy: 0.767 - ETA: 7s - loss: 0.7237 - accuracy: 0.76 - ETA: 6s - loss: 0.7230 - accuracy: 0.76 - ETA: 5s - loss: 0.7216 - accuracy: 0.76 - ETA: 4s - loss: 0.7209 - accuracy: 0.76 - ETA: 3s - loss: 0.7218 - accuracy: 0.76 - ETA: 2s - loss: 0.7216 - accuracy: 0.76 - ETA: 1s - loss: 0.7207 - accuracy: 0.76 - ETA: 0s - loss: 0.7203 - accuracy: 0.76 - 120s 9ms/step - loss: 0.7186 - accuracy: 0.7678 - val_loss: 2.6253 - val_accuracy: 0.3492\n",
      "Epoch 24/100\n",
      "13022/13022 [==============================] - ETA: 1:51 - loss: 0.5511 - accuracy: 0.80 - ETA: 1:42 - loss: 0.6278 - accuracy: 0.78 - ETA: 1:39 - loss: 0.6501 - accuracy: 0.78 - ETA: 1:38 - loss: 0.6607 - accuracy: 0.77 - ETA: 1:36 - loss: 0.6513 - accuracy: 0.77 - ETA: 1:36 - loss: 0.6322 - accuracy: 0.78 - ETA: 1:34 - loss: 0.6564 - accuracy: 0.78 - ETA: 1:34 - loss: 0.6839 - accuracy: 0.77 - ETA: 1:33 - loss: 0.6702 - accuracy: 0.77 - ETA: 1:32 - loss: 0.6893 - accuracy: 0.77 - ETA: 1:31 - loss: 0.6864 - accuracy: 0.77 - ETA: 1:30 - loss: 0.6805 - accuracy: 0.77 - ETA: 1:28 - loss: 0.6762 - accuracy: 0.77 - ETA: 1:27 - loss: 0.6720 - accuracy: 0.77 - ETA: 1:26 - loss: 0.6803 - accuracy: 0.77 - ETA: 1:25 - loss: 0.6827 - accuracy: 0.77 - ETA: 1:24 - loss: 0.6805 - accuracy: 0.77 - ETA: 1:23 - loss: 0.6835 - accuracy: 0.77 - ETA: 1:22 - loss: 0.6959 - accuracy: 0.77 - ETA: 1:21 - loss: 0.6860 - accuracy: 0.77 - ETA: 1:20 - loss: 0.6922 - accuracy: 0.77 - ETA: 1:19 - loss: 0.6926 - accuracy: 0.77 - ETA: 1:18 - loss: 0.6847 - accuracy: 0.77 - ETA: 1:17 - loss: 0.6875 - accuracy: 0.77 - ETA: 1:16 - loss: 0.6855 - accuracy: 0.77 - ETA: 1:15 - loss: 0.6904 - accuracy: 0.77 - ETA: 1:14 - loss: 0.6889 - accuracy: 0.77 - ETA: 1:13 - loss: 0.6884 - accuracy: 0.77 - ETA: 1:12 - loss: 0.6966 - accuracy: 0.77 - ETA: 1:11 - loss: 0.6935 - accuracy: 0.77 - ETA: 1:10 - loss: 0.6890 - accuracy: 0.77 - ETA: 1:10 - loss: 0.6872 - accuracy: 0.77 - ETA: 1:09 - loss: 0.6867 - accuracy: 0.77 - ETA: 1:07 - loss: 0.6864 - accuracy: 0.77 - ETA: 1:06 - loss: 0.6908 - accuracy: 0.77 - ETA: 1:05 - loss: 0.6881 - accuracy: 0.77 - ETA: 1:05 - loss: 0.6813 - accuracy: 0.78 - ETA: 1:03 - loss: 0.6795 - accuracy: 0.77 - ETA: 1:02 - loss: 0.6798 - accuracy: 0.77 - ETA: 1:01 - loss: 0.6818 - accuracy: 0.77 - ETA: 1:00 - loss: 0.6882 - accuracy: 0.77 - ETA: 59s - loss: 0.6858 - accuracy: 0.7792 - ETA: 59s - loss: 0.6831 - accuracy: 0.780 - ETA: 58s - loss: 0.6816 - accuracy: 0.780 - ETA: 56s - loss: 0.6808 - accuracy: 0.780 - ETA: 56s - loss: 0.6823 - accuracy: 0.780 - ETA: 55s - loss: 0.6817 - accuracy: 0.779 - ETA: 53s - loss: 0.6817 - accuracy: 0.779 - ETA: 53s - loss: 0.6804 - accuracy: 0.779 - ETA: 52s - loss: 0.6794 - accuracy: 0.779 - ETA: 51s - loss: 0.6802 - accuracy: 0.779 - ETA: 50s - loss: 0.6814 - accuracy: 0.778 - ETA: 49s - loss: 0.6810 - accuracy: 0.778 - ETA: 48s - loss: 0.6809 - accuracy: 0.778 - ETA: 47s - loss: 0.6819 - accuracy: 0.778 - ETA: 46s - loss: 0.6858 - accuracy: 0.778 - ETA: 45s - loss: 0.6855 - accuracy: 0.778 - ETA: 44s - loss: 0.6860 - accuracy: 0.778 - ETA: 43s - loss: 0.6859 - accuracy: 0.778 - ETA: 42s - loss: 0.6861 - accuracy: 0.778 - ETA: 41s - loss: 0.6867 - accuracy: 0.778 - ETA: 40s - loss: 0.6852 - accuracy: 0.779 - ETA: 39s - loss: 0.6860 - accuracy: 0.779 - ETA: 38s - loss: 0.6849 - accuracy: 0.780 - ETA: 37s - loss: 0.6857 - accuracy: 0.779 - ETA: 36s - loss: 0.6870 - accuracy: 0.779 - ETA: 35s - loss: 0.6870 - accuracy: 0.779 - ETA: 34s - loss: 0.6896 - accuracy: 0.778 - ETA: 33s - loss: 0.6922 - accuracy: 0.778 - ETA: 32s - loss: 0.6926 - accuracy: 0.777 - ETA: 31s - loss: 0.6925 - accuracy: 0.777 - ETA: 30s - loss: 0.6937 - accuracy: 0.777 - ETA: 29s - loss: 0.6941 - accuracy: 0.777 - ETA: 28s - loss: 0.6925 - accuracy: 0.777 - ETA: 27s - loss: 0.6922 - accuracy: 0.777 - ETA: 26s - loss: 0.6917 - accuracy: 0.778 - ETA: 25s - loss: 0.6904 - accuracy: 0.778 - ETA: 24s - loss: 0.6906 - accuracy: 0.777 - ETA: 23s - loss: 0.6902 - accuracy: 0.777 - ETA: 22s - loss: 0.6903 - accuracy: 0.777 - ETA: 21s - loss: 0.6929 - accuracy: 0.777 - ETA: 20s - loss: 0.6913 - accuracy: 0.777 - ETA: 19s - loss: 0.6898 - accuracy: 0.778 - ETA: 18s - loss: 0.6897 - accuracy: 0.778 - ETA: 17s - loss: 0.6938 - accuracy: 0.777 - ETA: 16s - loss: 0.6943 - accuracy: 0.777 - ETA: 15s - loss: 0.6948 - accuracy: 0.777 - ETA: 14s - loss: 0.6969 - accuracy: 0.776 - ETA: 13s - loss: 0.6963 - accuracy: 0.777 - ETA: 11s - loss: 0.6955 - accuracy: 0.777 - ETA: 10s - loss: 0.6949 - accuracy: 0.777 - ETA: 9s - loss: 0.6954 - accuracy: 0.777 - ETA: 8s - loss: 0.6976 - accuracy: 0.77 - ETA: 7s - loss: 0.6967 - accuracy: 0.77 - ETA: 6s - loss: 0.6970 - accuracy: 0.77 - ETA: 5s - loss: 0.6971 - accuracy: 0.77 - ETA: 4s - loss: 0.6976 - accuracy: 0.77 - ETA: 3s - loss: 0.6978 - accuracy: 0.77 - ETA: 2s - loss: 0.6971 - accuracy: 0.77 - ETA: 1s - loss: 0.6962 - accuracy: 0.77 - ETA: 0s - loss: 0.6962 - accuracy: 0.77 - 119s 9ms/step - loss: 0.6987 - accuracy: 0.7758 - val_loss: 2.4451 - val_accuracy: 0.3934\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:52 - loss: 0.7248 - accuracy: 0.78 - ETA: 1:49 - loss: 0.6987 - accuracy: 0.80 - ETA: 1:44 - loss: 0.6875 - accuracy: 0.79 - ETA: 1:41 - loss: 0.6713 - accuracy: 0.79 - ETA: 1:38 - loss: 0.6658 - accuracy: 0.79 - ETA: 1:36 - loss: 0.6926 - accuracy: 0.78 - ETA: 1:34 - loss: 0.6769 - accuracy: 0.79 - ETA: 1:33 - loss: 0.6965 - accuracy: 0.78 - ETA: 1:31 - loss: 0.7077 - accuracy: 0.78 - ETA: 1:30 - loss: 0.6958 - accuracy: 0.78 - ETA: 1:29 - loss: 0.6938 - accuracy: 0.78 - ETA: 1:28 - loss: 0.6834 - accuracy: 0.78 - ETA: 1:27 - loss: 0.6839 - accuracy: 0.78 - ETA: 1:26 - loss: 0.6831 - accuracy: 0.78 - ETA: 1:25 - loss: 0.6697 - accuracy: 0.78 - ETA: 1:24 - loss: 0.6802 - accuracy: 0.77 - ETA: 1:24 - loss: 0.6881 - accuracy: 0.77 - ETA: 1:23 - loss: 0.6830 - accuracy: 0.77 - ETA: 1:23 - loss: 0.6902 - accuracy: 0.77 - ETA: 1:22 - loss: 0.6826 - accuracy: 0.78 - ETA: 1:21 - loss: 0.6861 - accuracy: 0.78 - ETA: 1:19 - loss: 0.6863 - accuracy: 0.78 - ETA: 1:18 - loss: 0.6948 - accuracy: 0.77 - ETA: 1:18 - loss: 0.6997 - accuracy: 0.77 - ETA: 1:17 - loss: 0.6974 - accuracy: 0.77 - ETA: 1:16 - loss: 0.6977 - accuracy: 0.77 - ETA: 1:15 - loss: 0.7012 - accuracy: 0.77 - ETA: 1:14 - loss: 0.6964 - accuracy: 0.77 - ETA: 1:13 - loss: 0.6932 - accuracy: 0.78 - ETA: 1:13 - loss: 0.6888 - accuracy: 0.78 - ETA: 1:11 - loss: 0.6877 - accuracy: 0.78 - ETA: 1:10 - loss: 0.6914 - accuracy: 0.77 - ETA: 1:09 - loss: 0.6937 - accuracy: 0.77 - ETA: 1:09 - loss: 0.6926 - accuracy: 0.77 - ETA: 1:08 - loss: 0.6953 - accuracy: 0.77 - ETA: 1:06 - loss: 0.6911 - accuracy: 0.77 - ETA: 1:05 - loss: 0.6912 - accuracy: 0.77 - ETA: 1:04 - loss: 0.6886 - accuracy: 0.77 - ETA: 1:03 - loss: 0.6886 - accuracy: 0.77 - ETA: 1:03 - loss: 0.6863 - accuracy: 0.77 - ETA: 1:01 - loss: 0.6866 - accuracy: 0.77 - ETA: 1:01 - loss: 0.6911 - accuracy: 0.77 - ETA: 1:00 - loss: 0.6885 - accuracy: 0.77 - ETA: 59s - loss: 0.6904 - accuracy: 0.7786 - ETA: 58s - loss: 0.6891 - accuracy: 0.778 - ETA: 57s - loss: 0.6887 - accuracy: 0.778 - ETA: 56s - loss: 0.6876 - accuracy: 0.779 - ETA: 55s - loss: 0.6883 - accuracy: 0.779 - ETA: 54s - loss: 0.6865 - accuracy: 0.779 - ETA: 53s - loss: 0.6835 - accuracy: 0.781 - ETA: 52s - loss: 0.6823 - accuracy: 0.780 - ETA: 50s - loss: 0.6833 - accuracy: 0.780 - ETA: 49s - loss: 0.6823 - accuracy: 0.780 - ETA: 48s - loss: 0.6850 - accuracy: 0.779 - ETA: 47s - loss: 0.6869 - accuracy: 0.780 - ETA: 46s - loss: 0.6920 - accuracy: 0.779 - ETA: 45s - loss: 0.6919 - accuracy: 0.779 - ETA: 44s - loss: 0.6927 - accuracy: 0.779 - ETA: 43s - loss: 0.6921 - accuracy: 0.779 - ETA: 42s - loss: 0.6886 - accuracy: 0.780 - ETA: 41s - loss: 0.6856 - accuracy: 0.780 - ETA: 40s - loss: 0.6864 - accuracy: 0.781 - ETA: 39s - loss: 0.6892 - accuracy: 0.780 - ETA: 38s - loss: 0.6857 - accuracy: 0.782 - ETA: 37s - loss: 0.6877 - accuracy: 0.781 - ETA: 36s - loss: 0.6914 - accuracy: 0.780 - ETA: 35s - loss: 0.6919 - accuracy: 0.780 - ETA: 34s - loss: 0.6950 - accuracy: 0.780 - ETA: 33s - loss: 0.6944 - accuracy: 0.780 - ETA: 32s - loss: 0.6962 - accuracy: 0.780 - ETA: 31s - loss: 0.6963 - accuracy: 0.780 - ETA: 30s - loss: 0.6957 - accuracy: 0.780 - ETA: 29s - loss: 0.6924 - accuracy: 0.781 - ETA: 28s - loss: 0.6912 - accuracy: 0.782 - ETA: 27s - loss: 0.6915 - accuracy: 0.782 - ETA: 26s - loss: 0.6935 - accuracy: 0.782 - ETA: 25s - loss: 0.6942 - accuracy: 0.781 - ETA: 24s - loss: 0.6936 - accuracy: 0.781 - ETA: 23s - loss: 0.6935 - accuracy: 0.781 - ETA: 22s - loss: 0.6947 - accuracy: 0.781 - ETA: 21s - loss: 0.6944 - accuracy: 0.782 - ETA: 20s - loss: 0.6941 - accuracy: 0.782 - ETA: 19s - loss: 0.6912 - accuracy: 0.783 - ETA: 18s - loss: 0.6930 - accuracy: 0.782 - ETA: 17s - loss: 0.6934 - accuracy: 0.781 - ETA: 16s - loss: 0.6915 - accuracy: 0.782 - ETA: 15s - loss: 0.6904 - accuracy: 0.783 - ETA: 14s - loss: 0.6887 - accuracy: 0.783 - ETA: 13s - loss: 0.6879 - accuracy: 0.783 - ETA: 12s - loss: 0.6875 - accuracy: 0.783 - ETA: 11s - loss: 0.6872 - accuracy: 0.783 - ETA: 9s - loss: 0.6862 - accuracy: 0.783 - ETA: 8s - loss: 0.6867 - accuracy: 0.78 - ETA: 7s - loss: 0.6866 - accuracy: 0.78 - ETA: 6s - loss: 0.6878 - accuracy: 0.78 - ETA: 5s - loss: 0.6874 - accuracy: 0.78 - ETA: 4s - loss: 0.6878 - accuracy: 0.78 - ETA: 3s - loss: 0.6889 - accuracy: 0.78 - ETA: 2s - loss: 0.6887 - accuracy: 0.78 - ETA: 1s - loss: 0.6881 - accuracy: 0.78 - ETA: 0s - loss: 0.6912 - accuracy: 0.78 - 119s 9ms/step - loss: 0.6939 - accuracy: 0.7828 - val_loss: 2.7053 - val_accuracy: 0.3612\n",
      "Epoch 26/100\n",
      "13022/13022 [==============================] - ETA: 1:46 - loss: 0.6749 - accuracy: 0.78 - ETA: 1:45 - loss: 0.6934 - accuracy: 0.77 - ETA: 1:41 - loss: 0.7067 - accuracy: 0.77 - ETA: 1:39 - loss: 0.6974 - accuracy: 0.77 - ETA: 1:36 - loss: 0.6632 - accuracy: 0.78 - ETA: 1:34 - loss: 0.6618 - accuracy: 0.79 - ETA: 1:33 - loss: 0.6276 - accuracy: 0.80 - ETA: 1:32 - loss: 0.6082 - accuracy: 0.80 - ETA: 1:31 - loss: 0.6149 - accuracy: 0.80 - ETA: 1:30 - loss: 0.6215 - accuracy: 0.80 - ETA: 1:30 - loss: 0.6207 - accuracy: 0.80 - ETA: 1:29 - loss: 0.6253 - accuracy: 0.80 - ETA: 1:29 - loss: 0.6397 - accuracy: 0.79 - ETA: 1:28 - loss: 0.6456 - accuracy: 0.79 - ETA: 1:27 - loss: 0.6343 - accuracy: 0.79 - ETA: 1:26 - loss: 0.6344 - accuracy: 0.79 - ETA: 1:25 - loss: 0.6465 - accuracy: 0.79 - ETA: 1:23 - loss: 0.6427 - accuracy: 0.79 - ETA: 1:23 - loss: 0.6480 - accuracy: 0.79 - ETA: 1:22 - loss: 0.6454 - accuracy: 0.79 - ETA: 1:20 - loss: 0.6443 - accuracy: 0.79 - ETA: 1:20 - loss: 0.6483 - accuracy: 0.79 - ETA: 1:19 - loss: 0.6505 - accuracy: 0.79 - ETA: 1:18 - loss: 0.6546 - accuracy: 0.79 - ETA: 1:17 - loss: 0.6509 - accuracy: 0.79 - ETA: 1:16 - loss: 0.6513 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6413 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6356 - accuracy: 0.79 - ETA: 1:12 - loss: 0.6358 - accuracy: 0.79 - ETA: 1:11 - loss: 0.6420 - accuracy: 0.79 - ETA: 1:10 - loss: 0.6449 - accuracy: 0.79 - ETA: 1:09 - loss: 0.6475 - accuracy: 0.79 - ETA: 1:09 - loss: 0.6500 - accuracy: 0.79 - ETA: 1:08 - loss: 0.6519 - accuracy: 0.79 - ETA: 1:07 - loss: 0.6494 - accuracy: 0.79 - ETA: 1:06 - loss: 0.6484 - accuracy: 0.79 - ETA: 1:05 - loss: 0.6476 - accuracy: 0.79 - ETA: 1:04 - loss: 0.6504 - accuracy: 0.79 - ETA: 1:03 - loss: 0.6476 - accuracy: 0.79 - ETA: 1:02 - loss: 0.6506 - accuracy: 0.79 - ETA: 1:01 - loss: 0.6526 - accuracy: 0.79 - ETA: 1:00 - loss: 0.6531 - accuracy: 0.79 - ETA: 59s - loss: 0.6514 - accuracy: 0.7925 - ETA: 58s - loss: 0.6528 - accuracy: 0.792 - ETA: 57s - loss: 0.6560 - accuracy: 0.792 - ETA: 56s - loss: 0.6588 - accuracy: 0.791 - ETA: 55s - loss: 0.6594 - accuracy: 0.791 - ETA: 54s - loss: 0.6567 - accuracy: 0.792 - ETA: 53s - loss: 0.6542 - accuracy: 0.793 - ETA: 52s - loss: 0.6569 - accuracy: 0.792 - ETA: 51s - loss: 0.6550 - accuracy: 0.792 - ETA: 50s - loss: 0.6523 - accuracy: 0.792 - ETA: 49s - loss: 0.6535 - accuracy: 0.791 - ETA: 48s - loss: 0.6587 - accuracy: 0.790 - ETA: 47s - loss: 0.6579 - accuracy: 0.790 - ETA: 46s - loss: 0.6587 - accuracy: 0.789 - ETA: 45s - loss: 0.6591 - accuracy: 0.789 - ETA: 44s - loss: 0.6634 - accuracy: 0.788 - ETA: 43s - loss: 0.6651 - accuracy: 0.786 - ETA: 42s - loss: 0.6655 - accuracy: 0.786 - ETA: 41s - loss: 0.6609 - accuracy: 0.787 - ETA: 40s - loss: 0.6583 - accuracy: 0.788 - ETA: 39s - loss: 0.6583 - accuracy: 0.788 - ETA: 38s - loss: 0.6575 - accuracy: 0.788 - ETA: 37s - loss: 0.6573 - accuracy: 0.788 - ETA: 36s - loss: 0.6580 - accuracy: 0.789 - ETA: 35s - loss: 0.6570 - accuracy: 0.789 - ETA: 34s - loss: 0.6562 - accuracy: 0.789 - ETA: 32s - loss: 0.6537 - accuracy: 0.789 - ETA: 31s - loss: 0.6544 - accuracy: 0.789 - ETA: 31s - loss: 0.6563 - accuracy: 0.789 - ETA: 30s - loss: 0.6548 - accuracy: 0.789 - ETA: 29s - loss: 0.6549 - accuracy: 0.789 - ETA: 28s - loss: 0.6525 - accuracy: 0.790 - ETA: 26s - loss: 0.6511 - accuracy: 0.790 - ETA: 25s - loss: 0.6526 - accuracy: 0.790 - ETA: 24s - loss: 0.6534 - accuracy: 0.789 - ETA: 23s - loss: 0.6553 - accuracy: 0.788 - ETA: 22s - loss: 0.6558 - accuracy: 0.789 - ETA: 21s - loss: 0.6548 - accuracy: 0.789 - ETA: 20s - loss: 0.6556 - accuracy: 0.788 - ETA: 19s - loss: 0.6563 - accuracy: 0.788 - ETA: 18s - loss: 0.6568 - accuracy: 0.788 - ETA: 17s - loss: 0.6560 - accuracy: 0.788 - ETA: 16s - loss: 0.6570 - accuracy: 0.788 - ETA: 15s - loss: 0.6574 - accuracy: 0.788 - ETA: 14s - loss: 0.6566 - accuracy: 0.788 - ETA: 13s - loss: 0.6565 - accuracy: 0.788 - ETA: 12s - loss: 0.6568 - accuracy: 0.788 - ETA: 11s - loss: 0.6561 - accuracy: 0.789 - ETA: 10s - loss: 0.6570 - accuracy: 0.789 - ETA: 9s - loss: 0.6569 - accuracy: 0.788 - ETA: 8s - loss: 0.6573 - accuracy: 0.78 - ETA: 7s - loss: 0.6575 - accuracy: 0.78 - ETA: 6s - loss: 0.6571 - accuracy: 0.78 - ETA: 5s - loss: 0.6570 - accuracy: 0.78 - ETA: 4s - loss: 0.6571 - accuracy: 0.78 - ETA: 3s - loss: 0.6568 - accuracy: 0.78 - ETA: 2s - loss: 0.6575 - accuracy: 0.78 - ETA: 1s - loss: 0.6586 - accuracy: 0.78 - ETA: 0s - loss: 0.6576 - accuracy: 0.78 - 114s 9ms/step - loss: 0.6581 - accuracy: 0.7881 - val_loss: 2.6919 - val_accuracy: 0.3674\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:44 - loss: 0.5280 - accuracy: 0.83 - ETA: 1:41 - loss: 0.5849 - accuracy: 0.83 - ETA: 1:37 - loss: 0.5584 - accuracy: 0.83 - ETA: 1:35 - loss: 0.5400 - accuracy: 0.83 - ETA: 1:33 - loss: 0.5531 - accuracy: 0.82 - ETA: 1:32 - loss: 0.6034 - accuracy: 0.80 - ETA: 1:31 - loss: 0.6059 - accuracy: 0.80 - ETA: 1:29 - loss: 0.6119 - accuracy: 0.80 - ETA: 1:28 - loss: 0.6191 - accuracy: 0.80 - ETA: 1:27 - loss: 0.6216 - accuracy: 0.80 - ETA: 1:26 - loss: 0.6203 - accuracy: 0.80 - ETA: 1:24 - loss: 0.6241 - accuracy: 0.79 - ETA: 1:23 - loss: 0.6250 - accuracy: 0.79 - ETA: 1:22 - loss: 0.6269 - accuracy: 0.79 - ETA: 1:21 - loss: 0.6393 - accuracy: 0.79 - ETA: 1:20 - loss: 0.6331 - accuracy: 0.79 - ETA: 1:19 - loss: 0.6347 - accuracy: 0.79 - ETA: 1:18 - loss: 0.6374 - accuracy: 0.79 - ETA: 1:20 - loss: 0.6411 - accuracy: 0.79 - ETA: 1:19 - loss: 0.6428 - accuracy: 0.79 - ETA: 1:18 - loss: 0.6417 - accuracy: 0.79 - ETA: 1:17 - loss: 0.6362 - accuracy: 0.79 - ETA: 1:16 - loss: 0.6456 - accuracy: 0.79 - ETA: 1:16 - loss: 0.6371 - accuracy: 0.79 - ETA: 1:15 - loss: 0.6368 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6392 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6377 - accuracy: 0.79 - ETA: 1:13 - loss: 0.6408 - accuracy: 0.79 - ETA: 1:12 - loss: 0.6354 - accuracy: 0.79 - ETA: 1:11 - loss: 0.6284 - accuracy: 0.80 - ETA: 1:10 - loss: 0.6210 - accuracy: 0.80 - ETA: 1:09 - loss: 0.6218 - accuracy: 0.80 - ETA: 1:08 - loss: 0.6259 - accuracy: 0.80 - ETA: 1:07 - loss: 0.6306 - accuracy: 0.80 - ETA: 1:06 - loss: 0.6294 - accuracy: 0.80 - ETA: 1:04 - loss: 0.6306 - accuracy: 0.80 - ETA: 1:04 - loss: 0.6303 - accuracy: 0.80 - ETA: 1:03 - loss: 0.6258 - accuracy: 0.80 - ETA: 1:02 - loss: 0.6270 - accuracy: 0.80 - ETA: 1:01 - loss: 0.6254 - accuracy: 0.80 - ETA: 1:00 - loss: 0.6299 - accuracy: 0.79 - ETA: 59s - loss: 0.6294 - accuracy: 0.7995 - ETA: 58s - loss: 0.6320 - accuracy: 0.798 - ETA: 57s - loss: 0.6301 - accuracy: 0.798 - ETA: 56s - loss: 0.6299 - accuracy: 0.799 - ETA: 55s - loss: 0.6284 - accuracy: 0.799 - ETA: 54s - loss: 0.6309 - accuracy: 0.798 - ETA: 53s - loss: 0.6286 - accuracy: 0.799 - ETA: 52s - loss: 0.6303 - accuracy: 0.798 - ETA: 51s - loss: 0.6331 - accuracy: 0.797 - ETA: 50s - loss: 0.6328 - accuracy: 0.797 - ETA: 49s - loss: 0.6336 - accuracy: 0.796 - ETA: 48s - loss: 0.6300 - accuracy: 0.797 - ETA: 47s - loss: 0.6308 - accuracy: 0.796 - ETA: 47s - loss: 0.6332 - accuracy: 0.795 - ETA: 45s - loss: 0.6335 - accuracy: 0.796 - ETA: 44s - loss: 0.6385 - accuracy: 0.795 - ETA: 43s - loss: 0.6418 - accuracy: 0.794 - ETA: 42s - loss: 0.6456 - accuracy: 0.793 - ETA: 41s - loss: 0.6456 - accuracy: 0.793 - ETA: 40s - loss: 0.6441 - accuracy: 0.794 - ETA: 39s - loss: 0.6419 - accuracy: 0.794 - ETA: 38s - loss: 0.6414 - accuracy: 0.795 - ETA: 37s - loss: 0.6427 - accuracy: 0.794 - ETA: 36s - loss: 0.6429 - accuracy: 0.794 - ETA: 35s - loss: 0.6431 - accuracy: 0.794 - ETA: 34s - loss: 0.6425 - accuracy: 0.794 - ETA: 33s - loss: 0.6432 - accuracy: 0.794 - ETA: 32s - loss: 0.6435 - accuracy: 0.793 - ETA: 31s - loss: 0.6439 - accuracy: 0.793 - ETA: 30s - loss: 0.6444 - accuracy: 0.793 - ETA: 29s - loss: 0.6433 - accuracy: 0.793 - ETA: 28s - loss: 0.6448 - accuracy: 0.793 - ETA: 27s - loss: 0.6436 - accuracy: 0.794 - ETA: 26s - loss: 0.6418 - accuracy: 0.794 - ETA: 25s - loss: 0.6409 - accuracy: 0.795 - ETA: 24s - loss: 0.6400 - accuracy: 0.795 - ETA: 23s - loss: 0.6391 - accuracy: 0.796 - ETA: 22s - loss: 0.6397 - accuracy: 0.796 - ETA: 21s - loss: 0.6411 - accuracy: 0.795 - ETA: 20s - loss: 0.6439 - accuracy: 0.795 - ETA: 19s - loss: 0.6457 - accuracy: 0.795 - ETA: 18s - loss: 0.6460 - accuracy: 0.795 - ETA: 17s - loss: 0.6443 - accuracy: 0.795 - ETA: 16s - loss: 0.6434 - accuracy: 0.796 - ETA: 15s - loss: 0.6431 - accuracy: 0.795 - ETA: 14s - loss: 0.6425 - accuracy: 0.795 - ETA: 13s - loss: 0.6431 - accuracy: 0.795 - ETA: 12s - loss: 0.6454 - accuracy: 0.795 - ETA: 11s - loss: 0.6463 - accuracy: 0.795 - ETA: 10s - loss: 0.6456 - accuracy: 0.795 - ETA: 9s - loss: 0.6446 - accuracy: 0.795 - ETA: 8s - loss: 0.6442 - accuracy: 0.79 - ETA: 7s - loss: 0.6438 - accuracy: 0.79 - ETA: 6s - loss: 0.6451 - accuracy: 0.79 - ETA: 5s - loss: 0.6450 - accuracy: 0.79 - ETA: 4s - loss: 0.6480 - accuracy: 0.79 - ETA: 3s - loss: 0.6484 - accuracy: 0.79 - ETA: 2s - loss: 0.6494 - accuracy: 0.79 - ETA: 1s - loss: 0.6496 - accuracy: 0.79 - ETA: 0s - loss: 0.6496 - accuracy: 0.79 - 118s 9ms/step - loss: 0.6495 - accuracy: 0.7945 - val_loss: 2.6482 - val_accuracy: 0.3607\n",
      "Epoch 28/100\n",
      "13022/13022 [==============================] - ETA: 1:44 - loss: 1.0071 - accuracy: 0.69 - ETA: 1:42 - loss: 0.8847 - accuracy: 0.74 - ETA: 1:42 - loss: 0.8959 - accuracy: 0.74 - ETA: 1:42 - loss: 0.8243 - accuracy: 0.75 - ETA: 1:39 - loss: 0.7754 - accuracy: 0.77 - ETA: 1:38 - loss: 0.7283 - accuracy: 0.77 - ETA: 1:37 - loss: 0.7028 - accuracy: 0.79 - ETA: 1:36 - loss: 0.7205 - accuracy: 0.78 - ETA: 1:35 - loss: 0.7067 - accuracy: 0.79 - ETA: 1:34 - loss: 0.6925 - accuracy: 0.79 - ETA: 1:33 - loss: 0.6829 - accuracy: 0.79 - ETA: 1:32 - loss: 0.6676 - accuracy: 0.79 - ETA: 1:32 - loss: 0.6533 - accuracy: 0.80 - ETA: 1:31 - loss: 0.6431 - accuracy: 0.80 - ETA: 1:29 - loss: 0.6434 - accuracy: 0.80 - ETA: 1:28 - loss: 0.6296 - accuracy: 0.80 - ETA: 1:27 - loss: 0.6402 - accuracy: 0.80 - ETA: 1:26 - loss: 0.6393 - accuracy: 0.80 - ETA: 1:25 - loss: 0.6409 - accuracy: 0.80 - ETA: 1:24 - loss: 0.6446 - accuracy: 0.80 - ETA: 1:23 - loss: 0.6412 - accuracy: 0.80 - ETA: 1:22 - loss: 0.6512 - accuracy: 0.79 - ETA: 1:20 - loss: 0.6541 - accuracy: 0.79 - ETA: 1:19 - loss: 0.6563 - accuracy: 0.79 - ETA: 1:18 - loss: 0.6554 - accuracy: 0.79 - ETA: 1:17 - loss: 0.6570 - accuracy: 0.79 - ETA: 1:16 - loss: 0.6544 - accuracy: 0.79 - ETA: 1:15 - loss: 0.6578 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6602 - accuracy: 0.79 - ETA: 1:12 - loss: 0.6574 - accuracy: 0.79 - ETA: 1:11 - loss: 0.6578 - accuracy: 0.79 - ETA: 1:10 - loss: 0.6668 - accuracy: 0.79 - ETA: 1:09 - loss: 0.6683 - accuracy: 0.79 - ETA: 1:08 - loss: 0.6681 - accuracy: 0.79 - ETA: 1:07 - loss: 0.6678 - accuracy: 0.79 - ETA: 1:06 - loss: 0.6618 - accuracy: 0.79 - ETA: 1:05 - loss: 0.6579 - accuracy: 0.79 - ETA: 1:04 - loss: 0.6574 - accuracy: 0.79 - ETA: 1:03 - loss: 0.6575 - accuracy: 0.79 - ETA: 1:02 - loss: 0.6652 - accuracy: 0.79 - ETA: 1:01 - loss: 0.6661 - accuracy: 0.79 - ETA: 1:00 - loss: 0.6671 - accuracy: 0.79 - ETA: 59s - loss: 0.6709 - accuracy: 0.7932 - ETA: 58s - loss: 0.6695 - accuracy: 0.793 - ETA: 57s - loss: 0.6708 - accuracy: 0.794 - ETA: 56s - loss: 0.6707 - accuracy: 0.793 - ETA: 55s - loss: 0.6679 - accuracy: 0.793 - ETA: 54s - loss: 0.6650 - accuracy: 0.794 - ETA: 53s - loss: 0.6639 - accuracy: 0.794 - ETA: 52s - loss: 0.6627 - accuracy: 0.795 - ETA: 51s - loss: 0.6623 - accuracy: 0.794 - ETA: 50s - loss: 0.6606 - accuracy: 0.795 - ETA: 49s - loss: 0.6597 - accuracy: 0.795 - ETA: 48s - loss: 0.6554 - accuracy: 0.795 - ETA: 47s - loss: 0.6547 - accuracy: 0.795 - ETA: 46s - loss: 0.6548 - accuracy: 0.795 - ETA: 45s - loss: 0.6534 - accuracy: 0.795 - ETA: 44s - loss: 0.6491 - accuracy: 0.797 - ETA: 43s - loss: 0.6482 - accuracy: 0.797 - ETA: 42s - loss: 0.6449 - accuracy: 0.799 - ETA: 41s - loss: 0.6450 - accuracy: 0.798 - ETA: 40s - loss: 0.6437 - accuracy: 0.799 - ETA: 39s - loss: 0.6438 - accuracy: 0.799 - ETA: 38s - loss: 0.6428 - accuracy: 0.800 - ETA: 37s - loss: 0.6413 - accuracy: 0.800 - ETA: 36s - loss: 0.6439 - accuracy: 0.798 - ETA: 35s - loss: 0.6427 - accuracy: 0.799 - ETA: 34s - loss: 0.6412 - accuracy: 0.800 - ETA: 33s - loss: 0.6387 - accuracy: 0.800 - ETA: 32s - loss: 0.6389 - accuracy: 0.800 - ETA: 31s - loss: 0.6396 - accuracy: 0.800 - ETA: 30s - loss: 0.6382 - accuracy: 0.800 - ETA: 29s - loss: 0.6370 - accuracy: 0.801 - ETA: 28s - loss: 0.6377 - accuracy: 0.801 - ETA: 27s - loss: 0.6381 - accuracy: 0.800 - ETA: 26s - loss: 0.6364 - accuracy: 0.801 - ETA: 25s - loss: 0.6394 - accuracy: 0.799 - ETA: 24s - loss: 0.6376 - accuracy: 0.799 - ETA: 23s - loss: 0.6360 - accuracy: 0.799 - ETA: 22s - loss: 0.6346 - accuracy: 0.800 - ETA: 21s - loss: 0.6355 - accuracy: 0.799 - ETA: 20s - loss: 0.6342 - accuracy: 0.799 - ETA: 19s - loss: 0.6321 - accuracy: 0.799 - ETA: 18s - loss: 0.6325 - accuracy: 0.799 - ETA: 17s - loss: 0.6315 - accuracy: 0.799 - ETA: 15s - loss: 0.6298 - accuracy: 0.800 - ETA: 14s - loss: 0.6323 - accuracy: 0.799 - ETA: 13s - loss: 0.6312 - accuracy: 0.800 - ETA: 12s - loss: 0.6305 - accuracy: 0.800 - ETA: 11s - loss: 0.6311 - accuracy: 0.799 - ETA: 10s - loss: 0.6317 - accuracy: 0.800 - ETA: 9s - loss: 0.6321 - accuracy: 0.799 - ETA: 8s - loss: 0.6321 - accuracy: 0.79 - ETA: 7s - loss: 0.6327 - accuracy: 0.79 - ETA: 6s - loss: 0.6351 - accuracy: 0.79 - ETA: 5s - loss: 0.6348 - accuracy: 0.79 - ETA: 4s - loss: 0.6358 - accuracy: 0.79 - ETA: 3s - loss: 0.6366 - accuracy: 0.79 - ETA: 2s - loss: 0.6365 - accuracy: 0.79 - ETA: 1s - loss: 0.6381 - accuracy: 0.79 - ETA: 0s - loss: 0.6403 - accuracy: 0.79 - 106s 8ms/step - loss: 0.6419 - accuracy: 0.7986 - val_loss: 2.6257 - val_accuracy: 0.3634\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:11 - loss: 0.4785 - accuracy: 0.79 - ETA: 1:09 - loss: 0.5833 - accuracy: 0.78 - ETA: 1:07 - loss: 0.5794 - accuracy: 0.79 - ETA: 1:06 - loss: 0.6068 - accuracy: 0.78 - ETA: 1:06 - loss: 0.6255 - accuracy: 0.78 - ETA: 1:05 - loss: 0.6144 - accuracy: 0.79 - ETA: 1:05 - loss: 0.5950 - accuracy: 0.80 - ETA: 1:05 - loss: 0.5870 - accuracy: 0.79 - ETA: 1:05 - loss: 0.5855 - accuracy: 0.80 - ETA: 1:04 - loss: 0.5821 - accuracy: 0.80 - ETA: 1:04 - loss: 0.5794 - accuracy: 0.80 - ETA: 1:03 - loss: 0.5838 - accuracy: 0.80 - ETA: 1:02 - loss: 0.6027 - accuracy: 0.79 - ETA: 1:02 - loss: 0.5969 - accuracy: 0.79 - ETA: 1:01 - loss: 0.5894 - accuracy: 0.79 - ETA: 1:01 - loss: 0.5904 - accuracy: 0.79 - ETA: 1:00 - loss: 0.5903 - accuracy: 0.80 - ETA: 59s - loss: 0.5868 - accuracy: 0.8038 - ETA: 59s - loss: 0.5934 - accuracy: 0.802 - ETA: 58s - loss: 0.6073 - accuracy: 0.800 - ETA: 57s - loss: 0.6139 - accuracy: 0.799 - ETA: 57s - loss: 0.6156 - accuracy: 0.799 - ETA: 56s - loss: 0.6157 - accuracy: 0.799 - ETA: 55s - loss: 0.6147 - accuracy: 0.802 - ETA: 54s - loss: 0.6123 - accuracy: 0.803 - ETA: 54s - loss: 0.6045 - accuracy: 0.806 - ETA: 53s - loss: 0.6034 - accuracy: 0.807 - ETA: 52s - loss: 0.6105 - accuracy: 0.804 - ETA: 52s - loss: 0.6099 - accuracy: 0.804 - ETA: 51s - loss: 0.6087 - accuracy: 0.806 - ETA: 50s - loss: 0.6119 - accuracy: 0.802 - ETA: 49s - loss: 0.6095 - accuracy: 0.804 - ETA: 49s - loss: 0.6100 - accuracy: 0.804 - ETA: 48s - loss: 0.6082 - accuracy: 0.805 - ETA: 47s - loss: 0.6077 - accuracy: 0.805 - ETA: 47s - loss: 0.6098 - accuracy: 0.805 - ETA: 46s - loss: 0.6142 - accuracy: 0.804 - ETA: 45s - loss: 0.6173 - accuracy: 0.802 - ETA: 45s - loss: 0.6204 - accuracy: 0.802 - ETA: 44s - loss: 0.6203 - accuracy: 0.802 - ETA: 44s - loss: 0.6199 - accuracy: 0.802 - ETA: 43s - loss: 0.6182 - accuracy: 0.803 - ETA: 42s - loss: 0.6185 - accuracy: 0.802 - ETA: 42s - loss: 0.6177 - accuracy: 0.801 - ETA: 41s - loss: 0.6143 - accuracy: 0.802 - ETA: 40s - loss: 0.6137 - accuracy: 0.802 - ETA: 40s - loss: 0.6150 - accuracy: 0.801 - ETA: 39s - loss: 0.6191 - accuracy: 0.800 - ETA: 38s - loss: 0.6176 - accuracy: 0.800 - ETA: 37s - loss: 0.6144 - accuracy: 0.801 - ETA: 37s - loss: 0.6148 - accuracy: 0.801 - ETA: 36s - loss: 0.6167 - accuracy: 0.800 - ETA: 35s - loss: 0.6161 - accuracy: 0.801 - ETA: 34s - loss: 0.6159 - accuracy: 0.801 - ETA: 34s - loss: 0.6149 - accuracy: 0.802 - ETA: 33s - loss: 0.6162 - accuracy: 0.802 - ETA: 32s - loss: 0.6142 - accuracy: 0.803 - ETA: 32s - loss: 0.6145 - accuracy: 0.802 - ETA: 31s - loss: 0.6126 - accuracy: 0.803 - ETA: 30s - loss: 0.6151 - accuracy: 0.802 - ETA: 29s - loss: 0.6198 - accuracy: 0.802 - ETA: 29s - loss: 0.6220 - accuracy: 0.801 - ETA: 28s - loss: 0.6217 - accuracy: 0.801 - ETA: 27s - loss: 0.6201 - accuracy: 0.802 - ETA: 27s - loss: 0.6215 - accuracy: 0.802 - ETA: 26s - loss: 0.6213 - accuracy: 0.802 - ETA: 25s - loss: 0.6186 - accuracy: 0.802 - ETA: 24s - loss: 0.6182 - accuracy: 0.802 - ETA: 24s - loss: 0.6198 - accuracy: 0.801 - ETA: 23s - loss: 0.6179 - accuracy: 0.801 - ETA: 22s - loss: 0.6148 - accuracy: 0.802 - ETA: 21s - loss: 0.6151 - accuracy: 0.802 - ETA: 21s - loss: 0.6151 - accuracy: 0.802 - ETA: 20s - loss: 0.6151 - accuracy: 0.802 - ETA: 19s - loss: 0.6146 - accuracy: 0.802 - ETA: 18s - loss: 0.6128 - accuracy: 0.803 - ETA: 18s - loss: 0.6121 - accuracy: 0.803 - ETA: 17s - loss: 0.6130 - accuracy: 0.802 - ETA: 16s - loss: 0.6152 - accuracy: 0.802 - ETA: 15s - loss: 0.6141 - accuracy: 0.803 - ETA: 15s - loss: 0.6145 - accuracy: 0.802 - ETA: 14s - loss: 0.6151 - accuracy: 0.802 - ETA: 13s - loss: 0.6143 - accuracy: 0.802 - ETA: 12s - loss: 0.6146 - accuracy: 0.802 - ETA: 12s - loss: 0.6152 - accuracy: 0.802 - ETA: 11s - loss: 0.6182 - accuracy: 0.800 - ETA: 10s - loss: 0.6182 - accuracy: 0.800 - ETA: 10s - loss: 0.6194 - accuracy: 0.800 - ETA: 9s - loss: 0.6220 - accuracy: 0.799 - ETA: 8s - loss: 0.6221 - accuracy: 0.80 - ETA: 7s - loss: 0.6207 - accuracy: 0.80 - ETA: 7s - loss: 0.6213 - accuracy: 0.80 - ETA: 6s - loss: 0.6215 - accuracy: 0.80 - ETA: 5s - loss: 0.6206 - accuracy: 0.80 - ETA: 4s - loss: 0.6221 - accuracy: 0.80 - ETA: 4s - loss: 0.6245 - accuracy: 0.80 - ETA: 3s - loss: 0.6228 - accuracy: 0.80 - ETA: 2s - loss: 0.6222 - accuracy: 0.80 - ETA: 1s - loss: 0.6219 - accuracy: 0.80 - ETA: 1s - loss: 0.6220 - accuracy: 0.80 - ETA: 0s - loss: 0.6206 - accuracy: 0.80 - 82s 6ms/step - loss: 0.6216 - accuracy: 0.8015 - val_loss: 2.7140 - val_accuracy: 0.3639\n",
      "Epoch 30/100\n",
      "13022/13022 [==============================] - ETA: 1:11 - loss: 0.5992 - accuracy: 0.79 - ETA: 1:09 - loss: 0.5743 - accuracy: 0.80 - ETA: 1:09 - loss: 0.6218 - accuracy: 0.78 - ETA: 1:08 - loss: 0.6017 - accuracy: 0.80 - ETA: 1:07 - loss: 0.5922 - accuracy: 0.80 - ETA: 1:06 - loss: 0.5877 - accuracy: 0.81 - ETA: 1:05 - loss: 0.6102 - accuracy: 0.81 - ETA: 1:04 - loss: 0.6183 - accuracy: 0.80 - ETA: 1:03 - loss: 0.6044 - accuracy: 0.81 - ETA: 1:02 - loss: 0.6055 - accuracy: 0.81 - ETA: 1:02 - loss: 0.6080 - accuracy: 0.80 - ETA: 1:01 - loss: 0.6222 - accuracy: 0.80 - ETA: 1:01 - loss: 0.6219 - accuracy: 0.80 - ETA: 1:00 - loss: 0.6264 - accuracy: 0.80 - ETA: 1:00 - loss: 0.6347 - accuracy: 0.80 - ETA: 59s - loss: 0.6388 - accuracy: 0.7988 - ETA: 58s - loss: 0.6403 - accuracy: 0.795 - ETA: 58s - loss: 0.6382 - accuracy: 0.796 - ETA: 57s - loss: 0.6322 - accuracy: 0.798 - ETA: 56s - loss: 0.6350 - accuracy: 0.798 - ETA: 55s - loss: 0.6339 - accuracy: 0.799 - ETA: 55s - loss: 0.6357 - accuracy: 0.799 - ETA: 54s - loss: 0.6396 - accuracy: 0.799 - ETA: 53s - loss: 0.6427 - accuracy: 0.798 - ETA: 52s - loss: 0.6464 - accuracy: 0.798 - ETA: 52s - loss: 0.6486 - accuracy: 0.798 - ETA: 51s - loss: 0.6539 - accuracy: 0.796 - ETA: 51s - loss: 0.6527 - accuracy: 0.796 - ETA: 50s - loss: 0.6496 - accuracy: 0.797 - ETA: 49s - loss: 0.6451 - accuracy: 0.797 - ETA: 49s - loss: 0.6415 - accuracy: 0.799 - ETA: 48s - loss: 0.6393 - accuracy: 0.800 - ETA: 47s - loss: 0.6418 - accuracy: 0.800 - ETA: 47s - loss: 0.6430 - accuracy: 0.799 - ETA: 46s - loss: 0.6451 - accuracy: 0.800 - ETA: 45s - loss: 0.6446 - accuracy: 0.800 - ETA: 44s - loss: 0.6425 - accuracy: 0.800 - ETA: 44s - loss: 0.6406 - accuracy: 0.800 - ETA: 43s - loss: 0.6358 - accuracy: 0.801 - ETA: 42s - loss: 0.6355 - accuracy: 0.802 - ETA: 42s - loss: 0.6335 - accuracy: 0.803 - ETA: 41s - loss: 0.6341 - accuracy: 0.802 - ETA: 40s - loss: 0.6327 - accuracy: 0.801 - ETA: 40s - loss: 0.6286 - accuracy: 0.803 - ETA: 39s - loss: 0.6258 - accuracy: 0.803 - ETA: 38s - loss: 0.6258 - accuracy: 0.803 - ETA: 37s - loss: 0.6247 - accuracy: 0.804 - ETA: 37s - loss: 0.6264 - accuracy: 0.804 - ETA: 36s - loss: 0.6257 - accuracy: 0.804 - ETA: 35s - loss: 0.6282 - accuracy: 0.803 - ETA: 35s - loss: 0.6245 - accuracy: 0.804 - ETA: 34s - loss: 0.6260 - accuracy: 0.804 - ETA: 33s - loss: 0.6261 - accuracy: 0.804 - ETA: 32s - loss: 0.6265 - accuracy: 0.805 - ETA: 32s - loss: 0.6292 - accuracy: 0.804 - ETA: 31s - loss: 0.6298 - accuracy: 0.803 - ETA: 30s - loss: 0.6261 - accuracy: 0.804 - ETA: 30s - loss: 0.6260 - accuracy: 0.804 - ETA: 29s - loss: 0.6292 - accuracy: 0.803 - ETA: 28s - loss: 0.6255 - accuracy: 0.803 - ETA: 28s - loss: 0.6293 - accuracy: 0.802 - ETA: 27s - loss: 0.6265 - accuracy: 0.803 - ETA: 26s - loss: 0.6254 - accuracy: 0.803 - ETA: 26s - loss: 0.6256 - accuracy: 0.803 - ETA: 25s - loss: 0.6273 - accuracy: 0.802 - ETA: 24s - loss: 0.6283 - accuracy: 0.802 - ETA: 23s - loss: 0.6252 - accuracy: 0.803 - ETA: 23s - loss: 0.6239 - accuracy: 0.803 - ETA: 22s - loss: 0.6223 - accuracy: 0.803 - ETA: 21s - loss: 0.6228 - accuracy: 0.803 - ETA: 21s - loss: 0.6187 - accuracy: 0.804 - ETA: 20s - loss: 0.6176 - accuracy: 0.805 - ETA: 19s - loss: 0.6161 - accuracy: 0.805 - ETA: 19s - loss: 0.6142 - accuracy: 0.805 - ETA: 18s - loss: 0.6163 - accuracy: 0.804 - ETA: 17s - loss: 0.6155 - accuracy: 0.805 - ETA: 17s - loss: 0.6158 - accuracy: 0.804 - ETA: 16s - loss: 0.6144 - accuracy: 0.804 - ETA: 15s - loss: 0.6137 - accuracy: 0.805 - ETA: 14s - loss: 0.6128 - accuracy: 0.805 - ETA: 14s - loss: 0.6140 - accuracy: 0.805 - ETA: 13s - loss: 0.6139 - accuracy: 0.804 - ETA: 12s - loss: 0.6142 - accuracy: 0.804 - ETA: 12s - loss: 0.6133 - accuracy: 0.804 - ETA: 11s - loss: 0.6147 - accuracy: 0.804 - ETA: 10s - loss: 0.6163 - accuracy: 0.804 - ETA: 10s - loss: 0.6149 - accuracy: 0.804 - ETA: 9s - loss: 0.6152 - accuracy: 0.804 - ETA: 8s - loss: 0.6147 - accuracy: 0.80 - ETA: 8s - loss: 0.6142 - accuracy: 0.80 - ETA: 7s - loss: 0.6132 - accuracy: 0.80 - ETA: 6s - loss: 0.6146 - accuracy: 0.80 - ETA: 6s - loss: 0.6172 - accuracy: 0.80 - ETA: 5s - loss: 0.6167 - accuracy: 0.80 - ETA: 4s - loss: 0.6154 - accuracy: 0.80 - ETA: 3s - loss: 0.6154 - accuracy: 0.80 - ETA: 3s - loss: 0.6170 - accuracy: 0.80 - ETA: 2s - loss: 0.6158 - accuracy: 0.80 - ETA: 1s - loss: 0.6150 - accuracy: 0.80 - ETA: 1s - loss: 0.6145 - accuracy: 0.80 - ETA: 0s - loss: 0.6142 - accuracy: 0.80 - 79s 6ms/step - loss: 0.6143 - accuracy: 0.8055 - val_loss: 2.5555 - val_accuracy: 0.4081\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:17 - loss: 0.6274 - accuracy: 0.80 - ETA: 1:13 - loss: 0.5387 - accuracy: 0.82 - ETA: 1:11 - loss: 0.5521 - accuracy: 0.81 - ETA: 1:09 - loss: 0.5385 - accuracy: 0.82 - ETA: 1:07 - loss: 0.5716 - accuracy: 0.81 - ETA: 1:07 - loss: 0.5638 - accuracy: 0.82 - ETA: 1:05 - loss: 0.5829 - accuracy: 0.81 - ETA: 1:04 - loss: 0.5873 - accuracy: 0.81 - ETA: 1:04 - loss: 0.5906 - accuracy: 0.81 - ETA: 1:04 - loss: 0.5886 - accuracy: 0.81 - ETA: 1:03 - loss: 0.5959 - accuracy: 0.80 - ETA: 1:02 - loss: 0.5943 - accuracy: 0.80 - ETA: 1:02 - loss: 0.5876 - accuracy: 0.80 - ETA: 1:02 - loss: 0.5871 - accuracy: 0.80 - ETA: 1:01 - loss: 0.6001 - accuracy: 0.80 - ETA: 1:00 - loss: 0.6083 - accuracy: 0.80 - ETA: 1:00 - loss: 0.6044 - accuracy: 0.80 - ETA: 59s - loss: 0.6015 - accuracy: 0.8038 - ETA: 58s - loss: 0.5953 - accuracy: 0.808 - ETA: 57s - loss: 0.6010 - accuracy: 0.807 - ETA: 56s - loss: 0.5934 - accuracy: 0.808 - ETA: 56s - loss: 0.5941 - accuracy: 0.807 - ETA: 55s - loss: 0.5950 - accuracy: 0.806 - ETA: 54s - loss: 0.5983 - accuracy: 0.805 - ETA: 53s - loss: 0.5971 - accuracy: 0.804 - ETA: 52s - loss: 0.5987 - accuracy: 0.803 - ETA: 52s - loss: 0.5998 - accuracy: 0.803 - ETA: 51s - loss: 0.5997 - accuracy: 0.803 - ETA: 50s - loss: 0.6014 - accuracy: 0.803 - ETA: 50s - loss: 0.5968 - accuracy: 0.805 - ETA: 49s - loss: 0.6034 - accuracy: 0.804 - ETA: 48s - loss: 0.6046 - accuracy: 0.804 - ETA: 47s - loss: 0.6007 - accuracy: 0.805 - ETA: 47s - loss: 0.5968 - accuracy: 0.807 - ETA: 46s - loss: 0.5948 - accuracy: 0.807 - ETA: 45s - loss: 0.5948 - accuracy: 0.806 - ETA: 45s - loss: 0.5932 - accuracy: 0.806 - ETA: 44s - loss: 0.5927 - accuracy: 0.807 - ETA: 43s - loss: 0.5914 - accuracy: 0.808 - ETA: 43s - loss: 0.5949 - accuracy: 0.806 - ETA: 42s - loss: 0.5938 - accuracy: 0.807 - ETA: 41s - loss: 0.5955 - accuracy: 0.806 - ETA: 41s - loss: 0.5942 - accuracy: 0.807 - ETA: 40s - loss: 0.5899 - accuracy: 0.808 - ETA: 39s - loss: 0.5905 - accuracy: 0.808 - ETA: 38s - loss: 0.5897 - accuracy: 0.808 - ETA: 38s - loss: 0.5891 - accuracy: 0.808 - ETA: 37s - loss: 0.5887 - accuracy: 0.808 - ETA: 36s - loss: 0.5926 - accuracy: 0.807 - ETA: 36s - loss: 0.5936 - accuracy: 0.808 - ETA: 35s - loss: 0.5924 - accuracy: 0.808 - ETA: 34s - loss: 0.5929 - accuracy: 0.808 - ETA: 33s - loss: 0.5921 - accuracy: 0.808 - ETA: 33s - loss: 0.5928 - accuracy: 0.808 - ETA: 32s - loss: 0.5928 - accuracy: 0.809 - ETA: 31s - loss: 0.5941 - accuracy: 0.808 - ETA: 31s - loss: 0.5937 - accuracy: 0.808 - ETA: 30s - loss: 0.5934 - accuracy: 0.808 - ETA: 29s - loss: 0.5927 - accuracy: 0.808 - ETA: 29s - loss: 0.5935 - accuracy: 0.808 - ETA: 28s - loss: 0.5958 - accuracy: 0.807 - ETA: 27s - loss: 0.5936 - accuracy: 0.807 - ETA: 27s - loss: 0.5939 - accuracy: 0.807 - ETA: 26s - loss: 0.5950 - accuracy: 0.808 - ETA: 25s - loss: 0.5941 - accuracy: 0.808 - ETA: 25s - loss: 0.5969 - accuracy: 0.807 - ETA: 24s - loss: 0.5932 - accuracy: 0.808 - ETA: 23s - loss: 0.5937 - accuracy: 0.808 - ETA: 22s - loss: 0.5917 - accuracy: 0.809 - ETA: 22s - loss: 0.5939 - accuracy: 0.809 - ETA: 21s - loss: 0.5949 - accuracy: 0.809 - ETA: 20s - loss: 0.5958 - accuracy: 0.808 - ETA: 20s - loss: 0.5949 - accuracy: 0.808 - ETA: 19s - loss: 0.5969 - accuracy: 0.808 - ETA: 18s - loss: 0.5941 - accuracy: 0.809 - ETA: 18s - loss: 0.5962 - accuracy: 0.809 - ETA: 17s - loss: 0.5951 - accuracy: 0.809 - ETA: 16s - loss: 0.5959 - accuracy: 0.809 - ETA: 15s - loss: 0.5951 - accuracy: 0.809 - ETA: 15s - loss: 0.5955 - accuracy: 0.809 - ETA: 14s - loss: 0.5954 - accuracy: 0.809 - ETA: 13s - loss: 0.5973 - accuracy: 0.809 - ETA: 13s - loss: 0.5989 - accuracy: 0.808 - ETA: 12s - loss: 0.5990 - accuracy: 0.808 - ETA: 11s - loss: 0.6010 - accuracy: 0.808 - ETA: 11s - loss: 0.6011 - accuracy: 0.808 - ETA: 10s - loss: 0.6021 - accuracy: 0.807 - ETA: 9s - loss: 0.6016 - accuracy: 0.807 - ETA: 8s - loss: 0.6009 - accuracy: 0.80 - ETA: 8s - loss: 0.6007 - accuracy: 0.80 - ETA: 7s - loss: 0.6014 - accuracy: 0.80 - ETA: 6s - loss: 0.6012 - accuracy: 0.80 - ETA: 6s - loss: 0.5997 - accuracy: 0.80 - ETA: 5s - loss: 0.6001 - accuracy: 0.80 - ETA: 4s - loss: 0.6002 - accuracy: 0.80 - ETA: 4s - loss: 0.5995 - accuracy: 0.80 - ETA: 3s - loss: 0.5985 - accuracy: 0.80 - ETA: 2s - loss: 0.6009 - accuracy: 0.80 - ETA: 1s - loss: 0.6012 - accuracy: 0.80 - ETA: 1s - loss: 0.6023 - accuracy: 0.80 - ETA: 0s - loss: 0.6007 - accuracy: 0.80 - 80s 6ms/step - loss: 0.6023 - accuracy: 0.8079 - val_loss: 2.7725 - val_accuracy: 0.3643\n",
      "Epoch 32/100\n",
      "13022/13022 [==============================] - ETA: 1:12 - loss: 0.6884 - accuracy: 0.77 - ETA: 1:09 - loss: 0.6876 - accuracy: 0.75 - ETA: 1:07 - loss: 0.6671 - accuracy: 0.75 - ETA: 1:07 - loss: 0.6394 - accuracy: 0.77 - ETA: 1:06 - loss: 0.6201 - accuracy: 0.79 - ETA: 1:05 - loss: 0.6088 - accuracy: 0.79 - ETA: 1:04 - loss: 0.5790 - accuracy: 0.80 - ETA: 1:03 - loss: 0.5724 - accuracy: 0.80 - ETA: 1:03 - loss: 0.5739 - accuracy: 0.80 - ETA: 1:02 - loss: 0.5666 - accuracy: 0.81 - ETA: 1:01 - loss: 0.5675 - accuracy: 0.81 - ETA: 1:01 - loss: 0.5627 - accuracy: 0.81 - ETA: 1:00 - loss: 0.5688 - accuracy: 0.81 - ETA: 1:00 - loss: 0.5723 - accuracy: 0.81 - ETA: 59s - loss: 0.5909 - accuracy: 0.8073 - ETA: 59s - loss: 0.5990 - accuracy: 0.804 - ETA: 58s - loss: 0.5996 - accuracy: 0.807 - ETA: 57s - loss: 0.5963 - accuracy: 0.808 - ETA: 57s - loss: 0.5897 - accuracy: 0.809 - ETA: 56s - loss: 0.5973 - accuracy: 0.808 - ETA: 55s - loss: 0.6019 - accuracy: 0.806 - ETA: 55s - loss: 0.6000 - accuracy: 0.806 - ETA: 54s - loss: 0.6038 - accuracy: 0.802 - ETA: 53s - loss: 0.6057 - accuracy: 0.800 - ETA: 52s - loss: 0.6059 - accuracy: 0.798 - ETA: 52s - loss: 0.6075 - accuracy: 0.798 - ETA: 51s - loss: 0.6094 - accuracy: 0.797 - ETA: 50s - loss: 0.6061 - accuracy: 0.797 - ETA: 50s - loss: 0.6020 - accuracy: 0.799 - ETA: 49s - loss: 0.6030 - accuracy: 0.799 - ETA: 49s - loss: 0.6044 - accuracy: 0.799 - ETA: 48s - loss: 0.6031 - accuracy: 0.800 - ETA: 47s - loss: 0.6041 - accuracy: 0.800 - ETA: 47s - loss: 0.6047 - accuracy: 0.799 - ETA: 46s - loss: 0.6062 - accuracy: 0.798 - ETA: 45s - loss: 0.6043 - accuracy: 0.799 - ETA: 45s - loss: 0.6039 - accuracy: 0.798 - ETA: 44s - loss: 0.6059 - accuracy: 0.799 - ETA: 43s - loss: 0.6067 - accuracy: 0.799 - ETA: 43s - loss: 0.6098 - accuracy: 0.799 - ETA: 42s - loss: 0.6081 - accuracy: 0.799 - ETA: 42s - loss: 0.6088 - accuracy: 0.798 - ETA: 41s - loss: 0.6071 - accuracy: 0.799 - ETA: 40s - loss: 0.6080 - accuracy: 0.798 - ETA: 39s - loss: 0.6037 - accuracy: 0.799 - ETA: 39s - loss: 0.6045 - accuracy: 0.799 - ETA: 38s - loss: 0.6081 - accuracy: 0.798 - ETA: 37s - loss: 0.6071 - accuracy: 0.799 - ETA: 37s - loss: 0.6080 - accuracy: 0.798 - ETA: 36s - loss: 0.6056 - accuracy: 0.799 - ETA: 35s - loss: 0.6065 - accuracy: 0.800 - ETA: 34s - loss: 0.6064 - accuracy: 0.800 - ETA: 34s - loss: 0.6069 - accuracy: 0.799 - ETA: 33s - loss: 0.6030 - accuracy: 0.800 - ETA: 32s - loss: 0.6057 - accuracy: 0.799 - ETA: 32s - loss: 0.6066 - accuracy: 0.799 - ETA: 31s - loss: 0.6057 - accuracy: 0.800 - ETA: 30s - loss: 0.6061 - accuracy: 0.800 - ETA: 30s - loss: 0.6096 - accuracy: 0.799 - ETA: 29s - loss: 0.6075 - accuracy: 0.800 - ETA: 28s - loss: 0.6083 - accuracy: 0.800 - ETA: 28s - loss: 0.6069 - accuracy: 0.801 - ETA: 27s - loss: 0.6072 - accuracy: 0.801 - ETA: 26s - loss: 0.6055 - accuracy: 0.802 - ETA: 25s - loss: 0.6042 - accuracy: 0.802 - ETA: 25s - loss: 0.6006 - accuracy: 0.803 - ETA: 24s - loss: 0.5990 - accuracy: 0.804 - ETA: 23s - loss: 0.5986 - accuracy: 0.804 - ETA: 23s - loss: 0.5973 - accuracy: 0.804 - ETA: 22s - loss: 0.5966 - accuracy: 0.805 - ETA: 21s - loss: 0.5961 - accuracy: 0.806 - ETA: 20s - loss: 0.5989 - accuracy: 0.805 - ETA: 20s - loss: 0.5987 - accuracy: 0.805 - ETA: 19s - loss: 0.5979 - accuracy: 0.806 - ETA: 18s - loss: 0.5964 - accuracy: 0.806 - ETA: 18s - loss: 0.5942 - accuracy: 0.807 - ETA: 17s - loss: 0.5967 - accuracy: 0.806 - ETA: 16s - loss: 0.5974 - accuracy: 0.806 - ETA: 15s - loss: 0.5981 - accuracy: 0.806 - ETA: 15s - loss: 0.5992 - accuracy: 0.806 - ETA: 14s - loss: 0.5982 - accuracy: 0.806 - ETA: 13s - loss: 0.5986 - accuracy: 0.806 - ETA: 13s - loss: 0.5974 - accuracy: 0.806 - ETA: 12s - loss: 0.5985 - accuracy: 0.806 - ETA: 11s - loss: 0.5977 - accuracy: 0.807 - ETA: 10s - loss: 0.5992 - accuracy: 0.807 - ETA: 10s - loss: 0.5996 - accuracy: 0.807 - ETA: 9s - loss: 0.5996 - accuracy: 0.807 - ETA: 8s - loss: 0.6002 - accuracy: 0.80 - ETA: 8s - loss: 0.6018 - accuracy: 0.80 - ETA: 7s - loss: 0.6027 - accuracy: 0.80 - ETA: 6s - loss: 0.6036 - accuracy: 0.80 - ETA: 6s - loss: 0.6033 - accuracy: 0.80 - ETA: 5s - loss: 0.6028 - accuracy: 0.80 - ETA: 4s - loss: 0.6034 - accuracy: 0.80 - ETA: 3s - loss: 0.6059 - accuracy: 0.80 - ETA: 3s - loss: 0.6067 - accuracy: 0.80 - ETA: 2s - loss: 0.6071 - accuracy: 0.80 - ETA: 1s - loss: 0.6071 - accuracy: 0.80 - ETA: 1s - loss: 0.6088 - accuracy: 0.80 - ETA: 0s - loss: 0.6092 - accuracy: 0.80 - 78s 6ms/step - loss: 0.6093 - accuracy: 0.8053 - val_loss: 2.5771 - val_accuracy: 0.3500\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:07 - loss: 0.7404 - accuracy: 0.75 - ETA: 1:05 - loss: 0.5997 - accuracy: 0.82 - ETA: 1:04 - loss: 0.6913 - accuracy: 0.79 - ETA: 1:03 - loss: 0.6638 - accuracy: 0.81 - ETA: 1:02 - loss: 0.6361 - accuracy: 0.81 - ETA: 1:01 - loss: 0.6386 - accuracy: 0.81 - ETA: 1:00 - loss: 0.6533 - accuracy: 0.81 - ETA: 59s - loss: 0.6406 - accuracy: 0.8115 - ETA: 58s - loss: 0.6427 - accuracy: 0.804 - ETA: 57s - loss: 0.6275 - accuracy: 0.805 - ETA: 57s - loss: 0.6151 - accuracy: 0.806 - ETA: 56s - loss: 0.6287 - accuracy: 0.803 - ETA: 56s - loss: 0.6293 - accuracy: 0.804 - ETA: 55s - loss: 0.6206 - accuracy: 0.808 - ETA: 54s - loss: 0.6144 - accuracy: 0.810 - ETA: 54s - loss: 0.6067 - accuracy: 0.812 - ETA: 53s - loss: 0.6026 - accuracy: 0.814 - ETA: 52s - loss: 0.6039 - accuracy: 0.814 - ETA: 52s - loss: 0.6026 - accuracy: 0.815 - ETA: 52s - loss: 0.5966 - accuracy: 0.817 - ETA: 51s - loss: 0.6068 - accuracy: 0.814 - ETA: 51s - loss: 0.6057 - accuracy: 0.813 - ETA: 50s - loss: 0.6026 - accuracy: 0.813 - ETA: 49s - loss: 0.6023 - accuracy: 0.812 - ETA: 49s - loss: 0.6073 - accuracy: 0.810 - ETA: 48s - loss: 0.6026 - accuracy: 0.812 - ETA: 48s - loss: 0.6019 - accuracy: 0.812 - ETA: 47s - loss: 0.5979 - accuracy: 0.813 - ETA: 46s - loss: 0.5955 - accuracy: 0.814 - ETA: 46s - loss: 0.5973 - accuracy: 0.814 - ETA: 45s - loss: 0.6034 - accuracy: 0.812 - ETA: 44s - loss: 0.5997 - accuracy: 0.813 - ETA: 44s - loss: 0.5947 - accuracy: 0.814 - ETA: 43s - loss: 0.5952 - accuracy: 0.815 - ETA: 43s - loss: 0.5928 - accuracy: 0.815 - ETA: 42s - loss: 0.5882 - accuracy: 0.816 - ETA: 41s - loss: 0.5838 - accuracy: 0.817 - ETA: 41s - loss: 0.5831 - accuracy: 0.817 - ETA: 40s - loss: 0.5819 - accuracy: 0.816 - ETA: 39s - loss: 0.5812 - accuracy: 0.817 - ETA: 39s - loss: 0.5810 - accuracy: 0.816 - ETA: 38s - loss: 0.5838 - accuracy: 0.815 - ETA: 38s - loss: 0.5821 - accuracy: 0.815 - ETA: 37s - loss: 0.5831 - accuracy: 0.815 - ETA: 36s - loss: 0.5806 - accuracy: 0.815 - ETA: 36s - loss: 0.5784 - accuracy: 0.815 - ETA: 35s - loss: 0.5776 - accuracy: 0.816 - ETA: 34s - loss: 0.5756 - accuracy: 0.816 - ETA: 34s - loss: 0.5820 - accuracy: 0.815 - ETA: 33s - loss: 0.5835 - accuracy: 0.814 - ETA: 33s - loss: 0.5842 - accuracy: 0.813 - ETA: 32s - loss: 0.5815 - accuracy: 0.814 - ETA: 31s - loss: 0.5801 - accuracy: 0.814 - ETA: 31s - loss: 0.5798 - accuracy: 0.815 - ETA: 30s - loss: 0.5777 - accuracy: 0.815 - ETA: 29s - loss: 0.5768 - accuracy: 0.816 - ETA: 29s - loss: 0.5753 - accuracy: 0.816 - ETA: 28s - loss: 0.5731 - accuracy: 0.817 - ETA: 27s - loss: 0.5728 - accuracy: 0.817 - ETA: 27s - loss: 0.5714 - accuracy: 0.817 - ETA: 26s - loss: 0.5719 - accuracy: 0.817 - ETA: 25s - loss: 0.5729 - accuracy: 0.818 - ETA: 25s - loss: 0.5756 - accuracy: 0.817 - ETA: 24s - loss: 0.5762 - accuracy: 0.816 - ETA: 24s - loss: 0.5770 - accuracy: 0.816 - ETA: 23s - loss: 0.5828 - accuracy: 0.815 - ETA: 22s - loss: 0.5811 - accuracy: 0.816 - ETA: 22s - loss: 0.5803 - accuracy: 0.815 - ETA: 21s - loss: 0.5822 - accuracy: 0.815 - ETA: 20s - loss: 0.5805 - accuracy: 0.815 - ETA: 20s - loss: 0.5805 - accuracy: 0.815 - ETA: 19s - loss: 0.5803 - accuracy: 0.815 - ETA: 18s - loss: 0.5866 - accuracy: 0.813 - ETA: 18s - loss: 0.5860 - accuracy: 0.813 - ETA: 17s - loss: 0.5872 - accuracy: 0.813 - ETA: 16s - loss: 0.5881 - accuracy: 0.813 - ETA: 16s - loss: 0.5895 - accuracy: 0.812 - ETA: 15s - loss: 0.5887 - accuracy: 0.812 - ETA: 14s - loss: 0.5887 - accuracy: 0.812 - ETA: 14s - loss: 0.5891 - accuracy: 0.812 - ETA: 13s - loss: 0.5877 - accuracy: 0.813 - ETA: 12s - loss: 0.5874 - accuracy: 0.813 - ETA: 12s - loss: 0.5878 - accuracy: 0.813 - ETA: 11s - loss: 0.5871 - accuracy: 0.813 - ETA: 10s - loss: 0.5880 - accuracy: 0.812 - ETA: 10s - loss: 0.5871 - accuracy: 0.813 - ETA: 9s - loss: 0.5866 - accuracy: 0.813 - ETA: 8s - loss: 0.5855 - accuracy: 0.81 - ETA: 8s - loss: 0.5882 - accuracy: 0.81 - ETA: 7s - loss: 0.5860 - accuracy: 0.81 - ETA: 7s - loss: 0.5870 - accuracy: 0.81 - ETA: 6s - loss: 0.5857 - accuracy: 0.81 - ETA: 5s - loss: 0.5857 - accuracy: 0.81 - ETA: 5s - loss: 0.5870 - accuracy: 0.81 - ETA: 4s - loss: 0.5862 - accuracy: 0.81 - ETA: 3s - loss: 0.5871 - accuracy: 0.81 - ETA: 3s - loss: 0.5877 - accuracy: 0.81 - ETA: 2s - loss: 0.5871 - accuracy: 0.81 - ETA: 1s - loss: 0.5894 - accuracy: 0.81 - ETA: 1s - loss: 0.5899 - accuracy: 0.81 - ETA: 0s - loss: 0.5899 - accuracy: 0.81 - 74s 6ms/step - loss: 0.5892 - accuracy: 0.8136 - val_loss: 2.7862 - val_accuracy: 0.3467\n",
      "Epoch 34/100\n",
      "13022/13022 [==============================] - ETA: 1:12 - loss: 0.5329 - accuracy: 0.82 - ETA: 1:11 - loss: 0.5233 - accuracy: 0.81 - ETA: 1:09 - loss: 0.5288 - accuracy: 0.81 - ETA: 1:08 - loss: 0.5127 - accuracy: 0.82 - ETA: 1:07 - loss: 0.5379 - accuracy: 0.81 - ETA: 1:05 - loss: 0.5270 - accuracy: 0.81 - ETA: 1:04 - loss: 0.5315 - accuracy: 0.82 - ETA: 1:03 - loss: 0.5435 - accuracy: 0.82 - ETA: 1:03 - loss: 0.5472 - accuracy: 0.82 - ETA: 1:02 - loss: 0.5386 - accuracy: 0.82 - ETA: 1:03 - loss: 0.5352 - accuracy: 0.82 - ETA: 1:02 - loss: 0.5456 - accuracy: 0.81 - ETA: 1:01 - loss: 0.5548 - accuracy: 0.81 - ETA: 1:00 - loss: 0.5658 - accuracy: 0.81 - ETA: 59s - loss: 0.5655 - accuracy: 0.8130 - ETA: 58s - loss: 0.5672 - accuracy: 0.812 - ETA: 57s - loss: 0.5734 - accuracy: 0.809 - ETA: 57s - loss: 0.5845 - accuracy: 0.807 - ETA: 56s - loss: 0.5834 - accuracy: 0.807 - ETA: 55s - loss: 0.5777 - accuracy: 0.809 - ETA: 54s - loss: 0.5842 - accuracy: 0.808 - ETA: 53s - loss: 0.5826 - accuracy: 0.806 - ETA: 53s - loss: 0.5783 - accuracy: 0.807 - ETA: 52s - loss: 0.5735 - accuracy: 0.809 - ETA: 51s - loss: 0.5689 - accuracy: 0.812 - ETA: 51s - loss: 0.5743 - accuracy: 0.810 - ETA: 50s - loss: 0.5810 - accuracy: 0.809 - ETA: 50s - loss: 0.5781 - accuracy: 0.809 - ETA: 49s - loss: 0.5801 - accuracy: 0.809 - ETA: 48s - loss: 0.5797 - accuracy: 0.809 - ETA: 48s - loss: 0.5747 - accuracy: 0.810 - ETA: 47s - loss: 0.5767 - accuracy: 0.809 - ETA: 46s - loss: 0.5784 - accuracy: 0.808 - ETA: 45s - loss: 0.5789 - accuracy: 0.808 - ETA: 45s - loss: 0.5790 - accuracy: 0.808 - ETA: 44s - loss: 0.5783 - accuracy: 0.809 - ETA: 43s - loss: 0.5776 - accuracy: 0.809 - ETA: 42s - loss: 0.5767 - accuracy: 0.809 - ETA: 42s - loss: 0.5741 - accuracy: 0.810 - ETA: 41s - loss: 0.5729 - accuracy: 0.810 - ETA: 40s - loss: 0.5734 - accuracy: 0.812 - ETA: 40s - loss: 0.5692 - accuracy: 0.812 - ETA: 39s - loss: 0.5717 - accuracy: 0.811 - ETA: 38s - loss: 0.5748 - accuracy: 0.811 - ETA: 38s - loss: 0.5726 - accuracy: 0.810 - ETA: 37s - loss: 0.5737 - accuracy: 0.811 - ETA: 36s - loss: 0.5694 - accuracy: 0.812 - ETA: 35s - loss: 0.5679 - accuracy: 0.813 - ETA: 35s - loss: 0.5679 - accuracy: 0.813 - ETA: 34s - loss: 0.5720 - accuracy: 0.811 - ETA: 34s - loss: 0.5712 - accuracy: 0.812 - ETA: 33s - loss: 0.5689 - accuracy: 0.812 - ETA: 32s - loss: 0.5658 - accuracy: 0.814 - ETA: 31s - loss: 0.5653 - accuracy: 0.813 - ETA: 31s - loss: 0.5626 - accuracy: 0.814 - ETA: 30s - loss: 0.5625 - accuracy: 0.814 - ETA: 29s - loss: 0.5644 - accuracy: 0.813 - ETA: 29s - loss: 0.5646 - accuracy: 0.814 - ETA: 28s - loss: 0.5629 - accuracy: 0.813 - ETA: 27s - loss: 0.5644 - accuracy: 0.813 - ETA: 27s - loss: 0.5629 - accuracy: 0.813 - ETA: 26s - loss: 0.5610 - accuracy: 0.813 - ETA: 25s - loss: 0.5586 - accuracy: 0.814 - ETA: 25s - loss: 0.5625 - accuracy: 0.813 - ETA: 24s - loss: 0.5625 - accuracy: 0.813 - ETA: 23s - loss: 0.5619 - accuracy: 0.813 - ETA: 23s - loss: 0.5600 - accuracy: 0.814 - ETA: 22s - loss: 0.5638 - accuracy: 0.814 - ETA: 21s - loss: 0.5624 - accuracy: 0.814 - ETA: 21s - loss: 0.5653 - accuracy: 0.814 - ETA: 20s - loss: 0.5667 - accuracy: 0.813 - ETA: 19s - loss: 0.5701 - accuracy: 0.813 - ETA: 19s - loss: 0.5706 - accuracy: 0.813 - ETA: 18s - loss: 0.5714 - accuracy: 0.813 - ETA: 17s - loss: 0.5707 - accuracy: 0.814 - ETA: 17s - loss: 0.5705 - accuracy: 0.814 - ETA: 16s - loss: 0.5696 - accuracy: 0.814 - ETA: 15s - loss: 0.5684 - accuracy: 0.814 - ETA: 15s - loss: 0.5699 - accuracy: 0.814 - ETA: 14s - loss: 0.5714 - accuracy: 0.814 - ETA: 13s - loss: 0.5710 - accuracy: 0.815 - ETA: 13s - loss: 0.5728 - accuracy: 0.814 - ETA: 12s - loss: 0.5727 - accuracy: 0.815 - ETA: 11s - loss: 0.5745 - accuracy: 0.814 - ETA: 11s - loss: 0.5744 - accuracy: 0.814 - ETA: 10s - loss: 0.5745 - accuracy: 0.814 - ETA: 9s - loss: 0.5732 - accuracy: 0.815 - ETA: 9s - loss: 0.5722 - accuracy: 0.81 - ETA: 8s - loss: 0.5720 - accuracy: 0.81 - ETA: 7s - loss: 0.5702 - accuracy: 0.81 - ETA: 7s - loss: 0.5714 - accuracy: 0.81 - ETA: 6s - loss: 0.5726 - accuracy: 0.81 - ETA: 5s - loss: 0.5747 - accuracy: 0.81 - ETA: 5s - loss: 0.5740 - accuracy: 0.81 - ETA: 4s - loss: 0.5747 - accuracy: 0.81 - ETA: 3s - loss: 0.5765 - accuracy: 0.81 - ETA: 3s - loss: 0.5771 - accuracy: 0.81 - ETA: 2s - loss: 0.5761 - accuracy: 0.81 - ETA: 1s - loss: 0.5747 - accuracy: 0.81 - ETA: 1s - loss: 0.5755 - accuracy: 0.81 - ETA: 0s - loss: 0.5758 - accuracy: 0.81 - 76s 6ms/step - loss: 0.5785 - accuracy: 0.8151 - val_loss: 2.6661 - val_accuracy: 0.3703\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:09 - loss: 0.5760 - accuracy: 0.81 - ETA: 1:06 - loss: 0.5389 - accuracy: 0.80 - ETA: 1:05 - loss: 0.5278 - accuracy: 0.80 - ETA: 1:04 - loss: 0.5322 - accuracy: 0.81 - ETA: 1:02 - loss: 0.5629 - accuracy: 0.81 - ETA: 1:02 - loss: 0.5685 - accuracy: 0.82 - ETA: 1:01 - loss: 0.5733 - accuracy: 0.81 - ETA: 1:01 - loss: 0.5615 - accuracy: 0.81 - ETA: 1:00 - loss: 0.5642 - accuracy: 0.81 - ETA: 1:00 - loss: 0.5552 - accuracy: 0.81 - ETA: 1:00 - loss: 0.5611 - accuracy: 0.81 - ETA: 59s - loss: 0.5561 - accuracy: 0.8184 - ETA: 58s - loss: 0.5523 - accuracy: 0.819 - ETA: 57s - loss: 0.5512 - accuracy: 0.821 - ETA: 57s - loss: 0.5573 - accuracy: 0.821 - ETA: 56s - loss: 0.5466 - accuracy: 0.825 - ETA: 55s - loss: 0.5513 - accuracy: 0.823 - ETA: 54s - loss: 0.5597 - accuracy: 0.820 - ETA: 54s - loss: 0.5652 - accuracy: 0.818 - ETA: 53s - loss: 0.5744 - accuracy: 0.816 - ETA: 52s - loss: 0.5906 - accuracy: 0.812 - ETA: 52s - loss: 0.5924 - accuracy: 0.811 - ETA: 51s - loss: 0.5910 - accuracy: 0.811 - ETA: 50s - loss: 0.5863 - accuracy: 0.813 - ETA: 50s - loss: 0.6000 - accuracy: 0.809 - ETA: 49s - loss: 0.6010 - accuracy: 0.808 - ETA: 48s - loss: 0.6015 - accuracy: 0.808 - ETA: 48s - loss: 0.5969 - accuracy: 0.810 - ETA: 47s - loss: 0.5939 - accuracy: 0.811 - ETA: 46s - loss: 0.5924 - accuracy: 0.812 - ETA: 46s - loss: 0.5943 - accuracy: 0.811 - ETA: 45s - loss: 0.5959 - accuracy: 0.810 - ETA: 44s - loss: 0.5938 - accuracy: 0.811 - ETA: 44s - loss: 0.5892 - accuracy: 0.813 - ETA: 43s - loss: 0.5885 - accuracy: 0.813 - ETA: 43s - loss: 0.5895 - accuracy: 0.813 - ETA: 42s - loss: 0.5879 - accuracy: 0.814 - ETA: 41s - loss: 0.5902 - accuracy: 0.813 - ETA: 41s - loss: 0.5905 - accuracy: 0.814 - ETA: 40s - loss: 0.5894 - accuracy: 0.815 - ETA: 39s - loss: 0.5895 - accuracy: 0.815 - ETA: 39s - loss: 0.5927 - accuracy: 0.814 - ETA: 38s - loss: 0.5921 - accuracy: 0.814 - ETA: 37s - loss: 0.5913 - accuracy: 0.814 - ETA: 37s - loss: 0.5927 - accuracy: 0.813 - ETA: 36s - loss: 0.5951 - accuracy: 0.812 - ETA: 35s - loss: 0.5982 - accuracy: 0.811 - ETA: 35s - loss: 0.5980 - accuracy: 0.811 - ETA: 34s - loss: 0.5971 - accuracy: 0.811 - ETA: 33s - loss: 0.5955 - accuracy: 0.811 - ETA: 33s - loss: 0.5967 - accuracy: 0.812 - ETA: 32s - loss: 0.5979 - accuracy: 0.812 - ETA: 31s - loss: 0.5933 - accuracy: 0.814 - ETA: 31s - loss: 0.5959 - accuracy: 0.812 - ETA: 30s - loss: 0.5960 - accuracy: 0.812 - ETA: 29s - loss: 0.5941 - accuracy: 0.812 - ETA: 29s - loss: 0.5934 - accuracy: 0.812 - ETA: 28s - loss: 0.5939 - accuracy: 0.812 - ETA: 27s - loss: 0.5940 - accuracy: 0.812 - ETA: 27s - loss: 0.5936 - accuracy: 0.812 - ETA: 26s - loss: 0.5894 - accuracy: 0.813 - ETA: 26s - loss: 0.5890 - accuracy: 0.814 - ETA: 25s - loss: 0.5879 - accuracy: 0.814 - ETA: 24s - loss: 0.5893 - accuracy: 0.814 - ETA: 24s - loss: 0.5877 - accuracy: 0.814 - ETA: 23s - loss: 0.5852 - accuracy: 0.816 - ETA: 22s - loss: 0.5839 - accuracy: 0.816 - ETA: 22s - loss: 0.5822 - accuracy: 0.817 - ETA: 21s - loss: 0.5845 - accuracy: 0.817 - ETA: 20s - loss: 0.5845 - accuracy: 0.817 - ETA: 20s - loss: 0.5848 - accuracy: 0.817 - ETA: 19s - loss: 0.5862 - accuracy: 0.817 - ETA: 18s - loss: 0.5861 - accuracy: 0.817 - ETA: 18s - loss: 0.5852 - accuracy: 0.817 - ETA: 17s - loss: 0.5853 - accuracy: 0.817 - ETA: 16s - loss: 0.5844 - accuracy: 0.817 - ETA: 16s - loss: 0.5837 - accuracy: 0.817 - ETA: 15s - loss: 0.5829 - accuracy: 0.817 - ETA: 14s - loss: 0.5798 - accuracy: 0.818 - ETA: 14s - loss: 0.5775 - accuracy: 0.818 - ETA: 13s - loss: 0.5760 - accuracy: 0.819 - ETA: 13s - loss: 0.5745 - accuracy: 0.819 - ETA: 12s - loss: 0.5757 - accuracy: 0.819 - ETA: 11s - loss: 0.5762 - accuracy: 0.819 - ETA: 11s - loss: 0.5766 - accuracy: 0.819 - ETA: 10s - loss: 0.5767 - accuracy: 0.819 - ETA: 9s - loss: 0.5754 - accuracy: 0.820 - ETA: 9s - loss: 0.5731 - accuracy: 0.82 - ETA: 8s - loss: 0.5728 - accuracy: 0.82 - ETA: 7s - loss: 0.5738 - accuracy: 0.82 - ETA: 7s - loss: 0.5748 - accuracy: 0.82 - ETA: 6s - loss: 0.5747 - accuracy: 0.82 - ETA: 5s - loss: 0.5725 - accuracy: 0.82 - ETA: 5s - loss: 0.5715 - accuracy: 0.82 - ETA: 4s - loss: 0.5704 - accuracy: 0.82 - ETA: 3s - loss: 0.5694 - accuracy: 0.82 - ETA: 3s - loss: 0.5676 - accuracy: 0.82 - ETA: 2s - loss: 0.5678 - accuracy: 0.82 - ETA: 1s - loss: 0.5673 - accuracy: 0.82 - ETA: 1s - loss: 0.5673 - accuracy: 0.82 - ETA: 0s - loss: 0.5663 - accuracy: 0.82 - 75s 6ms/step - loss: 0.5675 - accuracy: 0.8222 - val_loss: 2.8816 - val_accuracy: 0.3423\n",
      "Epoch 36/100\n",
      "13022/13022 [==============================] - ETA: 1:14 - loss: 0.5559 - accuracy: 0.82 - ETA: 1:10 - loss: 0.5997 - accuracy: 0.81 - ETA: 1:07 - loss: 0.5977 - accuracy: 0.80 - ETA: 1:06 - loss: 0.5931 - accuracy: 0.82 - ETA: 1:04 - loss: 0.5565 - accuracy: 0.82 - ETA: 1:03 - loss: 0.5718 - accuracy: 0.82 - ETA: 1:02 - loss: 0.5839 - accuracy: 0.81 - ETA: 1:01 - loss: 0.5747 - accuracy: 0.81 - ETA: 1:01 - loss: 0.5842 - accuracy: 0.81 - ETA: 1:00 - loss: 0.5851 - accuracy: 0.81 - ETA: 59s - loss: 0.5819 - accuracy: 0.8189 - ETA: 58s - loss: 0.5741 - accuracy: 0.821 - ETA: 58s - loss: 0.5764 - accuracy: 0.824 - ETA: 57s - loss: 0.5891 - accuracy: 0.822 - ETA: 57s - loss: 0.5974 - accuracy: 0.821 - ETA: 56s - loss: 0.5944 - accuracy: 0.822 - ETA: 56s - loss: 0.5881 - accuracy: 0.821 - ETA: 55s - loss: 0.5916 - accuracy: 0.820 - ETA: 54s - loss: 0.5887 - accuracy: 0.817 - ETA: 54s - loss: 0.5914 - accuracy: 0.816 - ETA: 53s - loss: 0.5893 - accuracy: 0.815 - ETA: 52s - loss: 0.5907 - accuracy: 0.815 - ETA: 52s - loss: 0.5916 - accuracy: 0.815 - ETA: 51s - loss: 0.5836 - accuracy: 0.818 - ETA: 50s - loss: 0.5812 - accuracy: 0.819 - ETA: 50s - loss: 0.5759 - accuracy: 0.820 - ETA: 49s - loss: 0.5743 - accuracy: 0.820 - ETA: 48s - loss: 0.5694 - accuracy: 0.822 - ETA: 48s - loss: 0.5648 - accuracy: 0.823 - ETA: 47s - loss: 0.5648 - accuracy: 0.824 - ETA: 46s - loss: 0.5656 - accuracy: 0.823 - ETA: 45s - loss: 0.5725 - accuracy: 0.820 - ETA: 45s - loss: 0.5754 - accuracy: 0.820 - ETA: 44s - loss: 0.5747 - accuracy: 0.821 - ETA: 43s - loss: 0.5705 - accuracy: 0.822 - ETA: 43s - loss: 0.5744 - accuracy: 0.822 - ETA: 42s - loss: 0.5753 - accuracy: 0.821 - ETA: 41s - loss: 0.5761 - accuracy: 0.821 - ETA: 41s - loss: 0.5767 - accuracy: 0.820 - ETA: 40s - loss: 0.5747 - accuracy: 0.821 - ETA: 40s - loss: 0.5716 - accuracy: 0.822 - ETA: 39s - loss: 0.5700 - accuracy: 0.823 - ETA: 38s - loss: 0.5654 - accuracy: 0.824 - ETA: 38s - loss: 0.5650 - accuracy: 0.824 - ETA: 37s - loss: 0.5619 - accuracy: 0.826 - ETA: 36s - loss: 0.5627 - accuracy: 0.825 - ETA: 36s - loss: 0.5611 - accuracy: 0.826 - ETA: 35s - loss: 0.5610 - accuracy: 0.825 - ETA: 34s - loss: 0.5613 - accuracy: 0.825 - ETA: 33s - loss: 0.5591 - accuracy: 0.826 - ETA: 33s - loss: 0.5576 - accuracy: 0.825 - ETA: 32s - loss: 0.5590 - accuracy: 0.825 - ETA: 32s - loss: 0.5569 - accuracy: 0.826 - ETA: 31s - loss: 0.5593 - accuracy: 0.825 - ETA: 30s - loss: 0.5577 - accuracy: 0.826 - ETA: 30s - loss: 0.5562 - accuracy: 0.827 - ETA: 29s - loss: 0.5573 - accuracy: 0.826 - ETA: 28s - loss: 0.5572 - accuracy: 0.826 - ETA: 28s - loss: 0.5540 - accuracy: 0.827 - ETA: 27s - loss: 0.5521 - accuracy: 0.827 - ETA: 26s - loss: 0.5508 - accuracy: 0.828 - ETA: 26s - loss: 0.5487 - accuracy: 0.828 - ETA: 25s - loss: 0.5490 - accuracy: 0.829 - ETA: 24s - loss: 0.5481 - accuracy: 0.828 - ETA: 24s - loss: 0.5478 - accuracy: 0.829 - ETA: 23s - loss: 0.5476 - accuracy: 0.829 - ETA: 22s - loss: 0.5459 - accuracy: 0.830 - ETA: 22s - loss: 0.5468 - accuracy: 0.830 - ETA: 21s - loss: 0.5453 - accuracy: 0.831 - ETA: 20s - loss: 0.5442 - accuracy: 0.831 - ETA: 20s - loss: 0.5441 - accuracy: 0.830 - ETA: 19s - loss: 0.5440 - accuracy: 0.831 - ETA: 18s - loss: 0.5430 - accuracy: 0.830 - ETA: 18s - loss: 0.5434 - accuracy: 0.831 - ETA: 17s - loss: 0.5411 - accuracy: 0.832 - ETA: 16s - loss: 0.5405 - accuracy: 0.832 - ETA: 16s - loss: 0.5431 - accuracy: 0.831 - ETA: 15s - loss: 0.5430 - accuracy: 0.831 - ETA: 15s - loss: 0.5436 - accuracy: 0.831 - ETA: 14s - loss: 0.5435 - accuracy: 0.831 - ETA: 13s - loss: 0.5449 - accuracy: 0.830 - ETA: 13s - loss: 0.5441 - accuracy: 0.830 - ETA: 12s - loss: 0.5437 - accuracy: 0.830 - ETA: 11s - loss: 0.5434 - accuracy: 0.830 - ETA: 11s - loss: 0.5438 - accuracy: 0.830 - ETA: 10s - loss: 0.5414 - accuracy: 0.830 - ETA: 9s - loss: 0.5418 - accuracy: 0.830 - ETA: 9s - loss: 0.5414 - accuracy: 0.83 - ETA: 8s - loss: 0.5446 - accuracy: 0.83 - ETA: 7s - loss: 0.5463 - accuracy: 0.82 - ETA: 7s - loss: 0.5455 - accuracy: 0.83 - ETA: 6s - loss: 0.5484 - accuracy: 0.82 - ETA: 5s - loss: 0.5479 - accuracy: 0.82 - ETA: 5s - loss: 0.5490 - accuracy: 0.82 - ETA: 4s - loss: 0.5490 - accuracy: 0.82 - ETA: 3s - loss: 0.5503 - accuracy: 0.82 - ETA: 3s - loss: 0.5503 - accuracy: 0.82 - ETA: 2s - loss: 0.5506 - accuracy: 0.82 - ETA: 1s - loss: 0.5504 - accuracy: 0.82 - ETA: 1s - loss: 0.5511 - accuracy: 0.82 - ETA: 0s - loss: 0.5515 - accuracy: 0.82 - 75s 6ms/step - loss: 0.5513 - accuracy: 0.8278 - val_loss: 2.8293 - val_accuracy: 0.3453\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:10 - loss: 0.5869 - accuracy: 0.82 - ETA: 1:07 - loss: 0.5852 - accuracy: 0.85 - ETA: 1:06 - loss: 0.5449 - accuracy: 0.84 - ETA: 1:04 - loss: 0.5511 - accuracy: 0.84 - ETA: 1:03 - loss: 0.5548 - accuracy: 0.84 - ETA: 1:02 - loss: 0.5374 - accuracy: 0.84 - ETA: 1:01 - loss: 0.5394 - accuracy: 0.84 - ETA: 1:01 - loss: 0.5393 - accuracy: 0.84 - ETA: 1:00 - loss: 0.5296 - accuracy: 0.84 - ETA: 1:00 - loss: 0.5360 - accuracy: 0.84 - ETA: 1:00 - loss: 0.5424 - accuracy: 0.84 - ETA: 59s - loss: 0.5555 - accuracy: 0.8340 - ETA: 58s - loss: 0.5493 - accuracy: 0.835 - ETA: 58s - loss: 0.5572 - accuracy: 0.831 - ETA: 57s - loss: 0.5496 - accuracy: 0.835 - ETA: 57s - loss: 0.5500 - accuracy: 0.835 - ETA: 56s - loss: 0.5575 - accuracy: 0.835 - ETA: 55s - loss: 0.5502 - accuracy: 0.836 - ETA: 54s - loss: 0.5506 - accuracy: 0.836 - ETA: 54s - loss: 0.5563 - accuracy: 0.833 - ETA: 53s - loss: 0.5645 - accuracy: 0.832 - ETA: 52s - loss: 0.5602 - accuracy: 0.833 - ETA: 52s - loss: 0.5715 - accuracy: 0.831 - ETA: 51s - loss: 0.5777 - accuracy: 0.829 - ETA: 51s - loss: 0.5742 - accuracy: 0.828 - ETA: 50s - loss: 0.5732 - accuracy: 0.829 - ETA: 49s - loss: 0.5729 - accuracy: 0.828 - ETA: 49s - loss: 0.5706 - accuracy: 0.828 - ETA: 48s - loss: 0.5770 - accuracy: 0.826 - ETA: 47s - loss: 0.5746 - accuracy: 0.826 - ETA: 47s - loss: 0.5686 - accuracy: 0.828 - ETA: 46s - loss: 0.5661 - accuracy: 0.829 - ETA: 45s - loss: 0.5650 - accuracy: 0.828 - ETA: 45s - loss: 0.5685 - accuracy: 0.827 - ETA: 44s - loss: 0.5673 - accuracy: 0.827 - ETA: 43s - loss: 0.5666 - accuracy: 0.827 - ETA: 43s - loss: 0.5631 - accuracy: 0.828 - ETA: 42s - loss: 0.5618 - accuracy: 0.828 - ETA: 41s - loss: 0.5604 - accuracy: 0.828 - ETA: 41s - loss: 0.5604 - accuracy: 0.828 - ETA: 40s - loss: 0.5589 - accuracy: 0.829 - ETA: 39s - loss: 0.5575 - accuracy: 0.829 - ETA: 38s - loss: 0.5565 - accuracy: 0.828 - ETA: 38s - loss: 0.5567 - accuracy: 0.828 - ETA: 37s - loss: 0.5600 - accuracy: 0.826 - ETA: 36s - loss: 0.5606 - accuracy: 0.826 - ETA: 36s - loss: 0.5649 - accuracy: 0.824 - ETA: 35s - loss: 0.5631 - accuracy: 0.825 - ETA: 34s - loss: 0.5628 - accuracy: 0.825 - ETA: 34s - loss: 0.5602 - accuracy: 0.825 - ETA: 33s - loss: 0.5583 - accuracy: 0.826 - ETA: 32s - loss: 0.5577 - accuracy: 0.826 - ETA: 32s - loss: 0.5546 - accuracy: 0.826 - ETA: 31s - loss: 0.5514 - accuracy: 0.827 - ETA: 30s - loss: 0.5479 - accuracy: 0.828 - ETA: 30s - loss: 0.5452 - accuracy: 0.829 - ETA: 29s - loss: 0.5431 - accuracy: 0.829 - ETA: 28s - loss: 0.5424 - accuracy: 0.830 - ETA: 28s - loss: 0.5432 - accuracy: 0.830 - ETA: 27s - loss: 0.5424 - accuracy: 0.830 - ETA: 26s - loss: 0.5416 - accuracy: 0.830 - ETA: 26s - loss: 0.5417 - accuracy: 0.829 - ETA: 25s - loss: 0.5405 - accuracy: 0.830 - ETA: 24s - loss: 0.5411 - accuracy: 0.830 - ETA: 24s - loss: 0.5420 - accuracy: 0.830 - ETA: 23s - loss: 0.5418 - accuracy: 0.830 - ETA: 22s - loss: 0.5466 - accuracy: 0.828 - ETA: 22s - loss: 0.5453 - accuracy: 0.828 - ETA: 21s - loss: 0.5452 - accuracy: 0.828 - ETA: 20s - loss: 0.5462 - accuracy: 0.828 - ETA: 20s - loss: 0.5457 - accuracy: 0.828 - ETA: 19s - loss: 0.5467 - accuracy: 0.827 - ETA: 18s - loss: 0.5457 - accuracy: 0.827 - ETA: 18s - loss: 0.5472 - accuracy: 0.827 - ETA: 17s - loss: 0.5455 - accuracy: 0.828 - ETA: 16s - loss: 0.5463 - accuracy: 0.828 - ETA: 16s - loss: 0.5474 - accuracy: 0.828 - ETA: 15s - loss: 0.5457 - accuracy: 0.828 - ETA: 15s - loss: 0.5448 - accuracy: 0.828 - ETA: 14s - loss: 0.5464 - accuracy: 0.827 - ETA: 13s - loss: 0.5478 - accuracy: 0.827 - ETA: 13s - loss: 0.5484 - accuracy: 0.827 - ETA: 12s - loss: 0.5508 - accuracy: 0.827 - ETA: 11s - loss: 0.5488 - accuracy: 0.827 - ETA: 11s - loss: 0.5491 - accuracy: 0.827 - ETA: 10s - loss: 0.5502 - accuracy: 0.826 - ETA: 9s - loss: 0.5505 - accuracy: 0.826 - ETA: 9s - loss: 0.5513 - accuracy: 0.82 - ETA: 8s - loss: 0.5509 - accuracy: 0.82 - ETA: 7s - loss: 0.5506 - accuracy: 0.82 - ETA: 7s - loss: 0.5498 - accuracy: 0.82 - ETA: 6s - loss: 0.5510 - accuracy: 0.82 - ETA: 5s - loss: 0.5523 - accuracy: 0.82 - ETA: 5s - loss: 0.5520 - accuracy: 0.82 - ETA: 4s - loss: 0.5530 - accuracy: 0.82 - ETA: 3s - loss: 0.5552 - accuracy: 0.82 - ETA: 3s - loss: 0.5547 - accuracy: 0.82 - ETA: 2s - loss: 0.5568 - accuracy: 0.82 - ETA: 1s - loss: 0.5575 - accuracy: 0.82 - ETA: 1s - loss: 0.5564 - accuracy: 0.82 - ETA: 0s - loss: 0.5557 - accuracy: 0.82 - 75s 6ms/step - loss: 0.5554 - accuracy: 0.8261 - val_loss: 2.6663 - val_accuracy: 0.3670\n",
      "Epoch 38/100\n",
      "13022/13022 [==============================] - ETA: 1:10 - loss: 0.4956 - accuracy: 0.84 - ETA: 1:07 - loss: 0.5593 - accuracy: 0.82 - ETA: 1:05 - loss: 0.5307 - accuracy: 0.83 - ETA: 1:04 - loss: 0.5177 - accuracy: 0.83 - ETA: 1:03 - loss: 0.5059 - accuracy: 0.84 - ETA: 1:03 - loss: 0.5266 - accuracy: 0.83 - ETA: 1:02 - loss: 0.5629 - accuracy: 0.82 - ETA: 1:02 - loss: 0.5569 - accuracy: 0.82 - ETA: 1:03 - loss: 0.5613 - accuracy: 0.82 - ETA: 1:02 - loss: 0.5636 - accuracy: 0.82 - ETA: 1:01 - loss: 0.5655 - accuracy: 0.83 - ETA: 1:00 - loss: 0.5649 - accuracy: 0.83 - ETA: 59s - loss: 0.5611 - accuracy: 0.8335 - ETA: 58s - loss: 0.5539 - accuracy: 0.833 - ETA: 57s - loss: 0.5551 - accuracy: 0.831 - ETA: 57s - loss: 0.5508 - accuracy: 0.832 - ETA: 56s - loss: 0.5503 - accuracy: 0.832 - ETA: 55s - loss: 0.5471 - accuracy: 0.832 - ETA: 54s - loss: 0.5425 - accuracy: 0.833 - ETA: 53s - loss: 0.5476 - accuracy: 0.830 - ETA: 53s - loss: 0.5473 - accuracy: 0.830 - ETA: 52s - loss: 0.5499 - accuracy: 0.828 - ETA: 51s - loss: 0.5456 - accuracy: 0.829 - ETA: 51s - loss: 0.5496 - accuracy: 0.829 - ETA: 50s - loss: 0.5446 - accuracy: 0.828 - ETA: 49s - loss: 0.5428 - accuracy: 0.829 - ETA: 49s - loss: 0.5419 - accuracy: 0.829 - ETA: 48s - loss: 0.5424 - accuracy: 0.831 - ETA: 47s - loss: 0.5404 - accuracy: 0.830 - ETA: 47s - loss: 0.5380 - accuracy: 0.829 - ETA: 46s - loss: 0.5386 - accuracy: 0.829 - ETA: 46s - loss: 0.5387 - accuracy: 0.828 - ETA: 45s - loss: 0.5359 - accuracy: 0.829 - ETA: 44s - loss: 0.5374 - accuracy: 0.828 - ETA: 43s - loss: 0.5383 - accuracy: 0.827 - ETA: 43s - loss: 0.5405 - accuracy: 0.827 - ETA: 42s - loss: 0.5407 - accuracy: 0.828 - ETA: 41s - loss: 0.5423 - accuracy: 0.827 - ETA: 41s - loss: 0.5418 - accuracy: 0.828 - ETA: 40s - loss: 0.5457 - accuracy: 0.827 - ETA: 39s - loss: 0.5448 - accuracy: 0.827 - ETA: 39s - loss: 0.5416 - accuracy: 0.829 - ETA: 38s - loss: 0.5365 - accuracy: 0.831 - ETA: 37s - loss: 0.5321 - accuracy: 0.832 - ETA: 37s - loss: 0.5364 - accuracy: 0.832 - ETA: 36s - loss: 0.5309 - accuracy: 0.833 - ETA: 35s - loss: 0.5316 - accuracy: 0.833 - ETA: 35s - loss: 0.5331 - accuracy: 0.834 - ETA: 34s - loss: 0.5299 - accuracy: 0.834 - ETA: 33s - loss: 0.5287 - accuracy: 0.835 - ETA: 33s - loss: 0.5277 - accuracy: 0.835 - ETA: 32s - loss: 0.5289 - accuracy: 0.834 - ETA: 31s - loss: 0.5271 - accuracy: 0.835 - ETA: 31s - loss: 0.5285 - accuracy: 0.835 - ETA: 30s - loss: 0.5324 - accuracy: 0.834 - ETA: 29s - loss: 0.5307 - accuracy: 0.835 - ETA: 29s - loss: 0.5300 - accuracy: 0.835 - ETA: 28s - loss: 0.5296 - accuracy: 0.836 - ETA: 28s - loss: 0.5292 - accuracy: 0.836 - ETA: 27s - loss: 0.5305 - accuracy: 0.835 - ETA: 26s - loss: 0.5283 - accuracy: 0.836 - ETA: 26s - loss: 0.5301 - accuracy: 0.836 - ETA: 25s - loss: 0.5293 - accuracy: 0.836 - ETA: 24s - loss: 0.5282 - accuracy: 0.836 - ETA: 24s - loss: 0.5279 - accuracy: 0.836 - ETA: 23s - loss: 0.5275 - accuracy: 0.837 - ETA: 22s - loss: 0.5283 - accuracy: 0.837 - ETA: 22s - loss: 0.5280 - accuracy: 0.837 - ETA: 21s - loss: 0.5290 - accuracy: 0.837 - ETA: 20s - loss: 0.5286 - accuracy: 0.837 - ETA: 20s - loss: 0.5266 - accuracy: 0.838 - ETA: 19s - loss: 0.5265 - accuracy: 0.837 - ETA: 18s - loss: 0.5252 - accuracy: 0.837 - ETA: 18s - loss: 0.5249 - accuracy: 0.838 - ETA: 17s - loss: 0.5242 - accuracy: 0.837 - ETA: 16s - loss: 0.5239 - accuracy: 0.837 - ETA: 16s - loss: 0.5244 - accuracy: 0.837 - ETA: 15s - loss: 0.5246 - accuracy: 0.837 - ETA: 14s - loss: 0.5259 - accuracy: 0.837 - ETA: 14s - loss: 0.5255 - accuracy: 0.837 - ETA: 13s - loss: 0.5238 - accuracy: 0.837 - ETA: 12s - loss: 0.5216 - accuracy: 0.838 - ETA: 12s - loss: 0.5209 - accuracy: 0.838 - ETA: 11s - loss: 0.5213 - accuracy: 0.838 - ETA: 10s - loss: 0.5208 - accuracy: 0.838 - ETA: 10s - loss: 0.5197 - accuracy: 0.838 - ETA: 9s - loss: 0.5215 - accuracy: 0.837 - ETA: 9s - loss: 0.5216 - accuracy: 0.83 - ETA: 8s - loss: 0.5223 - accuracy: 0.83 - ETA: 7s - loss: 0.5201 - accuracy: 0.83 - ETA: 7s - loss: 0.5202 - accuracy: 0.83 - ETA: 6s - loss: 0.5198 - accuracy: 0.83 - ETA: 5s - loss: 0.5197 - accuracy: 0.83 - ETA: 5s - loss: 0.5188 - accuracy: 0.83 - ETA: 4s - loss: 0.5184 - accuracy: 0.83 - ETA: 3s - loss: 0.5185 - accuracy: 0.83 - ETA: 3s - loss: 0.5179 - accuracy: 0.83 - ETA: 2s - loss: 0.5194 - accuracy: 0.83 - ETA: 1s - loss: 0.5199 - accuracy: 0.83 - ETA: 1s - loss: 0.5195 - accuracy: 0.83 - ETA: 0s - loss: 0.5195 - accuracy: 0.83 - 75s 6ms/step - loss: 0.5191 - accuracy: 0.8373 - val_loss: 2.9912 - val_accuracy: 0.3545\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:10 - loss: 0.4223 - accuracy: 0.84 - ETA: 1:06 - loss: 0.4602 - accuracy: 0.83 - ETA: 1:04 - loss: 0.5041 - accuracy: 0.83 - ETA: 1:03 - loss: 0.5397 - accuracy: 0.83 - ETA: 1:02 - loss: 0.5451 - accuracy: 0.83 - ETA: 1:02 - loss: 0.5519 - accuracy: 0.83 - ETA: 1:01 - loss: 0.5487 - accuracy: 0.83 - ETA: 1:00 - loss: 0.5478 - accuracy: 0.83 - ETA: 1:00 - loss: 0.5376 - accuracy: 0.84 - ETA: 59s - loss: 0.5364 - accuracy: 0.8391 - ETA: 58s - loss: 0.5463 - accuracy: 0.837 - ETA: 58s - loss: 0.5603 - accuracy: 0.833 - ETA: 57s - loss: 0.5521 - accuracy: 0.834 - ETA: 57s - loss: 0.5533 - accuracy: 0.832 - ETA: 56s - loss: 0.5526 - accuracy: 0.834 - ETA: 56s - loss: 0.5617 - accuracy: 0.831 - ETA: 55s - loss: 0.5564 - accuracy: 0.834 - ETA: 55s - loss: 0.5649 - accuracy: 0.832 - ETA: 54s - loss: 0.5563 - accuracy: 0.834 - ETA: 53s - loss: 0.5629 - accuracy: 0.831 - ETA: 53s - loss: 0.5760 - accuracy: 0.828 - ETA: 52s - loss: 0.5709 - accuracy: 0.829 - ETA: 51s - loss: 0.5669 - accuracy: 0.831 - ETA: 51s - loss: 0.5645 - accuracy: 0.832 - ETA: 50s - loss: 0.5679 - accuracy: 0.832 - ETA: 49s - loss: 0.5679 - accuracy: 0.830 - ETA: 48s - loss: 0.5731 - accuracy: 0.829 - ETA: 48s - loss: 0.5674 - accuracy: 0.830 - ETA: 47s - loss: 0.5657 - accuracy: 0.831 - ETA: 46s - loss: 0.5642 - accuracy: 0.831 - ETA: 46s - loss: 0.5645 - accuracy: 0.830 - ETA: 45s - loss: 0.5594 - accuracy: 0.832 - ETA: 44s - loss: 0.5622 - accuracy: 0.832 - ETA: 44s - loss: 0.5612 - accuracy: 0.832 - ETA: 43s - loss: 0.5618 - accuracy: 0.832 - ETA: 42s - loss: 0.5577 - accuracy: 0.834 - ETA: 42s - loss: 0.5587 - accuracy: 0.833 - ETA: 41s - loss: 0.5596 - accuracy: 0.831 - ETA: 41s - loss: 0.5568 - accuracy: 0.832 - ETA: 40s - loss: 0.5538 - accuracy: 0.832 - ETA: 39s - loss: 0.5582 - accuracy: 0.831 - ETA: 39s - loss: 0.5555 - accuracy: 0.833 - ETA: 38s - loss: 0.5620 - accuracy: 0.831 - ETA: 37s - loss: 0.5637 - accuracy: 0.830 - ETA: 37s - loss: 0.5610 - accuracy: 0.831 - ETA: 36s - loss: 0.5601 - accuracy: 0.831 - ETA: 35s - loss: 0.5571 - accuracy: 0.831 - ETA: 35s - loss: 0.5533 - accuracy: 0.832 - ETA: 34s - loss: 0.5545 - accuracy: 0.832 - ETA: 33s - loss: 0.5522 - accuracy: 0.832 - ETA: 33s - loss: 0.5513 - accuracy: 0.833 - ETA: 32s - loss: 0.5511 - accuracy: 0.833 - ETA: 31s - loss: 0.5477 - accuracy: 0.834 - ETA: 31s - loss: 0.5473 - accuracy: 0.834 - ETA: 30s - loss: 0.5474 - accuracy: 0.833 - ETA: 30s - loss: 0.5483 - accuracy: 0.833 - ETA: 29s - loss: 0.5436 - accuracy: 0.834 - ETA: 28s - loss: 0.5436 - accuracy: 0.834 - ETA: 28s - loss: 0.5420 - accuracy: 0.835 - ETA: 27s - loss: 0.5397 - accuracy: 0.835 - ETA: 26s - loss: 0.5398 - accuracy: 0.835 - ETA: 26s - loss: 0.5390 - accuracy: 0.835 - ETA: 25s - loss: 0.5384 - accuracy: 0.835 - ETA: 24s - loss: 0.5397 - accuracy: 0.835 - ETA: 24s - loss: 0.5404 - accuracy: 0.835 - ETA: 23s - loss: 0.5418 - accuracy: 0.835 - ETA: 22s - loss: 0.5413 - accuracy: 0.835 - ETA: 22s - loss: 0.5408 - accuracy: 0.835 - ETA: 21s - loss: 0.5392 - accuracy: 0.836 - ETA: 20s - loss: 0.5413 - accuracy: 0.835 - ETA: 20s - loss: 0.5415 - accuracy: 0.835 - ETA: 19s - loss: 0.5432 - accuracy: 0.835 - ETA: 18s - loss: 0.5428 - accuracy: 0.834 - ETA: 18s - loss: 0.5422 - accuracy: 0.835 - ETA: 17s - loss: 0.5420 - accuracy: 0.836 - ETA: 16s - loss: 0.5410 - accuracy: 0.835 - ETA: 16s - loss: 0.5429 - accuracy: 0.835 - ETA: 15s - loss: 0.5427 - accuracy: 0.835 - ETA: 14s - loss: 0.5439 - accuracy: 0.835 - ETA: 14s - loss: 0.5457 - accuracy: 0.835 - ETA: 13s - loss: 0.5459 - accuracy: 0.835 - ETA: 12s - loss: 0.5445 - accuracy: 0.835 - ETA: 12s - loss: 0.5434 - accuracy: 0.835 - ETA: 11s - loss: 0.5417 - accuracy: 0.835 - ETA: 10s - loss: 0.5415 - accuracy: 0.836 - ETA: 10s - loss: 0.5402 - accuracy: 0.836 - ETA: 9s - loss: 0.5398 - accuracy: 0.836 - ETA: 8s - loss: 0.5395 - accuracy: 0.83 - ETA: 8s - loss: 0.5388 - accuracy: 0.83 - ETA: 7s - loss: 0.5385 - accuracy: 0.83 - ETA: 7s - loss: 0.5390 - accuracy: 0.83 - ETA: 6s - loss: 0.5404 - accuracy: 0.83 - ETA: 5s - loss: 0.5399 - accuracy: 0.83 - ETA: 5s - loss: 0.5396 - accuracy: 0.83 - ETA: 4s - loss: 0.5391 - accuracy: 0.83 - ETA: 3s - loss: 0.5406 - accuracy: 0.83 - ETA: 3s - loss: 0.5408 - accuracy: 0.83 - ETA: 2s - loss: 0.5396 - accuracy: 0.83 - ETA: 1s - loss: 0.5379 - accuracy: 0.83 - ETA: 1s - loss: 0.5377 - accuracy: 0.83 - ETA: 0s - loss: 0.5396 - accuracy: 0.83 - 76s 6ms/step - loss: 0.5400 - accuracy: 0.8367 - val_loss: 2.7733 - val_accuracy: 0.3567\n",
      "Epoch 40/100\n",
      "13022/13022 [==============================] - ETA: 1:02 - loss: 0.6023 - accuracy: 0.85 - ETA: 1:01 - loss: 0.5872 - accuracy: 0.82 - ETA: 1:01 - loss: 0.6043 - accuracy: 0.81 - ETA: 1:00 - loss: 0.6214 - accuracy: 0.80 - ETA: 1:00 - loss: 0.5709 - accuracy: 0.81 - ETA: 59s - loss: 0.5681 - accuracy: 0.8164 - ETA: 58s - loss: 0.5619 - accuracy: 0.817 - ETA: 58s - loss: 0.5536 - accuracy: 0.816 - ETA: 58s - loss: 0.5547 - accuracy: 0.820 - ETA: 57s - loss: 0.5566 - accuracy: 0.818 - ETA: 56s - loss: 0.5661 - accuracy: 0.819 - ETA: 55s - loss: 0.5630 - accuracy: 0.821 - ETA: 55s - loss: 0.5598 - accuracy: 0.824 - ETA: 54s - loss: 0.5593 - accuracy: 0.822 - ETA: 53s - loss: 0.5517 - accuracy: 0.822 - ETA: 52s - loss: 0.5493 - accuracy: 0.820 - ETA: 51s - loss: 0.5550 - accuracy: 0.820 - ETA: 50s - loss: 0.5491 - accuracy: 0.822 - ETA: 49s - loss: 0.5454 - accuracy: 0.822 - ETA: 48s - loss: 0.5411 - accuracy: 0.823 - ETA: 48s - loss: 0.5455 - accuracy: 0.821 - ETA: 47s - loss: 0.5453 - accuracy: 0.822 - ETA: 47s - loss: 0.5446 - accuracy: 0.824 - ETA: 46s - loss: 0.5415 - accuracy: 0.825 - ETA: 46s - loss: 0.5442 - accuracy: 0.823 - ETA: 45s - loss: 0.5359 - accuracy: 0.826 - ETA: 44s - loss: 0.5373 - accuracy: 0.824 - ETA: 44s - loss: 0.5308 - accuracy: 0.827 - ETA: 43s - loss: 0.5338 - accuracy: 0.826 - ETA: 42s - loss: 0.5318 - accuracy: 0.827 - ETA: 42s - loss: 0.5341 - accuracy: 0.826 - ETA: 41s - loss: 0.5339 - accuracy: 0.825 - ETA: 40s - loss: 0.5313 - accuracy: 0.826 - ETA: 40s - loss: 0.5307 - accuracy: 0.827 - ETA: 39s - loss: 0.5241 - accuracy: 0.829 - ETA: 38s - loss: 0.5263 - accuracy: 0.829 - ETA: 38s - loss: 0.5262 - accuracy: 0.830 - ETA: 37s - loss: 0.5269 - accuracy: 0.829 - ETA: 36s - loss: 0.5268 - accuracy: 0.830 - ETA: 36s - loss: 0.5272 - accuracy: 0.831 - ETA: 35s - loss: 0.5265 - accuracy: 0.830 - ETA: 35s - loss: 0.5234 - accuracy: 0.831 - ETA: 34s - loss: 0.5278 - accuracy: 0.830 - ETA: 33s - loss: 0.5259 - accuracy: 0.831 - ETA: 33s - loss: 0.5259 - accuracy: 0.830 - ETA: 32s - loss: 0.5276 - accuracy: 0.830 - ETA: 31s - loss: 0.5242 - accuracy: 0.831 - ETA: 31s - loss: 0.5229 - accuracy: 0.831 - ETA: 30s - loss: 0.5162 - accuracy: 0.833 - ETA: 30s - loss: 0.5157 - accuracy: 0.833 - ETA: 29s - loss: 0.5142 - accuracy: 0.833 - ETA: 29s - loss: 0.5158 - accuracy: 0.833 - ETA: 28s - loss: 0.5160 - accuracy: 0.832 - ETA: 27s - loss: 0.5127 - accuracy: 0.833 - ETA: 27s - loss: 0.5114 - accuracy: 0.833 - ETA: 26s - loss: 0.5138 - accuracy: 0.834 - ETA: 26s - loss: 0.5163 - accuracy: 0.833 - ETA: 25s - loss: 0.5166 - accuracy: 0.833 - ETA: 24s - loss: 0.5149 - accuracy: 0.834 - ETA: 24s - loss: 0.5134 - accuracy: 0.834 - ETA: 23s - loss: 0.5141 - accuracy: 0.834 - ETA: 23s - loss: 0.5121 - accuracy: 0.835 - ETA: 22s - loss: 0.5153 - accuracy: 0.834 - ETA: 22s - loss: 0.5113 - accuracy: 0.835 - ETA: 21s - loss: 0.5096 - accuracy: 0.836 - ETA: 20s - loss: 0.5072 - accuracy: 0.836 - ETA: 20s - loss: 0.5064 - accuracy: 0.836 - ETA: 19s - loss: 0.5062 - accuracy: 0.836 - ETA: 19s - loss: 0.5044 - accuracy: 0.837 - ETA: 18s - loss: 0.5059 - accuracy: 0.836 - ETA: 17s - loss: 0.5077 - accuracy: 0.835 - ETA: 17s - loss: 0.5081 - accuracy: 0.835 - ETA: 16s - loss: 0.5066 - accuracy: 0.836 - ETA: 16s - loss: 0.5054 - accuracy: 0.837 - ETA: 15s - loss: 0.5049 - accuracy: 0.836 - ETA: 14s - loss: 0.5062 - accuracy: 0.836 - ETA: 14s - loss: 0.5052 - accuracy: 0.837 - ETA: 13s - loss: 0.5031 - accuracy: 0.837 - ETA: 13s - loss: 0.5021 - accuracy: 0.838 - ETA: 12s - loss: 0.5040 - accuracy: 0.837 - ETA: 12s - loss: 0.5044 - accuracy: 0.838 - ETA: 11s - loss: 0.5038 - accuracy: 0.838 - ETA: 10s - loss: 0.5035 - accuracy: 0.838 - ETA: 10s - loss: 0.5019 - accuracy: 0.839 - ETA: 9s - loss: 0.5018 - accuracy: 0.838 - ETA: 9s - loss: 0.5024 - accuracy: 0.83 - ETA: 8s - loss: 0.5014 - accuracy: 0.83 - ETA: 7s - loss: 0.5021 - accuracy: 0.83 - ETA: 7s - loss: 0.5041 - accuracy: 0.83 - ETA: 6s - loss: 0.5029 - accuracy: 0.83 - ETA: 6s - loss: 0.5031 - accuracy: 0.83 - ETA: 5s - loss: 0.5028 - accuracy: 0.84 - ETA: 5s - loss: 0.5019 - accuracy: 0.84 - ETA: 4s - loss: 0.5047 - accuracy: 0.83 - ETA: 3s - loss: 0.5056 - accuracy: 0.83 - ETA: 3s - loss: 0.5050 - accuracy: 0.83 - ETA: 2s - loss: 0.5048 - accuracy: 0.84 - ETA: 2s - loss: 0.5048 - accuracy: 0.84 - ETA: 1s - loss: 0.5056 - accuracy: 0.84 - ETA: 1s - loss: 0.5048 - accuracy: 0.84 - ETA: 0s - loss: 0.5057 - accuracy: 0.84 - 66s 5ms/step - loss: 0.5059 - accuracy: 0.8401 - val_loss: 2.7908 - val_accuracy: 0.3516\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:05 - loss: 0.6323 - accuracy: 0.81 - ETA: 1:01 - loss: 0.6065 - accuracy: 0.83 - ETA: 58s - loss: 0.5598 - accuracy: 0.8438 - ETA: 56s - loss: 0.5288 - accuracy: 0.849 - ETA: 55s - loss: 0.5480 - accuracy: 0.837 - ETA: 55s - loss: 0.5515 - accuracy: 0.834 - ETA: 54s - loss: 0.5309 - accuracy: 0.833 - ETA: 53s - loss: 0.5302 - accuracy: 0.835 - ETA: 52s - loss: 0.5324 - accuracy: 0.831 - ETA: 51s - loss: 0.5318 - accuracy: 0.831 - ETA: 51s - loss: 0.5287 - accuracy: 0.833 - ETA: 50s - loss: 0.5130 - accuracy: 0.837 - ETA: 49s - loss: 0.5081 - accuracy: 0.841 - ETA: 49s - loss: 0.4981 - accuracy: 0.844 - ETA: 48s - loss: 0.4971 - accuracy: 0.841 - ETA: 48s - loss: 0.4832 - accuracy: 0.847 - ETA: 47s - loss: 0.4777 - accuracy: 0.847 - ETA: 47s - loss: 0.4732 - accuracy: 0.849 - ETA: 46s - loss: 0.4843 - accuracy: 0.847 - ETA: 46s - loss: 0.4767 - accuracy: 0.850 - ETA: 46s - loss: 0.4813 - accuracy: 0.848 - ETA: 45s - loss: 0.4802 - accuracy: 0.848 - ETA: 45s - loss: 0.4895 - accuracy: 0.845 - ETA: 44s - loss: 0.4840 - accuracy: 0.847 - ETA: 43s - loss: 0.4856 - accuracy: 0.847 - ETA: 43s - loss: 0.4834 - accuracy: 0.846 - ETA: 42s - loss: 0.4835 - accuracy: 0.845 - ETA: 42s - loss: 0.4769 - accuracy: 0.847 - ETA: 41s - loss: 0.4802 - accuracy: 0.847 - ETA: 41s - loss: 0.4851 - accuracy: 0.845 - ETA: 40s - loss: 0.4843 - accuracy: 0.846 - ETA: 39s - loss: 0.4883 - accuracy: 0.844 - ETA: 39s - loss: 0.4836 - accuracy: 0.845 - ETA: 38s - loss: 0.4813 - accuracy: 0.845 - ETA: 37s - loss: 0.4824 - accuracy: 0.845 - ETA: 37s - loss: 0.4842 - accuracy: 0.844 - ETA: 36s - loss: 0.4843 - accuracy: 0.844 - ETA: 36s - loss: 0.4859 - accuracy: 0.845 - ETA: 35s - loss: 0.4824 - accuracy: 0.846 - ETA: 34s - loss: 0.4822 - accuracy: 0.846 - ETA: 34s - loss: 0.4805 - accuracy: 0.847 - ETA: 33s - loss: 0.4826 - accuracy: 0.846 - ETA: 33s - loss: 0.4841 - accuracy: 0.845 - ETA: 32s - loss: 0.4850 - accuracy: 0.845 - ETA: 32s - loss: 0.4874 - accuracy: 0.844 - ETA: 31s - loss: 0.4855 - accuracy: 0.845 - ETA: 30s - loss: 0.4832 - accuracy: 0.845 - ETA: 30s - loss: 0.4838 - accuracy: 0.845 - ETA: 29s - loss: 0.4853 - accuracy: 0.845 - ETA: 29s - loss: 0.4854 - accuracy: 0.845 - ETA: 28s - loss: 0.4821 - accuracy: 0.846 - ETA: 28s - loss: 0.4783 - accuracy: 0.847 - ETA: 27s - loss: 0.4788 - accuracy: 0.847 - ETA: 27s - loss: 0.4783 - accuracy: 0.847 - ETA: 26s - loss: 0.4803 - accuracy: 0.847 - ETA: 26s - loss: 0.4819 - accuracy: 0.847 - ETA: 25s - loss: 0.4833 - accuracy: 0.846 - ETA: 24s - loss: 0.4827 - accuracy: 0.846 - ETA: 24s - loss: 0.4831 - accuracy: 0.846 - ETA: 23s - loss: 0.4852 - accuracy: 0.846 - ETA: 23s - loss: 0.4874 - accuracy: 0.846 - ETA: 22s - loss: 0.4883 - accuracy: 0.846 - ETA: 22s - loss: 0.4876 - accuracy: 0.846 - ETA: 21s - loss: 0.4875 - accuracy: 0.846 - ETA: 20s - loss: 0.4867 - accuracy: 0.846 - ETA: 20s - loss: 0.4862 - accuracy: 0.846 - ETA: 19s - loss: 0.4851 - accuracy: 0.847 - ETA: 19s - loss: 0.4846 - accuracy: 0.846 - ETA: 18s - loss: 0.4841 - accuracy: 0.847 - ETA: 18s - loss: 0.4833 - accuracy: 0.847 - ETA: 17s - loss: 0.4844 - accuracy: 0.846 - ETA: 16s - loss: 0.4835 - accuracy: 0.846 - ETA: 16s - loss: 0.4831 - accuracy: 0.847 - ETA: 15s - loss: 0.4817 - accuracy: 0.847 - ETA: 15s - loss: 0.4805 - accuracy: 0.848 - ETA: 14s - loss: 0.4826 - accuracy: 0.847 - ETA: 14s - loss: 0.4822 - accuracy: 0.847 - ETA: 13s - loss: 0.4816 - accuracy: 0.848 - ETA: 12s - loss: 0.4817 - accuracy: 0.848 - ETA: 12s - loss: 0.4819 - accuracy: 0.848 - ETA: 11s - loss: 0.4827 - accuracy: 0.848 - ETA: 11s - loss: 0.4832 - accuracy: 0.848 - ETA: 10s - loss: 0.4843 - accuracy: 0.847 - ETA: 10s - loss: 0.4834 - accuracy: 0.848 - ETA: 9s - loss: 0.4856 - accuracy: 0.847 - ETA: 8s - loss: 0.4851 - accuracy: 0.84 - ETA: 8s - loss: 0.4856 - accuracy: 0.84 - ETA: 7s - loss: 0.4852 - accuracy: 0.84 - ETA: 7s - loss: 0.4844 - accuracy: 0.84 - ETA: 6s - loss: 0.4841 - accuracy: 0.84 - ETA: 6s - loss: 0.4841 - accuracy: 0.84 - ETA: 5s - loss: 0.4839 - accuracy: 0.84 - ETA: 4s - loss: 0.4856 - accuracy: 0.84 - ETA: 4s - loss: 0.4850 - accuracy: 0.84 - ETA: 3s - loss: 0.4861 - accuracy: 0.84 - ETA: 3s - loss: 0.4853 - accuracy: 0.84 - ETA: 2s - loss: 0.4856 - accuracy: 0.84 - ETA: 2s - loss: 0.4864 - accuracy: 0.84 - ETA: 1s - loss: 0.4871 - accuracy: 0.84 - ETA: 0s - loss: 0.4854 - accuracy: 0.84 - ETA: 0s - loss: 0.4854 - accuracy: 0.84 - 65s 5ms/step - loss: 0.4872 - accuracy: 0.8469 - val_loss: 2.9477 - val_accuracy: 0.3420\n",
      "Epoch 42/100\n",
      "13022/13022 [==============================] - ETA: 1:01 - loss: 0.5902 - accuracy: 0.82 - ETA: 58s - loss: 0.4749 - accuracy: 0.8516 - ETA: 56s - loss: 0.4844 - accuracy: 0.843 - ETA: 55s - loss: 0.4777 - accuracy: 0.853 - ETA: 54s - loss: 0.4826 - accuracy: 0.856 - ETA: 54s - loss: 0.4849 - accuracy: 0.859 - ETA: 53s - loss: 0.4808 - accuracy: 0.860 - ETA: 52s - loss: 0.4869 - accuracy: 0.854 - ETA: 51s - loss: 0.4862 - accuracy: 0.855 - ETA: 51s - loss: 0.4914 - accuracy: 0.851 - ETA: 50s - loss: 0.4830 - accuracy: 0.851 - ETA: 49s - loss: 0.4723 - accuracy: 0.854 - ETA: 49s - loss: 0.4903 - accuracy: 0.849 - ETA: 48s - loss: 0.4753 - accuracy: 0.853 - ETA: 48s - loss: 0.4819 - accuracy: 0.852 - ETA: 47s - loss: 0.4841 - accuracy: 0.852 - ETA: 47s - loss: 0.4852 - accuracy: 0.852 - ETA: 47s - loss: 0.4835 - accuracy: 0.852 - ETA: 46s - loss: 0.4901 - accuracy: 0.851 - ETA: 46s - loss: 0.4959 - accuracy: 0.850 - ETA: 45s - loss: 0.4926 - accuracy: 0.851 - ETA: 45s - loss: 0.4882 - accuracy: 0.851 - ETA: 44s - loss: 0.4835 - accuracy: 0.854 - ETA: 44s - loss: 0.4852 - accuracy: 0.852 - ETA: 43s - loss: 0.4855 - accuracy: 0.852 - ETA: 43s - loss: 0.4897 - accuracy: 0.851 - ETA: 42s - loss: 0.4816 - accuracy: 0.853 - ETA: 42s - loss: 0.4792 - accuracy: 0.853 - ETA: 41s - loss: 0.4790 - accuracy: 0.853 - ETA: 41s - loss: 0.4795 - accuracy: 0.852 - ETA: 40s - loss: 0.4789 - accuracy: 0.852 - ETA: 39s - loss: 0.4851 - accuracy: 0.850 - ETA: 39s - loss: 0.4851 - accuracy: 0.850 - ETA: 38s - loss: 0.4798 - accuracy: 0.852 - ETA: 38s - loss: 0.4812 - accuracy: 0.852 - ETA: 37s - loss: 0.4832 - accuracy: 0.852 - ETA: 36s - loss: 0.4812 - accuracy: 0.853 - ETA: 36s - loss: 0.4770 - accuracy: 0.854 - ETA: 35s - loss: 0.4750 - accuracy: 0.855 - ETA: 35s - loss: 0.4796 - accuracy: 0.852 - ETA: 34s - loss: 0.4804 - accuracy: 0.852 - ETA: 34s - loss: 0.4808 - accuracy: 0.851 - ETA: 33s - loss: 0.4776 - accuracy: 0.853 - ETA: 33s - loss: 0.4775 - accuracy: 0.853 - ETA: 32s - loss: 0.4766 - accuracy: 0.853 - ETA: 32s - loss: 0.4784 - accuracy: 0.852 - ETA: 31s - loss: 0.4794 - accuracy: 0.852 - ETA: 31s - loss: 0.4794 - accuracy: 0.852 - ETA: 30s - loss: 0.4804 - accuracy: 0.851 - ETA: 29s - loss: 0.4799 - accuracy: 0.852 - ETA: 29s - loss: 0.4787 - accuracy: 0.852 - ETA: 28s - loss: 0.4770 - accuracy: 0.851 - ETA: 28s - loss: 0.4766 - accuracy: 0.851 - ETA: 27s - loss: 0.4798 - accuracy: 0.850 - ETA: 26s - loss: 0.4798 - accuracy: 0.850 - ETA: 26s - loss: 0.4804 - accuracy: 0.850 - ETA: 25s - loss: 0.4813 - accuracy: 0.850 - ETA: 25s - loss: 0.4816 - accuracy: 0.849 - ETA: 24s - loss: 0.4819 - accuracy: 0.850 - ETA: 23s - loss: 0.4800 - accuracy: 0.851 - ETA: 23s - loss: 0.4827 - accuracy: 0.850 - ETA: 22s - loss: 0.4840 - accuracy: 0.850 - ETA: 22s - loss: 0.4869 - accuracy: 0.849 - ETA: 21s - loss: 0.4859 - accuracy: 0.849 - ETA: 21s - loss: 0.4833 - accuracy: 0.850 - ETA: 20s - loss: 0.4830 - accuracy: 0.850 - ETA: 19s - loss: 0.4821 - accuracy: 0.850 - ETA: 19s - loss: 0.4830 - accuracy: 0.850 - ETA: 18s - loss: 0.4832 - accuracy: 0.849 - ETA: 18s - loss: 0.4828 - accuracy: 0.849 - ETA: 17s - loss: 0.4832 - accuracy: 0.849 - ETA: 17s - loss: 0.4838 - accuracy: 0.849 - ETA: 16s - loss: 0.4836 - accuracy: 0.849 - ETA: 15s - loss: 0.4816 - accuracy: 0.849 - ETA: 15s - loss: 0.4818 - accuracy: 0.849 - ETA: 14s - loss: 0.4810 - accuracy: 0.849 - ETA: 14s - loss: 0.4796 - accuracy: 0.850 - ETA: 13s - loss: 0.4810 - accuracy: 0.850 - ETA: 13s - loss: 0.4819 - accuracy: 0.849 - ETA: 12s - loss: 0.4817 - accuracy: 0.850 - ETA: 11s - loss: 0.4805 - accuracy: 0.850 - ETA: 11s - loss: 0.4789 - accuracy: 0.851 - ETA: 10s - loss: 0.4800 - accuracy: 0.851 - ETA: 10s - loss: 0.4806 - accuracy: 0.850 - ETA: 9s - loss: 0.4794 - accuracy: 0.851 - ETA: 8s - loss: 0.4811 - accuracy: 0.85 - ETA: 8s - loss: 0.4793 - accuracy: 0.85 - ETA: 7s - loss: 0.4802 - accuracy: 0.85 - ETA: 7s - loss: 0.4807 - accuracy: 0.85 - ETA: 6s - loss: 0.4806 - accuracy: 0.85 - ETA: 6s - loss: 0.4822 - accuracy: 0.85 - ETA: 5s - loss: 0.4830 - accuracy: 0.85 - ETA: 4s - loss: 0.4828 - accuracy: 0.85 - ETA: 4s - loss: 0.4829 - accuracy: 0.85 - ETA: 3s - loss: 0.4828 - accuracy: 0.85 - ETA: 3s - loss: 0.4847 - accuracy: 0.85 - ETA: 2s - loss: 0.4856 - accuracy: 0.84 - ETA: 2s - loss: 0.4849 - accuracy: 0.84 - ETA: 1s - loss: 0.4841 - accuracy: 0.84 - ETA: 0s - loss: 0.4843 - accuracy: 0.84 - ETA: 0s - loss: 0.4840 - accuracy: 0.84 - 65s 5ms/step - loss: 0.4846 - accuracy: 0.8496 - val_loss: 2.7756 - val_accuracy: 0.3687\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:01 - loss: 0.7461 - accuracy: 0.76 - ETA: 59s - loss: 0.6475 - accuracy: 0.8008 - ETA: 56s - loss: 0.6399 - accuracy: 0.791 - ETA: 55s - loss: 0.6203 - accuracy: 0.806 - ETA: 55s - loss: 0.5902 - accuracy: 0.818 - ETA: 54s - loss: 0.5591 - accuracy: 0.829 - ETA: 53s - loss: 0.5482 - accuracy: 0.827 - ETA: 53s - loss: 0.5356 - accuracy: 0.835 - ETA: 52s - loss: 0.5225 - accuracy: 0.840 - ETA: 51s - loss: 0.5180 - accuracy: 0.842 - ETA: 51s - loss: 0.5018 - accuracy: 0.847 - ETA: 50s - loss: 0.5056 - accuracy: 0.846 - ETA: 49s - loss: 0.5110 - accuracy: 0.848 - ETA: 49s - loss: 0.5165 - accuracy: 0.844 - ETA: 49s - loss: 0.5045 - accuracy: 0.847 - ETA: 49s - loss: 0.4995 - accuracy: 0.851 - ETA: 48s - loss: 0.4934 - accuracy: 0.852 - ETA: 47s - loss: 0.5013 - accuracy: 0.852 - ETA: 47s - loss: 0.5072 - accuracy: 0.850 - ETA: 46s - loss: 0.5067 - accuracy: 0.849 - ETA: 45s - loss: 0.5083 - accuracy: 0.849 - ETA: 45s - loss: 0.5080 - accuracy: 0.848 - ETA: 44s - loss: 0.5049 - accuracy: 0.848 - ETA: 44s - loss: 0.4984 - accuracy: 0.849 - ETA: 43s - loss: 0.4990 - accuracy: 0.849 - ETA: 43s - loss: 0.4907 - accuracy: 0.850 - ETA: 42s - loss: 0.4900 - accuracy: 0.850 - ETA: 42s - loss: 0.4911 - accuracy: 0.850 - ETA: 41s - loss: 0.4877 - accuracy: 0.850 - ETA: 41s - loss: 0.4869 - accuracy: 0.852 - ETA: 40s - loss: 0.4884 - accuracy: 0.850 - ETA: 40s - loss: 0.4866 - accuracy: 0.851 - ETA: 40s - loss: 0.4879 - accuracy: 0.851 - ETA: 39s - loss: 0.4891 - accuracy: 0.850 - ETA: 39s - loss: 0.4865 - accuracy: 0.852 - ETA: 38s - loss: 0.4846 - accuracy: 0.853 - ETA: 38s - loss: 0.4805 - accuracy: 0.853 - ETA: 37s - loss: 0.4798 - accuracy: 0.853 - ETA: 37s - loss: 0.4820 - accuracy: 0.853 - ETA: 36s - loss: 0.4822 - accuracy: 0.852 - ETA: 36s - loss: 0.4792 - accuracy: 0.852 - ETA: 35s - loss: 0.4784 - accuracy: 0.853 - ETA: 35s - loss: 0.4786 - accuracy: 0.853 - ETA: 34s - loss: 0.4817 - accuracy: 0.853 - ETA: 33s - loss: 0.4825 - accuracy: 0.853 - ETA: 33s - loss: 0.4819 - accuracy: 0.853 - ETA: 32s - loss: 0.4815 - accuracy: 0.853 - ETA: 32s - loss: 0.4831 - accuracy: 0.853 - ETA: 31s - loss: 0.4806 - accuracy: 0.853 - ETA: 30s - loss: 0.4841 - accuracy: 0.852 - ETA: 30s - loss: 0.4836 - accuracy: 0.852 - ETA: 29s - loss: 0.4863 - accuracy: 0.851 - ETA: 29s - loss: 0.4863 - accuracy: 0.851 - ETA: 28s - loss: 0.4872 - accuracy: 0.850 - ETA: 27s - loss: 0.4883 - accuracy: 0.851 - ETA: 27s - loss: 0.4884 - accuracy: 0.851 - ETA: 26s - loss: 0.4864 - accuracy: 0.851 - ETA: 26s - loss: 0.4851 - accuracy: 0.851 - ETA: 25s - loss: 0.4865 - accuracy: 0.851 - ETA: 24s - loss: 0.4870 - accuracy: 0.850 - ETA: 24s - loss: 0.4886 - accuracy: 0.849 - ETA: 23s - loss: 0.4873 - accuracy: 0.850 - ETA: 22s - loss: 0.4893 - accuracy: 0.849 - ETA: 22s - loss: 0.4911 - accuracy: 0.849 - ETA: 21s - loss: 0.4923 - accuracy: 0.848 - ETA: 21s - loss: 0.4911 - accuracy: 0.849 - ETA: 20s - loss: 0.4891 - accuracy: 0.849 - ETA: 20s - loss: 0.4865 - accuracy: 0.850 - ETA: 19s - loss: 0.4856 - accuracy: 0.850 - ETA: 18s - loss: 0.4850 - accuracy: 0.850 - ETA: 18s - loss: 0.4856 - accuracy: 0.850 - ETA: 17s - loss: 0.4899 - accuracy: 0.849 - ETA: 17s - loss: 0.4901 - accuracy: 0.849 - ETA: 16s - loss: 0.4904 - accuracy: 0.849 - ETA: 15s - loss: 0.4911 - accuracy: 0.848 - ETA: 15s - loss: 0.4915 - accuracy: 0.849 - ETA: 14s - loss: 0.4920 - accuracy: 0.849 - ETA: 14s - loss: 0.4904 - accuracy: 0.849 - ETA: 13s - loss: 0.4894 - accuracy: 0.849 - ETA: 12s - loss: 0.4902 - accuracy: 0.849 - ETA: 12s - loss: 0.4897 - accuracy: 0.850 - ETA: 11s - loss: 0.4898 - accuracy: 0.850 - ETA: 11s - loss: 0.4883 - accuracy: 0.850 - ETA: 10s - loss: 0.4875 - accuracy: 0.850 - ETA: 9s - loss: 0.4869 - accuracy: 0.851 - ETA: 9s - loss: 0.4872 - accuracy: 0.85 - ETA: 8s - loss: 0.4875 - accuracy: 0.85 - ETA: 8s - loss: 0.4855 - accuracy: 0.85 - ETA: 7s - loss: 0.4849 - accuracy: 0.85 - ETA: 6s - loss: 0.4841 - accuracy: 0.85 - ETA: 6s - loss: 0.4846 - accuracy: 0.85 - ETA: 5s - loss: 0.4849 - accuracy: 0.85 - ETA: 5s - loss: 0.4853 - accuracy: 0.85 - ETA: 4s - loss: 0.4870 - accuracy: 0.85 - ETA: 3s - loss: 0.4873 - accuracy: 0.85 - ETA: 3s - loss: 0.4873 - accuracy: 0.85 - ETA: 2s - loss: 0.4879 - accuracy: 0.85 - ETA: 2s - loss: 0.4882 - accuracy: 0.85 - ETA: 1s - loss: 0.4882 - accuracy: 0.85 - ETA: 1s - loss: 0.4880 - accuracy: 0.85 - ETA: 0s - loss: 0.4901 - accuracy: 0.85 - 67s 5ms/step - loss: 0.4905 - accuracy: 0.8498 - val_loss: 2.7754 - val_accuracy: 0.3669\n",
      "Epoch 44/100\n",
      "13022/13022 [==============================] - ETA: 1:02 - loss: 0.6378 - accuracy: 0.83 - ETA: 1:00 - loss: 0.6438 - accuracy: 0.83 - ETA: 58s - loss: 0.5351 - accuracy: 0.8646 - ETA: 57s - loss: 0.5136 - accuracy: 0.867 - ETA: 56s - loss: 0.5328 - accuracy: 0.854 - ETA: 55s - loss: 0.5178 - accuracy: 0.851 - ETA: 54s - loss: 0.5005 - accuracy: 0.854 - ETA: 54s - loss: 0.4957 - accuracy: 0.851 - ETA: 54s - loss: 0.4940 - accuracy: 0.855 - ETA: 54s - loss: 0.5129 - accuracy: 0.851 - ETA: 53s - loss: 0.5181 - accuracy: 0.848 - ETA: 53s - loss: 0.5126 - accuracy: 0.849 - ETA: 52s - loss: 0.5129 - accuracy: 0.846 - ETA: 51s - loss: 0.5137 - accuracy: 0.848 - ETA: 51s - loss: 0.5297 - accuracy: 0.842 - ETA: 50s - loss: 0.5313 - accuracy: 0.842 - ETA: 49s - loss: 0.5318 - accuracy: 0.842 - ETA: 49s - loss: 0.5367 - accuracy: 0.842 - ETA: 48s - loss: 0.5305 - accuracy: 0.843 - ETA: 48s - loss: 0.5291 - accuracy: 0.846 - ETA: 47s - loss: 0.5278 - accuracy: 0.846 - ETA: 47s - loss: 0.5304 - accuracy: 0.843 - ETA: 46s - loss: 0.5220 - accuracy: 0.844 - ETA: 46s - loss: 0.5187 - accuracy: 0.844 - ETA: 45s - loss: 0.5215 - accuracy: 0.844 - ETA: 44s - loss: 0.5188 - accuracy: 0.843 - ETA: 44s - loss: 0.5157 - accuracy: 0.844 - ETA: 43s - loss: 0.5163 - accuracy: 0.843 - ETA: 43s - loss: 0.5159 - accuracy: 0.844 - ETA: 42s - loss: 0.5134 - accuracy: 0.845 - ETA: 41s - loss: 0.5136 - accuracy: 0.844 - ETA: 41s - loss: 0.5142 - accuracy: 0.845 - ETA: 40s - loss: 0.5121 - accuracy: 0.845 - ETA: 39s - loss: 0.5114 - accuracy: 0.844 - ETA: 39s - loss: 0.5104 - accuracy: 0.845 - ETA: 38s - loss: 0.5053 - accuracy: 0.847 - ETA: 38s - loss: 0.5019 - accuracy: 0.848 - ETA: 37s - loss: 0.5005 - accuracy: 0.849 - ETA: 37s - loss: 0.4986 - accuracy: 0.851 - ETA: 36s - loss: 0.4948 - accuracy: 0.852 - ETA: 35s - loss: 0.4964 - accuracy: 0.852 - ETA: 35s - loss: 0.4924 - accuracy: 0.853 - ETA: 34s - loss: 0.4904 - accuracy: 0.853 - ETA: 34s - loss: 0.4917 - accuracy: 0.853 - ETA: 33s - loss: 0.4896 - accuracy: 0.854 - ETA: 32s - loss: 0.4921 - accuracy: 0.854 - ETA: 32s - loss: 0.4957 - accuracy: 0.852 - ETA: 31s - loss: 0.4944 - accuracy: 0.852 - ETA: 31s - loss: 0.4928 - accuracy: 0.853 - ETA: 30s - loss: 0.4921 - accuracy: 0.852 - ETA: 29s - loss: 0.4951 - accuracy: 0.852 - ETA: 29s - loss: 0.4958 - accuracy: 0.852 - ETA: 28s - loss: 0.4962 - accuracy: 0.851 - ETA: 28s - loss: 0.4959 - accuracy: 0.851 - ETA: 27s - loss: 0.4942 - accuracy: 0.851 - ETA: 26s - loss: 0.4916 - accuracy: 0.852 - ETA: 26s - loss: 0.4908 - accuracy: 0.852 - ETA: 25s - loss: 0.4893 - accuracy: 0.853 - ETA: 25s - loss: 0.4884 - accuracy: 0.853 - ETA: 24s - loss: 0.4868 - accuracy: 0.854 - ETA: 23s - loss: 0.4874 - accuracy: 0.854 - ETA: 23s - loss: 0.4876 - accuracy: 0.853 - ETA: 22s - loss: 0.4869 - accuracy: 0.853 - ETA: 22s - loss: 0.4861 - accuracy: 0.854 - ETA: 21s - loss: 0.4885 - accuracy: 0.854 - ETA: 21s - loss: 0.4879 - accuracy: 0.853 - ETA: 20s - loss: 0.4866 - accuracy: 0.853 - ETA: 19s - loss: 0.4866 - accuracy: 0.853 - ETA: 19s - loss: 0.4851 - accuracy: 0.853 - ETA: 18s - loss: 0.4851 - accuracy: 0.853 - ETA: 18s - loss: 0.4847 - accuracy: 0.854 - ETA: 17s - loss: 0.4833 - accuracy: 0.854 - ETA: 16s - loss: 0.4856 - accuracy: 0.854 - ETA: 16s - loss: 0.4836 - accuracy: 0.854 - ETA: 15s - loss: 0.4843 - accuracy: 0.854 - ETA: 15s - loss: 0.4832 - accuracy: 0.854 - ETA: 14s - loss: 0.4831 - accuracy: 0.854 - ETA: 13s - loss: 0.4814 - accuracy: 0.855 - ETA: 13s - loss: 0.4817 - accuracy: 0.855 - ETA: 12s - loss: 0.4822 - accuracy: 0.855 - ETA: 12s - loss: 0.4836 - accuracy: 0.854 - ETA: 11s - loss: 0.4840 - accuracy: 0.854 - ETA: 10s - loss: 0.4846 - accuracy: 0.854 - ETA: 10s - loss: 0.4836 - accuracy: 0.854 - ETA: 9s - loss: 0.4811 - accuracy: 0.854 - ETA: 9s - loss: 0.4807 - accuracy: 0.85 - ETA: 8s - loss: 0.4810 - accuracy: 0.85 - ETA: 8s - loss: 0.4807 - accuracy: 0.85 - ETA: 7s - loss: 0.4819 - accuracy: 0.85 - ETA: 6s - loss: 0.4830 - accuracy: 0.85 - ETA: 6s - loss: 0.4828 - accuracy: 0.85 - ETA: 5s - loss: 0.4817 - accuracy: 0.85 - ETA: 5s - loss: 0.4827 - accuracy: 0.85 - ETA: 4s - loss: 0.4829 - accuracy: 0.85 - ETA: 3s - loss: 0.4833 - accuracy: 0.85 - ETA: 3s - loss: 0.4830 - accuracy: 0.85 - ETA: 2s - loss: 0.4838 - accuracy: 0.85 - ETA: 2s - loss: 0.4834 - accuracy: 0.85 - ETA: 1s - loss: 0.4834 - accuracy: 0.85 - ETA: 1s - loss: 0.4836 - accuracy: 0.85 - ETA: 0s - loss: 0.4846 - accuracy: 0.85 - 66s 5ms/step - loss: 0.4846 - accuracy: 0.8533 - val_loss: 2.8904 - val_accuracy: 0.3500\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:01 - loss: 0.5547 - accuracy: 0.84 - ETA: 58s - loss: 0.4996 - accuracy: 0.8672 - ETA: 56s - loss: 0.4296 - accuracy: 0.875 - ETA: 55s - loss: 0.4414 - accuracy: 0.871 - ETA: 56s - loss: 0.4578 - accuracy: 0.864 - ETA: 56s - loss: 0.4465 - accuracy: 0.869 - ETA: 56s - loss: 0.4383 - accuracy: 0.871 - ETA: 55s - loss: 0.4448 - accuracy: 0.869 - ETA: 55s - loss: 0.4438 - accuracy: 0.866 - ETA: 55s - loss: 0.4381 - accuracy: 0.868 - ETA: 54s - loss: 0.4403 - accuracy: 0.863 - ETA: 53s - loss: 0.4354 - accuracy: 0.864 - ETA: 52s - loss: 0.4258 - accuracy: 0.865 - ETA: 51s - loss: 0.4424 - accuracy: 0.861 - ETA: 51s - loss: 0.4373 - accuracy: 0.861 - ETA: 50s - loss: 0.4343 - accuracy: 0.860 - ETA: 49s - loss: 0.4341 - accuracy: 0.860 - ETA: 48s - loss: 0.4352 - accuracy: 0.861 - ETA: 48s - loss: 0.4400 - accuracy: 0.859 - ETA: 47s - loss: 0.4514 - accuracy: 0.859 - ETA: 47s - loss: 0.4472 - accuracy: 0.859 - ETA: 46s - loss: 0.4502 - accuracy: 0.858 - ETA: 45s - loss: 0.4461 - accuracy: 0.860 - ETA: 45s - loss: 0.4610 - accuracy: 0.856 - ETA: 44s - loss: 0.4624 - accuracy: 0.856 - ETA: 43s - loss: 0.4601 - accuracy: 0.857 - ETA: 43s - loss: 0.4571 - accuracy: 0.857 - ETA: 42s - loss: 0.4554 - accuracy: 0.858 - ETA: 42s - loss: 0.4585 - accuracy: 0.858 - ETA: 41s - loss: 0.4576 - accuracy: 0.859 - ETA: 40s - loss: 0.4595 - accuracy: 0.859 - ETA: 40s - loss: 0.4603 - accuracy: 0.859 - ETA: 39s - loss: 0.4596 - accuracy: 0.860 - ETA: 39s - loss: 0.4631 - accuracy: 0.858 - ETA: 38s - loss: 0.4638 - accuracy: 0.858 - ETA: 38s - loss: 0.4683 - accuracy: 0.857 - ETA: 37s - loss: 0.4660 - accuracy: 0.859 - ETA: 36s - loss: 0.4656 - accuracy: 0.858 - ETA: 36s - loss: 0.4656 - accuracy: 0.858 - ETA: 35s - loss: 0.4651 - accuracy: 0.858 - ETA: 35s - loss: 0.4639 - accuracy: 0.858 - ETA: 34s - loss: 0.4633 - accuracy: 0.858 - ETA: 33s - loss: 0.4660 - accuracy: 0.858 - ETA: 33s - loss: 0.4668 - accuracy: 0.858 - ETA: 32s - loss: 0.4685 - accuracy: 0.858 - ETA: 32s - loss: 0.4703 - accuracy: 0.857 - ETA: 31s - loss: 0.4708 - accuracy: 0.856 - ETA: 30s - loss: 0.4708 - accuracy: 0.856 - ETA: 30s - loss: 0.4715 - accuracy: 0.855 - ETA: 29s - loss: 0.4710 - accuracy: 0.855 - ETA: 29s - loss: 0.4696 - accuracy: 0.856 - ETA: 28s - loss: 0.4688 - accuracy: 0.856 - ETA: 28s - loss: 0.4687 - accuracy: 0.856 - ETA: 27s - loss: 0.4687 - accuracy: 0.856 - ETA: 26s - loss: 0.4705 - accuracy: 0.855 - ETA: 26s - loss: 0.4705 - accuracy: 0.855 - ETA: 25s - loss: 0.4698 - accuracy: 0.855 - ETA: 25s - loss: 0.4684 - accuracy: 0.856 - ETA: 24s - loss: 0.4686 - accuracy: 0.856 - ETA: 23s - loss: 0.4681 - accuracy: 0.855 - ETA: 23s - loss: 0.4669 - accuracy: 0.855 - ETA: 22s - loss: 0.4667 - accuracy: 0.855 - ETA: 22s - loss: 0.4695 - accuracy: 0.854 - ETA: 21s - loss: 0.4688 - accuracy: 0.855 - ETA: 21s - loss: 0.4681 - accuracy: 0.854 - ETA: 20s - loss: 0.4684 - accuracy: 0.854 - ETA: 19s - loss: 0.4678 - accuracy: 0.855 - ETA: 19s - loss: 0.4668 - accuracy: 0.855 - ETA: 18s - loss: 0.4686 - accuracy: 0.854 - ETA: 18s - loss: 0.4710 - accuracy: 0.854 - ETA: 17s - loss: 0.4693 - accuracy: 0.854 - ETA: 17s - loss: 0.4693 - accuracy: 0.854 - ETA: 16s - loss: 0.4685 - accuracy: 0.855 - ETA: 15s - loss: 0.4706 - accuracy: 0.854 - ETA: 15s - loss: 0.4686 - accuracy: 0.855 - ETA: 14s - loss: 0.4666 - accuracy: 0.856 - ETA: 14s - loss: 0.4699 - accuracy: 0.855 - ETA: 13s - loss: 0.4707 - accuracy: 0.855 - ETA: 13s - loss: 0.4705 - accuracy: 0.854 - ETA: 12s - loss: 0.4717 - accuracy: 0.854 - ETA: 11s - loss: 0.4701 - accuracy: 0.855 - ETA: 11s - loss: 0.4700 - accuracy: 0.855 - ETA: 10s - loss: 0.4698 - accuracy: 0.856 - ETA: 10s - loss: 0.4698 - accuracy: 0.856 - ETA: 9s - loss: 0.4695 - accuracy: 0.856 - ETA: 9s - loss: 0.4683 - accuracy: 0.85 - ETA: 8s - loss: 0.4690 - accuracy: 0.85 - ETA: 7s - loss: 0.4695 - accuracy: 0.85 - ETA: 7s - loss: 0.4701 - accuracy: 0.85 - ETA: 6s - loss: 0.4706 - accuracy: 0.85 - ETA: 6s - loss: 0.4716 - accuracy: 0.85 - ETA: 5s - loss: 0.4723 - accuracy: 0.85 - ETA: 5s - loss: 0.4731 - accuracy: 0.85 - ETA: 4s - loss: 0.4724 - accuracy: 0.85 - ETA: 3s - loss: 0.4719 - accuracy: 0.85 - ETA: 3s - loss: 0.4708 - accuracy: 0.85 - ETA: 2s - loss: 0.4700 - accuracy: 0.85 - ETA: 2s - loss: 0.4700 - accuracy: 0.85 - ETA: 1s - loss: 0.4693 - accuracy: 0.85 - ETA: 0s - loss: 0.4683 - accuracy: 0.85 - ETA: 0s - loss: 0.4688 - accuracy: 0.85 - 66s 5ms/step - loss: 0.4680 - accuracy: 0.8567 - val_loss: 3.1515 - val_accuracy: 0.3396\n",
      "Epoch 46/100\n",
      "13022/13022 [==============================] - ETA: 1:04 - loss: 0.4691 - accuracy: 0.86 - ETA: 1:02 - loss: 0.4461 - accuracy: 0.85 - ETA: 1:01 - loss: 0.4515 - accuracy: 0.86 - ETA: 59s - loss: 0.4528 - accuracy: 0.8691 - ETA: 58s - loss: 0.4333 - accuracy: 0.879 - ETA: 57s - loss: 0.4559 - accuracy: 0.876 - ETA: 56s - loss: 0.4743 - accuracy: 0.870 - ETA: 55s - loss: 0.4487 - accuracy: 0.876 - ETA: 54s - loss: 0.4622 - accuracy: 0.870 - ETA: 53s - loss: 0.4870 - accuracy: 0.863 - ETA: 53s - loss: 0.4867 - accuracy: 0.862 - ETA: 52s - loss: 0.4724 - accuracy: 0.866 - ETA: 51s - loss: 0.4606 - accuracy: 0.868 - ETA: 50s - loss: 0.4686 - accuracy: 0.866 - ETA: 50s - loss: 0.4604 - accuracy: 0.867 - ETA: 49s - loss: 0.4682 - accuracy: 0.864 - ETA: 48s - loss: 0.4738 - accuracy: 0.863 - ETA: 48s - loss: 0.4717 - accuracy: 0.862 - ETA: 47s - loss: 0.4818 - accuracy: 0.859 - ETA: 46s - loss: 0.4816 - accuracy: 0.861 - ETA: 46s - loss: 0.4825 - accuracy: 0.860 - ETA: 45s - loss: 0.4821 - accuracy: 0.860 - ETA: 45s - loss: 0.4785 - accuracy: 0.862 - ETA: 45s - loss: 0.4776 - accuracy: 0.861 - ETA: 44s - loss: 0.4709 - accuracy: 0.861 - ETA: 43s - loss: 0.4791 - accuracy: 0.860 - ETA: 43s - loss: 0.4788 - accuracy: 0.860 - ETA: 42s - loss: 0.4791 - accuracy: 0.859 - ETA: 42s - loss: 0.4732 - accuracy: 0.861 - ETA: 41s - loss: 0.4799 - accuracy: 0.860 - ETA: 41s - loss: 0.4812 - accuracy: 0.859 - ETA: 40s - loss: 0.4817 - accuracy: 0.859 - ETA: 40s - loss: 0.4771 - accuracy: 0.861 - ETA: 39s - loss: 0.4763 - accuracy: 0.861 - ETA: 38s - loss: 0.4854 - accuracy: 0.858 - ETA: 38s - loss: 0.4878 - accuracy: 0.857 - ETA: 37s - loss: 0.4895 - accuracy: 0.856 - ETA: 36s - loss: 0.4919 - accuracy: 0.856 - ETA: 36s - loss: 0.4901 - accuracy: 0.856 - ETA: 35s - loss: 0.4908 - accuracy: 0.856 - ETA: 35s - loss: 0.4958 - accuracy: 0.856 - ETA: 34s - loss: 0.4912 - accuracy: 0.857 - ETA: 34s - loss: 0.4959 - accuracy: 0.854 - ETA: 33s - loss: 0.5016 - accuracy: 0.853 - ETA: 32s - loss: 0.5006 - accuracy: 0.853 - ETA: 32s - loss: 0.4974 - accuracy: 0.853 - ETA: 31s - loss: 0.4993 - accuracy: 0.853 - ETA: 30s - loss: 0.5012 - accuracy: 0.852 - ETA: 30s - loss: 0.5025 - accuracy: 0.852 - ETA: 29s - loss: 0.5000 - accuracy: 0.852 - ETA: 29s - loss: 0.5015 - accuracy: 0.850 - ETA: 28s - loss: 0.5015 - accuracy: 0.851 - ETA: 28s - loss: 0.5031 - accuracy: 0.850 - ETA: 27s - loss: 0.4991 - accuracy: 0.851 - ETA: 26s - loss: 0.4982 - accuracy: 0.852 - ETA: 26s - loss: 0.5003 - accuracy: 0.851 - ETA: 25s - loss: 0.4993 - accuracy: 0.852 - ETA: 25s - loss: 0.5042 - accuracy: 0.851 - ETA: 24s - loss: 0.5039 - accuracy: 0.851 - ETA: 24s - loss: 0.5027 - accuracy: 0.851 - ETA: 23s - loss: 0.5021 - accuracy: 0.851 - ETA: 22s - loss: 0.5022 - accuracy: 0.852 - ETA: 22s - loss: 0.5030 - accuracy: 0.852 - ETA: 21s - loss: 0.5032 - accuracy: 0.852 - ETA: 21s - loss: 0.5022 - accuracy: 0.852 - ETA: 20s - loss: 0.5008 - accuracy: 0.853 - ETA: 20s - loss: 0.5000 - accuracy: 0.853 - ETA: 19s - loss: 0.5008 - accuracy: 0.852 - ETA: 18s - loss: 0.5015 - accuracy: 0.852 - ETA: 18s - loss: 0.5012 - accuracy: 0.852 - ETA: 17s - loss: 0.5026 - accuracy: 0.851 - ETA: 17s - loss: 0.5014 - accuracy: 0.852 - ETA: 16s - loss: 0.5011 - accuracy: 0.852 - ETA: 15s - loss: 0.4990 - accuracy: 0.852 - ETA: 15s - loss: 0.4970 - accuracy: 0.853 - ETA: 14s - loss: 0.4959 - accuracy: 0.853 - ETA: 14s - loss: 0.4975 - accuracy: 0.852 - ETA: 13s - loss: 0.4953 - accuracy: 0.853 - ETA: 13s - loss: 0.4970 - accuracy: 0.852 - ETA: 12s - loss: 0.4961 - accuracy: 0.853 - ETA: 11s - loss: 0.4957 - accuracy: 0.853 - ETA: 11s - loss: 0.4955 - accuracy: 0.853 - ETA: 10s - loss: 0.4955 - accuracy: 0.853 - ETA: 10s - loss: 0.4967 - accuracy: 0.853 - ETA: 9s - loss: 0.4953 - accuracy: 0.853 - ETA: 9s - loss: 0.4959 - accuracy: 0.85 - ETA: 8s - loss: 0.4952 - accuracy: 0.85 - ETA: 7s - loss: 0.4942 - accuracy: 0.85 - ETA: 7s - loss: 0.4931 - accuracy: 0.85 - ETA: 6s - loss: 0.4931 - accuracy: 0.85 - ETA: 6s - loss: 0.4925 - accuracy: 0.85 - ETA: 5s - loss: 0.4910 - accuracy: 0.85 - ETA: 5s - loss: 0.4902 - accuracy: 0.85 - ETA: 4s - loss: 0.4893 - accuracy: 0.85 - ETA: 3s - loss: 0.4893 - accuracy: 0.85 - ETA: 3s - loss: 0.4903 - accuracy: 0.85 - ETA: 2s - loss: 0.4898 - accuracy: 0.85 - ETA: 2s - loss: 0.4904 - accuracy: 0.85 - ETA: 1s - loss: 0.4891 - accuracy: 0.85 - ETA: 0s - loss: 0.4879 - accuracy: 0.85 - ETA: 0s - loss: 0.4870 - accuracy: 0.85 - 66s 5ms/step - loss: 0.4868 - accuracy: 0.8546 - val_loss: 2.9556 - val_accuracy: 0.3605\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:01 - loss: 0.4106 - accuracy: 0.86 - ETA: 58s - loss: 0.3783 - accuracy: 0.8906 - ETA: 58s - loss: 0.3815 - accuracy: 0.898 - ETA: 56s - loss: 0.4293 - accuracy: 0.892 - ETA: 55s - loss: 0.4351 - accuracy: 0.890 - ETA: 55s - loss: 0.4290 - accuracy: 0.890 - ETA: 55s - loss: 0.4191 - accuracy: 0.889 - ETA: 54s - loss: 0.4452 - accuracy: 0.875 - ETA: 53s - loss: 0.4439 - accuracy: 0.874 - ETA: 53s - loss: 0.4351 - accuracy: 0.873 - ETA: 52s - loss: 0.4237 - accuracy: 0.877 - ETA: 51s - loss: 0.4233 - accuracy: 0.877 - ETA: 51s - loss: 0.4304 - accuracy: 0.872 - ETA: 50s - loss: 0.4357 - accuracy: 0.872 - ETA: 49s - loss: 0.4368 - accuracy: 0.872 - ETA: 49s - loss: 0.4379 - accuracy: 0.872 - ETA: 48s - loss: 0.4343 - accuracy: 0.872 - ETA: 47s - loss: 0.4422 - accuracy: 0.870 - ETA: 47s - loss: 0.4468 - accuracy: 0.869 - ETA: 46s - loss: 0.4545 - accuracy: 0.867 - ETA: 46s - loss: 0.4555 - accuracy: 0.865 - ETA: 45s - loss: 0.4549 - accuracy: 0.866 - ETA: 45s - loss: 0.4582 - accuracy: 0.864 - ETA: 44s - loss: 0.4542 - accuracy: 0.864 - ETA: 44s - loss: 0.4593 - accuracy: 0.862 - ETA: 43s - loss: 0.4638 - accuracy: 0.861 - ETA: 43s - loss: 0.4671 - accuracy: 0.861 - ETA: 42s - loss: 0.4760 - accuracy: 0.859 - ETA: 42s - loss: 0.4775 - accuracy: 0.859 - ETA: 41s - loss: 0.4757 - accuracy: 0.860 - ETA: 40s - loss: 0.4749 - accuracy: 0.859 - ETA: 40s - loss: 0.4762 - accuracy: 0.859 - ETA: 39s - loss: 0.4726 - accuracy: 0.859 - ETA: 39s - loss: 0.4739 - accuracy: 0.859 - ETA: 38s - loss: 0.4760 - accuracy: 0.859 - ETA: 37s - loss: 0.4764 - accuracy: 0.859 - ETA: 37s - loss: 0.4757 - accuracy: 0.859 - ETA: 36s - loss: 0.4758 - accuracy: 0.859 - ETA: 36s - loss: 0.4776 - accuracy: 0.859 - ETA: 35s - loss: 0.4739 - accuracy: 0.860 - ETA: 35s - loss: 0.4729 - accuracy: 0.860 - ETA: 34s - loss: 0.4702 - accuracy: 0.861 - ETA: 33s - loss: 0.4710 - accuracy: 0.861 - ETA: 33s - loss: 0.4714 - accuracy: 0.861 - ETA: 32s - loss: 0.4713 - accuracy: 0.861 - ETA: 32s - loss: 0.4735 - accuracy: 0.860 - ETA: 31s - loss: 0.4696 - accuracy: 0.861 - ETA: 30s - loss: 0.4671 - accuracy: 0.862 - ETA: 30s - loss: 0.4653 - accuracy: 0.862 - ETA: 29s - loss: 0.4686 - accuracy: 0.862 - ETA: 29s - loss: 0.4690 - accuracy: 0.861 - ETA: 28s - loss: 0.4699 - accuracy: 0.861 - ETA: 28s - loss: 0.4683 - accuracy: 0.861 - ETA: 27s - loss: 0.4666 - accuracy: 0.862 - ETA: 26s - loss: 0.4678 - accuracy: 0.861 - ETA: 26s - loss: 0.4678 - accuracy: 0.861 - ETA: 25s - loss: 0.4673 - accuracy: 0.861 - ETA: 25s - loss: 0.4678 - accuracy: 0.860 - ETA: 24s - loss: 0.4660 - accuracy: 0.861 - ETA: 24s - loss: 0.4677 - accuracy: 0.860 - ETA: 23s - loss: 0.4665 - accuracy: 0.860 - ETA: 22s - loss: 0.4643 - accuracy: 0.861 - ETA: 22s - loss: 0.4624 - accuracy: 0.861 - ETA: 21s - loss: 0.4615 - accuracy: 0.862 - ETA: 21s - loss: 0.4632 - accuracy: 0.861 - ETA: 20s - loss: 0.4631 - accuracy: 0.861 - ETA: 19s - loss: 0.4652 - accuracy: 0.860 - ETA: 19s - loss: 0.4641 - accuracy: 0.861 - ETA: 18s - loss: 0.4644 - accuracy: 0.861 - ETA: 18s - loss: 0.4657 - accuracy: 0.861 - ETA: 17s - loss: 0.4672 - accuracy: 0.860 - ETA: 17s - loss: 0.4686 - accuracy: 0.859 - ETA: 16s - loss: 0.4682 - accuracy: 0.859 - ETA: 15s - loss: 0.4670 - accuracy: 0.860 - ETA: 15s - loss: 0.4679 - accuracy: 0.860 - ETA: 14s - loss: 0.4674 - accuracy: 0.859 - ETA: 14s - loss: 0.4675 - accuracy: 0.859 - ETA: 13s - loss: 0.4672 - accuracy: 0.860 - ETA: 13s - loss: 0.4659 - accuracy: 0.860 - ETA: 12s - loss: 0.4651 - accuracy: 0.860 - ETA: 11s - loss: 0.4626 - accuracy: 0.861 - ETA: 11s - loss: 0.4616 - accuracy: 0.861 - ETA: 10s - loss: 0.4597 - accuracy: 0.861 - ETA: 10s - loss: 0.4591 - accuracy: 0.861 - ETA: 9s - loss: 0.4594 - accuracy: 0.861 - ETA: 9s - loss: 0.4581 - accuracy: 0.86 - ETA: 8s - loss: 0.4593 - accuracy: 0.86 - ETA: 7s - loss: 0.4585 - accuracy: 0.86 - ETA: 7s - loss: 0.4575 - accuracy: 0.86 - ETA: 6s - loss: 0.4572 - accuracy: 0.86 - ETA: 6s - loss: 0.4577 - accuracy: 0.86 - ETA: 5s - loss: 0.4556 - accuracy: 0.86 - ETA: 5s - loss: 0.4549 - accuracy: 0.86 - ETA: 4s - loss: 0.4532 - accuracy: 0.86 - ETA: 3s - loss: 0.4528 - accuracy: 0.86 - ETA: 3s - loss: 0.4524 - accuracy: 0.86 - ETA: 2s - loss: 0.4533 - accuracy: 0.86 - ETA: 2s - loss: 0.4547 - accuracy: 0.86 - ETA: 1s - loss: 0.4555 - accuracy: 0.86 - ETA: 1s - loss: 0.4541 - accuracy: 0.86 - ETA: 0s - loss: 0.4545 - accuracy: 0.86 - 66s 5ms/step - loss: 0.4546 - accuracy: 0.8625 - val_loss: 2.9969 - val_accuracy: 0.3669\n",
      "Epoch 48/100\n",
      "13022/13022 [==============================] - ETA: 59s - loss: 0.1889 - accuracy: 0.953 - ETA: 58s - loss: 0.4539 - accuracy: 0.890 - ETA: 58s - loss: 0.3926 - accuracy: 0.890 - ETA: 57s - loss: 0.4331 - accuracy: 0.880 - ETA: 56s - loss: 0.4369 - accuracy: 0.875 - ETA: 55s - loss: 0.4310 - accuracy: 0.872 - ETA: 55s - loss: 0.4283 - accuracy: 0.872 - ETA: 54s - loss: 0.4248 - accuracy: 0.874 - ETA: 53s - loss: 0.4167 - accuracy: 0.872 - ETA: 52s - loss: 0.4145 - accuracy: 0.873 - ETA: 52s - loss: 0.4160 - accuracy: 0.872 - ETA: 51s - loss: 0.4095 - accuracy: 0.875 - ETA: 50s - loss: 0.4033 - accuracy: 0.876 - ETA: 50s - loss: 0.4055 - accuracy: 0.874 - ETA: 49s - loss: 0.4131 - accuracy: 0.872 - ETA: 49s - loss: 0.4139 - accuracy: 0.871 - ETA: 48s - loss: 0.4209 - accuracy: 0.868 - ETA: 47s - loss: 0.4280 - accuracy: 0.867 - ETA: 47s - loss: 0.4287 - accuracy: 0.868 - ETA: 46s - loss: 0.4210 - accuracy: 0.869 - ETA: 46s - loss: 0.4267 - accuracy: 0.866 - ETA: 45s - loss: 0.4297 - accuracy: 0.866 - ETA: 45s - loss: 0.4254 - accuracy: 0.867 - ETA: 44s - loss: 0.4308 - accuracy: 0.865 - ETA: 44s - loss: 0.4367 - accuracy: 0.863 - ETA: 43s - loss: 0.4361 - accuracy: 0.863 - ETA: 43s - loss: 0.4338 - accuracy: 0.863 - ETA: 42s - loss: 0.4366 - accuracy: 0.862 - ETA: 41s - loss: 0.4414 - accuracy: 0.862 - ETA: 41s - loss: 0.4402 - accuracy: 0.862 - ETA: 40s - loss: 0.4401 - accuracy: 0.863 - ETA: 40s - loss: 0.4438 - accuracy: 0.862 - ETA: 39s - loss: 0.4478 - accuracy: 0.863 - ETA: 38s - loss: 0.4468 - accuracy: 0.864 - ETA: 38s - loss: 0.4457 - accuracy: 0.863 - ETA: 37s - loss: 0.4424 - accuracy: 0.865 - ETA: 37s - loss: 0.4453 - accuracy: 0.863 - ETA: 36s - loss: 0.4456 - accuracy: 0.863 - ETA: 35s - loss: 0.4462 - accuracy: 0.863 - ETA: 35s - loss: 0.4458 - accuracy: 0.863 - ETA: 34s - loss: 0.4545 - accuracy: 0.861 - ETA: 34s - loss: 0.4555 - accuracy: 0.861 - ETA: 33s - loss: 0.4582 - accuracy: 0.860 - ETA: 32s - loss: 0.4610 - accuracy: 0.858 - ETA: 32s - loss: 0.4608 - accuracy: 0.859 - ETA: 31s - loss: 0.4575 - accuracy: 0.859 - ETA: 31s - loss: 0.4566 - accuracy: 0.859 - ETA: 30s - loss: 0.4614 - accuracy: 0.858 - ETA: 30s - loss: 0.4620 - accuracy: 0.858 - ETA: 29s - loss: 0.4604 - accuracy: 0.859 - ETA: 28s - loss: 0.4603 - accuracy: 0.859 - ETA: 28s - loss: 0.4623 - accuracy: 0.859 - ETA: 27s - loss: 0.4645 - accuracy: 0.858 - ETA: 27s - loss: 0.4663 - accuracy: 0.858 - ETA: 26s - loss: 0.4635 - accuracy: 0.858 - ETA: 26s - loss: 0.4641 - accuracy: 0.858 - ETA: 25s - loss: 0.4650 - accuracy: 0.858 - ETA: 24s - loss: 0.4628 - accuracy: 0.858 - ETA: 24s - loss: 0.4660 - accuracy: 0.858 - ETA: 23s - loss: 0.4682 - accuracy: 0.858 - ETA: 23s - loss: 0.4660 - accuracy: 0.859 - ETA: 22s - loss: 0.4663 - accuracy: 0.859 - ETA: 22s - loss: 0.4665 - accuracy: 0.860 - ETA: 21s - loss: 0.4678 - accuracy: 0.860 - ETA: 20s - loss: 0.4673 - accuracy: 0.859 - ETA: 20s - loss: 0.4675 - accuracy: 0.859 - ETA: 19s - loss: 0.4695 - accuracy: 0.859 - ETA: 19s - loss: 0.4700 - accuracy: 0.859 - ETA: 18s - loss: 0.4689 - accuracy: 0.859 - ETA: 18s - loss: 0.4681 - accuracy: 0.859 - ETA: 17s - loss: 0.4681 - accuracy: 0.860 - ETA: 16s - loss: 0.4692 - accuracy: 0.859 - ETA: 16s - loss: 0.4695 - accuracy: 0.859 - ETA: 15s - loss: 0.4703 - accuracy: 0.859 - ETA: 15s - loss: 0.4705 - accuracy: 0.860 - ETA: 14s - loss: 0.4685 - accuracy: 0.860 - ETA: 14s - loss: 0.4688 - accuracy: 0.861 - ETA: 13s - loss: 0.4698 - accuracy: 0.861 - ETA: 12s - loss: 0.4690 - accuracy: 0.861 - ETA: 12s - loss: 0.4708 - accuracy: 0.860 - ETA: 11s - loss: 0.4717 - accuracy: 0.860 - ETA: 11s - loss: 0.4719 - accuracy: 0.860 - ETA: 10s - loss: 0.4715 - accuracy: 0.860 - ETA: 10s - loss: 0.4727 - accuracy: 0.860 - ETA: 9s - loss: 0.4724 - accuracy: 0.860 - ETA: 8s - loss: 0.4712 - accuracy: 0.86 - ETA: 8s - loss: 0.4709 - accuracy: 0.86 - ETA: 7s - loss: 0.4728 - accuracy: 0.85 - ETA: 7s - loss: 0.4722 - accuracy: 0.86 - ETA: 6s - loss: 0.4706 - accuracy: 0.86 - ETA: 6s - loss: 0.4710 - accuracy: 0.86 - ETA: 5s - loss: 0.4702 - accuracy: 0.86 - ETA: 4s - loss: 0.4715 - accuracy: 0.86 - ETA: 4s - loss: 0.4737 - accuracy: 0.85 - ETA: 3s - loss: 0.4735 - accuracy: 0.85 - ETA: 3s - loss: 0.4742 - accuracy: 0.85 - ETA: 2s - loss: 0.4742 - accuracy: 0.85 - ETA: 2s - loss: 0.4745 - accuracy: 0.85 - ETA: 1s - loss: 0.4739 - accuracy: 0.85 - ETA: 0s - loss: 0.4742 - accuracy: 0.85 - ETA: 0s - loss: 0.4753 - accuracy: 0.85 - 65s 5ms/step - loss: 0.4750 - accuracy: 0.8585 - val_loss: 2.8341 - val_accuracy: 0.3478\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:01 - loss: 0.4102 - accuracy: 0.87 - ETA: 59s - loss: 0.3755 - accuracy: 0.8906 - ETA: 58s - loss: 0.4006 - accuracy: 0.877 - ETA: 56s - loss: 0.4011 - accuracy: 0.877 - ETA: 56s - loss: 0.4345 - accuracy: 0.868 - ETA: 54s - loss: 0.4279 - accuracy: 0.869 - ETA: 54s - loss: 0.4395 - accuracy: 0.862 - ETA: 53s - loss: 0.4398 - accuracy: 0.859 - ETA: 53s - loss: 0.4250 - accuracy: 0.862 - ETA: 52s - loss: 0.4039 - accuracy: 0.868 - ETA: 51s - loss: 0.4116 - accuracy: 0.866 - ETA: 51s - loss: 0.4090 - accuracy: 0.868 - ETA: 50s - loss: 0.4034 - accuracy: 0.869 - ETA: 50s - loss: 0.4059 - accuracy: 0.866 - ETA: 49s - loss: 0.4164 - accuracy: 0.864 - ETA: 48s - loss: 0.4142 - accuracy: 0.865 - ETA: 48s - loss: 0.4151 - accuracy: 0.865 - ETA: 47s - loss: 0.4129 - accuracy: 0.866 - ETA: 47s - loss: 0.4129 - accuracy: 0.869 - ETA: 46s - loss: 0.4143 - accuracy: 0.868 - ETA: 46s - loss: 0.4176 - accuracy: 0.867 - ETA: 46s - loss: 0.4244 - accuracy: 0.865 - ETA: 45s - loss: 0.4275 - accuracy: 0.864 - ETA: 45s - loss: 0.4282 - accuracy: 0.862 - ETA: 44s - loss: 0.4271 - accuracy: 0.862 - ETA: 43s - loss: 0.4342 - accuracy: 0.861 - ETA: 43s - loss: 0.4376 - accuracy: 0.861 - ETA: 42s - loss: 0.4338 - accuracy: 0.862 - ETA: 42s - loss: 0.4319 - accuracy: 0.863 - ETA: 41s - loss: 0.4386 - accuracy: 0.862 - ETA: 41s - loss: 0.4377 - accuracy: 0.862 - ETA: 40s - loss: 0.4379 - accuracy: 0.863 - ETA: 39s - loss: 0.4355 - accuracy: 0.864 - ETA: 39s - loss: 0.4356 - accuracy: 0.863 - ETA: 38s - loss: 0.4391 - accuracy: 0.862 - ETA: 37s - loss: 0.4478 - accuracy: 0.860 - ETA: 37s - loss: 0.4473 - accuracy: 0.861 - ETA: 36s - loss: 0.4506 - accuracy: 0.861 - ETA: 36s - loss: 0.4502 - accuracy: 0.861 - ETA: 35s - loss: 0.4475 - accuracy: 0.861 - ETA: 35s - loss: 0.4501 - accuracy: 0.860 - ETA: 34s - loss: 0.4514 - accuracy: 0.859 - ETA: 33s - loss: 0.4487 - accuracy: 0.860 - ETA: 33s - loss: 0.4456 - accuracy: 0.861 - ETA: 32s - loss: 0.4441 - accuracy: 0.862 - ETA: 32s - loss: 0.4453 - accuracy: 0.862 - ETA: 31s - loss: 0.4459 - accuracy: 0.862 - ETA: 31s - loss: 0.4459 - accuracy: 0.862 - ETA: 30s - loss: 0.4448 - accuracy: 0.862 - ETA: 29s - loss: 0.4453 - accuracy: 0.862 - ETA: 29s - loss: 0.4443 - accuracy: 0.862 - ETA: 28s - loss: 0.4464 - accuracy: 0.863 - ETA: 28s - loss: 0.4478 - accuracy: 0.863 - ETA: 27s - loss: 0.4483 - accuracy: 0.863 - ETA: 27s - loss: 0.4507 - accuracy: 0.862 - ETA: 26s - loss: 0.4502 - accuracy: 0.863 - ETA: 25s - loss: 0.4512 - accuracy: 0.862 - ETA: 25s - loss: 0.4526 - accuracy: 0.862 - ETA: 24s - loss: 0.4521 - accuracy: 0.862 - ETA: 24s - loss: 0.4544 - accuracy: 0.861 - ETA: 23s - loss: 0.4538 - accuracy: 0.862 - ETA: 22s - loss: 0.4542 - accuracy: 0.861 - ETA: 22s - loss: 0.4519 - accuracy: 0.863 - ETA: 21s - loss: 0.4532 - accuracy: 0.862 - ETA: 21s - loss: 0.4531 - accuracy: 0.862 - ETA: 20s - loss: 0.4564 - accuracy: 0.861 - ETA: 19s - loss: 0.4551 - accuracy: 0.861 - ETA: 19s - loss: 0.4574 - accuracy: 0.861 - ETA: 18s - loss: 0.4574 - accuracy: 0.861 - ETA: 18s - loss: 0.4555 - accuracy: 0.861 - ETA: 17s - loss: 0.4551 - accuracy: 0.862 - ETA: 17s - loss: 0.4549 - accuracy: 0.861 - ETA: 16s - loss: 0.4554 - accuracy: 0.861 - ETA: 15s - loss: 0.4549 - accuracy: 0.861 - ETA: 15s - loss: 0.4539 - accuracy: 0.861 - ETA: 14s - loss: 0.4522 - accuracy: 0.862 - ETA: 14s - loss: 0.4511 - accuracy: 0.862 - ETA: 13s - loss: 0.4515 - accuracy: 0.862 - ETA: 13s - loss: 0.4519 - accuracy: 0.862 - ETA: 12s - loss: 0.4504 - accuracy: 0.862 - ETA: 11s - loss: 0.4496 - accuracy: 0.862 - ETA: 11s - loss: 0.4502 - accuracy: 0.862 - ETA: 10s - loss: 0.4490 - accuracy: 0.862 - ETA: 10s - loss: 0.4497 - accuracy: 0.862 - ETA: 9s - loss: 0.4501 - accuracy: 0.862 - ETA: 9s - loss: 0.4506 - accuracy: 0.86 - ETA: 8s - loss: 0.4507 - accuracy: 0.86 - ETA: 7s - loss: 0.4518 - accuracy: 0.86 - ETA: 7s - loss: 0.4505 - accuracy: 0.86 - ETA: 6s - loss: 0.4523 - accuracy: 0.86 - ETA: 6s - loss: 0.4523 - accuracy: 0.86 - ETA: 5s - loss: 0.4519 - accuracy: 0.86 - ETA: 5s - loss: 0.4518 - accuracy: 0.86 - ETA: 4s - loss: 0.4527 - accuracy: 0.86 - ETA: 3s - loss: 0.4526 - accuracy: 0.86 - ETA: 3s - loss: 0.4522 - accuracy: 0.86 - ETA: 2s - loss: 0.4533 - accuracy: 0.86 - ETA: 2s - loss: 0.4513 - accuracy: 0.86 - ETA: 1s - loss: 0.4503 - accuracy: 0.86 - ETA: 0s - loss: 0.4501 - accuracy: 0.86 - ETA: 0s - loss: 0.4493 - accuracy: 0.86 - 66s 5ms/step - loss: 0.4501 - accuracy: 0.8633 - val_loss: 2.8761 - val_accuracy: 0.3541\n",
      "Epoch 50/100\n",
      "13022/13022 [==============================] - ETA: 1:02 - loss: 0.2955 - accuracy: 0.89 - ETA: 59s - loss: 0.3573 - accuracy: 0.8984 - ETA: 56s - loss: 0.3256 - accuracy: 0.898 - ETA: 55s - loss: 0.3616 - accuracy: 0.888 - ETA: 55s - loss: 0.4398 - accuracy: 0.870 - ETA: 54s - loss: 0.4447 - accuracy: 0.868 - ETA: 53s - loss: 0.4277 - accuracy: 0.871 - ETA: 53s - loss: 0.4314 - accuracy: 0.870 - ETA: 52s - loss: 0.4308 - accuracy: 0.873 - ETA: 51s - loss: 0.4258 - accuracy: 0.873 - ETA: 51s - loss: 0.4229 - accuracy: 0.875 - ETA: 50s - loss: 0.4386 - accuracy: 0.869 - ETA: 50s - loss: 0.4337 - accuracy: 0.870 - ETA: 49s - loss: 0.4340 - accuracy: 0.871 - ETA: 49s - loss: 0.4433 - accuracy: 0.869 - ETA: 48s - loss: 0.4402 - accuracy: 0.869 - ETA: 48s - loss: 0.4353 - accuracy: 0.870 - ETA: 48s - loss: 0.4320 - accuracy: 0.871 - ETA: 47s - loss: 0.4253 - accuracy: 0.872 - ETA: 47s - loss: 0.4212 - accuracy: 0.872 - ETA: 46s - loss: 0.4203 - accuracy: 0.872 - ETA: 46s - loss: 0.4167 - accuracy: 0.873 - ETA: 45s - loss: 0.4172 - accuracy: 0.872 - ETA: 45s - loss: 0.4226 - accuracy: 0.871 - ETA: 44s - loss: 0.4236 - accuracy: 0.871 - ETA: 44s - loss: 0.4210 - accuracy: 0.870 - ETA: 43s - loss: 0.4233 - accuracy: 0.868 - ETA: 42s - loss: 0.4235 - accuracy: 0.869 - ETA: 42s - loss: 0.4191 - accuracy: 0.871 - ETA: 41s - loss: 0.4200 - accuracy: 0.871 - ETA: 40s - loss: 0.4196 - accuracy: 0.870 - ETA: 40s - loss: 0.4189 - accuracy: 0.870 - ETA: 39s - loss: 0.4190 - accuracy: 0.870 - ETA: 39s - loss: 0.4170 - accuracy: 0.870 - ETA: 38s - loss: 0.4207 - accuracy: 0.869 - ETA: 37s - loss: 0.4196 - accuracy: 0.869 - ETA: 37s - loss: 0.4185 - accuracy: 0.870 - ETA: 36s - loss: 0.4212 - accuracy: 0.870 - ETA: 36s - loss: 0.4209 - accuracy: 0.869 - ETA: 35s - loss: 0.4205 - accuracy: 0.870 - ETA: 34s - loss: 0.4211 - accuracy: 0.871 - ETA: 34s - loss: 0.4198 - accuracy: 0.870 - ETA: 33s - loss: 0.4186 - accuracy: 0.870 - ETA: 33s - loss: 0.4198 - accuracy: 0.870 - ETA: 32s - loss: 0.4229 - accuracy: 0.869 - ETA: 32s - loss: 0.4208 - accuracy: 0.870 - ETA: 31s - loss: 0.4263 - accuracy: 0.868 - ETA: 31s - loss: 0.4274 - accuracy: 0.868 - ETA: 30s - loss: 0.4272 - accuracy: 0.868 - ETA: 29s - loss: 0.4265 - accuracy: 0.868 - ETA: 29s - loss: 0.4245 - accuracy: 0.869 - ETA: 28s - loss: 0.4246 - accuracy: 0.868 - ETA: 28s - loss: 0.4223 - accuracy: 0.869 - ETA: 27s - loss: 0.4203 - accuracy: 0.869 - ETA: 26s - loss: 0.4197 - accuracy: 0.869 - ETA: 26s - loss: 0.4230 - accuracy: 0.868 - ETA: 25s - loss: 0.4214 - accuracy: 0.868 - ETA: 25s - loss: 0.4187 - accuracy: 0.869 - ETA: 24s - loss: 0.4200 - accuracy: 0.869 - ETA: 23s - loss: 0.4233 - accuracy: 0.868 - ETA: 23s - loss: 0.4253 - accuracy: 0.868 - ETA: 22s - loss: 0.4270 - accuracy: 0.867 - ETA: 22s - loss: 0.4266 - accuracy: 0.867 - ETA: 21s - loss: 0.4276 - accuracy: 0.867 - ETA: 21s - loss: 0.4253 - accuracy: 0.867 - ETA: 20s - loss: 0.4258 - accuracy: 0.867 - ETA: 20s - loss: 0.4266 - accuracy: 0.867 - ETA: 19s - loss: 0.4253 - accuracy: 0.867 - ETA: 18s - loss: 0.4238 - accuracy: 0.867 - ETA: 18s - loss: 0.4242 - accuracy: 0.867 - ETA: 17s - loss: 0.4248 - accuracy: 0.867 - ETA: 17s - loss: 0.4261 - accuracy: 0.866 - ETA: 16s - loss: 0.4267 - accuracy: 0.866 - ETA: 16s - loss: 0.4264 - accuracy: 0.867 - ETA: 15s - loss: 0.4264 - accuracy: 0.867 - ETA: 15s - loss: 0.4261 - accuracy: 0.867 - ETA: 14s - loss: 0.4269 - accuracy: 0.867 - ETA: 13s - loss: 0.4279 - accuracy: 0.866 - ETA: 13s - loss: 0.4296 - accuracy: 0.866 - ETA: 12s - loss: 0.4305 - accuracy: 0.866 - ETA: 12s - loss: 0.4318 - accuracy: 0.866 - ETA: 11s - loss: 0.4340 - accuracy: 0.865 - ETA: 10s - loss: 0.4340 - accuracy: 0.865 - ETA: 10s - loss: 0.4327 - accuracy: 0.865 - ETA: 9s - loss: 0.4335 - accuracy: 0.865 - ETA: 9s - loss: 0.4313 - accuracy: 0.86 - ETA: 8s - loss: 0.4324 - accuracy: 0.86 - ETA: 7s - loss: 0.4328 - accuracy: 0.86 - ETA: 7s - loss: 0.4332 - accuracy: 0.86 - ETA: 6s - loss: 0.4333 - accuracy: 0.86 - ETA: 6s - loss: 0.4328 - accuracy: 0.86 - ETA: 5s - loss: 0.4342 - accuracy: 0.86 - ETA: 5s - loss: 0.4342 - accuracy: 0.86 - ETA: 4s - loss: 0.4347 - accuracy: 0.86 - ETA: 3s - loss: 0.4343 - accuracy: 0.86 - ETA: 3s - loss: 0.4346 - accuracy: 0.86 - ETA: 2s - loss: 0.4352 - accuracy: 0.86 - ETA: 2s - loss: 0.4355 - accuracy: 0.86 - ETA: 1s - loss: 0.4351 - accuracy: 0.86 - ETA: 1s - loss: 0.4355 - accuracy: 0.86 - ETA: 0s - loss: 0.4371 - accuracy: 0.86 - 66s 5ms/step - loss: 0.4372 - accuracy: 0.8645 - val_loss: 2.9657 - val_accuracy: 0.3536\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:00 - loss: 0.4477 - accuracy: 0.85 - ETA: 58s - loss: 0.5169 - accuracy: 0.8281 - ETA: 56s - loss: 0.5031 - accuracy: 0.830 - ETA: 56s - loss: 0.4871 - accuracy: 0.843 - ETA: 55s - loss: 0.4414 - accuracy: 0.860 - ETA: 55s - loss: 0.4519 - accuracy: 0.864 - ETA: 54s - loss: 0.4646 - accuracy: 0.862 - ETA: 53s - loss: 0.4750 - accuracy: 0.860 - ETA: 52s - loss: 0.4637 - accuracy: 0.864 - ETA: 52s - loss: 0.4688 - accuracy: 0.864 - ETA: 51s - loss: 0.4640 - accuracy: 0.865 - ETA: 51s - loss: 0.4857 - accuracy: 0.857 - ETA: 50s - loss: 0.4819 - accuracy: 0.857 - ETA: 50s - loss: 0.4688 - accuracy: 0.860 - ETA: 50s - loss: 0.4760 - accuracy: 0.857 - ETA: 49s - loss: 0.4796 - accuracy: 0.855 - ETA: 49s - loss: 0.4697 - accuracy: 0.857 - ETA: 48s - loss: 0.4705 - accuracy: 0.857 - ETA: 47s - loss: 0.4794 - accuracy: 0.856 - ETA: 47s - loss: 0.4757 - accuracy: 0.856 - ETA: 46s - loss: 0.4707 - accuracy: 0.857 - ETA: 46s - loss: 0.4652 - accuracy: 0.858 - ETA: 45s - loss: 0.4654 - accuracy: 0.857 - ETA: 45s - loss: 0.4706 - accuracy: 0.855 - ETA: 44s - loss: 0.4694 - accuracy: 0.856 - ETA: 44s - loss: 0.4700 - accuracy: 0.856 - ETA: 43s - loss: 0.4689 - accuracy: 0.856 - ETA: 42s - loss: 0.4665 - accuracy: 0.856 - ETA: 42s - loss: 0.4660 - accuracy: 0.857 - ETA: 41s - loss: 0.4645 - accuracy: 0.858 - ETA: 41s - loss: 0.4614 - accuracy: 0.860 - ETA: 40s - loss: 0.4587 - accuracy: 0.859 - ETA: 39s - loss: 0.4611 - accuracy: 0.858 - ETA: 39s - loss: 0.4565 - accuracy: 0.859 - ETA: 38s - loss: 0.4582 - accuracy: 0.859 - ETA: 38s - loss: 0.4558 - accuracy: 0.860 - ETA: 37s - loss: 0.4575 - accuracy: 0.860 - ETA: 36s - loss: 0.4574 - accuracy: 0.860 - ETA: 36s - loss: 0.4543 - accuracy: 0.861 - ETA: 35s - loss: 0.4592 - accuracy: 0.859 - ETA: 35s - loss: 0.4585 - accuracy: 0.860 - ETA: 34s - loss: 0.4560 - accuracy: 0.861 - ETA: 34s - loss: 0.4534 - accuracy: 0.862 - ETA: 33s - loss: 0.4543 - accuracy: 0.861 - ETA: 33s - loss: 0.4546 - accuracy: 0.860 - ETA: 32s - loss: 0.4531 - accuracy: 0.861 - ETA: 31s - loss: 0.4557 - accuracy: 0.860 - ETA: 31s - loss: 0.4545 - accuracy: 0.861 - ETA: 30s - loss: 0.4564 - accuracy: 0.860 - ETA: 30s - loss: 0.4563 - accuracy: 0.860 - ETA: 29s - loss: 0.4545 - accuracy: 0.860 - ETA: 29s - loss: 0.4515 - accuracy: 0.861 - ETA: 28s - loss: 0.4540 - accuracy: 0.860 - ETA: 28s - loss: 0.4541 - accuracy: 0.860 - ETA: 27s - loss: 0.4522 - accuracy: 0.860 - ETA: 26s - loss: 0.4501 - accuracy: 0.861 - ETA: 26s - loss: 0.4472 - accuracy: 0.862 - ETA: 25s - loss: 0.4488 - accuracy: 0.861 - ETA: 25s - loss: 0.4492 - accuracy: 0.861 - ETA: 24s - loss: 0.4496 - accuracy: 0.862 - ETA: 23s - loss: 0.4506 - accuracy: 0.862 - ETA: 23s - loss: 0.4518 - accuracy: 0.861 - ETA: 22s - loss: 0.4508 - accuracy: 0.862 - ETA: 22s - loss: 0.4487 - accuracy: 0.862 - ETA: 21s - loss: 0.4473 - accuracy: 0.863 - ETA: 20s - loss: 0.4467 - accuracy: 0.862 - ETA: 20s - loss: 0.4475 - accuracy: 0.862 - ETA: 19s - loss: 0.4480 - accuracy: 0.862 - ETA: 19s - loss: 0.4479 - accuracy: 0.862 - ETA: 18s - loss: 0.4479 - accuracy: 0.862 - ETA: 17s - loss: 0.4474 - accuracy: 0.863 - ETA: 17s - loss: 0.4472 - accuracy: 0.862 - ETA: 16s - loss: 0.4481 - accuracy: 0.862 - ETA: 16s - loss: 0.4485 - accuracy: 0.862 - ETA: 15s - loss: 0.4457 - accuracy: 0.863 - ETA: 15s - loss: 0.4465 - accuracy: 0.862 - ETA: 14s - loss: 0.4454 - accuracy: 0.862 - ETA: 13s - loss: 0.4466 - accuracy: 0.862 - ETA: 13s - loss: 0.4476 - accuracy: 0.861 - ETA: 12s - loss: 0.4470 - accuracy: 0.861 - ETA: 12s - loss: 0.4482 - accuracy: 0.861 - ETA: 11s - loss: 0.4484 - accuracy: 0.861 - ETA: 10s - loss: 0.4477 - accuracy: 0.861 - ETA: 10s - loss: 0.4471 - accuracy: 0.861 - ETA: 9s - loss: 0.4466 - accuracy: 0.861 - ETA: 9s - loss: 0.4465 - accuracy: 0.86 - ETA: 8s - loss: 0.4476 - accuracy: 0.86 - ETA: 7s - loss: 0.4468 - accuracy: 0.86 - ETA: 7s - loss: 0.4476 - accuracy: 0.86 - ETA: 6s - loss: 0.4461 - accuracy: 0.86 - ETA: 6s - loss: 0.4462 - accuracy: 0.86 - ETA: 5s - loss: 0.4458 - accuracy: 0.86 - ETA: 5s - loss: 0.4461 - accuracy: 0.86 - ETA: 4s - loss: 0.4444 - accuracy: 0.86 - ETA: 3s - loss: 0.4448 - accuracy: 0.86 - ETA: 3s - loss: 0.4441 - accuracy: 0.86 - ETA: 2s - loss: 0.4433 - accuracy: 0.86 - ETA: 2s - loss: 0.4430 - accuracy: 0.86 - ETA: 1s - loss: 0.4441 - accuracy: 0.86 - ETA: 1s - loss: 0.4442 - accuracy: 0.86 - ETA: 0s - loss: 0.4438 - accuracy: 0.86 - 66s 5ms/step - loss: 0.4431 - accuracy: 0.8638 - val_loss: 2.9429 - val_accuracy: 0.3547\n",
      "Epoch 52/100\n",
      "13022/13022 [==============================] - ETA: 1:01 - loss: 0.4732 - accuracy: 0.85 - ETA: 57s - loss: 0.4340 - accuracy: 0.8672 - ETA: 1:01 - loss: 0.4018 - accuracy: 0.88 - ETA: 1:08 - loss: 0.3997 - accuracy: 0.87 - ETA: 1:12 - loss: 0.3686 - accuracy: 0.88 - ETA: 1:15 - loss: 0.3865 - accuracy: 0.88 - ETA: 1:16 - loss: 0.3898 - accuracy: 0.88 - ETA: 1:17 - loss: 0.3883 - accuracy: 0.88 - ETA: 1:16 - loss: 0.4188 - accuracy: 0.87 - ETA: 1:16 - loss: 0.4183 - accuracy: 0.87 - ETA: 1:16 - loss: 0.4353 - accuracy: 0.87 - ETA: 1:16 - loss: 0.4346 - accuracy: 0.87 - ETA: 1:16 - loss: 0.4261 - accuracy: 0.87 - ETA: 1:16 - loss: 0.4238 - accuracy: 0.87 - ETA: 1:16 - loss: 0.4207 - accuracy: 0.87 - ETA: 1:15 - loss: 0.4315 - accuracy: 0.87 - ETA: 1:15 - loss: 0.4365 - accuracy: 0.87 - ETA: 1:14 - loss: 0.4419 - accuracy: 0.87 - ETA: 1:13 - loss: 0.4370 - accuracy: 0.87 - ETA: 1:12 - loss: 0.4305 - accuracy: 0.87 - ETA: 1:11 - loss: 0.4300 - accuracy: 0.87 - ETA: 1:10 - loss: 0.4274 - accuracy: 0.87 - ETA: 1:10 - loss: 0.4222 - accuracy: 0.87 - ETA: 1:09 - loss: 0.4238 - accuracy: 0.87 - ETA: 1:08 - loss: 0.4226 - accuracy: 0.87 - ETA: 1:07 - loss: 0.4180 - accuracy: 0.87 - ETA: 1:07 - loss: 0.4196 - accuracy: 0.87 - ETA: 1:06 - loss: 0.4205 - accuracy: 0.87 - ETA: 1:05 - loss: 0.4191 - accuracy: 0.87 - ETA: 1:04 - loss: 0.4192 - accuracy: 0.87 - ETA: 1:03 - loss: 0.4183 - accuracy: 0.88 - ETA: 1:02 - loss: 0.4179 - accuracy: 0.88 - ETA: 1:01 - loss: 0.4176 - accuracy: 0.88 - ETA: 1:00 - loss: 0.4170 - accuracy: 0.88 - ETA: 59s - loss: 0.4212 - accuracy: 0.8795 - ETA: 59s - loss: 0.4213 - accuracy: 0.878 - ETA: 58s - loss: 0.4272 - accuracy: 0.877 - ETA: 57s - loss: 0.4263 - accuracy: 0.876 - ETA: 56s - loss: 0.4298 - accuracy: 0.875 - ETA: 55s - loss: 0.4280 - accuracy: 0.875 - ETA: 54s - loss: 0.4255 - accuracy: 0.876 - ETA: 53s - loss: 0.4213 - accuracy: 0.876 - ETA: 52s - loss: 0.4211 - accuracy: 0.877 - ETA: 51s - loss: 0.4206 - accuracy: 0.877 - ETA: 51s - loss: 0.4178 - accuracy: 0.877 - ETA: 50s - loss: 0.4170 - accuracy: 0.877 - ETA: 49s - loss: 0.4224 - accuracy: 0.876 - ETA: 48s - loss: 0.4214 - accuracy: 0.876 - ETA: 47s - loss: 0.4226 - accuracy: 0.876 - ETA: 46s - loss: 0.4251 - accuracy: 0.875 - ETA: 45s - loss: 0.4250 - accuracy: 0.875 - ETA: 44s - loss: 0.4233 - accuracy: 0.876 - ETA: 43s - loss: 0.4210 - accuracy: 0.877 - ETA: 43s - loss: 0.4181 - accuracy: 0.877 - ETA: 42s - loss: 0.4178 - accuracy: 0.878 - ETA: 41s - loss: 0.4159 - accuracy: 0.878 - ETA: 40s - loss: 0.4165 - accuracy: 0.878 - ETA: 39s - loss: 0.4194 - accuracy: 0.877 - ETA: 38s - loss: 0.4212 - accuracy: 0.877 - ETA: 37s - loss: 0.4195 - accuracy: 0.877 - ETA: 36s - loss: 0.4216 - accuracy: 0.876 - ETA: 36s - loss: 0.4202 - accuracy: 0.876 - ETA: 35s - loss: 0.4195 - accuracy: 0.876 - ETA: 34s - loss: 0.4212 - accuracy: 0.876 - ETA: 33s - loss: 0.4207 - accuracy: 0.876 - ETA: 32s - loss: 0.4227 - accuracy: 0.876 - ETA: 31s - loss: 0.4199 - accuracy: 0.877 - ETA: 30s - loss: 0.4217 - accuracy: 0.876 - ETA: 29s - loss: 0.4198 - accuracy: 0.876 - ETA: 28s - loss: 0.4184 - accuracy: 0.877 - ETA: 27s - loss: 0.4182 - accuracy: 0.877 - ETA: 26s - loss: 0.4183 - accuracy: 0.877 - ETA: 26s - loss: 0.4170 - accuracy: 0.878 - ETA: 25s - loss: 0.4198 - accuracy: 0.877 - ETA: 24s - loss: 0.4190 - accuracy: 0.877 - ETA: 23s - loss: 0.4175 - accuracy: 0.877 - ETA: 22s - loss: 0.4179 - accuracy: 0.877 - ETA: 21s - loss: 0.4191 - accuracy: 0.877 - ETA: 20s - loss: 0.4200 - accuracy: 0.877 - ETA: 19s - loss: 0.4218 - accuracy: 0.876 - ETA: 18s - loss: 0.4208 - accuracy: 0.876 - ETA: 17s - loss: 0.4202 - accuracy: 0.876 - ETA: 17s - loss: 0.4214 - accuracy: 0.875 - ETA: 16s - loss: 0.4241 - accuracy: 0.875 - ETA: 15s - loss: 0.4249 - accuracy: 0.875 - ETA: 14s - loss: 0.4230 - accuracy: 0.876 - ETA: 13s - loss: 0.4237 - accuracy: 0.875 - ETA: 12s - loss: 0.4232 - accuracy: 0.875 - ETA: 11s - loss: 0.4237 - accuracy: 0.875 - ETA: 10s - loss: 0.4225 - accuracy: 0.875 - ETA: 9s - loss: 0.4233 - accuracy: 0.875 - ETA: 8s - loss: 0.4221 - accuracy: 0.87 - ETA: 7s - loss: 0.4215 - accuracy: 0.87 - ETA: 7s - loss: 0.4229 - accuracy: 0.87 - ETA: 6s - loss: 0.4238 - accuracy: 0.87 - ETA: 5s - loss: 0.4227 - accuracy: 0.87 - ETA: 4s - loss: 0.4236 - accuracy: 0.87 - ETA: 3s - loss: 0.4244 - accuracy: 0.87 - ETA: 2s - loss: 0.4264 - accuracy: 0.87 - ETA: 1s - loss: 0.4263 - accuracy: 0.87 - ETA: 0s - loss: 0.4257 - accuracy: 0.87 - 106s 8ms/step - loss: 0.4258 - accuracy: 0.8743 - val_loss: 2.8036 - val_accuracy: 0.3690\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:34 - loss: 0.2677 - accuracy: 0.93 - ETA: 1:33 - loss: 0.2681 - accuracy: 0.91 - ETA: 1:31 - loss: 0.2852 - accuracy: 0.89 - ETA: 1:30 - loss: 0.3311 - accuracy: 0.89 - ETA: 1:28 - loss: 0.3415 - accuracy: 0.88 - ETA: 1:27 - loss: 0.3607 - accuracy: 0.87 - ETA: 1:26 - loss: 0.3997 - accuracy: 0.86 - ETA: 1:24 - loss: 0.4041 - accuracy: 0.86 - ETA: 1:24 - loss: 0.4187 - accuracy: 0.86 - ETA: 1:23 - loss: 0.4216 - accuracy: 0.86 - ETA: 1:22 - loss: 0.4165 - accuracy: 0.86 - ETA: 1:21 - loss: 0.4055 - accuracy: 0.87 - ETA: 1:20 - loss: 0.4113 - accuracy: 0.86 - ETA: 1:19 - loss: 0.4225 - accuracy: 0.86 - ETA: 1:18 - loss: 0.4166 - accuracy: 0.86 - ETA: 1:17 - loss: 0.4150 - accuracy: 0.86 - ETA: 1:16 - loss: 0.4076 - accuracy: 0.86 - ETA: 1:15 - loss: 0.4102 - accuracy: 0.86 - ETA: 1:14 - loss: 0.4038 - accuracy: 0.87 - ETA: 1:13 - loss: 0.4049 - accuracy: 0.87 - ETA: 1:12 - loss: 0.4038 - accuracy: 0.87 - ETA: 1:11 - loss: 0.4047 - accuracy: 0.87 - ETA: 1:10 - loss: 0.4015 - accuracy: 0.87 - ETA: 1:09 - loss: 0.4046 - accuracy: 0.87 - ETA: 1:08 - loss: 0.4067 - accuracy: 0.87 - ETA: 1:07 - loss: 0.4079 - accuracy: 0.87 - ETA: 1:06 - loss: 0.4061 - accuracy: 0.87 - ETA: 1:05 - loss: 0.4055 - accuracy: 0.87 - ETA: 1:05 - loss: 0.4040 - accuracy: 0.87 - ETA: 1:04 - loss: 0.4060 - accuracy: 0.87 - ETA: 1:03 - loss: 0.4051 - accuracy: 0.87 - ETA: 1:02 - loss: 0.4037 - accuracy: 0.87 - ETA: 1:01 - loss: 0.3992 - accuracy: 0.87 - ETA: 1:00 - loss: 0.3987 - accuracy: 0.87 - ETA: 59s - loss: 0.3961 - accuracy: 0.8799 - ETA: 59s - loss: 0.3970 - accuracy: 0.879 - ETA: 58s - loss: 0.3976 - accuracy: 0.879 - ETA: 57s - loss: 0.3995 - accuracy: 0.878 - ETA: 56s - loss: 0.3994 - accuracy: 0.878 - ETA: 55s - loss: 0.3980 - accuracy: 0.878 - ETA: 54s - loss: 0.4020 - accuracy: 0.878 - ETA: 53s - loss: 0.4003 - accuracy: 0.878 - ETA: 52s - loss: 0.4009 - accuracy: 0.878 - ETA: 52s - loss: 0.4028 - accuracy: 0.877 - ETA: 51s - loss: 0.4000 - accuracy: 0.878 - ETA: 50s - loss: 0.3993 - accuracy: 0.878 - ETA: 49s - loss: 0.4007 - accuracy: 0.878 - ETA: 48s - loss: 0.3995 - accuracy: 0.879 - ETA: 47s - loss: 0.4012 - accuracy: 0.878 - ETA: 46s - loss: 0.3990 - accuracy: 0.878 - ETA: 46s - loss: 0.3998 - accuracy: 0.878 - ETA: 45s - loss: 0.4022 - accuracy: 0.878 - ETA: 44s - loss: 0.4024 - accuracy: 0.877 - ETA: 43s - loss: 0.4026 - accuracy: 0.877 - ETA: 42s - loss: 0.4063 - accuracy: 0.876 - ETA: 41s - loss: 0.4068 - accuracy: 0.876 - ETA: 40s - loss: 0.4063 - accuracy: 0.876 - ETA: 39s - loss: 0.4071 - accuracy: 0.876 - ETA: 38s - loss: 0.4099 - accuracy: 0.875 - ETA: 37s - loss: 0.4122 - accuracy: 0.875 - ETA: 36s - loss: 0.4132 - accuracy: 0.874 - ETA: 35s - loss: 0.4130 - accuracy: 0.874 - ETA: 35s - loss: 0.4150 - accuracy: 0.873 - ETA: 34s - loss: 0.4130 - accuracy: 0.874 - ETA: 33s - loss: 0.4149 - accuracy: 0.873 - ETA: 32s - loss: 0.4171 - accuracy: 0.872 - ETA: 31s - loss: 0.4162 - accuracy: 0.873 - ETA: 30s - loss: 0.4183 - accuracy: 0.872 - ETA: 29s - loss: 0.4177 - accuracy: 0.872 - ETA: 28s - loss: 0.4165 - accuracy: 0.873 - ETA: 27s - loss: 0.4198 - accuracy: 0.872 - ETA: 26s - loss: 0.4189 - accuracy: 0.872 - ETA: 26s - loss: 0.4180 - accuracy: 0.873 - ETA: 25s - loss: 0.4166 - accuracy: 0.873 - ETA: 24s - loss: 0.4171 - accuracy: 0.872 - ETA: 23s - loss: 0.4167 - accuracy: 0.873 - ETA: 22s - loss: 0.4193 - accuracy: 0.872 - ETA: 21s - loss: 0.4187 - accuracy: 0.873 - ETA: 20s - loss: 0.4175 - accuracy: 0.873 - ETA: 19s - loss: 0.4166 - accuracy: 0.873 - ETA: 18s - loss: 0.4166 - accuracy: 0.873 - ETA: 17s - loss: 0.4152 - accuracy: 0.874 - ETA: 16s - loss: 0.4143 - accuracy: 0.874 - ETA: 16s - loss: 0.4155 - accuracy: 0.873 - ETA: 15s - loss: 0.4143 - accuracy: 0.874 - ETA: 14s - loss: 0.4164 - accuracy: 0.873 - ETA: 13s - loss: 0.4164 - accuracy: 0.873 - ETA: 12s - loss: 0.4168 - accuracy: 0.873 - ETA: 11s - loss: 0.4157 - accuracy: 0.873 - ETA: 10s - loss: 0.4161 - accuracy: 0.873 - ETA: 9s - loss: 0.4166 - accuracy: 0.873 - ETA: 8s - loss: 0.4152 - accuracy: 0.87 - ETA: 7s - loss: 0.4151 - accuracy: 0.87 - ETA: 7s - loss: 0.4138 - accuracy: 0.87 - ETA: 6s - loss: 0.4149 - accuracy: 0.87 - ETA: 5s - loss: 0.4146 - accuracy: 0.87 - ETA: 4s - loss: 0.4155 - accuracy: 0.87 - ETA: 3s - loss: 0.4158 - accuracy: 0.87 - ETA: 2s - loss: 0.4173 - accuracy: 0.87 - ETA: 1s - loss: 0.4175 - accuracy: 0.87 - ETA: 0s - loss: 0.4161 - accuracy: 0.87 - 105s 8ms/step - loss: 0.4183 - accuracy: 0.8734 - val_loss: 2.9440 - val_accuracy: 0.3494\n",
      "Epoch 54/100\n",
      "13022/13022 [==============================] - ETA: 1:36 - loss: 0.4751 - accuracy: 0.85 - ETA: 1:35 - loss: 0.4038 - accuracy: 0.87 - ETA: 1:35 - loss: 0.4808 - accuracy: 0.84 - ETA: 1:33 - loss: 0.5145 - accuracy: 0.85 - ETA: 1:32 - loss: 0.4944 - accuracy: 0.86 - ETA: 1:31 - loss: 0.4923 - accuracy: 0.86 - ETA: 1:29 - loss: 0.4834 - accuracy: 0.86 - ETA: 1:28 - loss: 0.4816 - accuracy: 0.86 - ETA: 1:27 - loss: 0.4827 - accuracy: 0.86 - ETA: 1:26 - loss: 0.4808 - accuracy: 0.86 - ETA: 1:25 - loss: 0.4766 - accuracy: 0.86 - ETA: 1:24 - loss: 0.4908 - accuracy: 0.85 - ETA: 1:23 - loss: 0.4831 - accuracy: 0.86 - ETA: 1:22 - loss: 0.4831 - accuracy: 0.85 - ETA: 1:21 - loss: 0.4804 - accuracy: 0.86 - ETA: 1:20 - loss: 0.4785 - accuracy: 0.86 - ETA: 1:18 - loss: 0.4764 - accuracy: 0.86 - ETA: 1:17 - loss: 0.4647 - accuracy: 0.86 - ETA: 1:16 - loss: 0.4619 - accuracy: 0.86 - ETA: 1:15 - loss: 0.4500 - accuracy: 0.86 - ETA: 1:15 - loss: 0.4462 - accuracy: 0.87 - ETA: 1:14 - loss: 0.4510 - accuracy: 0.87 - ETA: 1:13 - loss: 0.4502 - accuracy: 0.87 - ETA: 1:12 - loss: 0.4475 - accuracy: 0.87 - ETA: 1:11 - loss: 0.4502 - accuracy: 0.87 - ETA: 1:10 - loss: 0.4517 - accuracy: 0.87 - ETA: 1:09 - loss: 0.4459 - accuracy: 0.87 - ETA: 1:07 - loss: 0.4451 - accuracy: 0.87 - ETA: 1:07 - loss: 0.4414 - accuracy: 0.87 - ETA: 1:06 - loss: 0.4444 - accuracy: 0.87 - ETA: 1:05 - loss: 0.4414 - accuracy: 0.87 - ETA: 1:03 - loss: 0.4382 - accuracy: 0.87 - ETA: 1:03 - loss: 0.4363 - accuracy: 0.87 - ETA: 1:02 - loss: 0.4322 - accuracy: 0.87 - ETA: 1:01 - loss: 0.4354 - accuracy: 0.87 - ETA: 1:00 - loss: 0.4397 - accuracy: 0.87 - ETA: 59s - loss: 0.4391 - accuracy: 0.8725 - ETA: 58s - loss: 0.4385 - accuracy: 0.873 - ETA: 57s - loss: 0.4378 - accuracy: 0.873 - ETA: 56s - loss: 0.4368 - accuracy: 0.874 - ETA: 55s - loss: 0.4356 - accuracy: 0.873 - ETA: 54s - loss: 0.4306 - accuracy: 0.874 - ETA: 53s - loss: 0.4273 - accuracy: 0.875 - ETA: 53s - loss: 0.4277 - accuracy: 0.874 - ETA: 52s - loss: 0.4286 - accuracy: 0.873 - ETA: 51s - loss: 0.4281 - accuracy: 0.873 - ETA: 50s - loss: 0.4257 - accuracy: 0.874 - ETA: 49s - loss: 0.4254 - accuracy: 0.873 - ETA: 48s - loss: 0.4273 - accuracy: 0.873 - ETA: 47s - loss: 0.4271 - accuracy: 0.872 - ETA: 46s - loss: 0.4253 - accuracy: 0.873 - ETA: 45s - loss: 0.4272 - accuracy: 0.872 - ETA: 44s - loss: 0.4229 - accuracy: 0.874 - ETA: 43s - loss: 0.4235 - accuracy: 0.873 - ETA: 42s - loss: 0.4240 - accuracy: 0.873 - ETA: 41s - loss: 0.4223 - accuracy: 0.874 - ETA: 41s - loss: 0.4202 - accuracy: 0.875 - ETA: 40s - loss: 0.4186 - accuracy: 0.875 - ETA: 39s - loss: 0.4193 - accuracy: 0.876 - ETA: 38s - loss: 0.4199 - accuracy: 0.875 - ETA: 37s - loss: 0.4219 - accuracy: 0.875 - ETA: 36s - loss: 0.4224 - accuracy: 0.875 - ETA: 35s - loss: 0.4211 - accuracy: 0.875 - ETA: 34s - loss: 0.4196 - accuracy: 0.875 - ETA: 33s - loss: 0.4173 - accuracy: 0.876 - ETA: 32s - loss: 0.4185 - accuracy: 0.875 - ETA: 31s - loss: 0.4178 - accuracy: 0.876 - ETA: 30s - loss: 0.4188 - accuracy: 0.875 - ETA: 30s - loss: 0.4189 - accuracy: 0.875 - ETA: 29s - loss: 0.4212 - accuracy: 0.874 - ETA: 28s - loss: 0.4212 - accuracy: 0.874 - ETA: 27s - loss: 0.4207 - accuracy: 0.875 - ETA: 26s - loss: 0.4206 - accuracy: 0.875 - ETA: 25s - loss: 0.4188 - accuracy: 0.876 - ETA: 24s - loss: 0.4173 - accuracy: 0.876 - ETA: 23s - loss: 0.4179 - accuracy: 0.876 - ETA: 22s - loss: 0.4205 - accuracy: 0.876 - ETA: 21s - loss: 0.4213 - accuracy: 0.875 - ETA: 20s - loss: 0.4223 - accuracy: 0.875 - ETA: 20s - loss: 0.4227 - accuracy: 0.875 - ETA: 19s - loss: 0.4221 - accuracy: 0.875 - ETA: 18s - loss: 0.4227 - accuracy: 0.875 - ETA: 17s - loss: 0.4217 - accuracy: 0.876 - ETA: 16s - loss: 0.4217 - accuracy: 0.876 - ETA: 15s - loss: 0.4211 - accuracy: 0.876 - ETA: 14s - loss: 0.4216 - accuracy: 0.876 - ETA: 13s - loss: 0.4224 - accuracy: 0.876 - ETA: 12s - loss: 0.4225 - accuracy: 0.876 - ETA: 11s - loss: 0.4222 - accuracy: 0.876 - ETA: 10s - loss: 0.4206 - accuracy: 0.876 - ETA: 9s - loss: 0.4201 - accuracy: 0.876 - ETA: 8s - loss: 0.4203 - accuracy: 0.87 - ETA: 8s - loss: 0.4201 - accuracy: 0.87 - ETA: 7s - loss: 0.4222 - accuracy: 0.87 - ETA: 6s - loss: 0.4234 - accuracy: 0.87 - ETA: 5s - loss: 0.4230 - accuracy: 0.87 - ETA: 4s - loss: 0.4223 - accuracy: 0.87 - ETA: 3s - loss: 0.4231 - accuracy: 0.87 - ETA: 2s - loss: 0.4215 - accuracy: 0.87 - ETA: 1s - loss: 0.4235 - accuracy: 0.87 - ETA: 0s - loss: 0.4223 - accuracy: 0.87 - 106s 8ms/step - loss: 0.4221 - accuracy: 0.8767 - val_loss: 2.9340 - val_accuracy: 0.3796\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 0.5255 - accuracy: 0.87 - ETA: 1:29 - loss: 0.4524 - accuracy: 0.89 - ETA: 1:28 - loss: 0.4551 - accuracy: 0.87 - ETA: 1:27 - loss: 0.4532 - accuracy: 0.87 - ETA: 1:26 - loss: 0.4749 - accuracy: 0.87 - ETA: 1:25 - loss: 0.4846 - accuracy: 0.87 - ETA: 1:24 - loss: 0.4992 - accuracy: 0.86 - ETA: 1:24 - loss: 0.4786 - accuracy: 0.86 - ETA: 1:23 - loss: 0.4802 - accuracy: 0.86 - ETA: 1:23 - loss: 0.4552 - accuracy: 0.87 - ETA: 1:22 - loss: 0.4674 - accuracy: 0.86 - ETA: 1:21 - loss: 0.4587 - accuracy: 0.87 - ETA: 1:21 - loss: 0.4540 - accuracy: 0.87 - ETA: 1:20 - loss: 0.4554 - accuracy: 0.86 - ETA: 1:19 - loss: 0.4517 - accuracy: 0.86 - ETA: 1:18 - loss: 0.4527 - accuracy: 0.86 - ETA: 1:17 - loss: 0.4526 - accuracy: 0.86 - ETA: 1:16 - loss: 0.4513 - accuracy: 0.86 - ETA: 1:15 - loss: 0.4529 - accuracy: 0.87 - ETA: 1:14 - loss: 0.4431 - accuracy: 0.87 - ETA: 1:13 - loss: 0.4442 - accuracy: 0.87 - ETA: 1:12 - loss: 0.4351 - accuracy: 0.87 - ETA: 1:11 - loss: 0.4292 - accuracy: 0.87 - ETA: 1:10 - loss: 0.4387 - accuracy: 0.87 - ETA: 1:09 - loss: 0.4437 - accuracy: 0.86 - ETA: 1:08 - loss: 0.4422 - accuracy: 0.86 - ETA: 1:08 - loss: 0.4463 - accuracy: 0.86 - ETA: 1:07 - loss: 0.4457 - accuracy: 0.86 - ETA: 1:06 - loss: 0.4424 - accuracy: 0.86 - ETA: 1:05 - loss: 0.4420 - accuracy: 0.86 - ETA: 1:05 - loss: 0.4425 - accuracy: 0.86 - ETA: 1:04 - loss: 0.4426 - accuracy: 0.86 - ETA: 1:03 - loss: 0.4379 - accuracy: 0.87 - ETA: 1:02 - loss: 0.4377 - accuracy: 0.87 - ETA: 1:01 - loss: 0.4346 - accuracy: 0.87 - ETA: 1:00 - loss: 0.4312 - accuracy: 0.87 - ETA: 59s - loss: 0.4292 - accuracy: 0.8752 - ETA: 58s - loss: 0.4305 - accuracy: 0.874 - ETA: 57s - loss: 0.4298 - accuracy: 0.874 - ETA: 56s - loss: 0.4289 - accuracy: 0.874 - ETA: 55s - loss: 0.4283 - accuracy: 0.875 - ETA: 54s - loss: 0.4261 - accuracy: 0.875 - ETA: 53s - loss: 0.4231 - accuracy: 0.875 - ETA: 52s - loss: 0.4216 - accuracy: 0.876 - ETA: 51s - loss: 0.4174 - accuracy: 0.877 - ETA: 50s - loss: 0.4156 - accuracy: 0.877 - ETA: 49s - loss: 0.4178 - accuracy: 0.876 - ETA: 48s - loss: 0.4188 - accuracy: 0.876 - ETA: 47s - loss: 0.4157 - accuracy: 0.878 - ETA: 47s - loss: 0.4216 - accuracy: 0.876 - ETA: 46s - loss: 0.4209 - accuracy: 0.877 - ETA: 45s - loss: 0.4205 - accuracy: 0.877 - ETA: 44s - loss: 0.4229 - accuracy: 0.876 - ETA: 43s - loss: 0.4218 - accuracy: 0.875 - ETA: 42s - loss: 0.4227 - accuracy: 0.875 - ETA: 41s - loss: 0.4216 - accuracy: 0.875 - ETA: 40s - loss: 0.4232 - accuracy: 0.875 - ETA: 39s - loss: 0.4222 - accuracy: 0.875 - ETA: 38s - loss: 0.4243 - accuracy: 0.875 - ETA: 37s - loss: 0.4244 - accuracy: 0.875 - ETA: 37s - loss: 0.4254 - accuracy: 0.874 - ETA: 36s - loss: 0.4245 - accuracy: 0.874 - ETA: 35s - loss: 0.4243 - accuracy: 0.874 - ETA: 34s - loss: 0.4228 - accuracy: 0.874 - ETA: 33s - loss: 0.4205 - accuracy: 0.875 - ETA: 32s - loss: 0.4195 - accuracy: 0.875 - ETA: 31s - loss: 0.4186 - accuracy: 0.875 - ETA: 30s - loss: 0.4210 - accuracy: 0.874 - ETA: 29s - loss: 0.4189 - accuracy: 0.874 - ETA: 28s - loss: 0.4188 - accuracy: 0.874 - ETA: 28s - loss: 0.4196 - accuracy: 0.874 - ETA: 27s - loss: 0.4197 - accuracy: 0.874 - ETA: 26s - loss: 0.4200 - accuracy: 0.873 - ETA: 25s - loss: 0.4186 - accuracy: 0.874 - ETA: 24s - loss: 0.4211 - accuracy: 0.874 - ETA: 23s - loss: 0.4231 - accuracy: 0.873 - ETA: 22s - loss: 0.4223 - accuracy: 0.873 - ETA: 21s - loss: 0.4199 - accuracy: 0.874 - ETA: 20s - loss: 0.4202 - accuracy: 0.874 - ETA: 19s - loss: 0.4198 - accuracy: 0.874 - ETA: 18s - loss: 0.4195 - accuracy: 0.874 - ETA: 17s - loss: 0.4201 - accuracy: 0.874 - ETA: 17s - loss: 0.4224 - accuracy: 0.874 - ETA: 16s - loss: 0.4217 - accuracy: 0.874 - ETA: 15s - loss: 0.4221 - accuracy: 0.874 - ETA: 14s - loss: 0.4218 - accuracy: 0.874 - ETA: 13s - loss: 0.4212 - accuracy: 0.875 - ETA: 12s - loss: 0.4201 - accuracy: 0.875 - ETA: 11s - loss: 0.4196 - accuracy: 0.875 - ETA: 10s - loss: 0.4206 - accuracy: 0.875 - ETA: 9s - loss: 0.4191 - accuracy: 0.875 - ETA: 8s - loss: 0.4177 - accuracy: 0.87 - ETA: 7s - loss: 0.4171 - accuracy: 0.87 - ETA: 7s - loss: 0.4182 - accuracy: 0.87 - ETA: 6s - loss: 0.4175 - accuracy: 0.87 - ETA: 5s - loss: 0.4178 - accuracy: 0.87 - ETA: 4s - loss: 0.4171 - accuracy: 0.87 - ETA: 3s - loss: 0.4164 - accuracy: 0.87 - ETA: 2s - loss: 0.4163 - accuracy: 0.87 - ETA: 1s - loss: 0.4157 - accuracy: 0.87 - ETA: 0s - loss: 0.4159 - accuracy: 0.87 - 106s 8ms/step - loss: 0.4146 - accuracy: 0.8775 - val_loss: 2.9583 - val_accuracy: 0.3574\n",
      "Epoch 56/100\n",
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.4191 - accuracy: 0.87 - ETA: 1:31 - loss: 0.3522 - accuracy: 0.89 - ETA: 1:29 - loss: 0.3610 - accuracy: 0.88 - ETA: 1:28 - loss: 0.3596 - accuracy: 0.88 - ETA: 1:27 - loss: 0.3588 - accuracy: 0.89 - ETA: 1:26 - loss: 0.3806 - accuracy: 0.88 - ETA: 1:25 - loss: 0.3784 - accuracy: 0.88 - ETA: 1:24 - loss: 0.4050 - accuracy: 0.87 - ETA: 1:24 - loss: 0.4006 - accuracy: 0.87 - ETA: 1:23 - loss: 0.3935 - accuracy: 0.87 - ETA: 1:22 - loss: 0.4033 - accuracy: 0.87 - ETA: 1:21 - loss: 0.3961 - accuracy: 0.87 - ETA: 1:20 - loss: 0.4010 - accuracy: 0.87 - ETA: 1:19 - loss: 0.3973 - accuracy: 0.87 - ETA: 1:18 - loss: 0.3938 - accuracy: 0.87 - ETA: 1:17 - loss: 0.3892 - accuracy: 0.87 - ETA: 1:16 - loss: 0.3939 - accuracy: 0.88 - ETA: 1:16 - loss: 0.3878 - accuracy: 0.88 - ETA: 1:15 - loss: 0.3765 - accuracy: 0.88 - ETA: 1:14 - loss: 0.3763 - accuracy: 0.88 - ETA: 1:13 - loss: 0.3682 - accuracy: 0.88 - ETA: 1:12 - loss: 0.3650 - accuracy: 0.88 - ETA: 1:12 - loss: 0.3657 - accuracy: 0.88 - ETA: 1:11 - loss: 0.3755 - accuracy: 0.88 - ETA: 1:10 - loss: 0.3758 - accuracy: 0.88 - ETA: 1:09 - loss: 0.3713 - accuracy: 0.88 - ETA: 1:08 - loss: 0.3666 - accuracy: 0.88 - ETA: 1:07 - loss: 0.3698 - accuracy: 0.88 - ETA: 1:06 - loss: 0.3704 - accuracy: 0.88 - ETA: 1:05 - loss: 0.3688 - accuracy: 0.88 - ETA: 1:04 - loss: 0.3709 - accuracy: 0.88 - ETA: 1:03 - loss: 0.3728 - accuracy: 0.88 - ETA: 1:02 - loss: 0.3773 - accuracy: 0.88 - ETA: 1:01 - loss: 0.3798 - accuracy: 0.88 - ETA: 1:00 - loss: 0.3796 - accuracy: 0.88 - ETA: 59s - loss: 0.3792 - accuracy: 0.8843 - ETA: 58s - loss: 0.3800 - accuracy: 0.884 - ETA: 58s - loss: 0.3775 - accuracy: 0.884 - ETA: 57s - loss: 0.3795 - accuracy: 0.885 - ETA: 56s - loss: 0.3818 - accuracy: 0.883 - ETA: 55s - loss: 0.3823 - accuracy: 0.883 - ETA: 54s - loss: 0.3807 - accuracy: 0.884 - ETA: 53s - loss: 0.3805 - accuracy: 0.884 - ETA: 52s - loss: 0.3807 - accuracy: 0.884 - ETA: 51s - loss: 0.3815 - accuracy: 0.884 - ETA: 50s - loss: 0.3805 - accuracy: 0.884 - ETA: 49s - loss: 0.3805 - accuracy: 0.884 - ETA: 48s - loss: 0.3853 - accuracy: 0.884 - ETA: 47s - loss: 0.3852 - accuracy: 0.884 - ETA: 46s - loss: 0.3842 - accuracy: 0.884 - ETA: 45s - loss: 0.3841 - accuracy: 0.885 - ETA: 45s - loss: 0.3884 - accuracy: 0.884 - ETA: 44s - loss: 0.3909 - accuracy: 0.883 - ETA: 43s - loss: 0.3904 - accuracy: 0.884 - ETA: 42s - loss: 0.3918 - accuracy: 0.883 - ETA: 41s - loss: 0.3905 - accuracy: 0.883 - ETA: 40s - loss: 0.3897 - accuracy: 0.883 - ETA: 39s - loss: 0.3903 - accuracy: 0.883 - ETA: 38s - loss: 0.3891 - accuracy: 0.883 - ETA: 37s - loss: 0.3928 - accuracy: 0.882 - ETA: 37s - loss: 0.3939 - accuracy: 0.881 - ETA: 36s - loss: 0.3939 - accuracy: 0.881 - ETA: 35s - loss: 0.3953 - accuracy: 0.881 - ETA: 34s - loss: 0.3946 - accuracy: 0.881 - ETA: 33s - loss: 0.3952 - accuracy: 0.881 - ETA: 32s - loss: 0.3953 - accuracy: 0.881 - ETA: 31s - loss: 0.3939 - accuracy: 0.881 - ETA: 30s - loss: 0.3938 - accuracy: 0.881 - ETA: 29s - loss: 0.3954 - accuracy: 0.880 - ETA: 28s - loss: 0.3938 - accuracy: 0.881 - ETA: 27s - loss: 0.3928 - accuracy: 0.881 - ETA: 27s - loss: 0.3932 - accuracy: 0.881 - ETA: 26s - loss: 0.3929 - accuracy: 0.881 - ETA: 25s - loss: 0.3922 - accuracy: 0.881 - ETA: 24s - loss: 0.3925 - accuracy: 0.882 - ETA: 23s - loss: 0.3946 - accuracy: 0.881 - ETA: 22s - loss: 0.3939 - accuracy: 0.881 - ETA: 21s - loss: 0.3926 - accuracy: 0.881 - ETA: 20s - loss: 0.3913 - accuracy: 0.881 - ETA: 19s - loss: 0.3910 - accuracy: 0.882 - ETA: 18s - loss: 0.3911 - accuracy: 0.882 - ETA: 17s - loss: 0.3898 - accuracy: 0.882 - ETA: 17s - loss: 0.3885 - accuracy: 0.883 - ETA: 16s - loss: 0.3880 - accuracy: 0.883 - ETA: 15s - loss: 0.3878 - accuracy: 0.883 - ETA: 14s - loss: 0.3879 - accuracy: 0.883 - ETA: 13s - loss: 0.3882 - accuracy: 0.883 - ETA: 12s - loss: 0.3877 - accuracy: 0.883 - ETA: 11s - loss: 0.3880 - accuracy: 0.883 - ETA: 10s - loss: 0.3891 - accuracy: 0.882 - ETA: 9s - loss: 0.3906 - accuracy: 0.882 - ETA: 8s - loss: 0.3917 - accuracy: 0.88 - ETA: 7s - loss: 0.3900 - accuracy: 0.88 - ETA: 7s - loss: 0.3885 - accuracy: 0.88 - ETA: 6s - loss: 0.3881 - accuracy: 0.88 - ETA: 5s - loss: 0.3878 - accuracy: 0.88 - ETA: 4s - loss: 0.3886 - accuracy: 0.88 - ETA: 3s - loss: 0.3876 - accuracy: 0.88 - ETA: 2s - loss: 0.3879 - accuracy: 0.88 - ETA: 1s - loss: 0.3886 - accuracy: 0.88 - ETA: 0s - loss: 0.3902 - accuracy: 0.88 - 106s 8ms/step - loss: 0.3917 - accuracy: 0.8827 - val_loss: 2.9501 - val_accuracy: 0.3698\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:27 - loss: 0.3851 - accuracy: 0.89 - ETA: 1:29 - loss: 0.4822 - accuracy: 0.86 - ETA: 1:26 - loss: 0.4371 - accuracy: 0.87 - ETA: 1:26 - loss: 0.4634 - accuracy: 0.86 - ETA: 1:27 - loss: 0.4552 - accuracy: 0.87 - ETA: 1:26 - loss: 0.4578 - accuracy: 0.87 - ETA: 1:25 - loss: 0.4260 - accuracy: 0.88 - ETA: 1:24 - loss: 0.4032 - accuracy: 0.89 - ETA: 1:24 - loss: 0.4063 - accuracy: 0.88 - ETA: 1:23 - loss: 0.4185 - accuracy: 0.88 - ETA: 1:23 - loss: 0.4078 - accuracy: 0.88 - ETA: 1:22 - loss: 0.4055 - accuracy: 0.88 - ETA: 1:21 - loss: 0.4027 - accuracy: 0.88 - ETA: 1:20 - loss: 0.3911 - accuracy: 0.89 - ETA: 1:20 - loss: 0.3963 - accuracy: 0.88 - ETA: 1:19 - loss: 0.4059 - accuracy: 0.88 - ETA: 1:18 - loss: 0.4065 - accuracy: 0.88 - ETA: 1:17 - loss: 0.4014 - accuracy: 0.88 - ETA: 1:16 - loss: 0.3956 - accuracy: 0.88 - ETA: 1:15 - loss: 0.3986 - accuracy: 0.88 - ETA: 1:14 - loss: 0.4002 - accuracy: 0.88 - ETA: 1:12 - loss: 0.3943 - accuracy: 0.88 - ETA: 1:12 - loss: 0.4009 - accuracy: 0.88 - ETA: 1:11 - loss: 0.4008 - accuracy: 0.88 - ETA: 1:10 - loss: 0.3972 - accuracy: 0.88 - ETA: 1:09 - loss: 0.3999 - accuracy: 0.88 - ETA: 1:08 - loss: 0.4044 - accuracy: 0.88 - ETA: 1:07 - loss: 0.4022 - accuracy: 0.88 - ETA: 1:06 - loss: 0.3971 - accuracy: 0.88 - ETA: 1:05 - loss: 0.3994 - accuracy: 0.88 - ETA: 1:04 - loss: 0.3967 - accuracy: 0.88 - ETA: 1:03 - loss: 0.3929 - accuracy: 0.88 - ETA: 1:02 - loss: 0.3943 - accuracy: 0.88 - ETA: 1:01 - loss: 0.3952 - accuracy: 0.88 - ETA: 1:00 - loss: 0.3924 - accuracy: 0.88 - ETA: 59s - loss: 0.3928 - accuracy: 0.8832 - ETA: 58s - loss: 0.3943 - accuracy: 0.883 - ETA: 57s - loss: 0.3903 - accuracy: 0.884 - ETA: 57s - loss: 0.3939 - accuracy: 0.883 - ETA: 56s - loss: 0.3897 - accuracy: 0.884 - ETA: 55s - loss: 0.3885 - accuracy: 0.884 - ETA: 54s - loss: 0.3856 - accuracy: 0.885 - ETA: 53s - loss: 0.3864 - accuracy: 0.884 - ETA: 52s - loss: 0.3866 - accuracy: 0.884 - ETA: 51s - loss: 0.3855 - accuracy: 0.884 - ETA: 50s - loss: 0.3852 - accuracy: 0.884 - ETA: 49s - loss: 0.3847 - accuracy: 0.884 - ETA: 48s - loss: 0.3860 - accuracy: 0.884 - ETA: 48s - loss: 0.3853 - accuracy: 0.884 - ETA: 47s - loss: 0.3849 - accuracy: 0.884 - ETA: 46s - loss: 0.3851 - accuracy: 0.884 - ETA: 45s - loss: 0.3895 - accuracy: 0.883 - ETA: 44s - loss: 0.3880 - accuracy: 0.883 - ETA: 43s - loss: 0.3895 - accuracy: 0.883 - ETA: 42s - loss: 0.3927 - accuracy: 0.882 - ETA: 41s - loss: 0.3931 - accuracy: 0.881 - ETA: 40s - loss: 0.3926 - accuracy: 0.881 - ETA: 39s - loss: 0.3945 - accuracy: 0.881 - ETA: 39s - loss: 0.3965 - accuracy: 0.881 - ETA: 38s - loss: 0.3972 - accuracy: 0.880 - ETA: 37s - loss: 0.3997 - accuracy: 0.879 - ETA: 36s - loss: 0.4008 - accuracy: 0.879 - ETA: 35s - loss: 0.4022 - accuracy: 0.879 - ETA: 34s - loss: 0.4024 - accuracy: 0.878 - ETA: 33s - loss: 0.4007 - accuracy: 0.879 - ETA: 32s - loss: 0.4020 - accuracy: 0.878 - ETA: 31s - loss: 0.4010 - accuracy: 0.879 - ETA: 30s - loss: 0.4027 - accuracy: 0.878 - ETA: 29s - loss: 0.4030 - accuracy: 0.878 - ETA: 28s - loss: 0.4021 - accuracy: 0.878 - ETA: 28s - loss: 0.4029 - accuracy: 0.878 - ETA: 27s - loss: 0.4037 - accuracy: 0.878 - ETA: 26s - loss: 0.4018 - accuracy: 0.878 - ETA: 25s - loss: 0.4034 - accuracy: 0.878 - ETA: 24s - loss: 0.4021 - accuracy: 0.879 - ETA: 23s - loss: 0.4008 - accuracy: 0.879 - ETA: 22s - loss: 0.4012 - accuracy: 0.879 - ETA: 21s - loss: 0.4021 - accuracy: 0.879 - ETA: 20s - loss: 0.4010 - accuracy: 0.879 - ETA: 19s - loss: 0.4015 - accuracy: 0.879 - ETA: 18s - loss: 0.4016 - accuracy: 0.879 - ETA: 18s - loss: 0.4008 - accuracy: 0.879 - ETA: 17s - loss: 0.4007 - accuracy: 0.880 - ETA: 16s - loss: 0.4042 - accuracy: 0.879 - ETA: 15s - loss: 0.4063 - accuracy: 0.878 - ETA: 14s - loss: 0.4058 - accuracy: 0.878 - ETA: 13s - loss: 0.4039 - accuracy: 0.879 - ETA: 12s - loss: 0.4027 - accuracy: 0.879 - ETA: 11s - loss: 0.4038 - accuracy: 0.879 - ETA: 10s - loss: 0.4032 - accuracy: 0.879 - ETA: 9s - loss: 0.4037 - accuracy: 0.879 - ETA: 8s - loss: 0.4022 - accuracy: 0.88 - ETA: 7s - loss: 0.4011 - accuracy: 0.88 - ETA: 7s - loss: 0.4011 - accuracy: 0.88 - ETA: 6s - loss: 0.4012 - accuracy: 0.88 - ETA: 5s - loss: 0.4006 - accuracy: 0.88 - ETA: 4s - loss: 0.4002 - accuracy: 0.88 - ETA: 3s - loss: 0.4011 - accuracy: 0.88 - ETA: 2s - loss: 0.4002 - accuracy: 0.88 - ETA: 1s - loss: 0.4008 - accuracy: 0.88 - ETA: 0s - loss: 0.4023 - accuracy: 0.88 - 105s 8ms/step - loss: 0.4031 - accuracy: 0.8800 - val_loss: 2.9081 - val_accuracy: 0.3748\n",
      "Epoch 58/100\n",
      "13022/13022 [==============================] - ETA: 1:28 - loss: 0.2578 - accuracy: 0.92 - ETA: 1:29 - loss: 0.3229 - accuracy: 0.91 - ETA: 1:29 - loss: 0.3653 - accuracy: 0.89 - ETA: 1:29 - loss: 0.3659 - accuracy: 0.89 - ETA: 1:28 - loss: 0.3757 - accuracy: 0.89 - ETA: 1:26 - loss: 0.3756 - accuracy: 0.89 - ETA: 1:25 - loss: 0.3429 - accuracy: 0.89 - ETA: 1:24 - loss: 0.3577 - accuracy: 0.89 - ETA: 1:23 - loss: 0.3651 - accuracy: 0.89 - ETA: 1:22 - loss: 0.3717 - accuracy: 0.88 - ETA: 1:21 - loss: 0.3728 - accuracy: 0.88 - ETA: 1:20 - loss: 0.3662 - accuracy: 0.88 - ETA: 1:19 - loss: 0.3734 - accuracy: 0.88 - ETA: 1:18 - loss: 0.3765 - accuracy: 0.88 - ETA: 1:17 - loss: 0.3865 - accuracy: 0.88 - ETA: 1:17 - loss: 0.3809 - accuracy: 0.88 - ETA: 1:16 - loss: 0.3883 - accuracy: 0.88 - ETA: 1:15 - loss: 0.3796 - accuracy: 0.88 - ETA: 1:14 - loss: 0.3935 - accuracy: 0.88 - ETA: 1:13 - loss: 0.3920 - accuracy: 0.88 - ETA: 1:12 - loss: 0.3904 - accuracy: 0.88 - ETA: 1:11 - loss: 0.3914 - accuracy: 0.88 - ETA: 1:10 - loss: 0.3899 - accuracy: 0.88 - ETA: 1:09 - loss: 0.3851 - accuracy: 0.88 - ETA: 1:08 - loss: 0.3798 - accuracy: 0.89 - ETA: 1:08 - loss: 0.3878 - accuracy: 0.88 - ETA: 1:07 - loss: 0.3872 - accuracy: 0.88 - ETA: 1:06 - loss: 0.3876 - accuracy: 0.88 - ETA: 1:05 - loss: 0.3846 - accuracy: 0.88 - ETA: 1:04 - loss: 0.3872 - accuracy: 0.88 - ETA: 1:03 - loss: 0.3913 - accuracy: 0.88 - ETA: 1:02 - loss: 0.3932 - accuracy: 0.88 - ETA: 1:01 - loss: 0.4014 - accuracy: 0.88 - ETA: 1:00 - loss: 0.4041 - accuracy: 0.88 - ETA: 1:00 - loss: 0.4016 - accuracy: 0.88 - ETA: 59s - loss: 0.4030 - accuracy: 0.8848 - ETA: 58s - loss: 0.4045 - accuracy: 0.885 - ETA: 57s - loss: 0.4057 - accuracy: 0.885 - ETA: 56s - loss: 0.4037 - accuracy: 0.885 - ETA: 55s - loss: 0.4019 - accuracy: 0.886 - ETA: 55s - loss: 0.3991 - accuracy: 0.888 - ETA: 54s - loss: 0.3990 - accuracy: 0.888 - ETA: 53s - loss: 0.3986 - accuracy: 0.888 - ETA: 52s - loss: 0.3957 - accuracy: 0.888 - ETA: 51s - loss: 0.3938 - accuracy: 0.889 - ETA: 50s - loss: 0.3929 - accuracy: 0.889 - ETA: 49s - loss: 0.3941 - accuracy: 0.889 - ETA: 48s - loss: 0.3923 - accuracy: 0.888 - ETA: 47s - loss: 0.3938 - accuracy: 0.888 - ETA: 46s - loss: 0.3948 - accuracy: 0.887 - ETA: 46s - loss: 0.3965 - accuracy: 0.887 - ETA: 45s - loss: 0.3983 - accuracy: 0.886 - ETA: 44s - loss: 0.3984 - accuracy: 0.886 - ETA: 43s - loss: 0.3999 - accuracy: 0.886 - ETA: 42s - loss: 0.4005 - accuracy: 0.885 - ETA: 41s - loss: 0.4001 - accuracy: 0.885 - ETA: 40s - loss: 0.3996 - accuracy: 0.886 - ETA: 39s - loss: 0.3978 - accuracy: 0.886 - ETA: 38s - loss: 0.3945 - accuracy: 0.887 - ETA: 38s - loss: 0.3942 - accuracy: 0.887 - ETA: 37s - loss: 0.3974 - accuracy: 0.886 - ETA: 36s - loss: 0.3977 - accuracy: 0.886 - ETA: 35s - loss: 0.3984 - accuracy: 0.886 - ETA: 34s - loss: 0.3966 - accuracy: 0.887 - ETA: 33s - loss: 0.3948 - accuracy: 0.888 - ETA: 32s - loss: 0.3945 - accuracy: 0.887 - ETA: 31s - loss: 0.3947 - accuracy: 0.887 - ETA: 30s - loss: 0.3951 - accuracy: 0.887 - ETA: 29s - loss: 0.3933 - accuracy: 0.887 - ETA: 28s - loss: 0.3944 - accuracy: 0.886 - ETA: 27s - loss: 0.3950 - accuracy: 0.886 - ETA: 27s - loss: 0.3948 - accuracy: 0.886 - ETA: 26s - loss: 0.3965 - accuracy: 0.887 - ETA: 25s - loss: 0.3960 - accuracy: 0.887 - ETA: 24s - loss: 0.3947 - accuracy: 0.887 - ETA: 23s - loss: 0.3934 - accuracy: 0.887 - ETA: 22s - loss: 0.3926 - accuracy: 0.887 - ETA: 21s - loss: 0.3951 - accuracy: 0.887 - ETA: 20s - loss: 0.3962 - accuracy: 0.887 - ETA: 19s - loss: 0.3979 - accuracy: 0.886 - ETA: 18s - loss: 0.3985 - accuracy: 0.886 - ETA: 17s - loss: 0.3976 - accuracy: 0.886 - ETA: 17s - loss: 0.3991 - accuracy: 0.886 - ETA: 16s - loss: 0.4004 - accuracy: 0.886 - ETA: 15s - loss: 0.4008 - accuracy: 0.886 - ETA: 14s - loss: 0.4018 - accuracy: 0.885 - ETA: 13s - loss: 0.4021 - accuracy: 0.885 - ETA: 12s - loss: 0.4011 - accuracy: 0.885 - ETA: 11s - loss: 0.4000 - accuracy: 0.886 - ETA: 10s - loss: 0.3994 - accuracy: 0.886 - ETA: 9s - loss: 0.3980 - accuracy: 0.886 - ETA: 8s - loss: 0.3970 - accuracy: 0.88 - ETA: 7s - loss: 0.3970 - accuracy: 0.88 - ETA: 7s - loss: 0.3969 - accuracy: 0.88 - ETA: 6s - loss: 0.3958 - accuracy: 0.88 - ETA: 5s - loss: 0.3960 - accuracy: 0.88 - ETA: 4s - loss: 0.3957 - accuracy: 0.88 - ETA: 3s - loss: 0.3947 - accuracy: 0.88 - ETA: 2s - loss: 0.3961 - accuracy: 0.88 - ETA: 1s - loss: 0.3968 - accuracy: 0.88 - ETA: 0s - loss: 0.3951 - accuracy: 0.88 - 106s 8ms/step - loss: 0.3964 - accuracy: 0.8866 - val_loss: 3.0023 - val_accuracy: 0.3632\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.4052 - accuracy: 0.88 - ETA: 1:31 - loss: 0.3825 - accuracy: 0.88 - ETA: 1:31 - loss: 0.3419 - accuracy: 0.89 - ETA: 1:33 - loss: 0.3446 - accuracy: 0.89 - ETA: 1:31 - loss: 0.3504 - accuracy: 0.89 - ETA: 1:29 - loss: 0.3634 - accuracy: 0.89 - ETA: 1:28 - loss: 0.3552 - accuracy: 0.89 - ETA: 1:26 - loss: 0.3623 - accuracy: 0.89 - ETA: 1:25 - loss: 0.3523 - accuracy: 0.90 - ETA: 1:24 - loss: 0.3548 - accuracy: 0.89 - ETA: 1:23 - loss: 0.3626 - accuracy: 0.89 - ETA: 1:21 - loss: 0.3628 - accuracy: 0.89 - ETA: 1:20 - loss: 0.3631 - accuracy: 0.89 - ETA: 1:19 - loss: 0.3641 - accuracy: 0.89 - ETA: 1:18 - loss: 0.3776 - accuracy: 0.89 - ETA: 1:17 - loss: 0.3806 - accuracy: 0.89 - ETA: 1:16 - loss: 0.3789 - accuracy: 0.89 - ETA: 1:15 - loss: 0.3762 - accuracy: 0.89 - ETA: 1:14 - loss: 0.3712 - accuracy: 0.89 - ETA: 1:14 - loss: 0.3772 - accuracy: 0.89 - ETA: 1:13 - loss: 0.3779 - accuracy: 0.89 - ETA: 1:12 - loss: 0.3770 - accuracy: 0.89 - ETA: 1:12 - loss: 0.3733 - accuracy: 0.89 - ETA: 1:11 - loss: 0.3779 - accuracy: 0.89 - ETA: 1:10 - loss: 0.3920 - accuracy: 0.89 - ETA: 1:09 - loss: 0.3887 - accuracy: 0.89 - ETA: 1:08 - loss: 0.3892 - accuracy: 0.89 - ETA: 1:07 - loss: 0.3910 - accuracy: 0.88 - ETA: 1:06 - loss: 0.3906 - accuracy: 0.88 - ETA: 1:05 - loss: 0.3861 - accuracy: 0.88 - ETA: 1:04 - loss: 0.3905 - accuracy: 0.88 - ETA: 1:03 - loss: 0.3923 - accuracy: 0.88 - ETA: 1:02 - loss: 0.3916 - accuracy: 0.88 - ETA: 1:01 - loss: 0.3906 - accuracy: 0.88 - ETA: 1:00 - loss: 0.3887 - accuracy: 0.88 - ETA: 59s - loss: 0.3893 - accuracy: 0.8874 - ETA: 58s - loss: 0.3951 - accuracy: 0.886 - ETA: 57s - loss: 0.3935 - accuracy: 0.886 - ETA: 57s - loss: 0.3944 - accuracy: 0.886 - ETA: 56s - loss: 0.3941 - accuracy: 0.886 - ETA: 55s - loss: 0.3953 - accuracy: 0.885 - ETA: 54s - loss: 0.3948 - accuracy: 0.885 - ETA: 53s - loss: 0.3943 - accuracy: 0.884 - ETA: 52s - loss: 0.3947 - accuracy: 0.884 - ETA: 51s - loss: 0.3980 - accuracy: 0.884 - ETA: 50s - loss: 0.3990 - accuracy: 0.884 - ETA: 49s - loss: 0.3960 - accuracy: 0.884 - ETA: 48s - loss: 0.3969 - accuracy: 0.883 - ETA: 47s - loss: 0.3959 - accuracy: 0.883 - ETA: 46s - loss: 0.3948 - accuracy: 0.883 - ETA: 46s - loss: 0.3926 - accuracy: 0.883 - ETA: 45s - loss: 0.3917 - accuracy: 0.884 - ETA: 44s - loss: 0.3933 - accuracy: 0.883 - ETA: 43s - loss: 0.3919 - accuracy: 0.883 - ETA: 42s - loss: 0.3909 - accuracy: 0.884 - ETA: 41s - loss: 0.3959 - accuracy: 0.883 - ETA: 40s - loss: 0.3949 - accuracy: 0.883 - ETA: 39s - loss: 0.3982 - accuracy: 0.883 - ETA: 38s - loss: 0.3982 - accuracy: 0.884 - ETA: 37s - loss: 0.3983 - accuracy: 0.883 - ETA: 37s - loss: 0.3969 - accuracy: 0.884 - ETA: 36s - loss: 0.3948 - accuracy: 0.884 - ETA: 35s - loss: 0.3926 - accuracy: 0.885 - ETA: 34s - loss: 0.3919 - accuracy: 0.885 - ETA: 33s - loss: 0.3929 - accuracy: 0.885 - ETA: 32s - loss: 0.3936 - accuracy: 0.884 - ETA: 31s - loss: 0.3946 - accuracy: 0.884 - ETA: 30s - loss: 0.3957 - accuracy: 0.884 - ETA: 29s - loss: 0.3951 - accuracy: 0.884 - ETA: 28s - loss: 0.3958 - accuracy: 0.884 - ETA: 28s - loss: 0.3977 - accuracy: 0.883 - ETA: 27s - loss: 0.3961 - accuracy: 0.884 - ETA: 26s - loss: 0.3956 - accuracy: 0.885 - ETA: 25s - loss: 0.3967 - accuracy: 0.885 - ETA: 24s - loss: 0.3952 - accuracy: 0.885 - ETA: 23s - loss: 0.3950 - accuracy: 0.885 - ETA: 22s - loss: 0.3947 - accuracy: 0.885 - ETA: 21s - loss: 0.3938 - accuracy: 0.885 - ETA: 20s - loss: 0.3921 - accuracy: 0.886 - ETA: 19s - loss: 0.3928 - accuracy: 0.885 - ETA: 18s - loss: 0.3928 - accuracy: 0.885 - ETA: 17s - loss: 0.3927 - accuracy: 0.885 - ETA: 17s - loss: 0.3926 - accuracy: 0.885 - ETA: 16s - loss: 0.3925 - accuracy: 0.885 - ETA: 15s - loss: 0.3912 - accuracy: 0.885 - ETA: 14s - loss: 0.3899 - accuracy: 0.886 - ETA: 13s - loss: 0.3895 - accuracy: 0.886 - ETA: 12s - loss: 0.3896 - accuracy: 0.886 - ETA: 11s - loss: 0.3915 - accuracy: 0.885 - ETA: 10s - loss: 0.3916 - accuracy: 0.886 - ETA: 9s - loss: 0.3913 - accuracy: 0.886 - ETA: 8s - loss: 0.3937 - accuracy: 0.88 - ETA: 7s - loss: 0.3929 - accuracy: 0.88 - ETA: 7s - loss: 0.3929 - accuracy: 0.88 - ETA: 6s - loss: 0.3925 - accuracy: 0.88 - ETA: 5s - loss: 0.3953 - accuracy: 0.88 - ETA: 4s - loss: 0.3964 - accuracy: 0.88 - ETA: 3s - loss: 0.3961 - accuracy: 0.88 - ETA: 2s - loss: 0.3962 - accuracy: 0.88 - ETA: 1s - loss: 0.3951 - accuracy: 0.88 - ETA: 0s - loss: 0.3949 - accuracy: 0.88 - 105s 8ms/step - loss: 0.3950 - accuracy: 0.8852 - val_loss: 3.0858 - val_accuracy: 0.3351\n",
      "Epoch 60/100\n",
      "13022/13022 [==============================] - ETA: 1:28 - loss: 0.4584 - accuracy: 0.86 - ETA: 1:27 - loss: 0.3740 - accuracy: 0.88 - ETA: 1:27 - loss: 0.3863 - accuracy: 0.87 - ETA: 1:26 - loss: 0.3908 - accuracy: 0.87 - ETA: 1:25 - loss: 0.3667 - accuracy: 0.88 - ETA: 1:24 - loss: 0.3811 - accuracy: 0.87 - ETA: 1:24 - loss: 0.3801 - accuracy: 0.87 - ETA: 1:23 - loss: 0.3796 - accuracy: 0.87 - ETA: 1:22 - loss: 0.3866 - accuracy: 0.87 - ETA: 1:21 - loss: 0.3925 - accuracy: 0.87 - ETA: 1:21 - loss: 0.4026 - accuracy: 0.86 - ETA: 1:20 - loss: 0.3840 - accuracy: 0.87 - ETA: 1:20 - loss: 0.3856 - accuracy: 0.87 - ETA: 1:19 - loss: 0.3847 - accuracy: 0.87 - ETA: 1:18 - loss: 0.3827 - accuracy: 0.87 - ETA: 1:17 - loss: 0.3755 - accuracy: 0.87 - ETA: 1:17 - loss: 0.3759 - accuracy: 0.87 - ETA: 1:16 - loss: 0.3835 - accuracy: 0.87 - ETA: 1:15 - loss: 0.3827 - accuracy: 0.87 - ETA: 1:14 - loss: 0.3788 - accuracy: 0.87 - ETA: 1:13 - loss: 0.3782 - accuracy: 0.87 - ETA: 1:13 - loss: 0.3817 - accuracy: 0.87 - ETA: 1:11 - loss: 0.3788 - accuracy: 0.87 - ETA: 1:11 - loss: 0.3793 - accuracy: 0.87 - ETA: 1:10 - loss: 0.3751 - accuracy: 0.87 - ETA: 1:09 - loss: 0.3757 - accuracy: 0.87 - ETA: 1:08 - loss: 0.3773 - accuracy: 0.87 - ETA: 1:07 - loss: 0.3758 - accuracy: 0.87 - ETA: 1:06 - loss: 0.3759 - accuracy: 0.87 - ETA: 1:05 - loss: 0.3775 - accuracy: 0.87 - ETA: 1:04 - loss: 0.3812 - accuracy: 0.87 - ETA: 1:03 - loss: 0.3792 - accuracy: 0.87 - ETA: 1:03 - loss: 0.3847 - accuracy: 0.87 - ETA: 1:01 - loss: 0.3841 - accuracy: 0.87 - ETA: 1:01 - loss: 0.3838 - accuracy: 0.87 - ETA: 1:00 - loss: 0.3824 - accuracy: 0.87 - ETA: 59s - loss: 0.3856 - accuracy: 0.8746 - ETA: 58s - loss: 0.3856 - accuracy: 0.875 - ETA: 57s - loss: 0.3818 - accuracy: 0.876 - ETA: 56s - loss: 0.3839 - accuracy: 0.876 - ETA: 55s - loss: 0.3831 - accuracy: 0.875 - ETA: 54s - loss: 0.3814 - accuracy: 0.876 - ETA: 53s - loss: 0.3807 - accuracy: 0.876 - ETA: 52s - loss: 0.3776 - accuracy: 0.877 - ETA: 51s - loss: 0.3797 - accuracy: 0.877 - ETA: 50s - loss: 0.3800 - accuracy: 0.878 - ETA: 49s - loss: 0.3789 - accuracy: 0.878 - ETA: 48s - loss: 0.3783 - accuracy: 0.878 - ETA: 48s - loss: 0.3822 - accuracy: 0.877 - ETA: 47s - loss: 0.3835 - accuracy: 0.878 - ETA: 46s - loss: 0.3833 - accuracy: 0.877 - ETA: 45s - loss: 0.3830 - accuracy: 0.877 - ETA: 44s - loss: 0.3834 - accuracy: 0.877 - ETA: 43s - loss: 0.3805 - accuracy: 0.879 - ETA: 42s - loss: 0.3789 - accuracy: 0.879 - ETA: 41s - loss: 0.3794 - accuracy: 0.879 - ETA: 40s - loss: 0.3834 - accuracy: 0.879 - ETA: 39s - loss: 0.3844 - accuracy: 0.879 - ETA: 38s - loss: 0.3863 - accuracy: 0.879 - ETA: 37s - loss: 0.3880 - accuracy: 0.878 - ETA: 36s - loss: 0.3888 - accuracy: 0.878 - ETA: 36s - loss: 0.3889 - accuracy: 0.878 - ETA: 35s - loss: 0.3902 - accuracy: 0.878 - ETA: 34s - loss: 0.3890 - accuracy: 0.878 - ETA: 33s - loss: 0.3869 - accuracy: 0.879 - ETA: 32s - loss: 0.3851 - accuracy: 0.880 - ETA: 31s - loss: 0.3845 - accuracy: 0.880 - ETA: 30s - loss: 0.3834 - accuracy: 0.880 - ETA: 29s - loss: 0.3831 - accuracy: 0.880 - ETA: 28s - loss: 0.3816 - accuracy: 0.880 - ETA: 27s - loss: 0.3822 - accuracy: 0.879 - ETA: 26s - loss: 0.3825 - accuracy: 0.879 - ETA: 26s - loss: 0.3838 - accuracy: 0.879 - ETA: 25s - loss: 0.3846 - accuracy: 0.879 - ETA: 24s - loss: 0.3852 - accuracy: 0.878 - ETA: 23s - loss: 0.3856 - accuracy: 0.878 - ETA: 22s - loss: 0.3862 - accuracy: 0.878 - ETA: 21s - loss: 0.3860 - accuracy: 0.877 - ETA: 20s - loss: 0.3845 - accuracy: 0.878 - ETA: 19s - loss: 0.3843 - accuracy: 0.878 - ETA: 18s - loss: 0.3844 - accuracy: 0.878 - ETA: 17s - loss: 0.3839 - accuracy: 0.879 - ETA: 16s - loss: 0.3839 - accuracy: 0.879 - ETA: 16s - loss: 0.3832 - accuracy: 0.879 - ETA: 15s - loss: 0.3841 - accuracy: 0.879 - ETA: 14s - loss: 0.3846 - accuracy: 0.879 - ETA: 13s - loss: 0.3846 - accuracy: 0.879 - ETA: 12s - loss: 0.3844 - accuracy: 0.879 - ETA: 11s - loss: 0.3838 - accuracy: 0.880 - ETA: 10s - loss: 0.3843 - accuracy: 0.879 - ETA: 9s - loss: 0.3831 - accuracy: 0.879 - ETA: 8s - loss: 0.3852 - accuracy: 0.88 - ETA: 7s - loss: 0.3859 - accuracy: 0.87 - ETA: 6s - loss: 0.3855 - accuracy: 0.88 - ETA: 6s - loss: 0.3849 - accuracy: 0.88 - ETA: 5s - loss: 0.3848 - accuracy: 0.88 - ETA: 4s - loss: 0.3861 - accuracy: 0.87 - ETA: 3s - loss: 0.3853 - accuracy: 0.87 - ETA: 2s - loss: 0.3843 - accuracy: 0.88 - ETA: 1s - loss: 0.3828 - accuracy: 0.88 - ETA: 0s - loss: 0.3831 - accuracy: 0.88 - 106s 8ms/step - loss: 0.3848 - accuracy: 0.8804 - val_loss: 3.0390 - val_accuracy: 0.3649\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.3860 - accuracy: 0.87 - ETA: 1:30 - loss: 0.4580 - accuracy: 0.86 - ETA: 1:28 - loss: 0.4304 - accuracy: 0.86 - ETA: 1:27 - loss: 0.3985 - accuracy: 0.88 - ETA: 1:26 - loss: 0.3970 - accuracy: 0.88 - ETA: 1:25 - loss: 0.3834 - accuracy: 0.89 - ETA: 1:24 - loss: 0.3729 - accuracy: 0.89 - ETA: 1:23 - loss: 0.3795 - accuracy: 0.89 - ETA: 1:22 - loss: 0.3892 - accuracy: 0.88 - ETA: 1:21 - loss: 0.4113 - accuracy: 0.88 - ETA: 1:20 - loss: 0.4057 - accuracy: 0.87 - ETA: 1:19 - loss: 0.4011 - accuracy: 0.88 - ETA: 1:18 - loss: 0.3991 - accuracy: 0.88 - ETA: 1:17 - loss: 0.3863 - accuracy: 0.88 - ETA: 1:16 - loss: 0.3875 - accuracy: 0.88 - ETA: 1:16 - loss: 0.3858 - accuracy: 0.88 - ETA: 1:15 - loss: 0.3880 - accuracy: 0.88 - ETA: 1:14 - loss: 0.3853 - accuracy: 0.88 - ETA: 1:14 - loss: 0.3882 - accuracy: 0.88 - ETA: 1:13 - loss: 0.3926 - accuracy: 0.88 - ETA: 1:12 - loss: 0.3980 - accuracy: 0.88 - ETA: 1:11 - loss: 0.3927 - accuracy: 0.88 - ETA: 1:10 - loss: 0.3876 - accuracy: 0.88 - ETA: 1:09 - loss: 0.3811 - accuracy: 0.88 - ETA: 1:08 - loss: 0.3862 - accuracy: 0.88 - ETA: 1:08 - loss: 0.3868 - accuracy: 0.88 - ETA: 1:07 - loss: 0.3868 - accuracy: 0.88 - ETA: 1:06 - loss: 0.3910 - accuracy: 0.88 - ETA: 1:05 - loss: 0.3830 - accuracy: 0.88 - ETA: 1:04 - loss: 0.3864 - accuracy: 0.88 - ETA: 1:03 - loss: 0.3883 - accuracy: 0.88 - ETA: 1:02 - loss: 0.3862 - accuracy: 0.88 - ETA: 1:01 - loss: 0.3928 - accuracy: 0.88 - ETA: 1:00 - loss: 0.3938 - accuracy: 0.88 - ETA: 1:00 - loss: 0.3934 - accuracy: 0.88 - ETA: 59s - loss: 0.3978 - accuracy: 0.8828 - ETA: 58s - loss: 0.3984 - accuracy: 0.882 - ETA: 57s - loss: 0.4005 - accuracy: 0.882 - ETA: 56s - loss: 0.3988 - accuracy: 0.883 - ETA: 55s - loss: 0.3956 - accuracy: 0.884 - ETA: 54s - loss: 0.3995 - accuracy: 0.883 - ETA: 53s - loss: 0.3981 - accuracy: 0.883 - ETA: 53s - loss: 0.3951 - accuracy: 0.884 - ETA: 52s - loss: 0.3953 - accuracy: 0.884 - ETA: 51s - loss: 0.3925 - accuracy: 0.885 - ETA: 50s - loss: 0.3919 - accuracy: 0.884 - ETA: 49s - loss: 0.3935 - accuracy: 0.883 - ETA: 48s - loss: 0.3923 - accuracy: 0.884 - ETA: 47s - loss: 0.3893 - accuracy: 0.884 - ETA: 46s - loss: 0.3883 - accuracy: 0.884 - ETA: 45s - loss: 0.3897 - accuracy: 0.884 - ETA: 44s - loss: 0.3909 - accuracy: 0.884 - ETA: 44s - loss: 0.3909 - accuracy: 0.884 - ETA: 43s - loss: 0.3944 - accuracy: 0.884 - ETA: 42s - loss: 0.3920 - accuracy: 0.884 - ETA: 41s - loss: 0.3929 - accuracy: 0.884 - ETA: 40s - loss: 0.3904 - accuracy: 0.885 - ETA: 39s - loss: 0.3920 - accuracy: 0.885 - ETA: 38s - loss: 0.3927 - accuracy: 0.884 - ETA: 37s - loss: 0.3912 - accuracy: 0.884 - ETA: 36s - loss: 0.3904 - accuracy: 0.883 - ETA: 36s - loss: 0.3879 - accuracy: 0.884 - ETA: 35s - loss: 0.3871 - accuracy: 0.884 - ETA: 34s - loss: 0.3848 - accuracy: 0.885 - ETA: 33s - loss: 0.3850 - accuracy: 0.885 - ETA: 32s - loss: 0.3830 - accuracy: 0.886 - ETA: 31s - loss: 0.3829 - accuracy: 0.886 - ETA: 30s - loss: 0.3826 - accuracy: 0.886 - ETA: 29s - loss: 0.3826 - accuracy: 0.886 - ETA: 28s - loss: 0.3828 - accuracy: 0.886 - ETA: 27s - loss: 0.3824 - accuracy: 0.886 - ETA: 26s - loss: 0.3821 - accuracy: 0.886 - ETA: 26s - loss: 0.3816 - accuracy: 0.886 - ETA: 25s - loss: 0.3826 - accuracy: 0.885 - ETA: 24s - loss: 0.3818 - accuracy: 0.886 - ETA: 23s - loss: 0.3809 - accuracy: 0.886 - ETA: 22s - loss: 0.3792 - accuracy: 0.886 - ETA: 21s - loss: 0.3777 - accuracy: 0.887 - ETA: 20s - loss: 0.3785 - accuracy: 0.887 - ETA: 19s - loss: 0.3801 - accuracy: 0.887 - ETA: 18s - loss: 0.3816 - accuracy: 0.886 - ETA: 17s - loss: 0.3821 - accuracy: 0.886 - ETA: 16s - loss: 0.3809 - accuracy: 0.886 - ETA: 16s - loss: 0.3828 - accuracy: 0.886 - ETA: 15s - loss: 0.3834 - accuracy: 0.886 - ETA: 14s - loss: 0.3821 - accuracy: 0.886 - ETA: 13s - loss: 0.3818 - accuracy: 0.886 - ETA: 12s - loss: 0.3813 - accuracy: 0.886 - ETA: 11s - loss: 0.3797 - accuracy: 0.886 - ETA: 10s - loss: 0.3799 - accuracy: 0.886 - ETA: 9s - loss: 0.3782 - accuracy: 0.887 - ETA: 8s - loss: 0.3764 - accuracy: 0.88 - ETA: 7s - loss: 0.3766 - accuracy: 0.88 - ETA: 7s - loss: 0.3770 - accuracy: 0.88 - ETA: 6s - loss: 0.3767 - accuracy: 0.88 - ETA: 5s - loss: 0.3782 - accuracy: 0.88 - ETA: 4s - loss: 0.3785 - accuracy: 0.88 - ETA: 3s - loss: 0.3783 - accuracy: 0.88 - ETA: 2s - loss: 0.3776 - accuracy: 0.88 - ETA: 1s - loss: 0.3774 - accuracy: 0.88 - ETA: 0s - loss: 0.3781 - accuracy: 0.88 - 105s 8ms/step - loss: 0.3788 - accuracy: 0.8870 - val_loss: 3.1641 - val_accuracy: 0.3462\n",
      "Epoch 62/100\n",
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.4723 - accuracy: 0.86 - ETA: 1:29 - loss: 0.4148 - accuracy: 0.90 - ETA: 1:27 - loss: 0.4258 - accuracy: 0.88 - ETA: 1:27 - loss: 0.3996 - accuracy: 0.89 - ETA: 1:25 - loss: 0.4372 - accuracy: 0.88 - ETA: 1:25 - loss: 0.4069 - accuracy: 0.89 - ETA: 1:25 - loss: 0.3948 - accuracy: 0.89 - ETA: 1:25 - loss: 0.3789 - accuracy: 0.89 - ETA: 1:24 - loss: 0.3929 - accuracy: 0.89 - ETA: 1:24 - loss: 0.3912 - accuracy: 0.89 - ETA: 1:23 - loss: 0.3800 - accuracy: 0.89 - ETA: 1:22 - loss: 0.3750 - accuracy: 0.89 - ETA: 1:20 - loss: 0.3778 - accuracy: 0.89 - ETA: 1:18 - loss: 0.3704 - accuracy: 0.89 - ETA: 1:15 - loss: 0.3647 - accuracy: 0.89 - ETA: 1:13 - loss: 0.3612 - accuracy: 0.89 - ETA: 1:11 - loss: 0.3678 - accuracy: 0.89 - ETA: 1:10 - loss: 0.3811 - accuracy: 0.89 - ETA: 1:08 - loss: 0.3910 - accuracy: 0.88 - ETA: 1:06 - loss: 0.3961 - accuracy: 0.88 - ETA: 1:05 - loss: 0.3918 - accuracy: 0.88 - ETA: 1:04 - loss: 0.3879 - accuracy: 0.88 - ETA: 1:02 - loss: 0.3886 - accuracy: 0.88 - ETA: 1:01 - loss: 0.3950 - accuracy: 0.88 - ETA: 1:00 - loss: 0.3949 - accuracy: 0.88 - ETA: 58s - loss: 0.3915 - accuracy: 0.8873 - ETA: 57s - loss: 0.3910 - accuracy: 0.886 - ETA: 57s - loss: 0.3920 - accuracy: 0.887 - ETA: 57s - loss: 0.3931 - accuracy: 0.886 - ETA: 56s - loss: 0.3966 - accuracy: 0.886 - ETA: 56s - loss: 0.3983 - accuracy: 0.885 - ETA: 55s - loss: 0.3952 - accuracy: 0.885 - ETA: 55s - loss: 0.3937 - accuracy: 0.885 - ETA: 54s - loss: 0.3940 - accuracy: 0.885 - ETA: 53s - loss: 0.3945 - accuracy: 0.885 - ETA: 53s - loss: 0.3941 - accuracy: 0.885 - ETA: 52s - loss: 0.3928 - accuracy: 0.885 - ETA: 51s - loss: 0.3983 - accuracy: 0.885 - ETA: 51s - loss: 0.4010 - accuracy: 0.884 - ETA: 50s - loss: 0.4017 - accuracy: 0.884 - ETA: 49s - loss: 0.4002 - accuracy: 0.884 - ETA: 49s - loss: 0.3987 - accuracy: 0.884 - ETA: 48s - loss: 0.3988 - accuracy: 0.885 - ETA: 47s - loss: 0.3990 - accuracy: 0.884 - ETA: 46s - loss: 0.3974 - accuracy: 0.884 - ETA: 46s - loss: 0.3975 - accuracy: 0.883 - ETA: 45s - loss: 0.3970 - accuracy: 0.884 - ETA: 44s - loss: 0.3944 - accuracy: 0.884 - ETA: 44s - loss: 0.3928 - accuracy: 0.885 - ETA: 43s - loss: 0.3911 - accuracy: 0.885 - ETA: 42s - loss: 0.3909 - accuracy: 0.885 - ETA: 41s - loss: 0.3885 - accuracy: 0.885 - ETA: 40s - loss: 0.3860 - accuracy: 0.886 - ETA: 40s - loss: 0.3862 - accuracy: 0.887 - ETA: 39s - loss: 0.3863 - accuracy: 0.886 - ETA: 38s - loss: 0.3888 - accuracy: 0.886 - ETA: 37s - loss: 0.3875 - accuracy: 0.886 - ETA: 36s - loss: 0.3877 - accuracy: 0.886 - ETA: 36s - loss: 0.3913 - accuracy: 0.886 - ETA: 35s - loss: 0.3918 - accuracy: 0.886 - ETA: 34s - loss: 0.3904 - accuracy: 0.886 - ETA: 33s - loss: 0.3882 - accuracy: 0.886 - ETA: 32s - loss: 0.3893 - accuracy: 0.885 - ETA: 32s - loss: 0.3889 - accuracy: 0.885 - ETA: 31s - loss: 0.3896 - accuracy: 0.885 - ETA: 30s - loss: 0.3878 - accuracy: 0.886 - ETA: 29s - loss: 0.3874 - accuracy: 0.886 - ETA: 28s - loss: 0.3860 - accuracy: 0.886 - ETA: 27s - loss: 0.3897 - accuracy: 0.886 - ETA: 27s - loss: 0.3893 - accuracy: 0.886 - ETA: 26s - loss: 0.3903 - accuracy: 0.885 - ETA: 25s - loss: 0.3880 - accuracy: 0.886 - ETA: 24s - loss: 0.3894 - accuracy: 0.885 - ETA: 23s - loss: 0.3877 - accuracy: 0.886 - ETA: 22s - loss: 0.3881 - accuracy: 0.886 - ETA: 22s - loss: 0.3871 - accuracy: 0.887 - ETA: 21s - loss: 0.3879 - accuracy: 0.887 - ETA: 20s - loss: 0.3902 - accuracy: 0.886 - ETA: 19s - loss: 0.3896 - accuracy: 0.886 - ETA: 18s - loss: 0.3897 - accuracy: 0.886 - ETA: 17s - loss: 0.3883 - accuracy: 0.887 - ETA: 17s - loss: 0.3878 - accuracy: 0.886 - ETA: 16s - loss: 0.3870 - accuracy: 0.887 - ETA: 15s - loss: 0.3892 - accuracy: 0.886 - ETA: 14s - loss: 0.3883 - accuracy: 0.886 - ETA: 13s - loss: 0.3869 - accuracy: 0.887 - ETA: 12s - loss: 0.3849 - accuracy: 0.887 - ETA: 11s - loss: 0.3855 - accuracy: 0.887 - ETA: 11s - loss: 0.3844 - accuracy: 0.888 - ETA: 10s - loss: 0.3865 - accuracy: 0.887 - ETA: 9s - loss: 0.3849 - accuracy: 0.888 - ETA: 8s - loss: 0.3843 - accuracy: 0.88 - ETA: 7s - loss: 0.3849 - accuracy: 0.88 - ETA: 6s - loss: 0.3844 - accuracy: 0.88 - ETA: 5s - loss: 0.3842 - accuracy: 0.88 - ETA: 5s - loss: 0.3843 - accuracy: 0.88 - ETA: 4s - loss: 0.3845 - accuracy: 0.88 - ETA: 3s - loss: 0.3851 - accuracy: 0.88 - ETA: 2s - loss: 0.3847 - accuracy: 0.88 - ETA: 1s - loss: 0.3843 - accuracy: 0.88 - ETA: 0s - loss: 0.3831 - accuracy: 0.88 - 101s 8ms/step - loss: 0.3840 - accuracy: 0.8889 - val_loss: 3.1402 - val_accuracy: 0.3496\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:36 - loss: 0.3352 - accuracy: 0.92 - ETA: 1:35 - loss: 0.4505 - accuracy: 0.88 - ETA: 1:35 - loss: 0.4356 - accuracy: 0.88 - ETA: 1:34 - loss: 0.3970 - accuracy: 0.89 - ETA: 1:32 - loss: 0.3753 - accuracy: 0.89 - ETA: 1:29 - loss: 0.3667 - accuracy: 0.89 - ETA: 1:27 - loss: 0.3608 - accuracy: 0.89 - ETA: 1:26 - loss: 0.3633 - accuracy: 0.89 - ETA: 1:25 - loss: 0.3635 - accuracy: 0.89 - ETA: 1:24 - loss: 0.3648 - accuracy: 0.89 - ETA: 1:23 - loss: 0.3569 - accuracy: 0.90 - ETA: 1:22 - loss: 0.3672 - accuracy: 0.89 - ETA: 1:20 - loss: 0.3644 - accuracy: 0.89 - ETA: 1:19 - loss: 0.3606 - accuracy: 0.89 - ETA: 1:18 - loss: 0.3664 - accuracy: 0.89 - ETA: 1:17 - loss: 0.3687 - accuracy: 0.89 - ETA: 1:16 - loss: 0.3654 - accuracy: 0.89 - ETA: 1:15 - loss: 0.3635 - accuracy: 0.89 - ETA: 1:14 - loss: 0.3626 - accuracy: 0.89 - ETA: 1:13 - loss: 0.3565 - accuracy: 0.89 - ETA: 1:13 - loss: 0.3599 - accuracy: 0.89 - ETA: 1:12 - loss: 0.3561 - accuracy: 0.89 - ETA: 1:11 - loss: 0.3570 - accuracy: 0.89 - ETA: 1:10 - loss: 0.3557 - accuracy: 0.89 - ETA: 1:09 - loss: 0.3538 - accuracy: 0.89 - ETA: 1:08 - loss: 0.3523 - accuracy: 0.89 - ETA: 1:07 - loss: 0.3567 - accuracy: 0.89 - ETA: 1:06 - loss: 0.3570 - accuracy: 0.89 - ETA: 1:05 - loss: 0.3607 - accuracy: 0.89 - ETA: 1:04 - loss: 0.3700 - accuracy: 0.89 - ETA: 1:03 - loss: 0.3694 - accuracy: 0.89 - ETA: 1:02 - loss: 0.3691 - accuracy: 0.89 - ETA: 1:01 - loss: 0.3681 - accuracy: 0.89 - ETA: 1:00 - loss: 0.3705 - accuracy: 0.89 - ETA: 59s - loss: 0.3691 - accuracy: 0.8944 - ETA: 59s - loss: 0.3719 - accuracy: 0.893 - ETA: 58s - loss: 0.3697 - accuracy: 0.893 - ETA: 57s - loss: 0.3696 - accuracy: 0.893 - ETA: 56s - loss: 0.3735 - accuracy: 0.892 - ETA: 55s - loss: 0.3716 - accuracy: 0.892 - ETA: 54s - loss: 0.3738 - accuracy: 0.892 - ETA: 54s - loss: 0.3755 - accuracy: 0.891 - ETA: 53s - loss: 0.3774 - accuracy: 0.890 - ETA: 52s - loss: 0.3803 - accuracy: 0.889 - ETA: 51s - loss: 0.3801 - accuracy: 0.888 - ETA: 50s - loss: 0.3804 - accuracy: 0.888 - ETA: 49s - loss: 0.3810 - accuracy: 0.888 - ETA: 48s - loss: 0.3823 - accuracy: 0.888 - ETA: 47s - loss: 0.3812 - accuracy: 0.888 - ETA: 47s - loss: 0.3790 - accuracy: 0.889 - ETA: 46s - loss: 0.3819 - accuracy: 0.888 - ETA: 45s - loss: 0.3821 - accuracy: 0.889 - ETA: 44s - loss: 0.3816 - accuracy: 0.889 - ETA: 43s - loss: 0.3831 - accuracy: 0.889 - ETA: 42s - loss: 0.3826 - accuracy: 0.889 - ETA: 41s - loss: 0.3855 - accuracy: 0.889 - ETA: 40s - loss: 0.3862 - accuracy: 0.889 - ETA: 39s - loss: 0.3898 - accuracy: 0.888 - ETA: 38s - loss: 0.3922 - accuracy: 0.887 - ETA: 37s - loss: 0.3917 - accuracy: 0.887 - ETA: 36s - loss: 0.3912 - accuracy: 0.887 - ETA: 35s - loss: 0.3907 - accuracy: 0.887 - ETA: 35s - loss: 0.3907 - accuracy: 0.887 - ETA: 34s - loss: 0.3918 - accuracy: 0.887 - ETA: 33s - loss: 0.3942 - accuracy: 0.886 - ETA: 32s - loss: 0.3945 - accuracy: 0.887 - ETA: 31s - loss: 0.3954 - accuracy: 0.886 - ETA: 30s - loss: 0.3968 - accuracy: 0.886 - ETA: 29s - loss: 0.3964 - accuracy: 0.886 - ETA: 28s - loss: 0.3964 - accuracy: 0.886 - ETA: 27s - loss: 0.3954 - accuracy: 0.886 - ETA: 26s - loss: 0.3947 - accuracy: 0.886 - ETA: 25s - loss: 0.3932 - accuracy: 0.887 - ETA: 25s - loss: 0.3944 - accuracy: 0.887 - ETA: 24s - loss: 0.3924 - accuracy: 0.887 - ETA: 23s - loss: 0.3919 - accuracy: 0.887 - ETA: 22s - loss: 0.3928 - accuracy: 0.887 - ETA: 21s - loss: 0.3939 - accuracy: 0.886 - ETA: 20s - loss: 0.3944 - accuracy: 0.886 - ETA: 19s - loss: 0.3962 - accuracy: 0.885 - ETA: 18s - loss: 0.3970 - accuracy: 0.885 - ETA: 17s - loss: 0.3961 - accuracy: 0.885 - ETA: 16s - loss: 0.3969 - accuracy: 0.886 - ETA: 16s - loss: 0.3952 - accuracy: 0.886 - ETA: 15s - loss: 0.3954 - accuracy: 0.886 - ETA: 14s - loss: 0.3970 - accuracy: 0.886 - ETA: 13s - loss: 0.3972 - accuracy: 0.886 - ETA: 12s - loss: 0.3964 - accuracy: 0.886 - ETA: 11s - loss: 0.3972 - accuracy: 0.886 - ETA: 10s - loss: 0.3969 - accuracy: 0.885 - ETA: 9s - loss: 0.3966 - accuracy: 0.886 - ETA: 8s - loss: 0.3967 - accuracy: 0.88 - ETA: 7s - loss: 0.3963 - accuracy: 0.88 - ETA: 6s - loss: 0.3948 - accuracy: 0.88 - ETA: 6s - loss: 0.3950 - accuracy: 0.88 - ETA: 5s - loss: 0.3944 - accuracy: 0.88 - ETA: 4s - loss: 0.3943 - accuracy: 0.88 - ETA: 3s - loss: 0.3947 - accuracy: 0.88 - ETA: 2s - loss: 0.3928 - accuracy: 0.88 - ETA: 1s - loss: 0.3941 - accuracy: 0.88 - ETA: 0s - loss: 0.3937 - accuracy: 0.88 - 105s 8ms/step - loss: 0.3926 - accuracy: 0.8864 - val_loss: 2.8660 - val_accuracy: 0.3603\n",
      "Epoch 64/100\n",
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.4199 - accuracy: 0.85 - ETA: 1:30 - loss: 0.3443 - accuracy: 0.89 - ETA: 1:32 - loss: 0.2945 - accuracy: 0.91 - ETA: 1:30 - loss: 0.3065 - accuracy: 0.90 - ETA: 1:28 - loss: 0.3380 - accuracy: 0.90 - ETA: 1:26 - loss: 0.3287 - accuracy: 0.90 - ETA: 1:25 - loss: 0.3377 - accuracy: 0.90 - ETA: 1:23 - loss: 0.3297 - accuracy: 0.90 - ETA: 1:22 - loss: 0.3364 - accuracy: 0.89 - ETA: 1:22 - loss: 0.3449 - accuracy: 0.89 - ETA: 1:22 - loss: 0.3537 - accuracy: 0.89 - ETA: 1:20 - loss: 0.3589 - accuracy: 0.89 - ETA: 1:19 - loss: 0.3685 - accuracy: 0.88 - ETA: 1:18 - loss: 0.3800 - accuracy: 0.88 - ETA: 1:17 - loss: 0.3748 - accuracy: 0.88 - ETA: 1:16 - loss: 0.3837 - accuracy: 0.88 - ETA: 1:16 - loss: 0.3881 - accuracy: 0.88 - ETA: 1:15 - loss: 0.3916 - accuracy: 0.88 - ETA: 1:14 - loss: 0.3890 - accuracy: 0.88 - ETA: 1:13 - loss: 0.3872 - accuracy: 0.88 - ETA: 1:12 - loss: 0.3826 - accuracy: 0.88 - ETA: 1:11 - loss: 0.3770 - accuracy: 0.88 - ETA: 1:10 - loss: 0.3716 - accuracy: 0.89 - ETA: 1:09 - loss: 0.3788 - accuracy: 0.88 - ETA: 1:08 - loss: 0.3740 - accuracy: 0.89 - ETA: 1:07 - loss: 0.3737 - accuracy: 0.89 - ETA: 1:07 - loss: 0.3743 - accuracy: 0.88 - ETA: 1:06 - loss: 0.3775 - accuracy: 0.88 - ETA: 1:05 - loss: 0.3848 - accuracy: 0.88 - ETA: 1:04 - loss: 0.3851 - accuracy: 0.88 - ETA: 1:03 - loss: 0.3814 - accuracy: 0.88 - ETA: 1:02 - loss: 0.3775 - accuracy: 0.88 - ETA: 1:01 - loss: 0.3762 - accuracy: 0.88 - ETA: 1:00 - loss: 0.3744 - accuracy: 0.88 - ETA: 59s - loss: 0.3759 - accuracy: 0.8873 - ETA: 58s - loss: 0.3760 - accuracy: 0.887 - ETA: 57s - loss: 0.3872 - accuracy: 0.884 - ETA: 56s - loss: 0.3860 - accuracy: 0.885 - ETA: 56s - loss: 0.3853 - accuracy: 0.885 - ETA: 55s - loss: 0.3839 - accuracy: 0.885 - ETA: 54s - loss: 0.3838 - accuracy: 0.886 - ETA: 53s - loss: 0.3859 - accuracy: 0.886 - ETA: 52s - loss: 0.3857 - accuracy: 0.886 - ETA: 51s - loss: 0.3849 - accuracy: 0.886 - ETA: 50s - loss: 0.3870 - accuracy: 0.886 - ETA: 49s - loss: 0.3882 - accuracy: 0.886 - ETA: 48s - loss: 0.3922 - accuracy: 0.886 - ETA: 48s - loss: 0.3886 - accuracy: 0.887 - ETA: 47s - loss: 0.3839 - accuracy: 0.888 - ETA: 46s - loss: 0.3847 - accuracy: 0.888 - ETA: 45s - loss: 0.3860 - accuracy: 0.887 - ETA: 44s - loss: 0.3836 - accuracy: 0.888 - ETA: 43s - loss: 0.3796 - accuracy: 0.889 - ETA: 42s - loss: 0.3798 - accuracy: 0.889 - ETA: 41s - loss: 0.3779 - accuracy: 0.889 - ETA: 40s - loss: 0.3795 - accuracy: 0.889 - ETA: 40s - loss: 0.3787 - accuracy: 0.889 - ETA: 39s - loss: 0.3797 - accuracy: 0.889 - ETA: 38s - loss: 0.3789 - accuracy: 0.889 - ETA: 37s - loss: 0.3786 - accuracy: 0.889 - ETA: 36s - loss: 0.3782 - accuracy: 0.889 - ETA: 35s - loss: 0.3777 - accuracy: 0.889 - ETA: 34s - loss: 0.3771 - accuracy: 0.889 - ETA: 33s - loss: 0.3761 - accuracy: 0.889 - ETA: 32s - loss: 0.3778 - accuracy: 0.888 - ETA: 32s - loss: 0.3763 - accuracy: 0.888 - ETA: 31s - loss: 0.3759 - accuracy: 0.888 - ETA: 30s - loss: 0.3766 - accuracy: 0.888 - ETA: 29s - loss: 0.3766 - accuracy: 0.887 - ETA: 28s - loss: 0.3772 - accuracy: 0.887 - ETA: 27s - loss: 0.3757 - accuracy: 0.888 - ETA: 26s - loss: 0.3758 - accuracy: 0.887 - ETA: 25s - loss: 0.3786 - accuracy: 0.887 - ETA: 24s - loss: 0.3784 - accuracy: 0.887 - ETA: 24s - loss: 0.3774 - accuracy: 0.887 - ETA: 23s - loss: 0.3765 - accuracy: 0.887 - ETA: 22s - loss: 0.3778 - accuracy: 0.888 - ETA: 21s - loss: 0.3770 - accuracy: 0.888 - ETA: 20s - loss: 0.3765 - accuracy: 0.888 - ETA: 19s - loss: 0.3766 - accuracy: 0.889 - ETA: 18s - loss: 0.3763 - accuracy: 0.888 - ETA: 17s - loss: 0.3780 - accuracy: 0.888 - ETA: 16s - loss: 0.3784 - accuracy: 0.888 - ETA: 15s - loss: 0.3787 - accuracy: 0.888 - ETA: 15s - loss: 0.3783 - accuracy: 0.888 - ETA: 14s - loss: 0.3785 - accuracy: 0.888 - ETA: 13s - loss: 0.3778 - accuracy: 0.889 - ETA: 12s - loss: 0.3780 - accuracy: 0.889 - ETA: 11s - loss: 0.3778 - accuracy: 0.889 - ETA: 10s - loss: 0.3786 - accuracy: 0.888 - ETA: 9s - loss: 0.3780 - accuracy: 0.888 - ETA: 8s - loss: 0.3779 - accuracy: 0.88 - ETA: 7s - loss: 0.3759 - accuracy: 0.88 - ETA: 6s - loss: 0.3742 - accuracy: 0.89 - ETA: 6s - loss: 0.3745 - accuracy: 0.88 - ETA: 5s - loss: 0.3735 - accuracy: 0.89 - ETA: 4s - loss: 0.3743 - accuracy: 0.89 - ETA: 3s - loss: 0.3746 - accuracy: 0.88 - ETA: 2s - loss: 0.3745 - accuracy: 0.88 - ETA: 1s - loss: 0.3748 - accuracy: 0.89 - ETA: 0s - loss: 0.3740 - accuracy: 0.89 - 105s 8ms/step - loss: 0.3748 - accuracy: 0.8903 - val_loss: 2.9981 - val_accuracy: 0.3594\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.2493 - accuracy: 0.92 - ETA: 1:30 - loss: 0.3029 - accuracy: 0.91 - ETA: 1:28 - loss: 0.3409 - accuracy: 0.89 - ETA: 1:26 - loss: 0.3203 - accuracy: 0.90 - ETA: 1:26 - loss: 0.3234 - accuracy: 0.89 - ETA: 1:25 - loss: 0.3609 - accuracy: 0.88 - ETA: 1:23 - loss: 0.3568 - accuracy: 0.89 - ETA: 1:22 - loss: 0.3710 - accuracy: 0.88 - ETA: 1:21 - loss: 0.3723 - accuracy: 0.88 - ETA: 1:20 - loss: 0.3743 - accuracy: 0.88 - ETA: 1:19 - loss: 0.3846 - accuracy: 0.88 - ETA: 1:19 - loss: 0.3800 - accuracy: 0.88 - ETA: 1:18 - loss: 0.3884 - accuracy: 0.88 - ETA: 1:17 - loss: 0.3829 - accuracy: 0.88 - ETA: 1:16 - loss: 0.3794 - accuracy: 0.88 - ETA: 1:16 - loss: 0.3763 - accuracy: 0.89 - ETA: 1:15 - loss: 0.3732 - accuracy: 0.89 - ETA: 1:14 - loss: 0.3717 - accuracy: 0.89 - ETA: 1:13 - loss: 0.3729 - accuracy: 0.89 - ETA: 1:12 - loss: 0.3784 - accuracy: 0.88 - ETA: 1:11 - loss: 0.3739 - accuracy: 0.88 - ETA: 1:10 - loss: 0.3762 - accuracy: 0.88 - ETA: 1:09 - loss: 0.3784 - accuracy: 0.88 - ETA: 1:08 - loss: 0.3767 - accuracy: 0.88 - ETA: 1:07 - loss: 0.3751 - accuracy: 0.88 - ETA: 1:07 - loss: 0.3765 - accuracy: 0.88 - ETA: 1:06 - loss: 0.3789 - accuracy: 0.88 - ETA: 1:05 - loss: 0.3784 - accuracy: 0.88 - ETA: 1:04 - loss: 0.3817 - accuracy: 0.88 - ETA: 1:03 - loss: 0.3835 - accuracy: 0.88 - ETA: 1:02 - loss: 0.3879 - accuracy: 0.88 - ETA: 1:02 - loss: 0.3892 - accuracy: 0.88 - ETA: 1:01 - loss: 0.3911 - accuracy: 0.88 - ETA: 1:00 - loss: 0.3872 - accuracy: 0.88 - ETA: 59s - loss: 0.3854 - accuracy: 0.8877 - ETA: 58s - loss: 0.3856 - accuracy: 0.887 - ETA: 58s - loss: 0.3846 - accuracy: 0.888 - ETA: 57s - loss: 0.3836 - accuracy: 0.888 - ETA: 56s - loss: 0.3802 - accuracy: 0.890 - ETA: 55s - loss: 0.3804 - accuracy: 0.890 - ETA: 54s - loss: 0.3809 - accuracy: 0.890 - ETA: 53s - loss: 0.3799 - accuracy: 0.890 - ETA: 52s - loss: 0.3826 - accuracy: 0.890 - ETA: 51s - loss: 0.3807 - accuracy: 0.890 - ETA: 50s - loss: 0.3798 - accuracy: 0.890 - ETA: 49s - loss: 0.3819 - accuracy: 0.889 - ETA: 48s - loss: 0.3795 - accuracy: 0.889 - ETA: 48s - loss: 0.3810 - accuracy: 0.888 - ETA: 47s - loss: 0.3761 - accuracy: 0.890 - ETA: 46s - loss: 0.3761 - accuracy: 0.890 - ETA: 45s - loss: 0.3738 - accuracy: 0.890 - ETA: 44s - loss: 0.3726 - accuracy: 0.890 - ETA: 43s - loss: 0.3692 - accuracy: 0.891 - ETA: 42s - loss: 0.3711 - accuracy: 0.891 - ETA: 41s - loss: 0.3699 - accuracy: 0.892 - ETA: 40s - loss: 0.3720 - accuracy: 0.891 - ETA: 40s - loss: 0.3716 - accuracy: 0.891 - ETA: 39s - loss: 0.3706 - accuracy: 0.892 - ETA: 38s - loss: 0.3688 - accuracy: 0.892 - ETA: 37s - loss: 0.3717 - accuracy: 0.892 - ETA: 36s - loss: 0.3699 - accuracy: 0.892 - ETA: 35s - loss: 0.3710 - accuracy: 0.892 - ETA: 34s - loss: 0.3699 - accuracy: 0.892 - ETA: 33s - loss: 0.3706 - accuracy: 0.892 - ETA: 32s - loss: 0.3710 - accuracy: 0.892 - ETA: 31s - loss: 0.3704 - accuracy: 0.893 - ETA: 31s - loss: 0.3698 - accuracy: 0.893 - ETA: 30s - loss: 0.3703 - accuracy: 0.893 - ETA: 29s - loss: 0.3717 - accuracy: 0.892 - ETA: 28s - loss: 0.3723 - accuracy: 0.892 - ETA: 27s - loss: 0.3726 - accuracy: 0.892 - ETA: 26s - loss: 0.3709 - accuracy: 0.892 - ETA: 25s - loss: 0.3697 - accuracy: 0.893 - ETA: 24s - loss: 0.3703 - accuracy: 0.893 - ETA: 23s - loss: 0.3704 - accuracy: 0.894 - ETA: 23s - loss: 0.3689 - accuracy: 0.894 - ETA: 22s - loss: 0.3693 - accuracy: 0.894 - ETA: 21s - loss: 0.3703 - accuracy: 0.893 - ETA: 20s - loss: 0.3709 - accuracy: 0.893 - ETA: 19s - loss: 0.3703 - accuracy: 0.893 - ETA: 18s - loss: 0.3698 - accuracy: 0.894 - ETA: 17s - loss: 0.3702 - accuracy: 0.894 - ETA: 16s - loss: 0.3687 - accuracy: 0.894 - ETA: 15s - loss: 0.3712 - accuracy: 0.893 - ETA: 15s - loss: 0.3703 - accuracy: 0.893 - ETA: 14s - loss: 0.3697 - accuracy: 0.893 - ETA: 13s - loss: 0.3698 - accuracy: 0.893 - ETA: 12s - loss: 0.3718 - accuracy: 0.893 - ETA: 11s - loss: 0.3720 - accuracy: 0.893 - ETA: 10s - loss: 0.3718 - accuracy: 0.893 - ETA: 9s - loss: 0.3717 - accuracy: 0.893 - ETA: 8s - loss: 0.3722 - accuracy: 0.89 - ETA: 7s - loss: 0.3718 - accuracy: 0.89 - ETA: 6s - loss: 0.3734 - accuracy: 0.89 - ETA: 6s - loss: 0.3723 - accuracy: 0.89 - ETA: 5s - loss: 0.3719 - accuracy: 0.89 - ETA: 4s - loss: 0.3720 - accuracy: 0.89 - ETA: 3s - loss: 0.3746 - accuracy: 0.89 - ETA: 2s - loss: 0.3736 - accuracy: 0.89 - ETA: 1s - loss: 0.3729 - accuracy: 0.89 - ETA: 0s - loss: 0.3730 - accuracy: 0.89 - 104s 8ms/step - loss: 0.3725 - accuracy: 0.8941 - val_loss: 3.0502 - val_accuracy: 0.3549\n",
      "Epoch 66/100\n",
      "13022/13022 [==============================] - ETA: 1:20 - loss: 0.3620 - accuracy: 0.91 - ETA: 1:24 - loss: 0.3794 - accuracy: 0.89 - ETA: 1:26 - loss: 0.3683 - accuracy: 0.89 - ETA: 1:27 - loss: 0.3769 - accuracy: 0.89 - ETA: 1:27 - loss: 0.4119 - accuracy: 0.87 - ETA: 1:27 - loss: 0.4218 - accuracy: 0.88 - ETA: 1:28 - loss: 0.4081 - accuracy: 0.88 - ETA: 1:28 - loss: 0.4088 - accuracy: 0.88 - ETA: 1:27 - loss: 0.3970 - accuracy: 0.88 - ETA: 1:27 - loss: 0.3952 - accuracy: 0.88 - ETA: 1:27 - loss: 0.3997 - accuracy: 0.88 - ETA: 1:26 - loss: 0.3990 - accuracy: 0.88 - ETA: 1:25 - loss: 0.3904 - accuracy: 0.88 - ETA: 1:25 - loss: 0.3785 - accuracy: 0.89 - ETA: 1:25 - loss: 0.3700 - accuracy: 0.89 - ETA: 1:24 - loss: 0.3703 - accuracy: 0.89 - ETA: 1:24 - loss: 0.3687 - accuracy: 0.89 - ETA: 1:23 - loss: 0.3623 - accuracy: 0.89 - ETA: 1:22 - loss: 0.3618 - accuracy: 0.89 - ETA: 1:21 - loss: 0.3643 - accuracy: 0.89 - ETA: 1:20 - loss: 0.3604 - accuracy: 0.89 - ETA: 1:18 - loss: 0.3613 - accuracy: 0.89 - ETA: 1:17 - loss: 0.3601 - accuracy: 0.89 - ETA: 1:16 - loss: 0.3616 - accuracy: 0.89 - ETA: 1:15 - loss: 0.3596 - accuracy: 0.89 - ETA: 1:14 - loss: 0.3575 - accuracy: 0.89 - ETA: 1:13 - loss: 0.3634 - accuracy: 0.89 - ETA: 1:11 - loss: 0.3646 - accuracy: 0.89 - ETA: 1:10 - loss: 0.3687 - accuracy: 0.89 - ETA: 1:09 - loss: 0.3690 - accuracy: 0.89 - ETA: 1:08 - loss: 0.3669 - accuracy: 0.89 - ETA: 1:07 - loss: 0.3624 - accuracy: 0.89 - ETA: 1:05 - loss: 0.3600 - accuracy: 0.89 - ETA: 1:04 - loss: 0.3577 - accuracy: 0.89 - ETA: 1:03 - loss: 0.3566 - accuracy: 0.89 - ETA: 1:02 - loss: 0.3533 - accuracy: 0.89 - ETA: 1:01 - loss: 0.3556 - accuracy: 0.89 - ETA: 1:00 - loss: 0.3599 - accuracy: 0.89 - ETA: 59s - loss: 0.3628 - accuracy: 0.8930 - ETA: 58s - loss: 0.3605 - accuracy: 0.893 - ETA: 57s - loss: 0.3602 - accuracy: 0.893 - ETA: 56s - loss: 0.3574 - accuracy: 0.894 - ETA: 55s - loss: 0.3576 - accuracy: 0.893 - ETA: 54s - loss: 0.3592 - accuracy: 0.893 - ETA: 53s - loss: 0.3571 - accuracy: 0.894 - ETA: 52s - loss: 0.3554 - accuracy: 0.895 - ETA: 51s - loss: 0.3535 - accuracy: 0.895 - ETA: 50s - loss: 0.3511 - accuracy: 0.896 - ETA: 49s - loss: 0.3505 - accuracy: 0.897 - ETA: 48s - loss: 0.3474 - accuracy: 0.898 - ETA: 47s - loss: 0.3465 - accuracy: 0.898 - ETA: 46s - loss: 0.3475 - accuracy: 0.898 - ETA: 45s - loss: 0.3502 - accuracy: 0.897 - ETA: 44s - loss: 0.3537 - accuracy: 0.897 - ETA: 43s - loss: 0.3536 - accuracy: 0.896 - ETA: 42s - loss: 0.3527 - accuracy: 0.897 - ETA: 41s - loss: 0.3572 - accuracy: 0.896 - ETA: 40s - loss: 0.3575 - accuracy: 0.896 - ETA: 39s - loss: 0.3597 - accuracy: 0.895 - ETA: 38s - loss: 0.3585 - accuracy: 0.896 - ETA: 38s - loss: 0.3584 - accuracy: 0.895 - ETA: 37s - loss: 0.3571 - accuracy: 0.896 - ETA: 36s - loss: 0.3593 - accuracy: 0.895 - ETA: 35s - loss: 0.3593 - accuracy: 0.895 - ETA: 34s - loss: 0.3609 - accuracy: 0.895 - ETA: 33s - loss: 0.3612 - accuracy: 0.895 - ETA: 32s - loss: 0.3625 - accuracy: 0.894 - ETA: 31s - loss: 0.3638 - accuracy: 0.894 - ETA: 30s - loss: 0.3633 - accuracy: 0.894 - ETA: 29s - loss: 0.3633 - accuracy: 0.894 - ETA: 28s - loss: 0.3645 - accuracy: 0.894 - ETA: 27s - loss: 0.3645 - accuracy: 0.894 - ETA: 26s - loss: 0.3640 - accuracy: 0.894 - ETA: 25s - loss: 0.3634 - accuracy: 0.894 - ETA: 24s - loss: 0.3634 - accuracy: 0.894 - ETA: 23s - loss: 0.3637 - accuracy: 0.894 - ETA: 22s - loss: 0.3631 - accuracy: 0.894 - ETA: 22s - loss: 0.3622 - accuracy: 0.894 - ETA: 21s - loss: 0.3631 - accuracy: 0.894 - ETA: 20s - loss: 0.3650 - accuracy: 0.893 - ETA: 19s - loss: 0.3637 - accuracy: 0.894 - ETA: 18s - loss: 0.3649 - accuracy: 0.893 - ETA: 17s - loss: 0.3640 - accuracy: 0.893 - ETA: 16s - loss: 0.3653 - accuracy: 0.893 - ETA: 15s - loss: 0.3649 - accuracy: 0.893 - ETA: 14s - loss: 0.3663 - accuracy: 0.892 - ETA: 13s - loss: 0.3653 - accuracy: 0.893 - ETA: 12s - loss: 0.3665 - accuracy: 0.892 - ETA: 11s - loss: 0.3669 - accuracy: 0.892 - ETA: 10s - loss: 0.3664 - accuracy: 0.893 - ETA: 9s - loss: 0.3651 - accuracy: 0.893 - ETA: 9s - loss: 0.3657 - accuracy: 0.89 - ETA: 8s - loss: 0.3639 - accuracy: 0.89 - ETA: 7s - loss: 0.3631 - accuracy: 0.89 - ETA: 6s - loss: 0.3626 - accuracy: 0.89 - ETA: 5s - loss: 0.3632 - accuracy: 0.89 - ETA: 4s - loss: 0.3627 - accuracy: 0.89 - ETA: 3s - loss: 0.3629 - accuracy: 0.89 - ETA: 2s - loss: 0.3626 - accuracy: 0.89 - ETA: 1s - loss: 0.3620 - accuracy: 0.89 - ETA: 0s - loss: 0.3623 - accuracy: 0.89 - 107s 8ms/step - loss: 0.3622 - accuracy: 0.8948 - val_loss: 2.9675 - val_accuracy: 0.3654\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:36 - loss: 0.3045 - accuracy: 0.88 - ETA: 1:30 - loss: 0.2886 - accuracy: 0.91 - ETA: 1:29 - loss: 0.3407 - accuracy: 0.89 - ETA: 1:28 - loss: 0.3323 - accuracy: 0.90 - ETA: 1:28 - loss: 0.3520 - accuracy: 0.90 - ETA: 1:26 - loss: 0.4031 - accuracy: 0.89 - ETA: 1:24 - loss: 0.4115 - accuracy: 0.88 - ETA: 1:23 - loss: 0.4186 - accuracy: 0.88 - ETA: 1:22 - loss: 0.4134 - accuracy: 0.88 - ETA: 1:21 - loss: 0.4265 - accuracy: 0.88 - ETA: 1:20 - loss: 0.4196 - accuracy: 0.88 - ETA: 1:20 - loss: 0.4100 - accuracy: 0.88 - ETA: 1:19 - loss: 0.4045 - accuracy: 0.88 - ETA: 1:19 - loss: 0.3979 - accuracy: 0.89 - ETA: 1:18 - loss: 0.3942 - accuracy: 0.89 - ETA: 1:17 - loss: 0.3926 - accuracy: 0.88 - ETA: 1:16 - loss: 0.3850 - accuracy: 0.89 - ETA: 1:15 - loss: 0.3916 - accuracy: 0.88 - ETA: 1:14 - loss: 0.3882 - accuracy: 0.89 - ETA: 1:13 - loss: 0.3862 - accuracy: 0.89 - ETA: 1:12 - loss: 0.3783 - accuracy: 0.89 - ETA: 1:11 - loss: 0.3793 - accuracy: 0.89 - ETA: 1:10 - loss: 0.3824 - accuracy: 0.89 - ETA: 1:09 - loss: 0.3779 - accuracy: 0.89 - ETA: 1:08 - loss: 0.3835 - accuracy: 0.89 - ETA: 1:07 - loss: 0.3812 - accuracy: 0.89 - ETA: 1:07 - loss: 0.3762 - accuracy: 0.89 - ETA: 1:06 - loss: 0.3697 - accuracy: 0.89 - ETA: 1:05 - loss: 0.3741 - accuracy: 0.89 - ETA: 1:04 - loss: 0.3746 - accuracy: 0.89 - ETA: 1:03 - loss: 0.3747 - accuracy: 0.89 - ETA: 1:02 - loss: 0.3770 - accuracy: 0.89 - ETA: 1:01 - loss: 0.3745 - accuracy: 0.89 - ETA: 1:00 - loss: 0.3737 - accuracy: 0.89 - ETA: 59s - loss: 0.3747 - accuracy: 0.8955 - ETA: 59s - loss: 0.3736 - accuracy: 0.895 - ETA: 58s - loss: 0.3719 - accuracy: 0.895 - ETA: 57s - loss: 0.3697 - accuracy: 0.896 - ETA: 56s - loss: 0.3707 - accuracy: 0.895 - ETA: 55s - loss: 0.3691 - accuracy: 0.896 - ETA: 54s - loss: 0.3683 - accuracy: 0.896 - ETA: 53s - loss: 0.3668 - accuracy: 0.897 - ETA: 52s - loss: 0.3664 - accuracy: 0.897 - ETA: 51s - loss: 0.3640 - accuracy: 0.897 - ETA: 51s - loss: 0.3639 - accuracy: 0.897 - ETA: 50s - loss: 0.3639 - accuracy: 0.897 - ETA: 49s - loss: 0.3649 - accuracy: 0.897 - ETA: 48s - loss: 0.3658 - accuracy: 0.897 - ETA: 47s - loss: 0.3627 - accuracy: 0.897 - ETA: 46s - loss: 0.3613 - accuracy: 0.897 - ETA: 45s - loss: 0.3589 - accuracy: 0.898 - ETA: 44s - loss: 0.3582 - accuracy: 0.898 - ETA: 43s - loss: 0.3550 - accuracy: 0.899 - ETA: 42s - loss: 0.3553 - accuracy: 0.899 - ETA: 41s - loss: 0.3548 - accuracy: 0.899 - ETA: 41s - loss: 0.3526 - accuracy: 0.900 - ETA: 40s - loss: 0.3514 - accuracy: 0.900 - ETA: 39s - loss: 0.3516 - accuracy: 0.899 - ETA: 38s - loss: 0.3496 - accuracy: 0.900 - ETA: 37s - loss: 0.3499 - accuracy: 0.900 - ETA: 36s - loss: 0.3518 - accuracy: 0.900 - ETA: 35s - loss: 0.3518 - accuracy: 0.899 - ETA: 34s - loss: 0.3516 - accuracy: 0.900 - ETA: 33s - loss: 0.3501 - accuracy: 0.900 - ETA: 32s - loss: 0.3469 - accuracy: 0.901 - ETA: 32s - loss: 0.3462 - accuracy: 0.901 - ETA: 31s - loss: 0.3459 - accuracy: 0.901 - ETA: 30s - loss: 0.3468 - accuracy: 0.901 - ETA: 29s - loss: 0.3459 - accuracy: 0.901 - ETA: 28s - loss: 0.3473 - accuracy: 0.901 - ETA: 27s - loss: 0.3480 - accuracy: 0.900 - ETA: 26s - loss: 0.3475 - accuracy: 0.900 - ETA: 25s - loss: 0.3490 - accuracy: 0.900 - ETA: 24s - loss: 0.3493 - accuracy: 0.900 - ETA: 23s - loss: 0.3472 - accuracy: 0.900 - ETA: 23s - loss: 0.3479 - accuracy: 0.900 - ETA: 22s - loss: 0.3471 - accuracy: 0.901 - ETA: 21s - loss: 0.3470 - accuracy: 0.901 - ETA: 20s - loss: 0.3502 - accuracy: 0.900 - ETA: 19s - loss: 0.3510 - accuracy: 0.900 - ETA: 18s - loss: 0.3522 - accuracy: 0.899 - ETA: 17s - loss: 0.3523 - accuracy: 0.899 - ETA: 16s - loss: 0.3512 - accuracy: 0.900 - ETA: 15s - loss: 0.3501 - accuracy: 0.900 - ETA: 14s - loss: 0.3508 - accuracy: 0.899 - ETA: 14s - loss: 0.3501 - accuracy: 0.900 - ETA: 13s - loss: 0.3501 - accuracy: 0.900 - ETA: 12s - loss: 0.3507 - accuracy: 0.900 - ETA: 11s - loss: 0.3498 - accuracy: 0.900 - ETA: 10s - loss: 0.3493 - accuracy: 0.900 - ETA: 9s - loss: 0.3493 - accuracy: 0.900 - ETA: 8s - loss: 0.3496 - accuracy: 0.90 - ETA: 7s - loss: 0.3489 - accuracy: 0.90 - ETA: 6s - loss: 0.3487 - accuracy: 0.90 - ETA: 6s - loss: 0.3499 - accuracy: 0.90 - ETA: 5s - loss: 0.3504 - accuracy: 0.90 - ETA: 4s - loss: 0.3493 - accuracy: 0.90 - ETA: 3s - loss: 0.3487 - accuracy: 0.90 - ETA: 2s - loss: 0.3487 - accuracy: 0.90 - ETA: 1s - loss: 0.3480 - accuracy: 0.90 - ETA: 0s - loss: 0.3470 - accuracy: 0.90 - 104s 8ms/step - loss: 0.3469 - accuracy: 0.9009 - val_loss: 3.0000 - val_accuracy: 0.3509\n",
      "Epoch 68/100\n",
      "13022/13022 [==============================] - ETA: 1:34 - loss: 0.2546 - accuracy: 0.91 - ETA: 1:30 - loss: 0.3333 - accuracy: 0.90 - ETA: 1:29 - loss: 0.3200 - accuracy: 0.90 - ETA: 1:30 - loss: 0.3119 - accuracy: 0.90 - ETA: 1:29 - loss: 0.3088 - accuracy: 0.90 - ETA: 1:27 - loss: 0.2859 - accuracy: 0.91 - ETA: 1:27 - loss: 0.2885 - accuracy: 0.91 - ETA: 1:26 - loss: 0.2916 - accuracy: 0.91 - ETA: 1:25 - loss: 0.3021 - accuracy: 0.91 - ETA: 1:24 - loss: 0.3099 - accuracy: 0.90 - ETA: 1:22 - loss: 0.3239 - accuracy: 0.90 - ETA: 1:21 - loss: 0.3357 - accuracy: 0.90 - ETA: 1:20 - loss: 0.3507 - accuracy: 0.90 - ETA: 1:19 - loss: 0.3419 - accuracy: 0.90 - ETA: 1:17 - loss: 0.3395 - accuracy: 0.90 - ETA: 1:16 - loss: 0.3353 - accuracy: 0.90 - ETA: 1:15 - loss: 0.3318 - accuracy: 0.90 - ETA: 1:14 - loss: 0.3293 - accuracy: 0.90 - ETA: 1:14 - loss: 0.3309 - accuracy: 0.90 - ETA: 1:13 - loss: 0.3294 - accuracy: 0.90 - ETA: 1:12 - loss: 0.3364 - accuracy: 0.90 - ETA: 1:11 - loss: 0.3381 - accuracy: 0.90 - ETA: 1:10 - loss: 0.3386 - accuracy: 0.90 - ETA: 1:09 - loss: 0.3425 - accuracy: 0.90 - ETA: 1:09 - loss: 0.3408 - accuracy: 0.90 - ETA: 1:08 - loss: 0.3420 - accuracy: 0.90 - ETA: 1:07 - loss: 0.3432 - accuracy: 0.90 - ETA: 1:06 - loss: 0.3438 - accuracy: 0.90 - ETA: 1:05 - loss: 0.3412 - accuracy: 0.90 - ETA: 1:04 - loss: 0.3374 - accuracy: 0.90 - ETA: 1:03 - loss: 0.3339 - accuracy: 0.90 - ETA: 1:02 - loss: 0.3385 - accuracy: 0.90 - ETA: 1:01 - loss: 0.3369 - accuracy: 0.90 - ETA: 1:00 - loss: 0.3387 - accuracy: 0.90 - ETA: 59s - loss: 0.3369 - accuracy: 0.9054 - ETA: 58s - loss: 0.3362 - accuracy: 0.905 - ETA: 58s - loss: 0.3336 - accuracy: 0.906 - ETA: 57s - loss: 0.3387 - accuracy: 0.905 - ETA: 56s - loss: 0.3378 - accuracy: 0.905 - ETA: 55s - loss: 0.3355 - accuracy: 0.906 - ETA: 54s - loss: 0.3325 - accuracy: 0.906 - ETA: 53s - loss: 0.3321 - accuracy: 0.906 - ETA: 53s - loss: 0.3300 - accuracy: 0.906 - ETA: 52s - loss: 0.3290 - accuracy: 0.906 - ETA: 51s - loss: 0.3326 - accuracy: 0.905 - ETA: 50s - loss: 0.3336 - accuracy: 0.904 - ETA: 49s - loss: 0.3340 - accuracy: 0.905 - ETA: 48s - loss: 0.3336 - accuracy: 0.904 - ETA: 47s - loss: 0.3337 - accuracy: 0.905 - ETA: 46s - loss: 0.3342 - accuracy: 0.904 - ETA: 46s - loss: 0.3425 - accuracy: 0.903 - ETA: 45s - loss: 0.3441 - accuracy: 0.903 - ETA: 44s - loss: 0.3469 - accuracy: 0.903 - ETA: 43s - loss: 0.3461 - accuracy: 0.903 - ETA: 42s - loss: 0.3468 - accuracy: 0.903 - ETA: 41s - loss: 0.3477 - accuracy: 0.902 - ETA: 40s - loss: 0.3502 - accuracy: 0.901 - ETA: 39s - loss: 0.3503 - accuracy: 0.901 - ETA: 38s - loss: 0.3495 - accuracy: 0.902 - ETA: 37s - loss: 0.3497 - accuracy: 0.901 - ETA: 36s - loss: 0.3502 - accuracy: 0.901 - ETA: 35s - loss: 0.3504 - accuracy: 0.901 - ETA: 35s - loss: 0.3496 - accuracy: 0.901 - ETA: 34s - loss: 0.3481 - accuracy: 0.902 - ETA: 33s - loss: 0.3478 - accuracy: 0.901 - ETA: 32s - loss: 0.3485 - accuracy: 0.902 - ETA: 31s - loss: 0.3500 - accuracy: 0.901 - ETA: 30s - loss: 0.3523 - accuracy: 0.900 - ETA: 29s - loss: 0.3567 - accuracy: 0.899 - ETA: 28s - loss: 0.3564 - accuracy: 0.899 - ETA: 27s - loss: 0.3588 - accuracy: 0.899 - ETA: 26s - loss: 0.3594 - accuracy: 0.899 - ETA: 25s - loss: 0.3582 - accuracy: 0.899 - ETA: 25s - loss: 0.3580 - accuracy: 0.899 - ETA: 24s - loss: 0.3577 - accuracy: 0.899 - ETA: 23s - loss: 0.3579 - accuracy: 0.899 - ETA: 22s - loss: 0.3593 - accuracy: 0.899 - ETA: 21s - loss: 0.3614 - accuracy: 0.898 - ETA: 20s - loss: 0.3608 - accuracy: 0.899 - ETA: 19s - loss: 0.3628 - accuracy: 0.898 - ETA: 18s - loss: 0.3657 - accuracy: 0.897 - ETA: 17s - loss: 0.3648 - accuracy: 0.897 - ETA: 16s - loss: 0.3652 - accuracy: 0.897 - ETA: 15s - loss: 0.3658 - accuracy: 0.897 - ETA: 15s - loss: 0.3650 - accuracy: 0.897 - ETA: 14s - loss: 0.3643 - accuracy: 0.897 - ETA: 13s - loss: 0.3648 - accuracy: 0.897 - ETA: 12s - loss: 0.3627 - accuracy: 0.897 - ETA: 11s - loss: 0.3639 - accuracy: 0.897 - ETA: 10s - loss: 0.3658 - accuracy: 0.897 - ETA: 9s - loss: 0.3649 - accuracy: 0.897 - ETA: 8s - loss: 0.3647 - accuracy: 0.89 - ETA: 7s - loss: 0.3641 - accuracy: 0.89 - ETA: 7s - loss: 0.3645 - accuracy: 0.89 - ETA: 6s - loss: 0.3656 - accuracy: 0.89 - ETA: 5s - loss: 0.3641 - accuracy: 0.89 - ETA: 4s - loss: 0.3653 - accuracy: 0.89 - ETA: 3s - loss: 0.3661 - accuracy: 0.89 - ETA: 2s - loss: 0.3657 - accuracy: 0.89 - ETA: 1s - loss: 0.3649 - accuracy: 0.89 - ETA: 0s - loss: 0.3670 - accuracy: 0.89 - 106s 8ms/step - loss: 0.3670 - accuracy: 0.8962 - val_loss: 2.7528 - val_accuracy: 0.3743\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:39 - loss: 0.3716 - accuracy: 0.91 - ETA: 1:35 - loss: 0.3562 - accuracy: 0.90 - ETA: 1:34 - loss: 0.3253 - accuracy: 0.91 - ETA: 1:33 - loss: 0.2941 - accuracy: 0.91 - ETA: 1:30 - loss: 0.3052 - accuracy: 0.91 - ETA: 1:28 - loss: 0.3269 - accuracy: 0.91 - ETA: 1:27 - loss: 0.3381 - accuracy: 0.90 - ETA: 1:25 - loss: 0.3735 - accuracy: 0.90 - ETA: 1:24 - loss: 0.3713 - accuracy: 0.89 - ETA: 1:24 - loss: 0.3858 - accuracy: 0.89 - ETA: 1:22 - loss: 0.3853 - accuracy: 0.89 - ETA: 1:22 - loss: 0.3738 - accuracy: 0.89 - ETA: 1:21 - loss: 0.3639 - accuracy: 0.89 - ETA: 1:19 - loss: 0.3648 - accuracy: 0.89 - ETA: 1:18 - loss: 0.3588 - accuracy: 0.89 - ETA: 1:17 - loss: 0.3541 - accuracy: 0.89 - ETA: 1:16 - loss: 0.3627 - accuracy: 0.89 - ETA: 1:15 - loss: 0.3582 - accuracy: 0.89 - ETA: 1:14 - loss: 0.3509 - accuracy: 0.89 - ETA: 1:13 - loss: 0.3507 - accuracy: 0.89 - ETA: 1:12 - loss: 0.3433 - accuracy: 0.90 - ETA: 1:11 - loss: 0.3432 - accuracy: 0.90 - ETA: 1:10 - loss: 0.3515 - accuracy: 0.89 - ETA: 1:09 - loss: 0.3554 - accuracy: 0.89 - ETA: 1:08 - loss: 0.3535 - accuracy: 0.89 - ETA: 1:07 - loss: 0.3579 - accuracy: 0.89 - ETA: 1:06 - loss: 0.3627 - accuracy: 0.89 - ETA: 1:06 - loss: 0.3619 - accuracy: 0.89 - ETA: 1:05 - loss: 0.3611 - accuracy: 0.89 - ETA: 1:04 - loss: 0.3563 - accuracy: 0.89 - ETA: 1:03 - loss: 0.3546 - accuracy: 0.89 - ETA: 1:02 - loss: 0.3515 - accuracy: 0.90 - ETA: 1:01 - loss: 0.3513 - accuracy: 0.90 - ETA: 1:00 - loss: 0.3539 - accuracy: 0.90 - ETA: 59s - loss: 0.3549 - accuracy: 0.9013 - ETA: 58s - loss: 0.3543 - accuracy: 0.901 - ETA: 58s - loss: 0.3561 - accuracy: 0.900 - ETA: 57s - loss: 0.3560 - accuracy: 0.900 - ETA: 56s - loss: 0.3524 - accuracy: 0.901 - ETA: 55s - loss: 0.3519 - accuracy: 0.901 - ETA: 54s - loss: 0.3526 - accuracy: 0.901 - ETA: 53s - loss: 0.3501 - accuracy: 0.902 - ETA: 52s - loss: 0.3502 - accuracy: 0.901 - ETA: 51s - loss: 0.3501 - accuracy: 0.901 - ETA: 50s - loss: 0.3548 - accuracy: 0.899 - ETA: 49s - loss: 0.3604 - accuracy: 0.898 - ETA: 49s - loss: 0.3581 - accuracy: 0.898 - ETA: 48s - loss: 0.3557 - accuracy: 0.899 - ETA: 47s - loss: 0.3553 - accuracy: 0.899 - ETA: 46s - loss: 0.3562 - accuracy: 0.899 - ETA: 45s - loss: 0.3559 - accuracy: 0.898 - ETA: 44s - loss: 0.3569 - accuracy: 0.898 - ETA: 43s - loss: 0.3580 - accuracy: 0.898 - ETA: 42s - loss: 0.3573 - accuracy: 0.899 - ETA: 42s - loss: 0.3615 - accuracy: 0.898 - ETA: 41s - loss: 0.3606 - accuracy: 0.899 - ETA: 40s - loss: 0.3600 - accuracy: 0.899 - ETA: 39s - loss: 0.3608 - accuracy: 0.899 - ETA: 38s - loss: 0.3616 - accuracy: 0.899 - ETA: 37s - loss: 0.3602 - accuracy: 0.899 - ETA: 36s - loss: 0.3591 - accuracy: 0.899 - ETA: 35s - loss: 0.3583 - accuracy: 0.899 - ETA: 34s - loss: 0.3558 - accuracy: 0.900 - ETA: 34s - loss: 0.3558 - accuracy: 0.900 - ETA: 33s - loss: 0.3569 - accuracy: 0.900 - ETA: 32s - loss: 0.3584 - accuracy: 0.900 - ETA: 31s - loss: 0.3617 - accuracy: 0.899 - ETA: 30s - loss: 0.3593 - accuracy: 0.900 - ETA: 29s - loss: 0.3613 - accuracy: 0.899 - ETA: 28s - loss: 0.3618 - accuracy: 0.899 - ETA: 27s - loss: 0.3618 - accuracy: 0.900 - ETA: 26s - loss: 0.3633 - accuracy: 0.899 - ETA: 25s - loss: 0.3646 - accuracy: 0.899 - ETA: 25s - loss: 0.3615 - accuracy: 0.900 - ETA: 24s - loss: 0.3634 - accuracy: 0.899 - ETA: 23s - loss: 0.3632 - accuracy: 0.900 - ETA: 22s - loss: 0.3629 - accuracy: 0.900 - ETA: 21s - loss: 0.3631 - accuracy: 0.900 - ETA: 20s - loss: 0.3628 - accuracy: 0.899 - ETA: 19s - loss: 0.3622 - accuracy: 0.900 - ETA: 18s - loss: 0.3635 - accuracy: 0.899 - ETA: 17s - loss: 0.3616 - accuracy: 0.899 - ETA: 16s - loss: 0.3642 - accuracy: 0.899 - ETA: 16s - loss: 0.3652 - accuracy: 0.898 - ETA: 15s - loss: 0.3644 - accuracy: 0.898 - ETA: 14s - loss: 0.3633 - accuracy: 0.899 - ETA: 13s - loss: 0.3617 - accuracy: 0.899 - ETA: 12s - loss: 0.3604 - accuracy: 0.899 - ETA: 11s - loss: 0.3615 - accuracy: 0.899 - ETA: 10s - loss: 0.3611 - accuracy: 0.899 - ETA: 9s - loss: 0.3611 - accuracy: 0.899 - ETA: 8s - loss: 0.3629 - accuracy: 0.89 - ETA: 7s - loss: 0.3629 - accuracy: 0.89 - ETA: 6s - loss: 0.3633 - accuracy: 0.89 - ETA: 6s - loss: 0.3643 - accuracy: 0.89 - ETA: 5s - loss: 0.3642 - accuracy: 0.89 - ETA: 4s - loss: 0.3643 - accuracy: 0.89 - ETA: 3s - loss: 0.3647 - accuracy: 0.89 - ETA: 2s - loss: 0.3632 - accuracy: 0.89 - ETA: 1s - loss: 0.3635 - accuracy: 0.89 - ETA: 0s - loss: 0.3633 - accuracy: 0.89 - 104s 8ms/step - loss: 0.3639 - accuracy: 0.8990 - val_loss: 3.0739 - val_accuracy: 0.3398\n",
      "Epoch 70/100\n",
      "13022/13022 [==============================] - ETA: 1:37 - loss: 0.4516 - accuracy: 0.84 - ETA: 1:33 - loss: 0.3246 - accuracy: 0.89 - ETA: 1:33 - loss: 0.3516 - accuracy: 0.89 - ETA: 1:30 - loss: 0.3823 - accuracy: 0.88 - ETA: 1:29 - loss: 0.3810 - accuracy: 0.88 - ETA: 1:28 - loss: 0.3877 - accuracy: 0.88 - ETA: 1:27 - loss: 0.3990 - accuracy: 0.88 - ETA: 1:25 - loss: 0.3740 - accuracy: 0.89 - ETA: 1:24 - loss: 0.3730 - accuracy: 0.89 - ETA: 1:23 - loss: 0.3755 - accuracy: 0.89 - ETA: 1:22 - loss: 0.3746 - accuracy: 0.89 - ETA: 1:21 - loss: 0.3797 - accuracy: 0.89 - ETA: 1:20 - loss: 0.3738 - accuracy: 0.89 - ETA: 1:19 - loss: 0.3721 - accuracy: 0.89 - ETA: 1:18 - loss: 0.3779 - accuracy: 0.89 - ETA: 1:17 - loss: 0.3750 - accuracy: 0.89 - ETA: 1:16 - loss: 0.3714 - accuracy: 0.89 - ETA: 1:15 - loss: 0.3643 - accuracy: 0.89 - ETA: 1:14 - loss: 0.3615 - accuracy: 0.89 - ETA: 1:13 - loss: 0.3545 - accuracy: 0.90 - ETA: 1:13 - loss: 0.3590 - accuracy: 0.90 - ETA: 1:11 - loss: 0.3624 - accuracy: 0.90 - ETA: 1:10 - loss: 0.3593 - accuracy: 0.90 - ETA: 1:09 - loss: 0.3564 - accuracy: 0.90 - ETA: 1:08 - loss: 0.3602 - accuracy: 0.90 - ETA: 1:07 - loss: 0.3562 - accuracy: 0.90 - ETA: 1:07 - loss: 0.3610 - accuracy: 0.89 - ETA: 1:06 - loss: 0.3607 - accuracy: 0.89 - ETA: 1:05 - loss: 0.3609 - accuracy: 0.90 - ETA: 1:04 - loss: 0.3579 - accuracy: 0.90 - ETA: 1:03 - loss: 0.3615 - accuracy: 0.90 - ETA: 1:02 - loss: 0.3605 - accuracy: 0.89 - ETA: 1:02 - loss: 0.3603 - accuracy: 0.89 - ETA: 1:01 - loss: 0.3624 - accuracy: 0.89 - ETA: 1:00 - loss: 0.3608 - accuracy: 0.90 - ETA: 59s - loss: 0.3588 - accuracy: 0.9013 - ETA: 58s - loss: 0.3585 - accuracy: 0.901 - ETA: 57s - loss: 0.3566 - accuracy: 0.901 - ETA: 57s - loss: 0.3529 - accuracy: 0.902 - ETA: 56s - loss: 0.3566 - accuracy: 0.902 - ETA: 55s - loss: 0.3541 - accuracy: 0.903 - ETA: 54s - loss: 0.3522 - accuracy: 0.903 - ETA: 53s - loss: 0.3505 - accuracy: 0.903 - ETA: 52s - loss: 0.3497 - accuracy: 0.903 - ETA: 51s - loss: 0.3491 - accuracy: 0.903 - ETA: 50s - loss: 0.3480 - accuracy: 0.904 - ETA: 49s - loss: 0.3496 - accuracy: 0.904 - ETA: 48s - loss: 0.3478 - accuracy: 0.904 - ETA: 47s - loss: 0.3467 - accuracy: 0.904 - ETA: 46s - loss: 0.3511 - accuracy: 0.903 - ETA: 45s - loss: 0.3507 - accuracy: 0.903 - ETA: 44s - loss: 0.3493 - accuracy: 0.903 - ETA: 44s - loss: 0.3540 - accuracy: 0.902 - ETA: 43s - loss: 0.3566 - accuracy: 0.902 - ETA: 42s - loss: 0.3551 - accuracy: 0.903 - ETA: 41s - loss: 0.3543 - accuracy: 0.902 - ETA: 40s - loss: 0.3545 - accuracy: 0.902 - ETA: 39s - loss: 0.3558 - accuracy: 0.902 - ETA: 38s - loss: 0.3553 - accuracy: 0.902 - ETA: 37s - loss: 0.3547 - accuracy: 0.902 - ETA: 36s - loss: 0.3545 - accuracy: 0.901 - ETA: 35s - loss: 0.3546 - accuracy: 0.901 - ETA: 34s - loss: 0.3527 - accuracy: 0.901 - ETA: 34s - loss: 0.3507 - accuracy: 0.902 - ETA: 33s - loss: 0.3499 - accuracy: 0.902 - ETA: 32s - loss: 0.3488 - accuracy: 0.902 - ETA: 31s - loss: 0.3471 - accuracy: 0.902 - ETA: 30s - loss: 0.3471 - accuracy: 0.902 - ETA: 29s - loss: 0.3441 - accuracy: 0.903 - ETA: 28s - loss: 0.3444 - accuracy: 0.904 - ETA: 27s - loss: 0.3440 - accuracy: 0.904 - ETA: 26s - loss: 0.3433 - accuracy: 0.904 - ETA: 25s - loss: 0.3423 - accuracy: 0.904 - ETA: 24s - loss: 0.3421 - accuracy: 0.904 - ETA: 24s - loss: 0.3442 - accuracy: 0.903 - ETA: 23s - loss: 0.3433 - accuracy: 0.904 - ETA: 22s - loss: 0.3426 - accuracy: 0.904 - ETA: 21s - loss: 0.3419 - accuracy: 0.904 - ETA: 20s - loss: 0.3426 - accuracy: 0.904 - ETA: 19s - loss: 0.3419 - accuracy: 0.904 - ETA: 18s - loss: 0.3414 - accuracy: 0.904 - ETA: 17s - loss: 0.3400 - accuracy: 0.904 - ETA: 16s - loss: 0.3412 - accuracy: 0.904 - ETA: 15s - loss: 0.3404 - accuracy: 0.904 - ETA: 15s - loss: 0.3410 - accuracy: 0.904 - ETA: 14s - loss: 0.3394 - accuracy: 0.905 - ETA: 13s - loss: 0.3394 - accuracy: 0.905 - ETA: 12s - loss: 0.3391 - accuracy: 0.905 - ETA: 11s - loss: 0.3400 - accuracy: 0.905 - ETA: 10s - loss: 0.3400 - accuracy: 0.905 - ETA: 9s - loss: 0.3407 - accuracy: 0.905 - ETA: 8s - loss: 0.3420 - accuracy: 0.90 - ETA: 7s - loss: 0.3414 - accuracy: 0.90 - ETA: 6s - loss: 0.3404 - accuracy: 0.90 - ETA: 6s - loss: 0.3398 - accuracy: 0.90 - ETA: 5s - loss: 0.3388 - accuracy: 0.90 - ETA: 4s - loss: 0.3394 - accuracy: 0.90 - ETA: 3s - loss: 0.3388 - accuracy: 0.90 - ETA: 2s - loss: 0.3391 - accuracy: 0.90 - ETA: 1s - loss: 0.3405 - accuracy: 0.90 - ETA: 0s - loss: 0.3404 - accuracy: 0.90 - 105s 8ms/step - loss: 0.3402 - accuracy: 0.9048 - val_loss: 3.0363 - val_accuracy: 0.3607\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:35 - loss: 0.6313 - accuracy: 0.82 - ETA: 1:30 - loss: 0.4538 - accuracy: 0.88 - ETA: 1:29 - loss: 0.4029 - accuracy: 0.89 - ETA: 1:27 - loss: 0.4093 - accuracy: 0.89 - ETA: 1:26 - loss: 0.3740 - accuracy: 0.90 - ETA: 1:25 - loss: 0.3906 - accuracy: 0.89 - ETA: 1:24 - loss: 0.3630 - accuracy: 0.89 - ETA: 1:24 - loss: 0.3530 - accuracy: 0.89 - ETA: 1:24 - loss: 0.3558 - accuracy: 0.89 - ETA: 1:23 - loss: 0.3676 - accuracy: 0.89 - ETA: 1:23 - loss: 0.3702 - accuracy: 0.89 - ETA: 1:21 - loss: 0.3606 - accuracy: 0.89 - ETA: 1:20 - loss: 0.3537 - accuracy: 0.89 - ETA: 1:19 - loss: 0.3481 - accuracy: 0.89 - ETA: 1:18 - loss: 0.3530 - accuracy: 0.89 - ETA: 1:17 - loss: 0.3531 - accuracy: 0.89 - ETA: 1:16 - loss: 0.3534 - accuracy: 0.89 - ETA: 1:15 - loss: 0.3477 - accuracy: 0.89 - ETA: 1:14 - loss: 0.3543 - accuracy: 0.89 - ETA: 1:13 - loss: 0.3532 - accuracy: 0.89 - ETA: 1:12 - loss: 0.3493 - accuracy: 0.89 - ETA: 1:11 - loss: 0.3575 - accuracy: 0.89 - ETA: 1:10 - loss: 0.3598 - accuracy: 0.89 - ETA: 1:09 - loss: 0.3621 - accuracy: 0.89 - ETA: 1:09 - loss: 0.3579 - accuracy: 0.89 - ETA: 1:08 - loss: 0.3561 - accuracy: 0.89 - ETA: 1:07 - loss: 0.3567 - accuracy: 0.89 - ETA: 1:06 - loss: 0.3552 - accuracy: 0.89 - ETA: 1:05 - loss: 0.3591 - accuracy: 0.89 - ETA: 1:05 - loss: 0.3625 - accuracy: 0.89 - ETA: 1:04 - loss: 0.3624 - accuracy: 0.89 - ETA: 1:03 - loss: 0.3639 - accuracy: 0.89 - ETA: 1:02 - loss: 0.3660 - accuracy: 0.89 - ETA: 1:01 - loss: 0.3668 - accuracy: 0.89 - ETA: 1:00 - loss: 0.3683 - accuracy: 0.89 - ETA: 59s - loss: 0.3652 - accuracy: 0.8954 - ETA: 58s - loss: 0.3624 - accuracy: 0.896 - ETA: 57s - loss: 0.3598 - accuracy: 0.897 - ETA: 56s - loss: 0.3573 - accuracy: 0.897 - ETA: 55s - loss: 0.3608 - accuracy: 0.897 - ETA: 54s - loss: 0.3626 - accuracy: 0.896 - ETA: 53s - loss: 0.3656 - accuracy: 0.895 - ETA: 53s - loss: 0.3675 - accuracy: 0.895 - ETA: 52s - loss: 0.3674 - accuracy: 0.895 - ETA: 51s - loss: 0.3640 - accuracy: 0.896 - ETA: 50s - loss: 0.3654 - accuracy: 0.896 - ETA: 49s - loss: 0.3666 - accuracy: 0.895 - ETA: 48s - loss: 0.3701 - accuracy: 0.894 - ETA: 47s - loss: 0.3702 - accuracy: 0.894 - ETA: 46s - loss: 0.3729 - accuracy: 0.894 - ETA: 46s - loss: 0.3748 - accuracy: 0.894 - ETA: 45s - loss: 0.3738 - accuracy: 0.893 - ETA: 44s - loss: 0.3738 - accuracy: 0.893 - ETA: 43s - loss: 0.3749 - accuracy: 0.893 - ETA: 42s - loss: 0.3736 - accuracy: 0.893 - ETA: 41s - loss: 0.3718 - accuracy: 0.894 - ETA: 40s - loss: 0.3725 - accuracy: 0.894 - ETA: 39s - loss: 0.3716 - accuracy: 0.894 - ETA: 38s - loss: 0.3730 - accuracy: 0.893 - ETA: 37s - loss: 0.3748 - accuracy: 0.894 - ETA: 37s - loss: 0.3740 - accuracy: 0.894 - ETA: 36s - loss: 0.3720 - accuracy: 0.894 - ETA: 35s - loss: 0.3686 - accuracy: 0.895 - ETA: 34s - loss: 0.3668 - accuracy: 0.895 - ETA: 33s - loss: 0.3680 - accuracy: 0.895 - ETA: 32s - loss: 0.3658 - accuracy: 0.896 - ETA: 31s - loss: 0.3638 - accuracy: 0.896 - ETA: 30s - loss: 0.3643 - accuracy: 0.896 - ETA: 29s - loss: 0.3638 - accuracy: 0.896 - ETA: 28s - loss: 0.3636 - accuracy: 0.896 - ETA: 27s - loss: 0.3632 - accuracy: 0.896 - ETA: 26s - loss: 0.3653 - accuracy: 0.896 - ETA: 26s - loss: 0.3653 - accuracy: 0.895 - ETA: 25s - loss: 0.3654 - accuracy: 0.895 - ETA: 24s - loss: 0.3654 - accuracy: 0.895 - ETA: 23s - loss: 0.3657 - accuracy: 0.895 - ETA: 22s - loss: 0.3665 - accuracy: 0.895 - ETA: 21s - loss: 0.3678 - accuracy: 0.895 - ETA: 20s - loss: 0.3660 - accuracy: 0.895 - ETA: 19s - loss: 0.3636 - accuracy: 0.896 - ETA: 18s - loss: 0.3640 - accuracy: 0.896 - ETA: 17s - loss: 0.3637 - accuracy: 0.896 - ETA: 17s - loss: 0.3635 - accuracy: 0.896 - ETA: 16s - loss: 0.3619 - accuracy: 0.897 - ETA: 15s - loss: 0.3617 - accuracy: 0.897 - ETA: 14s - loss: 0.3616 - accuracy: 0.897 - ETA: 13s - loss: 0.3618 - accuracy: 0.897 - ETA: 12s - loss: 0.3604 - accuracy: 0.897 - ETA: 11s - loss: 0.3633 - accuracy: 0.897 - ETA: 10s - loss: 0.3632 - accuracy: 0.897 - ETA: 9s - loss: 0.3632 - accuracy: 0.896 - ETA: 8s - loss: 0.3637 - accuracy: 0.89 - ETA: 7s - loss: 0.3637 - accuracy: 0.89 - ETA: 7s - loss: 0.3647 - accuracy: 0.89 - ETA: 6s - loss: 0.3642 - accuracy: 0.89 - ETA: 5s - loss: 0.3651 - accuracy: 0.89 - ETA: 4s - loss: 0.3638 - accuracy: 0.89 - ETA: 3s - loss: 0.3637 - accuracy: 0.89 - ETA: 2s - loss: 0.3632 - accuracy: 0.89 - ETA: 1s - loss: 0.3629 - accuracy: 0.89 - ETA: 0s - loss: 0.3628 - accuracy: 0.89 - 106s 8ms/step - loss: 0.3628 - accuracy: 0.8968 - val_loss: 2.8317 - val_accuracy: 0.3699\n",
      "Epoch 72/100\n",
      "13022/13022 [==============================] - ETA: 1:35 - loss: 0.2815 - accuracy: 0.93 - ETA: 1:32 - loss: 0.3466 - accuracy: 0.91 - ETA: 1:29 - loss: 0.3573 - accuracy: 0.90 - ETA: 1:28 - loss: 0.3570 - accuracy: 0.90 - ETA: 1:28 - loss: 0.3893 - accuracy: 0.90 - ETA: 1:26 - loss: 0.3874 - accuracy: 0.89 - ETA: 1:26 - loss: 0.3828 - accuracy: 0.89 - ETA: 1:24 - loss: 0.3811 - accuracy: 0.89 - ETA: 1:23 - loss: 0.3702 - accuracy: 0.89 - ETA: 1:22 - loss: 0.3762 - accuracy: 0.89 - ETA: 1:21 - loss: 0.3662 - accuracy: 0.89 - ETA: 1:20 - loss: 0.3705 - accuracy: 0.89 - ETA: 1:19 - loss: 0.3684 - accuracy: 0.89 - ETA: 1:19 - loss: 0.3605 - accuracy: 0.90 - ETA: 1:19 - loss: 0.3518 - accuracy: 0.90 - ETA: 1:18 - loss: 0.3557 - accuracy: 0.90 - ETA: 1:17 - loss: 0.3596 - accuracy: 0.90 - ETA: 1:16 - loss: 0.3541 - accuracy: 0.90 - ETA: 1:16 - loss: 0.3531 - accuracy: 0.90 - ETA: 1:15 - loss: 0.3488 - accuracy: 0.90 - ETA: 1:14 - loss: 0.3510 - accuracy: 0.90 - ETA: 1:13 - loss: 0.3470 - accuracy: 0.90 - ETA: 1:12 - loss: 0.3475 - accuracy: 0.90 - ETA: 1:10 - loss: 0.3558 - accuracy: 0.90 - ETA: 1:09 - loss: 0.3559 - accuracy: 0.90 - ETA: 1:09 - loss: 0.3571 - accuracy: 0.90 - ETA: 1:08 - loss: 0.3512 - accuracy: 0.90 - ETA: 1:07 - loss: 0.3479 - accuracy: 0.90 - ETA: 1:06 - loss: 0.3481 - accuracy: 0.90 - ETA: 1:05 - loss: 0.3456 - accuracy: 0.90 - ETA: 1:04 - loss: 0.3524 - accuracy: 0.90 - ETA: 1:03 - loss: 0.3577 - accuracy: 0.90 - ETA: 1:02 - loss: 0.3575 - accuracy: 0.90 - ETA: 1:01 - loss: 0.3570 - accuracy: 0.90 - ETA: 1:00 - loss: 0.3587 - accuracy: 0.90 - ETA: 1:00 - loss: 0.3560 - accuracy: 0.90 - ETA: 59s - loss: 0.3566 - accuracy: 0.9008 - ETA: 58s - loss: 0.3623 - accuracy: 0.899 - ETA: 57s - loss: 0.3680 - accuracy: 0.899 - ETA: 56s - loss: 0.3684 - accuracy: 0.898 - ETA: 55s - loss: 0.3671 - accuracy: 0.899 - ETA: 54s - loss: 0.3673 - accuracy: 0.899 - ETA: 53s - loss: 0.3656 - accuracy: 0.899 - ETA: 52s - loss: 0.3639 - accuracy: 0.899 - ETA: 51s - loss: 0.3621 - accuracy: 0.900 - ETA: 50s - loss: 0.3627 - accuracy: 0.900 - ETA: 49s - loss: 0.3612 - accuracy: 0.900 - ETA: 48s - loss: 0.3619 - accuracy: 0.900 - ETA: 48s - loss: 0.3616 - accuracy: 0.901 - ETA: 47s - loss: 0.3649 - accuracy: 0.899 - ETA: 46s - loss: 0.3644 - accuracy: 0.899 - ETA: 45s - loss: 0.3648 - accuracy: 0.899 - ETA: 44s - loss: 0.3663 - accuracy: 0.899 - ETA: 43s - loss: 0.3641 - accuracy: 0.899 - ETA: 42s - loss: 0.3670 - accuracy: 0.898 - ETA: 41s - loss: 0.3649 - accuracy: 0.899 - ETA: 40s - loss: 0.3636 - accuracy: 0.899 - ETA: 39s - loss: 0.3627 - accuracy: 0.899 - ETA: 39s - loss: 0.3639 - accuracy: 0.898 - ETA: 38s - loss: 0.3617 - accuracy: 0.899 - ETA: 37s - loss: 0.3632 - accuracy: 0.899 - ETA: 36s - loss: 0.3636 - accuracy: 0.899 - ETA: 35s - loss: 0.3661 - accuracy: 0.898 - ETA: 34s - loss: 0.3674 - accuracy: 0.898 - ETA: 33s - loss: 0.3672 - accuracy: 0.898 - ETA: 32s - loss: 0.3667 - accuracy: 0.898 - ETA: 31s - loss: 0.3682 - accuracy: 0.897 - ETA: 30s - loss: 0.3704 - accuracy: 0.897 - ETA: 29s - loss: 0.3681 - accuracy: 0.898 - ETA: 28s - loss: 0.3666 - accuracy: 0.898 - ETA: 28s - loss: 0.3657 - accuracy: 0.898 - ETA: 27s - loss: 0.3644 - accuracy: 0.899 - ETA: 26s - loss: 0.3626 - accuracy: 0.899 - ETA: 25s - loss: 0.3614 - accuracy: 0.899 - ETA: 24s - loss: 0.3596 - accuracy: 0.899 - ETA: 23s - loss: 0.3592 - accuracy: 0.899 - ETA: 22s - loss: 0.3597 - accuracy: 0.898 - ETA: 21s - loss: 0.3619 - accuracy: 0.898 - ETA: 20s - loss: 0.3617 - accuracy: 0.898 - ETA: 19s - loss: 0.3630 - accuracy: 0.898 - ETA: 18s - loss: 0.3613 - accuracy: 0.899 - ETA: 17s - loss: 0.3602 - accuracy: 0.899 - ETA: 17s - loss: 0.3590 - accuracy: 0.899 - ETA: 16s - loss: 0.3594 - accuracy: 0.900 - ETA: 15s - loss: 0.3579 - accuracy: 0.900 - ETA: 14s - loss: 0.3583 - accuracy: 0.900 - ETA: 13s - loss: 0.3593 - accuracy: 0.899 - ETA: 12s - loss: 0.3621 - accuracy: 0.899 - ETA: 11s - loss: 0.3632 - accuracy: 0.898 - ETA: 10s - loss: 0.3655 - accuracy: 0.898 - ETA: 9s - loss: 0.3661 - accuracy: 0.898 - ETA: 8s - loss: 0.3661 - accuracy: 0.89 - ETA: 7s - loss: 0.3681 - accuracy: 0.89 - ETA: 7s - loss: 0.3686 - accuracy: 0.89 - ETA: 6s - loss: 0.3691 - accuracy: 0.89 - ETA: 5s - loss: 0.3702 - accuracy: 0.89 - ETA: 4s - loss: 0.3699 - accuracy: 0.89 - ETA: 3s - loss: 0.3690 - accuracy: 0.89 - ETA: 2s - loss: 0.3694 - accuracy: 0.89 - ETA: 1s - loss: 0.3681 - accuracy: 0.89 - ETA: 0s - loss: 0.3677 - accuracy: 0.89 - 107s 8ms/step - loss: 0.3683 - accuracy: 0.8982 - val_loss: 2.9416 - val_accuracy: 0.3667\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:36 - loss: 0.4097 - accuracy: 0.88 - ETA: 1:32 - loss: 0.4756 - accuracy: 0.88 - ETA: 1:32 - loss: 0.4398 - accuracy: 0.89 - ETA: 1:31 - loss: 0.4044 - accuracy: 0.89 - ETA: 1:30 - loss: 0.3602 - accuracy: 0.90 - ETA: 1:29 - loss: 0.3718 - accuracy: 0.89 - ETA: 1:29 - loss: 0.3794 - accuracy: 0.89 - ETA: 1:28 - loss: 0.3754 - accuracy: 0.89 - ETA: 1:25 - loss: 0.3698 - accuracy: 0.89 - ETA: 1:24 - loss: 0.3851 - accuracy: 0.88 - ETA: 1:23 - loss: 0.3743 - accuracy: 0.89 - ETA: 1:22 - loss: 0.3724 - accuracy: 0.89 - ETA: 1:21 - loss: 0.3773 - accuracy: 0.89 - ETA: 1:20 - loss: 0.3656 - accuracy: 0.89 - ETA: 1:19 - loss: 0.3609 - accuracy: 0.89 - ETA: 1:18 - loss: 0.3594 - accuracy: 0.89 - ETA: 1:17 - loss: 0.3532 - accuracy: 0.89 - ETA: 1:16 - loss: 0.3472 - accuracy: 0.89 - ETA: 1:14 - loss: 0.3471 - accuracy: 0.89 - ETA: 1:14 - loss: 0.3472 - accuracy: 0.90 - ETA: 1:13 - loss: 0.3481 - accuracy: 0.90 - ETA: 1:12 - loss: 0.3493 - accuracy: 0.89 - ETA: 1:11 - loss: 0.3486 - accuracy: 0.89 - ETA: 1:10 - loss: 0.3436 - accuracy: 0.90 - ETA: 1:09 - loss: 0.3449 - accuracy: 0.90 - ETA: 1:09 - loss: 0.3468 - accuracy: 0.90 - ETA: 1:08 - loss: 0.3437 - accuracy: 0.90 - ETA: 1:07 - loss: 0.3403 - accuracy: 0.90 - ETA: 1:06 - loss: 0.3406 - accuracy: 0.90 - ETA: 1:05 - loss: 0.3380 - accuracy: 0.90 - ETA: 1:04 - loss: 0.3383 - accuracy: 0.90 - ETA: 1:03 - loss: 0.3404 - accuracy: 0.90 - ETA: 1:02 - loss: 0.3435 - accuracy: 0.90 - ETA: 1:01 - loss: 0.3437 - accuracy: 0.90 - ETA: 1:00 - loss: 0.3404 - accuracy: 0.90 - ETA: 59s - loss: 0.3421 - accuracy: 0.9000 - ETA: 58s - loss: 0.3416 - accuracy: 0.899 - ETA: 57s - loss: 0.3420 - accuracy: 0.900 - ETA: 57s - loss: 0.3430 - accuracy: 0.899 - ETA: 56s - loss: 0.3442 - accuracy: 0.899 - ETA: 55s - loss: 0.3434 - accuracy: 0.899 - ETA: 54s - loss: 0.3423 - accuracy: 0.900 - ETA: 53s - loss: 0.3409 - accuracy: 0.900 - ETA: 52s - loss: 0.3427 - accuracy: 0.900 - ETA: 51s - loss: 0.3430 - accuracy: 0.900 - ETA: 50s - loss: 0.3419 - accuracy: 0.901 - ETA: 49s - loss: 0.3426 - accuracy: 0.901 - ETA: 48s - loss: 0.3408 - accuracy: 0.901 - ETA: 48s - loss: 0.3392 - accuracy: 0.901 - ETA: 47s - loss: 0.3372 - accuracy: 0.902 - ETA: 46s - loss: 0.3371 - accuracy: 0.902 - ETA: 45s - loss: 0.3351 - accuracy: 0.902 - ETA: 44s - loss: 0.3348 - accuracy: 0.903 - ETA: 43s - loss: 0.3366 - accuracy: 0.902 - ETA: 42s - loss: 0.3370 - accuracy: 0.903 - ETA: 41s - loss: 0.3361 - accuracy: 0.903 - ETA: 40s - loss: 0.3372 - accuracy: 0.902 - ETA: 39s - loss: 0.3391 - accuracy: 0.902 - ETA: 38s - loss: 0.3392 - accuracy: 0.902 - ETA: 38s - loss: 0.3408 - accuracy: 0.902 - ETA: 37s - loss: 0.3386 - accuracy: 0.902 - ETA: 36s - loss: 0.3385 - accuracy: 0.902 - ETA: 35s - loss: 0.3362 - accuracy: 0.902 - ETA: 34s - loss: 0.3367 - accuracy: 0.902 - ETA: 33s - loss: 0.3368 - accuracy: 0.902 - ETA: 32s - loss: 0.3371 - accuracy: 0.902 - ETA: 31s - loss: 0.3372 - accuracy: 0.902 - ETA: 30s - loss: 0.3372 - accuracy: 0.902 - ETA: 29s - loss: 0.3364 - accuracy: 0.902 - ETA: 28s - loss: 0.3361 - accuracy: 0.903 - ETA: 27s - loss: 0.3362 - accuracy: 0.902 - ETA: 26s - loss: 0.3351 - accuracy: 0.903 - ETA: 26s - loss: 0.3360 - accuracy: 0.903 - ETA: 25s - loss: 0.3374 - accuracy: 0.903 - ETA: 24s - loss: 0.3389 - accuracy: 0.902 - ETA: 23s - loss: 0.3367 - accuracy: 0.903 - ETA: 22s - loss: 0.3411 - accuracy: 0.902 - ETA: 21s - loss: 0.3425 - accuracy: 0.903 - ETA: 20s - loss: 0.3421 - accuracy: 0.903 - ETA: 19s - loss: 0.3415 - accuracy: 0.903 - ETA: 18s - loss: 0.3418 - accuracy: 0.903 - ETA: 17s - loss: 0.3413 - accuracy: 0.904 - ETA: 16s - loss: 0.3415 - accuracy: 0.904 - ETA: 16s - loss: 0.3432 - accuracy: 0.903 - ETA: 15s - loss: 0.3415 - accuracy: 0.904 - ETA: 14s - loss: 0.3413 - accuracy: 0.904 - ETA: 13s - loss: 0.3414 - accuracy: 0.903 - ETA: 12s - loss: 0.3435 - accuracy: 0.903 - ETA: 11s - loss: 0.3427 - accuracy: 0.903 - ETA: 10s - loss: 0.3418 - accuracy: 0.903 - ETA: 9s - loss: 0.3404 - accuracy: 0.904 - ETA: 8s - loss: 0.3409 - accuracy: 0.90 - ETA: 7s - loss: 0.3411 - accuracy: 0.90 - ETA: 7s - loss: 0.3408 - accuracy: 0.90 - ETA: 6s - loss: 0.3424 - accuracy: 0.90 - ETA: 5s - loss: 0.3419 - accuracy: 0.90 - ETA: 4s - loss: 0.3419 - accuracy: 0.90 - ETA: 3s - loss: 0.3432 - accuracy: 0.90 - ETA: 2s - loss: 0.3436 - accuracy: 0.90 - ETA: 1s - loss: 0.3431 - accuracy: 0.90 - ETA: 0s - loss: 0.3423 - accuracy: 0.90 - 106s 8ms/step - loss: 0.3425 - accuracy: 0.9038 - val_loss: 3.0854 - val_accuracy: 0.3590\n",
      "Epoch 74/100\n",
      "13022/13022 [==============================] - ETA: 1:36 - loss: 0.3765 - accuracy: 0.90 - ETA: 1:31 - loss: 0.4192 - accuracy: 0.89 - ETA: 1:30 - loss: 0.3729 - accuracy: 0.90 - ETA: 1:29 - loss: 0.3774 - accuracy: 0.90 - ETA: 1:28 - loss: 0.4124 - accuracy: 0.90 - ETA: 1:27 - loss: 0.4269 - accuracy: 0.89 - ETA: 1:25 - loss: 0.4249 - accuracy: 0.89 - ETA: 1:24 - loss: 0.4142 - accuracy: 0.89 - ETA: 1:23 - loss: 0.4009 - accuracy: 0.89 - ETA: 1:23 - loss: 0.4027 - accuracy: 0.89 - ETA: 1:22 - loss: 0.3925 - accuracy: 0.90 - ETA: 1:22 - loss: 0.3792 - accuracy: 0.90 - ETA: 1:21 - loss: 0.3904 - accuracy: 0.90 - ETA: 1:20 - loss: 0.3840 - accuracy: 0.90 - ETA: 1:19 - loss: 0.3744 - accuracy: 0.90 - ETA: 1:18 - loss: 0.3685 - accuracy: 0.90 - ETA: 1:17 - loss: 0.3650 - accuracy: 0.90 - ETA: 1:16 - loss: 0.3678 - accuracy: 0.90 - ETA: 1:15 - loss: 0.3609 - accuracy: 0.90 - ETA: 1:15 - loss: 0.3541 - accuracy: 0.90 - ETA: 1:14 - loss: 0.3543 - accuracy: 0.90 - ETA: 1:13 - loss: 0.3579 - accuracy: 0.90 - ETA: 1:11 - loss: 0.3558 - accuracy: 0.90 - ETA: 1:10 - loss: 0.3658 - accuracy: 0.90 - ETA: 1:09 - loss: 0.3643 - accuracy: 0.90 - ETA: 1:09 - loss: 0.3617 - accuracy: 0.90 - ETA: 1:08 - loss: 0.3627 - accuracy: 0.89 - ETA: 1:07 - loss: 0.3674 - accuracy: 0.89 - ETA: 1:06 - loss: 0.3676 - accuracy: 0.89 - ETA: 1:05 - loss: 0.3652 - accuracy: 0.89 - ETA: 1:04 - loss: 0.3695 - accuracy: 0.89 - ETA: 1:03 - loss: 0.3684 - accuracy: 0.89 - ETA: 1:02 - loss: 0.3655 - accuracy: 0.89 - ETA: 1:01 - loss: 0.3670 - accuracy: 0.89 - ETA: 1:00 - loss: 0.3691 - accuracy: 0.89 - ETA: 59s - loss: 0.3705 - accuracy: 0.8950 - ETA: 58s - loss: 0.3696 - accuracy: 0.894 - ETA: 57s - loss: 0.3660 - accuracy: 0.896 - ETA: 56s - loss: 0.3646 - accuracy: 0.896 - ETA: 55s - loss: 0.3619 - accuracy: 0.896 - ETA: 55s - loss: 0.3639 - accuracy: 0.896 - ETA: 54s - loss: 0.3659 - accuracy: 0.896 - ETA: 53s - loss: 0.3666 - accuracy: 0.896 - ETA: 52s - loss: 0.3685 - accuracy: 0.896 - ETA: 51s - loss: 0.3707 - accuracy: 0.896 - ETA: 50s - loss: 0.3685 - accuracy: 0.896 - ETA: 49s - loss: 0.3723 - accuracy: 0.896 - ETA: 48s - loss: 0.3738 - accuracy: 0.896 - ETA: 47s - loss: 0.3702 - accuracy: 0.897 - ETA: 46s - loss: 0.3696 - accuracy: 0.897 - ETA: 46s - loss: 0.3689 - accuracy: 0.897 - ETA: 45s - loss: 0.3681 - accuracy: 0.897 - ETA: 44s - loss: 0.3666 - accuracy: 0.898 - ETA: 43s - loss: 0.3651 - accuracy: 0.899 - ETA: 42s - loss: 0.3636 - accuracy: 0.899 - ETA: 41s - loss: 0.3670 - accuracy: 0.899 - ETA: 40s - loss: 0.3667 - accuracy: 0.899 - ETA: 39s - loss: 0.3669 - accuracy: 0.899 - ETA: 38s - loss: 0.3659 - accuracy: 0.899 - ETA: 37s - loss: 0.3656 - accuracy: 0.899 - ETA: 36s - loss: 0.3650 - accuracy: 0.899 - ETA: 36s - loss: 0.3630 - accuracy: 0.900 - ETA: 35s - loss: 0.3622 - accuracy: 0.900 - ETA: 34s - loss: 0.3594 - accuracy: 0.901 - ETA: 33s - loss: 0.3603 - accuracy: 0.901 - ETA: 32s - loss: 0.3603 - accuracy: 0.901 - ETA: 31s - loss: 0.3590 - accuracy: 0.901 - ETA: 30s - loss: 0.3594 - accuracy: 0.902 - ETA: 29s - loss: 0.3610 - accuracy: 0.901 - ETA: 28s - loss: 0.3608 - accuracy: 0.901 - ETA: 28s - loss: 0.3588 - accuracy: 0.901 - ETA: 27s - loss: 0.3596 - accuracy: 0.901 - ETA: 26s - loss: 0.3597 - accuracy: 0.901 - ETA: 25s - loss: 0.3607 - accuracy: 0.900 - ETA: 24s - loss: 0.3616 - accuracy: 0.900 - ETA: 23s - loss: 0.3597 - accuracy: 0.900 - ETA: 22s - loss: 0.3598 - accuracy: 0.900 - ETA: 21s - loss: 0.3608 - accuracy: 0.900 - ETA: 20s - loss: 0.3624 - accuracy: 0.900 - ETA: 19s - loss: 0.3641 - accuracy: 0.899 - ETA: 18s - loss: 0.3635 - accuracy: 0.899 - ETA: 17s - loss: 0.3649 - accuracy: 0.899 - ETA: 17s - loss: 0.3632 - accuracy: 0.900 - ETA: 16s - loss: 0.3629 - accuracy: 0.900 - ETA: 15s - loss: 0.3608 - accuracy: 0.900 - ETA: 14s - loss: 0.3602 - accuracy: 0.900 - ETA: 13s - loss: 0.3616 - accuracy: 0.900 - ETA: 12s - loss: 0.3610 - accuracy: 0.900 - ETA: 11s - loss: 0.3601 - accuracy: 0.901 - ETA: 10s - loss: 0.3596 - accuracy: 0.901 - ETA: 9s - loss: 0.3605 - accuracy: 0.900 - ETA: 8s - loss: 0.3621 - accuracy: 0.90 - ETA: 7s - loss: 0.3631 - accuracy: 0.90 - ETA: 7s - loss: 0.3622 - accuracy: 0.90 - ETA: 6s - loss: 0.3621 - accuracy: 0.90 - ETA: 5s - loss: 0.3613 - accuracy: 0.90 - ETA: 4s - loss: 0.3606 - accuracy: 0.90 - ETA: 3s - loss: 0.3610 - accuracy: 0.90 - ETA: 2s - loss: 0.3617 - accuracy: 0.90 - ETA: 1s - loss: 0.3604 - accuracy: 0.90 - ETA: 0s - loss: 0.3612 - accuracy: 0.90 - 105s 8ms/step - loss: 0.3609 - accuracy: 0.9010 - val_loss: 3.1214 - val_accuracy: 0.3463\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 0.1682 - accuracy: 0.93 - ETA: 1:31 - loss: 0.2863 - accuracy: 0.91 - ETA: 1:29 - loss: 0.3042 - accuracy: 0.90 - ETA: 1:27 - loss: 0.3034 - accuracy: 0.90 - ETA: 1:26 - loss: 0.3114 - accuracy: 0.90 - ETA: 1:25 - loss: 0.3412 - accuracy: 0.90 - ETA: 1:25 - loss: 0.3292 - accuracy: 0.90 - ETA: 1:24 - loss: 0.3148 - accuracy: 0.90 - ETA: 1:23 - loss: 0.3105 - accuracy: 0.90 - ETA: 1:22 - loss: 0.3186 - accuracy: 0.90 - ETA: 1:21 - loss: 0.3164 - accuracy: 0.90 - ETA: 1:20 - loss: 0.3255 - accuracy: 0.90 - ETA: 1:19 - loss: 0.3355 - accuracy: 0.90 - ETA: 1:19 - loss: 0.3495 - accuracy: 0.89 - ETA: 1:17 - loss: 0.3570 - accuracy: 0.89 - ETA: 1:17 - loss: 0.3472 - accuracy: 0.90 - ETA: 1:16 - loss: 0.3431 - accuracy: 0.90 - ETA: 1:15 - loss: 0.3420 - accuracy: 0.90 - ETA: 1:14 - loss: 0.3369 - accuracy: 0.90 - ETA: 1:13 - loss: 0.3372 - accuracy: 0.90 - ETA: 1:13 - loss: 0.3305 - accuracy: 0.90 - ETA: 1:12 - loss: 0.3337 - accuracy: 0.90 - ETA: 1:11 - loss: 0.3388 - accuracy: 0.90 - ETA: 1:10 - loss: 0.3389 - accuracy: 0.90 - ETA: 1:10 - loss: 0.3428 - accuracy: 0.90 - ETA: 1:09 - loss: 0.3513 - accuracy: 0.89 - ETA: 1:08 - loss: 0.3637 - accuracy: 0.89 - ETA: 1:07 - loss: 0.3667 - accuracy: 0.89 - ETA: 1:06 - loss: 0.3662 - accuracy: 0.89 - ETA: 1:05 - loss: 0.3630 - accuracy: 0.89 - ETA: 1:04 - loss: 0.3613 - accuracy: 0.89 - ETA: 1:03 - loss: 0.3646 - accuracy: 0.89 - ETA: 1:02 - loss: 0.3660 - accuracy: 0.89 - ETA: 1:01 - loss: 0.3677 - accuracy: 0.89 - ETA: 1:00 - loss: 0.3675 - accuracy: 0.89 - ETA: 1:00 - loss: 0.3670 - accuracy: 0.89 - ETA: 59s - loss: 0.3724 - accuracy: 0.8970 - ETA: 58s - loss: 0.3691 - accuracy: 0.897 - ETA: 57s - loss: 0.3697 - accuracy: 0.897 - ETA: 56s - loss: 0.3680 - accuracy: 0.897 - ETA: 55s - loss: 0.3701 - accuracy: 0.897 - ETA: 54s - loss: 0.3694 - accuracy: 0.897 - ETA: 53s - loss: 0.3689 - accuracy: 0.897 - ETA: 52s - loss: 0.3677 - accuracy: 0.897 - ETA: 51s - loss: 0.3679 - accuracy: 0.897 - ETA: 50s - loss: 0.3678 - accuracy: 0.897 - ETA: 49s - loss: 0.3667 - accuracy: 0.898 - ETA: 48s - loss: 0.3653 - accuracy: 0.898 - ETA: 47s - loss: 0.3620 - accuracy: 0.899 - ETA: 47s - loss: 0.3647 - accuracy: 0.898 - ETA: 46s - loss: 0.3631 - accuracy: 0.899 - ETA: 45s - loss: 0.3615 - accuracy: 0.899 - ETA: 44s - loss: 0.3592 - accuracy: 0.900 - ETA: 43s - loss: 0.3592 - accuracy: 0.899 - ETA: 42s - loss: 0.3585 - accuracy: 0.899 - ETA: 41s - loss: 0.3579 - accuracy: 0.900 - ETA: 40s - loss: 0.3599 - accuracy: 0.899 - ETA: 39s - loss: 0.3576 - accuracy: 0.900 - ETA: 38s - loss: 0.3592 - accuracy: 0.900 - ETA: 37s - loss: 0.3620 - accuracy: 0.899 - ETA: 37s - loss: 0.3621 - accuracy: 0.899 - ETA: 36s - loss: 0.3613 - accuracy: 0.899 - ETA: 35s - loss: 0.3615 - accuracy: 0.899 - ETA: 34s - loss: 0.3658 - accuracy: 0.898 - ETA: 33s - loss: 0.3639 - accuracy: 0.898 - ETA: 32s - loss: 0.3625 - accuracy: 0.899 - ETA: 31s - loss: 0.3606 - accuracy: 0.899 - ETA: 30s - loss: 0.3600 - accuracy: 0.899 - ETA: 29s - loss: 0.3591 - accuracy: 0.899 - ETA: 28s - loss: 0.3593 - accuracy: 0.899 - ETA: 27s - loss: 0.3588 - accuracy: 0.900 - ETA: 27s - loss: 0.3592 - accuracy: 0.899 - ETA: 26s - loss: 0.3593 - accuracy: 0.900 - ETA: 25s - loss: 0.3588 - accuracy: 0.900 - ETA: 24s - loss: 0.3596 - accuracy: 0.900 - ETA: 23s - loss: 0.3591 - accuracy: 0.900 - ETA: 22s - loss: 0.3581 - accuracy: 0.900 - ETA: 21s - loss: 0.3590 - accuracy: 0.900 - ETA: 20s - loss: 0.3590 - accuracy: 0.900 - ETA: 19s - loss: 0.3586 - accuracy: 0.900 - ETA: 18s - loss: 0.3582 - accuracy: 0.900 - ETA: 17s - loss: 0.3580 - accuracy: 0.901 - ETA: 16s - loss: 0.3582 - accuracy: 0.901 - ETA: 16s - loss: 0.3573 - accuracy: 0.901 - ETA: 15s - loss: 0.3582 - accuracy: 0.900 - ETA: 14s - loss: 0.3585 - accuracy: 0.900 - ETA: 13s - loss: 0.3587 - accuracy: 0.900 - ETA: 12s - loss: 0.3595 - accuracy: 0.900 - ETA: 11s - loss: 0.3607 - accuracy: 0.900 - ETA: 10s - loss: 0.3593 - accuracy: 0.900 - ETA: 9s - loss: 0.3589 - accuracy: 0.900 - ETA: 8s - loss: 0.3593 - accuracy: 0.90 - ETA: 7s - loss: 0.3591 - accuracy: 0.90 - ETA: 7s - loss: 0.3588 - accuracy: 0.90 - ETA: 6s - loss: 0.3606 - accuracy: 0.90 - ETA: 5s - loss: 0.3589 - accuracy: 0.90 - ETA: 4s - loss: 0.3588 - accuracy: 0.90 - ETA: 3s - loss: 0.3583 - accuracy: 0.90 - ETA: 2s - loss: 0.3592 - accuracy: 0.90 - ETA: 1s - loss: 0.3577 - accuracy: 0.90 - ETA: 0s - loss: 0.3572 - accuracy: 0.90 - 105s 8ms/step - loss: 0.3584 - accuracy: 0.9003 - val_loss: 2.9355 - val_accuracy: 0.3748\n",
      "Epoch 76/100\n",
      "13022/13022 [==============================] - ETA: 1:34 - loss: 0.4855 - accuracy: 0.85 - ETA: 1:32 - loss: 0.3169 - accuracy: 0.89 - ETA: 1:29 - loss: 0.3148 - accuracy: 0.90 - ETA: 1:27 - loss: 0.3038 - accuracy: 0.91 - ETA: 1:26 - loss: 0.3399 - accuracy: 0.90 - ETA: 1:25 - loss: 0.3443 - accuracy: 0.90 - ETA: 1:25 - loss: 0.3326 - accuracy: 0.90 - ETA: 1:24 - loss: 0.3248 - accuracy: 0.90 - ETA: 1:23 - loss: 0.3380 - accuracy: 0.90 - ETA: 1:22 - loss: 0.3382 - accuracy: 0.90 - ETA: 1:21 - loss: 0.3456 - accuracy: 0.89 - ETA: 1:21 - loss: 0.3404 - accuracy: 0.89 - ETA: 1:19 - loss: 0.3535 - accuracy: 0.89 - ETA: 1:19 - loss: 0.3453 - accuracy: 0.89 - ETA: 1:18 - loss: 0.3689 - accuracy: 0.89 - ETA: 1:17 - loss: 0.3728 - accuracy: 0.89 - ETA: 1:16 - loss: 0.3675 - accuracy: 0.89 - ETA: 1:15 - loss: 0.3648 - accuracy: 0.89 - ETA: 1:14 - loss: 0.3611 - accuracy: 0.89 - ETA: 1:13 - loss: 0.3558 - accuracy: 0.89 - ETA: 1:12 - loss: 0.3617 - accuracy: 0.89 - ETA: 1:11 - loss: 0.3618 - accuracy: 0.89 - ETA: 1:10 - loss: 0.3641 - accuracy: 0.89 - ETA: 1:10 - loss: 0.3727 - accuracy: 0.89 - ETA: 1:09 - loss: 0.3719 - accuracy: 0.89 - ETA: 1:08 - loss: 0.3682 - accuracy: 0.89 - ETA: 1:07 - loss: 0.3654 - accuracy: 0.89 - ETA: 1:06 - loss: 0.3699 - accuracy: 0.89 - ETA: 1:05 - loss: 0.3684 - accuracy: 0.89 - ETA: 1:04 - loss: 0.3659 - accuracy: 0.89 - ETA: 1:03 - loss: 0.3706 - accuracy: 0.89 - ETA: 1:02 - loss: 0.3681 - accuracy: 0.89 - ETA: 1:01 - loss: 0.3692 - accuracy: 0.89 - ETA: 1:00 - loss: 0.3684 - accuracy: 0.90 - ETA: 59s - loss: 0.3630 - accuracy: 0.9011 - ETA: 59s - loss: 0.3627 - accuracy: 0.901 - ETA: 58s - loss: 0.3643 - accuracy: 0.901 - ETA: 57s - loss: 0.3598 - accuracy: 0.902 - ETA: 56s - loss: 0.3588 - accuracy: 0.902 - ETA: 55s - loss: 0.3538 - accuracy: 0.903 - ETA: 54s - loss: 0.3494 - accuracy: 0.904 - ETA: 53s - loss: 0.3477 - accuracy: 0.905 - ETA: 52s - loss: 0.3500 - accuracy: 0.904 - ETA: 52s - loss: 0.3509 - accuracy: 0.904 - ETA: 51s - loss: 0.3485 - accuracy: 0.904 - ETA: 50s - loss: 0.3472 - accuracy: 0.904 - ETA: 49s - loss: 0.3474 - accuracy: 0.904 - ETA: 48s - loss: 0.3495 - accuracy: 0.904 - ETA: 47s - loss: 0.3501 - accuracy: 0.904 - ETA: 46s - loss: 0.3501 - accuracy: 0.904 - ETA: 45s - loss: 0.3515 - accuracy: 0.904 - ETA: 44s - loss: 0.3513 - accuracy: 0.904 - ETA: 44s - loss: 0.3494 - accuracy: 0.904 - ETA: 43s - loss: 0.3483 - accuracy: 0.904 - ETA: 42s - loss: 0.3485 - accuracy: 0.904 - ETA: 41s - loss: 0.3489 - accuracy: 0.904 - ETA: 40s - loss: 0.3509 - accuracy: 0.904 - ETA: 39s - loss: 0.3504 - accuracy: 0.904 - ETA: 38s - loss: 0.3472 - accuracy: 0.905 - ETA: 37s - loss: 0.3496 - accuracy: 0.904 - ETA: 37s - loss: 0.3488 - accuracy: 0.904 - ETA: 36s - loss: 0.3502 - accuracy: 0.904 - ETA: 35s - loss: 0.3531 - accuracy: 0.904 - ETA: 34s - loss: 0.3563 - accuracy: 0.903 - ETA: 33s - loss: 0.3579 - accuracy: 0.903 - ETA: 32s - loss: 0.3601 - accuracy: 0.902 - ETA: 31s - loss: 0.3611 - accuracy: 0.901 - ETA: 30s - loss: 0.3610 - accuracy: 0.901 - ETA: 29s - loss: 0.3607 - accuracy: 0.901 - ETA: 28s - loss: 0.3617 - accuracy: 0.901 - ETA: 27s - loss: 0.3607 - accuracy: 0.902 - ETA: 26s - loss: 0.3598 - accuracy: 0.902 - ETA: 26s - loss: 0.3632 - accuracy: 0.902 - ETA: 25s - loss: 0.3622 - accuracy: 0.902 - ETA: 24s - loss: 0.3618 - accuracy: 0.902 - ETA: 23s - loss: 0.3621 - accuracy: 0.902 - ETA: 22s - loss: 0.3618 - accuracy: 0.902 - ETA: 21s - loss: 0.3604 - accuracy: 0.901 - ETA: 20s - loss: 0.3600 - accuracy: 0.901 - ETA: 19s - loss: 0.3605 - accuracy: 0.901 - ETA: 18s - loss: 0.3592 - accuracy: 0.901 - ETA: 17s - loss: 0.3579 - accuracy: 0.902 - ETA: 16s - loss: 0.3584 - accuracy: 0.901 - ETA: 16s - loss: 0.3578 - accuracy: 0.902 - ETA: 15s - loss: 0.3570 - accuracy: 0.902 - ETA: 14s - loss: 0.3571 - accuracy: 0.902 - ETA: 13s - loss: 0.3554 - accuracy: 0.902 - ETA: 12s - loss: 0.3549 - accuracy: 0.902 - ETA: 11s - loss: 0.3537 - accuracy: 0.902 - ETA: 10s - loss: 0.3530 - accuracy: 0.903 - ETA: 9s - loss: 0.3529 - accuracy: 0.902 - ETA: 8s - loss: 0.3522 - accuracy: 0.90 - ETA: 7s - loss: 0.3516 - accuracy: 0.90 - ETA: 6s - loss: 0.3502 - accuracy: 0.90 - ETA: 6s - loss: 0.3492 - accuracy: 0.90 - ETA: 5s - loss: 0.3488 - accuracy: 0.90 - ETA: 4s - loss: 0.3492 - accuracy: 0.90 - ETA: 3s - loss: 0.3486 - accuracy: 0.90 - ETA: 2s - loss: 0.3471 - accuracy: 0.90 - ETA: 1s - loss: 0.3483 - accuracy: 0.90 - ETA: 0s - loss: 0.3485 - accuracy: 0.90 - 105s 8ms/step - loss: 0.3481 - accuracy: 0.9040 - val_loss: 3.1385 - val_accuracy: 0.3543\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:29 - loss: 0.3738 - accuracy: 0.92 - ETA: 1:27 - loss: 0.3060 - accuracy: 0.92 - ETA: 1:28 - loss: 0.3147 - accuracy: 0.92 - ETA: 1:28 - loss: 0.2808 - accuracy: 0.93 - ETA: 1:27 - loss: 0.2655 - accuracy: 0.93 - ETA: 1:25 - loss: 0.2712 - accuracy: 0.92 - ETA: 1:25 - loss: 0.2799 - accuracy: 0.92 - ETA: 1:23 - loss: 0.2766 - accuracy: 0.92 - ETA: 1:22 - loss: 0.2866 - accuracy: 0.92 - ETA: 1:22 - loss: 0.2883 - accuracy: 0.92 - ETA: 1:22 - loss: 0.2953 - accuracy: 0.91 - ETA: 1:21 - loss: 0.2951 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3144 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3150 - accuracy: 0.91 - ETA: 1:19 - loss: 0.3216 - accuracy: 0.91 - ETA: 1:19 - loss: 0.3246 - accuracy: 0.90 - ETA: 1:18 - loss: 0.3243 - accuracy: 0.90 - ETA: 1:17 - loss: 0.3241 - accuracy: 0.90 - ETA: 1:16 - loss: 0.3237 - accuracy: 0.91 - ETA: 1:15 - loss: 0.3217 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3237 - accuracy: 0.91 - ETA: 1:13 - loss: 0.3195 - accuracy: 0.91 - ETA: 1:12 - loss: 0.3167 - accuracy: 0.91 - ETA: 1:11 - loss: 0.3185 - accuracy: 0.91 - ETA: 1:10 - loss: 0.3209 - accuracy: 0.91 - ETA: 1:09 - loss: 0.3233 - accuracy: 0.90 - ETA: 1:08 - loss: 0.3300 - accuracy: 0.90 - ETA: 1:07 - loss: 0.3291 - accuracy: 0.90 - ETA: 1:06 - loss: 0.3308 - accuracy: 0.90 - ETA: 1:05 - loss: 0.3381 - accuracy: 0.90 - ETA: 1:05 - loss: 0.3387 - accuracy: 0.90 - ETA: 1:04 - loss: 0.3406 - accuracy: 0.90 - ETA: 1:03 - loss: 0.3425 - accuracy: 0.90 - ETA: 1:02 - loss: 0.3390 - accuracy: 0.90 - ETA: 1:01 - loss: 0.3472 - accuracy: 0.90 - ETA: 1:00 - loss: 0.3482 - accuracy: 0.90 - ETA: 59s - loss: 0.3465 - accuracy: 0.9048 - ETA: 58s - loss: 0.3461 - accuracy: 0.905 - ETA: 57s - loss: 0.3475 - accuracy: 0.904 - ETA: 56s - loss: 0.3525 - accuracy: 0.902 - ETA: 55s - loss: 0.3503 - accuracy: 0.903 - ETA: 54s - loss: 0.3496 - accuracy: 0.902 - ETA: 53s - loss: 0.3482 - accuracy: 0.902 - ETA: 52s - loss: 0.3472 - accuracy: 0.902 - ETA: 51s - loss: 0.3506 - accuracy: 0.902 - ETA: 51s - loss: 0.3504 - accuracy: 0.902 - ETA: 50s - loss: 0.3495 - accuracy: 0.903 - ETA: 49s - loss: 0.3488 - accuracy: 0.903 - ETA: 48s - loss: 0.3473 - accuracy: 0.904 - ETA: 47s - loss: 0.3471 - accuracy: 0.904 - ETA: 46s - loss: 0.3453 - accuracy: 0.904 - ETA: 45s - loss: 0.3442 - accuracy: 0.905 - ETA: 44s - loss: 0.3420 - accuracy: 0.905 - ETA: 43s - loss: 0.3397 - accuracy: 0.906 - ETA: 42s - loss: 0.3409 - accuracy: 0.906 - ETA: 41s - loss: 0.3411 - accuracy: 0.906 - ETA: 41s - loss: 0.3405 - accuracy: 0.906 - ETA: 40s - loss: 0.3396 - accuracy: 0.906 - ETA: 39s - loss: 0.3408 - accuracy: 0.906 - ETA: 38s - loss: 0.3409 - accuracy: 0.907 - ETA: 37s - loss: 0.3422 - accuracy: 0.906 - ETA: 36s - loss: 0.3424 - accuracy: 0.906 - ETA: 35s - loss: 0.3421 - accuracy: 0.906 - ETA: 34s - loss: 0.3424 - accuracy: 0.907 - ETA: 33s - loss: 0.3436 - accuracy: 0.906 - ETA: 32s - loss: 0.3418 - accuracy: 0.907 - ETA: 31s - loss: 0.3400 - accuracy: 0.907 - ETA: 30s - loss: 0.3407 - accuracy: 0.907 - ETA: 29s - loss: 0.3407 - accuracy: 0.907 - ETA: 29s - loss: 0.3425 - accuracy: 0.906 - ETA: 28s - loss: 0.3404 - accuracy: 0.907 - ETA: 27s - loss: 0.3386 - accuracy: 0.907 - ETA: 26s - loss: 0.3414 - accuracy: 0.906 - ETA: 25s - loss: 0.3401 - accuracy: 0.906 - ETA: 24s - loss: 0.3389 - accuracy: 0.907 - ETA: 23s - loss: 0.3402 - accuracy: 0.907 - ETA: 22s - loss: 0.3401 - accuracy: 0.907 - ETA: 21s - loss: 0.3415 - accuracy: 0.907 - ETA: 20s - loss: 0.3409 - accuracy: 0.907 - ETA: 19s - loss: 0.3392 - accuracy: 0.907 - ETA: 18s - loss: 0.3382 - accuracy: 0.908 - ETA: 17s - loss: 0.3391 - accuracy: 0.907 - ETA: 17s - loss: 0.3381 - accuracy: 0.908 - ETA: 16s - loss: 0.3390 - accuracy: 0.908 - ETA: 15s - loss: 0.3397 - accuracy: 0.908 - ETA: 14s - loss: 0.3391 - accuracy: 0.908 - ETA: 13s - loss: 0.3408 - accuracy: 0.907 - ETA: 12s - loss: 0.3399 - accuracy: 0.908 - ETA: 11s - loss: 0.3398 - accuracy: 0.908 - ETA: 10s - loss: 0.3408 - accuracy: 0.907 - ETA: 9s - loss: 0.3409 - accuracy: 0.907 - ETA: 8s - loss: 0.3409 - accuracy: 0.90 - ETA: 7s - loss: 0.3402 - accuracy: 0.90 - ETA: 7s - loss: 0.3391 - accuracy: 0.90 - ETA: 6s - loss: 0.3391 - accuracy: 0.90 - ETA: 5s - loss: 0.3393 - accuracy: 0.90 - ETA: 4s - loss: 0.3382 - accuracy: 0.90 - ETA: 3s - loss: 0.3373 - accuracy: 0.90 - ETA: 2s - loss: 0.3384 - accuracy: 0.90 - ETA: 1s - loss: 0.3386 - accuracy: 0.90 - ETA: 0s - loss: 0.3380 - accuracy: 0.90 - 107s 8ms/step - loss: 0.3389 - accuracy: 0.9070 - val_loss: 3.0592 - val_accuracy: 0.3536\n",
      "Epoch 78/100\n",
      "13022/13022 [==============================] - ETA: 1:35 - loss: 0.2043 - accuracy: 0.93 - ETA: 1:36 - loss: 0.3424 - accuracy: 0.89 - ETA: 1:35 - loss: 0.3108 - accuracy: 0.91 - ETA: 1:34 - loss: 0.3031 - accuracy: 0.92 - ETA: 1:32 - loss: 0.2900 - accuracy: 0.92 - ETA: 1:30 - loss: 0.2930 - accuracy: 0.92 - ETA: 1:28 - loss: 0.2726 - accuracy: 0.92 - ETA: 1:26 - loss: 0.2843 - accuracy: 0.92 - ETA: 1:25 - loss: 0.2715 - accuracy: 0.92 - ETA: 1:24 - loss: 0.2742 - accuracy: 0.92 - ETA: 1:23 - loss: 0.2922 - accuracy: 0.92 - ETA: 1:22 - loss: 0.2949 - accuracy: 0.91 - ETA: 1:21 - loss: 0.2876 - accuracy: 0.92 - ETA: 1:20 - loss: 0.2982 - accuracy: 0.91 - ETA: 1:18 - loss: 0.3009 - accuracy: 0.91 - ETA: 1:17 - loss: 0.2949 - accuracy: 0.91 - ETA: 1:16 - loss: 0.2941 - accuracy: 0.91 - ETA: 1:15 - loss: 0.2996 - accuracy: 0.91 - ETA: 1:14 - loss: 0.2970 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3028 - accuracy: 0.91 - ETA: 1:13 - loss: 0.3029 - accuracy: 0.91 - ETA: 1:12 - loss: 0.3139 - accuracy: 0.91 - ETA: 1:11 - loss: 0.3196 - accuracy: 0.91 - ETA: 1:10 - loss: 0.3246 - accuracy: 0.91 - ETA: 1:09 - loss: 0.3267 - accuracy: 0.91 - ETA: 1:08 - loss: 0.3226 - accuracy: 0.91 - ETA: 1:07 - loss: 0.3247 - accuracy: 0.91 - ETA: 1:06 - loss: 0.3273 - accuracy: 0.91 - ETA: 1:05 - loss: 0.3229 - accuracy: 0.91 - ETA: 1:04 - loss: 0.3191 - accuracy: 0.91 - ETA: 1:03 - loss: 0.3188 - accuracy: 0.91 - ETA: 1:03 - loss: 0.3179 - accuracy: 0.91 - ETA: 1:02 - loss: 0.3166 - accuracy: 0.91 - ETA: 1:01 - loss: 0.3141 - accuracy: 0.91 - ETA: 1:00 - loss: 0.3132 - accuracy: 0.91 - ETA: 59s - loss: 0.3160 - accuracy: 0.9121 - ETA: 58s - loss: 0.3145 - accuracy: 0.912 - ETA: 57s - loss: 0.3148 - accuracy: 0.912 - ETA: 57s - loss: 0.3158 - accuracy: 0.912 - ETA: 56s - loss: 0.3175 - accuracy: 0.911 - ETA: 55s - loss: 0.3210 - accuracy: 0.910 - ETA: 54s - loss: 0.3201 - accuracy: 0.910 - ETA: 53s - loss: 0.3223 - accuracy: 0.909 - ETA: 52s - loss: 0.3209 - accuracy: 0.910 - ETA: 51s - loss: 0.3205 - accuracy: 0.910 - ETA: 50s - loss: 0.3181 - accuracy: 0.910 - ETA: 49s - loss: 0.3182 - accuracy: 0.910 - ETA: 48s - loss: 0.3182 - accuracy: 0.911 - ETA: 48s - loss: 0.3163 - accuracy: 0.911 - ETA: 47s - loss: 0.3175 - accuracy: 0.910 - ETA: 46s - loss: 0.3181 - accuracy: 0.910 - ETA: 45s - loss: 0.3186 - accuracy: 0.909 - ETA: 44s - loss: 0.3148 - accuracy: 0.910 - ETA: 43s - loss: 0.3115 - accuracy: 0.911 - ETA: 42s - loss: 0.3132 - accuracy: 0.910 - ETA: 41s - loss: 0.3107 - accuracy: 0.911 - ETA: 40s - loss: 0.3127 - accuracy: 0.910 - ETA: 40s - loss: 0.3137 - accuracy: 0.910 - ETA: 39s - loss: 0.3118 - accuracy: 0.911 - ETA: 38s - loss: 0.3098 - accuracy: 0.911 - ETA: 37s - loss: 0.3092 - accuracy: 0.912 - ETA: 36s - loss: 0.3076 - accuracy: 0.912 - ETA: 35s - loss: 0.3099 - accuracy: 0.913 - ETA: 34s - loss: 0.3116 - accuracy: 0.912 - ETA: 33s - loss: 0.3120 - accuracy: 0.912 - ETA: 32s - loss: 0.3122 - accuracy: 0.912 - ETA: 31s - loss: 0.3146 - accuracy: 0.911 - ETA: 30s - loss: 0.3147 - accuracy: 0.911 - ETA: 29s - loss: 0.3157 - accuracy: 0.911 - ETA: 28s - loss: 0.3156 - accuracy: 0.912 - ETA: 28s - loss: 0.3166 - accuracy: 0.912 - ETA: 27s - loss: 0.3156 - accuracy: 0.912 - ETA: 26s - loss: 0.3167 - accuracy: 0.911 - ETA: 25s - loss: 0.3158 - accuracy: 0.912 - ETA: 24s - loss: 0.3145 - accuracy: 0.912 - ETA: 23s - loss: 0.3154 - accuracy: 0.912 - ETA: 22s - loss: 0.3137 - accuracy: 0.912 - ETA: 21s - loss: 0.3124 - accuracy: 0.912 - ETA: 20s - loss: 0.3118 - accuracy: 0.912 - ETA: 19s - loss: 0.3130 - accuracy: 0.912 - ETA: 18s - loss: 0.3112 - accuracy: 0.913 - ETA: 17s - loss: 0.3152 - accuracy: 0.912 - ETA: 17s - loss: 0.3162 - accuracy: 0.912 - ETA: 16s - loss: 0.3148 - accuracy: 0.912 - ETA: 15s - loss: 0.3138 - accuracy: 0.913 - ETA: 14s - loss: 0.3158 - accuracy: 0.912 - ETA: 13s - loss: 0.3177 - accuracy: 0.911 - ETA: 12s - loss: 0.3167 - accuracy: 0.911 - ETA: 11s - loss: 0.3169 - accuracy: 0.911 - ETA: 10s - loss: 0.3194 - accuracy: 0.911 - ETA: 9s - loss: 0.3200 - accuracy: 0.911 - ETA: 8s - loss: 0.3210 - accuracy: 0.91 - ETA: 7s - loss: 0.3214 - accuracy: 0.91 - ETA: 7s - loss: 0.3215 - accuracy: 0.91 - ETA: 6s - loss: 0.3214 - accuracy: 0.91 - ETA: 5s - loss: 0.3214 - accuracy: 0.91 - ETA: 4s - loss: 0.3218 - accuracy: 0.91 - ETA: 3s - loss: 0.3249 - accuracy: 0.91 - ETA: 2s - loss: 0.3257 - accuracy: 0.91 - ETA: 1s - loss: 0.3261 - accuracy: 0.90 - ETA: 0s - loss: 0.3265 - accuracy: 0.90 - 106s 8ms/step - loss: 0.3256 - accuracy: 0.9099 - val_loss: 3.1377 - val_accuracy: 0.3534\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:37 - loss: 0.3161 - accuracy: 0.89 - ETA: 1:31 - loss: 0.2740 - accuracy: 0.91 - ETA: 1:30 - loss: 0.3822 - accuracy: 0.89 - ETA: 1:28 - loss: 0.3699 - accuracy: 0.90 - ETA: 1:27 - loss: 0.3797 - accuracy: 0.90 - ETA: 1:26 - loss: 0.3789 - accuracy: 0.90 - ETA: 1:25 - loss: 0.3501 - accuracy: 0.90 - ETA: 1:24 - loss: 0.3647 - accuracy: 0.90 - ETA: 1:24 - loss: 0.3618 - accuracy: 0.90 - ETA: 1:23 - loss: 0.3747 - accuracy: 0.90 - ETA: 1:22 - loss: 0.3719 - accuracy: 0.90 - ETA: 1:21 - loss: 0.3665 - accuracy: 0.90 - ETA: 1:21 - loss: 0.3640 - accuracy: 0.90 - ETA: 1:19 - loss: 0.3614 - accuracy: 0.90 - ETA: 1:18 - loss: 0.3609 - accuracy: 0.90 - ETA: 1:17 - loss: 0.3553 - accuracy: 0.90 - ETA: 1:16 - loss: 0.3509 - accuracy: 0.90 - ETA: 1:15 - loss: 0.3410 - accuracy: 0.90 - ETA: 1:14 - loss: 0.3362 - accuracy: 0.90 - ETA: 1:13 - loss: 0.3396 - accuracy: 0.90 - ETA: 1:12 - loss: 0.3417 - accuracy: 0.90 - ETA: 1:11 - loss: 0.3360 - accuracy: 0.91 - ETA: 1:10 - loss: 0.3396 - accuracy: 0.90 - ETA: 1:09 - loss: 0.3404 - accuracy: 0.90 - ETA: 1:08 - loss: 0.3374 - accuracy: 0.90 - ETA: 1:07 - loss: 0.3441 - accuracy: 0.90 - ETA: 1:07 - loss: 0.3411 - accuracy: 0.90 - ETA: 1:06 - loss: 0.3446 - accuracy: 0.90 - ETA: 1:05 - loss: 0.3420 - accuracy: 0.90 - ETA: 1:04 - loss: 0.3400 - accuracy: 0.90 - ETA: 1:03 - loss: 0.3391 - accuracy: 0.90 - ETA: 1:02 - loss: 0.3371 - accuracy: 0.90 - ETA: 1:01 - loss: 0.3355 - accuracy: 0.90 - ETA: 1:00 - loss: 0.3355 - accuracy: 0.90 - ETA: 59s - loss: 0.3340 - accuracy: 0.9096 - ETA: 58s - loss: 0.3299 - accuracy: 0.910 - ETA: 57s - loss: 0.3302 - accuracy: 0.910 - ETA: 56s - loss: 0.3288 - accuracy: 0.911 - ETA: 55s - loss: 0.3295 - accuracy: 0.911 - ETA: 55s - loss: 0.3294 - accuracy: 0.911 - ETA: 54s - loss: 0.3279 - accuracy: 0.911 - ETA: 53s - loss: 0.3284 - accuracy: 0.911 - ETA: 52s - loss: 0.3269 - accuracy: 0.912 - ETA: 51s - loss: 0.3275 - accuracy: 0.911 - ETA: 50s - loss: 0.3250 - accuracy: 0.912 - ETA: 49s - loss: 0.3243 - accuracy: 0.911 - ETA: 48s - loss: 0.3231 - accuracy: 0.911 - ETA: 48s - loss: 0.3217 - accuracy: 0.911 - ETA: 47s - loss: 0.3257 - accuracy: 0.910 - ETA: 46s - loss: 0.3284 - accuracy: 0.909 - ETA: 45s - loss: 0.3270 - accuracy: 0.909 - ETA: 44s - loss: 0.3276 - accuracy: 0.909 - ETA: 43s - loss: 0.3271 - accuracy: 0.909 - ETA: 42s - loss: 0.3258 - accuracy: 0.909 - ETA: 41s - loss: 0.3255 - accuracy: 0.909 - ETA: 40s - loss: 0.3256 - accuracy: 0.909 - ETA: 39s - loss: 0.3225 - accuracy: 0.910 - ETA: 39s - loss: 0.3236 - accuracy: 0.910 - ETA: 38s - loss: 0.3242 - accuracy: 0.909 - ETA: 37s - loss: 0.3250 - accuracy: 0.909 - ETA: 36s - loss: 0.3276 - accuracy: 0.909 - ETA: 35s - loss: 0.3304 - accuracy: 0.908 - ETA: 34s - loss: 0.3288 - accuracy: 0.909 - ETA: 33s - loss: 0.3320 - accuracy: 0.908 - ETA: 32s - loss: 0.3324 - accuracy: 0.907 - ETA: 32s - loss: 0.3303 - accuracy: 0.908 - ETA: 31s - loss: 0.3283 - accuracy: 0.909 - ETA: 30s - loss: 0.3287 - accuracy: 0.909 - ETA: 29s - loss: 0.3289 - accuracy: 0.909 - ETA: 28s - loss: 0.3287 - accuracy: 0.909 - ETA: 27s - loss: 0.3286 - accuracy: 0.909 - ETA: 26s - loss: 0.3265 - accuracy: 0.909 - ETA: 25s - loss: 0.3255 - accuracy: 0.910 - ETA: 24s - loss: 0.3246 - accuracy: 0.910 - ETA: 24s - loss: 0.3240 - accuracy: 0.910 - ETA: 23s - loss: 0.3245 - accuracy: 0.910 - ETA: 22s - loss: 0.3239 - accuracy: 0.910 - ETA: 21s - loss: 0.3247 - accuracy: 0.910 - ETA: 20s - loss: 0.3256 - accuracy: 0.910 - ETA: 19s - loss: 0.3261 - accuracy: 0.910 - ETA: 18s - loss: 0.3265 - accuracy: 0.909 - ETA: 17s - loss: 0.3269 - accuracy: 0.909 - ETA: 16s - loss: 0.3273 - accuracy: 0.909 - ETA: 15s - loss: 0.3289 - accuracy: 0.909 - ETA: 15s - loss: 0.3287 - accuracy: 0.909 - ETA: 14s - loss: 0.3280 - accuracy: 0.909 - ETA: 13s - loss: 0.3267 - accuracy: 0.909 - ETA: 12s - loss: 0.3282 - accuracy: 0.909 - ETA: 11s - loss: 0.3278 - accuracy: 0.909 - ETA: 10s - loss: 0.3257 - accuracy: 0.910 - ETA: 9s - loss: 0.3250 - accuracy: 0.910 - ETA: 8s - loss: 0.3247 - accuracy: 0.91 - ETA: 7s - loss: 0.3241 - accuracy: 0.90 - ETA: 6s - loss: 0.3237 - accuracy: 0.91 - ETA: 6s - loss: 0.3260 - accuracy: 0.90 - ETA: 5s - loss: 0.3307 - accuracy: 0.90 - ETA: 4s - loss: 0.3321 - accuracy: 0.90 - ETA: 3s - loss: 0.3313 - accuracy: 0.90 - ETA: 2s - loss: 0.3313 - accuracy: 0.90 - ETA: 1s - loss: 0.3314 - accuracy: 0.90 - ETA: 0s - loss: 0.3328 - accuracy: 0.90 - 105s 8ms/step - loss: 0.3316 - accuracy: 0.9083 - val_loss: 3.0480 - val_accuracy: 0.3443\n",
      "Epoch 80/100\n",
      "13022/13022 [==============================] - ETA: 1:30 - loss: 0.2512 - accuracy: 0.92 - ETA: 1:28 - loss: 0.2150 - accuracy: 0.94 - ETA: 1:27 - loss: 0.2746 - accuracy: 0.92 - ETA: 1:26 - loss: 0.3602 - accuracy: 0.91 - ETA: 1:25 - loss: 0.3335 - accuracy: 0.91 - ETA: 1:25 - loss: 0.3301 - accuracy: 0.91 - ETA: 1:24 - loss: 0.3429 - accuracy: 0.91 - ETA: 1:23 - loss: 0.3294 - accuracy: 0.91 - ETA: 1:23 - loss: 0.3244 - accuracy: 0.91 - ETA: 1:22 - loss: 0.3363 - accuracy: 0.91 - ETA: 1:21 - loss: 0.3238 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3195 - accuracy: 0.91 - ETA: 1:19 - loss: 0.3129 - accuracy: 0.91 - ETA: 1:18 - loss: 0.3152 - accuracy: 0.91 - ETA: 1:18 - loss: 0.3094 - accuracy: 0.91 - ETA: 1:17 - loss: 0.3031 - accuracy: 0.91 - ETA: 1:16 - loss: 0.3152 - accuracy: 0.91 - ETA: 1:15 - loss: 0.3092 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3109 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3159 - accuracy: 0.91 - ETA: 1:13 - loss: 0.3130 - accuracy: 0.91 - ETA: 1:12 - loss: 0.3145 - accuracy: 0.91 - ETA: 1:12 - loss: 0.3136 - accuracy: 0.91 - ETA: 1:11 - loss: 0.3123 - accuracy: 0.91 - ETA: 1:10 - loss: 0.3181 - accuracy: 0.91 - ETA: 1:09 - loss: 0.3215 - accuracy: 0.91 - ETA: 1:08 - loss: 0.3176 - accuracy: 0.91 - ETA: 1:07 - loss: 0.3198 - accuracy: 0.91 - ETA: 1:06 - loss: 0.3196 - accuracy: 0.91 - ETA: 1:05 - loss: 0.3157 - accuracy: 0.91 - ETA: 1:04 - loss: 0.3138 - accuracy: 0.91 - ETA: 1:03 - loss: 0.3177 - accuracy: 0.91 - ETA: 1:03 - loss: 0.3166 - accuracy: 0.91 - ETA: 1:02 - loss: 0.3157 - accuracy: 0.91 - ETA: 1:01 - loss: 0.3167 - accuracy: 0.91 - ETA: 1:00 - loss: 0.3157 - accuracy: 0.91 - ETA: 59s - loss: 0.3160 - accuracy: 0.9101 - ETA: 58s - loss: 0.3151 - accuracy: 0.910 - ETA: 57s - loss: 0.3158 - accuracy: 0.910 - ETA: 56s - loss: 0.3121 - accuracy: 0.911 - ETA: 55s - loss: 0.3123 - accuracy: 0.911 - ETA: 54s - loss: 0.3118 - accuracy: 0.911 - ETA: 53s - loss: 0.3110 - accuracy: 0.910 - ETA: 52s - loss: 0.3136 - accuracy: 0.910 - ETA: 51s - loss: 0.3119 - accuracy: 0.910 - ETA: 50s - loss: 0.3097 - accuracy: 0.911 - ETA: 49s - loss: 0.3105 - accuracy: 0.911 - ETA: 49s - loss: 0.3121 - accuracy: 0.910 - ETA: 48s - loss: 0.3147 - accuracy: 0.909 - ETA: 47s - loss: 0.3143 - accuracy: 0.909 - ETA: 46s - loss: 0.3148 - accuracy: 0.909 - ETA: 45s - loss: 0.3123 - accuracy: 0.909 - ETA: 44s - loss: 0.3109 - accuracy: 0.910 - ETA: 43s - loss: 0.3119 - accuracy: 0.910 - ETA: 42s - loss: 0.3114 - accuracy: 0.910 - ETA: 41s - loss: 0.3103 - accuracy: 0.910 - ETA: 40s - loss: 0.3094 - accuracy: 0.910 - ETA: 39s - loss: 0.3077 - accuracy: 0.911 - ETA: 39s - loss: 0.3076 - accuracy: 0.911 - ETA: 38s - loss: 0.3078 - accuracy: 0.911 - ETA: 37s - loss: 0.3071 - accuracy: 0.911 - ETA: 36s - loss: 0.3072 - accuracy: 0.911 - ETA: 35s - loss: 0.3076 - accuracy: 0.911 - ETA: 34s - loss: 0.3095 - accuracy: 0.911 - ETA: 33s - loss: 0.3087 - accuracy: 0.911 - ETA: 32s - loss: 0.3097 - accuracy: 0.911 - ETA: 31s - loss: 0.3085 - accuracy: 0.911 - ETA: 30s - loss: 0.3075 - accuracy: 0.911 - ETA: 29s - loss: 0.3068 - accuracy: 0.912 - ETA: 28s - loss: 0.3066 - accuracy: 0.911 - ETA: 28s - loss: 0.3061 - accuracy: 0.912 - ETA: 27s - loss: 0.3060 - accuracy: 0.912 - ETA: 26s - loss: 0.3061 - accuracy: 0.912 - ETA: 25s - loss: 0.3057 - accuracy: 0.912 - ETA: 24s - loss: 0.3066 - accuracy: 0.912 - ETA: 23s - loss: 0.3073 - accuracy: 0.912 - ETA: 22s - loss: 0.3063 - accuracy: 0.912 - ETA: 21s - loss: 0.3050 - accuracy: 0.913 - ETA: 20s - loss: 0.3062 - accuracy: 0.912 - ETA: 19s - loss: 0.3045 - accuracy: 0.912 - ETA: 18s - loss: 0.3042 - accuracy: 0.912 - ETA: 17s - loss: 0.3038 - accuracy: 0.913 - ETA: 17s - loss: 0.3049 - accuracy: 0.912 - ETA: 16s - loss: 0.3071 - accuracy: 0.911 - ETA: 15s - loss: 0.3070 - accuracy: 0.911 - ETA: 14s - loss: 0.3070 - accuracy: 0.911 - ETA: 13s - loss: 0.3085 - accuracy: 0.911 - ETA: 12s - loss: 0.3077 - accuracy: 0.911 - ETA: 11s - loss: 0.3088 - accuracy: 0.911 - ETA: 10s - loss: 0.3097 - accuracy: 0.911 - ETA: 9s - loss: 0.3097 - accuracy: 0.911 - ETA: 8s - loss: 0.3095 - accuracy: 0.91 - ETA: 7s - loss: 0.3096 - accuracy: 0.91 - ETA: 7s - loss: 0.3091 - accuracy: 0.91 - ETA: 6s - loss: 0.3101 - accuracy: 0.91 - ETA: 5s - loss: 0.3104 - accuracy: 0.91 - ETA: 4s - loss: 0.3119 - accuracy: 0.91 - ETA: 3s - loss: 0.3112 - accuracy: 0.91 - ETA: 2s - loss: 0.3114 - accuracy: 0.91 - ETA: 1s - loss: 0.3106 - accuracy: 0.91 - ETA: 0s - loss: 0.3124 - accuracy: 0.91 - 106s 8ms/step - loss: 0.3124 - accuracy: 0.9112 - val_loss: 3.2060 - val_accuracy: 0.3451\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:31 - loss: 0.3162 - accuracy: 0.90 - ETA: 1:29 - loss: 0.2638 - accuracy: 0.92 - ETA: 1:28 - loss: 0.3070 - accuracy: 0.90 - ETA: 1:27 - loss: 0.3299 - accuracy: 0.90 - ETA: 1:27 - loss: 0.3832 - accuracy: 0.90 - ETA: 1:26 - loss: 0.3660 - accuracy: 0.90 - ETA: 1:25 - loss: 0.3607 - accuracy: 0.90 - ETA: 1:23 - loss: 0.3461 - accuracy: 0.91 - ETA: 1:23 - loss: 0.3335 - accuracy: 0.91 - ETA: 1:22 - loss: 0.3513 - accuracy: 0.90 - ETA: 1:22 - loss: 0.3438 - accuracy: 0.91 - ETA: 1:21 - loss: 0.3419 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3505 - accuracy: 0.90 - ETA: 1:19 - loss: 0.3448 - accuracy: 0.91 - ETA: 1:18 - loss: 0.3411 - accuracy: 0.91 - ETA: 1:17 - loss: 0.3471 - accuracy: 0.90 - ETA: 1:16 - loss: 0.3408 - accuracy: 0.90 - ETA: 1:15 - loss: 0.3358 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3334 - accuracy: 0.91 - ETA: 1:13 - loss: 0.3290 - accuracy: 0.91 - ETA: 1:12 - loss: 0.3345 - accuracy: 0.91 - ETA: 1:12 - loss: 0.3305 - accuracy: 0.91 - ETA: 1:11 - loss: 0.3310 - accuracy: 0.91 - ETA: 1:10 - loss: 0.3316 - accuracy: 0.90 - ETA: 1:09 - loss: 0.3276 - accuracy: 0.91 - ETA: 1:09 - loss: 0.3289 - accuracy: 0.90 - ETA: 1:08 - loss: 0.3270 - accuracy: 0.90 - ETA: 1:07 - loss: 0.3258 - accuracy: 0.91 - ETA: 1:06 - loss: 0.3247 - accuracy: 0.91 - ETA: 1:05 - loss: 0.3262 - accuracy: 0.91 - ETA: 1:04 - loss: 0.3282 - accuracy: 0.90 - ETA: 1:03 - loss: 0.3243 - accuracy: 0.91 - ETA: 1:02 - loss: 0.3275 - accuracy: 0.91 - ETA: 1:01 - loss: 0.3316 - accuracy: 0.91 - ETA: 1:00 - loss: 0.3312 - accuracy: 0.91 - ETA: 59s - loss: 0.3357 - accuracy: 0.9089 - ETA: 58s - loss: 0.3333 - accuracy: 0.909 - ETA: 57s - loss: 0.3335 - accuracy: 0.908 - ETA: 57s - loss: 0.3329 - accuracy: 0.909 - ETA: 55s - loss: 0.3323 - accuracy: 0.908 - ETA: 55s - loss: 0.3321 - accuracy: 0.908 - ETA: 54s - loss: 0.3363 - accuracy: 0.908 - ETA: 53s - loss: 0.3354 - accuracy: 0.908 - ETA: 52s - loss: 0.3323 - accuracy: 0.909 - ETA: 51s - loss: 0.3331 - accuracy: 0.909 - ETA: 50s - loss: 0.3334 - accuracy: 0.909 - ETA: 49s - loss: 0.3336 - accuracy: 0.908 - ETA: 48s - loss: 0.3358 - accuracy: 0.908 - ETA: 47s - loss: 0.3378 - accuracy: 0.907 - ETA: 47s - loss: 0.3399 - accuracy: 0.906 - ETA: 46s - loss: 0.3363 - accuracy: 0.907 - ETA: 45s - loss: 0.3340 - accuracy: 0.907 - ETA: 44s - loss: 0.3345 - accuracy: 0.907 - ETA: 43s - loss: 0.3326 - accuracy: 0.908 - ETA: 42s - loss: 0.3317 - accuracy: 0.908 - ETA: 42s - loss: 0.3294 - accuracy: 0.908 - ETA: 41s - loss: 0.3283 - accuracy: 0.909 - ETA: 40s - loss: 0.3268 - accuracy: 0.909 - ETA: 39s - loss: 0.3257 - accuracy: 0.909 - ETA: 38s - loss: 0.3267 - accuracy: 0.909 - ETA: 37s - loss: 0.3283 - accuracy: 0.909 - ETA: 36s - loss: 0.3282 - accuracy: 0.909 - ETA: 35s - loss: 0.3287 - accuracy: 0.909 - ETA: 34s - loss: 0.3286 - accuracy: 0.909 - ETA: 33s - loss: 0.3295 - accuracy: 0.909 - ETA: 32s - loss: 0.3310 - accuracy: 0.908 - ETA: 32s - loss: 0.3330 - accuracy: 0.908 - ETA: 31s - loss: 0.3304 - accuracy: 0.908 - ETA: 30s - loss: 0.3299 - accuracy: 0.908 - ETA: 29s - loss: 0.3290 - accuracy: 0.908 - ETA: 28s - loss: 0.3295 - accuracy: 0.908 - ETA: 27s - loss: 0.3307 - accuracy: 0.908 - ETA: 26s - loss: 0.3302 - accuracy: 0.908 - ETA: 25s - loss: 0.3284 - accuracy: 0.909 - ETA: 24s - loss: 0.3264 - accuracy: 0.909 - ETA: 23s - loss: 0.3264 - accuracy: 0.909 - ETA: 22s - loss: 0.3265 - accuracy: 0.909 - ETA: 21s - loss: 0.3259 - accuracy: 0.909 - ETA: 20s - loss: 0.3247 - accuracy: 0.909 - ETA: 19s - loss: 0.3262 - accuracy: 0.909 - ETA: 18s - loss: 0.3256 - accuracy: 0.909 - ETA: 18s - loss: 0.3266 - accuracy: 0.909 - ETA: 17s - loss: 0.3273 - accuracy: 0.909 - ETA: 16s - loss: 0.3285 - accuracy: 0.909 - ETA: 15s - loss: 0.3291 - accuracy: 0.909 - ETA: 14s - loss: 0.3290 - accuracy: 0.909 - ETA: 13s - loss: 0.3310 - accuracy: 0.908 - ETA: 12s - loss: 0.3318 - accuracy: 0.908 - ETA: 11s - loss: 0.3321 - accuracy: 0.908 - ETA: 10s - loss: 0.3345 - accuracy: 0.907 - ETA: 9s - loss: 0.3337 - accuracy: 0.908 - ETA: 8s - loss: 0.3349 - accuracy: 0.90 - ETA: 7s - loss: 0.3351 - accuracy: 0.90 - ETA: 7s - loss: 0.3340 - accuracy: 0.90 - ETA: 6s - loss: 0.3329 - accuracy: 0.90 - ETA: 5s - loss: 0.3337 - accuracy: 0.90 - ETA: 4s - loss: 0.3334 - accuracy: 0.90 - ETA: 3s - loss: 0.3336 - accuracy: 0.90 - ETA: 2s - loss: 0.3335 - accuracy: 0.90 - ETA: 1s - loss: 0.3344 - accuracy: 0.90 - ETA: 0s - loss: 0.3339 - accuracy: 0.90 - 106s 8ms/step - loss: 0.3333 - accuracy: 0.9085 - val_loss: 3.4257 - val_accuracy: 0.3436\n",
      "Epoch 82/100\n",
      "13022/13022 [==============================] - ETA: 1:36 - loss: 0.3188 - accuracy: 0.90 - ETA: 1:34 - loss: 0.3098 - accuracy: 0.91 - ETA: 1:32 - loss: 0.3284 - accuracy: 0.90 - ETA: 1:31 - loss: 0.3453 - accuracy: 0.90 - ETA: 1:30 - loss: 0.3519 - accuracy: 0.90 - ETA: 1:29 - loss: 0.3451 - accuracy: 0.91 - ETA: 1:29 - loss: 0.3513 - accuracy: 0.90 - ETA: 1:28 - loss: 0.3414 - accuracy: 0.90 - ETA: 1:27 - loss: 0.3304 - accuracy: 0.91 - ETA: 1:26 - loss: 0.3215 - accuracy: 0.91 - ETA: 1:25 - loss: 0.3269 - accuracy: 0.90 - ETA: 1:24 - loss: 0.3192 - accuracy: 0.90 - ETA: 1:23 - loss: 0.3168 - accuracy: 0.90 - ETA: 1:22 - loss: 0.3073 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3041 - accuracy: 0.91 - ETA: 1:19 - loss: 0.3063 - accuracy: 0.90 - ETA: 1:18 - loss: 0.3081 - accuracy: 0.90 - ETA: 1:17 - loss: 0.3076 - accuracy: 0.91 - ETA: 1:16 - loss: 0.3057 - accuracy: 0.91 - ETA: 1:15 - loss: 0.3085 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3137 - accuracy: 0.91 - ETA: 1:13 - loss: 0.3133 - accuracy: 0.91 - ETA: 1:12 - loss: 0.3133 - accuracy: 0.91 - ETA: 1:11 - loss: 0.3159 - accuracy: 0.91 - ETA: 1:10 - loss: 0.3191 - accuracy: 0.91 - ETA: 1:09 - loss: 0.3170 - accuracy: 0.91 - ETA: 1:08 - loss: 0.3152 - accuracy: 0.91 - ETA: 1:07 - loss: 0.3142 - accuracy: 0.91 - ETA: 1:06 - loss: 0.3124 - accuracy: 0.91 - ETA: 1:05 - loss: 0.3111 - accuracy: 0.91 - ETA: 1:04 - loss: 0.3133 - accuracy: 0.91 - ETA: 1:03 - loss: 0.3120 - accuracy: 0.91 - ETA: 1:02 - loss: 0.3089 - accuracy: 0.91 - ETA: 1:01 - loss: 0.3095 - accuracy: 0.91 - ETA: 1:00 - loss: 0.3114 - accuracy: 0.91 - ETA: 1:00 - loss: 0.3147 - accuracy: 0.91 - ETA: 59s - loss: 0.3148 - accuracy: 0.9126 - ETA: 58s - loss: 0.3171 - accuracy: 0.912 - ETA: 57s - loss: 0.3170 - accuracy: 0.911 - ETA: 56s - loss: 0.3177 - accuracy: 0.910 - ETA: 55s - loss: 0.3167 - accuracy: 0.911 - ETA: 54s - loss: 0.3180 - accuracy: 0.911 - ETA: 53s - loss: 0.3203 - accuracy: 0.911 - ETA: 52s - loss: 0.3190 - accuracy: 0.911 - ETA: 51s - loss: 0.3191 - accuracy: 0.911 - ETA: 50s - loss: 0.3183 - accuracy: 0.912 - ETA: 50s - loss: 0.3139 - accuracy: 0.913 - ETA: 49s - loss: 0.3132 - accuracy: 0.913 - ETA: 48s - loss: 0.3131 - accuracy: 0.912 - ETA: 47s - loss: 0.3163 - accuracy: 0.912 - ETA: 46s - loss: 0.3175 - accuracy: 0.912 - ETA: 45s - loss: 0.3236 - accuracy: 0.912 - ETA: 44s - loss: 0.3226 - accuracy: 0.912 - ETA: 43s - loss: 0.3218 - accuracy: 0.913 - ETA: 42s - loss: 0.3201 - accuracy: 0.913 - ETA: 41s - loss: 0.3220 - accuracy: 0.912 - ETA: 40s - loss: 0.3217 - accuracy: 0.912 - ETA: 39s - loss: 0.3218 - accuracy: 0.912 - ETA: 38s - loss: 0.3229 - accuracy: 0.912 - ETA: 38s - loss: 0.3216 - accuracy: 0.912 - ETA: 37s - loss: 0.3239 - accuracy: 0.912 - ETA: 36s - loss: 0.3236 - accuracy: 0.911 - ETA: 35s - loss: 0.3228 - accuracy: 0.912 - ETA: 34s - loss: 0.3229 - accuracy: 0.912 - ETA: 33s - loss: 0.3239 - accuracy: 0.911 - ETA: 32s - loss: 0.3276 - accuracy: 0.910 - ETA: 31s - loss: 0.3279 - accuracy: 0.909 - ETA: 30s - loss: 0.3278 - accuracy: 0.910 - ETA: 29s - loss: 0.3279 - accuracy: 0.910 - ETA: 28s - loss: 0.3269 - accuracy: 0.910 - ETA: 28s - loss: 0.3289 - accuracy: 0.909 - ETA: 27s - loss: 0.3291 - accuracy: 0.909 - ETA: 26s - loss: 0.3288 - accuracy: 0.908 - ETA: 25s - loss: 0.3307 - accuracy: 0.908 - ETA: 24s - loss: 0.3292 - accuracy: 0.908 - ETA: 23s - loss: 0.3301 - accuracy: 0.908 - ETA: 22s - loss: 0.3297 - accuracy: 0.908 - ETA: 21s - loss: 0.3292 - accuracy: 0.908 - ETA: 20s - loss: 0.3281 - accuracy: 0.908 - ETA: 19s - loss: 0.3290 - accuracy: 0.908 - ETA: 18s - loss: 0.3302 - accuracy: 0.908 - ETA: 18s - loss: 0.3305 - accuracy: 0.908 - ETA: 17s - loss: 0.3296 - accuracy: 0.908 - ETA: 16s - loss: 0.3278 - accuracy: 0.909 - ETA: 15s - loss: 0.3290 - accuracy: 0.909 - ETA: 14s - loss: 0.3305 - accuracy: 0.908 - ETA: 13s - loss: 0.3299 - accuracy: 0.908 - ETA: 12s - loss: 0.3289 - accuracy: 0.909 - ETA: 11s - loss: 0.3301 - accuracy: 0.908 - ETA: 10s - loss: 0.3313 - accuracy: 0.908 - ETA: 9s - loss: 0.3308 - accuracy: 0.908 - ETA: 8s - loss: 0.3298 - accuracy: 0.90 - ETA: 7s - loss: 0.3293 - accuracy: 0.90 - ETA: 7s - loss: 0.3302 - accuracy: 0.90 - ETA: 6s - loss: 0.3295 - accuracy: 0.90 - ETA: 5s - loss: 0.3286 - accuracy: 0.90 - ETA: 4s - loss: 0.3284 - accuracy: 0.90 - ETA: 3s - loss: 0.3280 - accuracy: 0.90 - ETA: 2s - loss: 0.3278 - accuracy: 0.90 - ETA: 1s - loss: 0.3281 - accuracy: 0.90 - ETA: 0s - loss: 0.3276 - accuracy: 0.90 - 106s 8ms/step - loss: 0.3269 - accuracy: 0.9088 - val_loss: 3.0952 - val_accuracy: 0.3514\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:35 - loss: 0.2363 - accuracy: 0.92 - ETA: 1:31 - loss: 0.3822 - accuracy: 0.88 - ETA: 1:31 - loss: 0.3346 - accuracy: 0.89 - ETA: 1:28 - loss: 0.3294 - accuracy: 0.89 - ETA: 1:26 - loss: 0.3474 - accuracy: 0.89 - ETA: 1:25 - loss: 0.3287 - accuracy: 0.90 - ETA: 1:25 - loss: 0.3169 - accuracy: 0.90 - ETA: 1:24 - loss: 0.3093 - accuracy: 0.90 - ETA: 1:23 - loss: 0.3042 - accuracy: 0.90 - ETA: 1:22 - loss: 0.2935 - accuracy: 0.91 - ETA: 1:22 - loss: 0.2934 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3007 - accuracy: 0.91 - ETA: 1:20 - loss: 0.2897 - accuracy: 0.91 - ETA: 1:19 - loss: 0.2866 - accuracy: 0.91 - ETA: 1:18 - loss: 0.2878 - accuracy: 0.91 - ETA: 1:17 - loss: 0.2804 - accuracy: 0.91 - ETA: 1:16 - loss: 0.2861 - accuracy: 0.91 - ETA: 1:15 - loss: 0.2854 - accuracy: 0.91 - ETA: 1:14 - loss: 0.2881 - accuracy: 0.91 - ETA: 1:13 - loss: 0.2850 - accuracy: 0.91 - ETA: 1:13 - loss: 0.2813 - accuracy: 0.91 - ETA: 1:12 - loss: 0.2823 - accuracy: 0.91 - ETA: 1:11 - loss: 0.2782 - accuracy: 0.91 - ETA: 1:10 - loss: 0.2857 - accuracy: 0.91 - ETA: 1:09 - loss: 0.2862 - accuracy: 0.91 - ETA: 1:08 - loss: 0.2833 - accuracy: 0.91 - ETA: 1:07 - loss: 0.2814 - accuracy: 0.91 - ETA: 1:06 - loss: 0.2830 - accuracy: 0.91 - ETA: 1:05 - loss: 0.2876 - accuracy: 0.91 - ETA: 1:04 - loss: 0.2889 - accuracy: 0.91 - ETA: 1:03 - loss: 0.2904 - accuracy: 0.91 - ETA: 1:02 - loss: 0.2913 - accuracy: 0.91 - ETA: 1:02 - loss: 0.2965 - accuracy: 0.91 - ETA: 1:01 - loss: 0.3000 - accuracy: 0.91 - ETA: 1:00 - loss: 0.3033 - accuracy: 0.91 - ETA: 59s - loss: 0.3055 - accuracy: 0.9149 - ETA: 58s - loss: 0.3054 - accuracy: 0.915 - ETA: 58s - loss: 0.3052 - accuracy: 0.915 - ETA: 57s - loss: 0.3042 - accuracy: 0.915 - ETA: 56s - loss: 0.3041 - accuracy: 0.916 - ETA: 55s - loss: 0.3046 - accuracy: 0.915 - ETA: 54s - loss: 0.3049 - accuracy: 0.915 - ETA: 53s - loss: 0.3055 - accuracy: 0.915 - ETA: 52s - loss: 0.3110 - accuracy: 0.915 - ETA: 51s - loss: 0.3123 - accuracy: 0.914 - ETA: 50s - loss: 0.3130 - accuracy: 0.914 - ETA: 49s - loss: 0.3147 - accuracy: 0.914 - ETA: 48s - loss: 0.3131 - accuracy: 0.915 - ETA: 47s - loss: 0.3137 - accuracy: 0.915 - ETA: 46s - loss: 0.3119 - accuracy: 0.915 - ETA: 45s - loss: 0.3100 - accuracy: 0.916 - ETA: 45s - loss: 0.3122 - accuracy: 0.915 - ETA: 44s - loss: 0.3140 - accuracy: 0.915 - ETA: 43s - loss: 0.3161 - accuracy: 0.914 - ETA: 42s - loss: 0.3195 - accuracy: 0.913 - ETA: 41s - loss: 0.3183 - accuracy: 0.914 - ETA: 40s - loss: 0.3177 - accuracy: 0.914 - ETA: 39s - loss: 0.3168 - accuracy: 0.913 - ETA: 38s - loss: 0.3165 - accuracy: 0.914 - ETA: 37s - loss: 0.3177 - accuracy: 0.913 - ETA: 36s - loss: 0.3176 - accuracy: 0.913 - ETA: 35s - loss: 0.3173 - accuracy: 0.913 - ETA: 35s - loss: 0.3181 - accuracy: 0.913 - ETA: 34s - loss: 0.3185 - accuracy: 0.913 - ETA: 33s - loss: 0.3199 - accuracy: 0.912 - ETA: 32s - loss: 0.3188 - accuracy: 0.913 - ETA: 31s - loss: 0.3195 - accuracy: 0.912 - ETA: 30s - loss: 0.3210 - accuracy: 0.912 - ETA: 29s - loss: 0.3199 - accuracy: 0.912 - ETA: 28s - loss: 0.3219 - accuracy: 0.911 - ETA: 27s - loss: 0.3230 - accuracy: 0.911 - ETA: 26s - loss: 0.3228 - accuracy: 0.911 - ETA: 26s - loss: 0.3264 - accuracy: 0.910 - ETA: 25s - loss: 0.3251 - accuracy: 0.911 - ETA: 24s - loss: 0.3255 - accuracy: 0.911 - ETA: 23s - loss: 0.3254 - accuracy: 0.911 - ETA: 22s - loss: 0.3251 - accuracy: 0.910 - ETA: 21s - loss: 0.3250 - accuracy: 0.911 - ETA: 20s - loss: 0.3252 - accuracy: 0.910 - ETA: 19s - loss: 0.3259 - accuracy: 0.911 - ETA: 18s - loss: 0.3259 - accuracy: 0.911 - ETA: 17s - loss: 0.3245 - accuracy: 0.911 - ETA: 16s - loss: 0.3228 - accuracy: 0.911 - ETA: 16s - loss: 0.3214 - accuracy: 0.911 - ETA: 15s - loss: 0.3227 - accuracy: 0.911 - ETA: 14s - loss: 0.3209 - accuracy: 0.912 - ETA: 13s - loss: 0.3202 - accuracy: 0.912 - ETA: 12s - loss: 0.3198 - accuracy: 0.912 - ETA: 11s - loss: 0.3197 - accuracy: 0.912 - ETA: 10s - loss: 0.3180 - accuracy: 0.912 - ETA: 9s - loss: 0.3171 - accuracy: 0.912 - ETA: 8s - loss: 0.3167 - accuracy: 0.91 - ETA: 7s - loss: 0.3164 - accuracy: 0.91 - ETA: 7s - loss: 0.3150 - accuracy: 0.91 - ETA: 6s - loss: 0.3143 - accuracy: 0.91 - ETA: 5s - loss: 0.3151 - accuracy: 0.91 - ETA: 4s - loss: 0.3151 - accuracy: 0.91 - ETA: 3s - loss: 0.3148 - accuracy: 0.91 - ETA: 2s - loss: 0.3137 - accuracy: 0.91 - ETA: 1s - loss: 0.3144 - accuracy: 0.91 - ETA: 0s - loss: 0.3148 - accuracy: 0.91 - 105s 8ms/step - loss: 0.3146 - accuracy: 0.9132 - val_loss: 3.1409 - val_accuracy: 0.3612\n",
      "Epoch 84/100\n",
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.3202 - accuracy: 0.92 - ETA: 1:28 - loss: 0.2980 - accuracy: 0.92 - ETA: 1:29 - loss: 0.3273 - accuracy: 0.91 - ETA: 1:29 - loss: 0.3333 - accuracy: 0.91 - ETA: 1:29 - loss: 0.3468 - accuracy: 0.91 - ETA: 1:28 - loss: 0.3482 - accuracy: 0.91 - ETA: 1:27 - loss: 0.3376 - accuracy: 0.91 - ETA: 1:26 - loss: 0.3372 - accuracy: 0.91 - ETA: 1:26 - loss: 0.3198 - accuracy: 0.91 - ETA: 1:25 - loss: 0.3067 - accuracy: 0.91 - ETA: 1:24 - loss: 0.3138 - accuracy: 0.91 - ETA: 1:23 - loss: 0.3111 - accuracy: 0.91 - ETA: 1:21 - loss: 0.3107 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3063 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3019 - accuracy: 0.91 - ETA: 1:19 - loss: 0.3037 - accuracy: 0.91 - ETA: 1:18 - loss: 0.3001 - accuracy: 0.91 - ETA: 1:16 - loss: 0.3044 - accuracy: 0.91 - ETA: 1:15 - loss: 0.3058 - accuracy: 0.91 - ETA: 1:14 - loss: 0.2958 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3002 - accuracy: 0.91 - ETA: 1:13 - loss: 0.3098 - accuracy: 0.91 - ETA: 1:12 - loss: 0.3076 - accuracy: 0.91 - ETA: 1:10 - loss: 0.3051 - accuracy: 0.91 - ETA: 1:10 - loss: 0.3138 - accuracy: 0.91 - ETA: 1:09 - loss: 0.3063 - accuracy: 0.91 - ETA: 1:08 - loss: 0.3008 - accuracy: 0.92 - ETA: 1:07 - loss: 0.3020 - accuracy: 0.92 - ETA: 1:07 - loss: 0.3084 - accuracy: 0.91 - ETA: 1:06 - loss: 0.3135 - accuracy: 0.91 - ETA: 1:05 - loss: 0.3111 - accuracy: 0.91 - ETA: 1:04 - loss: 0.3088 - accuracy: 0.91 - ETA: 1:03 - loss: 0.3113 - accuracy: 0.91 - ETA: 1:02 - loss: 0.3124 - accuracy: 0.91 - ETA: 1:01 - loss: 0.3081 - accuracy: 0.91 - ETA: 1:00 - loss: 0.3066 - accuracy: 0.91 - ETA: 59s - loss: 0.3083 - accuracy: 0.9168 - ETA: 58s - loss: 0.3046 - accuracy: 0.917 - ETA: 57s - loss: 0.3027 - accuracy: 0.917 - ETA: 56s - loss: 0.3010 - accuracy: 0.917 - ETA: 55s - loss: 0.3025 - accuracy: 0.917 - ETA: 54s - loss: 0.3045 - accuracy: 0.916 - ETA: 53s - loss: 0.3021 - accuracy: 0.917 - ETA: 52s - loss: 0.3003 - accuracy: 0.916 - ETA: 51s - loss: 0.3014 - accuracy: 0.917 - ETA: 50s - loss: 0.2997 - accuracy: 0.917 - ETA: 49s - loss: 0.3019 - accuracy: 0.917 - ETA: 48s - loss: 0.2997 - accuracy: 0.917 - ETA: 47s - loss: 0.2991 - accuracy: 0.917 - ETA: 47s - loss: 0.2988 - accuracy: 0.917 - ETA: 46s - loss: 0.2976 - accuracy: 0.917 - ETA: 45s - loss: 0.2970 - accuracy: 0.918 - ETA: 44s - loss: 0.2974 - accuracy: 0.918 - ETA: 43s - loss: 0.2984 - accuracy: 0.917 - ETA: 42s - loss: 0.2976 - accuracy: 0.917 - ETA: 41s - loss: 0.2993 - accuracy: 0.917 - ETA: 40s - loss: 0.3022 - accuracy: 0.916 - ETA: 39s - loss: 0.3020 - accuracy: 0.916 - ETA: 38s - loss: 0.3006 - accuracy: 0.916 - ETA: 37s - loss: 0.3008 - accuracy: 0.916 - ETA: 36s - loss: 0.3005 - accuracy: 0.915 - ETA: 36s - loss: 0.3010 - accuracy: 0.915 - ETA: 35s - loss: 0.2997 - accuracy: 0.915 - ETA: 34s - loss: 0.3030 - accuracy: 0.915 - ETA: 33s - loss: 0.3040 - accuracy: 0.915 - ETA: 32s - loss: 0.3052 - accuracy: 0.915 - ETA: 31s - loss: 0.3055 - accuracy: 0.915 - ETA: 30s - loss: 0.3080 - accuracy: 0.915 - ETA: 29s - loss: 0.3095 - accuracy: 0.915 - ETA: 28s - loss: 0.3092 - accuracy: 0.915 - ETA: 28s - loss: 0.3106 - accuracy: 0.914 - ETA: 27s - loss: 0.3119 - accuracy: 0.914 - ETA: 26s - loss: 0.3104 - accuracy: 0.914 - ETA: 25s - loss: 0.3092 - accuracy: 0.914 - ETA: 24s - loss: 0.3074 - accuracy: 0.915 - ETA: 23s - loss: 0.3082 - accuracy: 0.915 - ETA: 22s - loss: 0.3065 - accuracy: 0.915 - ETA: 21s - loss: 0.3060 - accuracy: 0.915 - ETA: 20s - loss: 0.3057 - accuracy: 0.915 - ETA: 19s - loss: 0.3063 - accuracy: 0.915 - ETA: 18s - loss: 0.3053 - accuracy: 0.916 - ETA: 18s - loss: 0.3051 - accuracy: 0.916 - ETA: 17s - loss: 0.3060 - accuracy: 0.916 - ETA: 16s - loss: 0.3055 - accuracy: 0.916 - ETA: 15s - loss: 0.3067 - accuracy: 0.916 - ETA: 14s - loss: 0.3065 - accuracy: 0.916 - ETA: 13s - loss: 0.3048 - accuracy: 0.916 - ETA: 12s - loss: 0.3044 - accuracy: 0.916 - ETA: 11s - loss: 0.3036 - accuracy: 0.916 - ETA: 10s - loss: 0.3029 - accuracy: 0.916 - ETA: 9s - loss: 0.3039 - accuracy: 0.916 - ETA: 8s - loss: 0.3028 - accuracy: 0.91 - ETA: 7s - loss: 0.3028 - accuracy: 0.91 - ETA: 7s - loss: 0.3034 - accuracy: 0.91 - ETA: 6s - loss: 0.3037 - accuracy: 0.91 - ETA: 5s - loss: 0.3046 - accuracy: 0.91 - ETA: 4s - loss: 0.3030 - accuracy: 0.91 - ETA: 3s - loss: 0.3048 - accuracy: 0.91 - ETA: 2s - loss: 0.3054 - accuracy: 0.91 - ETA: 1s - loss: 0.3065 - accuracy: 0.91 - ETA: 0s - loss: 0.3056 - accuracy: 0.91 - 106s 8ms/step - loss: 0.3048 - accuracy: 0.9167 - val_loss: 3.1069 - val_accuracy: 0.3453\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:31 - loss: 0.2395 - accuracy: 0.92 - ETA: 1:31 - loss: 0.2732 - accuracy: 0.92 - ETA: 1:29 - loss: 0.3342 - accuracy: 0.91 - ETA: 1:28 - loss: 0.3267 - accuracy: 0.91 - ETA: 1:27 - loss: 0.3230 - accuracy: 0.91 - ETA: 1:26 - loss: 0.3197 - accuracy: 0.91 - ETA: 1:26 - loss: 0.3162 - accuracy: 0.91 - ETA: 1:25 - loss: 0.3232 - accuracy: 0.91 - ETA: 1:24 - loss: 0.3267 - accuracy: 0.91 - ETA: 1:23 - loss: 0.3290 - accuracy: 0.91 - ETA: 1:22 - loss: 0.3267 - accuracy: 0.91 - ETA: 1:21 - loss: 0.3253 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3171 - accuracy: 0.91 - ETA: 1:18 - loss: 0.3250 - accuracy: 0.91 - ETA: 1:18 - loss: 0.3283 - accuracy: 0.91 - ETA: 1:17 - loss: 0.3216 - accuracy: 0.91 - ETA: 1:17 - loss: 0.3113 - accuracy: 0.91 - ETA: 1:16 - loss: 0.3090 - accuracy: 0.91 - ETA: 1:15 - loss: 0.3136 - accuracy: 0.91 - ETA: 1:15 - loss: 0.3114 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3097 - accuracy: 0.91 - ETA: 1:13 - loss: 0.3082 - accuracy: 0.91 - ETA: 1:12 - loss: 0.3014 - accuracy: 0.91 - ETA: 1:11 - loss: 0.2940 - accuracy: 0.91 - ETA: 1:10 - loss: 0.2895 - accuracy: 0.92 - ETA: 1:09 - loss: 0.2917 - accuracy: 0.91 - ETA: 1:08 - loss: 0.2972 - accuracy: 0.91 - ETA: 1:07 - loss: 0.2941 - accuracy: 0.91 - ETA: 1:06 - loss: 0.2968 - accuracy: 0.91 - ETA: 1:05 - loss: 0.2939 - accuracy: 0.91 - ETA: 1:04 - loss: 0.2954 - accuracy: 0.91 - ETA: 1:04 - loss: 0.2942 - accuracy: 0.91 - ETA: 1:03 - loss: 0.2920 - accuracy: 0.91 - ETA: 1:02 - loss: 0.2906 - accuracy: 0.92 - ETA: 1:01 - loss: 0.2886 - accuracy: 0.92 - ETA: 1:00 - loss: 0.2892 - accuracy: 0.92 - ETA: 59s - loss: 0.2893 - accuracy: 0.9210 - ETA: 58s - loss: 0.2967 - accuracy: 0.920 - ETA: 57s - loss: 0.2964 - accuracy: 0.920 - ETA: 56s - loss: 0.2972 - accuracy: 0.920 - ETA: 55s - loss: 0.2978 - accuracy: 0.921 - ETA: 54s - loss: 0.2967 - accuracy: 0.921 - ETA: 53s - loss: 0.2953 - accuracy: 0.921 - ETA: 52s - loss: 0.2940 - accuracy: 0.921 - ETA: 51s - loss: 0.2943 - accuracy: 0.921 - ETA: 50s - loss: 0.2930 - accuracy: 0.921 - ETA: 49s - loss: 0.2946 - accuracy: 0.921 - ETA: 48s - loss: 0.2953 - accuracy: 0.921 - ETA: 47s - loss: 0.3000 - accuracy: 0.920 - ETA: 47s - loss: 0.2978 - accuracy: 0.921 - ETA: 46s - loss: 0.2972 - accuracy: 0.921 - ETA: 45s - loss: 0.2972 - accuracy: 0.921 - ETA: 44s - loss: 0.2960 - accuracy: 0.921 - ETA: 43s - loss: 0.2965 - accuracy: 0.921 - ETA: 42s - loss: 0.2964 - accuracy: 0.920 - ETA: 41s - loss: 0.2984 - accuracy: 0.920 - ETA: 40s - loss: 0.3000 - accuracy: 0.920 - ETA: 39s - loss: 0.3011 - accuracy: 0.920 - ETA: 38s - loss: 0.3027 - accuracy: 0.919 - ETA: 38s - loss: 0.3018 - accuracy: 0.919 - ETA: 37s - loss: 0.3021 - accuracy: 0.919 - ETA: 36s - loss: 0.3008 - accuracy: 0.919 - ETA: 35s - loss: 0.3025 - accuracy: 0.918 - ETA: 34s - loss: 0.3042 - accuracy: 0.918 - ETA: 33s - loss: 0.3024 - accuracy: 0.919 - ETA: 32s - loss: 0.3010 - accuracy: 0.919 - ETA: 31s - loss: 0.3010 - accuracy: 0.919 - ETA: 30s - loss: 0.3016 - accuracy: 0.919 - ETA: 29s - loss: 0.3040 - accuracy: 0.919 - ETA: 28s - loss: 0.3024 - accuracy: 0.919 - ETA: 28s - loss: 0.3033 - accuracy: 0.919 - ETA: 27s - loss: 0.3032 - accuracy: 0.919 - ETA: 26s - loss: 0.3051 - accuracy: 0.918 - ETA: 25s - loss: 0.3061 - accuracy: 0.918 - ETA: 24s - loss: 0.3049 - accuracy: 0.919 - ETA: 23s - loss: 0.3048 - accuracy: 0.919 - ETA: 22s - loss: 0.3038 - accuracy: 0.919 - ETA: 21s - loss: 0.3068 - accuracy: 0.918 - ETA: 20s - loss: 0.3089 - accuracy: 0.918 - ETA: 19s - loss: 0.3076 - accuracy: 0.918 - ETA: 18s - loss: 0.3090 - accuracy: 0.918 - ETA: 17s - loss: 0.3093 - accuracy: 0.918 - ETA: 17s - loss: 0.3089 - accuracy: 0.918 - ETA: 16s - loss: 0.3087 - accuracy: 0.918 - ETA: 15s - loss: 0.3086 - accuracy: 0.918 - ETA: 14s - loss: 0.3095 - accuracy: 0.917 - ETA: 13s - loss: 0.3120 - accuracy: 0.917 - ETA: 12s - loss: 0.3133 - accuracy: 0.916 - ETA: 11s - loss: 0.3133 - accuracy: 0.916 - ETA: 10s - loss: 0.3145 - accuracy: 0.916 - ETA: 9s - loss: 0.3147 - accuracy: 0.916 - ETA: 8s - loss: 0.3147 - accuracy: 0.91 - ETA: 7s - loss: 0.3140 - accuracy: 0.91 - ETA: 7s - loss: 0.3148 - accuracy: 0.91 - ETA: 6s - loss: 0.3158 - accuracy: 0.91 - ETA: 5s - loss: 0.3155 - accuracy: 0.91 - ETA: 4s - loss: 0.3163 - accuracy: 0.91 - ETA: 3s - loss: 0.3160 - accuracy: 0.91 - ETA: 2s - loss: 0.3162 - accuracy: 0.91 - ETA: 1s - loss: 0.3159 - accuracy: 0.91 - ETA: 0s - loss: 0.3162 - accuracy: 0.91 - 106s 8ms/step - loss: 0.3160 - accuracy: 0.9143 - val_loss: 3.2694 - val_accuracy: 0.3438\n",
      "Epoch 86/100\n",
      "13022/13022 [==============================] - ETA: 1:31 - loss: 0.2607 - accuracy: 0.92 - ETA: 1:30 - loss: 0.2710 - accuracy: 0.92 - ETA: 1:30 - loss: 0.2847 - accuracy: 0.91 - ETA: 1:30 - loss: 0.3014 - accuracy: 0.91 - ETA: 1:29 - loss: 0.3231 - accuracy: 0.90 - ETA: 1:29 - loss: 0.3148 - accuracy: 0.90 - ETA: 1:27 - loss: 0.3009 - accuracy: 0.91 - ETA: 1:26 - loss: 0.3131 - accuracy: 0.91 - ETA: 1:25 - loss: 0.2952 - accuracy: 0.91 - ETA: 1:24 - loss: 0.2915 - accuracy: 0.91 - ETA: 1:23 - loss: 0.2898 - accuracy: 0.91 - ETA: 1:22 - loss: 0.2880 - accuracy: 0.91 - ETA: 1:20 - loss: 0.2885 - accuracy: 0.91 - ETA: 1:19 - loss: 0.2856 - accuracy: 0.91 - ETA: 1:18 - loss: 0.2779 - accuracy: 0.91 - ETA: 1:17 - loss: 0.2835 - accuracy: 0.91 - ETA: 1:16 - loss: 0.2928 - accuracy: 0.91 - ETA: 1:15 - loss: 0.2900 - accuracy: 0.91 - ETA: 1:14 - loss: 0.2885 - accuracy: 0.91 - ETA: 1:13 - loss: 0.2889 - accuracy: 0.91 - ETA: 1:12 - loss: 0.2913 - accuracy: 0.91 - ETA: 1:11 - loss: 0.2942 - accuracy: 0.91 - ETA: 1:11 - loss: 0.2987 - accuracy: 0.91 - ETA: 1:10 - loss: 0.2959 - accuracy: 0.91 - ETA: 1:09 - loss: 0.3008 - accuracy: 0.91 - ETA: 1:08 - loss: 0.3068 - accuracy: 0.91 - ETA: 1:07 - loss: 0.3050 - accuracy: 0.91 - ETA: 1:06 - loss: 0.3044 - accuracy: 0.91 - ETA: 1:05 - loss: 0.3052 - accuracy: 0.91 - ETA: 1:04 - loss: 0.3048 - accuracy: 0.91 - ETA: 1:03 - loss: 0.3030 - accuracy: 0.91 - ETA: 1:02 - loss: 0.3050 - accuracy: 0.91 - ETA: 1:01 - loss: 0.3085 - accuracy: 0.91 - ETA: 1:00 - loss: 0.3091 - accuracy: 0.91 - ETA: 59s - loss: 0.3083 - accuracy: 0.9129 - ETA: 59s - loss: 0.3073 - accuracy: 0.913 - ETA: 58s - loss: 0.3051 - accuracy: 0.913 - ETA: 57s - loss: 0.3069 - accuracy: 0.912 - ETA: 56s - loss: 0.3071 - accuracy: 0.912 - ETA: 55s - loss: 0.3106 - accuracy: 0.910 - ETA: 54s - loss: 0.3103 - accuracy: 0.911 - ETA: 53s - loss: 0.3107 - accuracy: 0.910 - ETA: 52s - loss: 0.3145 - accuracy: 0.910 - ETA: 51s - loss: 0.3169 - accuracy: 0.910 - ETA: 51s - loss: 0.3141 - accuracy: 0.911 - ETA: 50s - loss: 0.3143 - accuracy: 0.911 - ETA: 49s - loss: 0.3118 - accuracy: 0.912 - ETA: 48s - loss: 0.3096 - accuracy: 0.913 - ETA: 47s - loss: 0.3098 - accuracy: 0.912 - ETA: 46s - loss: 0.3110 - accuracy: 0.912 - ETA: 45s - loss: 0.3111 - accuracy: 0.912 - ETA: 44s - loss: 0.3136 - accuracy: 0.911 - ETA: 44s - loss: 0.3099 - accuracy: 0.912 - ETA: 43s - loss: 0.3112 - accuracy: 0.912 - ETA: 42s - loss: 0.3090 - accuracy: 0.913 - ETA: 41s - loss: 0.3090 - accuracy: 0.913 - ETA: 40s - loss: 0.3074 - accuracy: 0.913 - ETA: 39s - loss: 0.3061 - accuracy: 0.914 - ETA: 39s - loss: 0.3070 - accuracy: 0.913 - ETA: 38s - loss: 0.3075 - accuracy: 0.913 - ETA: 37s - loss: 0.3064 - accuracy: 0.913 - ETA: 36s - loss: 0.3066 - accuracy: 0.913 - ETA: 35s - loss: 0.3083 - accuracy: 0.912 - ETA: 34s - loss: 0.3082 - accuracy: 0.912 - ETA: 33s - loss: 0.3096 - accuracy: 0.911 - ETA: 32s - loss: 0.3118 - accuracy: 0.912 - ETA: 31s - loss: 0.3154 - accuracy: 0.911 - ETA: 30s - loss: 0.3141 - accuracy: 0.911 - ETA: 29s - loss: 0.3133 - accuracy: 0.912 - ETA: 28s - loss: 0.3126 - accuracy: 0.912 - ETA: 28s - loss: 0.3131 - accuracy: 0.912 - ETA: 27s - loss: 0.3130 - accuracy: 0.912 - ETA: 26s - loss: 0.3125 - accuracy: 0.912 - ETA: 25s - loss: 0.3161 - accuracy: 0.912 - ETA: 24s - loss: 0.3153 - accuracy: 0.912 - ETA: 23s - loss: 0.3161 - accuracy: 0.912 - ETA: 22s - loss: 0.3169 - accuracy: 0.911 - ETA: 21s - loss: 0.3169 - accuracy: 0.912 - ETA: 20s - loss: 0.3165 - accuracy: 0.912 - ETA: 19s - loss: 0.3168 - accuracy: 0.911 - ETA: 18s - loss: 0.3159 - accuracy: 0.912 - ETA: 17s - loss: 0.3162 - accuracy: 0.912 - ETA: 17s - loss: 0.3162 - accuracy: 0.912 - ETA: 16s - loss: 0.3160 - accuracy: 0.912 - ETA: 15s - loss: 0.3167 - accuracy: 0.912 - ETA: 14s - loss: 0.3159 - accuracy: 0.912 - ETA: 13s - loss: 0.3192 - accuracy: 0.912 - ETA: 12s - loss: 0.3192 - accuracy: 0.912 - ETA: 11s - loss: 0.3195 - accuracy: 0.912 - ETA: 10s - loss: 0.3206 - accuracy: 0.911 - ETA: 9s - loss: 0.3208 - accuracy: 0.911 - ETA: 8s - loss: 0.3203 - accuracy: 0.91 - ETA: 7s - loss: 0.3203 - accuracy: 0.91 - ETA: 7s - loss: 0.3199 - accuracy: 0.91 - ETA: 6s - loss: 0.3199 - accuracy: 0.91 - ETA: 5s - loss: 0.3204 - accuracy: 0.91 - ETA: 4s - loss: 0.3207 - accuracy: 0.91 - ETA: 3s - loss: 0.3196 - accuracy: 0.91 - ETA: 2s - loss: 0.3202 - accuracy: 0.91 - ETA: 1s - loss: 0.3211 - accuracy: 0.91 - ETA: 0s - loss: 0.3217 - accuracy: 0.91 - 106s 8ms/step - loss: 0.3213 - accuracy: 0.9110 - val_loss: 3.2257 - val_accuracy: 0.3487\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:29 - loss: 0.4191 - accuracy: 0.87 - ETA: 1:30 - loss: 0.3209 - accuracy: 0.91 - ETA: 1:32 - loss: 0.3122 - accuracy: 0.90 - ETA: 1:30 - loss: 0.3064 - accuracy: 0.90 - ETA: 1:29 - loss: 0.2881 - accuracy: 0.90 - ETA: 1:29 - loss: 0.2781 - accuracy: 0.91 - ETA: 1:29 - loss: 0.2610 - accuracy: 0.92 - ETA: 1:27 - loss: 0.2634 - accuracy: 0.92 - ETA: 1:25 - loss: 0.2682 - accuracy: 0.91 - ETA: 1:25 - loss: 0.2736 - accuracy: 0.91 - ETA: 1:24 - loss: 0.2822 - accuracy: 0.91 - ETA: 1:23 - loss: 0.2919 - accuracy: 0.91 - ETA: 1:22 - loss: 0.2822 - accuracy: 0.91 - ETA: 1:20 - loss: 0.2814 - accuracy: 0.91 - ETA: 1:19 - loss: 0.2916 - accuracy: 0.91 - ETA: 1:18 - loss: 0.2911 - accuracy: 0.91 - ETA: 1:17 - loss: 0.2837 - accuracy: 0.91 - ETA: 1:16 - loss: 0.2808 - accuracy: 0.91 - ETA: 1:15 - loss: 0.2789 - accuracy: 0.91 - ETA: 1:14 - loss: 0.2823 - accuracy: 0.91 - ETA: 1:13 - loss: 0.2896 - accuracy: 0.91 - ETA: 1:12 - loss: 0.2922 - accuracy: 0.91 - ETA: 1:11 - loss: 0.2866 - accuracy: 0.91 - ETA: 1:10 - loss: 0.2879 - accuracy: 0.91 - ETA: 1:09 - loss: 0.2844 - accuracy: 0.91 - ETA: 1:08 - loss: 0.2890 - accuracy: 0.91 - ETA: 1:07 - loss: 0.2873 - accuracy: 0.91 - ETA: 1:07 - loss: 0.2835 - accuracy: 0.91 - ETA: 1:06 - loss: 0.2816 - accuracy: 0.91 - ETA: 1:05 - loss: 0.2812 - accuracy: 0.91 - ETA: 1:04 - loss: 0.2822 - accuracy: 0.91 - ETA: 1:03 - loss: 0.2866 - accuracy: 0.91 - ETA: 1:02 - loss: 0.2871 - accuracy: 0.91 - ETA: 1:01 - loss: 0.2854 - accuracy: 0.91 - ETA: 1:00 - loss: 0.2854 - accuracy: 0.91 - ETA: 59s - loss: 0.2909 - accuracy: 0.9180 - ETA: 59s - loss: 0.2899 - accuracy: 0.917 - ETA: 58s - loss: 0.2891 - accuracy: 0.917 - ETA: 57s - loss: 0.2906 - accuracy: 0.917 - ETA: 56s - loss: 0.2914 - accuracy: 0.916 - ETA: 55s - loss: 0.2908 - accuracy: 0.916 - ETA: 54s - loss: 0.2899 - accuracy: 0.916 - ETA: 53s - loss: 0.2933 - accuracy: 0.916 - ETA: 52s - loss: 0.2931 - accuracy: 0.916 - ETA: 51s - loss: 0.2924 - accuracy: 0.916 - ETA: 50s - loss: 0.2928 - accuracy: 0.916 - ETA: 49s - loss: 0.2939 - accuracy: 0.916 - ETA: 48s - loss: 0.2960 - accuracy: 0.915 - ETA: 48s - loss: 0.2996 - accuracy: 0.914 - ETA: 47s - loss: 0.2993 - accuracy: 0.914 - ETA: 46s - loss: 0.3003 - accuracy: 0.913 - ETA: 45s - loss: 0.3026 - accuracy: 0.913 - ETA: 44s - loss: 0.3001 - accuracy: 0.914 - ETA: 43s - loss: 0.3001 - accuracy: 0.914 - ETA: 42s - loss: 0.2999 - accuracy: 0.913 - ETA: 41s - loss: 0.3013 - accuracy: 0.912 - ETA: 40s - loss: 0.3028 - accuracy: 0.913 - ETA: 39s - loss: 0.3000 - accuracy: 0.913 - ETA: 38s - loss: 0.2995 - accuracy: 0.914 - ETA: 37s - loss: 0.2996 - accuracy: 0.913 - ETA: 36s - loss: 0.3009 - accuracy: 0.913 - ETA: 36s - loss: 0.2996 - accuracy: 0.913 - ETA: 35s - loss: 0.3028 - accuracy: 0.913 - ETA: 34s - loss: 0.3018 - accuracy: 0.913 - ETA: 33s - loss: 0.3003 - accuracy: 0.914 - ETA: 32s - loss: 0.3033 - accuracy: 0.913 - ETA: 31s - loss: 0.3019 - accuracy: 0.913 - ETA: 30s - loss: 0.3051 - accuracy: 0.912 - ETA: 29s - loss: 0.3051 - accuracy: 0.912 - ETA: 28s - loss: 0.3073 - accuracy: 0.911 - ETA: 27s - loss: 0.3061 - accuracy: 0.912 - ETA: 27s - loss: 0.3063 - accuracy: 0.912 - ETA: 26s - loss: 0.3069 - accuracy: 0.911 - ETA: 25s - loss: 0.3051 - accuracy: 0.912 - ETA: 24s - loss: 0.3047 - accuracy: 0.912 - ETA: 23s - loss: 0.3041 - accuracy: 0.912 - ETA: 22s - loss: 0.3038 - accuracy: 0.912 - ETA: 21s - loss: 0.3058 - accuracy: 0.912 - ETA: 20s - loss: 0.3039 - accuracy: 0.912 - ETA: 19s - loss: 0.3033 - accuracy: 0.913 - ETA: 18s - loss: 0.3032 - accuracy: 0.913 - ETA: 17s - loss: 0.3028 - accuracy: 0.913 - ETA: 17s - loss: 0.3022 - accuracy: 0.913 - ETA: 16s - loss: 0.3024 - accuracy: 0.913 - ETA: 15s - loss: 0.3035 - accuracy: 0.913 - ETA: 14s - loss: 0.3028 - accuracy: 0.913 - ETA: 13s - loss: 0.3031 - accuracy: 0.913 - ETA: 12s - loss: 0.3042 - accuracy: 0.913 - ETA: 11s - loss: 0.3056 - accuracy: 0.913 - ETA: 10s - loss: 0.3059 - accuracy: 0.913 - ETA: 9s - loss: 0.3044 - accuracy: 0.913 - ETA: 8s - loss: 0.3050 - accuracy: 0.91 - ETA: 7s - loss: 0.3039 - accuracy: 0.91 - ETA: 7s - loss: 0.3015 - accuracy: 0.91 - ETA: 6s - loss: 0.2999 - accuracy: 0.91 - ETA: 5s - loss: 0.2996 - accuracy: 0.91 - ETA: 4s - loss: 0.3006 - accuracy: 0.91 - ETA: 3s - loss: 0.3004 - accuracy: 0.91 - ETA: 2s - loss: 0.2995 - accuracy: 0.91 - ETA: 1s - loss: 0.2997 - accuracy: 0.91 - ETA: 0s - loss: 0.3001 - accuracy: 0.91 - 106s 8ms/step - loss: 0.3012 - accuracy: 0.9141 - val_loss: 3.1833 - val_accuracy: 0.3627\n",
      "Epoch 88/100\n",
      "13022/13022 [==============================] - ETA: 1:34 - loss: 0.3046 - accuracy: 0.89 - ETA: 1:30 - loss: 0.2278 - accuracy: 0.92 - ETA: 1:28 - loss: 0.1995 - accuracy: 0.93 - ETA: 1:28 - loss: 0.2341 - accuracy: 0.93 - ETA: 1:28 - loss: 0.2753 - accuracy: 0.91 - ETA: 1:26 - loss: 0.2696 - accuracy: 0.91 - ETA: 1:24 - loss: 0.2489 - accuracy: 0.92 - ETA: 1:25 - loss: 0.2671 - accuracy: 0.92 - ETA: 1:23 - loss: 0.2735 - accuracy: 0.92 - ETA: 1:22 - loss: 0.2789 - accuracy: 0.91 - ETA: 1:21 - loss: 0.2825 - accuracy: 0.91 - ETA: 1:20 - loss: 0.2893 - accuracy: 0.91 - ETA: 1:19 - loss: 0.2961 - accuracy: 0.91 - ETA: 1:18 - loss: 0.2950 - accuracy: 0.91 - ETA: 1:17 - loss: 0.2955 - accuracy: 0.91 - ETA: 1:17 - loss: 0.2942 - accuracy: 0.91 - ETA: 1:16 - loss: 0.2942 - accuracy: 0.91 - ETA: 1:15 - loss: 0.2889 - accuracy: 0.91 - ETA: 1:15 - loss: 0.2821 - accuracy: 0.91 - ETA: 1:14 - loss: 0.2891 - accuracy: 0.91 - ETA: 1:13 - loss: 0.2939 - accuracy: 0.91 - ETA: 1:12 - loss: 0.2921 - accuracy: 0.91 - ETA: 1:11 - loss: 0.2877 - accuracy: 0.91 - ETA: 1:10 - loss: 0.2825 - accuracy: 0.91 - ETA: 1:09 - loss: 0.2897 - accuracy: 0.91 - ETA: 1:08 - loss: 0.2850 - accuracy: 0.91 - ETA: 1:07 - loss: 0.2830 - accuracy: 0.91 - ETA: 1:06 - loss: 0.2804 - accuracy: 0.91 - ETA: 1:05 - loss: 0.2772 - accuracy: 0.91 - ETA: 1:05 - loss: 0.2796 - accuracy: 0.91 - ETA: 1:04 - loss: 0.2784 - accuracy: 0.91 - ETA: 1:03 - loss: 0.2786 - accuracy: 0.91 - ETA: 1:02 - loss: 0.2778 - accuracy: 0.91 - ETA: 1:01 - loss: 0.2759 - accuracy: 0.91 - ETA: 1:00 - loss: 0.2808 - accuracy: 0.91 - ETA: 1:00 - loss: 0.2846 - accuracy: 0.91 - ETA: 59s - loss: 0.2849 - accuracy: 0.9183 - ETA: 58s - loss: 0.2876 - accuracy: 0.917 - ETA: 57s - loss: 0.2877 - accuracy: 0.918 - ETA: 56s - loss: 0.2894 - accuracy: 0.917 - ETA: 55s - loss: 0.2903 - accuracy: 0.916 - ETA: 54s - loss: 0.2891 - accuracy: 0.917 - ETA: 53s - loss: 0.2909 - accuracy: 0.917 - ETA: 52s - loss: 0.2937 - accuracy: 0.916 - ETA: 51s - loss: 0.2944 - accuracy: 0.915 - ETA: 50s - loss: 0.2921 - accuracy: 0.916 - ETA: 49s - loss: 0.2911 - accuracy: 0.916 - ETA: 48s - loss: 0.2899 - accuracy: 0.917 - ETA: 47s - loss: 0.2915 - accuracy: 0.916 - ETA: 46s - loss: 0.2911 - accuracy: 0.916 - ETA: 45s - loss: 0.2914 - accuracy: 0.916 - ETA: 45s - loss: 0.2909 - accuracy: 0.916 - ETA: 44s - loss: 0.2891 - accuracy: 0.916 - ETA: 43s - loss: 0.2888 - accuracy: 0.917 - ETA: 42s - loss: 0.2869 - accuracy: 0.917 - ETA: 41s - loss: 0.2867 - accuracy: 0.917 - ETA: 40s - loss: 0.2912 - accuracy: 0.916 - ETA: 39s - loss: 0.2907 - accuracy: 0.916 - ETA: 38s - loss: 0.2927 - accuracy: 0.916 - ETA: 37s - loss: 0.2927 - accuracy: 0.915 - ETA: 36s - loss: 0.2942 - accuracy: 0.915 - ETA: 35s - loss: 0.2921 - accuracy: 0.916 - ETA: 35s - loss: 0.2919 - accuracy: 0.916 - ETA: 34s - loss: 0.2911 - accuracy: 0.916 - ETA: 33s - loss: 0.2924 - accuracy: 0.916 - ETA: 32s - loss: 0.2923 - accuracy: 0.916 - ETA: 31s - loss: 0.2955 - accuracy: 0.915 - ETA: 30s - loss: 0.2960 - accuracy: 0.915 - ETA: 29s - loss: 0.2976 - accuracy: 0.915 - ETA: 28s - loss: 0.2960 - accuracy: 0.915 - ETA: 27s - loss: 0.2954 - accuracy: 0.915 - ETA: 26s - loss: 0.2943 - accuracy: 0.915 - ETA: 25s - loss: 0.2961 - accuracy: 0.915 - ETA: 25s - loss: 0.2975 - accuracy: 0.914 - ETA: 24s - loss: 0.2962 - accuracy: 0.915 - ETA: 23s - loss: 0.2983 - accuracy: 0.914 - ETA: 22s - loss: 0.3001 - accuracy: 0.914 - ETA: 21s - loss: 0.2992 - accuracy: 0.915 - ETA: 20s - loss: 0.3011 - accuracy: 0.914 - ETA: 19s - loss: 0.3007 - accuracy: 0.914 - ETA: 18s - loss: 0.3017 - accuracy: 0.914 - ETA: 17s - loss: 0.3011 - accuracy: 0.914 - ETA: 16s - loss: 0.3006 - accuracy: 0.914 - ETA: 16s - loss: 0.3027 - accuracy: 0.914 - ETA: 15s - loss: 0.3035 - accuracy: 0.914 - ETA: 14s - loss: 0.3037 - accuracy: 0.914 - ETA: 13s - loss: 0.3031 - accuracy: 0.914 - ETA: 12s - loss: 0.3017 - accuracy: 0.914 - ETA: 11s - loss: 0.3013 - accuracy: 0.914 - ETA: 10s - loss: 0.3000 - accuracy: 0.914 - ETA: 9s - loss: 0.2991 - accuracy: 0.915 - ETA: 8s - loss: 0.2995 - accuracy: 0.91 - ETA: 7s - loss: 0.2991 - accuracy: 0.91 - ETA: 7s - loss: 0.3017 - accuracy: 0.91 - ETA: 6s - loss: 0.3024 - accuracy: 0.91 - ETA: 5s - loss: 0.3026 - accuracy: 0.91 - ETA: 4s - loss: 0.3033 - accuracy: 0.91 - ETA: 3s - loss: 0.3027 - accuracy: 0.91 - ETA: 2s - loss: 0.3027 - accuracy: 0.91 - ETA: 1s - loss: 0.3018 - accuracy: 0.91 - ETA: 0s - loss: 0.3036 - accuracy: 0.91 - 106s 8ms/step - loss: 0.3032 - accuracy: 0.9142 - val_loss: 3.2541 - val_accuracy: 0.3600\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 0.1734 - accuracy: 0.96 - ETA: 1:28 - loss: 0.2092 - accuracy: 0.94 - ETA: 1:27 - loss: 0.2545 - accuracy: 0.93 - ETA: 1:28 - loss: 0.2825 - accuracy: 0.93 - ETA: 1:29 - loss: 0.2869 - accuracy: 0.92 - ETA: 1:28 - loss: 0.2911 - accuracy: 0.91 - ETA: 1:28 - loss: 0.2989 - accuracy: 0.91 - ETA: 1:27 - loss: 0.2933 - accuracy: 0.91 - ETA: 1:27 - loss: 0.3086 - accuracy: 0.91 - ETA: 1:25 - loss: 0.3191 - accuracy: 0.91 - ETA: 1:24 - loss: 0.3171 - accuracy: 0.91 - ETA: 1:23 - loss: 0.3076 - accuracy: 0.91 - ETA: 1:21 - loss: 0.2980 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3048 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3206 - accuracy: 0.91 - ETA: 1:19 - loss: 0.3153 - accuracy: 0.91 - ETA: 1:17 - loss: 0.3181 - accuracy: 0.91 - ETA: 1:16 - loss: 0.3123 - accuracy: 0.91 - ETA: 1:15 - loss: 0.3078 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3055 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3075 - accuracy: 0.91 - ETA: 1:13 - loss: 0.3021 - accuracy: 0.91 - ETA: 1:12 - loss: 0.3032 - accuracy: 0.91 - ETA: 1:11 - loss: 0.2999 - accuracy: 0.91 - ETA: 1:10 - loss: 0.3013 - accuracy: 0.91 - ETA: 1:09 - loss: 0.3111 - accuracy: 0.91 - ETA: 1:08 - loss: 0.3159 - accuracy: 0.91 - ETA: 1:07 - loss: 0.3161 - accuracy: 0.91 - ETA: 1:06 - loss: 0.3142 - accuracy: 0.91 - ETA: 1:05 - loss: 0.3131 - accuracy: 0.91 - ETA: 1:04 - loss: 0.3160 - accuracy: 0.91 - ETA: 1:03 - loss: 0.3150 - accuracy: 0.91 - ETA: 1:02 - loss: 0.3150 - accuracy: 0.91 - ETA: 1:01 - loss: 0.3160 - accuracy: 0.91 - ETA: 1:00 - loss: 0.3145 - accuracy: 0.91 - ETA: 59s - loss: 0.3154 - accuracy: 0.9132 - ETA: 59s - loss: 0.3145 - accuracy: 0.912 - ETA: 58s - loss: 0.3163 - accuracy: 0.912 - ETA: 57s - loss: 0.3165 - accuracy: 0.911 - ETA: 56s - loss: 0.3176 - accuracy: 0.911 - ETA: 55s - loss: 0.3165 - accuracy: 0.912 - ETA: 54s - loss: 0.3178 - accuracy: 0.911 - ETA: 53s - loss: 0.3182 - accuracy: 0.911 - ETA: 52s - loss: 0.3158 - accuracy: 0.912 - ETA: 51s - loss: 0.3156 - accuracy: 0.912 - ETA: 50s - loss: 0.3167 - accuracy: 0.912 - ETA: 49s - loss: 0.3176 - accuracy: 0.911 - ETA: 48s - loss: 0.3201 - accuracy: 0.910 - ETA: 48s - loss: 0.3205 - accuracy: 0.911 - ETA: 47s - loss: 0.3197 - accuracy: 0.911 - ETA: 46s - loss: 0.3204 - accuracy: 0.911 - ETA: 45s - loss: 0.3198 - accuracy: 0.911 - ETA: 44s - loss: 0.3194 - accuracy: 0.911 - ETA: 43s - loss: 0.3190 - accuracy: 0.910 - ETA: 42s - loss: 0.3198 - accuracy: 0.910 - ETA: 41s - loss: 0.3165 - accuracy: 0.911 - ETA: 40s - loss: 0.3149 - accuracy: 0.911 - ETA: 39s - loss: 0.3149 - accuracy: 0.911 - ETA: 38s - loss: 0.3144 - accuracy: 0.911 - ETA: 37s - loss: 0.3144 - accuracy: 0.911 - ETA: 37s - loss: 0.3137 - accuracy: 0.912 - ETA: 36s - loss: 0.3126 - accuracy: 0.912 - ETA: 35s - loss: 0.3132 - accuracy: 0.912 - ETA: 34s - loss: 0.3140 - accuracy: 0.912 - ETA: 33s - loss: 0.3169 - accuracy: 0.911 - ETA: 32s - loss: 0.3183 - accuracy: 0.911 - ETA: 31s - loss: 0.3170 - accuracy: 0.911 - ETA: 30s - loss: 0.3187 - accuracy: 0.911 - ETA: 29s - loss: 0.3174 - accuracy: 0.911 - ETA: 28s - loss: 0.3167 - accuracy: 0.912 - ETA: 27s - loss: 0.3149 - accuracy: 0.912 - ETA: 27s - loss: 0.3140 - accuracy: 0.912 - ETA: 26s - loss: 0.3152 - accuracy: 0.912 - ETA: 25s - loss: 0.3180 - accuracy: 0.911 - ETA: 24s - loss: 0.3175 - accuracy: 0.911 - ETA: 23s - loss: 0.3178 - accuracy: 0.911 - ETA: 22s - loss: 0.3166 - accuracy: 0.912 - ETA: 21s - loss: 0.3170 - accuracy: 0.912 - ETA: 20s - loss: 0.3159 - accuracy: 0.912 - ETA: 19s - loss: 0.3152 - accuracy: 0.912 - ETA: 18s - loss: 0.3137 - accuracy: 0.912 - ETA: 17s - loss: 0.3132 - accuracy: 0.912 - ETA: 17s - loss: 0.3137 - accuracy: 0.912 - ETA: 16s - loss: 0.3133 - accuracy: 0.912 - ETA: 15s - loss: 0.3142 - accuracy: 0.912 - ETA: 14s - loss: 0.3142 - accuracy: 0.912 - ETA: 13s - loss: 0.3140 - accuracy: 0.912 - ETA: 12s - loss: 0.3146 - accuracy: 0.912 - ETA: 11s - loss: 0.3154 - accuracy: 0.911 - ETA: 10s - loss: 0.3135 - accuracy: 0.912 - ETA: 9s - loss: 0.3126 - accuracy: 0.912 - ETA: 8s - loss: 0.3131 - accuracy: 0.91 - ETA: 7s - loss: 0.3145 - accuracy: 0.91 - ETA: 7s - loss: 0.3142 - accuracy: 0.91 - ETA: 6s - loss: 0.3132 - accuracy: 0.91 - ETA: 5s - loss: 0.3134 - accuracy: 0.91 - ETA: 4s - loss: 0.3134 - accuracy: 0.91 - ETA: 3s - loss: 0.3137 - accuracy: 0.91 - ETA: 2s - loss: 0.3121 - accuracy: 0.91 - ETA: 1s - loss: 0.3125 - accuracy: 0.91 - ETA: 0s - loss: 0.3134 - accuracy: 0.91 - 106s 8ms/step - loss: 0.3133 - accuracy: 0.9126 - val_loss: 3.0887 - val_accuracy: 0.3596\n",
      "Epoch 90/100\n",
      "13022/13022 [==============================] - ETA: 1:36 - loss: 0.4182 - accuracy: 0.89 - ETA: 1:32 - loss: 0.3576 - accuracy: 0.89 - ETA: 1:29 - loss: 0.3362 - accuracy: 0.89 - ETA: 1:27 - loss: 0.3400 - accuracy: 0.90 - ETA: 1:25 - loss: 0.3304 - accuracy: 0.90 - ETA: 1:27 - loss: 0.3368 - accuracy: 0.90 - ETA: 1:26 - loss: 0.3184 - accuracy: 0.91 - ETA: 1:26 - loss: 0.3209 - accuracy: 0.91 - ETA: 1:25 - loss: 0.3063 - accuracy: 0.91 - ETA: 1:25 - loss: 0.2951 - accuracy: 0.92 - ETA: 1:25 - loss: 0.3008 - accuracy: 0.92 - ETA: 1:24 - loss: 0.3064 - accuracy: 0.92 - ETA: 1:24 - loss: 0.3033 - accuracy: 0.92 - ETA: 1:23 - loss: 0.3054 - accuracy: 0.92 - ETA: 1:22 - loss: 0.3119 - accuracy: 0.92 - ETA: 1:21 - loss: 0.3087 - accuracy: 0.92 - ETA: 1:21 - loss: 0.3024 - accuracy: 0.92 - ETA: 1:20 - loss: 0.3007 - accuracy: 0.92 - ETA: 1:19 - loss: 0.2984 - accuracy: 0.92 - ETA: 1:17 - loss: 0.2978 - accuracy: 0.92 - ETA: 1:16 - loss: 0.2912 - accuracy: 0.92 - ETA: 1:15 - loss: 0.2871 - accuracy: 0.92 - ETA: 1:14 - loss: 0.2812 - accuracy: 0.92 - ETA: 1:13 - loss: 0.2834 - accuracy: 0.92 - ETA: 1:12 - loss: 0.2842 - accuracy: 0.92 - ETA: 1:11 - loss: 0.2811 - accuracy: 0.92 - ETA: 1:09 - loss: 0.2778 - accuracy: 0.92 - ETA: 1:08 - loss: 0.2773 - accuracy: 0.92 - ETA: 1:07 - loss: 0.2807 - accuracy: 0.92 - ETA: 1:06 - loss: 0.2797 - accuracy: 0.92 - ETA: 1:05 - loss: 0.2798 - accuracy: 0.92 - ETA: 1:04 - loss: 0.2787 - accuracy: 0.92 - ETA: 1:04 - loss: 0.2775 - accuracy: 0.92 - ETA: 1:03 - loss: 0.2820 - accuracy: 0.92 - ETA: 1:02 - loss: 0.2827 - accuracy: 0.92 - ETA: 1:01 - loss: 0.2831 - accuracy: 0.92 - ETA: 1:00 - loss: 0.2892 - accuracy: 0.92 - ETA: 59s - loss: 0.2874 - accuracy: 0.9208 - ETA: 58s - loss: 0.2879 - accuracy: 0.920 - ETA: 57s - loss: 0.2847 - accuracy: 0.921 - ETA: 56s - loss: 0.2830 - accuracy: 0.922 - ETA: 55s - loss: 0.2863 - accuracy: 0.921 - ETA: 54s - loss: 0.2869 - accuracy: 0.921 - ETA: 53s - loss: 0.2891 - accuracy: 0.920 - ETA: 52s - loss: 0.2921 - accuracy: 0.920 - ETA: 51s - loss: 0.2928 - accuracy: 0.919 - ETA: 50s - loss: 0.2934 - accuracy: 0.919 - ETA: 49s - loss: 0.2947 - accuracy: 0.918 - ETA: 48s - loss: 0.2955 - accuracy: 0.917 - ETA: 47s - loss: 0.2946 - accuracy: 0.917 - ETA: 46s - loss: 0.2930 - accuracy: 0.918 - ETA: 46s - loss: 0.2927 - accuracy: 0.918 - ETA: 45s - loss: 0.2957 - accuracy: 0.917 - ETA: 44s - loss: 0.2971 - accuracy: 0.916 - ETA: 43s - loss: 0.2974 - accuracy: 0.916 - ETA: 42s - loss: 0.2980 - accuracy: 0.916 - ETA: 41s - loss: 0.2996 - accuracy: 0.916 - ETA: 40s - loss: 0.3014 - accuracy: 0.916 - ETA: 39s - loss: 0.3029 - accuracy: 0.916 - ETA: 38s - loss: 0.3014 - accuracy: 0.916 - ETA: 37s - loss: 0.3027 - accuracy: 0.916 - ETA: 36s - loss: 0.3014 - accuracy: 0.916 - ETA: 35s - loss: 0.3021 - accuracy: 0.916 - ETA: 34s - loss: 0.3024 - accuracy: 0.916 - ETA: 33s - loss: 0.3018 - accuracy: 0.916 - ETA: 32s - loss: 0.3029 - accuracy: 0.915 - ETA: 31s - loss: 0.3032 - accuracy: 0.915 - ETA: 31s - loss: 0.3025 - accuracy: 0.916 - ETA: 30s - loss: 0.3019 - accuracy: 0.916 - ETA: 29s - loss: 0.3007 - accuracy: 0.916 - ETA: 28s - loss: 0.2998 - accuracy: 0.916 - ETA: 27s - loss: 0.2994 - accuracy: 0.916 - ETA: 26s - loss: 0.2997 - accuracy: 0.916 - ETA: 25s - loss: 0.2990 - accuracy: 0.916 - ETA: 24s - loss: 0.3008 - accuracy: 0.916 - ETA: 23s - loss: 0.2999 - accuracy: 0.916 - ETA: 22s - loss: 0.3001 - accuracy: 0.916 - ETA: 21s - loss: 0.3008 - accuracy: 0.916 - ETA: 20s - loss: 0.3014 - accuracy: 0.916 - ETA: 19s - loss: 0.3029 - accuracy: 0.915 - ETA: 18s - loss: 0.3023 - accuracy: 0.915 - ETA: 18s - loss: 0.3025 - accuracy: 0.915 - ETA: 17s - loss: 0.3024 - accuracy: 0.915 - ETA: 16s - loss: 0.3010 - accuracy: 0.916 - ETA: 15s - loss: 0.3005 - accuracy: 0.915 - ETA: 14s - loss: 0.3003 - accuracy: 0.916 - ETA: 13s - loss: 0.3009 - accuracy: 0.915 - ETA: 12s - loss: 0.3002 - accuracy: 0.916 - ETA: 11s - loss: 0.3005 - accuracy: 0.916 - ETA: 10s - loss: 0.3008 - accuracy: 0.916 - ETA: 9s - loss: 0.3025 - accuracy: 0.915 - ETA: 8s - loss: 0.3027 - accuracy: 0.91 - ETA: 8s - loss: 0.3048 - accuracy: 0.91 - ETA: 7s - loss: 0.3064 - accuracy: 0.91 - ETA: 6s - loss: 0.3066 - accuracy: 0.91 - ETA: 5s - loss: 0.3069 - accuracy: 0.91 - ETA: 4s - loss: 0.3068 - accuracy: 0.91 - ETA: 3s - loss: 0.3075 - accuracy: 0.91 - ETA: 2s - loss: 0.3082 - accuracy: 0.91 - ETA: 1s - loss: 0.3084 - accuracy: 0.91 - ETA: 0s - loss: 0.3091 - accuracy: 0.91 - 106s 8ms/step - loss: 0.3095 - accuracy: 0.9139 - val_loss: 3.1192 - val_accuracy: 0.3632\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:34 - loss: 0.3867 - accuracy: 0.91 - ETA: 1:34 - loss: 0.3024 - accuracy: 0.92 - ETA: 1:32 - loss: 0.2646 - accuracy: 0.93 - ETA: 1:31 - loss: 0.2642 - accuracy: 0.93 - ETA: 1:29 - loss: 0.2597 - accuracy: 0.92 - ETA: 1:27 - loss: 0.2481 - accuracy: 0.93 - ETA: 1:27 - loss: 0.2745 - accuracy: 0.92 - ETA: 1:26 - loss: 0.2639 - accuracy: 0.93 - ETA: 1:25 - loss: 0.2761 - accuracy: 0.92 - ETA: 1:23 - loss: 0.2791 - accuracy: 0.92 - ETA: 1:22 - loss: 0.2777 - accuracy: 0.92 - ETA: 1:21 - loss: 0.2883 - accuracy: 0.92 - ETA: 1:20 - loss: 0.2763 - accuracy: 0.92 - ETA: 1:19 - loss: 0.2815 - accuracy: 0.92 - ETA: 1:18 - loss: 0.2898 - accuracy: 0.92 - ETA: 1:17 - loss: 0.2950 - accuracy: 0.92 - ETA: 1:16 - loss: 0.2892 - accuracy: 0.92 - ETA: 1:15 - loss: 0.2903 - accuracy: 0.92 - ETA: 1:14 - loss: 0.2865 - accuracy: 0.92 - ETA: 1:13 - loss: 0.2817 - accuracy: 0.92 - ETA: 1:13 - loss: 0.2787 - accuracy: 0.92 - ETA: 1:12 - loss: 0.2813 - accuracy: 0.92 - ETA: 1:11 - loss: 0.2795 - accuracy: 0.92 - ETA: 1:10 - loss: 0.2871 - accuracy: 0.92 - ETA: 1:09 - loss: 0.2879 - accuracy: 0.92 - ETA: 1:08 - loss: 0.2839 - accuracy: 0.92 - ETA: 1:07 - loss: 0.2844 - accuracy: 0.92 - ETA: 1:06 - loss: 0.2879 - accuracy: 0.92 - ETA: 1:05 - loss: 0.2854 - accuracy: 0.92 - ETA: 1:04 - loss: 0.2903 - accuracy: 0.92 - ETA: 1:03 - loss: 0.2888 - accuracy: 0.92 - ETA: 1:02 - loss: 0.2864 - accuracy: 0.92 - ETA: 1:01 - loss: 0.2836 - accuracy: 0.92 - ETA: 1:01 - loss: 0.2802 - accuracy: 0.92 - ETA: 1:00 - loss: 0.2805 - accuracy: 0.92 - ETA: 59s - loss: 0.2764 - accuracy: 0.9245 - ETA: 58s - loss: 0.2776 - accuracy: 0.924 - ETA: 57s - loss: 0.2767 - accuracy: 0.923 - ETA: 56s - loss: 0.2792 - accuracy: 0.923 - ETA: 56s - loss: 0.2802 - accuracy: 0.923 - ETA: 55s - loss: 0.2806 - accuracy: 0.922 - ETA: 54s - loss: 0.2828 - accuracy: 0.921 - ETA: 53s - loss: 0.2830 - accuracy: 0.921 - ETA: 52s - loss: 0.2852 - accuracy: 0.920 - ETA: 51s - loss: 0.2826 - accuracy: 0.921 - ETA: 50s - loss: 0.2870 - accuracy: 0.920 - ETA: 49s - loss: 0.2852 - accuracy: 0.920 - ETA: 48s - loss: 0.2872 - accuracy: 0.920 - ETA: 47s - loss: 0.2891 - accuracy: 0.919 - ETA: 46s - loss: 0.2883 - accuracy: 0.919 - ETA: 46s - loss: 0.2866 - accuracy: 0.919 - ETA: 45s - loss: 0.2856 - accuracy: 0.919 - ETA: 44s - loss: 0.2865 - accuracy: 0.919 - ETA: 43s - loss: 0.2863 - accuracy: 0.919 - ETA: 42s - loss: 0.2889 - accuracy: 0.918 - ETA: 41s - loss: 0.2909 - accuracy: 0.918 - ETA: 40s - loss: 0.2921 - accuracy: 0.917 - ETA: 39s - loss: 0.2898 - accuracy: 0.918 - ETA: 38s - loss: 0.2896 - accuracy: 0.918 - ETA: 37s - loss: 0.2888 - accuracy: 0.918 - ETA: 36s - loss: 0.2898 - accuracy: 0.918 - ETA: 36s - loss: 0.2905 - accuracy: 0.918 - ETA: 35s - loss: 0.2899 - accuracy: 0.918 - ETA: 34s - loss: 0.2893 - accuracy: 0.918 - ETA: 33s - loss: 0.2895 - accuracy: 0.917 - ETA: 32s - loss: 0.2904 - accuracy: 0.918 - ETA: 31s - loss: 0.2891 - accuracy: 0.918 - ETA: 30s - loss: 0.2897 - accuracy: 0.918 - ETA: 29s - loss: 0.2912 - accuracy: 0.918 - ETA: 28s - loss: 0.2918 - accuracy: 0.918 - ETA: 27s - loss: 0.2917 - accuracy: 0.918 - ETA: 26s - loss: 0.2920 - accuracy: 0.918 - ETA: 26s - loss: 0.2928 - accuracy: 0.918 - ETA: 25s - loss: 0.2932 - accuracy: 0.918 - ETA: 24s - loss: 0.2936 - accuracy: 0.918 - ETA: 23s - loss: 0.2939 - accuracy: 0.918 - ETA: 22s - loss: 0.2930 - accuracy: 0.918 - ETA: 21s - loss: 0.2918 - accuracy: 0.918 - ETA: 20s - loss: 0.2934 - accuracy: 0.918 - ETA: 19s - loss: 0.2952 - accuracy: 0.917 - ETA: 18s - loss: 0.2955 - accuracy: 0.917 - ETA: 17s - loss: 0.2946 - accuracy: 0.917 - ETA: 16s - loss: 0.2958 - accuracy: 0.917 - ETA: 16s - loss: 0.2945 - accuracy: 0.918 - ETA: 15s - loss: 0.2968 - accuracy: 0.917 - ETA: 14s - loss: 0.2992 - accuracy: 0.917 - ETA: 13s - loss: 0.2979 - accuracy: 0.917 - ETA: 12s - loss: 0.2988 - accuracy: 0.917 - ETA: 11s - loss: 0.2996 - accuracy: 0.917 - ETA: 10s - loss: 0.2989 - accuracy: 0.917 - ETA: 9s - loss: 0.2988 - accuracy: 0.917 - ETA: 8s - loss: 0.3025 - accuracy: 0.91 - ETA: 7s - loss: 0.3023 - accuracy: 0.91 - ETA: 7s - loss: 0.3014 - accuracy: 0.91 - ETA: 6s - loss: 0.3005 - accuracy: 0.91 - ETA: 5s - loss: 0.2996 - accuracy: 0.91 - ETA: 4s - loss: 0.3003 - accuracy: 0.91 - ETA: 3s - loss: 0.2995 - accuracy: 0.91 - ETA: 2s - loss: 0.2994 - accuracy: 0.91 - ETA: 1s - loss: 0.2996 - accuracy: 0.91 - ETA: 0s - loss: 0.3005 - accuracy: 0.91 - 105s 8ms/step - loss: 0.2992 - accuracy: 0.9173 - val_loss: 3.1474 - val_accuracy: 0.3652\n",
      "Epoch 92/100\n",
      "13022/13022 [==============================] - ETA: 1:36 - loss: 0.4484 - accuracy: 0.88 - ETA: 1:31 - loss: 0.3853 - accuracy: 0.89 - ETA: 1:31 - loss: 0.3559 - accuracy: 0.90 - ETA: 1:30 - loss: 0.3505 - accuracy: 0.91 - ETA: 1:29 - loss: 0.3342 - accuracy: 0.91 - ETA: 1:28 - loss: 0.3295 - accuracy: 0.91 - ETA: 1:26 - loss: 0.3063 - accuracy: 0.91 - ETA: 1:25 - loss: 0.3089 - accuracy: 0.91 - ETA: 1:25 - loss: 0.3153 - accuracy: 0.91 - ETA: 1:25 - loss: 0.3165 - accuracy: 0.91 - ETA: 1:24 - loss: 0.3022 - accuracy: 0.92 - ETA: 1:23 - loss: 0.2863 - accuracy: 0.92 - ETA: 1:21 - loss: 0.2978 - accuracy: 0.92 - ETA: 1:20 - loss: 0.2994 - accuracy: 0.92 - ETA: 1:19 - loss: 0.3135 - accuracy: 0.91 - ETA: 1:18 - loss: 0.3228 - accuracy: 0.91 - ETA: 1:17 - loss: 0.3276 - accuracy: 0.91 - ETA: 1:16 - loss: 0.3265 - accuracy: 0.91 - ETA: 1:15 - loss: 0.3279 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3250 - accuracy: 0.91 - ETA: 1:13 - loss: 0.3178 - accuracy: 0.91 - ETA: 1:12 - loss: 0.3166 - accuracy: 0.91 - ETA: 1:11 - loss: 0.3147 - accuracy: 0.91 - ETA: 1:10 - loss: 0.3176 - accuracy: 0.91 - ETA: 1:09 - loss: 0.3205 - accuracy: 0.91 - ETA: 1:08 - loss: 0.3243 - accuracy: 0.91 - ETA: 1:08 - loss: 0.3268 - accuracy: 0.91 - ETA: 1:07 - loss: 0.3216 - accuracy: 0.91 - ETA: 1:06 - loss: 0.3185 - accuracy: 0.91 - ETA: 1:05 - loss: 0.3164 - accuracy: 0.91 - ETA: 1:04 - loss: 0.3138 - accuracy: 0.91 - ETA: 1:03 - loss: 0.3198 - accuracy: 0.91 - ETA: 1:02 - loss: 0.3176 - accuracy: 0.91 - ETA: 1:01 - loss: 0.3136 - accuracy: 0.91 - ETA: 1:00 - loss: 0.3120 - accuracy: 0.91 - ETA: 59s - loss: 0.3134 - accuracy: 0.9160 - ETA: 58s - loss: 0.3143 - accuracy: 0.915 - ETA: 57s - loss: 0.3142 - accuracy: 0.915 - ETA: 56s - loss: 0.3144 - accuracy: 0.915 - ETA: 56s - loss: 0.3126 - accuracy: 0.915 - ETA: 55s - loss: 0.3124 - accuracy: 0.916 - ETA: 54s - loss: 0.3171 - accuracy: 0.915 - ETA: 53s - loss: 0.3181 - accuracy: 0.914 - ETA: 52s - loss: 0.3177 - accuracy: 0.915 - ETA: 51s - loss: 0.3152 - accuracy: 0.915 - ETA: 50s - loss: 0.3140 - accuracy: 0.916 - ETA: 49s - loss: 0.3139 - accuracy: 0.915 - ETA: 48s - loss: 0.3138 - accuracy: 0.916 - ETA: 47s - loss: 0.3126 - accuracy: 0.916 - ETA: 46s - loss: 0.3111 - accuracy: 0.917 - ETA: 46s - loss: 0.3087 - accuracy: 0.918 - ETA: 45s - loss: 0.3091 - accuracy: 0.917 - ETA: 44s - loss: 0.3071 - accuracy: 0.917 - ETA: 43s - loss: 0.3058 - accuracy: 0.918 - ETA: 42s - loss: 0.3084 - accuracy: 0.917 - ETA: 41s - loss: 0.3072 - accuracy: 0.917 - ETA: 40s - loss: 0.3066 - accuracy: 0.917 - ETA: 39s - loss: 0.3061 - accuracy: 0.917 - ETA: 38s - loss: 0.3053 - accuracy: 0.917 - ETA: 37s - loss: 0.3049 - accuracy: 0.917 - ETA: 36s - loss: 0.3023 - accuracy: 0.917 - ETA: 35s - loss: 0.3069 - accuracy: 0.917 - ETA: 35s - loss: 0.3070 - accuracy: 0.917 - ETA: 34s - loss: 0.3071 - accuracy: 0.917 - ETA: 33s - loss: 0.3075 - accuracy: 0.917 - ETA: 32s - loss: 0.3051 - accuracy: 0.917 - ETA: 31s - loss: 0.3062 - accuracy: 0.917 - ETA: 30s - loss: 0.3057 - accuracy: 0.917 - ETA: 29s - loss: 0.3079 - accuracy: 0.917 - ETA: 28s - loss: 0.3084 - accuracy: 0.917 - ETA: 27s - loss: 0.3076 - accuracy: 0.917 - ETA: 26s - loss: 0.3060 - accuracy: 0.917 - ETA: 26s - loss: 0.3088 - accuracy: 0.916 - ETA: 25s - loss: 0.3075 - accuracy: 0.917 - ETA: 24s - loss: 0.3073 - accuracy: 0.917 - ETA: 23s - loss: 0.3083 - accuracy: 0.917 - ETA: 22s - loss: 0.3088 - accuracy: 0.917 - ETA: 21s - loss: 0.3086 - accuracy: 0.917 - ETA: 20s - loss: 0.3083 - accuracy: 0.917 - ETA: 19s - loss: 0.3085 - accuracy: 0.916 - ETA: 18s - loss: 0.3092 - accuracy: 0.916 - ETA: 17s - loss: 0.3083 - accuracy: 0.917 - ETA: 16s - loss: 0.3078 - accuracy: 0.917 - ETA: 16s - loss: 0.3064 - accuracy: 0.917 - ETA: 15s - loss: 0.3084 - accuracy: 0.916 - ETA: 14s - loss: 0.3087 - accuracy: 0.916 - ETA: 13s - loss: 0.3081 - accuracy: 0.917 - ETA: 12s - loss: 0.3096 - accuracy: 0.916 - ETA: 11s - loss: 0.3078 - accuracy: 0.916 - ETA: 10s - loss: 0.3094 - accuracy: 0.916 - ETA: 9s - loss: 0.3098 - accuracy: 0.916 - ETA: 8s - loss: 0.3123 - accuracy: 0.91 - ETA: 7s - loss: 0.3125 - accuracy: 0.91 - ETA: 7s - loss: 0.3116 - accuracy: 0.91 - ETA: 6s - loss: 0.3111 - accuracy: 0.91 - ETA: 5s - loss: 0.3112 - accuracy: 0.91 - ETA: 4s - loss: 0.3114 - accuracy: 0.91 - ETA: 3s - loss: 0.3124 - accuracy: 0.91 - ETA: 2s - loss: 0.3117 - accuracy: 0.91 - ETA: 1s - loss: 0.3104 - accuracy: 0.91 - ETA: 0s - loss: 0.3100 - accuracy: 0.91 - 105s 8ms/step - loss: 0.3089 - accuracy: 0.9148 - val_loss: 3.1267 - val_accuracy: 0.3623\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.3108 - accuracy: 0.92 - ETA: 1:30 - loss: 0.3330 - accuracy: 0.92 - ETA: 1:28 - loss: 0.3540 - accuracy: 0.92 - ETA: 1:26 - loss: 0.3356 - accuracy: 0.92 - ETA: 1:25 - loss: 0.3346 - accuracy: 0.92 - ETA: 1:24 - loss: 0.3580 - accuracy: 0.91 - ETA: 1:23 - loss: 0.3685 - accuracy: 0.91 - ETA: 1:22 - loss: 0.3658 - accuracy: 0.90 - ETA: 1:21 - loss: 0.3568 - accuracy: 0.91 - ETA: 1:21 - loss: 0.3519 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3377 - accuracy: 0.91 - ETA: 1:19 - loss: 0.3268 - accuracy: 0.91 - ETA: 1:18 - loss: 0.3306 - accuracy: 0.91 - ETA: 1:17 - loss: 0.3334 - accuracy: 0.91 - ETA: 1:17 - loss: 0.3302 - accuracy: 0.91 - ETA: 1:16 - loss: 0.3306 - accuracy: 0.91 - ETA: 1:15 - loss: 0.3285 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3341 - accuracy: 0.91 - ETA: 1:13 - loss: 0.3335 - accuracy: 0.91 - ETA: 1:12 - loss: 0.3282 - accuracy: 0.91 - ETA: 1:12 - loss: 0.3319 - accuracy: 0.91 - ETA: 1:11 - loss: 0.3268 - accuracy: 0.91 - ETA: 1:10 - loss: 0.3243 - accuracy: 0.91 - ETA: 1:09 - loss: 0.3223 - accuracy: 0.91 - ETA: 1:08 - loss: 0.3236 - accuracy: 0.91 - ETA: 1:07 - loss: 0.3207 - accuracy: 0.91 - ETA: 1:06 - loss: 0.3153 - accuracy: 0.91 - ETA: 1:05 - loss: 0.3205 - accuracy: 0.91 - ETA: 1:05 - loss: 0.3231 - accuracy: 0.91 - ETA: 1:04 - loss: 0.3220 - accuracy: 0.91 - ETA: 1:03 - loss: 0.3230 - accuracy: 0.91 - ETA: 1:03 - loss: 0.3242 - accuracy: 0.91 - ETA: 1:02 - loss: 0.3237 - accuracy: 0.91 - ETA: 1:01 - loss: 0.3204 - accuracy: 0.91 - ETA: 1:00 - loss: 0.3214 - accuracy: 0.91 - ETA: 59s - loss: 0.3220 - accuracy: 0.9160 - ETA: 58s - loss: 0.3209 - accuracy: 0.916 - ETA: 57s - loss: 0.3185 - accuracy: 0.916 - ETA: 56s - loss: 0.3174 - accuracy: 0.917 - ETA: 55s - loss: 0.3142 - accuracy: 0.918 - ETA: 54s - loss: 0.3144 - accuracy: 0.918 - ETA: 53s - loss: 0.3132 - accuracy: 0.917 - ETA: 52s - loss: 0.3171 - accuracy: 0.916 - ETA: 52s - loss: 0.3160 - accuracy: 0.917 - ETA: 51s - loss: 0.3158 - accuracy: 0.917 - ETA: 50s - loss: 0.3132 - accuracy: 0.918 - ETA: 49s - loss: 0.3140 - accuracy: 0.917 - ETA: 48s - loss: 0.3161 - accuracy: 0.916 - ETA: 47s - loss: 0.3177 - accuracy: 0.915 - ETA: 46s - loss: 0.3193 - accuracy: 0.915 - ETA: 45s - loss: 0.3173 - accuracy: 0.915 - ETA: 44s - loss: 0.3153 - accuracy: 0.916 - ETA: 43s - loss: 0.3152 - accuracy: 0.916 - ETA: 43s - loss: 0.3140 - accuracy: 0.916 - ETA: 42s - loss: 0.3143 - accuracy: 0.916 - ETA: 41s - loss: 0.3146 - accuracy: 0.916 - ETA: 40s - loss: 0.3159 - accuracy: 0.915 - ETA: 39s - loss: 0.3159 - accuracy: 0.915 - ETA: 38s - loss: 0.3178 - accuracy: 0.914 - ETA: 37s - loss: 0.3205 - accuracy: 0.914 - ETA: 36s - loss: 0.3204 - accuracy: 0.913 - ETA: 35s - loss: 0.3191 - accuracy: 0.914 - ETA: 34s - loss: 0.3191 - accuracy: 0.913 - ETA: 33s - loss: 0.3186 - accuracy: 0.913 - ETA: 33s - loss: 0.3167 - accuracy: 0.914 - ETA: 32s - loss: 0.3165 - accuracy: 0.914 - ETA: 31s - loss: 0.3159 - accuracy: 0.913 - ETA: 30s - loss: 0.3149 - accuracy: 0.914 - ETA: 29s - loss: 0.3143 - accuracy: 0.914 - ETA: 28s - loss: 0.3140 - accuracy: 0.914 - ETA: 27s - loss: 0.3127 - accuracy: 0.914 - ETA: 26s - loss: 0.3127 - accuracy: 0.914 - ETA: 25s - loss: 0.3137 - accuracy: 0.914 - ETA: 25s - loss: 0.3128 - accuracy: 0.914 - ETA: 24s - loss: 0.3125 - accuracy: 0.914 - ETA: 23s - loss: 0.3133 - accuracy: 0.914 - ETA: 22s - loss: 0.3132 - accuracy: 0.914 - ETA: 21s - loss: 0.3123 - accuracy: 0.914 - ETA: 20s - loss: 0.3108 - accuracy: 0.915 - ETA: 19s - loss: 0.3117 - accuracy: 0.914 - ETA: 18s - loss: 0.3114 - accuracy: 0.914 - ETA: 17s - loss: 0.3124 - accuracy: 0.914 - ETA: 16s - loss: 0.3119 - accuracy: 0.914 - ETA: 15s - loss: 0.3126 - accuracy: 0.914 - ETA: 15s - loss: 0.3125 - accuracy: 0.914 - ETA: 14s - loss: 0.3119 - accuracy: 0.914 - ETA: 13s - loss: 0.3114 - accuracy: 0.914 - ETA: 12s - loss: 0.3130 - accuracy: 0.914 - ETA: 11s - loss: 0.3124 - accuracy: 0.914 - ETA: 10s - loss: 0.3117 - accuracy: 0.914 - ETA: 9s - loss: 0.3112 - accuracy: 0.914 - ETA: 8s - loss: 0.3093 - accuracy: 0.91 - ETA: 7s - loss: 0.3086 - accuracy: 0.91 - ETA: 6s - loss: 0.3080 - accuracy: 0.91 - ETA: 6s - loss: 0.3090 - accuracy: 0.91 - ETA: 5s - loss: 0.3100 - accuracy: 0.91 - ETA: 4s - loss: 0.3088 - accuracy: 0.91 - ETA: 3s - loss: 0.3085 - accuracy: 0.91 - ETA: 2s - loss: 0.3087 - accuracy: 0.91 - ETA: 1s - loss: 0.3094 - accuracy: 0.91 - ETA: 0s - loss: 0.3082 - accuracy: 0.91 - 105s 8ms/step - loss: 0.3072 - accuracy: 0.9162 - val_loss: 3.1027 - val_accuracy: 0.3757\n",
      "Epoch 94/100\n",
      "13022/13022 [==============================] - ETA: 1:27 - loss: 0.3326 - accuracy: 0.92 - ETA: 1:27 - loss: 0.3626 - accuracy: 0.90 - ETA: 1:27 - loss: 0.3577 - accuracy: 0.90 - ETA: 1:28 - loss: 0.3502 - accuracy: 0.91 - ETA: 1:28 - loss: 0.3243 - accuracy: 0.91 - ETA: 1:28 - loss: 0.3363 - accuracy: 0.91 - ETA: 1:27 - loss: 0.3360 - accuracy: 0.91 - ETA: 1:26 - loss: 0.3196 - accuracy: 0.91 - ETA: 1:25 - loss: 0.3065 - accuracy: 0.92 - ETA: 1:24 - loss: 0.2972 - accuracy: 0.92 - ETA: 1:24 - loss: 0.2989 - accuracy: 0.92 - ETA: 1:22 - loss: 0.3164 - accuracy: 0.91 - ETA: 1:21 - loss: 0.3322 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3278 - accuracy: 0.91 - ETA: 1:19 - loss: 0.3253 - accuracy: 0.91 - ETA: 1:18 - loss: 0.3196 - accuracy: 0.91 - ETA: 1:17 - loss: 0.3241 - accuracy: 0.91 - ETA: 1:16 - loss: 0.3167 - accuracy: 0.91 - ETA: 1:15 - loss: 0.3149 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3148 - accuracy: 0.91 - ETA: 1:13 - loss: 0.3169 - accuracy: 0.91 - ETA: 1:12 - loss: 0.3117 - accuracy: 0.91 - ETA: 1:11 - loss: 0.3114 - accuracy: 0.91 - ETA: 1:10 - loss: 0.3096 - accuracy: 0.91 - ETA: 1:09 - loss: 0.3163 - accuracy: 0.91 - ETA: 1:08 - loss: 0.3166 - accuracy: 0.91 - ETA: 1:08 - loss: 0.3192 - accuracy: 0.91 - ETA: 1:07 - loss: 0.3260 - accuracy: 0.90 - ETA: 1:06 - loss: 0.3232 - accuracy: 0.91 - ETA: 1:05 - loss: 0.3198 - accuracy: 0.91 - ETA: 1:04 - loss: 0.3198 - accuracy: 0.91 - ETA: 1:03 - loss: 0.3230 - accuracy: 0.91 - ETA: 1:02 - loss: 0.3217 - accuracy: 0.91 - ETA: 1:01 - loss: 0.3198 - accuracy: 0.91 - ETA: 1:00 - loss: 0.3182 - accuracy: 0.91 - ETA: 59s - loss: 0.3158 - accuracy: 0.9117 - ETA: 58s - loss: 0.3140 - accuracy: 0.912 - ETA: 57s - loss: 0.3084 - accuracy: 0.914 - ETA: 56s - loss: 0.3051 - accuracy: 0.914 - ETA: 55s - loss: 0.3044 - accuracy: 0.914 - ETA: 54s - loss: 0.3034 - accuracy: 0.914 - ETA: 53s - loss: 0.3021 - accuracy: 0.915 - ETA: 52s - loss: 0.3015 - accuracy: 0.914 - ETA: 52s - loss: 0.2994 - accuracy: 0.915 - ETA: 51s - loss: 0.2972 - accuracy: 0.915 - ETA: 50s - loss: 0.2973 - accuracy: 0.915 - ETA: 49s - loss: 0.2982 - accuracy: 0.915 - ETA: 48s - loss: 0.2990 - accuracy: 0.915 - ETA: 47s - loss: 0.2995 - accuracy: 0.915 - ETA: 46s - loss: 0.2980 - accuracy: 0.916 - ETA: 45s - loss: 0.2968 - accuracy: 0.916 - ETA: 44s - loss: 0.2957 - accuracy: 0.917 - ETA: 44s - loss: 0.2963 - accuracy: 0.917 - ETA: 43s - loss: 0.2966 - accuracy: 0.917 - ETA: 42s - loss: 0.2946 - accuracy: 0.917 - ETA: 41s - loss: 0.2943 - accuracy: 0.917 - ETA: 40s - loss: 0.2934 - accuracy: 0.917 - ETA: 39s - loss: 0.2945 - accuracy: 0.917 - ETA: 38s - loss: 0.2960 - accuracy: 0.917 - ETA: 37s - loss: 0.2929 - accuracy: 0.917 - ETA: 36s - loss: 0.2920 - accuracy: 0.917 - ETA: 36s - loss: 0.2936 - accuracy: 0.917 - ETA: 35s - loss: 0.2941 - accuracy: 0.917 - ETA: 34s - loss: 0.2974 - accuracy: 0.917 - ETA: 33s - loss: 0.2991 - accuracy: 0.916 - ETA: 32s - loss: 0.2993 - accuracy: 0.916 - ETA: 31s - loss: 0.3001 - accuracy: 0.917 - ETA: 30s - loss: 0.2990 - accuracy: 0.917 - ETA: 29s - loss: 0.2991 - accuracy: 0.917 - ETA: 28s - loss: 0.2994 - accuracy: 0.917 - ETA: 27s - loss: 0.2991 - accuracy: 0.917 - ETA: 26s - loss: 0.3000 - accuracy: 0.917 - ETA: 25s - loss: 0.2984 - accuracy: 0.917 - ETA: 25s - loss: 0.2976 - accuracy: 0.917 - ETA: 24s - loss: 0.2983 - accuracy: 0.917 - ETA: 23s - loss: 0.2994 - accuracy: 0.916 - ETA: 22s - loss: 0.2982 - accuracy: 0.916 - ETA: 21s - loss: 0.2984 - accuracy: 0.916 - ETA: 20s - loss: 0.2982 - accuracy: 0.916 - ETA: 19s - loss: 0.2992 - accuracy: 0.916 - ETA: 18s - loss: 0.3006 - accuracy: 0.915 - ETA: 17s - loss: 0.2989 - accuracy: 0.915 - ETA: 16s - loss: 0.2982 - accuracy: 0.916 - ETA: 16s - loss: 0.2990 - accuracy: 0.916 - ETA: 15s - loss: 0.2993 - accuracy: 0.915 - ETA: 14s - loss: 0.2984 - accuracy: 0.916 - ETA: 13s - loss: 0.2983 - accuracy: 0.915 - ETA: 12s - loss: 0.2977 - accuracy: 0.916 - ETA: 11s - loss: 0.2989 - accuracy: 0.916 - ETA: 10s - loss: 0.2980 - accuracy: 0.916 - ETA: 9s - loss: 0.2983 - accuracy: 0.916 - ETA: 8s - loss: 0.2983 - accuracy: 0.91 - ETA: 7s - loss: 0.2972 - accuracy: 0.91 - ETA: 6s - loss: 0.2974 - accuracy: 0.91 - ETA: 6s - loss: 0.2984 - accuracy: 0.91 - ETA: 5s - loss: 0.2973 - accuracy: 0.91 - ETA: 4s - loss: 0.2973 - accuracy: 0.91 - ETA: 3s - loss: 0.2980 - accuracy: 0.91 - ETA: 2s - loss: 0.2982 - accuracy: 0.91 - ETA: 1s - loss: 0.2991 - accuracy: 0.91 - ETA: 0s - loss: 0.2996 - accuracy: 0.91 - 105s 8ms/step - loss: 0.2990 - accuracy: 0.9162 - val_loss: 3.1799 - val_accuracy: 0.3469\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.2209 - accuracy: 0.92 - ETA: 1:30 - loss: 0.2270 - accuracy: 0.92 - ETA: 1:29 - loss: 0.2391 - accuracy: 0.92 - ETA: 1:27 - loss: 0.2523 - accuracy: 0.91 - ETA: 1:26 - loss: 0.2520 - accuracy: 0.91 - ETA: 1:26 - loss: 0.2597 - accuracy: 0.91 - ETA: 1:24 - loss: 0.2475 - accuracy: 0.92 - ETA: 1:23 - loss: 0.2575 - accuracy: 0.91 - ETA: 1:22 - loss: 0.2904 - accuracy: 0.91 - ETA: 1:21 - loss: 0.2998 - accuracy: 0.90 - ETA: 1:21 - loss: 0.2900 - accuracy: 0.91 - ETA: 1:20 - loss: 0.2923 - accuracy: 0.91 - ETA: 1:20 - loss: 0.2789 - accuracy: 0.91 - ETA: 1:19 - loss: 0.2787 - accuracy: 0.91 - ETA: 1:19 - loss: 0.2829 - accuracy: 0.91 - ETA: 1:18 - loss: 0.2732 - accuracy: 0.91 - ETA: 1:18 - loss: 0.2794 - accuracy: 0.91 - ETA: 1:16 - loss: 0.2872 - accuracy: 0.91 - ETA: 1:16 - loss: 0.2950 - accuracy: 0.91 - ETA: 1:15 - loss: 0.3060 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3009 - accuracy: 0.91 - ETA: 1:13 - loss: 0.2997 - accuracy: 0.91 - ETA: 1:12 - loss: 0.2975 - accuracy: 0.91 - ETA: 1:11 - loss: 0.2963 - accuracy: 0.91 - ETA: 1:10 - loss: 0.2937 - accuracy: 0.91 - ETA: 1:09 - loss: 0.2925 - accuracy: 0.91 - ETA: 1:08 - loss: 0.2925 - accuracy: 0.91 - ETA: 1:07 - loss: 0.2934 - accuracy: 0.91 - ETA: 1:06 - loss: 0.2933 - accuracy: 0.91 - ETA: 1:05 - loss: 0.2963 - accuracy: 0.91 - ETA: 1:04 - loss: 0.2964 - accuracy: 0.91 - ETA: 1:03 - loss: 0.2921 - accuracy: 0.91 - ETA: 1:02 - loss: 0.2907 - accuracy: 0.91 - ETA: 1:01 - loss: 0.2919 - accuracy: 0.91 - ETA: 1:00 - loss: 0.2888 - accuracy: 0.91 - ETA: 59s - loss: 0.2891 - accuracy: 0.9178 - ETA: 58s - loss: 0.2882 - accuracy: 0.917 - ETA: 57s - loss: 0.2866 - accuracy: 0.918 - ETA: 56s - loss: 0.2856 - accuracy: 0.918 - ETA: 55s - loss: 0.2829 - accuracy: 0.918 - ETA: 54s - loss: 0.2842 - accuracy: 0.919 - ETA: 54s - loss: 0.2849 - accuracy: 0.919 - ETA: 53s - loss: 0.2836 - accuracy: 0.919 - ETA: 52s - loss: 0.2853 - accuracy: 0.918 - ETA: 51s - loss: 0.2827 - accuracy: 0.918 - ETA: 50s - loss: 0.2835 - accuracy: 0.918 - ETA: 49s - loss: 0.2815 - accuracy: 0.919 - ETA: 48s - loss: 0.2826 - accuracy: 0.918 - ETA: 47s - loss: 0.2815 - accuracy: 0.919 - ETA: 46s - loss: 0.2810 - accuracy: 0.918 - ETA: 45s - loss: 0.2821 - accuracy: 0.918 - ETA: 44s - loss: 0.2818 - accuracy: 0.919 - ETA: 43s - loss: 0.2815 - accuracy: 0.919 - ETA: 43s - loss: 0.2825 - accuracy: 0.919 - ETA: 42s - loss: 0.2834 - accuracy: 0.918 - ETA: 41s - loss: 0.2842 - accuracy: 0.918 - ETA: 40s - loss: 0.2863 - accuracy: 0.917 - ETA: 39s - loss: 0.2853 - accuracy: 0.918 - ETA: 38s - loss: 0.2841 - accuracy: 0.918 - ETA: 37s - loss: 0.2845 - accuracy: 0.918 - ETA: 36s - loss: 0.2858 - accuracy: 0.917 - ETA: 35s - loss: 0.2849 - accuracy: 0.918 - ETA: 34s - loss: 0.2835 - accuracy: 0.918 - ETA: 34s - loss: 0.2865 - accuracy: 0.917 - ETA: 33s - loss: 0.2883 - accuracy: 0.917 - ETA: 32s - loss: 0.2890 - accuracy: 0.917 - ETA: 31s - loss: 0.2894 - accuracy: 0.917 - ETA: 30s - loss: 0.2912 - accuracy: 0.917 - ETA: 29s - loss: 0.2892 - accuracy: 0.917 - ETA: 28s - loss: 0.2894 - accuracy: 0.917 - ETA: 27s - loss: 0.2907 - accuracy: 0.917 - ETA: 26s - loss: 0.2892 - accuracy: 0.918 - ETA: 25s - loss: 0.2901 - accuracy: 0.917 - ETA: 25s - loss: 0.2891 - accuracy: 0.918 - ETA: 24s - loss: 0.2895 - accuracy: 0.917 - ETA: 23s - loss: 0.2908 - accuracy: 0.917 - ETA: 22s - loss: 0.2905 - accuracy: 0.918 - ETA: 21s - loss: 0.2895 - accuracy: 0.918 - ETA: 20s - loss: 0.2891 - accuracy: 0.918 - ETA: 19s - loss: 0.2904 - accuracy: 0.917 - ETA: 18s - loss: 0.2905 - accuracy: 0.917 - ETA: 17s - loss: 0.2909 - accuracy: 0.917 - ETA: 16s - loss: 0.2908 - accuracy: 0.917 - ETA: 16s - loss: 0.2898 - accuracy: 0.917 - ETA: 15s - loss: 0.2903 - accuracy: 0.917 - ETA: 14s - loss: 0.2894 - accuracy: 0.918 - ETA: 13s - loss: 0.2903 - accuracy: 0.917 - ETA: 12s - loss: 0.2912 - accuracy: 0.917 - ETA: 11s - loss: 0.2916 - accuracy: 0.917 - ETA: 10s - loss: 0.2927 - accuracy: 0.917 - ETA: 9s - loss: 0.2919 - accuracy: 0.917 - ETA: 8s - loss: 0.2930 - accuracy: 0.91 - ETA: 7s - loss: 0.2946 - accuracy: 0.91 - ETA: 7s - loss: 0.2953 - accuracy: 0.91 - ETA: 6s - loss: 0.2936 - accuracy: 0.91 - ETA: 5s - loss: 0.2943 - accuracy: 0.91 - ETA: 4s - loss: 0.2954 - accuracy: 0.91 - ETA: 3s - loss: 0.2950 - accuracy: 0.91 - ETA: 2s - loss: 0.2943 - accuracy: 0.91 - ETA: 1s - loss: 0.2942 - accuracy: 0.91 - ETA: 0s - loss: 0.2932 - accuracy: 0.91 - 105s 8ms/step - loss: 0.2927 - accuracy: 0.9174 - val_loss: 3.1440 - val_accuracy: 0.3712\n",
      "Epoch 96/100\n",
      "13022/13022 [==============================] - ETA: 1:28 - loss: 0.3251 - accuracy: 0.90 - ETA: 1:26 - loss: 0.3325 - accuracy: 0.91 - ETA: 1:26 - loss: 0.3466 - accuracy: 0.91 - ETA: 1:29 - loss: 0.3065 - accuracy: 0.92 - ETA: 1:29 - loss: 0.2967 - accuracy: 0.91 - ETA: 1:28 - loss: 0.3060 - accuracy: 0.91 - ETA: 1:27 - loss: 0.3087 - accuracy: 0.91 - ETA: 1:26 - loss: 0.3066 - accuracy: 0.91 - ETA: 1:25 - loss: 0.2895 - accuracy: 0.91 - ETA: 1:24 - loss: 0.2863 - accuracy: 0.91 - ETA: 1:23 - loss: 0.2801 - accuracy: 0.91 - ETA: 1:21 - loss: 0.2909 - accuracy: 0.91 - ETA: 1:21 - loss: 0.3128 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3141 - accuracy: 0.91 - ETA: 1:19 - loss: 0.3090 - accuracy: 0.91 - ETA: 1:17 - loss: 0.3083 - accuracy: 0.91 - ETA: 1:17 - loss: 0.3034 - accuracy: 0.91 - ETA: 1:16 - loss: 0.3060 - accuracy: 0.91 - ETA: 1:15 - loss: 0.3049 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3019 - accuracy: 0.91 - ETA: 1:13 - loss: 0.3050 - accuracy: 0.91 - ETA: 1:12 - loss: 0.3032 - accuracy: 0.91 - ETA: 1:11 - loss: 0.2998 - accuracy: 0.91 - ETA: 1:10 - loss: 0.2997 - accuracy: 0.91 - ETA: 1:09 - loss: 0.2965 - accuracy: 0.91 - ETA: 1:08 - loss: 0.2944 - accuracy: 0.91 - ETA: 1:07 - loss: 0.2946 - accuracy: 0.91 - ETA: 1:06 - loss: 0.2971 - accuracy: 0.91 - ETA: 1:04 - loss: 0.2995 - accuracy: 0.91 - ETA: 1:03 - loss: 0.2974 - accuracy: 0.91 - ETA: 1:01 - loss: 0.2940 - accuracy: 0.91 - ETA: 1:00 - loss: 0.2967 - accuracy: 0.91 - ETA: 59s - loss: 0.2949 - accuracy: 0.9122 - ETA: 57s - loss: 0.2960 - accuracy: 0.911 - ETA: 56s - loss: 0.2934 - accuracy: 0.912 - ETA: 55s - loss: 0.2996 - accuracy: 0.911 - ETA: 54s - loss: 0.3013 - accuracy: 0.911 - ETA: 53s - loss: 0.2999 - accuracy: 0.912 - ETA: 53s - loss: 0.2992 - accuracy: 0.911 - ETA: 52s - loss: 0.2985 - accuracy: 0.911 - ETA: 51s - loss: 0.2962 - accuracy: 0.911 - ETA: 50s - loss: 0.2980 - accuracy: 0.911 - ETA: 49s - loss: 0.2968 - accuracy: 0.911 - ETA: 49s - loss: 0.2997 - accuracy: 0.911 - ETA: 48s - loss: 0.2985 - accuracy: 0.911 - ETA: 47s - loss: 0.2997 - accuracy: 0.911 - ETA: 47s - loss: 0.3012 - accuracy: 0.911 - ETA: 46s - loss: 0.3004 - accuracy: 0.911 - ETA: 45s - loss: 0.3006 - accuracy: 0.912 - ETA: 44s - loss: 0.2992 - accuracy: 0.913 - ETA: 43s - loss: 0.2994 - accuracy: 0.912 - ETA: 43s - loss: 0.2996 - accuracy: 0.912 - ETA: 42s - loss: 0.2986 - accuracy: 0.912 - ETA: 41s - loss: 0.2967 - accuracy: 0.913 - ETA: 40s - loss: 0.2964 - accuracy: 0.913 - ETA: 39s - loss: 0.2965 - accuracy: 0.913 - ETA: 38s - loss: 0.2985 - accuracy: 0.912 - ETA: 38s - loss: 0.2976 - accuracy: 0.913 - ETA: 37s - loss: 0.2953 - accuracy: 0.913 - ETA: 36s - loss: 0.2938 - accuracy: 0.913 - ETA: 35s - loss: 0.2937 - accuracy: 0.914 - ETA: 34s - loss: 0.2946 - accuracy: 0.913 - ETA: 33s - loss: 0.2934 - accuracy: 0.914 - ETA: 32s - loss: 0.2915 - accuracy: 0.915 - ETA: 32s - loss: 0.2928 - accuracy: 0.915 - ETA: 31s - loss: 0.2932 - accuracy: 0.915 - ETA: 30s - loss: 0.2920 - accuracy: 0.916 - ETA: 29s - loss: 0.2913 - accuracy: 0.916 - ETA: 28s - loss: 0.2934 - accuracy: 0.916 - ETA: 27s - loss: 0.2935 - accuracy: 0.915 - ETA: 26s - loss: 0.2949 - accuracy: 0.915 - ETA: 26s - loss: 0.2951 - accuracy: 0.915 - ETA: 25s - loss: 0.2934 - accuracy: 0.916 - ETA: 24s - loss: 0.2925 - accuracy: 0.916 - ETA: 23s - loss: 0.2921 - accuracy: 0.916 - ETA: 22s - loss: 0.2910 - accuracy: 0.916 - ETA: 21s - loss: 0.2919 - accuracy: 0.916 - ETA: 20s - loss: 0.2896 - accuracy: 0.916 - ETA: 20s - loss: 0.2891 - accuracy: 0.916 - ETA: 19s - loss: 0.2896 - accuracy: 0.916 - ETA: 18s - loss: 0.2921 - accuracy: 0.916 - ETA: 17s - loss: 0.2917 - accuracy: 0.916 - ETA: 16s - loss: 0.2928 - accuracy: 0.916 - ETA: 15s - loss: 0.2930 - accuracy: 0.916 - ETA: 14s - loss: 0.2922 - accuracy: 0.916 - ETA: 13s - loss: 0.2932 - accuracy: 0.916 - ETA: 12s - loss: 0.2932 - accuracy: 0.916 - ETA: 12s - loss: 0.2927 - accuracy: 0.916 - ETA: 11s - loss: 0.2922 - accuracy: 0.917 - ETA: 10s - loss: 0.2921 - accuracy: 0.916 - ETA: 9s - loss: 0.2921 - accuracy: 0.917 - ETA: 8s - loss: 0.2916 - accuracy: 0.91 - ETA: 7s - loss: 0.2910 - accuracy: 0.91 - ETA: 6s - loss: 0.2928 - accuracy: 0.91 - ETA: 5s - loss: 0.2921 - accuracy: 0.91 - ETA: 5s - loss: 0.2917 - accuracy: 0.91 - ETA: 4s - loss: 0.2909 - accuracy: 0.91 - ETA: 3s - loss: 0.2926 - accuracy: 0.91 - ETA: 2s - loss: 0.2947 - accuracy: 0.91 - ETA: 1s - loss: 0.2946 - accuracy: 0.91 - ETA: 0s - loss: 0.2942 - accuracy: 0.91 - 104s 8ms/step - loss: 0.2934 - accuracy: 0.9171 - val_loss: 3.3041 - val_accuracy: 0.3623\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:42 - loss: 0.3208 - accuracy: 0.90 - ETA: 1:38 - loss: 0.2815 - accuracy: 0.91 - ETA: 1:37 - loss: 0.3579 - accuracy: 0.90 - ETA: 1:34 - loss: 0.3241 - accuracy: 0.91 - ETA: 1:31 - loss: 0.2998 - accuracy: 0.92 - ETA: 1:29 - loss: 0.2897 - accuracy: 0.92 - ETA: 1:28 - loss: 0.2861 - accuracy: 0.92 - ETA: 1:26 - loss: 0.2747 - accuracy: 0.92 - ETA: 1:25 - loss: 0.2909 - accuracy: 0.92 - ETA: 1:24 - loss: 0.2927 - accuracy: 0.92 - ETA: 1:23 - loss: 0.3043 - accuracy: 0.92 - ETA: 1:22 - loss: 0.2998 - accuracy: 0.92 - ETA: 1:21 - loss: 0.3162 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3106 - accuracy: 0.91 - ETA: 1:20 - loss: 0.3243 - accuracy: 0.91 - ETA: 1:18 - loss: 0.3337 - accuracy: 0.91 - ETA: 1:17 - loss: 0.3277 - accuracy: 0.91 - ETA: 1:17 - loss: 0.3284 - accuracy: 0.91 - ETA: 1:16 - loss: 0.3310 - accuracy: 0.91 - ETA: 1:15 - loss: 0.3283 - accuracy: 0.91 - ETA: 1:13 - loss: 0.3335 - accuracy: 0.91 - ETA: 1:13 - loss: 0.3288 - accuracy: 0.91 - ETA: 1:11 - loss: 0.3273 - accuracy: 0.91 - ETA: 1:11 - loss: 0.3251 - accuracy: 0.91 - ETA: 1:09 - loss: 0.3271 - accuracy: 0.91 - ETA: 1:09 - loss: 0.3240 - accuracy: 0.91 - ETA: 1:07 - loss: 0.3247 - accuracy: 0.91 - ETA: 1:07 - loss: 0.3266 - accuracy: 0.91 - ETA: 1:06 - loss: 0.3258 - accuracy: 0.91 - ETA: 1:05 - loss: 0.3200 - accuracy: 0.91 - ETA: 1:04 - loss: 0.3156 - accuracy: 0.91 - ETA: 1:03 - loss: 0.3144 - accuracy: 0.91 - ETA: 1:02 - loss: 0.3129 - accuracy: 0.91 - ETA: 1:01 - loss: 0.3117 - accuracy: 0.91 - ETA: 1:00 - loss: 0.3108 - accuracy: 0.91 - ETA: 1:00 - loss: 0.3163 - accuracy: 0.91 - ETA: 59s - loss: 0.3178 - accuracy: 0.9141 - ETA: 58s - loss: 0.3179 - accuracy: 0.913 - ETA: 57s - loss: 0.3183 - accuracy: 0.912 - ETA: 56s - loss: 0.3149 - accuracy: 0.913 - ETA: 55s - loss: 0.3209 - accuracy: 0.912 - ETA: 54s - loss: 0.3223 - accuracy: 0.912 - ETA: 53s - loss: 0.3245 - accuracy: 0.912 - ETA: 52s - loss: 0.3217 - accuracy: 0.913 - ETA: 51s - loss: 0.3238 - accuracy: 0.912 - ETA: 50s - loss: 0.3250 - accuracy: 0.913 - ETA: 49s - loss: 0.3238 - accuracy: 0.912 - ETA: 49s - loss: 0.3226 - accuracy: 0.913 - ETA: 48s - loss: 0.3218 - accuracy: 0.913 - ETA: 47s - loss: 0.3228 - accuracy: 0.913 - ETA: 46s - loss: 0.3240 - accuracy: 0.913 - ETA: 45s - loss: 0.3250 - accuracy: 0.912 - ETA: 44s - loss: 0.3233 - accuracy: 0.912 - ETA: 43s - loss: 0.3214 - accuracy: 0.913 - ETA: 42s - loss: 0.3199 - accuracy: 0.913 - ETA: 41s - loss: 0.3173 - accuracy: 0.913 - ETA: 40s - loss: 0.3170 - accuracy: 0.913 - ETA: 39s - loss: 0.3165 - accuracy: 0.914 - ETA: 39s - loss: 0.3151 - accuracy: 0.914 - ETA: 38s - loss: 0.3155 - accuracy: 0.914 - ETA: 37s - loss: 0.3157 - accuracy: 0.913 - ETA: 36s - loss: 0.3156 - accuracy: 0.913 - ETA: 35s - loss: 0.3165 - accuracy: 0.913 - ETA: 34s - loss: 0.3172 - accuracy: 0.913 - ETA: 33s - loss: 0.3173 - accuracy: 0.913 - ETA: 32s - loss: 0.3185 - accuracy: 0.912 - ETA: 31s - loss: 0.3187 - accuracy: 0.912 - ETA: 30s - loss: 0.3204 - accuracy: 0.911 - ETA: 29s - loss: 0.3191 - accuracy: 0.912 - ETA: 28s - loss: 0.3176 - accuracy: 0.912 - ETA: 28s - loss: 0.3178 - accuracy: 0.912 - ETA: 27s - loss: 0.3188 - accuracy: 0.912 - ETA: 26s - loss: 0.3192 - accuracy: 0.911 - ETA: 25s - loss: 0.3193 - accuracy: 0.911 - ETA: 24s - loss: 0.3208 - accuracy: 0.911 - ETA: 23s - loss: 0.3233 - accuracy: 0.910 - ETA: 22s - loss: 0.3242 - accuracy: 0.910 - ETA: 21s - loss: 0.3233 - accuracy: 0.910 - ETA: 20s - loss: 0.3214 - accuracy: 0.911 - ETA: 19s - loss: 0.3211 - accuracy: 0.911 - ETA: 18s - loss: 0.3204 - accuracy: 0.911 - ETA: 18s - loss: 0.3249 - accuracy: 0.910 - ETA: 17s - loss: 0.3255 - accuracy: 0.910 - ETA: 16s - loss: 0.3247 - accuracy: 0.911 - ETA: 15s - loss: 0.3225 - accuracy: 0.911 - ETA: 14s - loss: 0.3236 - accuracy: 0.912 - ETA: 13s - loss: 0.3240 - accuracy: 0.911 - ETA: 12s - loss: 0.3241 - accuracy: 0.911 - ETA: 11s - loss: 0.3238 - accuracy: 0.911 - ETA: 10s - loss: 0.3230 - accuracy: 0.912 - ETA: 9s - loss: 0.3227 - accuracy: 0.912 - ETA: 8s - loss: 0.3223 - accuracy: 0.91 - ETA: 7s - loss: 0.3227 - accuracy: 0.91 - ETA: 7s - loss: 0.3214 - accuracy: 0.91 - ETA: 6s - loss: 0.3201 - accuracy: 0.91 - ETA: 5s - loss: 0.3211 - accuracy: 0.91 - ETA: 4s - loss: 0.3214 - accuracy: 0.91 - ETA: 3s - loss: 0.3208 - accuracy: 0.91 - ETA: 2s - loss: 0.3220 - accuracy: 0.91 - ETA: 1s - loss: 0.3213 - accuracy: 0.91 - ETA: 0s - loss: 0.3205 - accuracy: 0.91 - 106s 8ms/step - loss: 0.3207 - accuracy: 0.9126 - val_loss: 3.2728 - val_accuracy: 0.3389\n",
      "Epoch 98/100\n",
      "13022/13022 [==============================] - ETA: 1:31 - loss: 0.4249 - accuracy: 0.87 - ETA: 1:33 - loss: 0.3242 - accuracy: 0.90 - ETA: 1:31 - loss: 0.3037 - accuracy: 0.92 - ETA: 1:31 - loss: 0.2664 - accuracy: 0.93 - ETA: 1:29 - loss: 0.2693 - accuracy: 0.93 - ETA: 1:28 - loss: 0.3117 - accuracy: 0.92 - ETA: 1:27 - loss: 0.3106 - accuracy: 0.92 - ETA: 1:26 - loss: 0.2944 - accuracy: 0.92 - ETA: 1:24 - loss: 0.2928 - accuracy: 0.92 - ETA: 1:23 - loss: 0.3085 - accuracy: 0.92 - ETA: 1:22 - loss: 0.3089 - accuracy: 0.92 - ETA: 1:21 - loss: 0.3077 - accuracy: 0.92 - ETA: 1:20 - loss: 0.3103 - accuracy: 0.92 - ETA: 1:19 - loss: 0.3126 - accuracy: 0.92 - ETA: 1:18 - loss: 0.3102 - accuracy: 0.92 - ETA: 1:17 - loss: 0.3211 - accuracy: 0.91 - ETA: 1:16 - loss: 0.3187 - accuracy: 0.91 - ETA: 1:15 - loss: 0.3191 - accuracy: 0.91 - ETA: 1:14 - loss: 0.3237 - accuracy: 0.91 - ETA: 1:13 - loss: 0.3138 - accuracy: 0.92 - ETA: 1:12 - loss: 0.3108 - accuracy: 0.92 - ETA: 1:12 - loss: 0.3066 - accuracy: 0.92 - ETA: 1:11 - loss: 0.3029 - accuracy: 0.92 - ETA: 1:10 - loss: 0.3005 - accuracy: 0.92 - ETA: 1:09 - loss: 0.2984 - accuracy: 0.92 - ETA: 1:08 - loss: 0.3045 - accuracy: 0.92 - ETA: 1:07 - loss: 0.3034 - accuracy: 0.92 - ETA: 1:06 - loss: 0.3051 - accuracy: 0.92 - ETA: 1:05 - loss: 0.3032 - accuracy: 0.92 - ETA: 1:05 - loss: 0.3023 - accuracy: 0.92 - ETA: 1:04 - loss: 0.3026 - accuracy: 0.92 - ETA: 1:03 - loss: 0.2980 - accuracy: 0.92 - ETA: 1:02 - loss: 0.2974 - accuracy: 0.92 - ETA: 1:01 - loss: 0.2981 - accuracy: 0.92 - ETA: 1:00 - loss: 0.2934 - accuracy: 0.92 - ETA: 59s - loss: 0.2917 - accuracy: 0.9230 - ETA: 58s - loss: 0.2923 - accuracy: 0.922 - ETA: 58s - loss: 0.2985 - accuracy: 0.921 - ETA: 57s - loss: 0.2985 - accuracy: 0.921 - ETA: 56s - loss: 0.2980 - accuracy: 0.920 - ETA: 55s - loss: 0.2973 - accuracy: 0.920 - ETA: 54s - loss: 0.2955 - accuracy: 0.920 - ETA: 53s - loss: 0.2927 - accuracy: 0.921 - ETA: 52s - loss: 0.2922 - accuracy: 0.921 - ETA: 51s - loss: 0.2957 - accuracy: 0.920 - ETA: 50s - loss: 0.2946 - accuracy: 0.920 - ETA: 49s - loss: 0.3026 - accuracy: 0.918 - ETA: 48s - loss: 0.3026 - accuracy: 0.918 - ETA: 48s - loss: 0.3017 - accuracy: 0.918 - ETA: 47s - loss: 0.3013 - accuracy: 0.918 - ETA: 46s - loss: 0.3020 - accuracy: 0.917 - ETA: 45s - loss: 0.3012 - accuracy: 0.917 - ETA: 44s - loss: 0.3021 - accuracy: 0.917 - ETA: 43s - loss: 0.3027 - accuracy: 0.917 - ETA: 42s - loss: 0.3028 - accuracy: 0.918 - ETA: 41s - loss: 0.3042 - accuracy: 0.917 - ETA: 40s - loss: 0.3035 - accuracy: 0.918 - ETA: 39s - loss: 0.3019 - accuracy: 0.918 - ETA: 38s - loss: 0.3046 - accuracy: 0.918 - ETA: 37s - loss: 0.3071 - accuracy: 0.918 - ETA: 37s - loss: 0.3067 - accuracy: 0.918 - ETA: 36s - loss: 0.3081 - accuracy: 0.917 - ETA: 35s - loss: 0.3063 - accuracy: 0.918 - ETA: 34s - loss: 0.3054 - accuracy: 0.918 - ETA: 33s - loss: 0.3055 - accuracy: 0.918 - ETA: 32s - loss: 0.3069 - accuracy: 0.918 - ETA: 31s - loss: 0.3062 - accuracy: 0.918 - ETA: 30s - loss: 0.3064 - accuracy: 0.918 - ETA: 29s - loss: 0.3079 - accuracy: 0.917 - ETA: 28s - loss: 0.3072 - accuracy: 0.917 - ETA: 27s - loss: 0.3087 - accuracy: 0.917 - ETA: 26s - loss: 0.3088 - accuracy: 0.917 - ETA: 26s - loss: 0.3079 - accuracy: 0.917 - ETA: 25s - loss: 0.3100 - accuracy: 0.917 - ETA: 24s - loss: 0.3099 - accuracy: 0.917 - ETA: 23s - loss: 0.3093 - accuracy: 0.917 - ETA: 22s - loss: 0.3088 - accuracy: 0.917 - ETA: 21s - loss: 0.3082 - accuracy: 0.917 - ETA: 20s - loss: 0.3064 - accuracy: 0.917 - ETA: 19s - loss: 0.3049 - accuracy: 0.918 - ETA: 18s - loss: 0.3049 - accuracy: 0.918 - ETA: 17s - loss: 0.3036 - accuracy: 0.918 - ETA: 16s - loss: 0.3049 - accuracy: 0.917 - ETA: 16s - loss: 0.3085 - accuracy: 0.917 - ETA: 15s - loss: 0.3085 - accuracy: 0.917 - ETA: 14s - loss: 0.3092 - accuracy: 0.916 - ETA: 13s - loss: 0.3106 - accuracy: 0.916 - ETA: 12s - loss: 0.3113 - accuracy: 0.916 - ETA: 11s - loss: 0.3125 - accuracy: 0.916 - ETA: 10s - loss: 0.3111 - accuracy: 0.916 - ETA: 9s - loss: 0.3108 - accuracy: 0.917 - ETA: 8s - loss: 0.3100 - accuracy: 0.91 - ETA: 7s - loss: 0.3104 - accuracy: 0.91 - ETA: 7s - loss: 0.3112 - accuracy: 0.91 - ETA: 6s - loss: 0.3100 - accuracy: 0.91 - ETA: 5s - loss: 0.3098 - accuracy: 0.91 - ETA: 4s - loss: 0.3095 - accuracy: 0.91 - ETA: 3s - loss: 0.3091 - accuracy: 0.91 - ETA: 2s - loss: 0.3086 - accuracy: 0.91 - ETA: 1s - loss: 0.3098 - accuracy: 0.91 - ETA: 0s - loss: 0.3089 - accuracy: 0.91 - 106s 8ms/step - loss: 0.3088 - accuracy: 0.9166 - val_loss: 3.1599 - val_accuracy: 0.3371\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.3349 - accuracy: 0.89 - ETA: 1:32 - loss: 0.3306 - accuracy: 0.89 - ETA: 1:31 - loss: 0.2846 - accuracy: 0.91 - ETA: 1:29 - loss: 0.3052 - accuracy: 0.90 - ETA: 1:27 - loss: 0.2805 - accuracy: 0.90 - ETA: 1:27 - loss: 0.2597 - accuracy: 0.91 - ETA: 1:25 - loss: 0.2628 - accuracy: 0.91 - ETA: 1:25 - loss: 0.2734 - accuracy: 0.91 - ETA: 1:25 - loss: 0.2778 - accuracy: 0.91 - ETA: 1:24 - loss: 0.2815 - accuracy: 0.91 - ETA: 1:23 - loss: 0.2768 - accuracy: 0.91 - ETA: 1:22 - loss: 0.2791 - accuracy: 0.91 - ETA: 1:21 - loss: 0.2752 - accuracy: 0.91 - ETA: 1:19 - loss: 0.2818 - accuracy: 0.91 - ETA: 1:19 - loss: 0.2827 - accuracy: 0.91 - ETA: 1:18 - loss: 0.2832 - accuracy: 0.91 - ETA: 1:16 - loss: 0.2803 - accuracy: 0.91 - ETA: 1:15 - loss: 0.2899 - accuracy: 0.91 - ETA: 1:15 - loss: 0.2868 - accuracy: 0.91 - ETA: 1:14 - loss: 0.2898 - accuracy: 0.91 - ETA: 1:13 - loss: 0.2936 - accuracy: 0.91 - ETA: 1:12 - loss: 0.2971 - accuracy: 0.91 - ETA: 1:11 - loss: 0.2986 - accuracy: 0.91 - ETA: 1:10 - loss: 0.3005 - accuracy: 0.90 - ETA: 1:09 - loss: 0.3001 - accuracy: 0.90 - ETA: 1:08 - loss: 0.2967 - accuracy: 0.91 - ETA: 1:07 - loss: 0.2943 - accuracy: 0.91 - ETA: 1:07 - loss: 0.2945 - accuracy: 0.91 - ETA: 1:06 - loss: 0.2924 - accuracy: 0.91 - ETA: 1:05 - loss: 0.2888 - accuracy: 0.91 - ETA: 1:04 - loss: 0.2891 - accuracy: 0.91 - ETA: 1:03 - loss: 0.2961 - accuracy: 0.91 - ETA: 1:02 - loss: 0.2958 - accuracy: 0.91 - ETA: 1:01 - loss: 0.2951 - accuracy: 0.91 - ETA: 1:00 - loss: 0.2943 - accuracy: 0.91 - ETA: 59s - loss: 0.2948 - accuracy: 0.9132 - ETA: 58s - loss: 0.2931 - accuracy: 0.913 - ETA: 57s - loss: 0.2977 - accuracy: 0.912 - ETA: 56s - loss: 0.3024 - accuracy: 0.911 - ETA: 55s - loss: 0.3030 - accuracy: 0.910 - ETA: 55s - loss: 0.3026 - accuracy: 0.911 - ETA: 54s - loss: 0.3021 - accuracy: 0.912 - ETA: 53s - loss: 0.3019 - accuracy: 0.911 - ETA: 52s - loss: 0.3029 - accuracy: 0.911 - ETA: 51s - loss: 0.3047 - accuracy: 0.911 - ETA: 50s - loss: 0.3068 - accuracy: 0.911 - ETA: 49s - loss: 0.3061 - accuracy: 0.911 - ETA: 48s - loss: 0.3072 - accuracy: 0.911 - ETA: 47s - loss: 0.3068 - accuracy: 0.911 - ETA: 47s - loss: 0.3069 - accuracy: 0.911 - ETA: 46s - loss: 0.3060 - accuracy: 0.911 - ETA: 45s - loss: 0.3069 - accuracy: 0.911 - ETA: 44s - loss: 0.3101 - accuracy: 0.910 - ETA: 43s - loss: 0.3116 - accuracy: 0.910 - ETA: 42s - loss: 0.3120 - accuracy: 0.910 - ETA: 41s - loss: 0.3150 - accuracy: 0.909 - ETA: 40s - loss: 0.3175 - accuracy: 0.909 - ETA: 39s - loss: 0.3168 - accuracy: 0.909 - ETA: 39s - loss: 0.3192 - accuracy: 0.909 - ETA: 38s - loss: 0.3159 - accuracy: 0.910 - ETA: 37s - loss: 0.3152 - accuracy: 0.911 - ETA: 36s - loss: 0.3130 - accuracy: 0.911 - ETA: 35s - loss: 0.3125 - accuracy: 0.911 - ETA: 34s - loss: 0.3103 - accuracy: 0.912 - ETA: 33s - loss: 0.3081 - accuracy: 0.912 - ETA: 32s - loss: 0.3073 - accuracy: 0.912 - ETA: 31s - loss: 0.3050 - accuracy: 0.913 - ETA: 30s - loss: 0.3056 - accuracy: 0.913 - ETA: 29s - loss: 0.3051 - accuracy: 0.914 - ETA: 29s - loss: 0.3047 - accuracy: 0.914 - ETA: 28s - loss: 0.3025 - accuracy: 0.914 - ETA: 27s - loss: 0.3039 - accuracy: 0.914 - ETA: 26s - loss: 0.3039 - accuracy: 0.914 - ETA: 25s - loss: 0.3036 - accuracy: 0.914 - ETA: 24s - loss: 0.3016 - accuracy: 0.914 - ETA: 23s - loss: 0.3015 - accuracy: 0.915 - ETA: 22s - loss: 0.3010 - accuracy: 0.915 - ETA: 21s - loss: 0.2994 - accuracy: 0.915 - ETA: 20s - loss: 0.2994 - accuracy: 0.915 - ETA: 19s - loss: 0.2991 - accuracy: 0.915 - ETA: 18s - loss: 0.2966 - accuracy: 0.916 - ETA: 18s - loss: 0.2956 - accuracy: 0.916 - ETA: 17s - loss: 0.2958 - accuracy: 0.916 - ETA: 16s - loss: 0.2950 - accuracy: 0.916 - ETA: 15s - loss: 0.2941 - accuracy: 0.916 - ETA: 14s - loss: 0.2934 - accuracy: 0.916 - ETA: 13s - loss: 0.2917 - accuracy: 0.917 - ETA: 12s - loss: 0.2923 - accuracy: 0.917 - ETA: 11s - loss: 0.2927 - accuracy: 0.917 - ETA: 10s - loss: 0.2916 - accuracy: 0.917 - ETA: 9s - loss: 0.2929 - accuracy: 0.917 - ETA: 8s - loss: 0.2920 - accuracy: 0.91 - ETA: 7s - loss: 0.2915 - accuracy: 0.91 - ETA: 7s - loss: 0.2915 - accuracy: 0.91 - ETA: 6s - loss: 0.2911 - accuracy: 0.91 - ETA: 5s - loss: 0.2909 - accuracy: 0.91 - ETA: 4s - loss: 0.2895 - accuracy: 0.91 - ETA: 3s - loss: 0.2903 - accuracy: 0.91 - ETA: 2s - loss: 0.2912 - accuracy: 0.91 - ETA: 1s - loss: 0.2910 - accuracy: 0.91 - ETA: 0s - loss: 0.2918 - accuracy: 0.91 - 106s 8ms/step - loss: 0.2912 - accuracy: 0.9182 - val_loss: 3.1718 - val_accuracy: 0.3754\n",
      "Epoch 100/100\n",
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.2615 - accuracy: 0.92 - ETA: 1:28 - loss: 0.2443 - accuracy: 0.92 - ETA: 1:28 - loss: 0.2445 - accuracy: 0.93 - ETA: 1:26 - loss: 0.2604 - accuracy: 0.92 - ETA: 1:26 - loss: 0.2548 - accuracy: 0.92 - ETA: 1:25 - loss: 0.2858 - accuracy: 0.92 - ETA: 1:24 - loss: 0.2704 - accuracy: 0.93 - ETA: 1:23 - loss: 0.2927 - accuracy: 0.92 - ETA: 1:22 - loss: 0.3047 - accuracy: 0.92 - ETA: 1:21 - loss: 0.3103 - accuracy: 0.92 - ETA: 1:20 - loss: 0.3016 - accuracy: 0.92 - ETA: 1:19 - loss: 0.3071 - accuracy: 0.92 - ETA: 1:19 - loss: 0.3066 - accuracy: 0.92 - ETA: 1:19 - loss: 0.3013 - accuracy: 0.92 - ETA: 1:18 - loss: 0.2901 - accuracy: 0.92 - ETA: 1:18 - loss: 0.2922 - accuracy: 0.92 - ETA: 1:17 - loss: 0.2890 - accuracy: 0.92 - ETA: 1:16 - loss: 0.2874 - accuracy: 0.92 - ETA: 1:15 - loss: 0.2953 - accuracy: 0.92 - ETA: 1:15 - loss: 0.2933 - accuracy: 0.92 - ETA: 1:14 - loss: 0.2901 - accuracy: 0.92 - ETA: 1:13 - loss: 0.2887 - accuracy: 0.92 - ETA: 1:13 - loss: 0.2865 - accuracy: 0.92 - ETA: 1:12 - loss: 0.2884 - accuracy: 0.92 - ETA: 1:11 - loss: 0.2897 - accuracy: 0.92 - ETA: 1:10 - loss: 0.2907 - accuracy: 0.92 - ETA: 1:10 - loss: 0.2857 - accuracy: 0.92 - ETA: 1:09 - loss: 0.2818 - accuracy: 0.92 - ETA: 1:08 - loss: 0.2891 - accuracy: 0.92 - ETA: 1:07 - loss: 0.2836 - accuracy: 0.92 - ETA: 1:06 - loss: 0.2841 - accuracy: 0.92 - ETA: 1:05 - loss: 0.2873 - accuracy: 0.92 - ETA: 1:04 - loss: 0.2920 - accuracy: 0.92 - ETA: 1:03 - loss: 0.2935 - accuracy: 0.92 - ETA: 1:02 - loss: 0.2928 - accuracy: 0.92 - ETA: 1:01 - loss: 0.2904 - accuracy: 0.92 - ETA: 1:00 - loss: 0.2926 - accuracy: 0.92 - ETA: 59s - loss: 0.2956 - accuracy: 0.9202 - ETA: 58s - loss: 0.2951 - accuracy: 0.920 - ETA: 57s - loss: 0.2940 - accuracy: 0.919 - ETA: 56s - loss: 0.2938 - accuracy: 0.920 - ETA: 55s - loss: 0.2950 - accuracy: 0.919 - ETA: 54s - loss: 0.2928 - accuracy: 0.920 - ETA: 53s - loss: 0.2936 - accuracy: 0.920 - ETA: 51s - loss: 0.2937 - accuracy: 0.920 - ETA: 50s - loss: 0.2939 - accuracy: 0.920 - ETA: 49s - loss: 0.2982 - accuracy: 0.919 - ETA: 48s - loss: 0.2996 - accuracy: 0.918 - ETA: 48s - loss: 0.2996 - accuracy: 0.918 - ETA: 47s - loss: 0.2985 - accuracy: 0.919 - ETA: 46s - loss: 0.2980 - accuracy: 0.918 - ETA: 45s - loss: 0.2964 - accuracy: 0.919 - ETA: 44s - loss: 0.3000 - accuracy: 0.918 - ETA: 43s - loss: 0.3003 - accuracy: 0.919 - ETA: 42s - loss: 0.2985 - accuracy: 0.920 - ETA: 41s - loss: 0.2980 - accuracy: 0.920 - ETA: 40s - loss: 0.2979 - accuracy: 0.920 - ETA: 39s - loss: 0.3006 - accuracy: 0.919 - ETA: 38s - loss: 0.2990 - accuracy: 0.920 - ETA: 37s - loss: 0.3019 - accuracy: 0.919 - ETA: 36s - loss: 0.2996 - accuracy: 0.920 - ETA: 36s - loss: 0.3030 - accuracy: 0.919 - ETA: 35s - loss: 0.3033 - accuracy: 0.919 - ETA: 34s - loss: 0.3031 - accuracy: 0.919 - ETA: 33s - loss: 0.3048 - accuracy: 0.919 - ETA: 32s - loss: 0.3055 - accuracy: 0.918 - ETA: 31s - loss: 0.3038 - accuracy: 0.919 - ETA: 30s - loss: 0.3025 - accuracy: 0.919 - ETA: 29s - loss: 0.3012 - accuracy: 0.919 - ETA: 28s - loss: 0.3015 - accuracy: 0.919 - ETA: 27s - loss: 0.3012 - accuracy: 0.919 - ETA: 26s - loss: 0.2993 - accuracy: 0.920 - ETA: 26s - loss: 0.2988 - accuracy: 0.920 - ETA: 25s - loss: 0.2995 - accuracy: 0.920 - ETA: 24s - loss: 0.3001 - accuracy: 0.920 - ETA: 23s - loss: 0.3009 - accuracy: 0.919 - ETA: 22s - loss: 0.2986 - accuracy: 0.920 - ETA: 21s - loss: 0.2984 - accuracy: 0.920 - ETA: 20s - loss: 0.2987 - accuracy: 0.920 - ETA: 19s - loss: 0.2997 - accuracy: 0.920 - ETA: 18s - loss: 0.3006 - accuracy: 0.920 - ETA: 17s - loss: 0.3013 - accuracy: 0.920 - ETA: 16s - loss: 0.3005 - accuracy: 0.920 - ETA: 16s - loss: 0.3009 - accuracy: 0.919 - ETA: 15s - loss: 0.2993 - accuracy: 0.920 - ETA: 14s - loss: 0.2984 - accuracy: 0.920 - ETA: 13s - loss: 0.2976 - accuracy: 0.921 - ETA: 12s - loss: 0.2976 - accuracy: 0.921 - ETA: 11s - loss: 0.2960 - accuracy: 0.921 - ETA: 10s - loss: 0.2954 - accuracy: 0.921 - ETA: 9s - loss: 0.2965 - accuracy: 0.920 - ETA: 8s - loss: 0.2970 - accuracy: 0.92 - ETA: 7s - loss: 0.2959 - accuracy: 0.92 - ETA: 7s - loss: 0.2959 - accuracy: 0.92 - ETA: 6s - loss: 0.2979 - accuracy: 0.92 - ETA: 5s - loss: 0.2966 - accuracy: 0.92 - ETA: 4s - loss: 0.2970 - accuracy: 0.92 - ETA: 3s - loss: 0.2969 - accuracy: 0.92 - ETA: 2s - loss: 0.2963 - accuracy: 0.92 - ETA: 1s - loss: 0.2957 - accuracy: 0.92 - ETA: 0s - loss: 0.2958 - accuracy: 0.92 - 106s 8ms/step - loss: 0.2960 - accuracy: 0.9205 - val_loss: 3.2126 - val_accuracy: 0.3465\n",
      "2020-03-22 01:36:47.619897\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "t5=datetime.datetime.now()\n",
    "print(t5)\n",
    "history=model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[mcp_save], batch_size=128)\n",
    "t6=datetime.datetime.now()\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Pickle/DenseNet121_history.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model as a pickle in a file \n",
    "joblib.dump(history, '../Pickle/DenseNet121_history.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file \n",
    "history = joblib.load('../Pickle/DenseNet121_history.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [3.803984838684104, 3.527491473874632, 3.254278598755279, 2.944564595242381, 2.697574514986967, 2.574310893435988, 2.557861442184206, 2.521513760901337, 2.4549115276700206, 2.460423416246583, 2.4689205948443957, 2.500191342356764, 2.5314417407957994, 2.5538535526972064, 2.598166832605363, 2.4052267731977994, 2.662408266748701, 2.509187522698799, 2.559379996156753, 2.609023425643555, 2.4604195051150826, 2.487497533205136, 2.625287466785786, 2.4451294236573533, 2.7053346498450948, 2.691941991348322, 2.648216434357322, 2.625741447894321, 2.7139599389920845, 2.5554936616384825, 2.772526381106229, 2.577108154540963, 2.7862015393062043, 2.6660752969637262, 2.8816278670223205, 2.8292576122422424, 2.666341469062394, 2.991247299947027, 2.773307244603269, 2.790805335874933, 2.9476544290083164, 2.7755881267269937, 2.775441699894441, 2.8904129456080203, 3.151508058560784, 2.9556286305562405, 2.9968605492460814, 2.8340996032315995, 2.876126703884494, 2.9657439683089133, 2.942902791952347, 2.8036185144447634, 2.944021583209347, 2.9339801432714636, 2.958301427208981, 2.9501128206562055, 2.9080558663856575, 3.002268377801731, 3.0857527718004762, 3.0389941100482174, 3.164088771702006, 3.1401910772555577, 2.86603990997284, 2.998145641465221, 3.0502316011753243, 2.967481249959267, 2.9999876251712343, 2.7527537644071747, 3.0739063806338196, 3.0363336748080583, 2.831661792216056, 2.9415722719920616, 3.0854107212254305, 3.1214159745922494, 2.935486829308984, 3.138492672558942, 3.0592171253883635, 3.137701747378067, 3.04797795581004, 3.205981922876729, 3.4257009120705035, 3.095165952664145, 3.1408692632316138, 3.106936693234806, 3.2694388949778017, 3.2257232482949627, 3.183272249751871, 3.2540640277467854, 3.0887191125973965, 3.1191658752543194, 3.147390841247599, 3.1267207452185812, 3.102740168117053, 3.1798829981713355, 3.144040138843943, 3.304131517827478, 3.272840793822201, 3.159903236906073, 3.1717913499824535, 3.2126081916852707], 'val_accuracy': [0.06698130071163177, 0.10110727697610855, 0.18932655453681946, 0.286984920501709, 0.36503902077674866, 0.39353784918785095, 0.35995644330978394, 0.3853693902492523, 0.37974223494529724, 0.3862769901752472, 0.39753130078315735, 0.39317479729652405, 0.3637683689594269, 0.3561444878578186, 0.3505173325538635, 0.3853693902492523, 0.32474133372306824, 0.3786531090736389, 0.37338900566101074, 0.3510619103908539, 0.3920856714248657, 0.3614085912704468, 0.34924668073654175, 0.3933563232421875, 0.3612270951271057, 0.36739879846572876, 0.36068251729011536, 0.36340534687042236, 0.36394989490509033, 0.4080595374107361, 0.36431294679641724, 0.34997278451919556, 0.3467053771018982, 0.3703031539916992, 0.3423488736152649, 0.34525322914123535, 0.36703574657440186, 0.3545108139514923, 0.35668906569480896, 0.35160645842552185, 0.3419858515262604, 0.36866945028305054, 0.3668542504310608, 0.34997278451919556, 0.3396260738372803, 0.3605009913444519, 0.3668542504310608, 0.3477945327758789, 0.3541477620601654, 0.35360318422317505, 0.3546923100948334, 0.36903250217437744, 0.3494282066822052, 0.3795607089996338, 0.3574151396751404, 0.36975857615470886, 0.37484118342399597, 0.3632238209247589, 0.3350880444049835, 0.3648574948310852, 0.3461608290672302, 0.34960973262786865, 0.36031946539878845, 0.3594118654727936, 0.3548738360404968, 0.36540207266807556, 0.35088038444519043, 0.3742966055870056, 0.3398075997829437, 0.36068251729011536, 0.3699401021003723, 0.36667272448539734, 0.35904884338378906, 0.3463423550128937, 0.37484118342399597, 0.35432928800582886, 0.35360318422317505, 0.353421688079834, 0.3443456292152405, 0.3450717031955719, 0.34361952543258667, 0.3514249324798584, 0.3612270951271057, 0.34525322914123535, 0.3438010513782501, 0.3487021327018738, 0.36267924308776855, 0.35995644330978394, 0.35959339141845703, 0.3632238209247589, 0.3652205467224121, 0.36231622099876404, 0.37574878334999084, 0.34688690304756165, 0.3712107539176941, 0.36231622099876404, 0.33889997005462646, 0.3370847702026367, 0.37538573145866394, 0.34652388095855713], 'loss': [3.808754884524749, 3.221209765469272, 2.6896686672596792, 2.246109312790144, 1.9774071950550691, 1.7219138443149595, 1.5822601350216872, 1.4435275992459553, 1.3278810326138724, 1.2510379664525515, 1.1677977694580401, 1.087004444526282, 1.0503250445716852, 1.0041411306667285, 0.9582573311007749, 0.9031818695852712, 0.871107080742211, 0.8371012588009419, 0.8218932412696349, 0.8084754896610677, 0.7814104899047725, 0.753786302449571, 0.7185878607511265, 0.6987112915437171, 0.6938823822401138, 0.6581142253177065, 0.649548272624431, 0.6419470069357518, 0.6215957976709476, 0.6142969665357437, 0.6023472029480621, 0.6092993584834125, 0.5892249743952361, 0.578475073494865, 0.5674857276776037, 0.5513225313412797, 0.555390389938542, 0.5190926603532354, 0.5400315477558406, 0.5059369522348708, 0.48720321579657655, 0.4845941402252653, 0.4904725245663399, 0.4845788355540834, 0.4679867367584426, 0.48675612419857406, 0.45455765718231733, 0.47499953310366255, 0.4501423405952817, 0.43716665138903166, 0.44308855056744384, 0.4257861512988197, 0.4182801548007666, 0.4220601135529858, 0.4146092972563555, 0.3916867410526941, 0.4030880031727038, 0.3963921445614547, 0.3950180504393823, 0.3847576462898436, 0.3787506819549491, 0.38395612489021863, 0.3926499659537976, 0.37476042082389915, 0.3725259768360497, 0.3622244136882, 0.3468853332007321, 0.3669631309066934, 0.36385614426431523, 0.34018648841600413, 0.3628448773420932, 0.3683371606454965, 0.34251738807569543, 0.36085526767624376, 0.35838214033046567, 0.3480538938098964, 0.338873656741523, 0.3255975432379274, 0.3316240912551357, 0.31240030536818586, 0.3332651383709091, 0.3269283250168783, 0.3145844386325529, 0.3048418548648725, 0.31596691954284195, 0.3212881103041278, 0.3012233000758459, 0.3032185987377291, 0.3132809903229497, 0.30947503814626226, 0.29916474435176926, 0.30894936328246614, 0.3072372976604136, 0.29902080263049347, 0.2926834777164635, 0.293362677793059, 0.3207242424505278, 0.3088257282702279, 0.29115080846480157, 0.2960066031730407], 'accuracy': [0.05390877, 0.17362924, 0.28551683, 0.38780525, 0.45062202, 0.506681, 0.5368607, 0.57180154, 0.5962218, 0.6177238, 0.64659804, 0.6608816, 0.6750883, 0.68791276, 0.6933651, 0.7124098, 0.721625, 0.7337583, 0.73851943, 0.7422823, 0.7511903, 0.75856245, 0.7677776, 0.7758409, 0.7827523, 0.7881278, 0.7945016, 0.7986484, 0.8014898, 0.80548304, 0.8079404, 0.8052527, 0.8136231, 0.8150822, 0.8222239, 0.827753, 0.8261404, 0.8372754, 0.83666104, 0.84011674, 0.84687454, 0.8495623, 0.84979266, 0.8533251, 0.85670406, 0.8545538, 0.86246353, 0.8585471, 0.86330825, 0.8644602, 0.863769, 0.8742897, 0.8734449, 0.87667024, 0.87751496, 0.8827369, 0.88004917, 0.88657653, 0.8851943, 0.8803563, 0.88703734, 0.8888804, 0.886423, 0.89033943, 0.8941023, 0.89479345, 0.9008601, 0.8961757, 0.89901704, 0.9047765, 0.89679, 0.8982491, 0.9037782, 0.9010137, 0.90032256, 0.9040086, 0.9070035, 0.90992165, 0.90830904, 0.91122717, 0.9085394, 0.90884656, 0.9132238, 0.91667944, 0.9142989, 0.9109968, 0.9140685, 0.91422206, 0.91260946, 0.9139149, 0.9172938, 0.9148364, 0.9162187, 0.9162187, 0.9174474, 0.9170634, 0.91260946, 0.9166027, 0.91821533, 0.9205191]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd3gc1dX48e9Z9S6ruUjuvXdj02y6Te8tJCQhMQQSIAkJJT9IyPsmIQkvSQidQIBAqMZgOhjc6C6427jbKrYkS1bv2vv7485aK3klr2ytV9Kez/Po0e7s7M4drT1nbjtXjDEopZQKXa5gF0AppVRwaSBQSqkQp4FAKaVCnAYCpZQKcRoIlFIqxGkgUEqpEKeBQCk/icgzIvK/fu67S0ROP9rPUepY0ECglFIhTgOBUkqFOA0EqltxmmR+JSJrRaRSRJ4SkZ4i8p6IlIvIQhHp4bX/+SKyQURKRGSxiIz0em2iiKxy3vcyEN3iWOeKyGrnvZ+LyLgjLPOPRWSbiBSLyAIR6eNsFxH5m4gUiEipc05jnNfOFpGNTtlyReS2I/qDKYUGAtU9XQKcAQwDzgPeA+4C0rD/5m8GEJFhwIvArUA68C7wlohEikgk8AbwHyAFeNX5XJz3TgKeBq4HUoHHgQUiEtWegorIqcCfgMuB3sBu4CXn5TOBk53zSAauAIqc154CrjfGJABjgE/ac1ylvGkgUN3RP40x+caYXGAZ8JUx5htjTC0wH5jo7HcF8I4x5iNjTD1wPxADHA9MByKAvxtj6o0xrwHLvY7xY+BxY8xXxphGY8yzQK3zvvb4DvC0MWaVU747gRkiMgCoBxKAEYAYYzYZY/Y676sHRolIojHmgDFmVTuPq9RBGghUd5Tv9bjax/N453Ef7B04AMYYN5ANZDqv5ZrmWRl3ez3uD/zSaRYqEZESoK/zvvZoWYYK7F1/pjHmE+Ah4GEgX0SeEJFEZ9dLgLOB3SKyRERmtPO4Sh2kgUCFsjzsBR2wbfLYi3kusBfIdLZ59PN6nA38wRiT7PUTa4x58SjLEIdtasoFMMY8aIyZDIzGNhH9ytm+3BhzAZCBbcJ6pZ3HVeogDQQqlL0CnCMip4lIBPBLbPPO58AXQANws4iEi8jFwDSv9z4J3CAixzmdunEico6IJLSzDP8FfiAiE5z+hT9im7J2ichU5/MjgEqgBmh0+jC+IyJJTpNWGdB4FH8HFeI0EKiQZYz5FrgG+CewH9uxfJ4xps4YUwdcDHwfOIDtT3jd670rsP0EDzmvb3P2bW8ZPgbuBuZhayGDgSudlxOxAecAtvmoCNuPAfBdYJeIlAE3OOeh1BERXZhGKaVCm9YIlFIqxGkgUEqpEKeBQCmlQpwGAqWUCnHhwS5Ae6WlpZkBAwYEuxhKKdWlrFy5cr8xJt3Xa10uEAwYMIAVK1YEuxhKKdWliMju1l7TpiGllApxGgiUUirEaSBQSqkQ1+X6CHypr68nJyeHmpqaYBcl4KKjo8nKyiIiIiLYRVFKdRPdIhDk5OSQkJDAgAEDaJ4ssnsxxlBUVEROTg4DBw4MdnGUUt1Et2gaqqmpITU1tVsHAQARITU1NSRqPkqpY6dbBAKg2wcBj1A5T6XUsdNtAsHh1NTWUVy8n4ZGTduulFLeQiYQuGtKSanJpqG2usM/u6SkhEceeaTd7zv77LMpKSnp8PIopVR7hEwgkIhYAEx9VYd/dmuBoPEwtY93332X5OTkDi+PUkq1R7cYNeSPsMhoGo0g9R1fI7jjjjvYvn07EyZMICIigvj4eHr37s3q1avZuHEjF154IdnZ2dTU1HDLLbcwd+5coCldRkVFBXPmzOHEE0/k888/JzMzkzfffJOYmJgOL6tSSrXU7QLBvW9tYGNemc/XGuuqcFGEROa16zNH9Unkt+eNbvX1++67j/Xr17N69WoWL17MOeecw/r16w8O8Xz66adJSUmhurqaqVOncskll5CamtrsM7Zu3cqLL77Ik08+yeWXX868efO45hpdfVApFXjdLhC0xeBCaAj4caZNm9ZsnP+DDz7I/PnzAcjOzmbr1q2HBIKBAwcyYcIEACZPnsyuXbsCXk6llIJuGAjaunPfuzeX3qYA0kdAROCaXeLi4g4+Xrx4MQsXLuSLL74gNjaWWbNm+ZwHEBUVdfBxWFgY1dUd34SllFK+hExnMUBDmHPx7+AO44SEBMrLy32+VlpaSo8ePYiNjWXz5s18+eWXHXpspZQ6Wt2uRtAWExaFu0FwdXCHcWpqKieccAJjxowhJiaGnj17Hnxt9uzZPPbYY4wbN47hw4czffr0Dj22UkodLTHGBLsM7TJlyhTTcmGaTZs2MXLkyMO+N6+kmuSqXcRGhkHasEAVMeD8PV+llPIQkZXGmCm+XguppqFwl1BlIjH11dDFAqBSSgVKaAWCMBfVRCHGDQ21wS6OUkp1CqEVCFxCtYm0TwIww1gppbqikAoEEWFCLZEYRAOBUko5QioQhIe5MEBDWDQEINWEUkp1RQELBCISLSJfi8gaEdkgIvf62Of7IlIoIqudnx8FqjwAYS6by7/eFW1rBNphrJRSAa0R1AKnGmPGAxOA2SLiaxD9y8aYCc7PvwJYHlwihLtc1BIFHdhhfKRpqAH+/ve/U1WlzVRKqeAJWCAwVoXzNML5CfoteHiYUI2TzqGD+gk0ECilurKAziwWkTBgJTAEeNgY85WP3S4RkZOBLcDPjTHZgSyTHTkUAUiH9RN4p6E+44wzyMjI4JVXXqG2tpaLLrqIe++9l8rKSi6//HJycnJobGzk7rvvJj8/n7y8PE455RTS0tJYtGhRh5RHKaXaI6CBwBjTCEwQkWRgvoiMMcas99rlLeBFY0ytiNwAPAuc2vJzRGQuMBegX79+bR/0vTtg37pWX+7T0IjbbUDq7AZnwZo29RoLc+5r9WXvNNQffvghr732Gl9//TXGGM4//3yWLl1KYWEhffr04Z133gFsDqKkpCQeeOABFi1aRFpa2uHLoZRSAXBMRg0ZY0qAxcDsFtuLjDGehvongcmtvP8JY8wUY8yU9PT0oyqLiG2fMuKy/QQd3Fr14Ycf8uGHHzJx4kQmTZrE5s2b2bp1K2PHjmXhwoXcfvvtLFu2jKSkpA49rlJKHamA1QhEJB2oN8aUiEgMcDrw5xb79DbG7HWeng9sOuoDt3HnDlBWXsPe0hrGJDcgZdmQPhIioo/6sB7GGO68806uv/76Q15buXIl7777LnfeeSdnnnkm99xzT4cdVymljlQgawS9gUUishZYDnxkjHlbRH4vIuc7+9zsDC1dA9wMfD+A5QHsXAJw5hJAh3QYe6ehPuuss3j66aepqLD95Lm5uRQUFJCXl0dsbCzXXHMNt912G6tWrTrkvUqpdtq5DP45GWpKg12SLi1gNQJjzFpgoo/t93g9vhO4M1Bl8CXcM5dAoojsoA5j7zTUc+bM4eqrr2bGjBkAxMfH8/zzz7Nt2zZ+9atf4XK5iIiI4NFHHwVg7ty5zJkzh969e2tnsVLtteF1KNoGe9fCwJOCXZouK6TSUAPU1DeyJb+cfimxJFfuBHFB2tBAFDVgNA21Uo6HpsH+b+Hs+2Haj4Ndmk5N01B78dQIGtzGjhjSGcZKdU0VBTYIABR+G9yydHEhFwjCXIIgNDS6bSDQlNRKdU27PrW/IxOgcHNwy9LFdZtA4G8Tl4gQHiY0NBqIDMwaxoHU1ZrylAqYXZ9CZDyMPFcDwVHqFoEgOjqaoqIivy+S4S6h3m0gPJqOnGEcaMYYioqKiI7uuOGuSnVZuz+DfjOg52ioLITKomCXqH060UinbrF4fVZWFjk5ORQWFvq1//6KWtxuQ01hNJSXgJRCfFmAS9kxoqOjycrKCnYxlAquikJbCxh/pZ0LBPZ53AnBLZe/dn0Kz5wLI86BM34PqYODWpxuEQgiIiIYOHCg3/v/+rU1LNmyn6/uOh3efhLWvgJ37AFXt6ggKdX97Xb6BwacBPE97ePCzTDAj0BQWQQxPYL7/331f22LxPZFsOU4mP4TOPVuCI8MSnFC8sqXnhBFUUWdzTnUZyLUlUPx9mAXSynlL0//QO/xkJRlH/vTT1CaA38bBc9fBFXFre93YBdUH+iw4jbTUAub3obRF8HNq2Dc5fD5g/DlkWUw7gihGQjio2hwG0qq6+0/JIC9a4JbKKWU/3Z9Bn2Pg7AIm0Asfbh/gWDDG9BQA7s/hydm2oloLeVvhEdmwPyfdHy5AbZ/ArWlMOZiSOgFFz4Cg0+FLx6CuuAMXAnJQJCWYNcjKCyvtf+AJAzyNwS5VEopv1QUQuEmGHBi07b0Ef7NJdgw3978/eB9aGyAp860TcMeNWXwynftSMJtC6G6pOPLv36ebZoaNKtp28m/sh3eq55r/X2V+6GxvuPLQ4gGgowEO+omv6wGwqMgbRgUbAxyqZRSftn9mf09wCulRPoIqMg/THPPbshdAaMvhqzJcP0SyJwEr/8Y3rnNNtm8eRMU77QduO56+Pa9ji17XZX9zJHn29qMR//jof8J8Nk/fM9r2rkMHj0BFv2xY8vjCMlA0DPR1ggKyp0/eM9RtjqolOr8vn0XopOgz4Smbekj7O+2agUb37C/R19of8dnwPfehBk/heVPwoOTYNMCOP13cPzNkNS36T1HypjmwWnrh1BXYZuFWjr5NijPgzUvNm1zN8LiP8Nz50NUAoy55OjK04oQDQReNQKAjFFQuqdTjetVqt1KsqE8P9ilCKz6atj8zqF31BmeQNBGJvsN86HPJOgxoGlbWASc9Qe47Fn7/3/UBXD8z2y/w6gLbHv+0VwXPvkf+Otg+OA3tjaw4XWIy2hem/EYdApkToZlD8DOpfb302fB4j/C2Mtg7mLoNebIy9KGkAwE0RFhJMVENAWCnqPt74KjXw5BdUHGwNdPQmlusEty5Oqr4clT7IiYeT+G3JXBLlFgbPnA3lGPvbT59sQsiIhrvUZQvBPyvrEjdXwZfSH8cjNc+owNAmADQWMdfPu+7/cc2AXZy8Ht9v36zmX2Yp42zHYEP3q8Lf+oC8AVduj+IravoGQ3PHsefHyvHbl0wcNw0eMQFe/7OB0gJAMB2OahQwKBdhiHpuId8O5t9j9rV7XuVdvZOPJ82wb95Knwyf8Gu1T+WfJXWPi7Q7eveRneu715Usj1r/m+o3a5mo8cWvMy/N8IWPQnqK85tFnIl6j45nMLMqdAYuahzUMl2bDgZ7Yp6anT4cEJsPg+KNnTtE/1AZh/g50o9uNP4Nq3AWNHLLUMYt6GzYaLn4TvzINf74SfrYSJ1zQFpwDpFhPKjkTPxGj2lTl9BEl9ISpRO4xDlefuedtC4E9BLcoRMQa+eAR6joVLn4bacnjpajs65dT/F+zSWfkb4aO77R35xGuatrsbbQCuKbHNIiPPs9v3rbMdt+56u33c5XZEz5YPYfL3fd9Rp4+A7R/Dwnvh0wcgqR8suc8GSdMIWVMh+TBrnntzuezd+/Kn7LEb62HpX2HFU/b1qT+y/RRrXoTFf4Ilf7YzhaffaGuYFfvguo8gMs6ulfCTz+3NZt9prR9TxJ7rMRbSgWBbwX77RAQyRmqHcajKtavFsX+Lvatrz8WiM9ix2LaNX/CI/bccnWiHJn7yP3b4Y0xy8MrWWA+f/g2W/MVe1MvzmweCvNU2CETGw1u32LkBUYnw+lyITbHj7D/4DQw909Z0Gmtb7zBNHw5r/muDwKRr7RoFu5bBO7+0zTjT5ra//KMusBO93rwJdiyxk08nXgMzb7cT2QAmXG1HJK38N6z4N2x6y24/7R47KskjMq7tIBBEIRwIoigotzmHXC6xzUPr5tm7qwBXw1Qnk7sSEnpD+V7Y9jFM+UGwS9S62nJ45Xsw+DSYcZP9t/rloxCX3rzJwTOiZu8aGDSz445fvs+mdPDn/4jbDc+cA9lf2Yt3Ul/47O92dq/nIrrjE/v76pfhPxfDgpttc0rBRvjOaxCXZpu5Fv3BNuEl9Wv9YjrgJAiPgdN/C8fdYMs45DS48QvbNj98TvvPN2saJPSxo4mGnmmHlWb4WBSqR3872ujkX9saQmk2nHBr+48XJCEcCKJpdBv2V9baeQUZo6D2aSjLbfpHqrq/xnrYtxamXAcb37RNC505ECy8145k2f6JrcFM/wls/QBm3WnnxHj0dlaJ3bu64wLBni/tKJaR58O5f4e41Lb337XUBoHZf4bpN0DBZhsItn4IU35o99m+GHqNs5PDTv8dfOCsXDvlOhh6hn089Uew/F+ANI3o8SVrMtyVd2gOoYiYtvsG2uJywZUv2M54f/IYRcbC1OuO7FhBFMKdxXYIaYGnn+Bgh7E2D4WUgo22Ay9zkr173LEkYLM3j9quz+x49+NugJNug1XP2pmxYZFNF1aPuFR7B5632vdnGWPHp7/0HWio8+/4Ocvt72/fg0emH36y1Tcv2PH+k79vn6cPh+T+9u4coLbCBorBp9jnx90AQ063N2Vn/k/T55zyG4hNs+38hxtHH4hEcpmT/AsCXVjAAoGIRIvI1yKyRkQ2iMi9PvaJEpGXRWSbiHwlIgMCVZ6WDp1L4FT3CnTkUEjxdBRnTraBoLYMcla0/Z5gqKuCBT+1Y+BPuwdOu9v2CdRXw7gr7OSolnqPtzWCltxueO/Xdnz65rdtG74/CjbZZqG5i+3xXrzSBidfqktsc8rYyyDCWT9DBIadZYNtfbXN9+Out+PnwV7Er34Vrl9q29M9YpLhokdtoOg11r+yqnYJZI2gFjjVGDMemADMFpHpLfa5DjhgjBkC/A34cwDL04xndvE+TyCI6WHHIusQ0tCSuxJiUuwFduBMm3dq+8fBLtWhFv/RtpGf92DTRXLid+CWNXDO//l+T58J9j3eE6LcjXbo49dP2Bm1Yy6FpX+BfesPX4aCTfaGqdcYuO5DCIuys3x92fC6rWlN+E7z7UPPgoZqmz10xyKbirnfjKbXXa7mE8U8hpwOc/6s/XcBErBAYKwK52mE89NyCbELgGedx68Bp4kcm286PT4KEcgv88rroakmOr/8DbDutY77vNxvbNVfxN55Zk11hpFiR5o8f6lt4gimvWvhi4dtE0vL9v6kzOZ9A94O9hN4ZdZd+FtY/TzMvAPO/F84+6/2JujNG22TWPFOePEquH+YHTLp4XbbMfqeRWAi46DfdJtP35dvnoeM0TbNu7cBJ9q1wre8b9/bb0ZTjUEFTUD7CEQkTERWAwXAR8aYr1rskglkAxhjGoBS4JAeKBGZKyIrRGSFv6uQHU54mIu0+CjyS2uaNmaMsh1wnbWNWNmZmvOvt+3LR6uu0g67zJzctG3IabZdfeWz8NjJsO0j28HZ1jKoDXWBSx9sDLx/h71Yn35I62rbPCOHPP0ENWWw/GkYezmccqcNfrEptkaxdw08fwk8fJxtw6/Ib+oTADvbtb6q+YiZwafYptSWaS0KNtua1sTvHHoHHxFth7ZumG//9p7+ARVUAQ0ExphGY8wEIAuYJiItE2X4uvs/5H+cMeYJY8wUY8yU9PT0Ditfz8Qo8su9AkHP0bbNcv/WDjuG6mCFm8HdYDsZ2+LPOtR714BxHxoIMPDWzZA6CE78ub05aG2yoTHw38tsPpm3bvGvicWjodauVPXyd+0M2q+ftE0m3kFnw3ybbfPUu9s/HyAuzTZ3evoJ1r4M9ZV2BI+3URfYiV47l9gJXTd+AUjzvhLPjN2MUU3bBs2yv3cubf55q58HV7jtu/Bl6JlQ5awvPEgDQWdwTIaPGmNKRGQxMBvw/p+SA/QFckQkHEgC2sgj27F6JUaTW9IiEADkr7fNRKpzaWywF2WwF8whp/neb9NbMO9HcNNXzROMteTpKO7jNemn90R7gcsYZYcz1pTZ1MAb5jf9+/C27lU7oWvASbDmJVj5jO0gveRfLcr0tp1ZmzrUfo4r3I76qci3aQy2f2Jz6ACMONfmlwmLhA/vth2kk753mD9OK/pMsDUCY+wQzD4Tmwc+j4seh1l3Qfow+zxjFOR83fS6JxCmD2/a1mucransWATjLrPbGupseodhs20g8mXYWfZ3bBr0DEwSNdU+gRw1lC4iyc7jGOB0oOUSQguAa53HlwKfGNNWHbxjZSRGN40aApscKjymaaap6lwO7LRJwMAGgtases52VG58s+3Py11pJyjFe9UyXS6bmnj2n2zbe3y6vchvmH9o81B1CXxwl72wfm8B/GKTHca57tVDawaf/s0GldJsuyzh0r/YC+l358PPN8CdOfCLzbbdfsv7dvWsd34JZTkw5y++Uyr4o88EuwzrlvftXf3UH/neLzyqKQgAZE2xTUOehGoFm+xw1OhEr79VmO1g37G46W+z+nmoLGh7LH1iH/s3HX2hrhPeSQTyW+gNLBKRtcBybB/B2yLyexE539nnKSBVRLYBvwDuCGB5DtEzIZriyjpqGxrthrAI23HofSekOg9PdtjBp0LeKt/9BJVF9u4a7F14W3JXNU8B0JoxF0PRNltT9PbJ/9omjnMesBe02BTbhBMW2XylqfyNdkGUk35hm13uyoPbtsE1r9lzEbE/ib3thKnvv+vcWf/XLqLS//jDl7E1ng7j934N0cn28/zRd5odbVS0zT4v2NyU89/boFl2Eub+rbbMyx6wHe6Ha/L5/tutj3ZSx1wgRw2tNcZMNMaMM8aMMcb83tl+jzFmgfO4xhhzmTFmiDFmmjFmR6DK40uvJK8lKz2yptpRGvU1rbxLBUzht1C0vY3XnQrllOucfoIvD91n4xv2tZHn24Bevs/3Z5XttR2g/gSCEefZYaUb5jdty/vGNrVM/XHzBVJiU2w7+9qXmvopvvkPuCJg3JX2uaem0Zp+x9mx9DNvt0Mmj4anbCV7bI6cyFj/3pc11f7O+dppkvvWd2qFQbPs7x2LYfULtsYz8w4d5tnFhHS9LKPlpDKw/wHc9b4n4qjA+u8V8PjJrTf7FGyyM1MHn2Lb2H3tt34epA2HU+6yzze3Uiv4+glAYPg5hy9XXKodtulpHtq7Fl651k6qOvU3h+4/6Vp7N73pLdshvOZFm5XycCkZvMWn23PwNVGsPTwdxnDo7OO2pA61s4Kzv7ZzERrrmncUe6QMtP0wWz+0tYHMKa333ahOK6QDQc+Daxd71Qg8Ca28h86pwDuw2+kDqLfDGD1j+b0VbrZ3pZFxtl2+ZSAozbWzVcdeapsxUgb7bh6qLrF386MvhLQh/pVv9EX2gvjRPfDUGbacV/7XXixbGnCSvTiues4GouoDR97Z2xFGXwjjr7LJ3PzlctmbopwVTat+ZfhoGgJbK9j2kV3lb5bWBrqikA4EvZJ81AjiM+xdZ7b2ExxTu5bZ39e8ZjvtX7yq+cpQjc6wXk879YATbRt/bXnTPhteB4zNRyNim2h2LbMXYm/L/2VTSZz4C//LN+JcWwv5/EE7keqGZbZD1ReXy174dy2z6ZeT+gV3mORZf4CLHmv/+7Km2tFC2V8DYmtavnjOLXOynQGsupyQDgQ9YiOICJOmNBMeWVNtjeDYDWBSO5dBbCr0PxGufcsOU3znl02jVoq22yY7Tzv1gJNsErI9XvMJ1r1qh0d67nxHnmf7CzxJzsBO/PryETuWvfc4/8sXm2KTn532W7jm9daHRnpM+I7tVyjcbNvmu+LomKypgLHzD1IGtt6/MPgU2yR0xv9obaCL6oL/OjuOiJCREN2UgdSj7zSbm76sC69h25kZ0zzIGmPvngecaC+YMck2D05ZTtNYf0/zhKdG0Hea7YD11CT2rbcTxMZ45+Sf5OSSf6tp26rn7Eifk37Z/nKf9Av7489QzoRediw9Yhcu6Yo88w0qC5tSS/gSnQQ//rjbZ+jszkI6EIBtHsr3VSMAbR46WoVbbJIzb263TWOw7P6mbcU7bND1Xod2+Bw7DNMzUqdgM7Z5whnr7uknWP86PD0bHjvRzgEZ4zU80uWynbTbPobP/wmfP+Q07Rxvm3cCbc59cNVLkNw38McKhJjkpsDra8SQ6jZCPhD0TIw6tGmo11ibFVE7jI/cgd02Z/2Kp5tvz19vhyJ+9s+m9n3PXf3Ak5v2i06y7c0b33ASnm2yHbDezRNDz7AdlLUVdmGWGz61k5W8jbvCjnj58P/Bh7+BsjyYdXuHn65Pyf1g+Oxjc6xA8dwUaSDo1kJ2hTKPjIRolm7Z33xjWIRta9YawZHbucS24W95H6b9uPl2gNpSWPUfmHGjHf0T37Ppbt9j9EU2zXHOclsjaDl88YRbbadsW0Ms+061E7ga6wBjO3y9c92rtvU/3s6D0HUAurWQrxH0SoqmoraBitqG5i9kTbVLGDbU+n6japtnaOeuz5ongNuxxF7w+x1vO20b621H8YATD+1oHDbb5rxf+7JNk9By+GJYuH/j7COibWqE6CQNAu017gr40cfNcwypbifkA4FngZpD+gn6TrN3kd653ENB8U746gnbSdvYcPj9s5fDA6Oa0j+A7fzducze5Tc4K1GBTUGw+3Obn+aEm+0s1KX3Q8W+5v0DHtGJtvnnm+ft6J+2OixVYLjCWh8mq7oNDQS+ZhdDU9uopykjVCy9H977FTx5Kvy5v02RXFHge9/GepuuuSzXXqw9indAeR4cf7O9o9/mrPiVu9KmQR40065UlTbMJl+D5v0D3kZfBI1Oray1CU1KqaMS8oGglxMI8kpaBIKEXvYudcUz/t0ZHyuBLkvuSnvelz5tmwW2fgSPz4SclYfu+8XDdsJRYpYd3eMZ8+/p/B16pm1j9iz9uHMJiKtpmOiMn9r1ABL6QMog3+UZdpbtuBeXTXuglOpwIR8I+iTHAJB7wMdCJjNusmPZNx0mnfGxUrYX7usH374XmM+vLbcToAacaGfnnvuAszZtOPx7Nqz4d9Nw0AO7YfF9NlfP6b+ztQJPEriDnb9D7cifws1Qkm37B3qPtznswQaaxEwYenrrE5GiEuzEsF7jdElDpQIk5ANBdEQYGQlR5Jb4WGpw6Fk2X80XD3eOWcZ7PrdNKyv+HZjPz1sNmOYLl/QeB3OX2LVl374VHpwAy/4P3vmFvUs/+y92zH94jF1L2LCw8soAACAASURBVNM/MOAke3H3JCDb9JYd/ePdBBQRbYd8zvlL2+U6/59w7YIOP12llBXygQAgs0cMOb5qBC4XTP+JbS7pDENJPQvmbFsIlftb38/dePjA9e6vbAbNZp/vY8UusOkVvjsfLnvG5mH6+Pe2DKfcBUlZEBVvx8tvfMOmkq7YZ2sVYCckJWbahVnc9bajuOVnR8S0XdaIGN/J3ZRSHUIDAZDVI5bcklbWuJ1wtV3Q44uHjl2Bqg/Yu2tPm7tHzgrb5GIam+fG95a3Gv42Ghb9ofXPr9xvl1TctKB5QMlbZSdt+UqX7AqzHbfffxtu+toupXic19q3Yy61qRs8x/Xc+XtqBZUFdqZwvxmHO3ul1DGmgQDITI4hr6Qat9vHXXRkHEz5gU0nfGBX4AtTsNmO2Jl3HWz1SpbWWO/k0rkEMkbD2lcOfe/2RfDMOTZP0oqn7XBNX9a8aIfGGrfNI++Ru8r3erYtpQ+3idTCvOYjDjkdohJtcEno3bzzd7DTPJQ1zf+FUZRSx4wGAiCrRwz1jYaC8lYmj02bazNJPnYyvPoDuzi3d/rjjrL5HfjXaTZlQkSsHbHjUbDRjsnPnAzjLrcrRxV7Lei2fh68cJltujnvH/bu3FdOf2Ng5bP2opzQp6njuTzfjutv2Szkr4hom6oZmvoHPAbNgoi4pkXLlVKdigYCbCAAyDngo8MYbP6aa9+CUefbETHz59rEaVt9XGjby+22n/Pi1fDS1XakzdzFTYt9eNr6Pe33mZPtwiuIbT4C+PpJeO06O/fhB+/aFMhx6fbOv6Xdn0PRVlvLGXaWHeNfX2ObhTyff6TGXmJ/D2wxOSwmGW5ZDdNvPPLPVkoFTMByDYlIX+A5oBfgBp4wxvyjxT6zgDeBnc6m1z1rGx9LTYGgmikDWtmp/wz743bD7s9srvwXLrFNJGf+wV7s2sPttmu8LrvfNjnFpsFJt8HJt9nO0SGn2Tw7RdtscMhZafP19xhg77b7n2Cbh9yNsOQ+GH62Hfvv6Xgde5ldgKWq2HbIeqx8BqKSYNSF9pgr/22DW+5KW+tpT47+lgafZlftGnLGoa8d7ZKLSqmACWSNoAH4pTFmJDAduElEfCx6yjJjzATn55gHAYDMZNtu3WqHsTeXy97xXr8UTvw5rP4vPHse1FX6f8Bdn8ETM2HBTyEmBS55Cn6xEU67u+lC7rmYepqHclfau3VPk8u4y+2d/ZL7bA3g8v80H30z/irbD7Dh9aZtVcWw8U373shY26EbEQtb3rOfnzHq6HLxiNi0z+GRR/4ZSqljLmCBwBiz1xizynlcDmwCMgN1vKMRExlGalxk601DvkRE24lUV74I+9bBGzcefsjmgV3wyvfgmbPtRfmSp+DHn9imnvCo5vv26G9TMGxbCDVldlJWplfOl1EX2A7ZE261I3jCWlTueo21ncprXmratuYlm65h8rVN5zD4VNtPkLsKMo+wf0Ap1aUdkzTUIjIAmAh85ePlGSKyBsgDbjPGbPDx/rnAXIB+/foFpIxZrc0lOJzhs21AWPhbm6dn5q+av+52207Ylc/YiWmuMJh1Fxz/s8OPoBlyOix/CvZ8wSETvWKS4eZvWn+vCIy/Ej6629YqNr5hA0Hf45qnFB4+x46IAg0ESoWogAcCEYkH5gG3GmPKWry8CuhvjKkQkbOBN4BDEsoYY54AngCYMmVKQKb4ZvWIZdPelsXz0wm32FE9i/4Xqottk0z5PpuGoWibHe0DMO5KOO0eSPKzYjTkdJuq+TOna6W9F+pxl9sA9cKlNl/P1B8dukTj0LMA4ZBAo5QKGQENBCISgQ0CLxhjXm/5undgMMa8KyKPiEiaMaaNabOBkdkjho825eN2G1yudi7ALQLnPQilOfbCHdPDjqVPyrKZNlOH2LTWPUe373P7n2BTN+z+zDYDeXf6+iOhl6191JTYGkhCr0P3iU+3Zdu7VtM8KxWiAjlqSICngE3GmAda2acXkG+MMSIyDdtnURSoMrUlq0cMdQ1u9lfWkpFwBMnNIqLh++/YhWw6KjlaRLTtmN76YfP+gfZo2VTly+n3woGdh/YzKKVCQiD/558AfBdYJyKrnW13Af0AjDGPAZcCPxGRBqAauNKY4GR3y0xuGkJ6RIEAbM2gozNkDjnDCQQBbLbxDI1VSoWkgAUCY8yn2MbntvZ5CDiGSXxal9XDGUJ6oJpJ/XoEuTReRp1vO3q7+iLoSqlOS9sCHJlek8o6lYRedrawUkoFiKaYcMRHhZMcG+F7XQKllOrGNBB4OeK5BEop1YVpIPCSmayBQCkVejQQeMnqEUvugWqCNHBJKaWCQgOBl6weMVTXN1Jc2cqCLkop1Q1pIPDimUvgVxZSpZTqJjQQePHMJcgu1kCglAodGgi89E+NRQS2FgRgGUqllOqkNBB4iYsKZ0h6POtySoNdFKWUOmY0ELQwNiuJtbmlOnJIKRUyNBC0MC4zicLyWvLLaoNdFKWUOiY0ELQwNssuQr82pyTIJVFKqWPDr0AgIreISKJYT4nIKhE5M9CFC4bRfRIJcwnrcrWfQCkVGvytEfzQWU3sTCAd+AFwX8BKFUTREWEM65nAWu0wVkqFCH8DgWddgbOBfxtj1nCYtQa6snGZSazNKdEOY6VUSPA3EKwUkQ+xgeADEUkA3IErVnCNzUriQFW9JqBTSoUEfxemuQ6YAOwwxlSJSAq2eahbGpeVBMC63FL6psQGuTRKKRVY/tYIZgDfGmNKROQa4P8B3bYRfXivBCLCRPsJlFIhwd9A8ChQJSLjgV8Du4Hn2nqDiPQVkUUisklENojILT72ERF5UES2ichaEZnU7jMIgKjwMEb0SmRdrg4hVUp1f/4GggZje04vAP5hjPkHkHC49wC/NMaMBKYDN4nIqBb7zAGGOj9zsQGnUxiblcTaHJ1hrJTq/vwNBOUicifwXeAdEQkDItp6gzFmrzFmlfO4HNgEZLbY7QLgOWN9CSSLSO92nUGAjMtMorymgd1FuoaxUqp78zcQXAHUYucT7MNe0P/q70FEZAAwEfiqxUuZQLbX8xwODRaIyFwRWSEiKwoLC/097FEZ63QYr9WJZUqpbs6vQOBc/F8AkkTkXKDGGNNmH4GHiMQD84BbnUlpzV72dTgfx3/CGDPFGDMlPT3dn8MetWE9E4gKd7EmW/sJlFLdm78pJi4HvgYuAy4HvhKRS/14XwQ2CLxgjHndxy45QF+v51lAnj9lCrSIMBdjMpNYrYFAKdXN+ds09BtgqjHmWmPM94BpwN1tvUFEBHgK2GSMeaCV3RYA33NGD00HSo0xe/0sU8BN6JvM+txS6hu77dw5pZTyOxC4jDEFXs+L/HjvCdjO5VNFZLXzc7aI3CAiNzj7vAvsALYBTwI3tqPsATehbzK1DW4279UVy5RS3Ze/M4vfF5EPgBed51dgL+KtMsZ8ymHyETlDUm/yswzH3IS+NiX16uwDBzuPlVKqu/G3s/hXwBPAOGA88IQx5vZAFqwzyOoRQ1p8JKuzdeSQUqr78rdGgDFmHrbjN2SICBP6JrM6+0Cwi6KUUgHTZiAQkXJ8DOfENvkYY0xiQErViUzom8zCTQWUVteTFNPmHDqllOqS2mwaMsYkGGMSffwkhEIQABjfV5euVEp1b7pm8WGMc9YwXr1HA4FSqnvSQHAYSTERDE6P04llSqluSwOBHyb07cHqbF26UinVPWkg8MOEfskUVdbp0pVKqW5JA4EfJh6cWKbNQ0qp7kcDgR+G90ogJiKMz7fvD3ZRlFKqw2kg8ENEmIs5Y3rx9pq9VNU1BLs4SinVoTQQ+OmKqX0pr23gnbWdJjmqUkp1CA0Efpo2MIVBaXG8vDz78DsrpVQXooHATyLCFVP7smL3AbYVaFpqpVT3oYGgHS6elEW4S7RWoJTqVjQQtEN6QhRnjOrJvFW51DY0Brs4SinVITQQtNMVU/tSXFnHwo0Fh99ZKaW6AA0E7XTS0HQyk2N4afmeYBdFKaU6hAaCdgpzCZdNyWLZ1v1kF1cFuzhKKXXUAhYIRORpESkQkfWtvD5LREq9Fra/J1Bl6WiXT+mLS9BOY6VUtxDIGsEzwOzD7LPMGDPB+fl9AMvSofokxzBzWDqvrsymodEd7OIopdRRCVggMMYsBYoD9fnBduW0fuSX1bLo28JgF0UppY5KsPsIZojIGhF5T0RGt7aTiMwVkRUisqKwsHNceE8dkUF6QhQvfa2dxkqpri2YgWAV0N8YMx74J/BGazsaY54wxkwxxkxJT08/ZgVsS0SYi8smZ7Ho2wL2luo6BUqpritogcAYU2aMqXAevwtEiEhasMpzJK6Y2he3gVdX5AS7KEopdcSCFghEpJeIiPN4mlOWomCV50j0T43jpKFp/OfL3dTU60xjpVTXFMjhoy8CXwDDRSRHRK4TkRtE5AZnl0uB9SKyBngQuNJ0wUWBb5g5mMLyWl5bqbUCpVTXFB6oDzbGXHWY1x8CHgrU8Y+V4wenMr5vMo8v3c6VU/sSHhbs/nellGofvWodJRHhxlmDyS6u5p11umiNUqrr0UDQAc4Y2ZOhGfE8ung7XbB1SykV4jQQdACXS7hh5mA27yvnk82alVQp1bVoIOgg50/oQ2ZyDPd/uIW6Bk07oZTqOjQQdJCIMBf3nDeKTXvL+L+Pvg12cZRSym8aCDrQWaN7cdW0fjyxdAefb9sf7OIopZRfNBB0sLvPHcnA1Dh+8coaSqrqgl0cpZQ6LA0EHSw2Mpx/XDmRospa7pq/LtjFUUqpw9JAEABjs5K49fRhvLtuH+/q3AKlVCengSBArj95EGMzk7jnzfUUV2oTkVKq89JAECDhYS7+etk4Sqvr+f1bG4JdHKWUapUGggAa0SuRm04Zwhur81i4MT/YxVFKKZ80EATYjbOGMKJXAnfNX6ejiJRSnZIGggCLDHdx/2XjKa6s43cLtIlIKdX5aCA4BsZkJvHTU20T0fvrdRSRUqpz0UBwjNx0yhDGZCbym/nrKaqoDXZxlFLqIA0Ex0hEmIsHLp9AeU0Dt89bq0tbKqU6DQ0Ex9CwngncMWcECzcVcO4/P2VNdkmwi6SUUhoIjrUfnjiQZ384jcraBi5+9HPu/+BbGho1bbVSKngCuXj90yJSICLrW3ldRORBEdkmImtFZFKgytLZzByWzgc/P5mLJ2by0KJt/Oi5FZTX1Ae7WEqpEBXIGsEzwOw2Xp8DDHV+5gKPBrAsnU5idAR/vWw8f7xoLJ9u3c8lj35OdnFVsIullApBAQsExpilQHEbu1wAPGesL4FkEekdqPJ0Vlcf149nfziNfaU1XPTIZ6zc3dafTCmlOl4w+wgygWyv5znOtkOIyFwRWSEiKwoLC49J4Y6lE4akMf+mE4iPCueqJ75i/jc5wS6SUiqEBDMQiI9txteOxpgnjDFTjDFT0tPTA1ys4BicHs/8G09gUv9kfv7yGv76wWbcbp9/DqWU6lDBDAQ5QF+v51lAXpDK0in0iIvkuR8ex1XT+vLwou3cs2A9xmgwUEoFVngQj70A+KmIvAQcB5QaY0I+/0JkuIs/XjSWxJgIHl+yA5cI954/GhFfFSillDp6AQsEIvIiMAtIE5Ec4LdABIAx5jHgXeBsYBtQBfwgUGXpakSEO2aPAAOPL7XB4J5zR+FyaTBQSnW8gAUCY8xVh3ndADcF6vhdnYhwx5wRuI3hyWU7WZNTwr3nj2ZcVnKwi6aU6mZ0ZnEnJiLcdfZI7r9sPNnF1Vzw8Gfc/tpaTVqnlOpQGgg6ORHh0slZfHLbTH504kDmrcrhtAeW8PLyPTqqSCnVITQQdBGJ0RH85pxRvHvLSQzNiOf2eeu48okveX/9Pso0PYVS6ihIVxueOGXKFLNixYpgFyOo3G7Dqyuzue+9zRyoqifMJUzql8zxg9M4blAKE/v2ICYyLNjFVEp1IiKy0hgzxedrGgi6rvpGN6t2H2Dp1kKWbtnPhrxS3AYiwoQzR/fiZ6cOYUSvxGAXUynVCWggCBFlNfWs3HWAZVv388qKbCpqG5g9uhc3nzaUUX00ICgVyjQQhKCSqjqe/nQn//5sF+W1DcwZ04tbTx/G8F4JwS6aUioINBCEsNKqep76bCdPf7qTyroGLpqQyW/OGUlqfFSwi6aUOobaCgQ6aqibS4qN4BdnDOPT20/hhpmDeWttHmf8bSlvfJOreYyUUoAGgpCRHBvJ7bNH8PbPTqJfSiy3vryayx77gleWZ+vqaEqFOG0aCkGNbsPzX+7m35/tZFdRFVHhLqYNTKFnYjSp8ZGM6p3IeeP6aG4jpboR7SNQPhljWLWnhPnf5LA6u4Siijr2V9RS32iYNiCF+y4Zy6D0+GAXUynVATQQKL8ZY3htZQ7/8/ZGahvczD15EFMGpDAkI57eidFaS1Cqi2orEARzPQLVCYkIl03py8xh6dz95nr++cm2g69FR7jonxJH/9RYBmfEc/aY3ozJTNS1EpTq4rRGoNq0v6KWbQUVbC+sYEdhJbuLqthdVMmuokrqGw3DesZz4cRMRvVOpH9qHJnJMUSG6xgEpTobrRGoI5YWH0VafBTTB6U2215aVc/b6/KYtzKHv7z/7cHtYS5heM8EJvRLZmxmElV1jWQXV7GvtIYxmYlcODGTrB6xx/o0lFJt0BqBOmoF5TXsLqpiT1EVO/dXsianhNXZJZTXNAAQFxlGekIUu4qqAJgxKJWZw9MZ0yeJ0X0S6REXGcziKxUStEagAiojIZqMhGimDkg5uM3tNmQfqCIxOoLk2AhEhOziKuZ/k8sbq3O5773NB/fN6hHD2MwkxmYlMXVAChP7JhMeps1LSh0rWiNQQVFSVceGvDLW5ZayLreU9bml7HZqDMmxEcwals6AtDhKquo5UFVH3x6x3HjKYGIj9d5FqSMRtOGjIjIb+AcQBvzLGHNfi9e/D/wVyHU2PWSM+Vdbn6mBoPsqqarjs21FfLw5n8XfFlJcWUdCdDhJMRHkHKimX0osf7l0HMcNTGFtTimvrMimoLyWW08fyug+ScEuvlKdWlACgYiEAVuAM4AcYDlwlTFmo9c+3wemGGN+6u/naiAIDW63wW3MwSaiL3cU8evX1rKnuIr+qbHsdmZEx0SGUV7TwA+OH8DPzxhGXJTWGJTyJVh9BNOAbcaYHU4hXgIuADa2+S6lAJdLcNE0P2H6oFTev/Uk/vbRFjbklXH9yYM5d3xvjBvue38z//p0JwvW5HHhxEzOG9en1fkNDY1u7X9QqoVA1gguBWYbY37kPP8ucJz33b9TI/gTUIitPfzcGJPt47PmAnMB+vXrN3n37t0BKbPqulbsKuaRxdtZtrWQ+kZDr8Ro0hIiiY0MJzLMRWF5LfvKaiitrichOpxeidH0To7huIEpnDGqJ0Mz4skvq+WddXtZtLmAif2S+cms5n0SZTX1RIa5iI7QZUBV1xOspqHLgLNaBIJpxpifee2TClQYY2pF5AbgcmPMqW19rjYNqbaUVNXxwYZ9fL69iIqaBirrGqhtcJMWH0UvJ6leSVU9e0ur2V1UxeZ95QBkJERRWFGLMTAgNZZdRVX0TormzrNHEhnm4rWVOSz+toC4qHCumd6Pa48fQEZCdJDPVin/BSsQzAB+Z4w5y3l+J4Ax5k+t7B8GFBtj2uz100CgOtK+0ho+3pzP59uLGJaRwLnjezM4PZ4Vu4r57YINbMgrAyA9IYqLJmaSXVzF+xv2EeFyceqIDKYNTGHawBQyEqPIL61lb2k1keEupg9K1ZqD6lSCFQjCsc09p2FHBS0HrjbGbPDap7cxZq/z+CLgdmPM9LY+VwOBOlYa3Yb31+8jNiqMk4akHexb2LW/kn9/tpOFmwrILan2+d64yDBmDc9g5rB0BqXH0S81ltS4KCpqGiitrqe2oZE+yTHaua2OmWAOHz0b+Dt2+OjTxpg/iMjvgRXGmAUi8ifgfKABKAZ+YozZ3PonaiBQnUteSTXLdxVTWl1v+x2SYiiqrOWDDfl8tDGf/RW1bb4/LT6SQenxXDwxkwsmZBITaWsRxhj2V9SRGBNOVLjWLNTR0zTUSgVBo9uwu6iS3cU2/UZRZR1JMREkx0QQHibkllSzp6iKVXsOsCW/gsTocOaM6U1+eQ1rc0oprqxDBNLjo+iTHEN8VDiR4S4iw1z0T41lVJ9ERvZOpF9K7CHNUPWNbrKLq5yEgZXERYVx5qhe9ErSfo1QpYFAqU7MGMPyXQd47otdLNyUT/+UOMZlJTGydyLlNQ3kHKgir7Sa6rpG6hsN1fWN7Cmqoq7RffAzUuMi6ZkYjdsYCsprKa6s83msyf17MG1gCnGRYcREhpMaF8nI3okMSo8jopVhtSVVdURHhBEV7moz5bgxhvLaBg5U1pGZHKPDdDsZzTWkVCcmIgc7nf1V3+hmR2ElG/eWklNczd6yGvaV1hDmEib170FGQhSZyTEMyYhncEY8BWW1vL9+L++u28fjS7bjbnH/FxnuYlTvRE4amsbMYelk9ojhnbV7mbcql017bYd5uEtIjY/k/PF9+N6MAfRNiaW4so5XVmQzf1Uue4qrqK5vBKBfSizXzxzEJZOyaHQbPt5cwAcb9lFWXU9sZBhxkeH0SY5hVJ9ERjm1Gl30KHi0RqBUiDHG2JpFXSP55TVs2lvGhrwyVuwqZnV2SbMgMb5vMmeN7glARU0DOwor+WhTPsYYJvfvwZqcUuoa3EwbmMK4zCQyEqOIiQzntRXZrMkpJTUukopaO4Q3IyGKzB4xVNU2UlHbwL6yGhqdg8VGhjEkI56hGQmM7pPItIEpjOydSJgTHKrrGmk0hvgWnev1jW527q8kLT6KHk5yQ+WbNg0ppfxSWlXPp9v2s6e4ijNG9WRIxqFrVu8treb5L3fz4YZ8ZgxO5Zrp/RnWM6HZPsYYvthexPNf7SYtPopzx/VhSv8eze76a+ob2ZpfwYa8Ur7NL2drfgVb8sspKLcd7AlR4WT2iGFfWQ0lVfWEu4SThqZxwYRMhmTEs2BNHq+vymF/hW0Gi45wkdUjlvFZyUzqn8ykfj0YkhHfrMmrpr6RLfnl5JXUUFheQ1FlHWP6JDFzeHqrTWO1DY00ug0xEWEBCTQFZTW8s24vmckxTOiXHLD5KRoIlFJdhmck1pc7iiksr6FXkh2NVVpdz9tr8sgrrQFsU9WpIzI4Y1RPymsayCupZldRFd/sOUCR00cSGeZicEY8A9Nsfqpv95XT0LJdDNvHct74PvRPjaWm3u30w1SyaW852wsraHAbIsKEpJhI+iRHM6p3IqP7JBITGc63+8rYvK+cgrJaIsNdRIW7SIgOZ0BaHIPS4hiQZlfu65Mc06xTv6CshseW7OCFr3ZT29DU35OZHMMpI9KZPbo3xw1KISLMRaPbcKCqjnCXkBx7ZOt3aCBQSnULbrdhxe4D7Cis4PRRPUmLjzpkH2MMe4qr+GZPCZv2lbF5bzm7iirplxJr173ITKJvSiwZCVEkxkTw2bb9zFuVw8KNBc064HslRjsjsxJIiI6gtLqekqo6dhdVsXFvGSVV9YDtXxnWM54+STHUN7qpbXBTUlXPrqJKquoam5UtITr84HlU1zciIlw0MZPrTx5EWU093+wpYfmuYpZu2U91fSOJ0XakWHFlHW4DN50ymF+dNeKI/nYaCJRS6jCq6hqoqXcT44yQaqvz2hhDXmkNNfWN9E+J9TlCyhhDflktO/dXsre0mr2lNRSU1SAihLmEuMgwLp6UxYC0uEPeW13XyNKthSzaXICIkBYfSVp8FBP7JTMuK/mIzk8DgVJKhbi2AoEO9FVKqRCngUAppUKcBgKllApxGgiUUirEaSBQSqkQp4FAKaVCnAYCpZQKcRoIlFIqxHW5CWUiUgjsPsK3pwH7O7A4XUUonnconjOE5nmH4jlD+8+7vzEm3dcLXS4QHA0RWdHazLruLBTPOxTPGULzvEPxnKFjz1ubhpRSKsRpIFBKqRAXaoHgiWAXIEhC8bxD8ZwhNM87FM8ZOvC8Q6qPQCml1KFCrUaglFKqBQ0ESikV4kImEIjIbBH5VkS2icgdwS5PIIhIXxFZJCKbRGSDiNzibE8RkY9EZKvzu0ewyxoIIhImIt+IyNvO84Ei8pVz3i+LyJEt9tpJiUiyiLwmIpud73xGKHzXIvJz59/3ehF5UUSiu+N3LSJPi0iBiKz32ubz+xXrQef6tlZEJrXnWCERCEQkDHgYmAOMAq4SkVHBLVVANAC/NMaMBKYDNznneQfwsTFmKPCx87w7ugXY5PX8z8DfnPM+AFwXlFIFzj+A940xI4Dx2HPv1t+1iGQCNwNTjDFjgDDgSrrnd/0MMLvFtta+3znAUOdnLvBoew4UEoEAmAZsM8bsMMbUAS8BFwS5TB3OGLPXGLPKeVyOvTBkYs/1WWe3Z4ELg1PCwBGRLOAc4F/OcwFOBV5zdulW5y0iicDJwFMAxpg6Y0wJIfBdA+FAjIiEA7HAXrrhd22MWQoUt9jc2vd7AfCcsb4EkkWkt7/HCpVAkAlkez3PcbZ1WyIyAJgIfAX0NMbsBRssgIzglSxg/g78GnA7z1OBEmNMg/O8u33ng4BC4N9Oc9i/RCSObv5dG2NygfuBPdgAUAqspHt/195a+36P6hoXKoFAfGzrtuNmRSQemAfcaowpC3Z5Ak1EzgUKjDErvTf72LU7fefhwCTgUWPMRKCSbtYM5IvTJn4BMBDoA8Rhm0Va6k7ftT+O6t97qASCHKCv1/MsIC9IZQkoEYnABoEXjDGvO5vzPdVE53dBsMoXICcA54vILmyz36nYGkKy03wA3e87zwFyjDFfOc9fwwaG7v5dnw7sNMYUGmPqgdeB4+ne37W31r7fo7rGhUogWA4MdUYWRGI7lxYEuUwdzmkXfwrYZIx5wOulBcC1zuNrgTePddkCyRhzpzEmD8IvjQAAApZJREFUyxgzAPvdfmKM+Q6wCLjU2a1bnbcxZh+QLSLDnU2nARvp5t81tklouojEOv/ePefdbb/rFlr7fhcA33NGD00HSj1NSH4xxoTED3A2sAXYDvwm2OUJ0DmeiK0OrgVWOz9nY9vLPwa2Or9Tgl3WAP4NZgFvO48HAV8D24BXgahgl6+Dz3UCsML5vt8AeoTCdw3cC2wG1gP/AaK643cNvIjtB6nH3vFf19r3i20aeti5vq3Djqry+1iaYkIppUJcqDQNKaWUaoUGAqWUCnEaCJRSKsRpIFBKqRCngUAppUKcBgKljiERmeXJjqpUZ6GBQCmlQpwGAqV8EJFrRORrEVktIo87ax1UiMj/icgqEflYRNKdfSeIyJdOHvj5Xjnih4jIQhFZ47xnsPPx8V7rCLzgzJBVKmg0ECjVgoiMBK4ATjDGTAAage9gE5ytMsZMApYAv3Xe8hxwuzFmHHZWp2f7C8DDxpjx2Hw4nin/E4FbsWtjDMLmSlIqaMIPv4tSIec0YDKw3LlZj8Em93IDLzv7PA+8LiJJQLIxZomz/VngVRFJADKNMfMBjDE1AM7nfW2MyXGerwYGAJ8G/rSU8k0DgVKHEuBZY8ydzTaK3N1iv7bys7TV3FPr9bgR/X+ogkybhpQ61MfApSKSAQfXie2P/f/iyXB5NfCpMaYUOCAiJznbvwssMXYdiBwRudD5jCgRiT2mZ6GUn/RORKkWjDEbReT/AR+KiAub/fEm7OIvo0VkJXZlrCuct1wLPOZc6HcAP3C2fxd4XER+73zGZcfwNJTym2YfVcpPIlJhjIkPdjmU6mjaNKSUUiFOawRKKRXitEaglFIhTgOBUkqFOA0ESikV4jQQKKVUiNNAoJRSIe7/A5TV2+hWBMlmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot history: MAE\n",
    "plt.plot(history.history['loss'], label='MAE (testing data)')\n",
    "plt.plot(history.history['val_loss'], label='MAE (validation data)')\n",
    "plt.title('MAE')\n",
    "plt.ylabel('MAE value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = np.arange(0, len(history.history['loss']))\n",
    "\n",
    "# You can chose the style of your preference\n",
    "# print(plt.style.available) to see the available options\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "# Plot train loss, train acc, val loss and val acc against epochs passed\n",
    "plt.figure()\n",
    "plt.plot(N, history.history['loss'], label = \"train_loss\")\n",
    "plt.plot(N, history.history['accuracy'], label = \"train_acc\")\n",
    "plt.plot(N, history.history['val_loss'], label = \"val_loss\")\n",
    "plt.plot(N, history.history['val_accuracy'], label = \"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "# Make sure there exists a folder called output in the current directory\n",
    "# or replace 'output' with whatever direcory you want to put in the plots\n",
    "plt.show()\n",
    "plt.savefig('../Output/EpochDenseNet121.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
