{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\HH TRADERS\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.applications.nasnet import NASNetMobile\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42373</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_flow6.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42374</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_flow7.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42375</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_flow7.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42376</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_flow8.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42377</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_flow8.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         image class\n",
       "42373  winKen_wave_u_cm_np1_ri_bad_1_flow6.jpg  wave\n",
       "42374  winKen_wave_u_cm_np1_ri_bad_1_flow7.jpg  wave\n",
       "42375  winKen_wave_u_cm_np1_ri_bad_1_flow7.jpg  wave\n",
       "42376  winKen_wave_u_cm_np1_ri_bad_1_flow8.jpg  wave\n",
       "42377  winKen_wave_u_cm_np1_ri_bad_1_flow8.jpg  wave"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train_OF.csv')\n",
    "train.sort_values(by=['class', 'image'])\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42378/42378 [05:03<00:00, 139.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating an empty list\n",
    "train_image = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    # loading the image and keeping the target size as (224,224,3)\n",
    "    img = image.load_img('../data/train_frame_OF/'+train['image'][i], target_size=(224,224,3))\n",
    "    # converting it to array\n",
    "    img = image.img_to_array(img)\n",
    "    # normalizing the pixel value\n",
    "    img = img/255\n",
    "    # appending the image to the train_image list\n",
    "    train_image.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42378, 224, 224, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the list to numpy array\n",
    "X_train = np.array(train_image,np.float16)\n",
    "\n",
    "# shape of the array\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8798</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8799</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8800</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8801</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8802</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  image class\n",
       "8798  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "8799  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "8800  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "8801  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "8802  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = pd.read_csv('../data/val_OF.csv')\n",
    "val.sort_values(by=['class', 'image'])\n",
    "val.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8803/8803 [01:22<00:00, 107.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating an empty list\n",
    "val_image = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(val.shape[0])):\n",
    "    # loading the image and keeping the target size as (224,224,3)\n",
    "    img = image.load_img('../data/val_frame_OF/'+val['image'][i], target_size=(224,224,3))\n",
    "    # converting it to array\n",
    "    img = image.img_to_array(img)\n",
    "    # normalizing the pixel value\n",
    "    img = img/255\n",
    "    # appending the image to the train_image list\n",
    "    val_image.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8803, 224, 224, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the list to numpy array\n",
    "X_test = np.array(val_image,np.float16)\n",
    "\n",
    "# shape of the array\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image    21189\n",
      "class       51\n",
      "dtype: int64\n",
      "image    8803\n",
      "class      51\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# separating the target\n",
    "y_train = train['class']\n",
    "y_test = val['class']\n",
    "print(train.nunique())\n",
    "print(val.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42378, 51)\n",
      "(8803, 51)\n"
     ]
    }
   ],
   "source": [
    "# creating dummies of target variable for train and validation set\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = NASNetMobile(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 7, 7, 1056)\n"
     ]
    }
   ],
   "source": [
    "print(base_model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"NASNet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "stem_conv1 (Conv2D)             (None, 111, 111, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "stem_bn1 (BatchNormalization)   (None, 111, 111, 32) 128         stem_conv1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 111, 111, 32) 0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_stem_1 (Conv2D (None, 111, 111, 11) 352         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_stem_1 (BatchNor (None, 111, 111, 11) 44          reduction_conv_1_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 111, 111, 11) 0           reduction_bn_1_stem_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 111, 111, 32) 0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 115, 115, 11) 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 117, 117, 32) 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 56, 56, 11)   396         separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 56, 56, 11)   1920        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 56, 56, 11)   44          separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 56, 56, 11)   44          separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 56, 56, 11)   0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 56, 56, 11)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 56, 56, 11)   396         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 56, 56, 11)   660         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 111, 111, 32) 0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 56, 56, 11)   44          separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 56, 56, 11)   44          separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 117, 117, 32) 0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 111, 111, 32) 0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_stem_1 (Add)    (None, 56, 56, 11)   0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 56, 56, 11)   1920        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 115, 115, 32) 0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 56, 56, 11)   0           reduction_add_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 56, 56, 11)   44          separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 56, 56, 11)   1152        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 56, 56, 11)   220         activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 56, 56, 11)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 56, 56, 11)   44          separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 56, 56, 11)   44          separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_stem_1 (ZeroPad (None, 113, 113, 11) 0           reduction_bn_1_stem_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 56, 56, 11)   660         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 56, 56, 11)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 56, 56, 11)   0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_stem_1 (MaxPool (None, 56, 56, 11)   0           reduction_pad_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 56, 56, 11)   44          separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "separable_conv_2_reduction_righ (None, 56, 56, 11)   396         activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 56, 56, 11)   220         activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_stem_2 (Activatio (None, 111, 111, 32) 0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_stem_1 (Add)    (None, 56, 56, 11)   0           reduction_left2_stem_1[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_stem_1 (Average (None, 56, 56, 11)   0           reduction_pad_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 56, 56, 11)   44          separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_stem_1 (Average (None, 56, 56, 11)   0           reduction_add_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 56, 56, 11)   44          separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_stem_1 (MaxPoo (None, 56, 56, 11)   0           reduction_pad_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 112, 112, 32) 0           adjust_relu_1_stem_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_stem_1 (Add)     (None, 56, 56, 11)   0           reduction_left3_stem_1[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 56, 56, 11)   0           reduction_add_2_stem_1[0][0]     \n",
      "                                                                 reduction_left4_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_stem_1 (Add)     (None, 56, 56, 11)   0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)       (None, 111, 111, 32) 0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_stem_1 (Concat (None, 56, 56, 44)   0           reduction_add_2_stem_1[0][0]     \n",
      "                                                                 reduction_add3_stem_1[0][0]      \n",
      "                                                                 add_1[0][0]                      \n",
      "                                                                 reduction_add4_stem_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_stem_2 (Avera (None, 56, 56, 32)   0           adjust_relu_1_stem_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_stem_2 (Avera (None, 56, 56, 32)   0           cropping2d_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 56, 56, 44)   0           reduction_concat_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_1_stem_2 (Conv2D)   (None, 56, 56, 11)   352         adjust_avg_pool_1_stem_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_stem_2 (Conv2D)   (None, 56, 56, 11)   352         adjust_avg_pool_2_stem_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_stem_2 (Conv2D (None, 56, 56, 22)   968         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 56, 56, 22)   0           adjust_conv_1_stem_2[0][0]       \n",
      "                                                                 adjust_conv_2_stem_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_stem_2 (BatchNor (None, 56, 56, 22)   88          reduction_conv_1_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_stem_2 (BatchNormaliz (None, 56, 56, 22)   88          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 56, 56, 22)   0           reduction_bn_1_stem_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 56, 56, 22)   0           adjust_bn_stem_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 59, 59, 22)   0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 61, 61, 22)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 28, 28, 22)   1034        separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 28, 28, 22)   1562        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 28, 28, 22)   88          separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 28, 28, 22)   88          separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 28, 28, 22)   0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 28, 28, 22)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 28, 28, 22)   1034        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 28, 28, 22)   1562        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 56, 56, 22)   0           adjust_bn_stem_2[0][0]           \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "separable_conv_2_bn_reduction_l (None, 28, 28, 22)   88          separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 28, 28, 22)   88          separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 61, 61, 22)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 56, 56, 22)   0           adjust_bn_stem_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_stem_2 (Add)    (None, 28, 28, 22)   0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 28, 28, 22)   1562        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 59, 59, 22)   0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 28, 28, 22)   0           reduction_add_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 28, 28, 22)   88          separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 28, 28, 22)   1034        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 28, 28, 22)   682         activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 28, 28, 22)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 28, 28, 22)   88          separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 28, 28, 22)   88          separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_stem_2 (ZeroPad (None, 57, 57, 22)   0           reduction_bn_1_stem_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 28, 28, 22)   1562        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 28, 28, 22)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 28, 28, 22)   0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_stem_2 (MaxPool (None, 28, 28, 22)   0           reduction_pad_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 28, 28, 22)   88          separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 28, 28, 22)   1034        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 28, 28, 22)   682         activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_0 (Activation)    (None, 56, 56, 44)   0           reduction_concat_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_stem_2 (Add)    (None, 28, 28, 22)   0           reduction_left2_stem_2[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_stem_2 (Average (None, 28, 28, 22)   0           reduction_pad_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 28, 28, 22)   88          separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_stem_2 (Average (None, 28, 28, 22)   0           reduction_add_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 28, 28, 22)   88          separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_stem_2 (MaxPoo (None, 28, 28, 22)   0           reduction_pad_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 57, 57, 44)   0           adjust_relu_1_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_stem_2 (Add)     (None, 28, 28, 22)   0           reduction_left3_stem_2[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 28, 28, 22)   0           reduction_add_2_stem_2[0][0]     \n",
      "                                                                 reduction_left4_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_stem_2 (Add)     (None, 28, 28, 22)   0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_2 (Cropping2D)       (None, 56, 56, 44)   0           zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_stem_2 (Concat (None, 28, 28, 88)   0           reduction_add_2_stem_2[0][0]     \n",
      "                                                                 reduction_add3_stem_2[0][0]      \n",
      "                                                                 add_2[0][0]                      \n",
      "                                                                 reduction_add4_stem_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_0 (AveragePoo (None, 28, 28, 44)   0           adjust_relu_1_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_0 (AveragePoo (None, 28, 28, 44)   0           cropping2d_2[0][0]               \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjust_conv_1_0 (Conv2D)        (None, 28, 28, 22)   968         adjust_avg_pool_1_0[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_0 (Conv2D)        (None, 28, 28, 22)   968         adjust_avg_pool_2_0[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 28, 28, 88)   0           reduction_concat_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 28, 28, 44)   0           adjust_conv_1_0[0][0]            \n",
      "                                                                 adjust_conv_2_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_0 (Conv2D)        (None, 28, 28, 44)   3872        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_0 (BatchNormalization (None, 28, 28, 44)   176         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_0 (BatchNormalizati (None, 28, 28, 44)   176         normal_conv_1_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 28, 28, 44)   0           normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 28, 28, 44)   0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 28, 28, 44)   0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 28, 28, 44)   0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 28, 28, 44)   0           normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_0 (None, 28, 28, 44)   3036        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 28, 28, 44)   2332        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_0 (None, 28, 28, 44)   3036        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 28, 28, 44)   2332        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_0 (None, 28, 28, 44)   2332        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 28, 28, 44)   176         separable_conv_1_normal_left1_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_1_normal_right1_0[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 28, 28, 44)   176         separable_conv_1_normal_left2_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_1_normal_right2_0[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 28, 28, 44)   176         separable_conv_1_normal_left5_0[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_0 (None, 28, 28, 44)   3036        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 28, 28, 44)   2332        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_0 (None, 28, 28, 44)   3036        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 28, 28, 44)   2332        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_0 (None, 28, 28, 44)   2332        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 28, 28, 44)   176         separable_conv_2_normal_left1_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_2_normal_right1_0[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 28, 28, 44)   176         separable_conv_2_normal_left2_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_2_normal_right2_0[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_0 (AveragePooling2 (None, 28, 28, 44)   0           normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_0 (AveragePooling2 (None, 28, 28, 44)   0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_0 (AveragePooling (None, 28, 28, 44)   0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 28, 28, 44)   176         separable_conv_2_normal_left5_0[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_0 (Add)            (None, 28, 28, 44)   0           separable_conv_2_bn_normal_left1_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_0 (Add)            (None, 28, 28, 44)   0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_0 (Add)            (None, 28, 28, 44)   0           normal_left3_0[0][0]             \n",
      "                                                                 adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_0 (Add)            (None, 28, 28, 44)   0           normal_left4_0[0][0]             \n",
      "                                                                 normal_right4_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_0 (Add)            (None, 28, 28, 44)   0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_0 (Concatenate)   (None, 28, 28, 264)  0           adjust_bn_0[0][0]                \n",
      "                                                                 normal_add_1_0[0][0]             \n",
      "                                                                 normal_add_2_0[0][0]             \n",
      "                                                                 normal_add_3_0[0][0]             \n",
      "                                                                 normal_add_4_0[0][0]             \n",
      "                                                                 normal_add_5_0[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 28, 28, 88)   0           reduction_concat_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 28, 28, 264)  0           normal_concat_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_1 (Conv2 (None, 28, 28, 44)   3872        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_1 (Conv2D)        (None, 28, 28, 44)   11616       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_1 (BatchNormalization (None, 28, 28, 44)   176         adjust_conv_projection_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_1 (BatchNormalizati (None, 28, 28, 44)   176         normal_conv_1_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 28, 28, 44)   0           normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 28, 28, 44)   0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 28, 28, 44)   0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 28, 28, 44)   0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 28, 28, 44)   0           normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 28, 28, 44)   3036        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 28, 28, 44)   2332        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 28, 28, 44)   3036        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 28, 28, 44)   2332        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 28, 28, 44)   2332        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 28, 28, 44)   176         separable_conv_1_normal_left1_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_1_normal_right1_1[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 28, 28, 44)   176         separable_conv_1_normal_left2_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_1_normal_right2_1[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 28, 28, 44)   176         separable_conv_1_normal_left5_1[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 28, 28, 44)   3036        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 28, 28, 44)   2332        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 28, 28, 44)   3036        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 28, 28, 44)   2332        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 28, 28, 44)   2332        activation_45[0][0]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 28, 28, 44)   176         separable_conv_2_normal_left1_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_2_normal_right1_1[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 28, 28, 44)   176         separable_conv_2_normal_left2_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_2_normal_right2_1[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_1 (AveragePooling2 (None, 28, 28, 44)   0           normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_1 (AveragePooling2 (None, 28, 28, 44)   0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_1 (AveragePooling (None, 28, 28, 44)   0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 28, 28, 44)   176         separable_conv_2_normal_left5_1[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_1 (Add)            (None, 28, 28, 44)   0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_1 (Add)            (None, 28, 28, 44)   0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_1 (Add)            (None, 28, 28, 44)   0           normal_left3_1[0][0]             \n",
      "                                                                 adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_1 (Add)            (None, 28, 28, 44)   0           normal_left4_1[0][0]             \n",
      "                                                                 normal_right4_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_1 (Add)            (None, 28, 28, 44)   0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_1 (Concatenate)   (None, 28, 28, 264)  0           adjust_bn_1[0][0]                \n",
      "                                                                 normal_add_1_1[0][0]             \n",
      "                                                                 normal_add_2_1[0][0]             \n",
      "                                                                 normal_add_3_1[0][0]             \n",
      "                                                                 normal_add_4_1[0][0]             \n",
      "                                                                 normal_add_5_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 28, 28, 264)  0           normal_concat_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 28, 28, 264)  0           normal_concat_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_2 (Conv2 (None, 28, 28, 44)   11616       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_2 (Conv2D)        (None, 28, 28, 44)   11616       activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_2 (BatchNormalization (None, 28, 28, 44)   176         adjust_conv_projection_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_2 (BatchNormalizati (None, 28, 28, 44)   176         normal_conv_1_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 28, 28, 44)   0           normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 28, 28, 44)   0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 28, 28, 44)   0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 28, 28, 44)   0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 28, 28, 44)   0           normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_2 (None, 28, 28, 44)   3036        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 28, 28, 44)   2332        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_2 (None, 28, 28, 44)   3036        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 28, 28, 44)   2332        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_2 (None, 28, 28, 44)   2332        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 28, 28, 44)   176         separable_conv_1_normal_left1_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_1_normal_right1_2[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 28, 28, 44)   176         separable_conv_1_normal_left2_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_1_normal_right2_2[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 28, 28, 44)   176         separable_conv_1_normal_left5_2[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_left1_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_2 (None, 28, 28, 44)   3036        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 28, 28, 44)   2332        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_2 (None, 28, 28, 44)   3036        activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 28, 28, 44)   2332        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_2 (None, 28, 28, 44)   2332        activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 28, 28, 44)   176         separable_conv_2_normal_left1_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_2_normal_right1_2[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 28, 28, 44)   176         separable_conv_2_normal_left2_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_2_normal_right2_2[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_2 (AveragePooling2 (None, 28, 28, 44)   0           normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_2 (AveragePooling2 (None, 28, 28, 44)   0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_2 (AveragePooling (None, 28, 28, 44)   0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 28, 28, 44)   176         separable_conv_2_normal_left5_2[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_2 (Add)            (None, 28, 28, 44)   0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_2 (Add)            (None, 28, 28, 44)   0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_2 (Add)            (None, 28, 28, 44)   0           normal_left3_2[0][0]             \n",
      "                                                                 adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_2 (Add)            (None, 28, 28, 44)   0           normal_left4_2[0][0]             \n",
      "                                                                 normal_right4_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_2 (Add)            (None, 28, 28, 44)   0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_2 (Concatenate)   (None, 28, 28, 264)  0           adjust_bn_2[0][0]                \n",
      "                                                                 normal_add_1_2[0][0]             \n",
      "                                                                 normal_add_2_2[0][0]             \n",
      "                                                                 normal_add_3_2[0][0]             \n",
      "                                                                 normal_add_4_2[0][0]             \n",
      "                                                                 normal_add_5_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 28, 28, 264)  0           normal_concat_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 28, 28, 264)  0           normal_concat_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_3 (Conv2 (None, 28, 28, 44)   11616       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_3 (Conv2D)        (None, 28, 28, 44)   11616       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_3 (BatchNormalization (None, 28, 28, 44)   176         adjust_conv_projection_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_3 (BatchNormalizati (None, 28, 28, 44)   176         normal_conv_1_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 28, 28, 44)   0           normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 28, 28, 44)   0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 28, 28, 44)   0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 28, 28, 44)   0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 28, 28, 44)   0           normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_3 (None, 28, 28, 44)   3036        activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 28, 28, 44)   2332        activation_62[0][0]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_3 (None, 28, 28, 44)   3036        activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 28, 28, 44)   2332        activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_3 (None, 28, 28, 44)   2332        activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 28, 28, 44)   176         separable_conv_1_normal_left1_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_1_normal_right1_3[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 28, 28, 44)   176         separable_conv_1_normal_left2_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_1_normal_right2_3[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 28, 28, 44)   176         separable_conv_1_normal_left5_3[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 28, 28, 44)   0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_3 (None, 28, 28, 44)   3036        activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 28, 28, 44)   2332        activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_3 (None, 28, 28, 44)   3036        activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 28, 28, 44)   2332        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_3 (None, 28, 28, 44)   2332        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 28, 28, 44)   176         separable_conv_2_normal_left1_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_2_normal_right1_3[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 28, 28, 44)   176         separable_conv_2_normal_left2_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 28, 28, 44)   176         separable_conv_2_normal_right2_3[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_3 (AveragePooling2 (None, 28, 28, 44)   0           normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_3 (AveragePooling2 (None, 28, 28, 44)   0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_3 (AveragePooling (None, 28, 28, 44)   0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 28, 28, 44)   176         separable_conv_2_normal_left5_3[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_3 (Add)            (None, 28, 28, 44)   0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_3 (Add)            (None, 28, 28, 44)   0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_3 (Add)            (None, 28, 28, 44)   0           normal_left3_3[0][0]             \n",
      "                                                                 adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_3 (Add)            (None, 28, 28, 44)   0           normal_left4_3[0][0]             \n",
      "                                                                 normal_right4_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_3 (Add)            (None, 28, 28, 44)   0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_3 (Concatenate)   (None, 28, 28, 264)  0           adjust_bn_3[0][0]                \n",
      "                                                                 normal_add_1_3[0][0]             \n",
      "                                                                 normal_add_2_3[0][0]             \n",
      "                                                                 normal_add_3_3[0][0]             \n",
      "                                                                 normal_add_4_3[0][0]             \n",
      "                                                                 normal_add_5_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 28, 28, 264)  0           normal_concat_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 28, 28, 264)  0           normal_concat_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_reduce_4 (Conv (None, 28, 28, 88)   23232       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_reduce_4 (None, 28, 28, 88)   23232       activation_70[0][0]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_reduce_4 (BatchN (None, 28, 28, 88)   352         reduction_conv_1_reduce_4[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_reduce_4 (BatchNormal (None, 28, 28, 88)   352         adjust_conv_projection_reduce_4[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 28, 28, 88)   0           reduction_bn_1_reduce_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 28, 28, 88)   0           adjust_bn_reduce_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 31, 31, 88)   0           activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 33, 33, 88)   0           activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 14, 14, 88)   9944        separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 14, 14, 88)   12056       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 14, 14, 88)   352         separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 14, 14, 88)   352         separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 14, 14, 88)   0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 14, 14, 88)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 14, 14, 88)   9944        activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 14, 14, 88)   12056       activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 28, 28, 88)   0           adjust_bn_reduce_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 14, 14, 88)   352         separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 14, 14, 88)   352         separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 33, 33, 88)   0           activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 28, 28, 88)   0           adjust_bn_reduce_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_reduce_4 (Add)  (None, 14, 14, 88)   0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 14, 14, 88)   12056       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 31, 31, 88)   0           activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 14, 14, 88)   0           reduction_add_1_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 14, 14, 88)   352         separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 14, 14, 88)   9944        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 14, 14, 88)   8536        activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 14, 14, 88)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 14, 14, 88)   352         separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 14, 14, 88)   352         separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_reduce_4 (ZeroP (None, 29, 29, 88)   0           reduction_bn_1_reduce_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 14, 14, 88)   12056       activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 14, 14, 88)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 14, 14, 88)   0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_reduce_4 (MaxPo (None, 14, 14, 88)   0           reduction_pad_1_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 14, 14, 88)   352         separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 14, 14, 88)   9944        activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 14, 14, 88)   8536        activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_5 (Activation)    (None, 28, 28, 264)  0           normal_concat_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_reduce_4 (Add)  (None, 14, 14, 88)   0           reduction_left2_reduce_4[0][0]   \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_reduce_4 (Avera (None, 14, 14, 88)   0           reduction_pad_1_reduce_4[0][0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 14, 14, 88)   352         separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_reduce_4 (Avera (None, 14, 14, 88)   0           reduction_add_1_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 14, 14, 88)   352         separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_reduce_4 (MaxP (None, 14, 14, 88)   0           reduction_pad_1_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 29, 29, 264)  0           adjust_relu_1_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_reduce_4 (Add)   (None, 14, 14, 88)   0           reduction_left3_reduce_4[0][0]   \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 14, 14, 88)   0           reduction_add_2_reduce_4[0][0]   \n",
      "                                                                 reduction_left4_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_reduce_4 (Add)   (None, 14, 14, 88)   0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_reduce_4[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_3 (Cropping2D)       (None, 28, 28, 264)  0           zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_reduce_4 (Conc (None, 14, 14, 352)  0           reduction_add_2_reduce_4[0][0]   \n",
      "                                                                 reduction_add3_reduce_4[0][0]    \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 reduction_add4_reduce_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_5 (AveragePoo (None, 14, 14, 264)  0           adjust_relu_1_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_5 (AveragePoo (None, 14, 14, 264)  0           cropping2d_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_1_5 (Conv2D)        (None, 14, 14, 44)   11616       adjust_avg_pool_1_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_5 (Conv2D)        (None, 14, 14, 44)   11616       adjust_avg_pool_2_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 14, 14, 352)  0           reduction_concat_reduce_4[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 14, 14, 88)   0           adjust_conv_1_5[0][0]            \n",
      "                                                                 adjust_conv_2_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_5 (Conv2D)        (None, 14, 14, 88)   30976       activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_5 (BatchNormalization (None, 14, 14, 88)   352         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_5 (BatchNormalizati (None, 14, 14, 88)   352         normal_conv_1_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 14, 14, 88)   0           normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 14, 14, 88)   0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 14, 14, 88)   0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 14, 14, 88)   0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 14, 14, 88)   0           normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_5 (None, 14, 14, 88)   9944        activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 14, 14, 88)   8536        activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_5 (None, 14, 14, 88)   9944        activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 14, 14, 88)   8536        activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_5 (None, 14, 14, 88)   8536        activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 14, 14, 88)   352         separable_conv_1_normal_left1_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_1_normal_right1_5[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 14, 14, 88)   352         separable_conv_1_normal_left2_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_1_normal_right2_5[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 14, 14, 88)   352         separable_conv_1_normal_left5_5[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 14, 14, 88)   0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 14, 14, 88)   0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 14, 14, 88)   0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_90 (Activation)      (None, 14, 14, 88)   0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 14, 14, 88)   0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_5 (None, 14, 14, 88)   9944        activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 14, 14, 88)   8536        activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_5 (None, 14, 14, 88)   9944        activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 14, 14, 88)   8536        activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_5 (None, 14, 14, 88)   8536        activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 14, 14, 88)   352         separable_conv_2_normal_left1_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_2_normal_right1_5[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 14, 14, 88)   352         separable_conv_2_normal_left2_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_2_normal_right2_5[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_5 (AveragePooling2 (None, 14, 14, 88)   0           normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_5 (AveragePooling2 (None, 14, 14, 88)   0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_5 (AveragePooling (None, 14, 14, 88)   0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 14, 14, 88)   352         separable_conv_2_normal_left5_5[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_5 (Add)            (None, 14, 14, 88)   0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_5 (Add)            (None, 14, 14, 88)   0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_5 (Add)            (None, 14, 14, 88)   0           normal_left3_5[0][0]             \n",
      "                                                                 adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_5 (Add)            (None, 14, 14, 88)   0           normal_left4_5[0][0]             \n",
      "                                                                 normal_right4_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_5 (Add)            (None, 14, 14, 88)   0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_5 (Concatenate)   (None, 14, 14, 528)  0           adjust_bn_5[0][0]                \n",
      "                                                                 normal_add_1_5[0][0]             \n",
      "                                                                 normal_add_2_5[0][0]             \n",
      "                                                                 normal_add_3_5[0][0]             \n",
      "                                                                 normal_add_4_5[0][0]             \n",
      "                                                                 normal_add_5_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 14, 14, 352)  0           reduction_concat_reduce_4[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 14, 14, 528)  0           normal_concat_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_6 (Conv2 (None, 14, 14, 88)   30976       activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_6 (Conv2D)        (None, 14, 14, 88)   46464       activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_6 (BatchNormalization (None, 14, 14, 88)   352         adjust_conv_projection_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_6 (BatchNormalizati (None, 14, 14, 88)   352         normal_conv_1_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 14, 14, 88)   0           normal_bn_1_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 14, 14, 88)   0           adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 14, 14, 88)   0           adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 14, 14, 88)   0           adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 14, 14, 88)   0           normal_bn_1_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_6 (None, 14, 14, 88)   9944        activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 14, 14, 88)   8536        activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_6 (None, 14, 14, 88)   9944        activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 14, 14, 88)   8536        activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "separable_conv_1_normal_left5_6 (None, 14, 14, 88)   8536        activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 14, 14, 88)   352         separable_conv_1_normal_left1_6[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_1_normal_right1_6[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 14, 14, 88)   352         separable_conv_1_normal_left2_6[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_1_normal_right2_6[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 14, 14, 88)   352         separable_conv_1_normal_left5_6[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 14, 14, 88)   0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 14, 14, 88)   0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 14, 14, 88)   0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 14, 14, 88)   0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 14, 14, 88)   0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_6 (None, 14, 14, 88)   9944        activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 14, 14, 88)   8536        activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_6 (None, 14, 14, 88)   9944        activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 14, 14, 88)   8536        activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_6 (None, 14, 14, 88)   8536        activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 14, 14, 88)   352         separable_conv_2_normal_left1_6[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_2_normal_right1_6[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 14, 14, 88)   352         separable_conv_2_normal_left2_6[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_2_normal_right2_6[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_6 (AveragePooling2 (None, 14, 14, 88)   0           normal_bn_1_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_6 (AveragePooling2 (None, 14, 14, 88)   0           adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_6 (AveragePooling (None, 14, 14, 88)   0           adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 14, 14, 88)   352         separable_conv_2_normal_left5_6[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_6 (Add)            (None, 14, 14, 88)   0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_6 (Add)            (None, 14, 14, 88)   0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_6 (Add)            (None, 14, 14, 88)   0           normal_left3_6[0][0]             \n",
      "                                                                 adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_6 (Add)            (None, 14, 14, 88)   0           normal_left4_6[0][0]             \n",
      "                                                                 normal_right4_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_6 (Add)            (None, 14, 14, 88)   0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_6 (Concatenate)   (None, 14, 14, 528)  0           adjust_bn_6[0][0]                \n",
      "                                                                 normal_add_1_6[0][0]             \n",
      "                                                                 normal_add_2_6[0][0]             \n",
      "                                                                 normal_add_3_6[0][0]             \n",
      "                                                                 normal_add_4_6[0][0]             \n",
      "                                                                 normal_add_5_6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 14, 14, 528)  0           normal_concat_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 14, 14, 528)  0           normal_concat_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_7 (Conv2 (None, 14, 14, 88)   46464       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_7 (Conv2D)        (None, 14, 14, 88)   46464       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_7 (BatchNormalization (None, 14, 14, 88)   352         adjust_conv_projection_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_7 (BatchNormalizati (None, 14, 14, 88)   352         normal_conv_1_7[0][0]            \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_107 (Activation)     (None, 14, 14, 88)   0           normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 14, 14, 88)   0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 14, 14, 88)   0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 14, 14, 88)   0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 14, 14, 88)   0           normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_7 (None, 14, 14, 88)   9944        activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 14, 14, 88)   8536        activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_7 (None, 14, 14, 88)   9944        activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 14, 14, 88)   8536        activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_7 (None, 14, 14, 88)   8536        activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 14, 14, 88)   352         separable_conv_1_normal_left1_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_1_normal_right1_7[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 14, 14, 88)   352         separable_conv_1_normal_left2_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_1_normal_right2_7[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 14, 14, 88)   352         separable_conv_1_normal_left5_7[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 14, 14, 88)   0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 14, 14, 88)   0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 14, 14, 88)   0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 14, 14, 88)   0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 14, 14, 88)   0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_7 (None, 14, 14, 88)   9944        activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 14, 14, 88)   8536        activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_7 (None, 14, 14, 88)   9944        activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 14, 14, 88)   8536        activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_7 (None, 14, 14, 88)   8536        activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 14, 14, 88)   352         separable_conv_2_normal_left1_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_2_normal_right1_7[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 14, 14, 88)   352         separable_conv_2_normal_left2_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_2_normal_right2_7[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_7 (AveragePooling2 (None, 14, 14, 88)   0           normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_7 (AveragePooling2 (None, 14, 14, 88)   0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_7 (AveragePooling (None, 14, 14, 88)   0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 14, 14, 88)   352         separable_conv_2_normal_left5_7[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_7 (Add)            (None, 14, 14, 88)   0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_7 (Add)            (None, 14, 14, 88)   0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_7 (Add)            (None, 14, 14, 88)   0           normal_left3_7[0][0]             \n",
      "                                                                 adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_7 (Add)            (None, 14, 14, 88)   0           normal_left4_7[0][0]             \n",
      "                                                                 normal_right4_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_7 (Add)            (None, 14, 14, 88)   0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_7 (Concatenate)   (None, 14, 14, 528)  0           adjust_bn_7[0][0]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 normal_add_1_7[0][0]             \n",
      "                                                                 normal_add_2_7[0][0]             \n",
      "                                                                 normal_add_3_7[0][0]             \n",
      "                                                                 normal_add_4_7[0][0]             \n",
      "                                                                 normal_add_5_7[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 14, 14, 528)  0           normal_concat_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 14, 14, 528)  0           normal_concat_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_8 (Conv2 (None, 14, 14, 88)   46464       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_8 (Conv2D)        (None, 14, 14, 88)   46464       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_8 (BatchNormalization (None, 14, 14, 88)   352         adjust_conv_projection_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_8 (BatchNormalizati (None, 14, 14, 88)   352         normal_conv_1_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 14, 14, 88)   0           normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 14, 14, 88)   0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 14, 14, 88)   0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 14, 14, 88)   0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 14, 14, 88)   0           normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_8 (None, 14, 14, 88)   9944        activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 14, 14, 88)   8536        activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_8 (None, 14, 14, 88)   9944        activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 14, 14, 88)   8536        activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_8 (None, 14, 14, 88)   8536        activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 14, 14, 88)   352         separable_conv_1_normal_left1_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_1_normal_right1_8[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 14, 14, 88)   352         separable_conv_1_normal_left2_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_1_normal_right2_8[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 14, 14, 88)   352         separable_conv_1_normal_left5_8[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 14, 14, 88)   0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 14, 14, 88)   0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 14, 14, 88)   0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 14, 14, 88)   0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 14, 14, 88)   0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_8 (None, 14, 14, 88)   9944        activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 14, 14, 88)   8536        activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_8 (None, 14, 14, 88)   9944        activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 14, 14, 88)   8536        activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_8 (None, 14, 14, 88)   8536        activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 14, 14, 88)   352         separable_conv_2_normal_left1_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_2_normal_right1_8[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 14, 14, 88)   352         separable_conv_2_normal_left2_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 14, 14, 88)   352         separable_conv_2_normal_right2_8[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_8 (AveragePooling2 (None, 14, 14, 88)   0           normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_8 (AveragePooling2 (None, 14, 14, 88)   0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_8 (AveragePooling (None, 14, 14, 88)   0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "separable_conv_2_bn_normal_left (None, 14, 14, 88)   352         separable_conv_2_normal_left5_8[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_8 (Add)            (None, 14, 14, 88)   0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_8 (Add)            (None, 14, 14, 88)   0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_8 (Add)            (None, 14, 14, 88)   0           normal_left3_8[0][0]             \n",
      "                                                                 adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_8 (Add)            (None, 14, 14, 88)   0           normal_left4_8[0][0]             \n",
      "                                                                 normal_right4_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_8 (Add)            (None, 14, 14, 88)   0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_8 (Concatenate)   (None, 14, 14, 528)  0           adjust_bn_8[0][0]                \n",
      "                                                                 normal_add_1_8[0][0]             \n",
      "                                                                 normal_add_2_8[0][0]             \n",
      "                                                                 normal_add_3_8[0][0]             \n",
      "                                                                 normal_add_4_8[0][0]             \n",
      "                                                                 normal_add_5_8[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 14, 14, 528)  0           normal_concat_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 14, 14, 528)  0           normal_concat_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_reduce_8 (Conv (None, 14, 14, 176)  92928       activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_reduce_8 (None, 14, 14, 176)  92928       activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_reduce_8 (BatchN (None, 14, 14, 176)  704         reduction_conv_1_reduce_8[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_reduce_8 (BatchNormal (None, 14, 14, 176)  704         adjust_conv_projection_reduce_8[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 14, 14, 176)  0           reduction_bn_1_reduce_8[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 14, 14, 176)  0           adjust_bn_reduce_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 17, 17, 176)  0           activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 19, 19, 176)  0           activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 7, 7, 176)    35376       separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 7, 7, 176)    39600       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 7, 7, 176)    704         separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 7, 7, 176)    704         separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 7, 7, 176)    35376       activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 7, 7, 176)    39600       activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 14, 14, 176)  0           adjust_bn_reduce_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 7, 7, 176)    704         separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 7, 7, 176)    704         separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 19, 19, 176)  0           activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 14, 14, 176)  0           adjust_bn_reduce_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_reduce_8 (Add)  (None, 7, 7, 176)    0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 7, 7, 176)    39600       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 17, 17, 176)  0           activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 7, 7, 176)    0           reduction_add_1_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 7, 7, 176)    704         separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 7, 7, 176)    35376       separable_conv_1_pad_reduction_ri\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 7, 7, 176)    32560       activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 7, 7, 176)    704         separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 7, 7, 176)    704         separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_reduce_8 (ZeroP (None, 15, 15, 176)  0           reduction_bn_1_reduce_8[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 7, 7, 176)    39600       activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_reduce_8 (MaxPo (None, 7, 7, 176)    0           reduction_pad_1_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 7, 7, 176)    704         separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 7, 7, 176)    35376       activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 7, 7, 176)    32560       activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_9 (Activation)    (None, 14, 14, 528)  0           normal_concat_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_reduce_8 (Add)  (None, 7, 7, 176)    0           reduction_left2_reduce_8[0][0]   \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_reduce_8 (Avera (None, 7, 7, 176)    0           reduction_pad_1_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 7, 7, 176)    704         separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_reduce_8 (Avera (None, 7, 7, 176)    0           reduction_add_1_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 7, 7, 176)    704         separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_reduce_8 (MaxP (None, 7, 7, 176)    0           reduction_pad_1_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 15, 15, 528)  0           adjust_relu_1_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_reduce_8 (Add)   (None, 7, 7, 176)    0           reduction_left3_reduce_8[0][0]   \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 7, 7, 176)    0           reduction_add_2_reduce_8[0][0]   \n",
      "                                                                 reduction_left4_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_reduce_8 (Add)   (None, 7, 7, 176)    0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_reduce_8[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_4 (Cropping2D)       (None, 14, 14, 528)  0           zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_reduce_8 (Conc (None, 7, 7, 704)    0           reduction_add_2_reduce_8[0][0]   \n",
      "                                                                 reduction_add3_reduce_8[0][0]    \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 reduction_add4_reduce_8[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_9 (AveragePoo (None, 7, 7, 528)    0           adjust_relu_1_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_9 (AveragePoo (None, 7, 7, 528)    0           cropping2d_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_1_9 (Conv2D)        (None, 7, 7, 88)     46464       adjust_avg_pool_1_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_9 (Conv2D)        (None, 7, 7, 88)     46464       adjust_avg_pool_2_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 7, 7, 704)    0           reduction_concat_reduce_8[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 7, 7, 176)    0           adjust_conv_1_9[0][0]            \n",
      "                                                                 adjust_conv_2_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_9 (Conv2D)        (None, 7, 7, 176)    123904      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_9 (BatchNormalization (None, 7, 7, 176)    704         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_9 (BatchNormalizati (None, 7, 7, 176)    704         normal_conv_1_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 7, 7, 176)    0           normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 7, 7, 176)    0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 7, 7, 176)    0           adjust_bn_9[0][0]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 7, 7, 176)    0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 7, 7, 176)    0           normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_9 (None, 7, 7, 176)    35376       activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 7, 7, 176)    32560       activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_9 (None, 7, 7, 176)    35376       activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 7, 7, 176)    32560       activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_9 (None, 7, 7, 176)    32560       activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 7, 7, 176)    704         separable_conv_1_normal_left1_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_1_normal_right1_9[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 7, 7, 176)    704         separable_conv_1_normal_left2_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_1_normal_right2_9[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 7, 7, 176)    704         separable_conv_1_normal_left5_9[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_9 (None, 7, 7, 176)    35376       activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 7, 7, 176)    32560       activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_9 (None, 7, 7, 176)    35376       activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 7, 7, 176)    32560       activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_9 (None, 7, 7, 176)    32560       activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 7, 7, 176)    704         separable_conv_2_normal_left1_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_2_normal_right1_9[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 7, 7, 176)    704         separable_conv_2_normal_left2_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_2_normal_right2_9[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_9 (AveragePooling2 (None, 7, 7, 176)    0           normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_9 (AveragePooling2 (None, 7, 7, 176)    0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_9 (AveragePooling (None, 7, 7, 176)    0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 7, 7, 176)    704         separable_conv_2_normal_left5_9[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_9 (Add)            (None, 7, 7, 176)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_9 (Add)            (None, 7, 7, 176)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_9 (Add)            (None, 7, 7, 176)    0           normal_left3_9[0][0]             \n",
      "                                                                 adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_9 (Add)            (None, 7, 7, 176)    0           normal_left4_9[0][0]             \n",
      "                                                                 normal_right4_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_9 (Add)            (None, 7, 7, 176)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_9 (Concatenate)   (None, 7, 7, 1056)   0           adjust_bn_9[0][0]                \n",
      "                                                                 normal_add_1_9[0][0]             \n",
      "                                                                 normal_add_2_9[0][0]             \n",
      "                                                                 normal_add_3_9[0][0]             \n",
      "                                                                 normal_add_4_9[0][0]             \n",
      "                                                                 normal_add_5_9[0][0]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 7, 7, 704)    0           reduction_concat_reduce_8[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 7, 7, 1056)   0           normal_concat_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_10 (Conv (None, 7, 7, 176)    123904      activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_10 (Conv2D)       (None, 7, 7, 176)    185856      activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_10 (BatchNormalizatio (None, 7, 7, 176)    704         adjust_conv_projection_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_10 (BatchNormalizat (None, 7, 7, 176)    704         normal_conv_1_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 7, 7, 176)    0           normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 7, 7, 176)    0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 7, 7, 176)    0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 7, 7, 176)    0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 7, 7, 176)    0           normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 7, 7, 176)    35376       activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 7, 7, 176)    32560       activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 7, 7, 176)    35376       activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 7, 7, 176)    32560       activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 7, 7, 176)    32560       activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 7, 7, 176)    704         separable_conv_1_normal_left1_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_1_normal_right1_10\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 7, 7, 176)    704         separable_conv_1_normal_left2_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_1_normal_right2_10\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 7, 7, 176)    704         separable_conv_1_normal_left5_10[\n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 7, 7, 176)    35376       activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 7, 7, 176)    32560       activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 7, 7, 176)    35376       activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 7, 7, 176)    32560       activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 7, 7, 176)    32560       activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 7, 7, 176)    704         separable_conv_2_normal_left1_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_2_normal_right1_10\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 7, 7, 176)    704         separable_conv_2_normal_left2_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_2_normal_right2_10\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_10 (AveragePooling (None, 7, 7, 176)    0           normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_10 (AveragePooling (None, 7, 7, 176)    0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_10 (AveragePoolin (None, 7, 7, 176)    0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 7, 7, 176)    704         separable_conv_2_normal_left5_10[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_10 (Add)           (None, 7, 7, 176)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_add_2_10 (Add)           (None, 7, 7, 176)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_10 (Add)           (None, 7, 7, 176)    0           normal_left3_10[0][0]            \n",
      "                                                                 adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_10 (Add)           (None, 7, 7, 176)    0           normal_left4_10[0][0]            \n",
      "                                                                 normal_right4_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_10 (Add)           (None, 7, 7, 176)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_10 (Concatenate)  (None, 7, 7, 1056)   0           adjust_bn_10[0][0]               \n",
      "                                                                 normal_add_1_10[0][0]            \n",
      "                                                                 normal_add_2_10[0][0]            \n",
      "                                                                 normal_add_3_10[0][0]            \n",
      "                                                                 normal_add_4_10[0][0]            \n",
      "                                                                 normal_add_5_10[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 7, 7, 1056)   0           normal_concat_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 7, 7, 1056)   0           normal_concat_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_11 (Conv (None, 7, 7, 176)    185856      activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_11 (Conv2D)       (None, 7, 7, 176)    185856      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_11 (BatchNormalizatio (None, 7, 7, 176)    704         adjust_conv_projection_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_11 (BatchNormalizat (None, 7, 7, 176)    704         normal_conv_1_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 7, 7, 176)    0           normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 7, 7, 176)    0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 7, 7, 176)    0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 7, 7, 176)    0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 7, 7, 176)    0           normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 7, 7, 176)    35376       activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 7, 7, 176)    32560       activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 7, 7, 176)    35376       activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 7, 7, 176)    32560       activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 7, 7, 176)    32560       activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 7, 7, 176)    704         separable_conv_1_normal_left1_11[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_1_normal_right1_11\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 7, 7, 176)    704         separable_conv_1_normal_left2_11[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_1_normal_right2_11\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 7, 7, 176)    704         separable_conv_1_normal_left5_11[\n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 7, 7, 176)    35376       activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 7, 7, 176)    32560       activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 7, 7, 176)    35376       activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 7, 7, 176)    32560       activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 7, 7, 176)    32560       activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 7, 7, 176)    704         separable_conv_2_normal_left1_11[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_2_normal_right1_11\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 7, 7, 176)    704         separable_conv_2_normal_left2_11[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_2_normal_right2_11\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_11 (AveragePooling (None, 7, 7, 176)    0           normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_11 (AveragePooling (None, 7, 7, 176)    0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_11 (AveragePoolin (None, 7, 7, 176)    0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 7, 7, 176)    704         separable_conv_2_normal_left5_11[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_11 (Add)           (None, 7, 7, 176)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_11 (Add)           (None, 7, 7, 176)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_11 (Add)           (None, 7, 7, 176)    0           normal_left3_11[0][0]            \n",
      "                                                                 adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_11 (Add)           (None, 7, 7, 176)    0           normal_left4_11[0][0]            \n",
      "                                                                 normal_right4_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_11 (Add)           (None, 7, 7, 176)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_11 (Concatenate)  (None, 7, 7, 1056)   0           adjust_bn_11[0][0]               \n",
      "                                                                 normal_add_1_11[0][0]            \n",
      "                                                                 normal_add_2_11[0][0]            \n",
      "                                                                 normal_add_3_11[0][0]            \n",
      "                                                                 normal_add_4_11[0][0]            \n",
      "                                                                 normal_add_5_11[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 7, 7, 1056)   0           normal_concat_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 7, 7, 1056)   0           normal_concat_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_12 (Conv (None, 7, 7, 176)    185856      activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_12 (Conv2D)       (None, 7, 7, 176)    185856      activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_12 (BatchNormalizatio (None, 7, 7, 176)    704         adjust_conv_projection_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_12 (BatchNormalizat (None, 7, 7, 176)    704         normal_conv_1_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 7, 7, 176)    0           normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 7, 7, 176)    0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 7, 7, 176)    0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 7, 7, 176)    0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 7, 7, 176)    0           normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 7, 7, 176)    35376       activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 7, 7, 176)    32560       activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 7, 7, 176)    35376       activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 7, 7, 176)    32560       activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 7, 7, 176)    32560       activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 7, 7, 176)    704         separable_conv_1_normal_left1_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_1_normal_right1_12\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 7, 7, 176)    704         separable_conv_1_normal_left2_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_1_normal_right2_12\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 7, 7, 176)    704         separable_conv_1_normal_left5_12[\n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_right1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 7, 7, 176)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 7, 7, 176)    35376       activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 7, 7, 176)    32560       activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 7, 7, 176)    35376       activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 7, 7, 176)    32560       activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 7, 7, 176)    32560       activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 7, 7, 176)    704         separable_conv_2_normal_left1_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_2_normal_right1_12\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 7, 7, 176)    704         separable_conv_2_normal_left2_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 7, 7, 176)    704         separable_conv_2_normal_right2_12\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_12 (AveragePooling (None, 7, 7, 176)    0           normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_12 (AveragePooling (None, 7, 7, 176)    0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_12 (AveragePoolin (None, 7, 7, 176)    0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 7, 7, 176)    704         separable_conv_2_normal_left5_12[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_12 (Add)           (None, 7, 7, 176)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_12 (Add)           (None, 7, 7, 176)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_12 (Add)           (None, 7, 7, 176)    0           normal_left3_12[0][0]            \n",
      "                                                                 adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_12 (Add)           (None, 7, 7, 176)    0           normal_left4_12[0][0]            \n",
      "                                                                 normal_right4_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_12 (Add)           (None, 7, 7, 176)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_12 (Concatenate)  (None, 7, 7, 1056)   0           adjust_bn_12[0][0]               \n",
      "                                                                 normal_add_1_12[0][0]            \n",
      "                                                                 normal_add_2_12[0][0]            \n",
      "                                                                 normal_add_3_12[0][0]            \n",
      "                                                                 normal_add_4_12[0][0]            \n",
      "                                                                 normal_add_5_12[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 7, 7, 1056)   0           normal_concat_12[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 4,269,716\n",
      "Trainable params: 4,232,978\n",
      "Non-trainable params: 36,738\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'NASNet',\n",
       " 'layers': [{'name': 'input_1',\n",
       "   'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 224, 224, 3),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'name': 'input_1'},\n",
       "   'inbound_nodes': []},\n",
       "  {'name': 'stem_conv1',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'stem_conv1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['input_1', 0, 0, {}]]]},\n",
       "  {'name': 'stem_bn1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'stem_bn1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['stem_conv1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_1',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['stem_bn1', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_conv_1_stem_1',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'reduction_conv_1_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 11,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_1', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_bn_1_stem_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'reduction_bn_1_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['reduction_conv_1_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_2',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_bn_1_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_4',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['stem_bn1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_left1_stem_1',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_left1_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((2, 2), (2, 2)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_right1_stem_1',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_right1_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((3, 3), (3, 3)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_4', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_left1_stem_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_left1_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 11,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_left1_stem_1',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_right1_stem_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_right1_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 11,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_right1_stem_1',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_left1_stem_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_left1_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_left1_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_right1_stem_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_right1_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_right1_stem_1',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'activation_3',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_left1_stem_1',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'activation_5',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_right1_stem_1',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_left1_stem_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_left1_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 11,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_3', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_right1_stem_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_right1_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 11,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_6',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['stem_bn1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_left1_stem_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_left1_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_left1_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_right1_stem_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_right1_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_right1_stem_1',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_right2_stem_1',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_right2_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((3, 3), (3, 3)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_6', 0, 0, {}]]]},\n",
       "  {'name': 'activation_8',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['stem_bn1', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add_1_stem_1',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add_1_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_reduction_left1_stem_1', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_reduction_right1_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_right2_stem_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_right2_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 11,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_right2_stem_1',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_right3_stem_1',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_right3_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((2, 2), (2, 2)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_10',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_add_1_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_right2_stem_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_right2_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_right2_stem_1',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_right3_stem_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_right3_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 11,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_right3_stem_1',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_left4_stem_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_left4_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 11,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_10', 0, 0, {}]]]},\n",
       "  {'name': 'activation_7',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_right2_stem_1',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_right3_stem_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_right3_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_right3_stem_1',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_left4_stem_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_left4_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_left4_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_pad_1_stem_1',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'reduction_pad_1_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((1, 1), (1, 1)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_bn_1_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_right2_stem_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_right2_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 11,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_7', 0, 0, {}]]]},\n",
       "  {'name': 'activation_9',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_right3_stem_1',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'activation_11',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_left4_stem_1',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'reduction_left2_stem_1',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'reduction_left2_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_pad_1_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_right2_stem_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_right2_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_right2_stem_1',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_right3_stem_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_right3_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 11,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_9', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_left4_stem_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_left4_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 11,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_11', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_relu_1_stem_2',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'adjust_relu_1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['stem_bn1', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add_2_stem_1',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add_2_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['reduction_left2_stem_1', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_reduction_right2_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_left3_stem_1',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'reduction_left3_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_pad_1_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_right3_stem_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_right3_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_right3_stem_1',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'reduction_left4_stem_1',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'reduction_left4_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_add_1_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_left4_stem_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_left4_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_left4_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_right5_stem_1',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'reduction_right5_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_pad_1_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'zero_padding2d_1',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'zero_padding2d_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((0, 1), (0, 1)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_relu_1_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add3_stem_1',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add3_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['reduction_left3_stem_1', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_reduction_right3_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'add_1',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'add_1', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['reduction_add_2_stem_1', 0, 0, {}],\n",
       "     ['reduction_left4_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add4_stem_1',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add4_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_reduction_left4_stem_1', 0, 0, {}],\n",
       "     ['reduction_right5_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'cropping2d_1',\n",
       "   'class_name': 'Cropping2D',\n",
       "   'config': {'name': 'cropping2d_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'cropping': ((1, 0), (1, 0)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['zero_padding2d_1', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_concat_stem_1',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'reduction_concat_stem_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['reduction_add_2_stem_1', 0, 0, {}],\n",
       "     ['reduction_add3_stem_1', 0, 0, {}],\n",
       "     ['add_1', 0, 0, {}],\n",
       "     ['reduction_add4_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_avg_pool_1_stem_2',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'adjust_avg_pool_1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_relu_1_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_avg_pool_2_stem_2',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'adjust_avg_pool_2_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['cropping2d_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_12',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_concat_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_1_stem_2',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 11,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_avg_pool_1_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_2_stem_2',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_2_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 11,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_avg_pool_2_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_conv_1_stem_2',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'reduction_conv_1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 22,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_12', 0, 0, {}]]]},\n",
       "  {'name': 'concatenate_1',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'concatenate_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_conv_1_stem_2', 0, 0, {}],\n",
       "     ['adjust_conv_2_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_bn_1_stem_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'reduction_bn_1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['reduction_conv_1_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_bn_stem_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'adjust_bn_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['concatenate_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_13',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_13',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_bn_1_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'activation_15',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_15',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_left1_stem_2',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_left1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((1, 2), (1, 2)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_13', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_right1_stem_2',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_right1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((2, 3), (2, 3)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_15', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_left1_stem_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_left1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 22,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_left1_stem_2',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_right1_stem_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_right1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 22,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_right1_stem_2',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_left1_stem_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_left1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_left1_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_right1_stem_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_right1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_right1_stem_2',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'activation_14',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_14',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_left1_stem_2',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'activation_16',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_16',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_right1_stem_2',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_left1_stem_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_left1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 22,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_14', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_right1_stem_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_right1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 22,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_16', 0, 0, {}]]]},\n",
       "  {'name': 'activation_17',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_17',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_left1_stem_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_left1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_left1_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_right1_stem_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_right1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_right1_stem_2',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_right2_stem_2',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_right2_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((2, 3), (2, 3)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_17', 0, 0, {}]]]},\n",
       "  {'name': 'activation_19',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_19',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add_1_stem_2',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add_1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_reduction_left1_stem_2', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_reduction_right1_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_right2_stem_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_right2_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 22,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_right2_stem_2',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_right3_stem_2',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_right3_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((1, 2), (1, 2)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_19', 0, 0, {}]]]},\n",
       "  {'name': 'activation_21',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_21',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_add_1_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_right2_stem_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_right2_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_right2_stem_2',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_right3_stem_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_right3_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 22,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_right3_stem_2',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_left4_stem_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_left4_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 22,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_21', 0, 0, {}]]]},\n",
       "  {'name': 'activation_18',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_18',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_right2_stem_2',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_right3_stem_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_right3_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_right3_stem_2',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_left4_stem_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_left4_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_left4_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_pad_1_stem_2',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'reduction_pad_1_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((0, 1), (0, 1)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_bn_1_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_right2_stem_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_right2_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 22,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_18', 0, 0, {}]]]},\n",
       "  {'name': 'activation_20',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_20',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_right3_stem_2',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'activation_22',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_22',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_left4_stem_2',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'reduction_left2_stem_2',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'reduction_left2_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_pad_1_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_right2_stem_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_right2_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_right2_stem_2',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_right3_stem_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_right3_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 22,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_20', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_left4_stem_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_left4_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 22,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_22', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_relu_1_0',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'adjust_relu_1_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_concat_stem_1', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add_2_stem_2',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add_2_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['reduction_left2_stem_2', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_reduction_right2_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_left3_stem_2',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'reduction_left3_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_pad_1_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_right3_stem_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_right3_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_right3_stem_2',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'reduction_left4_stem_2',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'reduction_left4_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_add_1_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_left4_stem_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_left4_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_left4_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_right5_stem_2',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'reduction_right5_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_pad_1_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'zero_padding2d_2',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'zero_padding2d_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((0, 1), (0, 1)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_relu_1_0', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add3_stem_2',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add3_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['reduction_left3_stem_2', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_reduction_right3_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'add_2',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'add_2', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['reduction_add_2_stem_2', 0, 0, {}],\n",
       "     ['reduction_left4_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add4_stem_2',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add4_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_reduction_left4_stem_2', 0, 0, {}],\n",
       "     ['reduction_right5_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'cropping2d_2',\n",
       "   'class_name': 'Cropping2D',\n",
       "   'config': {'name': 'cropping2d_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'cropping': ((1, 0), (1, 0)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['zero_padding2d_2', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_concat_stem_2',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'reduction_concat_stem_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['reduction_add_2_stem_2', 0, 0, {}],\n",
       "     ['reduction_add3_stem_2', 0, 0, {}],\n",
       "     ['add_2', 0, 0, {}],\n",
       "     ['reduction_add4_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_avg_pool_1_0',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'adjust_avg_pool_1_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_relu_1_0', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_avg_pool_2_0',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'adjust_avg_pool_2_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['cropping2d_2', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_1_0',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_1_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 22,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_avg_pool_1_0', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_2_0',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_2_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 22,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_avg_pool_2_0', 0, 0, {}]]]},\n",
       "  {'name': 'activation_23',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_23',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_concat_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'concatenate_2',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'concatenate_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_conv_1_0', 0, 0, {}],\n",
       "     ['adjust_conv_2_0', 0, 0, {}]]]},\n",
       "  {'name': 'normal_conv_1_0',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'normal_conv_1_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_23', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_bn_0',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'adjust_bn_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['concatenate_2', 0, 0, {}]]]},\n",
       "  {'name': 'normal_bn_1_0',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'normal_bn_1_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['normal_conv_1_0', 0, 0, {}]]]},\n",
       "  {'name': 'activation_24',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_24',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_0', 0, 0, {}]]]},\n",
       "  {'name': 'activation_26',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_26',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_0', 0, 0, {}]]]},\n",
       "  {'name': 'activation_28',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_28',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_0', 0, 0, {}]]]},\n",
       "  {'name': 'activation_30',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_30',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_0', 0, 0, {}]]]},\n",
       "  {'name': 'activation_32',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_32',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_0', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left1_0',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left1_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_24', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right1_0',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right1_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_26', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left2_0',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left2_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_28', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right2_0',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right2_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_30', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left5_0',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left5_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_32', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left1_0',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left1_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left1_0', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right1_0',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right1_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right1_0', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left2_0',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left2_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left2_0', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right2_0',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right2_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right2_0', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left5_0',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left5_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left5_0', 0, 0, {}]]]},\n",
       "  {'name': 'activation_25',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_25',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left1_0', 0, 0, {}]]]},\n",
       "  {'name': 'activation_27',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_27',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right1_0', 0, 0, {}]]]},\n",
       "  {'name': 'activation_29',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_29',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left2_0', 0, 0, {}]]]},\n",
       "  {'name': 'activation_31',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_31',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right2_0', 0, 0, {}]]]},\n",
       "  {'name': 'activation_33',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_33',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left5_0', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left1_0',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left1_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_25', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right1_0',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right1_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_27', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left2_0',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left2_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_29', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right2_0',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right2_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_31', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left5_0',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left5_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_33', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left1_0',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left1_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left1_0', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right1_0',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right1_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right1_0', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left2_0',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left2_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left2_0', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right2_0',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right2_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right2_0', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left3_0',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left3_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['normal_bn_1_0', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left4_0',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left4_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_0', 0, 0, {}]]]},\n",
       "  {'name': 'normal_right4_0',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_right4_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_0', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left5_0',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left5_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left5_0', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_1_0',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_1_0', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left1_0', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right1_0', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_2_0',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_2_0', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left2_0', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right2_0', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_3_0',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_3_0', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left3_0', 0, 0, {}],\n",
       "     ['adjust_bn_0', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_4_0',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_4_0', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left4_0', 0, 0, {}],\n",
       "     ['normal_right4_0', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_5_0',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_5_0', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left5_0', 0, 0, {}],\n",
       "     ['normal_bn_1_0', 0, 0, {}]]]},\n",
       "  {'name': 'normal_concat_0',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'normal_concat_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_bn_0', 0, 0, {}],\n",
       "     ['normal_add_1_0', 0, 0, {}],\n",
       "     ['normal_add_2_0', 0, 0, {}],\n",
       "     ['normal_add_3_0', 0, 0, {}],\n",
       "     ['normal_add_4_0', 0, 0, {}],\n",
       "     ['normal_add_5_0', 0, 0, {}]]]},\n",
       "  {'name': 'activation_34',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_34',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_concat_stem_2', 0, 0, {}]]]},\n",
       "  {'name': 'activation_35',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_35',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_0', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_projection_1',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_projection_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_34', 0, 0, {}]]]},\n",
       "  {'name': 'normal_conv_1_1',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'normal_conv_1_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_35', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_bn_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'adjust_bn_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_conv_projection_1', 0, 0, {}]]]},\n",
       "  {'name': 'normal_bn_1_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'normal_bn_1_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['normal_conv_1_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_36',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_36',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_38',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_38',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_40',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_40',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_42',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_42',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_44',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_44',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left1_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left1_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_36', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right1_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right1_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_38', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left2_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left2_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_40', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right2_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right2_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_42', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left5_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left5_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_44', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left1_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left1_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left1_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right1_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right1_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right1_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left2_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left2_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left2_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right2_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right2_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right2_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left5_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left5_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left5_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_37',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_37',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left1_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_39',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_39',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right1_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_41',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_41',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left2_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_43',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_43',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right2_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_45',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_45',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left5_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left1_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left1_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_37', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right1_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right1_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_39', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left2_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left2_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_41', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right2_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right2_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_43', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left5_1',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left5_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_45', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left1_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left1_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left1_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right1_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right1_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right1_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left2_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left2_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left2_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right2_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right2_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right2_1', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left3_1',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left3_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['normal_bn_1_1', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left4_1',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left4_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_1', 0, 0, {}]]]},\n",
       "  {'name': 'normal_right4_1',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_right4_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_1', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left5_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left5_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left5_1', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_1_1',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_1_1', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left1_1', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right1_1', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_2_1',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_2_1', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left2_1', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right2_1', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_3_1',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_3_1', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left3_1', 0, 0, {}],\n",
       "     ['adjust_bn_1', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_4_1',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_4_1', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left4_1', 0, 0, {}],\n",
       "     ['normal_right4_1', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_5_1',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_5_1', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left5_1', 0, 0, {}],\n",
       "     ['normal_bn_1_1', 0, 0, {}]]]},\n",
       "  {'name': 'normal_concat_1',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'normal_concat_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_bn_1', 0, 0, {}],\n",
       "     ['normal_add_1_1', 0, 0, {}],\n",
       "     ['normal_add_2_1', 0, 0, {}],\n",
       "     ['normal_add_3_1', 0, 0, {}],\n",
       "     ['normal_add_4_1', 0, 0, {}],\n",
       "     ['normal_add_5_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_46',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_46',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_0', 0, 0, {}]]]},\n",
       "  {'name': 'activation_47',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_47',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_1', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_projection_2',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_projection_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_46', 0, 0, {}]]]},\n",
       "  {'name': 'normal_conv_1_2',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'normal_conv_1_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_47', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_bn_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'adjust_bn_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_conv_projection_2', 0, 0, {}]]]},\n",
       "  {'name': 'normal_bn_1_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'normal_bn_1_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['normal_conv_1_2', 0, 0, {}]]]},\n",
       "  {'name': 'activation_48',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_48',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_2', 0, 0, {}]]]},\n",
       "  {'name': 'activation_50',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_50',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_2', 0, 0, {}]]]},\n",
       "  {'name': 'activation_52',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_52',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_2', 0, 0, {}]]]},\n",
       "  {'name': 'activation_54',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_54',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_2', 0, 0, {}]]]},\n",
       "  {'name': 'activation_56',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_56',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left1_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left1_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_48', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right1_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right1_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_50', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left2_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left2_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_52', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right2_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right2_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_54', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left5_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left5_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_56', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left1_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left1_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left1_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right1_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right1_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right1_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left2_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left2_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left2_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right2_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right2_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right2_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left5_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left5_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left5_2', 0, 0, {}]]]},\n",
       "  {'name': 'activation_49',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_49',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left1_2', 0, 0, {}]]]},\n",
       "  {'name': 'activation_51',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_51',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right1_2', 0, 0, {}]]]},\n",
       "  {'name': 'activation_53',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_53',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left2_2', 0, 0, {}]]]},\n",
       "  {'name': 'activation_55',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_55',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right2_2', 0, 0, {}]]]},\n",
       "  {'name': 'activation_57',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_57',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left5_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left1_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left1_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_49', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right1_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right1_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_51', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left2_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left2_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_53', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right2_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right2_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_55', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left5_2',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left5_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_57', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left1_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left1_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left1_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right1_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right1_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right1_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left2_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left2_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left2_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right2_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right2_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right2_2', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left3_2',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left3_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['normal_bn_1_2', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left4_2',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left4_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_2', 0, 0, {}]]]},\n",
       "  {'name': 'normal_right4_2',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_right4_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_2', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left5_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left5_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left5_2', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_1_2',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_1_2', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left1_2', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right1_2', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_2_2',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_2_2', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left2_2', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right2_2', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_3_2',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_3_2', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left3_2', 0, 0, {}],\n",
       "     ['adjust_bn_2', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_4_2',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_4_2', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left4_2', 0, 0, {}],\n",
       "     ['normal_right4_2', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_5_2',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_5_2', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left5_2', 0, 0, {}],\n",
       "     ['normal_bn_1_2', 0, 0, {}]]]},\n",
       "  {'name': 'normal_concat_2',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'normal_concat_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_bn_2', 0, 0, {}],\n",
       "     ['normal_add_1_2', 0, 0, {}],\n",
       "     ['normal_add_2_2', 0, 0, {}],\n",
       "     ['normal_add_3_2', 0, 0, {}],\n",
       "     ['normal_add_4_2', 0, 0, {}],\n",
       "     ['normal_add_5_2', 0, 0, {}]]]},\n",
       "  {'name': 'activation_58',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_58',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_59',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_59',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_2', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_projection_3',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_projection_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_58', 0, 0, {}]]]},\n",
       "  {'name': 'normal_conv_1_3',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'normal_conv_1_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_59', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_bn_3',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'adjust_bn_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_conv_projection_3', 0, 0, {}]]]},\n",
       "  {'name': 'normal_bn_1_3',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'normal_bn_1_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['normal_conv_1_3', 0, 0, {}]]]},\n",
       "  {'name': 'activation_60',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_60',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_3', 0, 0, {}]]]},\n",
       "  {'name': 'activation_62',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_62',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_3', 0, 0, {}]]]},\n",
       "  {'name': 'activation_64',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_64',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_3', 0, 0, {}]]]},\n",
       "  {'name': 'activation_66',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_66',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_3', 0, 0, {}]]]},\n",
       "  {'name': 'activation_68',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_68',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_3', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left1_3',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left1_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_60', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right1_3',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right1_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_62', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left2_3',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left2_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_64', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right2_3',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right2_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_66', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left5_3',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left5_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_68', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left1_3',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left1_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left1_3', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right1_3',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right1_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right1_3', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left2_3',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left2_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left2_3', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right2_3',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right2_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right2_3', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left5_3',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left5_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left5_3', 0, 0, {}]]]},\n",
       "  {'name': 'activation_61',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_61',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left1_3', 0, 0, {}]]]},\n",
       "  {'name': 'activation_63',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_63',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right1_3', 0, 0, {}]]]},\n",
       "  {'name': 'activation_65',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_65',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left2_3', 0, 0, {}]]]},\n",
       "  {'name': 'activation_67',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_67',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right2_3', 0, 0, {}]]]},\n",
       "  {'name': 'activation_69',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_69',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left5_3', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left1_3',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left1_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_61', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right1_3',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right1_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_63', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left2_3',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left2_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_65', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right2_3',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right2_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_67', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left5_3',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left5_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_69', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left1_3',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left1_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left1_3', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right1_3',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right1_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right1_3', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left2_3',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left2_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left2_3', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right2_3',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right2_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right2_3', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left3_3',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left3_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['normal_bn_1_3', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left4_3',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left4_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_3', 0, 0, {}]]]},\n",
       "  {'name': 'normal_right4_3',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_right4_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_3', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left5_3',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left5_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left5_3', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_1_3',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_1_3', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left1_3', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right1_3', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_2_3',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_2_3', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left2_3', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right2_3', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_3_3',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_3_3', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left3_3', 0, 0, {}],\n",
       "     ['adjust_bn_3', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_4_3',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_4_3', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left4_3', 0, 0, {}],\n",
       "     ['normal_right4_3', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_5_3',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_5_3', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left5_3', 0, 0, {}],\n",
       "     ['normal_bn_1_3', 0, 0, {}]]]},\n",
       "  {'name': 'normal_concat_3',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'normal_concat_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_bn_3', 0, 0, {}],\n",
       "     ['normal_add_1_3', 0, 0, {}],\n",
       "     ['normal_add_2_3', 0, 0, {}],\n",
       "     ['normal_add_3_3', 0, 0, {}],\n",
       "     ['normal_add_4_3', 0, 0, {}],\n",
       "     ['normal_add_5_3', 0, 0, {}]]]},\n",
       "  {'name': 'activation_71',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_71',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_3', 0, 0, {}]]]},\n",
       "  {'name': 'activation_70',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_70',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_2', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_conv_1_reduce_4',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'reduction_conv_1_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_71', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_projection_reduce_4',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_projection_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_70', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_bn_1_reduce_4',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'reduction_bn_1_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['reduction_conv_1_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_bn_reduce_4',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'adjust_bn_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_conv_projection_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'activation_72',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_72',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_bn_1_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'activation_74',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_74',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_left1_reduce_4',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_left1_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((1, 2), (1, 2)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_72', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_right1_reduce_4',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_right1_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((2, 3), (2, 3)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_74', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_left1_reduce_4',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_left1_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_left1_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_right1_reduce_4',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_right1_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_right1_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_left1_reduce_4',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_left1_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_left1_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_right1_reduce_4',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_right1_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_right1_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'activation_73',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_73',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_left1_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'activation_75',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_75',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_right1_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_left1_reduce_4',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_left1_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_73', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_right1_reduce_4',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_right1_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_75', 0, 0, {}]]]},\n",
       "  {'name': 'activation_76',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_76',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_left1_reduce_4',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_left1_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_left1_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_right1_reduce_4',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_right1_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_right1_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_right2_reduce_4',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_right2_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((2, 3), (2, 3)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_76', 0, 0, {}]]]},\n",
       "  {'name': 'activation_78',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_78',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add_1_reduce_4',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add_1_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_reduction_left1_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}],\n",
       "     ['separable_conv_2_bn_reduction_right1_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_right2_reduce_4',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_right2_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_right2_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_right3_reduce_4',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_right3_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((1, 2), (1, 2)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_78', 0, 0, {}]]]},\n",
       "  {'name': 'activation_80',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_80',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_add_1_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_right2_reduce_4',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_right2_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_right2_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_right3_reduce_4',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_right3_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_right3_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_left4_reduce_4',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_left4_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_80', 0, 0, {}]]]},\n",
       "  {'name': 'activation_77',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_77',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_right2_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_right3_reduce_4',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_right3_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_right3_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_left4_reduce_4',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_left4_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_left4_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'reduction_pad_1_reduce_4',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'reduction_pad_1_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((0, 1), (0, 1)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_bn_1_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_right2_reduce_4',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_right2_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_77', 0, 0, {}]]]},\n",
       "  {'name': 'activation_79',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_79',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_right3_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'activation_81',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_81',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_left4_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'reduction_left2_reduce_4',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'reduction_left2_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_pad_1_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_right2_reduce_4',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_right2_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_right2_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_right3_reduce_4',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_right3_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_79', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_left4_reduce_4',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_left4_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_81', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_relu_1_5',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'adjust_relu_1_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_3', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add_2_reduce_4',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add_2_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['reduction_left2_reduce_4', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_reduction_right2_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_left3_reduce_4',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'reduction_left3_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_pad_1_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_right3_reduce_4',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_right3_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_right3_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'reduction_left4_reduce_4',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'reduction_left4_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_add_1_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_left4_reduce_4',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_left4_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_left4_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'reduction_right5_reduce_4',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'reduction_right5_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_pad_1_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'zero_padding2d_3',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'zero_padding2d_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((0, 1), (0, 1)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_relu_1_5', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add3_reduce_4',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add3_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['reduction_left3_reduce_4', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_reduction_right3_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'add_3',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'add_3', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['reduction_add_2_reduce_4', 0, 0, {}],\n",
       "     ['reduction_left4_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add4_reduce_4',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add4_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_reduction_left4_reduce_4',\n",
       "      0,\n",
       "      0,\n",
       "      {}],\n",
       "     ['reduction_right5_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'cropping2d_3',\n",
       "   'class_name': 'Cropping2D',\n",
       "   'config': {'name': 'cropping2d_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'cropping': ((1, 0), (1, 0)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['zero_padding2d_3', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_concat_reduce_4',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'reduction_concat_reduce_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['reduction_add_2_reduce_4', 0, 0, {}],\n",
       "     ['reduction_add3_reduce_4', 0, 0, {}],\n",
       "     ['add_3', 0, 0, {}],\n",
       "     ['reduction_add4_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_avg_pool_1_5',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'adjust_avg_pool_1_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_relu_1_5', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_avg_pool_2_5',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'adjust_avg_pool_2_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['cropping2d_3', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_1_5',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_1_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_avg_pool_1_5', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_2_5',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_2_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 44,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_avg_pool_2_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_82',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_82',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_concat_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'concatenate_3',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'concatenate_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_conv_1_5', 0, 0, {}],\n",
       "     ['adjust_conv_2_5', 0, 0, {}]]]},\n",
       "  {'name': 'normal_conv_1_5',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'normal_conv_1_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_82', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_bn_5',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'adjust_bn_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['concatenate_3', 0, 0, {}]]]},\n",
       "  {'name': 'normal_bn_1_5',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'normal_bn_1_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['normal_conv_1_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_83',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_83',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_85',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_85',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_87',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_87',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_89',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_89',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_91',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_91',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_5', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left1_5',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left1_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_83', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right1_5',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right1_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_85', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left2_5',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left2_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_87', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right2_5',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right2_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_89', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left5_5',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left5_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_91', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left1_5',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left1_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left1_5', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right1_5',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right1_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right1_5', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left2_5',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left2_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left2_5', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right2_5',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right2_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right2_5', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left5_5',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left5_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left5_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_84',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_84',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left1_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_86',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_86',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right1_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_88',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_88',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left2_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_90',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_90',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right2_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_92',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_92',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left5_5', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left1_5',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left1_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_84', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right1_5',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right1_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_86', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left2_5',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left2_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_88', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right2_5',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right2_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_90', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left5_5',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left5_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_92', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left1_5',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left1_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left1_5', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right1_5',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right1_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right1_5', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left2_5',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left2_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left2_5', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right2_5',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right2_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right2_5', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left3_5',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left3_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['normal_bn_1_5', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left4_5',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left4_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_5', 0, 0, {}]]]},\n",
       "  {'name': 'normal_right4_5',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_right4_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_5', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left5_5',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left5_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left5_5', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_1_5',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_1_5', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left1_5', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right1_5', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_2_5',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_2_5', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left2_5', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right2_5', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_3_5',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_3_5', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left3_5', 0, 0, {}],\n",
       "     ['adjust_bn_5', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_4_5',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_4_5', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left4_5', 0, 0, {}],\n",
       "     ['normal_right4_5', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_5_5',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_5_5', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left5_5', 0, 0, {}],\n",
       "     ['normal_bn_1_5', 0, 0, {}]]]},\n",
       "  {'name': 'normal_concat_5',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'normal_concat_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_bn_5', 0, 0, {}],\n",
       "     ['normal_add_1_5', 0, 0, {}],\n",
       "     ['normal_add_2_5', 0, 0, {}],\n",
       "     ['normal_add_3_5', 0, 0, {}],\n",
       "     ['normal_add_4_5', 0, 0, {}],\n",
       "     ['normal_add_5_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_93',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_93',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_concat_reduce_4', 0, 0, {}]]]},\n",
       "  {'name': 'activation_94',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_94',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_5', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_projection_6',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_projection_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_93', 0, 0, {}]]]},\n",
       "  {'name': 'normal_conv_1_6',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'normal_conv_1_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_94', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_bn_6',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'adjust_bn_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_conv_projection_6', 0, 0, {}]]]},\n",
       "  {'name': 'normal_bn_1_6',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'normal_bn_1_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['normal_conv_1_6', 0, 0, {}]]]},\n",
       "  {'name': 'activation_95',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_95',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_6', 0, 0, {}]]]},\n",
       "  {'name': 'activation_97',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_97',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_6', 0, 0, {}]]]},\n",
       "  {'name': 'activation_99',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_99',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_6', 0, 0, {}]]]},\n",
       "  {'name': 'activation_101',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_101',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_6', 0, 0, {}]]]},\n",
       "  {'name': 'activation_103',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_103',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_6', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left1_6',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left1_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_95', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right1_6',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right1_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_97', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left2_6',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left2_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_99', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right2_6',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right2_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_101', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left5_6',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left5_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_103', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left1_6',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left1_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left1_6', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right1_6',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right1_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right1_6', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left2_6',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left2_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left2_6', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right2_6',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right2_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right2_6', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left5_6',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left5_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left5_6', 0, 0, {}]]]},\n",
       "  {'name': 'activation_96',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_96',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left1_6', 0, 0, {}]]]},\n",
       "  {'name': 'activation_98',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_98',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right1_6', 0, 0, {}]]]},\n",
       "  {'name': 'activation_100',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_100',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left2_6', 0, 0, {}]]]},\n",
       "  {'name': 'activation_102',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_102',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right2_6', 0, 0, {}]]]},\n",
       "  {'name': 'activation_104',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_104',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left5_6', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left1_6',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left1_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_96', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right1_6',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right1_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_98', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left2_6',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left2_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_100', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right2_6',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right2_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_102', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left5_6',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left5_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_104', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left1_6',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left1_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left1_6', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right1_6',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right1_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right1_6', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left2_6',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left2_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left2_6', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right2_6',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right2_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right2_6', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left3_6',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left3_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['normal_bn_1_6', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left4_6',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left4_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_6', 0, 0, {}]]]},\n",
       "  {'name': 'normal_right4_6',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_right4_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_6', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left5_6',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left5_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left5_6', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_1_6',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_1_6', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left1_6', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right1_6', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_2_6',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_2_6', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left2_6', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right2_6', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_3_6',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_3_6', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left3_6', 0, 0, {}],\n",
       "     ['adjust_bn_6', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_4_6',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_4_6', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left4_6', 0, 0, {}],\n",
       "     ['normal_right4_6', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_5_6',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_5_6', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left5_6', 0, 0, {}],\n",
       "     ['normal_bn_1_6', 0, 0, {}]]]},\n",
       "  {'name': 'normal_concat_6',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'normal_concat_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_bn_6', 0, 0, {}],\n",
       "     ['normal_add_1_6', 0, 0, {}],\n",
       "     ['normal_add_2_6', 0, 0, {}],\n",
       "     ['normal_add_3_6', 0, 0, {}],\n",
       "     ['normal_add_4_6', 0, 0, {}],\n",
       "     ['normal_add_5_6', 0, 0, {}]]]},\n",
       "  {'name': 'activation_105',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_105',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_106',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_106',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_6', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_projection_7',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_projection_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_105', 0, 0, {}]]]},\n",
       "  {'name': 'normal_conv_1_7',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'normal_conv_1_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_106', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_bn_7',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'adjust_bn_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_conv_projection_7', 0, 0, {}]]]},\n",
       "  {'name': 'normal_bn_1_7',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'normal_bn_1_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['normal_conv_1_7', 0, 0, {}]]]},\n",
       "  {'name': 'activation_107',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_107',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_7', 0, 0, {}]]]},\n",
       "  {'name': 'activation_109',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_109',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_7', 0, 0, {}]]]},\n",
       "  {'name': 'activation_111',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_111',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_7', 0, 0, {}]]]},\n",
       "  {'name': 'activation_113',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_113',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_7', 0, 0, {}]]]},\n",
       "  {'name': 'activation_115',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_115',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_7', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left1_7',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left1_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_107', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right1_7',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right1_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_109', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left2_7',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left2_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_111', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right2_7',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right2_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_113', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left5_7',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left5_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_115', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left1_7',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left1_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left1_7', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right1_7',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right1_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right1_7', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left2_7',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left2_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left2_7', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right2_7',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right2_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right2_7', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left5_7',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left5_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left5_7', 0, 0, {}]]]},\n",
       "  {'name': 'activation_108',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_108',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left1_7', 0, 0, {}]]]},\n",
       "  {'name': 'activation_110',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_110',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right1_7', 0, 0, {}]]]},\n",
       "  {'name': 'activation_112',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_112',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left2_7', 0, 0, {}]]]},\n",
       "  {'name': 'activation_114',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_114',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right2_7', 0, 0, {}]]]},\n",
       "  {'name': 'activation_116',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_116',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left5_7', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left1_7',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left1_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_108', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right1_7',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right1_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_110', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left2_7',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left2_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_112', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right2_7',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right2_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_114', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left5_7',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left5_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_116', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left1_7',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left1_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left1_7', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right1_7',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right1_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right1_7', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left2_7',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left2_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left2_7', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right2_7',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right2_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right2_7', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left3_7',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left3_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['normal_bn_1_7', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left4_7',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left4_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_7', 0, 0, {}]]]},\n",
       "  {'name': 'normal_right4_7',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_right4_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_7', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left5_7',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left5_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left5_7', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_1_7',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_1_7', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left1_7', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right1_7', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_2_7',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_2_7', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left2_7', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right2_7', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_3_7',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_3_7', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left3_7', 0, 0, {}],\n",
       "     ['adjust_bn_7', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_4_7',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_4_7', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left4_7', 0, 0, {}],\n",
       "     ['normal_right4_7', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_5_7',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_5_7', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left5_7', 0, 0, {}],\n",
       "     ['normal_bn_1_7', 0, 0, {}]]]},\n",
       "  {'name': 'normal_concat_7',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'normal_concat_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_bn_7', 0, 0, {}],\n",
       "     ['normal_add_1_7', 0, 0, {}],\n",
       "     ['normal_add_2_7', 0, 0, {}],\n",
       "     ['normal_add_3_7', 0, 0, {}],\n",
       "     ['normal_add_4_7', 0, 0, {}],\n",
       "     ['normal_add_5_7', 0, 0, {}]]]},\n",
       "  {'name': 'activation_117',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_117',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_6', 0, 0, {}]]]},\n",
       "  {'name': 'activation_118',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_118',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_7', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_projection_8',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_projection_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_117', 0, 0, {}]]]},\n",
       "  {'name': 'normal_conv_1_8',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'normal_conv_1_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_118', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_bn_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'adjust_bn_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_conv_projection_8', 0, 0, {}]]]},\n",
       "  {'name': 'normal_bn_1_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'normal_bn_1_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['normal_conv_1_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_119',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_119',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_121',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_121',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_123',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_123',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_125',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_125',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_127',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_127',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left1_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left1_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_119', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right1_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right1_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_121', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left2_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left2_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_123', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right2_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right2_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_125', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left5_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left5_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_127', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left1_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left1_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left1_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right1_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right1_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right1_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left2_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left2_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left2_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right2_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right2_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right2_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left5_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left5_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left5_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_120',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_120',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left1_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_122',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_122',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right1_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_124',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_124',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left2_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_126',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_126',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right2_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_128',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_128',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left5_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left1_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left1_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_120', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right1_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right1_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_122', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left2_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left2_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_124', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right2_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right2_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_126', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left5_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left5_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_128', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left1_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left1_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left1_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right1_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right1_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right1_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left2_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left2_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left2_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right2_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right2_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right2_8', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left3_8',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left3_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['normal_bn_1_8', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left4_8',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left4_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_8', 0, 0, {}]]]},\n",
       "  {'name': 'normal_right4_8',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_right4_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left5_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left5_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left5_8', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_1_8',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_1_8', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left1_8', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right1_8', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_2_8',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_2_8', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left2_8', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right2_8', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_3_8',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_3_8', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left3_8', 0, 0, {}],\n",
       "     ['adjust_bn_8', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_4_8',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_4_8', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left4_8', 0, 0, {}],\n",
       "     ['normal_right4_8', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_5_8',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_5_8', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left5_8', 0, 0, {}],\n",
       "     ['normal_bn_1_8', 0, 0, {}]]]},\n",
       "  {'name': 'normal_concat_8',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'normal_concat_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_bn_8', 0, 0, {}],\n",
       "     ['normal_add_1_8', 0, 0, {}],\n",
       "     ['normal_add_2_8', 0, 0, {}],\n",
       "     ['normal_add_3_8', 0, 0, {}],\n",
       "     ['normal_add_4_8', 0, 0, {}],\n",
       "     ['normal_add_5_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_130',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_130',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_129',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_129',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_7', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_conv_1_reduce_8',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'reduction_conv_1_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_130', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_projection_reduce_8',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_projection_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_129', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_bn_1_reduce_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'reduction_bn_1_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['reduction_conv_1_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_bn_reduce_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'adjust_bn_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_conv_projection_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_131',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_131',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_bn_1_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_133',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_133',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_left1_reduce_8',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_left1_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((1, 2), (1, 2)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_131', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_right1_reduce_8',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_right1_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((2, 3), (2, 3)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_133', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_left1_reduce_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_left1_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_left1_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_right1_reduce_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_right1_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_right1_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_left1_reduce_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_left1_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_left1_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_right1_reduce_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_right1_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_right1_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'activation_132',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_132',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_left1_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'activation_134',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_134',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_right1_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_left1_reduce_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_left1_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_132', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_right1_reduce_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_right1_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_134', 0, 0, {}]]]},\n",
       "  {'name': 'activation_135',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_135',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_left1_reduce_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_left1_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_left1_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_right1_reduce_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_right1_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_right1_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_right2_reduce_8',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_right2_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((2, 3), (2, 3)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_135', 0, 0, {}]]]},\n",
       "  {'name': 'activation_137',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_137',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add_1_reduce_8',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add_1_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_reduction_left1_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}],\n",
       "     ['separable_conv_2_bn_reduction_right1_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_right2_reduce_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_right2_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_right2_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_pad_reduction_right3_reduce_8',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'separable_conv_1_pad_reduction_right3_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((1, 2), (1, 2)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_137', 0, 0, {}]]]},\n",
       "  {'name': 'activation_139',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_139',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_add_1_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_right2_reduce_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_right2_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_right2_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_right3_reduce_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_right3_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_pad_reduction_right3_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_reduction_left4_reduce_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_reduction_left4_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_139', 0, 0, {}]]]},\n",
       "  {'name': 'activation_136',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_136',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_right2_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_right3_reduce_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_right3_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_right3_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_reduction_left4_reduce_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_reduction_left4_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_reduction_left4_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'reduction_pad_1_reduce_8',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'reduction_pad_1_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((0, 1), (0, 1)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_bn_1_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_right2_reduce_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_right2_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (7, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_136', 0, 0, {}]]]},\n",
       "  {'name': 'activation_138',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_138',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_right3_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'activation_140',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_140',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_reduction_left4_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'reduction_left2_reduce_8',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'reduction_left2_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_pad_1_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_right2_reduce_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_right2_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_right2_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_right3_reduce_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_right3_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_138', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_reduction_left4_reduce_8',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_reduction_left4_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_140', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_relu_1_9',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'adjust_relu_1_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_8', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add_2_reduce_8',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add_2_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['reduction_left2_reduce_8', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_reduction_right2_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_left3_reduce_8',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'reduction_left3_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_pad_1_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_right3_reduce_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_right3_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_right3_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'reduction_left4_reduce_8',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'reduction_left4_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_add_1_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_reduction_left4_reduce_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_reduction_left4_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_reduction_left4_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}]]]},\n",
       "  {'name': 'reduction_right5_reduce_8',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'reduction_right5_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['reduction_pad_1_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'zero_padding2d_4',\n",
       "   'class_name': 'ZeroPadding2D',\n",
       "   'config': {'name': 'zero_padding2d_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'padding': ((0, 1), (0, 1)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_relu_1_9', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add3_reduce_8',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add3_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['reduction_left3_reduce_8', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_reduction_right3_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'add_4',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'add_4', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['reduction_add_2_reduce_8', 0, 0, {}],\n",
       "     ['reduction_left4_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_add4_reduce_8',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'reduction_add4_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_reduction_left4_reduce_8',\n",
       "      0,\n",
       "      0,\n",
       "      {}],\n",
       "     ['reduction_right5_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'cropping2d_4',\n",
       "   'class_name': 'Cropping2D',\n",
       "   'config': {'name': 'cropping2d_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'cropping': ((1, 0), (1, 0)),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['zero_padding2d_4', 0, 0, {}]]]},\n",
       "  {'name': 'reduction_concat_reduce_8',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'reduction_concat_reduce_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['reduction_add_2_reduce_8', 0, 0, {}],\n",
       "     ['reduction_add3_reduce_8', 0, 0, {}],\n",
       "     ['add_4', 0, 0, {}],\n",
       "     ['reduction_add4_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_avg_pool_1_9',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'adjust_avg_pool_1_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_relu_1_9', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_avg_pool_2_9',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'adjust_avg_pool_2_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['cropping2d_4', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_1_9',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_1_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_avg_pool_1_9', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_2_9',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_2_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 88,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_avg_pool_2_9', 0, 0, {}]]]},\n",
       "  {'name': 'activation_141',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_141',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_concat_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'concatenate_4',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'concatenate_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_conv_1_9', 0, 0, {}],\n",
       "     ['adjust_conv_2_9', 0, 0, {}]]]},\n",
       "  {'name': 'normal_conv_1_9',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'normal_conv_1_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_141', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_bn_9',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'adjust_bn_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['concatenate_4', 0, 0, {}]]]},\n",
       "  {'name': 'normal_bn_1_9',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'normal_bn_1_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['normal_conv_1_9', 0, 0, {}]]]},\n",
       "  {'name': 'activation_142',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_142',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_9', 0, 0, {}]]]},\n",
       "  {'name': 'activation_144',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_144',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_9', 0, 0, {}]]]},\n",
       "  {'name': 'activation_146',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_146',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_9', 0, 0, {}]]]},\n",
       "  {'name': 'activation_148',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_148',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_9', 0, 0, {}]]]},\n",
       "  {'name': 'activation_150',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_150',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_9', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left1_9',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left1_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_142', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right1_9',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right1_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_144', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left2_9',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left2_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_146', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right2_9',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right2_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_148', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left5_9',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left5_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_150', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left1_9',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left1_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left1_9', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right1_9',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right1_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right1_9', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left2_9',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left2_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left2_9', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right2_9',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right2_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right2_9', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left5_9',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left5_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left5_9', 0, 0, {}]]]},\n",
       "  {'name': 'activation_143',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_143',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left1_9', 0, 0, {}]]]},\n",
       "  {'name': 'activation_145',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_145',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right1_9', 0, 0, {}]]]},\n",
       "  {'name': 'activation_147',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_147',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left2_9', 0, 0, {}]]]},\n",
       "  {'name': 'activation_149',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_149',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right2_9', 0, 0, {}]]]},\n",
       "  {'name': 'activation_151',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_151',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left5_9', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left1_9',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left1_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_143', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right1_9',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right1_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_145', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left2_9',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left2_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_147', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right2_9',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right2_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_149', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left5_9',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left5_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_151', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left1_9',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left1_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left1_9', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right1_9',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right1_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right1_9', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left2_9',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left2_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left2_9', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right2_9',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right2_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right2_9', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left3_9',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left3_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['normal_bn_1_9', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left4_9',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left4_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_9', 0, 0, {}]]]},\n",
       "  {'name': 'normal_right4_9',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_right4_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_9', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left5_9',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left5_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left5_9', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_1_9',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_1_9', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left1_9', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right1_9', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_2_9',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_2_9', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left2_9', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right2_9', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_3_9',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_3_9', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left3_9', 0, 0, {}],\n",
       "     ['adjust_bn_9', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_4_9',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_4_9', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left4_9', 0, 0, {}],\n",
       "     ['normal_right4_9', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_5_9',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_5_9', 'trainable': True, 'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left5_9', 0, 0, {}],\n",
       "     ['normal_bn_1_9', 0, 0, {}]]]},\n",
       "  {'name': 'normal_concat_9',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'normal_concat_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_bn_9', 0, 0, {}],\n",
       "     ['normal_add_1_9', 0, 0, {}],\n",
       "     ['normal_add_2_9', 0, 0, {}],\n",
       "     ['normal_add_3_9', 0, 0, {}],\n",
       "     ['normal_add_4_9', 0, 0, {}],\n",
       "     ['normal_add_5_9', 0, 0, {}]]]},\n",
       "  {'name': 'activation_152',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_152',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['reduction_concat_reduce_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_153',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_153',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_9', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_projection_10',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_projection_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_152', 0, 0, {}]]]},\n",
       "  {'name': 'normal_conv_1_10',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'normal_conv_1_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_153', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_bn_10',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'adjust_bn_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_conv_projection_10', 0, 0, {}]]]},\n",
       "  {'name': 'normal_bn_1_10',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'normal_bn_1_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['normal_conv_1_10', 0, 0, {}]]]},\n",
       "  {'name': 'activation_154',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_154',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_10', 0, 0, {}]]]},\n",
       "  {'name': 'activation_156',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_156',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_10', 0, 0, {}]]]},\n",
       "  {'name': 'activation_158',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_158',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_10', 0, 0, {}]]]},\n",
       "  {'name': 'activation_160',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_160',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_10', 0, 0, {}]]]},\n",
       "  {'name': 'activation_162',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_162',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_10', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left1_10',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left1_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_154', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right1_10',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right1_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_156', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left2_10',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left2_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_158', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right2_10',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right2_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_160', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left5_10',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left5_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_162', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left1_10',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left1_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left1_10', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right1_10',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right1_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right1_10', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left2_10',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left2_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left2_10', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right2_10',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right2_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right2_10', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left5_10',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left5_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left5_10', 0, 0, {}]]]},\n",
       "  {'name': 'activation_155',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_155',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left1_10', 0, 0, {}]]]},\n",
       "  {'name': 'activation_157',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_157',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right1_10', 0, 0, {}]]]},\n",
       "  {'name': 'activation_159',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_159',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left2_10', 0, 0, {}]]]},\n",
       "  {'name': 'activation_161',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_161',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right2_10', 0, 0, {}]]]},\n",
       "  {'name': 'activation_163',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_163',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left5_10', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left1_10',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left1_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_155', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right1_10',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right1_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_157', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left2_10',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left2_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_159', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right2_10',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right2_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_161', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left5_10',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left5_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_163', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left1_10',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left1_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left1_10', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right1_10',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right1_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right1_10', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left2_10',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left2_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left2_10', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right2_10',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right2_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right2_10', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left3_10',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left3_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['normal_bn_1_10', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left4_10',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left4_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_10', 0, 0, {}]]]},\n",
       "  {'name': 'normal_right4_10',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_right4_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_10', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left5_10',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left5_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left5_10', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_1_10',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_1_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left1_10', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right1_10', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_2_10',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_2_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left2_10', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right2_10', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_3_10',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_3_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left3_10', 0, 0, {}],\n",
       "     ['adjust_bn_10', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_4_10',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_4_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left4_10', 0, 0, {}],\n",
       "     ['normal_right4_10', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_5_10',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_5_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left5_10', 0, 0, {}],\n",
       "     ['normal_bn_1_10', 0, 0, {}]]]},\n",
       "  {'name': 'normal_concat_10',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'normal_concat_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_bn_10', 0, 0, {}],\n",
       "     ['normal_add_1_10', 0, 0, {}],\n",
       "     ['normal_add_2_10', 0, 0, {}],\n",
       "     ['normal_add_3_10', 0, 0, {}],\n",
       "     ['normal_add_4_10', 0, 0, {}],\n",
       "     ['normal_add_5_10', 0, 0, {}]]]},\n",
       "  {'name': 'activation_164',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_164',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_9', 0, 0, {}]]]},\n",
       "  {'name': 'activation_165',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_165',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_10', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_projection_11',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_projection_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_164', 0, 0, {}]]]},\n",
       "  {'name': 'normal_conv_1_11',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'normal_conv_1_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_165', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_bn_11',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'adjust_bn_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_conv_projection_11', 0, 0, {}]]]},\n",
       "  {'name': 'normal_bn_1_11',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'normal_bn_1_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['normal_conv_1_11', 0, 0, {}]]]},\n",
       "  {'name': 'activation_166',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_166',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_11', 0, 0, {}]]]},\n",
       "  {'name': 'activation_168',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_168',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_11', 0, 0, {}]]]},\n",
       "  {'name': 'activation_170',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_170',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_11', 0, 0, {}]]]},\n",
       "  {'name': 'activation_172',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_172',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_11', 0, 0, {}]]]},\n",
       "  {'name': 'activation_174',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_174',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_11', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left1_11',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left1_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_166', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right1_11',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right1_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_168', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left2_11',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left2_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_170', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right2_11',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right2_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_172', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left5_11',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left5_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_174', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left1_11',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left1_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left1_11', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right1_11',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right1_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right1_11', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left2_11',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left2_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left2_11', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right2_11',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right2_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right2_11', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left5_11',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left5_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left5_11', 0, 0, {}]]]},\n",
       "  {'name': 'activation_167',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_167',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left1_11', 0, 0, {}]]]},\n",
       "  {'name': 'activation_169',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_169',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right1_11', 0, 0, {}]]]},\n",
       "  {'name': 'activation_171',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_171',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left2_11', 0, 0, {}]]]},\n",
       "  {'name': 'activation_173',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_173',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right2_11', 0, 0, {}]]]},\n",
       "  {'name': 'activation_175',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_175',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left5_11', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left1_11',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left1_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_167', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right1_11',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right1_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_169', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left2_11',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left2_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_171', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right2_11',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right2_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_173', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left5_11',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left5_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_175', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left1_11',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left1_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left1_11', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right1_11',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right1_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right1_11', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left2_11',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left2_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left2_11', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right2_11',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right2_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right2_11', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left3_11',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left3_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['normal_bn_1_11', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left4_11',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left4_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_11', 0, 0, {}]]]},\n",
       "  {'name': 'normal_right4_11',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_right4_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_11', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left5_11',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left5_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left5_11', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_1_11',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_1_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left1_11', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right1_11', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_2_11',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_2_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left2_11', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right2_11', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_3_11',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_3_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left3_11', 0, 0, {}],\n",
       "     ['adjust_bn_11', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_4_11',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_4_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left4_11', 0, 0, {}],\n",
       "     ['normal_right4_11', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_5_11',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_5_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left5_11', 0, 0, {}],\n",
       "     ['normal_bn_1_11', 0, 0, {}]]]},\n",
       "  {'name': 'normal_concat_11',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'normal_concat_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_bn_11', 0, 0, {}],\n",
       "     ['normal_add_1_11', 0, 0, {}],\n",
       "     ['normal_add_2_11', 0, 0, {}],\n",
       "     ['normal_add_3_11', 0, 0, {}],\n",
       "     ['normal_add_4_11', 0, 0, {}],\n",
       "     ['normal_add_5_11', 0, 0, {}]]]},\n",
       "  {'name': 'activation_176',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_176',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_10', 0, 0, {}]]]},\n",
       "  {'name': 'activation_177',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_177',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_11', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_conv_projection_12',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'adjust_conv_projection_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_176', 0, 0, {}]]]},\n",
       "  {'name': 'normal_conv_1_12',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'normal_conv_1_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 2.0,\n",
       "      'mode': 'fan_in',\n",
       "      'distribution': 'normal',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_177', 0, 0, {}]]]},\n",
       "  {'name': 'adjust_bn_12',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'adjust_bn_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['adjust_conv_projection_12', 0, 0, {}]]]},\n",
       "  {'name': 'normal_bn_1_12',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'normal_bn_1_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['normal_conv_1_12', 0, 0, {}]]]},\n",
       "  {'name': 'activation_178',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_178',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_12', 0, 0, {}]]]},\n",
       "  {'name': 'activation_180',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_180',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_12', 0, 0, {}]]]},\n",
       "  {'name': 'activation_182',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_182',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_12', 0, 0, {}]]]},\n",
       "  {'name': 'activation_184',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_184',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['adjust_bn_12', 0, 0, {}]]]},\n",
       "  {'name': 'activation_186',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_186',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_bn_1_12', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left1_12',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left1_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_178', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right1_12',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right1_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_180', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left2_12',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left2_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_182', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_right2_12',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_right2_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_184', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_normal_left5_12',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_1_normal_left5_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_186', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left1_12',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left1_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left1_12', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right1_12',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right1_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right1_12', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left2_12',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left2_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left2_12', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_right2_12',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_right2_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_right2_12', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_1_bn_normal_left5_12',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_1_bn_normal_left5_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_1_normal_left5_12', 0, 0, {}]]]},\n",
       "  {'name': 'activation_179',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_179',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left1_12', 0, 0, {}]]]},\n",
       "  {'name': 'activation_181',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_181',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right1_12', 0, 0, {}]]]},\n",
       "  {'name': 'activation_183',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_183',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left2_12', 0, 0, {}]]]},\n",
       "  {'name': 'activation_185',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_185',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_right2_12', 0, 0, {}]]]},\n",
       "  {'name': 'activation_187',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_187',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['separable_conv_1_bn_normal_left5_12', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left1_12',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left1_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_179', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right1_12',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right1_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_181', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left2_12',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left2_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_183', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_right2_12',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_right2_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_185', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_normal_left5_12',\n",
       "   'class_name': 'SeparableConv2D',\n",
       "   'config': {'name': 'separable_conv_2_normal_left5_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 176,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'depth_multiplier': 1,\n",
       "    'depthwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'pointwise_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'depthwise_regularizer': None,\n",
       "    'pointwise_regularizer': None,\n",
       "    'depthwise_constraint': None,\n",
       "    'pointwise_constraint': None},\n",
       "   'inbound_nodes': [[['activation_187', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left1_12',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left1_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left1_12', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right1_12',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right1_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right1_12', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left2_12',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left2_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left2_12', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_right2_12',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_right2_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_right2_12', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left3_12',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left3_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['normal_bn_1_12', 0, 0, {}]]]},\n",
       "  {'name': 'normal_left4_12',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_left4_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_12', 0, 0, {}]]]},\n",
       "  {'name': 'normal_right4_12',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'normal_right4_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['adjust_bn_12', 0, 0, {}]]]},\n",
       "  {'name': 'separable_conv_2_bn_normal_left5_12',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'separable_conv_2_bn_normal_left5_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1,\n",
       "    'momentum': 0.9997,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['separable_conv_2_normal_left5_12', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_1_12',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_1_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left1_12', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right1_12', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_2_12',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_2_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left2_12', 0, 0, {}],\n",
       "     ['separable_conv_2_bn_normal_right2_12', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_3_12',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_3_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left3_12', 0, 0, {}],\n",
       "     ['adjust_bn_12', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_4_12',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_4_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['normal_left4_12', 0, 0, {}],\n",
       "     ['normal_right4_12', 0, 0, {}]]]},\n",
       "  {'name': 'normal_add_5_12',\n",
       "   'class_name': 'Add',\n",
       "   'config': {'name': 'normal_add_5_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32'},\n",
       "   'inbound_nodes': [[['separable_conv_2_bn_normal_left5_12', 0, 0, {}],\n",
       "     ['normal_bn_1_12', 0, 0, {}]]]},\n",
       "  {'name': 'normal_concat_12',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'normal_concat_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': -1},\n",
       "   'inbound_nodes': [[['adjust_bn_12', 0, 0, {}],\n",
       "     ['normal_add_1_12', 0, 0, {}],\n",
       "     ['normal_add_2_12', 0, 0, {}],\n",
       "     ['normal_add_3_12', 0, 0, {}],\n",
       "     ['normal_add_4_12', 0, 0, {}],\n",
       "     ['normal_add_5_12', 0, 0, {}]]]},\n",
       "  {'name': 'activation_188',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_188',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['normal_concat_12', 0, 0, {}]]]}],\n",
       " 'input_layers': [['input_1', 0, 0]],\n",
       " 'output_layers': [['activation_188', 0, 0]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-01 21:12:13.554892\n",
      "(42378, 7, 7, 1056)\n",
      "1:28:21.292371\n"
     ]
    }
   ],
   "source": [
    "t1=datetime.datetime.now()\n",
    "print(t1)\n",
    "# extracting features for training frames\n",
    "X_train = base_model.predict(X_train)\n",
    "print(X_train.shape)\n",
    "t2=datetime.datetime.now()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the training as well as validation frames in single dimension\n",
    "X_train = X_train.reshape(42378, 7*7*1056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open('../Pickle/NASNetMobile_X_train_OF.pickle',\"wb\")\n",
    "pickle.dump(X_train, pickle_out, protocol=4)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42378, 51744)\n"
     ]
    }
   ],
   "source": [
    "pickle_in = open('../Pickle/NASNetMobile_X_train_OF.pickle',\"rb\")\n",
    "X_train = pickle.load(pickle_in)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-01 22:43:20.411402\n",
      "(8803, 7, 7, 1056)\n",
      "0:17:51.562199\n"
     ]
    }
   ],
   "source": [
    "t3=datetime.datetime.now()\n",
    "print(t3)\n",
    "# extracting features for validation frames\n",
    "X_test = base_model.predict(X_test)\n",
    "print(X_test.shape)\n",
    "t4=datetime.datetime.now()\n",
    "print(t4-t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(8803, 7*7*1056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open('../Pickle/NASNetMobile_X_test_OF.pickle',\"wb\")\n",
    "pickle.dump(X_test, pickle_out, protocol=4)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8803, 51744)\n"
     ]
    }
   ],
   "source": [
    "pickle_in = open('../Pickle/NASNetMobile_X_test_OF.pickle',\"rb\")\n",
    "X_test = pickle.load(pickle_in)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42378, 51744)\n",
      "(8803, 51744)\n",
      "(42378, 51)\n",
      "(8803, 51)\n"
     ]
    }
   ],
   "source": [
    "# shape of images\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(51744,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(51, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 51)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 1024)              52986880  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 51)                6579      \n",
      "=================================================================\n",
      "Total params: 53,682,483\n",
      "Trainable params: 53,682,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential_2',\n",
       " 'layers': [{'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_6',\n",
       "    'trainable': True,\n",
       "    'batch_input_shape': (None, 51744),\n",
       "    'dtype': 'float32',\n",
       "    'units': 1024,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 512,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 256,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 128,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 51,\n",
       "    'activation': 'softmax',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}}]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_save = ModelCheckpoint('../Models/weightNASNetMobile_OF.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-01 23:02:57.477050\n",
      "Train on 42378 samples, validate on 8803 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23424/42378 [===============>..............] - ETA: 11:26 - loss: 5.8352 - accuracy: 0.015 - ETA: 7:32 - loss: 10.9095 - accuracy: 0.023 - ETA: 6:11 - loss: 12.1886 - accuracy: 0.020 - ETA: 5:29 - loss: 12.7638 - accuracy: 0.021 - ETA: 5:01 - loss: 12.7048 - accuracy: 0.023 - ETA: 4:42 - loss: 12.4716 - accuracy: 0.020 - ETA: 4:29 - loss: 12.0598 - accuracy: 0.024 - ETA: 4:18 - loss: 11.6482 - accuracy: 0.022 - ETA: 4:10 - loss: 11.1511 - accuracy: 0.022 - ETA: 4:04 - loss: 10.7033 - accuracy: 0.025 - ETA: 3:59 - loss: 10.2995 - accuracy: 0.026 - ETA: 3:54 - loss: 9.9270 - accuracy: 0.027 - ETA: 3:50 - loss: 9.5818 - accuracy: 0.02 - ETA: 3:46 - loss: 9.2822 - accuracy: 0.02 - ETA: 3:43 - loss: 9.0130 - accuracy: 0.02 - ETA: 3:40 - loss: 8.7548 - accuracy: 0.02 - ETA: 3:38 - loss: 8.5013 - accuracy: 0.02 - ETA: 3:35 - loss: 8.2702 - accuracy: 0.02 - ETA: 3:33 - loss: 8.0628 - accuracy: 0.02 - ETA: 3:31 - loss: 7.8757 - accuracy: 0.02 - ETA: 3:29 - loss: 7.7073 - accuracy: 0.02 - ETA: 3:27 - loss: 7.5448 - accuracy: 0.02 - ETA: 3:26 - loss: 7.4048 - accuracy: 0.02 - ETA: 3:24 - loss: 7.2723 - accuracy: 0.02 - ETA: 3:22 - loss: 7.1407 - accuracy: 0.02 - ETA: 3:21 - loss: 7.0280 - accuracy: 0.02 - ETA: 3:20 - loss: 6.9172 - accuracy: 0.02 - ETA: 3:19 - loss: 6.8175 - accuracy: 0.02 - ETA: 3:19 - loss: 6.7231 - accuracy: 0.02 - ETA: 3:17 - loss: 6.6328 - accuracy: 0.02 - ETA: 3:16 - loss: 6.5465 - accuracy: 0.02 - ETA: 3:15 - loss: 6.4667 - accuracy: 0.02 - ETA: 3:14 - loss: 6.3912 - accuracy: 0.02 - ETA: 3:13 - loss: 6.3220 - accuracy: 0.02 - ETA: 3:12 - loss: 6.2570 - accuracy: 0.02 - ETA: 3:11 - loss: 6.1940 - accuracy: 0.02 - ETA: 3:10 - loss: 6.1337 - accuracy: 0.02 - ETA: 3:09 - loss: 6.0749 - accuracy: 0.02 - ETA: 3:08 - loss: 6.0226 - accuracy: 0.02 - ETA: 3:07 - loss: 5.9723 - accuracy: 0.02 - ETA: 3:06 - loss: 5.9234 - accuracy: 0.02 - ETA: 3:05 - loss: 5.8772 - accuracy: 0.02 - ETA: 3:04 - loss: 5.8364 - accuracy: 0.02 - ETA: 3:03 - loss: 5.7935 - accuracy: 0.02 - ETA: 3:02 - loss: 5.7537 - accuracy: 0.02 - ETA: 3:01 - loss: 5.7154 - accuracy: 0.02 - ETA: 3:01 - loss: 5.6786 - accuracy: 0.02 - ETA: 3:00 - loss: 5.6442 - accuracy: 0.02 - ETA: 2:59 - loss: 5.6096 - accuracy: 0.02 - ETA: 2:58 - loss: 5.5765 - accuracy: 0.02 - ETA: 2:57 - loss: 5.5447 - accuracy: 0.02 - ETA: 2:56 - loss: 5.5157 - accuracy: 0.02 - ETA: 2:55 - loss: 5.4851 - accuracy: 0.02 - ETA: 2:55 - loss: 5.4573 - accuracy: 0.02 - ETA: 2:54 - loss: 5.4302 - accuracy: 0.02 - ETA: 2:54 - loss: 5.4041 - accuracy: 0.02 - ETA: 2:53 - loss: 5.3783 - accuracy: 0.02 - ETA: 2:52 - loss: 5.3540 - accuracy: 0.02 - ETA: 2:52 - loss: 5.3307 - accuracy: 0.02 - ETA: 2:51 - loss: 5.3066 - accuracy: 0.02 - ETA: 2:50 - loss: 5.2837 - accuracy: 0.02 - ETA: 2:50 - loss: 5.2634 - accuracy: 0.02 - ETA: 2:49 - loss: 5.2421 - accuracy: 0.02 - ETA: 2:48 - loss: 5.2215 - accuracy: 0.02 - ETA: 2:47 - loss: 5.2016 - accuracy: 0.02 - ETA: 2:47 - loss: 5.1826 - accuracy: 0.02 - ETA: 2:46 - loss: 5.1653 - accuracy: 0.02 - ETA: 2:45 - loss: 5.1471 - accuracy: 0.02 - ETA: 2:45 - loss: 5.1303 - accuracy: 0.02 - ETA: 2:44 - loss: 5.1136 - accuracy: 0.02 - ETA: 2:43 - loss: 5.0965 - accuracy: 0.02 - ETA: 2:43 - loss: 5.0812 - accuracy: 0.02 - ETA: 2:42 - loss: 5.0662 - accuracy: 0.02 - ETA: 2:42 - loss: 5.0511 - accuracy: 0.02 - ETA: 2:41 - loss: 5.0366 - accuracy: 0.02 - ETA: 2:41 - loss: 5.0218 - accuracy: 0.02 - ETA: 2:40 - loss: 5.0080 - accuracy: 0.02 - ETA: 2:39 - loss: 4.9941 - accuracy: 0.02 - ETA: 2:39 - loss: 4.9806 - accuracy: 0.02 - ETA: 2:39 - loss: 4.9674 - accuracy: 0.02 - ETA: 2:38 - loss: 4.9551 - accuracy: 0.02 - ETA: 2:37 - loss: 4.9423 - accuracy: 0.02 - ETA: 2:37 - loss: 4.9297 - accuracy: 0.02 - ETA: 2:36 - loss: 4.9182 - accuracy: 0.02 - ETA: 2:35 - loss: 4.9068 - accuracy: 0.02 - ETA: 2:35 - loss: 4.8955 - accuracy: 0.02 - ETA: 2:34 - loss: 4.8851 - accuracy: 0.02 - ETA: 2:33 - loss: 4.8741 - accuracy: 0.02 - ETA: 2:33 - loss: 4.8637 - accuracy: 0.03 - ETA: 2:32 - loss: 4.8537 - accuracy: 0.03 - ETA: 2:31 - loss: 4.8430 - accuracy: 0.03 - ETA: 2:31 - loss: 4.8334 - accuracy: 0.03 - ETA: 2:30 - loss: 4.8236 - accuracy: 0.03 - ETA: 2:29 - loss: 4.8139 - accuracy: 0.03 - ETA: 2:29 - loss: 4.8045 - accuracy: 0.03 - ETA: 2:28 - loss: 4.7953 - accuracy: 0.03 - ETA: 2:27 - loss: 4.7868 - accuracy: 0.03 - ETA: 2:27 - loss: 4.7783 - accuracy: 0.03 - ETA: 2:26 - loss: 4.7702 - accuracy: 0.03 - ETA: 2:25 - loss: 4.7616 - accuracy: 0.03 - ETA: 2:25 - loss: 4.7534 - accuracy: 0.03 - ETA: 2:24 - loss: 4.7457 - accuracy: 0.03 - ETA: 2:23 - loss: 4.7378 - accuracy: 0.03 - ETA: 2:23 - loss: 4.7298 - accuracy: 0.03 - ETA: 2:22 - loss: 4.7227 - accuracy: 0.03 - ETA: 2:22 - loss: 4.7149 - accuracy: 0.03 - ETA: 2:21 - loss: 4.7079 - accuracy: 0.03 - ETA: 2:20 - loss: 4.7008 - accuracy: 0.03 - ETA: 2:20 - loss: 4.6939 - accuracy: 0.03 - ETA: 2:19 - loss: 4.6865 - accuracy: 0.03 - ETA: 2:18 - loss: 4.6795 - accuracy: 0.03 - ETA: 2:18 - loss: 4.6725 - accuracy: 0.03 - ETA: 2:17 - loss: 4.6661 - accuracy: 0.03 - ETA: 2:17 - loss: 4.6596 - accuracy: 0.03 - ETA: 2:16 - loss: 4.6532 - accuracy: 0.03 - ETA: 2:15 - loss: 4.6466 - accuracy: 0.03 - ETA: 2:15 - loss: 4.6404 - accuracy: 0.03 - ETA: 2:14 - loss: 4.6342 - accuracy: 0.03 - ETA: 2:13 - loss: 4.6278 - accuracy: 0.03 - ETA: 2:13 - loss: 4.6218 - accuracy: 0.03 - ETA: 2:12 - loss: 4.6160 - accuracy: 0.03 - ETA: 2:11 - loss: 4.6102 - accuracy: 0.03 - ETA: 2:11 - loss: 4.6044 - accuracy: 0.03 - ETA: 2:10 - loss: 4.5984 - accuracy: 0.03 - ETA: 2:09 - loss: 4.5930 - accuracy: 0.03 - ETA: 2:09 - loss: 4.5879 - accuracy: 0.03 - ETA: 2:08 - loss: 4.5827 - accuracy: 0.03 - ETA: 2:08 - loss: 4.5773 - accuracy: 0.03 - ETA: 2:07 - loss: 4.5722 - accuracy: 0.03 - ETA: 2:06 - loss: 4.5673 - accuracy: 0.03 - ETA: 2:06 - loss: 4.5626 - accuracy: 0.03 - ETA: 2:05 - loss: 4.5576 - accuracy: 0.03 - ETA: 2:05 - loss: 4.5522 - accuracy: 0.03 - ETA: 2:04 - loss: 4.5473 - accuracy: 0.03 - ETA: 2:03 - loss: 4.5423 - accuracy: 0.03 - ETA: 2:03 - loss: 4.5380 - accuracy: 0.03 - ETA: 2:02 - loss: 4.5332 - accuracy: 0.03 - ETA: 2:01 - loss: 4.5286 - accuracy: 0.03 - ETA: 2:01 - loss: 4.5238 - accuracy: 0.03 - ETA: 2:00 - loss: 4.5192 - accuracy: 0.03 - ETA: 1:59 - loss: 4.5148 - accuracy: 0.03 - ETA: 1:59 - loss: 4.5105 - accuracy: 0.03 - ETA: 1:58 - loss: 4.5063 - accuracy: 0.03 - ETA: 1:58 - loss: 4.5020 - accuracy: 0.03 - ETA: 1:57 - loss: 4.4981 - accuracy: 0.03 - ETA: 1:56 - loss: 4.4945 - accuracy: 0.03 - ETA: 1:56 - loss: 4.4907 - accuracy: 0.03 - ETA: 1:55 - loss: 4.4864 - accuracy: 0.03 - ETA: 1:55 - loss: 4.4826 - accuracy: 0.03 - ETA: 1:54 - loss: 4.4784 - accuracy: 0.03 - ETA: 1:54 - loss: 4.4747 - accuracy: 0.03 - ETA: 1:53 - loss: 4.4709 - accuracy: 0.03 - ETA: 1:52 - loss: 4.4675 - accuracy: 0.03 - ETA: 1:52 - loss: 4.4643 - accuracy: 0.03 - ETA: 1:51 - loss: 4.4604 - accuracy: 0.03 - ETA: 1:50 - loss: 4.4571 - accuracy: 0.03 - ETA: 1:50 - loss: 4.4534 - accuracy: 0.03 - ETA: 1:49 - loss: 4.4501 - accuracy: 0.03 - ETA: 1:49 - loss: 4.4471 - accuracy: 0.03 - ETA: 1:48 - loss: 4.4440 - accuracy: 0.03 - ETA: 1:47 - loss: 4.4409 - accuracy: 0.03 - ETA: 1:47 - loss: 4.4375 - accuracy: 0.03 - ETA: 1:46 - loss: 4.4341 - accuracy: 0.03 - ETA: 1:45 - loss: 4.4307 - accuracy: 0.03 - ETA: 1:45 - loss: 4.4274 - accuracy: 0.03 - ETA: 1:44 - loss: 4.4242 - accuracy: 0.03 - ETA: 1:44 - loss: 4.4210 - accuracy: 0.03 - ETA: 1:43 - loss: 4.4177 - accuracy: 0.03 - ETA: 1:42 - loss: 4.4144 - accuracy: 0.03 - ETA: 1:42 - loss: 4.4111 - accuracy: 0.03 - ETA: 1:41 - loss: 4.4082 - accuracy: 0.03 - ETA: 1:41 - loss: 4.4048 - accuracy: 0.03 - ETA: 1:40 - loss: 4.4017 - accuracy: 0.03 - ETA: 1:39 - loss: 4.3990 - accuracy: 0.03 - ETA: 1:39 - loss: 4.3957 - accuracy: 0.03 - ETA: 1:38 - loss: 4.3933 - accuracy: 0.03 - ETA: 1:37 - loss: 4.3908 - accuracy: 0.03 - ETA: 1:37 - loss: 4.3881 - accuracy: 0.03 - ETA: 1:36 - loss: 4.3855 - accuracy: 0.03 - ETA: 1:35 - loss: 4.3826 - accuracy: 0.03 - ETA: 1:35 - loss: 4.3794 - accuracy: 0.03 - ETA: 1:34 - loss: 4.3765 - accuracy: 0.03 - ETA: 1:34 - loss: 4.3736 - accuracy: 0.0371\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:33 - loss: 4.3706 - accuracy: 0.03 - ETA: 1:32 - loss: 4.3682 - accuracy: 0.03 - ETA: 1:32 - loss: 4.3656 - accuracy: 0.03 - ETA: 1:31 - loss: 4.3631 - accuracy: 0.03 - ETA: 1:30 - loss: 4.3607 - accuracy: 0.03 - ETA: 1:30 - loss: 4.3585 - accuracy: 0.03 - ETA: 1:29 - loss: 4.3561 - accuracy: 0.03 - ETA: 1:29 - loss: 4.3534 - accuracy: 0.03 - ETA: 1:28 - loss: 4.3512 - accuracy: 0.03 - ETA: 1:27 - loss: 4.3488 - accuracy: 0.03 - ETA: 1:27 - loss: 4.3467 - accuracy: 0.03 - ETA: 1:26 - loss: 4.3444 - accuracy: 0.03 - ETA: 1:25 - loss: 4.3423 - accuracy: 0.03 - ETA: 1:25 - loss: 4.3392 - accuracy: 0.03 - ETA: 1:24 - loss: 4.3373 - accuracy: 0.03 - ETA: 1:23 - loss: 4.3354 - accuracy: 0.03 - ETA: 1:23 - loss: 4.3330 - accuracy: 0.03 - ETA: 1:22 - loss: 4.3308 - accuracy: 0.03 - ETA: 1:21 - loss: 4.3284 - accuracy: 0.03 - ETA: 1:21 - loss: 4.3263 - accuracy: 0.03 - ETA: 1:20 - loss: 4.3244 - accuracy: 0.03 - ETA: 1:19 - loss: 4.3222 - accuracy: 0.03 - ETA: 1:19 - loss: 4.3199 - accuracy: 0.03 - ETA: 1:18 - loss: 4.3176 - accuracy: 0.03 - ETA: 1:18 - loss: 4.3158 - accuracy: 0.03 - ETA: 1:17 - loss: 4.3137 - accuracy: 0.03 - ETA: 1:16 - loss: 4.3114 - accuracy: 0.03 - ETA: 1:16 - loss: 4.3097 - accuracy: 0.03 - ETA: 1:15 - loss: 4.3074 - accuracy: 0.03 - ETA: 1:14 - loss: 4.3056 - accuracy: 0.03 - ETA: 1:14 - loss: 4.3035 - accuracy: 0.03 - ETA: 1:13 - loss: 4.3011 - accuracy: 0.03 - ETA: 1:12 - loss: 4.2992 - accuracy: 0.03 - ETA: 1:12 - loss: 4.2971 - accuracy: 0.03 - ETA: 1:11 - loss: 4.2949 - accuracy: 0.03 - ETA: 1:11 - loss: 4.2930 - accuracy: 0.03 - ETA: 1:10 - loss: 4.2912 - accuracy: 0.03 - ETA: 1:09 - loss: 4.2892 - accuracy: 0.03 - ETA: 1:09 - loss: 4.2869 - accuracy: 0.03 - ETA: 1:08 - loss: 4.2852 - accuracy: 0.03 - ETA: 1:07 - loss: 4.2834 - accuracy: 0.03 - ETA: 1:07 - loss: 4.2815 - accuracy: 0.03 - ETA: 1:06 - loss: 4.2798 - accuracy: 0.03 - ETA: 1:05 - loss: 4.2781 - accuracy: 0.03 - ETA: 1:05 - loss: 4.2761 - accuracy: 0.03 - ETA: 1:04 - loss: 4.2746 - accuracy: 0.03 - ETA: 1:03 - loss: 4.2727 - accuracy: 0.03 - ETA: 1:03 - loss: 4.2710 - accuracy: 0.03 - ETA: 1:02 - loss: 4.2691 - accuracy: 0.03 - ETA: 1:02 - loss: 4.2672 - accuracy: 0.03 - ETA: 1:01 - loss: 4.2653 - accuracy: 0.03 - ETA: 1:00 - loss: 4.2634 - accuracy: 0.03 - ETA: 1:00 - loss: 4.2620 - accuracy: 0.03 - ETA: 59s - loss: 4.2604 - accuracy: 0.0398 - ETA: 58s - loss: 4.2592 - accuracy: 0.039 - ETA: 58s - loss: 4.2574 - accuracy: 0.039 - ETA: 57s - loss: 4.2557 - accuracy: 0.039 - ETA: 57s - loss: 4.2538 - accuracy: 0.039 - ETA: 56s - loss: 4.2522 - accuracy: 0.040 - ETA: 55s - loss: 4.2502 - accuracy: 0.040 - ETA: 55s - loss: 4.2485 - accuracy: 0.040 - ETA: 54s - loss: 4.2468 - accuracy: 0.040 - ETA: 53s - loss: 4.2450 - accuracy: 0.040 - ETA: 53s - loss: 4.2435 - accuracy: 0.040 - ETA: 52s - loss: 4.2419 - accuracy: 0.040 - ETA: 52s - loss: 4.2403 - accuracy: 0.040 - ETA: 51s - loss: 4.2386 - accuracy: 0.040 - ETA: 50s - loss: 4.2373 - accuracy: 0.040 - ETA: 50s - loss: 4.2362 - accuracy: 0.040 - ETA: 49s - loss: 4.2349 - accuracy: 0.040 - ETA: 48s - loss: 4.2331 - accuracy: 0.040 - ETA: 48s - loss: 4.2317 - accuracy: 0.040 - ETA: 47s - loss: 4.2302 - accuracy: 0.040 - ETA: 46s - loss: 4.2287 - accuracy: 0.040 - ETA: 46s - loss: 4.2273 - accuracy: 0.040 - ETA: 45s - loss: 4.2258 - accuracy: 0.040 - ETA: 45s - loss: 4.2243 - accuracy: 0.041 - ETA: 44s - loss: 4.2228 - accuracy: 0.041 - ETA: 43s - loss: 4.2211 - accuracy: 0.041 - ETA: 43s - loss: 4.2197 - accuracy: 0.041 - ETA: 42s - loss: 4.2181 - accuracy: 0.041 - ETA: 41s - loss: 4.2163 - accuracy: 0.041 - ETA: 41s - loss: 4.2152 - accuracy: 0.041 - ETA: 40s - loss: 4.2137 - accuracy: 0.041 - ETA: 40s - loss: 4.2123 - accuracy: 0.041 - ETA: 39s - loss: 4.2113 - accuracy: 0.041 - ETA: 38s - loss: 4.2099 - accuracy: 0.041 - ETA: 38s - loss: 4.2084 - accuracy: 0.041 - ETA: 37s - loss: 4.2071 - accuracy: 0.041 - ETA: 36s - loss: 4.2059 - accuracy: 0.041 - ETA: 36s - loss: 4.2044 - accuracy: 0.042 - ETA: 35s - loss: 4.2033 - accuracy: 0.042 - ETA: 34s - loss: 4.2016 - accuracy: 0.042 - ETA: 34s - loss: 4.2005 - accuracy: 0.042 - ETA: 33s - loss: 4.1993 - accuracy: 0.042 - ETA: 33s - loss: 4.1982 - accuracy: 0.042 - ETA: 32s - loss: 4.1970 - accuracy: 0.042 - ETA: 31s - loss: 4.1958 - accuracy: 0.042 - ETA: 31s - loss: 4.1946 - accuracy: 0.042 - ETA: 30s - loss: 4.1933 - accuracy: 0.042 - ETA: 29s - loss: 4.1921 - accuracy: 0.042 - ETA: 29s - loss: 4.1905 - accuracy: 0.042 - ETA: 28s - loss: 4.1894 - accuracy: 0.042 - ETA: 28s - loss: 4.1881 - accuracy: 0.042 - ETA: 27s - loss: 4.1870 - accuracy: 0.042 - ETA: 26s - loss: 4.1859 - accuracy: 0.042 - ETA: 26s - loss: 4.1847 - accuracy: 0.042 - ETA: 25s - loss: 4.1834 - accuracy: 0.042 - ETA: 24s - loss: 4.1821 - accuracy: 0.042 - ETA: 24s - loss: 4.1812 - accuracy: 0.042 - ETA: 23s - loss: 4.1801 - accuracy: 0.042 - ETA: 22s - loss: 4.1789 - accuracy: 0.042 - ETA: 22s - loss: 4.1776 - accuracy: 0.042 - ETA: 21s - loss: 4.1765 - accuracy: 0.042 - ETA: 21s - loss: 4.1754 - accuracy: 0.042 - ETA: 20s - loss: 4.1741 - accuracy: 0.043 - ETA: 19s - loss: 4.1728 - accuracy: 0.043 - ETA: 19s - loss: 4.1720 - accuracy: 0.043 - ETA: 18s - loss: 4.1708 - accuracy: 0.043 - ETA: 17s - loss: 4.1696 - accuracy: 0.043 - ETA: 17s - loss: 4.1683 - accuracy: 0.043 - ETA: 16s - loss: 4.1672 - accuracy: 0.043 - ETA: 15s - loss: 4.1661 - accuracy: 0.043 - ETA: 15s - loss: 4.1649 - accuracy: 0.043 - ETA: 14s - loss: 4.1641 - accuracy: 0.043 - ETA: 14s - loss: 4.1633 - accuracy: 0.043 - ETA: 13s - loss: 4.1622 - accuracy: 0.043 - ETA: 12s - loss: 4.1615 - accuracy: 0.043 - ETA: 12s - loss: 4.1605 - accuracy: 0.043 - ETA: 11s - loss: 4.1596 - accuracy: 0.043 - ETA: 10s - loss: 4.1587 - accuracy: 0.043 - ETA: 10s - loss: 4.1577 - accuracy: 0.043 - ETA: 9s - loss: 4.1568 - accuracy: 0.043 - ETA: 8s - loss: 4.1558 - accuracy: 0.04 - ETA: 8s - loss: 4.1549 - accuracy: 0.04 - ETA: 7s - loss: 4.1537 - accuracy: 0.04 - ETA: 7s - loss: 4.1530 - accuracy: 0.04 - ETA: 6s - loss: 4.1520 - accuracy: 0.04 - ETA: 5s - loss: 4.1509 - accuracy: 0.04 - ETA: 5s - loss: 4.1498 - accuracy: 0.04 - ETA: 4s - loss: 4.1488 - accuracy: 0.04 - ETA: 3s - loss: 4.1477 - accuracy: 0.04 - ETA: 3s - loss: 4.1466 - accuracy: 0.04 - ETA: 2s - loss: 4.1456 - accuracy: 0.04 - ETA: 1s - loss: 4.1444 - accuracy: 0.04 - ETA: 1s - loss: 4.1435 - accuracy: 0.04 - ETA: 0s - loss: 4.1427 - accuracy: 0.04 - ETA: 0s - loss: 4.1417 - accuracy: 0.04 - 224s 5ms/step - loss: 4.1416 - accuracy: 0.0436 - val_loss: 3.9445 - val_accuracy: 0.0148\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:52 - loss: 3.8536 - accuracy: 0.03 - ETA: 3:35 - loss: 3.8567 - accuracy: 0.02 - ETA: 3:31 - loss: 3.8231 - accuracy: 0.04 - ETA: 3:31 - loss: 3.8370 - accuracy: 0.04 - ETA: 3:27 - loss: 3.8451 - accuracy: 0.04 - ETA: 3:26 - loss: 3.8493 - accuracy: 0.04 - ETA: 3:23 - loss: 3.8490 - accuracy: 0.04 - ETA: 3:22 - loss: 3.8558 - accuracy: 0.05 - ETA: 3:21 - loss: 3.8546 - accuracy: 0.04 - ETA: 3:20 - loss: 3.8500 - accuracy: 0.04 - ETA: 3:18 - loss: 3.8525 - accuracy: 0.05 - ETA: 3:17 - loss: 3.8534 - accuracy: 0.05 - ETA: 3:16 - loss: 3.8511 - accuracy: 0.04 - ETA: 3:15 - loss: 3.8459 - accuracy: 0.04 - ETA: 3:14 - loss: 3.8437 - accuracy: 0.04 - ETA: 3:15 - loss: 3.8394 - accuracy: 0.05 - ETA: 3:15 - loss: 3.8375 - accuracy: 0.05 - ETA: 3:15 - loss: 3.8308 - accuracy: 0.05 - ETA: 3:14 - loss: 3.8316 - accuracy: 0.05 - ETA: 3:13 - loss: 3.8320 - accuracy: 0.05 - ETA: 3:12 - loss: 3.8303 - accuracy: 0.05 - ETA: 3:12 - loss: 3.8311 - accuracy: 0.05 - ETA: 3:12 - loss: 3.8310 - accuracy: 0.05 - ETA: 3:12 - loss: 3.8271 - accuracy: 0.05 - ETA: 3:11 - loss: 3.8247 - accuracy: 0.05 - ETA: 3:11 - loss: 3.8234 - accuracy: 0.05 - ETA: 3:10 - loss: 3.8230 - accuracy: 0.05 - ETA: 3:09 - loss: 3.8197 - accuracy: 0.05 - ETA: 3:08 - loss: 3.8190 - accuracy: 0.05 - ETA: 3:07 - loss: 3.8217 - accuracy: 0.05 - ETA: 3:07 - loss: 3.8222 - accuracy: 0.05 - ETA: 3:06 - loss: 3.8216 - accuracy: 0.05 - ETA: 3:05 - loss: 3.8215 - accuracy: 0.05 - ETA: 3:05 - loss: 3.8206 - accuracy: 0.05 - ETA: 3:04 - loss: 3.8205 - accuracy: 0.05 - ETA: 3:03 - loss: 3.8213 - accuracy: 0.05 - ETA: 3:02 - loss: 3.8197 - accuracy: 0.05 - ETA: 3:02 - loss: 3.8192 - accuracy: 0.05 - ETA: 3:01 - loss: 3.8187 - accuracy: 0.05 - ETA: 3:00 - loss: 3.8180 - accuracy: 0.05 - ETA: 3:00 - loss: 3.8176 - accuracy: 0.05 - ETA: 2:59 - loss: 3.8172 - accuracy: 0.05 - ETA: 2:58 - loss: 3.8164 - accuracy: 0.05 - ETA: 2:57 - loss: 3.8169 - accuracy: 0.05 - ETA: 2:56 - loss: 3.8169 - accuracy: 0.05 - ETA: 2:56 - loss: 3.8160 - accuracy: 0.05 - ETA: 2:55 - loss: 3.8166 - accuracy: 0.05 - ETA: 2:55 - loss: 3.8165 - accuracy: 0.05 - ETA: 2:54 - loss: 3.8161 - accuracy: 0.05 - ETA: 2:53 - loss: 3.8124 - accuracy: 0.05 - ETA: 2:53 - loss: 3.8127 - accuracy: 0.05 - ETA: 2:52 - loss: 3.8112 - accuracy: 0.05 - ETA: 2:51 - loss: 3.8138 - accuracy: 0.05 - ETA: 2:51 - loss: 3.8142 - accuracy: 0.05 - ETA: 2:50 - loss: 3.8144 - accuracy: 0.05 - ETA: 2:49 - loss: 3.8126 - accuracy: 0.05 - ETA: 2:49 - loss: 3.8116 - accuracy: 0.05 - ETA: 2:48 - loss: 3.8111 - accuracy: 0.05 - ETA: 2:47 - loss: 3.8134 - accuracy: 0.05 - ETA: 2:46 - loss: 3.8147 - accuracy: 0.05 - ETA: 2:46 - loss: 3.8136 - accuracy: 0.05 - ETA: 2:45 - loss: 3.8128 - accuracy: 0.05 - ETA: 2:44 - loss: 3.8132 - accuracy: 0.05 - ETA: 2:44 - loss: 3.8143 - accuracy: 0.05 - ETA: 2:43 - loss: 3.8141 - accuracy: 0.05 - ETA: 2:43 - loss: 3.8134 - accuracy: 0.05 - ETA: 2:42 - loss: 3.8143 - accuracy: 0.05 - ETA: 2:41 - loss: 3.8144 - accuracy: 0.05 - ETA: 2:41 - loss: 3.8138 - accuracy: 0.05 - ETA: 2:40 - loss: 3.8139 - accuracy: 0.05 - ETA: 2:39 - loss: 3.8142 - accuracy: 0.05 - ETA: 2:39 - loss: 3.8133 - accuracy: 0.05 - ETA: 2:38 - loss: 3.8147 - accuracy: 0.05 - ETA: 2:37 - loss: 3.8144 - accuracy: 0.05 - ETA: 2:37 - loss: 3.8144 - accuracy: 0.05 - ETA: 2:36 - loss: 3.8149 - accuracy: 0.05 - ETA: 2:36 - loss: 3.8144 - accuracy: 0.05 - ETA: 2:35 - loss: 3.8123 - accuracy: 0.05 - ETA: 2:35 - loss: 3.8118 - accuracy: 0.05 - ETA: 2:34 - loss: 3.8109 - accuracy: 0.05 - ETA: 2:33 - loss: 3.8103 - accuracy: 0.05 - ETA: 2:33 - loss: 3.8107 - accuracy: 0.05 - ETA: 2:32 - loss: 3.8103 - accuracy: 0.05 - ETA: 2:32 - loss: 3.8113 - accuracy: 0.05 - ETA: 2:31 - loss: 3.8117 - accuracy: 0.05 - ETA: 2:30 - loss: 3.8116 - accuracy: 0.05 - ETA: 2:30 - loss: 3.8118 - accuracy: 0.05 - ETA: 2:29 - loss: 3.8108 - accuracy: 0.05 - ETA: 2:28 - loss: 3.8109 - accuracy: 0.05 - ETA: 2:28 - loss: 3.8101 - accuracy: 0.05 - ETA: 2:27 - loss: 3.8088 - accuracy: 0.05 - ETA: 2:26 - loss: 3.8083 - accuracy: 0.05 - ETA: 2:26 - loss: 3.8085 - accuracy: 0.05 - ETA: 2:25 - loss: 3.8081 - accuracy: 0.05 - ETA: 2:24 - loss: 3.8074 - accuracy: 0.05 - ETA: 2:24 - loss: 3.8072 - accuracy: 0.05 - ETA: 2:23 - loss: 3.8075 - accuracy: 0.05 - ETA: 2:22 - loss: 3.8080 - accuracy: 0.05 - ETA: 2:22 - loss: 3.8087 - accuracy: 0.05 - ETA: 2:21 - loss: 3.8089 - accuracy: 0.05 - ETA: 2:21 - loss: 3.8090 - accuracy: 0.05 - ETA: 2:20 - loss: 3.8095 - accuracy: 0.05 - ETA: 2:20 - loss: 3.8100 - accuracy: 0.05 - ETA: 2:19 - loss: 3.8089 - accuracy: 0.05 - ETA: 2:18 - loss: 3.8084 - accuracy: 0.05 - ETA: 2:18 - loss: 3.8081 - accuracy: 0.05 - ETA: 2:17 - loss: 3.8082 - accuracy: 0.05 - ETA: 2:16 - loss: 3.8081 - accuracy: 0.05 - ETA: 2:16 - loss: 3.8073 - accuracy: 0.05 - ETA: 2:15 - loss: 3.8075 - accuracy: 0.05 - ETA: 2:14 - loss: 3.8079 - accuracy: 0.05 - ETA: 2:14 - loss: 3.8086 - accuracy: 0.05 - ETA: 2:13 - loss: 3.8086 - accuracy: 0.05 - ETA: 2:13 - loss: 3.8086 - accuracy: 0.05 - ETA: 2:12 - loss: 3.8082 - accuracy: 0.05 - ETA: 2:12 - loss: 3.8082 - accuracy: 0.05 - ETA: 2:11 - loss: 3.8075 - accuracy: 0.05 - ETA: 2:10 - loss: 3.8071 - accuracy: 0.05 - ETA: 2:10 - loss: 3.8067 - accuracy: 0.05 - ETA: 2:09 - loss: 3.8070 - accuracy: 0.05 - ETA: 2:09 - loss: 3.8073 - accuracy: 0.05 - ETA: 2:08 - loss: 3.8075 - accuracy: 0.05 - ETA: 2:07 - loss: 3.8076 - accuracy: 0.05 - ETA: 2:07 - loss: 3.8075 - accuracy: 0.05 - ETA: 2:06 - loss: 3.8073 - accuracy: 0.05 - ETA: 2:05 - loss: 3.8079 - accuracy: 0.05 - ETA: 2:05 - loss: 3.8078 - accuracy: 0.05 - ETA: 2:04 - loss: 3.8083 - accuracy: 0.05 - ETA: 2:04 - loss: 3.8080 - accuracy: 0.05 - ETA: 2:03 - loss: 3.8078 - accuracy: 0.05 - ETA: 2:03 - loss: 3.8078 - accuracy: 0.05 - ETA: 2:02 - loss: 3.8083 - accuracy: 0.05 - ETA: 2:01 - loss: 3.8080 - accuracy: 0.05 - ETA: 2:01 - loss: 3.8082 - accuracy: 0.05 - ETA: 2:00 - loss: 3.8082 - accuracy: 0.05 - ETA: 1:59 - loss: 3.8077 - accuracy: 0.05 - ETA: 1:59 - loss: 3.8074 - accuracy: 0.05 - ETA: 1:58 - loss: 3.8073 - accuracy: 0.05 - ETA: 1:57 - loss: 3.8073 - accuracy: 0.05 - ETA: 1:57 - loss: 3.8070 - accuracy: 0.05 - ETA: 1:56 - loss: 3.8068 - accuracy: 0.05 - ETA: 1:55 - loss: 3.8070 - accuracy: 0.05 - ETA: 1:55 - loss: 3.8069 - accuracy: 0.05 - ETA: 1:54 - loss: 3.8074 - accuracy: 0.05 - ETA: 1:54 - loss: 3.8072 - accuracy: 0.05 - ETA: 1:53 - loss: 3.8074 - accuracy: 0.05 - ETA: 1:52 - loss: 3.8071 - accuracy: 0.05 - ETA: 1:52 - loss: 3.8069 - accuracy: 0.05 - ETA: 1:51 - loss: 3.8068 - accuracy: 0.05 - ETA: 1:50 - loss: 3.8070 - accuracy: 0.05 - ETA: 1:50 - loss: 3.8070 - accuracy: 0.05 - ETA: 1:49 - loss: 3.8063 - accuracy: 0.05 - ETA: 1:48 - loss: 3.8056 - accuracy: 0.05 - ETA: 1:48 - loss: 3.8050 - accuracy: 0.05 - ETA: 1:47 - loss: 3.8041 - accuracy: 0.05 - ETA: 1:47 - loss: 3.8049 - accuracy: 0.05 - ETA: 1:46 - loss: 3.8044 - accuracy: 0.05 - ETA: 1:46 - loss: 3.8041 - accuracy: 0.05 - ETA: 1:45 - loss: 3.8043 - accuracy: 0.05 - ETA: 1:44 - loss: 3.8042 - accuracy: 0.05 - ETA: 1:44 - loss: 3.8045 - accuracy: 0.05 - ETA: 1:43 - loss: 3.8046 - accuracy: 0.05 - ETA: 1:43 - loss: 3.8046 - accuracy: 0.05 - ETA: 1:42 - loss: 3.8048 - accuracy: 0.05 - ETA: 1:41 - loss: 3.8044 - accuracy: 0.05 - ETA: 1:41 - loss: 3.8044 - accuracy: 0.05 - ETA: 1:40 - loss: 3.8042 - accuracy: 0.05 - ETA: 1:40 - loss: 3.8043 - accuracy: 0.05 - ETA: 1:39 - loss: 3.8047 - accuracy: 0.05 - ETA: 1:38 - loss: 3.8044 - accuracy: 0.05 - ETA: 1:38 - loss: 3.8044 - accuracy: 0.05 - ETA: 1:37 - loss: 3.8042 - accuracy: 0.05 - ETA: 1:37 - loss: 3.8044 - accuracy: 0.05 - ETA: 1:36 - loss: 3.8042 - accuracy: 0.05 - ETA: 1:35 - loss: 3.8037 - accuracy: 0.05 - ETA: 1:35 - loss: 3.8039 - accuracy: 0.05 - ETA: 1:34 - loss: 3.8035 - accuracy: 0.05 - ETA: 1:34 - loss: 3.8029 - accuracy: 0.05 - ETA: 1:33 - loss: 3.8028 - accuracy: 0.05 - ETA: 1:32 - loss: 3.8022 - accuracy: 0.06 - ETA: 1:32 - loss: 3.8019 - accuracy: 0.05 - ETA: 1:31 - loss: 3.8014 - accuracy: 0.05 - ETA: 1:31 - loss: 3.8006 - accuracy: 0.05 - ETA: 1:30 - loss: 3.8002 - accuracy: 0.05 - ETA: 1:29 - loss: 3.8005 - accuracy: 0.05 - ETA: 1:29 - loss: 3.8008 - accuracy: 0.0596"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:28 - loss: 3.8004 - accuracy: 0.05 - ETA: 1:27 - loss: 3.8005 - accuracy: 0.05 - ETA: 1:27 - loss: 3.8000 - accuracy: 0.05 - ETA: 1:26 - loss: 3.8003 - accuracy: 0.05 - ETA: 1:26 - loss: 3.8001 - accuracy: 0.05 - ETA: 1:25 - loss: 3.7997 - accuracy: 0.06 - ETA: 1:24 - loss: 3.7992 - accuracy: 0.06 - ETA: 1:24 - loss: 3.7995 - accuracy: 0.06 - ETA: 1:23 - loss: 3.7990 - accuracy: 0.06 - ETA: 1:23 - loss: 3.7989 - accuracy: 0.06 - ETA: 1:22 - loss: 3.7985 - accuracy: 0.06 - ETA: 1:21 - loss: 3.7983 - accuracy: 0.06 - ETA: 1:21 - loss: 3.7982 - accuracy: 0.06 - ETA: 1:20 - loss: 3.7978 - accuracy: 0.06 - ETA: 1:19 - loss: 3.7974 - accuracy: 0.06 - ETA: 1:19 - loss: 3.7974 - accuracy: 0.06 - ETA: 1:18 - loss: 3.7970 - accuracy: 0.06 - ETA: 1:17 - loss: 3.7965 - accuracy: 0.06 - ETA: 1:17 - loss: 3.7963 - accuracy: 0.06 - ETA: 1:16 - loss: 3.7956 - accuracy: 0.06 - ETA: 1:16 - loss: 3.7955 - accuracy: 0.06 - ETA: 1:15 - loss: 3.7958 - accuracy: 0.05 - ETA: 1:14 - loss: 3.7960 - accuracy: 0.06 - ETA: 1:14 - loss: 3.7958 - accuracy: 0.06 - ETA: 1:13 - loss: 3.7959 - accuracy: 0.06 - ETA: 1:13 - loss: 3.7960 - accuracy: 0.06 - ETA: 1:12 - loss: 3.7957 - accuracy: 0.05 - ETA: 1:11 - loss: 3.7954 - accuracy: 0.06 - ETA: 1:11 - loss: 3.7949 - accuracy: 0.05 - ETA: 1:10 - loss: 3.7951 - accuracy: 0.05 - ETA: 1:09 - loss: 3.7944 - accuracy: 0.06 - ETA: 1:09 - loss: 3.7941 - accuracy: 0.06 - ETA: 1:08 - loss: 3.7938 - accuracy: 0.06 - ETA: 1:08 - loss: 3.7933 - accuracy: 0.06 - ETA: 1:07 - loss: 3.7928 - accuracy: 0.06 - ETA: 1:06 - loss: 3.7933 - accuracy: 0.06 - ETA: 1:06 - loss: 3.7932 - accuracy: 0.06 - ETA: 1:05 - loss: 3.7932 - accuracy: 0.06 - ETA: 1:04 - loss: 3.7936 - accuracy: 0.06 - ETA: 1:04 - loss: 3.7934 - accuracy: 0.05 - ETA: 1:03 - loss: 3.7929 - accuracy: 0.06 - ETA: 1:03 - loss: 3.7930 - accuracy: 0.06 - ETA: 1:02 - loss: 3.7932 - accuracy: 0.06 - ETA: 1:01 - loss: 3.7932 - accuracy: 0.06 - ETA: 1:01 - loss: 3.7928 - accuracy: 0.06 - ETA: 1:00 - loss: 3.7928 - accuracy: 0.06 - ETA: 59s - loss: 3.7928 - accuracy: 0.0600 - ETA: 59s - loss: 3.7929 - accuracy: 0.060 - ETA: 58s - loss: 3.7930 - accuracy: 0.059 - ETA: 58s - loss: 3.7931 - accuracy: 0.060 - ETA: 57s - loss: 3.7929 - accuracy: 0.059 - ETA: 56s - loss: 3.7932 - accuracy: 0.059 - ETA: 56s - loss: 3.7930 - accuracy: 0.059 - ETA: 55s - loss: 3.7933 - accuracy: 0.059 - ETA: 55s - loss: 3.7932 - accuracy: 0.059 - ETA: 54s - loss: 3.7930 - accuracy: 0.059 - ETA: 53s - loss: 3.7933 - accuracy: 0.059 - ETA: 53s - loss: 3.7933 - accuracy: 0.059 - ETA: 52s - loss: 3.7935 - accuracy: 0.059 - ETA: 51s - loss: 3.7927 - accuracy: 0.060 - ETA: 51s - loss: 3.7928 - accuracy: 0.060 - ETA: 50s - loss: 3.7930 - accuracy: 0.060 - ETA: 50s - loss: 3.7930 - accuracy: 0.060 - ETA: 49s - loss: 3.7929 - accuracy: 0.060 - ETA: 48s - loss: 3.7930 - accuracy: 0.060 - ETA: 48s - loss: 3.7931 - accuracy: 0.060 - ETA: 47s - loss: 3.7932 - accuracy: 0.060 - ETA: 47s - loss: 3.7932 - accuracy: 0.060 - ETA: 46s - loss: 3.7932 - accuracy: 0.060 - ETA: 45s - loss: 3.7934 - accuracy: 0.060 - ETA: 45s - loss: 3.7932 - accuracy: 0.059 - ETA: 44s - loss: 3.7929 - accuracy: 0.059 - ETA: 43s - loss: 3.7928 - accuracy: 0.060 - ETA: 43s - loss: 3.7926 - accuracy: 0.059 - ETA: 42s - loss: 3.7924 - accuracy: 0.059 - ETA: 42s - loss: 3.7923 - accuracy: 0.059 - ETA: 41s - loss: 3.7921 - accuracy: 0.059 - ETA: 40s - loss: 3.7920 - accuracy: 0.059 - ETA: 40s - loss: 3.7919 - accuracy: 0.059 - ETA: 39s - loss: 3.7920 - accuracy: 0.059 - ETA: 39s - loss: 3.7921 - accuracy: 0.060 - ETA: 38s - loss: 3.7922 - accuracy: 0.059 - ETA: 37s - loss: 3.7920 - accuracy: 0.059 - ETA: 37s - loss: 3.7917 - accuracy: 0.060 - ETA: 36s - loss: 3.7918 - accuracy: 0.059 - ETA: 36s - loss: 3.7918 - accuracy: 0.059 - ETA: 35s - loss: 3.7913 - accuracy: 0.059 - ETA: 34s - loss: 3.7912 - accuracy: 0.059 - ETA: 34s - loss: 3.7915 - accuracy: 0.059 - ETA: 33s - loss: 3.7911 - accuracy: 0.059 - ETA: 32s - loss: 3.7910 - accuracy: 0.059 - ETA: 32s - loss: 3.7911 - accuracy: 0.059 - ETA: 31s - loss: 3.7909 - accuracy: 0.059 - ETA: 31s - loss: 3.7905 - accuracy: 0.060 - ETA: 30s - loss: 3.7905 - accuracy: 0.060 - ETA: 29s - loss: 3.7907 - accuracy: 0.060 - ETA: 29s - loss: 3.7906 - accuracy: 0.060 - ETA: 28s - loss: 3.7905 - accuracy: 0.060 - ETA: 28s - loss: 3.7902 - accuracy: 0.060 - ETA: 27s - loss: 3.7899 - accuracy: 0.060 - ETA: 26s - loss: 3.7898 - accuracy: 0.060 - ETA: 26s - loss: 3.7900 - accuracy: 0.060 - ETA: 25s - loss: 3.7901 - accuracy: 0.060 - ETA: 25s - loss: 3.7896 - accuracy: 0.060 - ETA: 24s - loss: 3.7898 - accuracy: 0.060 - ETA: 23s - loss: 3.7899 - accuracy: 0.060 - ETA: 23s - loss: 3.7895 - accuracy: 0.060 - ETA: 22s - loss: 3.7893 - accuracy: 0.060 - ETA: 21s - loss: 3.7892 - accuracy: 0.060 - ETA: 21s - loss: 3.7893 - accuracy: 0.060 - ETA: 20s - loss: 3.7893 - accuracy: 0.060 - ETA: 20s - loss: 3.7893 - accuracy: 0.060 - ETA: 19s - loss: 3.7891 - accuracy: 0.060 - ETA: 18s - loss: 3.7886 - accuracy: 0.060 - ETA: 18s - loss: 3.7892 - accuracy: 0.060 - ETA: 17s - loss: 3.7890 - accuracy: 0.060 - ETA: 17s - loss: 3.7891 - accuracy: 0.060 - ETA: 16s - loss: 3.7889 - accuracy: 0.060 - ETA: 15s - loss: 3.7885 - accuracy: 0.060 - ETA: 15s - loss: 3.7883 - accuracy: 0.060 - ETA: 14s - loss: 3.7882 - accuracy: 0.060 - ETA: 14s - loss: 3.7881 - accuracy: 0.060 - ETA: 13s - loss: 3.7878 - accuracy: 0.061 - ETA: 12s - loss: 3.7877 - accuracy: 0.060 - ETA: 12s - loss: 3.7876 - accuracy: 0.060 - ETA: 11s - loss: 3.7876 - accuracy: 0.060 - ETA: 10s - loss: 3.7874 - accuracy: 0.060 - ETA: 10s - loss: 3.7872 - accuracy: 0.061 - ETA: 9s - loss: 3.7873 - accuracy: 0.060 - ETA: 9s - loss: 3.7871 - accuracy: 0.06 - ETA: 8s - loss: 3.7869 - accuracy: 0.06 - ETA: 7s - loss: 3.7863 - accuracy: 0.06 - ETA: 7s - loss: 3.7861 - accuracy: 0.06 - ETA: 6s - loss: 3.7862 - accuracy: 0.06 - ETA: 6s - loss: 3.7855 - accuracy: 0.06 - ETA: 5s - loss: 3.7853 - accuracy: 0.06 - ETA: 4s - loss: 3.7850 - accuracy: 0.06 - ETA: 4s - loss: 3.7850 - accuracy: 0.06 - ETA: 3s - loss: 3.7848 - accuracy: 0.06 - ETA: 3s - loss: 3.7848 - accuracy: 0.06 - ETA: 2s - loss: 3.7849 - accuracy: 0.06 - ETA: 1s - loss: 3.7850 - accuracy: 0.06 - ETA: 1s - loss: 3.7848 - accuracy: 0.06 - ETA: 0s - loss: 3.7846 - accuracy: 0.06 - ETA: 0s - loss: 3.7848 - accuracy: 0.06 - 213s 5ms/step - loss: 3.7848 - accuracy: 0.0614 - val_loss: 3.9571 - val_accuracy: 0.0257\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:08 - loss: 3.7722 - accuracy: 0.10 - ETA: 3:07 - loss: 3.7197 - accuracy: 0.10 - ETA: 3:08 - loss: 3.7546 - accuracy: 0.08 - ETA: 3:09 - loss: 3.7194 - accuracy: 0.09 - ETA: 3:12 - loss: 3.7201 - accuracy: 0.08 - ETA: 3:11 - loss: 3.7285 - accuracy: 0.08 - ETA: 3:11 - loss: 3.7404 - accuracy: 0.07 - ETA: 3:11 - loss: 3.7393 - accuracy: 0.07 - ETA: 3:11 - loss: 3.7469 - accuracy: 0.07 - ETA: 3:11 - loss: 3.7436 - accuracy: 0.07 - ETA: 3:10 - loss: 3.7419 - accuracy: 0.07 - ETA: 3:09 - loss: 3.7457 - accuracy: 0.07 - ETA: 3:08 - loss: 3.7433 - accuracy: 0.07 - ETA: 3:08 - loss: 3.7393 - accuracy: 0.07 - ETA: 3:08 - loss: 3.7405 - accuracy: 0.07 - ETA: 3:08 - loss: 3.7393 - accuracy: 0.07 - ETA: 3:08 - loss: 3.7355 - accuracy: 0.07 - ETA: 3:06 - loss: 3.7374 - accuracy: 0.07 - ETA: 3:06 - loss: 3.7394 - accuracy: 0.07 - ETA: 3:05 - loss: 3.7432 - accuracy: 0.07 - ETA: 3:04 - loss: 3.7458 - accuracy: 0.07 - ETA: 3:03 - loss: 3.7418 - accuracy: 0.07 - ETA: 3:02 - loss: 3.7384 - accuracy: 0.07 - ETA: 3:02 - loss: 3.7346 - accuracy: 0.07 - ETA: 3:01 - loss: 3.7335 - accuracy: 0.07 - ETA: 3:01 - loss: 3.7319 - accuracy: 0.07 - ETA: 3:00 - loss: 3.7310 - accuracy: 0.07 - ETA: 2:59 - loss: 3.7385 - accuracy: 0.06 - ETA: 2:58 - loss: 3.7377 - accuracy: 0.07 - ETA: 2:58 - loss: 3.7374 - accuracy: 0.07 - ETA: 2:57 - loss: 3.7371 - accuracy: 0.06 - ETA: 2:56 - loss: 3.7361 - accuracy: 0.06 - ETA: 2:55 - loss: 3.7317 - accuracy: 0.07 - ETA: 2:55 - loss: 3.7345 - accuracy: 0.07 - ETA: 2:54 - loss: 3.7319 - accuracy: 0.07 - ETA: 2:53 - loss: 3.7333 - accuracy: 0.07 - ETA: 2:53 - loss: 3.7322 - accuracy: 0.07 - ETA: 2:52 - loss: 3.7352 - accuracy: 0.07 - ETA: 2:51 - loss: 3.7348 - accuracy: 0.06 - ETA: 2:51 - loss: 3.7356 - accuracy: 0.06 - ETA: 2:50 - loss: 3.7349 - accuracy: 0.06 - ETA: 2:50 - loss: 3.7345 - accuracy: 0.06 - ETA: 2:49 - loss: 3.7359 - accuracy: 0.06 - ETA: 2:49 - loss: 3.7320 - accuracy: 0.06 - ETA: 2:48 - loss: 3.7324 - accuracy: 0.06 - ETA: 2:48 - loss: 3.7307 - accuracy: 0.06 - ETA: 2:47 - loss: 3.7299 - accuracy: 0.06 - ETA: 2:46 - loss: 3.7279 - accuracy: 0.06 - ETA: 2:46 - loss: 3.7287 - accuracy: 0.06 - ETA: 2:45 - loss: 3.7289 - accuracy: 0.06 - ETA: 2:44 - loss: 3.7294 - accuracy: 0.06 - ETA: 2:44 - loss: 3.7279 - accuracy: 0.06 - ETA: 2:43 - loss: 3.7298 - accuracy: 0.06 - ETA: 2:42 - loss: 3.7297 - accuracy: 0.06 - ETA: 2:42 - loss: 3.7288 - accuracy: 0.06 - ETA: 2:41 - loss: 3.7299 - accuracy: 0.06 - ETA: 2:40 - loss: 3.7305 - accuracy: 0.06 - ETA: 2:40 - loss: 3.7294 - accuracy: 0.06 - ETA: 2:39 - loss: 3.7288 - accuracy: 0.06 - ETA: 2:39 - loss: 3.7275 - accuracy: 0.06 - ETA: 2:38 - loss: 3.7259 - accuracy: 0.06 - ETA: 2:38 - loss: 3.7267 - accuracy: 0.06 - ETA: 2:37 - loss: 3.7261 - accuracy: 0.07 - ETA: 2:36 - loss: 3.7256 - accuracy: 0.07 - ETA: 2:36 - loss: 3.7241 - accuracy: 0.07 - ETA: 2:35 - loss: 3.7242 - accuracy: 0.07 - ETA: 2:34 - loss: 3.7231 - accuracy: 0.07 - ETA: 2:34 - loss: 3.7222 - accuracy: 0.07 - ETA: 2:33 - loss: 3.7217 - accuracy: 0.07 - ETA: 2:33 - loss: 3.7223 - accuracy: 0.07 - ETA: 2:33 - loss: 3.7227 - accuracy: 0.07 - ETA: 2:32 - loss: 3.7228 - accuracy: 0.07 - ETA: 2:31 - loss: 3.7232 - accuracy: 0.07 - ETA: 2:30 - loss: 3.7235 - accuracy: 0.07 - ETA: 2:30 - loss: 3.7230 - accuracy: 0.07 - ETA: 2:29 - loss: 3.7235 - accuracy: 0.07 - ETA: 2:29 - loss: 3.7241 - accuracy: 0.06 - ETA: 2:28 - loss: 3.7221 - accuracy: 0.07 - ETA: 2:27 - loss: 3.7207 - accuracy: 0.07 - ETA: 2:27 - loss: 3.7203 - accuracy: 0.07 - ETA: 2:26 - loss: 3.7205 - accuracy: 0.07 - ETA: 2:26 - loss: 3.7208 - accuracy: 0.07 - ETA: 2:25 - loss: 3.7205 - accuracy: 0.07 - ETA: 2:24 - loss: 3.7192 - accuracy: 0.07 - ETA: 2:24 - loss: 3.7178 - accuracy: 0.07 - ETA: 2:23 - loss: 3.7177 - accuracy: 0.07 - ETA: 2:22 - loss: 3.7173 - accuracy: 0.07 - ETA: 2:22 - loss: 3.7183 - accuracy: 0.07 - ETA: 2:21 - loss: 3.7194 - accuracy: 0.07 - ETA: 2:21 - loss: 3.7201 - accuracy: 0.07 - ETA: 2:20 - loss: 3.7206 - accuracy: 0.07 - ETA: 2:19 - loss: 3.7199 - accuracy: 0.07 - ETA: 2:19 - loss: 3.7204 - accuracy: 0.07 - ETA: 2:18 - loss: 3.7205 - accuracy: 0.07 - ETA: 2:18 - loss: 3.7204 - accuracy: 0.07 - ETA: 2:17 - loss: 3.7200 - accuracy: 0.07 - ETA: 2:16 - loss: 3.7207 - accuracy: 0.07 - ETA: 2:16 - loss: 3.7213 - accuracy: 0.07 - ETA: 2:16 - loss: 3.7228 - accuracy: 0.07 - ETA: 2:15 - loss: 3.7220 - accuracy: 0.07 - ETA: 2:14 - loss: 3.7226 - accuracy: 0.07 - ETA: 2:14 - loss: 3.7224 - accuracy: 0.07 - ETA: 2:13 - loss: 3.7220 - accuracy: 0.07 - ETA: 2:12 - loss: 3.7224 - accuracy: 0.07 - ETA: 2:12 - loss: 3.7218 - accuracy: 0.07 - ETA: 2:11 - loss: 3.7220 - accuracy: 0.07 - ETA: 2:11 - loss: 3.7221 - accuracy: 0.07 - ETA: 2:10 - loss: 3.7224 - accuracy: 0.07 - ETA: 2:09 - loss: 3.7222 - accuracy: 0.07 - ETA: 2:09 - loss: 3.7230 - accuracy: 0.07 - ETA: 2:08 - loss: 3.7229 - accuracy: 0.07 - ETA: 2:08 - loss: 3.7226 - accuracy: 0.07 - ETA: 2:07 - loss: 3.7225 - accuracy: 0.07 - ETA: 2:07 - loss: 3.7221 - accuracy: 0.07 - ETA: 2:06 - loss: 3.7209 - accuracy: 0.07 - ETA: 2:05 - loss: 3.7198 - accuracy: 0.07 - ETA: 2:05 - loss: 3.7192 - accuracy: 0.07 - ETA: 2:04 - loss: 3.7196 - accuracy: 0.07 - ETA: 2:04 - loss: 3.7190 - accuracy: 0.07 - ETA: 2:03 - loss: 3.7180 - accuracy: 0.07 - ETA: 2:03 - loss: 3.7187 - accuracy: 0.07 - ETA: 2:02 - loss: 3.7189 - accuracy: 0.07 - ETA: 2:02 - loss: 3.7193 - accuracy: 0.07 - ETA: 2:01 - loss: 3.7194 - accuracy: 0.07 - ETA: 2:01 - loss: 3.7184 - accuracy: 0.07 - ETA: 2:00 - loss: 3.7179 - accuracy: 0.07 - ETA: 2:00 - loss: 3.7188 - accuracy: 0.07 - ETA: 1:59 - loss: 3.7185 - accuracy: 0.07 - ETA: 1:58 - loss: 3.7185 - accuracy: 0.07 - ETA: 1:58 - loss: 3.7185 - accuracy: 0.07 - ETA: 1:57 - loss: 3.7176 - accuracy: 0.07 - ETA: 1:57 - loss: 3.7182 - accuracy: 0.07 - ETA: 1:56 - loss: 3.7185 - accuracy: 0.07 - ETA: 1:55 - loss: 3.7187 - accuracy: 0.07 - ETA: 1:55 - loss: 3.7189 - accuracy: 0.07 - ETA: 1:54 - loss: 3.7181 - accuracy: 0.07 - ETA: 1:54 - loss: 3.7179 - accuracy: 0.07 - ETA: 1:53 - loss: 3.7178 - accuracy: 0.07 - ETA: 1:52 - loss: 3.7181 - accuracy: 0.07 - ETA: 1:52 - loss: 3.7173 - accuracy: 0.07 - ETA: 1:51 - loss: 3.7173 - accuracy: 0.07 - ETA: 1:51 - loss: 3.7179 - accuracy: 0.07 - ETA: 1:50 - loss: 3.7171 - accuracy: 0.07 - ETA: 1:49 - loss: 3.7176 - accuracy: 0.07 - ETA: 1:49 - loss: 3.7173 - accuracy: 0.07 - ETA: 1:48 - loss: 3.7174 - accuracy: 0.07 - ETA: 1:48 - loss: 3.7175 - accuracy: 0.07 - ETA: 1:47 - loss: 3.7179 - accuracy: 0.07 - ETA: 1:46 - loss: 3.7169 - accuracy: 0.07 - ETA: 1:46 - loss: 3.7168 - accuracy: 0.07 - ETA: 1:45 - loss: 3.7172 - accuracy: 0.07 - ETA: 1:45 - loss: 3.7168 - accuracy: 0.07 - ETA: 1:44 - loss: 3.7173 - accuracy: 0.07 - ETA: 1:44 - loss: 3.7165 - accuracy: 0.07 - ETA: 1:43 - loss: 3.7168 - accuracy: 0.07 - ETA: 1:43 - loss: 3.7170 - accuracy: 0.07 - ETA: 1:42 - loss: 3.7177 - accuracy: 0.07 - ETA: 1:41 - loss: 3.7179 - accuracy: 0.07 - ETA: 1:41 - loss: 3.7186 - accuracy: 0.07 - ETA: 1:40 - loss: 3.7182 - accuracy: 0.07 - ETA: 1:40 - loss: 3.7181 - accuracy: 0.07 - ETA: 1:39 - loss: 3.7176 - accuracy: 0.07 - ETA: 1:38 - loss: 3.7174 - accuracy: 0.07 - ETA: 1:38 - loss: 3.7172 - accuracy: 0.07 - ETA: 1:37 - loss: 3.7164 - accuracy: 0.07 - ETA: 1:37 - loss: 3.7168 - accuracy: 0.07 - ETA: 1:36 - loss: 3.7171 - accuracy: 0.07 - ETA: 1:35 - loss: 3.7173 - accuracy: 0.07 - ETA: 1:35 - loss: 3.7162 - accuracy: 0.07 - ETA: 1:34 - loss: 3.7160 - accuracy: 0.07 - ETA: 1:34 - loss: 3.7153 - accuracy: 0.07 - ETA: 1:33 - loss: 3.7151 - accuracy: 0.07 - ETA: 1:32 - loss: 3.7150 - accuracy: 0.07 - ETA: 1:32 - loss: 3.7155 - accuracy: 0.07 - ETA: 1:31 - loss: 3.7150 - accuracy: 0.07 - ETA: 1:31 - loss: 3.7148 - accuracy: 0.07 - ETA: 1:30 - loss: 3.7146 - accuracy: 0.07 - ETA: 1:30 - loss: 3.7145 - accuracy: 0.07 - ETA: 1:29 - loss: 3.7139 - accuracy: 0.07 - ETA: 1:28 - loss: 3.7138 - accuracy: 0.07 - ETA: 1:28 - loss: 3.7140 - accuracy: 0.07 - ETA: 1:27 - loss: 3.7137 - accuracy: 0.07 - ETA: 1:27 - loss: 3.7136 - accuracy: 0.07 - ETA: 1:26 - loss: 3.7137 - accuracy: 0.07 - ETA: 1:25 - loss: 3.7142 - accuracy: 0.07 - ETA: 1:25 - loss: 3.7133 - accuracy: 0.0720"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.7139 - accuracy: 0.07 - ETA: 1:24 - loss: 3.7135 - accuracy: 0.07 - ETA: 1:23 - loss: 3.7129 - accuracy: 0.07 - ETA: 1:22 - loss: 3.7130 - accuracy: 0.07 - ETA: 1:22 - loss: 3.7124 - accuracy: 0.07 - ETA: 1:21 - loss: 3.7121 - accuracy: 0.07 - ETA: 1:21 - loss: 3.7116 - accuracy: 0.07 - ETA: 1:20 - loss: 3.7109 - accuracy: 0.07 - ETA: 1:19 - loss: 3.7113 - accuracy: 0.07 - ETA: 1:19 - loss: 3.7113 - accuracy: 0.07 - ETA: 1:18 - loss: 3.7108 - accuracy: 0.07 - ETA: 1:18 - loss: 3.7109 - accuracy: 0.07 - ETA: 1:17 - loss: 3.7109 - accuracy: 0.07 - ETA: 1:17 - loss: 3.7111 - accuracy: 0.07 - ETA: 1:16 - loss: 3.7102 - accuracy: 0.07 - ETA: 1:15 - loss: 3.7098 - accuracy: 0.07 - ETA: 1:15 - loss: 3.7095 - accuracy: 0.07 - ETA: 1:14 - loss: 3.7093 - accuracy: 0.07 - ETA: 1:14 - loss: 3.7092 - accuracy: 0.07 - ETA: 1:13 - loss: 3.7090 - accuracy: 0.07 - ETA: 1:12 - loss: 3.7085 - accuracy: 0.07 - ETA: 1:12 - loss: 3.7081 - accuracy: 0.07 - ETA: 1:11 - loss: 3.7083 - accuracy: 0.07 - ETA: 1:11 - loss: 3.7075 - accuracy: 0.07 - ETA: 1:10 - loss: 3.7078 - accuracy: 0.07 - ETA: 1:09 - loss: 3.7077 - accuracy: 0.07 - ETA: 1:09 - loss: 3.7079 - accuracy: 0.07 - ETA: 1:08 - loss: 3.7081 - accuracy: 0.07 - ETA: 1:08 - loss: 3.7082 - accuracy: 0.07 - ETA: 1:07 - loss: 3.7072 - accuracy: 0.07 - ETA: 1:07 - loss: 3.7071 - accuracy: 0.07 - ETA: 1:06 - loss: 3.7066 - accuracy: 0.07 - ETA: 1:05 - loss: 3.7065 - accuracy: 0.07 - ETA: 1:05 - loss: 3.7063 - accuracy: 0.07 - ETA: 1:04 - loss: 3.7059 - accuracy: 0.07 - ETA: 1:04 - loss: 3.7061 - accuracy: 0.07 - ETA: 1:03 - loss: 3.7054 - accuracy: 0.07 - ETA: 1:02 - loss: 3.7054 - accuracy: 0.07 - ETA: 1:02 - loss: 3.7050 - accuracy: 0.07 - ETA: 1:01 - loss: 3.7043 - accuracy: 0.07 - ETA: 1:01 - loss: 3.7039 - accuracy: 0.07 - ETA: 1:00 - loss: 3.7040 - accuracy: 0.07 - ETA: 59s - loss: 3.7041 - accuracy: 0.0744 - ETA: 59s - loss: 3.7041 - accuracy: 0.074 - ETA: 58s - loss: 3.7043 - accuracy: 0.074 - ETA: 58s - loss: 3.7046 - accuracy: 0.074 - ETA: 57s - loss: 3.7040 - accuracy: 0.074 - ETA: 57s - loss: 3.7042 - accuracy: 0.074 - ETA: 56s - loss: 3.7039 - accuracy: 0.074 - ETA: 55s - loss: 3.7042 - accuracy: 0.074 - ETA: 55s - loss: 3.7043 - accuracy: 0.074 - ETA: 54s - loss: 3.7042 - accuracy: 0.074 - ETA: 54s - loss: 3.7041 - accuracy: 0.074 - ETA: 53s - loss: 3.7037 - accuracy: 0.074 - ETA: 52s - loss: 3.7036 - accuracy: 0.074 - ETA: 52s - loss: 3.7039 - accuracy: 0.074 - ETA: 51s - loss: 3.7037 - accuracy: 0.074 - ETA: 51s - loss: 3.7038 - accuracy: 0.074 - ETA: 50s - loss: 3.7040 - accuracy: 0.074 - ETA: 49s - loss: 3.7040 - accuracy: 0.074 - ETA: 49s - loss: 3.7035 - accuracy: 0.074 - ETA: 48s - loss: 3.7036 - accuracy: 0.074 - ETA: 48s - loss: 3.7034 - accuracy: 0.074 - ETA: 47s - loss: 3.7034 - accuracy: 0.074 - ETA: 47s - loss: 3.7037 - accuracy: 0.074 - ETA: 46s - loss: 3.7040 - accuracy: 0.074 - ETA: 45s - loss: 3.7035 - accuracy: 0.074 - ETA: 45s - loss: 3.7037 - accuracy: 0.074 - ETA: 44s - loss: 3.7038 - accuracy: 0.074 - ETA: 44s - loss: 3.7039 - accuracy: 0.074 - ETA: 43s - loss: 3.7036 - accuracy: 0.074 - ETA: 42s - loss: 3.7026 - accuracy: 0.075 - ETA: 42s - loss: 3.7028 - accuracy: 0.075 - ETA: 41s - loss: 3.7031 - accuracy: 0.075 - ETA: 41s - loss: 3.7026 - accuracy: 0.075 - ETA: 40s - loss: 3.7022 - accuracy: 0.075 - ETA: 39s - loss: 3.7021 - accuracy: 0.075 - ETA: 39s - loss: 3.7021 - accuracy: 0.075 - ETA: 38s - loss: 3.7025 - accuracy: 0.075 - ETA: 38s - loss: 3.7029 - accuracy: 0.074 - ETA: 37s - loss: 3.7031 - accuracy: 0.074 - ETA: 37s - loss: 3.7030 - accuracy: 0.074 - ETA: 36s - loss: 3.7027 - accuracy: 0.074 - ETA: 35s - loss: 3.7027 - accuracy: 0.074 - ETA: 35s - loss: 3.7024 - accuracy: 0.075 - ETA: 34s - loss: 3.7021 - accuracy: 0.075 - ETA: 34s - loss: 3.7020 - accuracy: 0.075 - ETA: 33s - loss: 3.7018 - accuracy: 0.075 - ETA: 32s - loss: 3.7019 - accuracy: 0.075 - ETA: 32s - loss: 3.7019 - accuracy: 0.075 - ETA: 31s - loss: 3.7018 - accuracy: 0.075 - ETA: 31s - loss: 3.7018 - accuracy: 0.075 - ETA: 30s - loss: 3.7020 - accuracy: 0.074 - ETA: 30s - loss: 3.7020 - accuracy: 0.074 - ETA: 29s - loss: 3.7021 - accuracy: 0.074 - ETA: 28s - loss: 3.7019 - accuracy: 0.074 - ETA: 28s - loss: 3.7023 - accuracy: 0.074 - ETA: 27s - loss: 3.7020 - accuracy: 0.074 - ETA: 27s - loss: 3.7020 - accuracy: 0.074 - ETA: 26s - loss: 3.7016 - accuracy: 0.074 - ETA: 25s - loss: 3.7016 - accuracy: 0.074 - ETA: 25s - loss: 3.7012 - accuracy: 0.074 - ETA: 24s - loss: 3.7011 - accuracy: 0.074 - ETA: 24s - loss: 3.7014 - accuracy: 0.074 - ETA: 23s - loss: 3.7011 - accuracy: 0.074 - ETA: 22s - loss: 3.7009 - accuracy: 0.075 - ETA: 22s - loss: 3.7007 - accuracy: 0.075 - ETA: 21s - loss: 3.7002 - accuracy: 0.075 - ETA: 21s - loss: 3.7002 - accuracy: 0.074 - ETA: 20s - loss: 3.7005 - accuracy: 0.074 - ETA: 20s - loss: 3.7005 - accuracy: 0.074 - ETA: 19s - loss: 3.7007 - accuracy: 0.074 - ETA: 18s - loss: 3.7004 - accuracy: 0.075 - ETA: 18s - loss: 3.7003 - accuracy: 0.074 - ETA: 17s - loss: 3.7003 - accuracy: 0.075 - ETA: 17s - loss: 3.7000 - accuracy: 0.075 - ETA: 16s - loss: 3.7000 - accuracy: 0.075 - ETA: 15s - loss: 3.7003 - accuracy: 0.075 - ETA: 15s - loss: 3.6999 - accuracy: 0.075 - ETA: 14s - loss: 3.7000 - accuracy: 0.075 - ETA: 14s - loss: 3.6998 - accuracy: 0.075 - ETA: 13s - loss: 3.6999 - accuracy: 0.075 - ETA: 12s - loss: 3.6999 - accuracy: 0.075 - ETA: 12s - loss: 3.6997 - accuracy: 0.075 - ETA: 11s - loss: 3.6996 - accuracy: 0.075 - ETA: 11s - loss: 3.6995 - accuracy: 0.075 - ETA: 10s - loss: 3.6993 - accuracy: 0.075 - ETA: 10s - loss: 3.6990 - accuracy: 0.075 - ETA: 9s - loss: 3.6991 - accuracy: 0.075 - ETA: 8s - loss: 3.6992 - accuracy: 0.07 - ETA: 8s - loss: 3.6992 - accuracy: 0.07 - ETA: 7s - loss: 3.6992 - accuracy: 0.07 - ETA: 7s - loss: 3.6987 - accuracy: 0.07 - ETA: 6s - loss: 3.6990 - accuracy: 0.07 - ETA: 5s - loss: 3.6985 - accuracy: 0.07 - ETA: 5s - loss: 3.6984 - accuracy: 0.07 - ETA: 4s - loss: 3.6982 - accuracy: 0.07 - ETA: 4s - loss: 3.6984 - accuracy: 0.07 - ETA: 3s - loss: 3.6984 - accuracy: 0.07 - ETA: 2s - loss: 3.6983 - accuracy: 0.07 - ETA: 2s - loss: 3.6981 - accuracy: 0.07 - ETA: 1s - loss: 3.6983 - accuracy: 0.07 - ETA: 1s - loss: 3.6981 - accuracy: 0.07 - ETA: 0s - loss: 3.6978 - accuracy: 0.07 - ETA: 0s - loss: 3.6978 - accuracy: 0.07 - 207s 5ms/step - loss: 3.6978 - accuracy: 0.0760 - val_loss: 3.9297 - val_accuracy: 0.0197\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:07 - loss: 3.6628 - accuracy: 0.09 - ETA: 3:04 - loss: 3.6615 - accuracy: 0.09 - ETA: 3:04 - loss: 3.6586 - accuracy: 0.09 - ETA: 3:05 - loss: 3.6569 - accuracy: 0.09 - ETA: 3:03 - loss: 3.6557 - accuracy: 0.09 - ETA: 3:02 - loss: 3.6815 - accuracy: 0.08 - ETA: 3:02 - loss: 3.6829 - accuracy: 0.08 - ETA: 3:02 - loss: 3.6928 - accuracy: 0.08 - ETA: 3:01 - loss: 3.6967 - accuracy: 0.08 - ETA: 3:01 - loss: 3.6795 - accuracy: 0.08 - ETA: 3:01 - loss: 3.6704 - accuracy: 0.08 - ETA: 3:00 - loss: 3.6705 - accuracy: 0.09 - ETA: 3:01 - loss: 3.6531 - accuracy: 0.09 - ETA: 3:02 - loss: 3.6566 - accuracy: 0.09 - ETA: 3:02 - loss: 3.6470 - accuracy: 0.09 - ETA: 3:03 - loss: 3.6401 - accuracy: 0.09 - ETA: 3:02 - loss: 3.6435 - accuracy: 0.09 - ETA: 3:01 - loss: 3.6498 - accuracy: 0.09 - ETA: 3:01 - loss: 3.6502 - accuracy: 0.09 - ETA: 3:00 - loss: 3.6489 - accuracy: 0.09 - ETA: 2:59 - loss: 3.6504 - accuracy: 0.09 - ETA: 2:59 - loss: 3.6499 - accuracy: 0.09 - ETA: 2:58 - loss: 3.6505 - accuracy: 0.09 - ETA: 2:58 - loss: 3.6509 - accuracy: 0.09 - ETA: 2:57 - loss: 3.6516 - accuracy: 0.09 - ETA: 2:56 - loss: 3.6496 - accuracy: 0.09 - ETA: 2:56 - loss: 3.6500 - accuracy: 0.09 - ETA: 2:55 - loss: 3.6494 - accuracy: 0.09 - ETA: 2:55 - loss: 3.6500 - accuracy: 0.09 - ETA: 2:55 - loss: 3.6514 - accuracy: 0.09 - ETA: 2:54 - loss: 3.6513 - accuracy: 0.09 - ETA: 2:53 - loss: 3.6496 - accuracy: 0.09 - ETA: 2:53 - loss: 3.6467 - accuracy: 0.09 - ETA: 2:52 - loss: 3.6508 - accuracy: 0.09 - ETA: 2:52 - loss: 3.6519 - accuracy: 0.09 - ETA: 2:51 - loss: 3.6523 - accuracy: 0.09 - ETA: 2:50 - loss: 3.6520 - accuracy: 0.09 - ETA: 2:50 - loss: 3.6509 - accuracy: 0.09 - ETA: 2:49 - loss: 3.6509 - accuracy: 0.09 - ETA: 2:49 - loss: 3.6524 - accuracy: 0.09 - ETA: 2:49 - loss: 3.6552 - accuracy: 0.09 - ETA: 2:49 - loss: 3.6571 - accuracy: 0.09 - ETA: 2:48 - loss: 3.6560 - accuracy: 0.09 - ETA: 2:48 - loss: 3.6562 - accuracy: 0.09 - ETA: 2:47 - loss: 3.6575 - accuracy: 0.09 - ETA: 2:46 - loss: 3.6590 - accuracy: 0.08 - ETA: 2:46 - loss: 3.6570 - accuracy: 0.08 - ETA: 2:45 - loss: 3.6557 - accuracy: 0.08 - ETA: 2:44 - loss: 3.6536 - accuracy: 0.08 - ETA: 2:44 - loss: 3.6521 - accuracy: 0.09 - ETA: 2:43 - loss: 3.6516 - accuracy: 0.09 - ETA: 2:43 - loss: 3.6513 - accuracy: 0.09 - ETA: 2:42 - loss: 3.6512 - accuracy: 0.09 - ETA: 2:42 - loss: 3.6518 - accuracy: 0.08 - ETA: 2:41 - loss: 3.6488 - accuracy: 0.09 - ETA: 2:40 - loss: 3.6492 - accuracy: 0.09 - ETA: 2:40 - loss: 3.6481 - accuracy: 0.09 - ETA: 2:39 - loss: 3.6506 - accuracy: 0.09 - ETA: 2:39 - loss: 3.6518 - accuracy: 0.09 - ETA: 2:38 - loss: 3.6520 - accuracy: 0.09 - ETA: 2:37 - loss: 3.6518 - accuracy: 0.09 - ETA: 2:37 - loss: 3.6515 - accuracy: 0.09 - ETA: 2:36 - loss: 3.6497 - accuracy: 0.09 - ETA: 2:35 - loss: 3.6481 - accuracy: 0.09 - ETA: 2:35 - loss: 3.6468 - accuracy: 0.09 - ETA: 2:34 - loss: 3.6466 - accuracy: 0.09 - ETA: 2:34 - loss: 3.6463 - accuracy: 0.09 - ETA: 2:34 - loss: 3.6465 - accuracy: 0.09 - ETA: 2:33 - loss: 3.6445 - accuracy: 0.09 - ETA: 2:33 - loss: 3.6450 - accuracy: 0.09 - ETA: 2:32 - loss: 3.6444 - accuracy: 0.09 - ETA: 2:31 - loss: 3.6438 - accuracy: 0.09 - ETA: 2:31 - loss: 3.6464 - accuracy: 0.09 - ETA: 2:30 - loss: 3.6450 - accuracy: 0.09 - ETA: 2:30 - loss: 3.6454 - accuracy: 0.09 - ETA: 2:29 - loss: 3.6466 - accuracy: 0.09 - ETA: 2:28 - loss: 3.6458 - accuracy: 0.09 - ETA: 2:28 - loss: 3.6475 - accuracy: 0.09 - ETA: 2:27 - loss: 3.6463 - accuracy: 0.09 - ETA: 2:26 - loss: 3.6462 - accuracy: 0.09 - ETA: 2:26 - loss: 3.6443 - accuracy: 0.09 - ETA: 2:25 - loss: 3.6450 - accuracy: 0.09 - ETA: 2:25 - loss: 3.6437 - accuracy: 0.09 - ETA: 2:24 - loss: 3.6431 - accuracy: 0.09 - ETA: 2:23 - loss: 3.6421 - accuracy: 0.09 - ETA: 2:23 - loss: 3.6421 - accuracy: 0.09 - ETA: 2:22 - loss: 3.6436 - accuracy: 0.09 - ETA: 2:22 - loss: 3.6439 - accuracy: 0.09 - ETA: 2:21 - loss: 3.6453 - accuracy: 0.09 - ETA: 2:20 - loss: 3.6449 - accuracy: 0.09 - ETA: 2:20 - loss: 3.6469 - accuracy: 0.09 - ETA: 2:19 - loss: 3.6471 - accuracy: 0.09 - ETA: 2:19 - loss: 3.6461 - accuracy: 0.09 - ETA: 2:18 - loss: 3.6467 - accuracy: 0.09 - ETA: 2:18 - loss: 3.6470 - accuracy: 0.09 - ETA: 2:17 - loss: 3.6465 - accuracy: 0.09 - ETA: 2:17 - loss: 3.6471 - accuracy: 0.09 - ETA: 2:16 - loss: 3.6469 - accuracy: 0.09 - ETA: 2:16 - loss: 3.6467 - accuracy: 0.09 - ETA: 2:15 - loss: 3.6454 - accuracy: 0.09 - ETA: 2:14 - loss: 3.6444 - accuracy: 0.09 - ETA: 2:14 - loss: 3.6450 - accuracy: 0.09 - ETA: 2:13 - loss: 3.6434 - accuracy: 0.09 - ETA: 2:13 - loss: 3.6447 - accuracy: 0.09 - ETA: 2:12 - loss: 3.6446 - accuracy: 0.09 - ETA: 2:11 - loss: 3.6451 - accuracy: 0.09 - ETA: 2:11 - loss: 3.6447 - accuracy: 0.09 - ETA: 2:10 - loss: 3.6439 - accuracy: 0.09 - ETA: 2:10 - loss: 3.6433 - accuracy: 0.09 - ETA: 2:09 - loss: 3.6428 - accuracy: 0.09 - ETA: 2:08 - loss: 3.6427 - accuracy: 0.09 - ETA: 2:08 - loss: 3.6423 - accuracy: 0.09 - ETA: 2:07 - loss: 3.6426 - accuracy: 0.09 - ETA: 2:07 - loss: 3.6415 - accuracy: 0.09 - ETA: 2:06 - loss: 3.6413 - accuracy: 0.09 - ETA: 2:05 - loss: 3.6416 - accuracy: 0.09 - ETA: 2:05 - loss: 3.6432 - accuracy: 0.09 - ETA: 2:04 - loss: 3.6431 - accuracy: 0.09 - ETA: 2:04 - loss: 3.6443 - accuracy: 0.09 - ETA: 2:03 - loss: 3.6445 - accuracy: 0.09 - ETA: 2:02 - loss: 3.6446 - accuracy: 0.09 - ETA: 2:02 - loss: 3.6447 - accuracy: 0.09 - ETA: 2:01 - loss: 3.6439 - accuracy: 0.09 - ETA: 2:01 - loss: 3.6444 - accuracy: 0.09 - ETA: 2:00 - loss: 3.6433 - accuracy: 0.09 - ETA: 2:00 - loss: 3.6436 - accuracy: 0.09 - ETA: 1:59 - loss: 3.6438 - accuracy: 0.09 - ETA: 1:58 - loss: 3.6443 - accuracy: 0.09 - ETA: 1:58 - loss: 3.6439 - accuracy: 0.09 - ETA: 1:57 - loss: 3.6428 - accuracy: 0.09 - ETA: 1:57 - loss: 3.6419 - accuracy: 0.09 - ETA: 1:56 - loss: 3.6407 - accuracy: 0.09 - ETA: 1:55 - loss: 3.6401 - accuracy: 0.09 - ETA: 1:55 - loss: 3.6400 - accuracy: 0.09 - ETA: 1:54 - loss: 3.6400 - accuracy: 0.09 - ETA: 1:54 - loss: 3.6402 - accuracy: 0.09 - ETA: 1:53 - loss: 3.6404 - accuracy: 0.09 - ETA: 1:52 - loss: 3.6410 - accuracy: 0.09 - ETA: 1:52 - loss: 3.6402 - accuracy: 0.09 - ETA: 1:51 - loss: 3.6412 - accuracy: 0.09 - ETA: 1:51 - loss: 3.6402 - accuracy: 0.09 - ETA: 1:50 - loss: 3.6396 - accuracy: 0.09 - ETA: 1:49 - loss: 3.6403 - accuracy: 0.09 - ETA: 1:49 - loss: 3.6390 - accuracy: 0.09 - ETA: 1:48 - loss: 3.6383 - accuracy: 0.09 - ETA: 1:48 - loss: 3.6373 - accuracy: 0.09 - ETA: 1:47 - loss: 3.6372 - accuracy: 0.09 - ETA: 1:47 - loss: 3.6374 - accuracy: 0.09 - ETA: 1:46 - loss: 3.6380 - accuracy: 0.09 - ETA: 1:45 - loss: 3.6382 - accuracy: 0.09 - ETA: 1:45 - loss: 3.6380 - accuracy: 0.09 - ETA: 1:44 - loss: 3.6374 - accuracy: 0.09 - ETA: 1:44 - loss: 3.6377 - accuracy: 0.09 - ETA: 1:43 - loss: 3.6379 - accuracy: 0.09 - ETA: 1:43 - loss: 3.6386 - accuracy: 0.09 - ETA: 1:42 - loss: 3.6387 - accuracy: 0.09 - ETA: 1:41 - loss: 3.6386 - accuracy: 0.09 - ETA: 1:41 - loss: 3.6391 - accuracy: 0.09 - ETA: 1:40 - loss: 3.6389 - accuracy: 0.09 - ETA: 1:40 - loss: 3.6386 - accuracy: 0.09 - ETA: 1:39 - loss: 3.6385 - accuracy: 0.09 - ETA: 1:39 - loss: 3.6384 - accuracy: 0.09 - ETA: 1:38 - loss: 3.6381 - accuracy: 0.09 - ETA: 1:37 - loss: 3.6386 - accuracy: 0.09 - ETA: 1:37 - loss: 3.6382 - accuracy: 0.09 - ETA: 1:36 - loss: 3.6377 - accuracy: 0.09 - ETA: 1:36 - loss: 3.6384 - accuracy: 0.09 - ETA: 1:35 - loss: 3.6384 - accuracy: 0.09 - ETA: 1:34 - loss: 3.6380 - accuracy: 0.09 - ETA: 1:34 - loss: 3.6386 - accuracy: 0.09 - ETA: 1:33 - loss: 3.6390 - accuracy: 0.09 - ETA: 1:33 - loss: 3.6393 - accuracy: 0.09 - ETA: 1:32 - loss: 3.6397 - accuracy: 0.09 - ETA: 1:31 - loss: 3.6391 - accuracy: 0.09 - ETA: 1:31 - loss: 3.6393 - accuracy: 0.09 - ETA: 1:30 - loss: 3.6388 - accuracy: 0.09 - ETA: 1:30 - loss: 3.6387 - accuracy: 0.09 - ETA: 1:29 - loss: 3.6385 - accuracy: 0.09 - ETA: 1:29 - loss: 3.6382 - accuracy: 0.09 - ETA: 1:28 - loss: 3.6379 - accuracy: 0.09 - ETA: 1:27 - loss: 3.6387 - accuracy: 0.09 - ETA: 1:27 - loss: 3.6389 - accuracy: 0.09 - ETA: 1:26 - loss: 3.6386 - accuracy: 0.09 - ETA: 1:26 - loss: 3.6385 - accuracy: 0.09 - ETA: 1:25 - loss: 3.6384 - accuracy: 0.09 - ETA: 1:24 - loss: 3.6387 - accuracy: 0.0914"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.6390 - accuracy: 0.09 - ETA: 1:23 - loss: 3.6391 - accuracy: 0.09 - ETA: 1:23 - loss: 3.6391 - accuracy: 0.09 - ETA: 1:22 - loss: 3.6389 - accuracy: 0.09 - ETA: 1:22 - loss: 3.6391 - accuracy: 0.09 - ETA: 1:21 - loss: 3.6391 - accuracy: 0.09 - ETA: 1:20 - loss: 3.6389 - accuracy: 0.09 - ETA: 1:20 - loss: 3.6384 - accuracy: 0.09 - ETA: 1:19 - loss: 3.6383 - accuracy: 0.09 - ETA: 1:19 - loss: 3.6376 - accuracy: 0.09 - ETA: 1:18 - loss: 3.6371 - accuracy: 0.09 - ETA: 1:17 - loss: 3.6381 - accuracy: 0.09 - ETA: 1:17 - loss: 3.6374 - accuracy: 0.09 - ETA: 1:16 - loss: 3.6375 - accuracy: 0.09 - ETA: 1:16 - loss: 3.6369 - accuracy: 0.09 - ETA: 1:15 - loss: 3.6363 - accuracy: 0.09 - ETA: 1:14 - loss: 3.6366 - accuracy: 0.09 - ETA: 1:14 - loss: 3.6369 - accuracy: 0.09 - ETA: 1:13 - loss: 3.6367 - accuracy: 0.09 - ETA: 1:13 - loss: 3.6366 - accuracy: 0.09 - ETA: 1:12 - loss: 3.6358 - accuracy: 0.09 - ETA: 1:12 - loss: 3.6353 - accuracy: 0.09 - ETA: 1:11 - loss: 3.6351 - accuracy: 0.09 - ETA: 1:10 - loss: 3.6350 - accuracy: 0.09 - ETA: 1:10 - loss: 3.6345 - accuracy: 0.09 - ETA: 1:09 - loss: 3.6336 - accuracy: 0.09 - ETA: 1:09 - loss: 3.6340 - accuracy: 0.09 - ETA: 1:08 - loss: 3.6339 - accuracy: 0.09 - ETA: 1:08 - loss: 3.6342 - accuracy: 0.09 - ETA: 1:07 - loss: 3.6344 - accuracy: 0.09 - ETA: 1:06 - loss: 3.6341 - accuracy: 0.09 - ETA: 1:06 - loss: 3.6341 - accuracy: 0.09 - ETA: 1:05 - loss: 3.6339 - accuracy: 0.09 - ETA: 1:05 - loss: 3.6344 - accuracy: 0.09 - ETA: 1:04 - loss: 3.6339 - accuracy: 0.09 - ETA: 1:03 - loss: 3.6343 - accuracy: 0.09 - ETA: 1:03 - loss: 3.6337 - accuracy: 0.09 - ETA: 1:02 - loss: 3.6340 - accuracy: 0.09 - ETA: 1:02 - loss: 3.6341 - accuracy: 0.09 - ETA: 1:01 - loss: 3.6342 - accuracy: 0.09 - ETA: 1:01 - loss: 3.6345 - accuracy: 0.09 - ETA: 1:00 - loss: 3.6346 - accuracy: 0.09 - ETA: 59s - loss: 3.6348 - accuracy: 0.0916 - ETA: 59s - loss: 3.6353 - accuracy: 0.091 - ETA: 58s - loss: 3.6350 - accuracy: 0.091 - ETA: 58s - loss: 3.6342 - accuracy: 0.091 - ETA: 57s - loss: 3.6334 - accuracy: 0.091 - ETA: 57s - loss: 3.6327 - accuracy: 0.091 - ETA: 56s - loss: 3.6326 - accuracy: 0.091 - ETA: 56s - loss: 3.6324 - accuracy: 0.091 - ETA: 55s - loss: 3.6321 - accuracy: 0.091 - ETA: 54s - loss: 3.6315 - accuracy: 0.092 - ETA: 54s - loss: 3.6319 - accuracy: 0.091 - ETA: 53s - loss: 3.6321 - accuracy: 0.091 - ETA: 53s - loss: 3.6313 - accuracy: 0.092 - ETA: 52s - loss: 3.6307 - accuracy: 0.092 - ETA: 52s - loss: 3.6312 - accuracy: 0.091 - ETA: 51s - loss: 3.6307 - accuracy: 0.092 - ETA: 51s - loss: 3.6307 - accuracy: 0.091 - ETA: 50s - loss: 3.6306 - accuracy: 0.091 - ETA: 49s - loss: 3.6309 - accuracy: 0.091 - ETA: 49s - loss: 3.6302 - accuracy: 0.092 - ETA: 48s - loss: 3.6311 - accuracy: 0.091 - ETA: 48s - loss: 3.6308 - accuracy: 0.092 - ETA: 47s - loss: 3.6312 - accuracy: 0.091 - ETA: 46s - loss: 3.6310 - accuracy: 0.091 - ETA: 46s - loss: 3.6310 - accuracy: 0.091 - ETA: 45s - loss: 3.6307 - accuracy: 0.091 - ETA: 45s - loss: 3.6303 - accuracy: 0.092 - ETA: 44s - loss: 3.6303 - accuracy: 0.092 - ETA: 43s - loss: 3.6303 - accuracy: 0.092 - ETA: 43s - loss: 3.6304 - accuracy: 0.092 - ETA: 42s - loss: 3.6301 - accuracy: 0.092 - ETA: 42s - loss: 3.6297 - accuracy: 0.092 - ETA: 41s - loss: 3.6297 - accuracy: 0.092 - ETA: 40s - loss: 3.6297 - accuracy: 0.092 - ETA: 40s - loss: 3.6292 - accuracy: 0.092 - ETA: 39s - loss: 3.6293 - accuracy: 0.092 - ETA: 39s - loss: 3.6294 - accuracy: 0.092 - ETA: 38s - loss: 3.6294 - accuracy: 0.092 - ETA: 38s - loss: 3.6291 - accuracy: 0.092 - ETA: 37s - loss: 3.6288 - accuracy: 0.092 - ETA: 36s - loss: 3.6287 - accuracy: 0.092 - ETA: 36s - loss: 3.6284 - accuracy: 0.092 - ETA: 35s - loss: 3.6285 - accuracy: 0.092 - ETA: 35s - loss: 3.6284 - accuracy: 0.092 - ETA: 34s - loss: 3.6280 - accuracy: 0.093 - ETA: 33s - loss: 3.6278 - accuracy: 0.093 - ETA: 33s - loss: 3.6281 - accuracy: 0.092 - ETA: 32s - loss: 3.6279 - accuracy: 0.092 - ETA: 32s - loss: 3.6279 - accuracy: 0.092 - ETA: 31s - loss: 3.6279 - accuracy: 0.092 - ETA: 30s - loss: 3.6278 - accuracy: 0.092 - ETA: 30s - loss: 3.6280 - accuracy: 0.092 - ETA: 29s - loss: 3.6277 - accuracy: 0.093 - ETA: 29s - loss: 3.6271 - accuracy: 0.093 - ETA: 28s - loss: 3.6266 - accuracy: 0.093 - ETA: 27s - loss: 3.6265 - accuracy: 0.093 - ETA: 27s - loss: 3.6265 - accuracy: 0.093 - ETA: 26s - loss: 3.6265 - accuracy: 0.093 - ETA: 26s - loss: 3.6270 - accuracy: 0.093 - ETA: 25s - loss: 3.6274 - accuracy: 0.093 - ETA: 24s - loss: 3.6270 - accuracy: 0.093 - ETA: 24s - loss: 3.6266 - accuracy: 0.093 - ETA: 23s - loss: 3.6262 - accuracy: 0.093 - ETA: 23s - loss: 3.6260 - accuracy: 0.093 - ETA: 22s - loss: 3.6257 - accuracy: 0.093 - ETA: 22s - loss: 3.6254 - accuracy: 0.093 - ETA: 21s - loss: 3.6254 - accuracy: 0.093 - ETA: 20s - loss: 3.6250 - accuracy: 0.093 - ETA: 20s - loss: 3.6250 - accuracy: 0.093 - ETA: 19s - loss: 3.6255 - accuracy: 0.093 - ETA: 19s - loss: 3.6257 - accuracy: 0.093 - ETA: 18s - loss: 3.6260 - accuracy: 0.093 - ETA: 17s - loss: 3.6258 - accuracy: 0.093 - ETA: 17s - loss: 3.6257 - accuracy: 0.093 - ETA: 16s - loss: 3.6254 - accuracy: 0.093 - ETA: 16s - loss: 3.6255 - accuracy: 0.093 - ETA: 15s - loss: 3.6252 - accuracy: 0.093 - ETA: 14s - loss: 3.6252 - accuracy: 0.093 - ETA: 14s - loss: 3.6253 - accuracy: 0.093 - ETA: 13s - loss: 3.6255 - accuracy: 0.093 - ETA: 13s - loss: 3.6258 - accuracy: 0.093 - ETA: 12s - loss: 3.6257 - accuracy: 0.093 - ETA: 11s - loss: 3.6253 - accuracy: 0.093 - ETA: 11s - loss: 3.6251 - accuracy: 0.093 - ETA: 10s - loss: 3.6248 - accuracy: 0.093 - ETA: 10s - loss: 3.6244 - accuracy: 0.093 - ETA: 9s - loss: 3.6241 - accuracy: 0.093 - ETA: 8s - loss: 3.6245 - accuracy: 0.09 - ETA: 8s - loss: 3.6242 - accuracy: 0.09 - ETA: 7s - loss: 3.6238 - accuracy: 0.09 - ETA: 7s - loss: 3.6239 - accuracy: 0.09 - ETA: 6s - loss: 3.6242 - accuracy: 0.09 - ETA: 5s - loss: 3.6245 - accuracy: 0.09 - ETA: 5s - loss: 3.6246 - accuracy: 0.09 - ETA: 4s - loss: 3.6249 - accuracy: 0.09 - ETA: 4s - loss: 3.6246 - accuracy: 0.09 - ETA: 3s - loss: 3.6246 - accuracy: 0.09 - ETA: 3s - loss: 3.6248 - accuracy: 0.09 - ETA: 2s - loss: 3.6239 - accuracy: 0.09 - ETA: 1s - loss: 3.6232 - accuracy: 0.09 - ETA: 1s - loss: 3.6232 - accuracy: 0.09 - ETA: 0s - loss: 3.6229 - accuracy: 0.09 - ETA: 0s - loss: 3.6228 - accuracy: 0.09 - 209s 5ms/step - loss: 3.6226 - accuracy: 0.0938 - val_loss: 3.9130 - val_accuracy: 0.0248\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:13 - loss: 3.6400 - accuracy: 0.10 - ETA: 3:08 - loss: 3.6471 - accuracy: 0.08 - ETA: 3:07 - loss: 3.6630 - accuracy: 0.08 - ETA: 3:06 - loss: 3.6230 - accuracy: 0.09 - ETA: 3:07 - loss: 3.6134 - accuracy: 0.09 - ETA: 3:08 - loss: 3.6023 - accuracy: 0.09 - ETA: 3:09 - loss: 3.5938 - accuracy: 0.09 - ETA: 3:11 - loss: 3.6314 - accuracy: 0.08 - ETA: 3:10 - loss: 3.6367 - accuracy: 0.08 - ETA: 3:09 - loss: 3.6475 - accuracy: 0.08 - ETA: 3:08 - loss: 3.6333 - accuracy: 0.08 - ETA: 3:06 - loss: 3.6300 - accuracy: 0.08 - ETA: 3:05 - loss: 3.6172 - accuracy: 0.09 - ETA: 3:05 - loss: 3.6174 - accuracy: 0.08 - ETA: 3:05 - loss: 3.6185 - accuracy: 0.08 - ETA: 3:05 - loss: 3.6229 - accuracy: 0.08 - ETA: 3:05 - loss: 3.6224 - accuracy: 0.08 - ETA: 3:04 - loss: 3.6307 - accuracy: 0.08 - ETA: 3:04 - loss: 3.6325 - accuracy: 0.08 - ETA: 3:03 - loss: 3.6321 - accuracy: 0.08 - ETA: 3:02 - loss: 3.6321 - accuracy: 0.08 - ETA: 3:02 - loss: 3.6267 - accuracy: 0.09 - ETA: 3:01 - loss: 3.6207 - accuracy: 0.09 - ETA: 3:00 - loss: 3.6197 - accuracy: 0.09 - ETA: 3:00 - loss: 3.6206 - accuracy: 0.08 - ETA: 2:59 - loss: 3.6146 - accuracy: 0.09 - ETA: 2:59 - loss: 3.6111 - accuracy: 0.09 - ETA: 2:58 - loss: 3.6099 - accuracy: 0.09 - ETA: 2:58 - loss: 3.6111 - accuracy: 0.09 - ETA: 2:57 - loss: 3.6106 - accuracy: 0.09 - ETA: 2:57 - loss: 3.6123 - accuracy: 0.09 - ETA: 2:56 - loss: 3.6105 - accuracy: 0.09 - ETA: 2:57 - loss: 3.6109 - accuracy: 0.09 - ETA: 2:57 - loss: 3.6116 - accuracy: 0.09 - ETA: 2:56 - loss: 3.6127 - accuracy: 0.09 - ETA: 2:55 - loss: 3.6094 - accuracy: 0.09 - ETA: 2:55 - loss: 3.6066 - accuracy: 0.09 - ETA: 2:54 - loss: 3.6047 - accuracy: 0.09 - ETA: 2:53 - loss: 3.6034 - accuracy: 0.09 - ETA: 2:53 - loss: 3.6046 - accuracy: 0.09 - ETA: 2:52 - loss: 3.6000 - accuracy: 0.09 - ETA: 2:51 - loss: 3.5998 - accuracy: 0.09 - ETA: 2:51 - loss: 3.5992 - accuracy: 0.09 - ETA: 2:50 - loss: 3.5994 - accuracy: 0.09 - ETA: 2:49 - loss: 3.5999 - accuracy: 0.09 - ETA: 2:48 - loss: 3.6003 - accuracy: 0.09 - ETA: 2:48 - loss: 3.5983 - accuracy: 0.09 - ETA: 2:47 - loss: 3.5960 - accuracy: 0.09 - ETA: 2:46 - loss: 3.5971 - accuracy: 0.09 - ETA: 2:46 - loss: 3.5964 - accuracy: 0.09 - ETA: 2:45 - loss: 3.5973 - accuracy: 0.09 - ETA: 2:44 - loss: 3.5959 - accuracy: 0.09 - ETA: 2:44 - loss: 3.5939 - accuracy: 0.09 - ETA: 2:43 - loss: 3.5957 - accuracy: 0.09 - ETA: 2:42 - loss: 3.5954 - accuracy: 0.09 - ETA: 2:42 - loss: 3.5962 - accuracy: 0.09 - ETA: 2:41 - loss: 3.5976 - accuracy: 0.09 - ETA: 2:40 - loss: 3.5961 - accuracy: 0.09 - ETA: 2:40 - loss: 3.5963 - accuracy: 0.09 - ETA: 2:39 - loss: 3.5960 - accuracy: 0.09 - ETA: 2:39 - loss: 3.5958 - accuracy: 0.09 - ETA: 2:38 - loss: 3.5928 - accuracy: 0.09 - ETA: 2:38 - loss: 3.5907 - accuracy: 0.09 - ETA: 2:37 - loss: 3.5893 - accuracy: 0.09 - ETA: 2:36 - loss: 3.5875 - accuracy: 0.09 - ETA: 2:36 - loss: 3.5852 - accuracy: 0.09 - ETA: 2:35 - loss: 3.5844 - accuracy: 0.09 - ETA: 2:35 - loss: 3.5828 - accuracy: 0.09 - ETA: 2:34 - loss: 3.5837 - accuracy: 0.09 - ETA: 2:33 - loss: 3.5806 - accuracy: 0.10 - ETA: 2:33 - loss: 3.5806 - accuracy: 0.10 - ETA: 2:32 - loss: 3.5789 - accuracy: 0.10 - ETA: 2:32 - loss: 3.5784 - accuracy: 0.10 - ETA: 2:31 - loss: 3.5791 - accuracy: 0.10 - ETA: 2:31 - loss: 3.5781 - accuracy: 0.10 - ETA: 2:30 - loss: 3.5781 - accuracy: 0.10 - ETA: 2:30 - loss: 3.5787 - accuracy: 0.10 - ETA: 2:29 - loss: 3.5779 - accuracy: 0.10 - ETA: 2:28 - loss: 3.5758 - accuracy: 0.10 - ETA: 2:28 - loss: 3.5753 - accuracy: 0.10 - ETA: 2:27 - loss: 3.5763 - accuracy: 0.10 - ETA: 2:27 - loss: 3.5750 - accuracy: 0.10 - ETA: 2:26 - loss: 3.5755 - accuracy: 0.10 - ETA: 2:25 - loss: 3.5746 - accuracy: 0.10 - ETA: 2:25 - loss: 3.5732 - accuracy: 0.10 - ETA: 2:24 - loss: 3.5739 - accuracy: 0.10 - ETA: 2:24 - loss: 3.5746 - accuracy: 0.10 - ETA: 2:23 - loss: 3.5733 - accuracy: 0.10 - ETA: 2:23 - loss: 3.5741 - accuracy: 0.10 - ETA: 2:22 - loss: 3.5748 - accuracy: 0.10 - ETA: 2:21 - loss: 3.5745 - accuracy: 0.10 - ETA: 2:21 - loss: 3.5739 - accuracy: 0.10 - ETA: 2:20 - loss: 3.5735 - accuracy: 0.10 - ETA: 2:20 - loss: 3.5732 - accuracy: 0.10 - ETA: 2:19 - loss: 3.5737 - accuracy: 0.10 - ETA: 2:18 - loss: 3.5739 - accuracy: 0.10 - ETA: 2:18 - loss: 3.5744 - accuracy: 0.10 - ETA: 2:17 - loss: 3.5750 - accuracy: 0.10 - ETA: 2:16 - loss: 3.5769 - accuracy: 0.10 - ETA: 2:16 - loss: 3.5776 - accuracy: 0.10 - ETA: 2:15 - loss: 3.5764 - accuracy: 0.10 - ETA: 2:15 - loss: 3.5768 - accuracy: 0.09 - ETA: 2:14 - loss: 3.5777 - accuracy: 0.09 - ETA: 2:13 - loss: 3.5793 - accuracy: 0.09 - ETA: 2:13 - loss: 3.5792 - accuracy: 0.09 - ETA: 2:12 - loss: 3.5792 - accuracy: 0.09 - ETA: 2:11 - loss: 3.5791 - accuracy: 0.09 - ETA: 2:11 - loss: 3.5799 - accuracy: 0.09 - ETA: 2:10 - loss: 3.5791 - accuracy: 0.09 - ETA: 2:10 - loss: 3.5780 - accuracy: 0.09 - ETA: 2:09 - loss: 3.5777 - accuracy: 0.09 - ETA: 2:08 - loss: 3.5782 - accuracy: 0.09 - ETA: 2:08 - loss: 3.5751 - accuracy: 0.10 - ETA: 2:07 - loss: 3.5749 - accuracy: 0.09 - ETA: 2:07 - loss: 3.5762 - accuracy: 0.09 - ETA: 2:06 - loss: 3.5759 - accuracy: 0.09 - ETA: 2:06 - loss: 3.5761 - accuracy: 0.09 - ETA: 2:05 - loss: 3.5762 - accuracy: 0.09 - ETA: 2:05 - loss: 3.5755 - accuracy: 0.09 - ETA: 2:04 - loss: 3.5764 - accuracy: 0.09 - ETA: 2:03 - loss: 3.5760 - accuracy: 0.10 - ETA: 2:03 - loss: 3.5764 - accuracy: 0.10 - ETA: 2:02 - loss: 3.5768 - accuracy: 0.09 - ETA: 2:02 - loss: 3.5762 - accuracy: 0.10 - ETA: 2:01 - loss: 3.5769 - accuracy: 0.10 - ETA: 2:00 - loss: 3.5763 - accuracy: 0.10 - ETA: 2:00 - loss: 3.5772 - accuracy: 0.09 - ETA: 1:59 - loss: 3.5773 - accuracy: 0.09 - ETA: 1:59 - loss: 3.5784 - accuracy: 0.09 - ETA: 1:58 - loss: 3.5789 - accuracy: 0.09 - ETA: 1:57 - loss: 3.5782 - accuracy: 0.09 - ETA: 1:57 - loss: 3.5788 - accuracy: 0.09 - ETA: 1:56 - loss: 3.5794 - accuracy: 0.09 - ETA: 1:55 - loss: 3.5790 - accuracy: 0.09 - ETA: 1:55 - loss: 3.5798 - accuracy: 0.09 - ETA: 1:54 - loss: 3.5807 - accuracy: 0.09 - ETA: 1:54 - loss: 3.5804 - accuracy: 0.09 - ETA: 1:53 - loss: 3.5806 - accuracy: 0.09 - ETA: 1:52 - loss: 3.5801 - accuracy: 0.09 - ETA: 1:52 - loss: 3.5801 - accuracy: 0.09 - ETA: 1:51 - loss: 3.5798 - accuracy: 0.09 - ETA: 1:51 - loss: 3.5802 - accuracy: 0.09 - ETA: 1:50 - loss: 3.5801 - accuracy: 0.09 - ETA: 1:50 - loss: 3.5800 - accuracy: 0.09 - ETA: 1:49 - loss: 3.5808 - accuracy: 0.09 - ETA: 1:48 - loss: 3.5807 - accuracy: 0.09 - ETA: 1:48 - loss: 3.5797 - accuracy: 0.09 - ETA: 1:47 - loss: 3.5803 - accuracy: 0.09 - ETA: 1:47 - loss: 3.5797 - accuracy: 0.09 - ETA: 1:46 - loss: 3.5801 - accuracy: 0.09 - ETA: 1:45 - loss: 3.5802 - accuracy: 0.09 - ETA: 1:45 - loss: 3.5807 - accuracy: 0.09 - ETA: 1:44 - loss: 3.5804 - accuracy: 0.09 - ETA: 1:44 - loss: 3.5797 - accuracy: 0.09 - ETA: 1:43 - loss: 3.5801 - accuracy: 0.09 - ETA: 1:42 - loss: 3.5796 - accuracy: 0.09 - ETA: 1:42 - loss: 3.5798 - accuracy: 0.09 - ETA: 1:41 - loss: 3.5794 - accuracy: 0.09 - ETA: 1:41 - loss: 3.5795 - accuracy: 0.09 - ETA: 1:40 - loss: 3.5796 - accuracy: 0.09 - ETA: 1:39 - loss: 3.5784 - accuracy: 0.09 - ETA: 1:39 - loss: 3.5782 - accuracy: 0.09 - ETA: 1:38 - loss: 3.5782 - accuracy: 0.09 - ETA: 1:38 - loss: 3.5781 - accuracy: 0.09 - ETA: 1:37 - loss: 3.5781 - accuracy: 0.09 - ETA: 1:36 - loss: 3.5772 - accuracy: 0.09 - ETA: 1:36 - loss: 3.5783 - accuracy: 0.09 - ETA: 1:35 - loss: 3.5781 - accuracy: 0.09 - ETA: 1:35 - loss: 3.5781 - accuracy: 0.09 - ETA: 1:34 - loss: 3.5781 - accuracy: 0.09 - ETA: 1:34 - loss: 3.5780 - accuracy: 0.09 - ETA: 1:33 - loss: 3.5779 - accuracy: 0.09 - ETA: 1:32 - loss: 3.5788 - accuracy: 0.09 - ETA: 1:32 - loss: 3.5789 - accuracy: 0.09 - ETA: 1:31 - loss: 3.5787 - accuracy: 0.09 - ETA: 1:31 - loss: 3.5783 - accuracy: 0.09 - ETA: 1:30 - loss: 3.5783 - accuracy: 0.09 - ETA: 1:29 - loss: 3.5777 - accuracy: 0.09 - ETA: 1:29 - loss: 3.5788 - accuracy: 0.09 - ETA: 1:28 - loss: 3.5791 - accuracy: 0.09 - ETA: 1:28 - loss: 3.5796 - accuracy: 0.09 - ETA: 1:27 - loss: 3.5792 - accuracy: 0.09 - ETA: 1:27 - loss: 3.5794 - accuracy: 0.09 - ETA: 1:26 - loss: 3.5794 - accuracy: 0.09 - ETA: 1:25 - loss: 3.5795 - accuracy: 0.09 - ETA: 1:25 - loss: 3.5800 - accuracy: 0.0976"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.5808 - accuracy: 0.09 - ETA: 1:24 - loss: 3.5810 - accuracy: 0.09 - ETA: 1:23 - loss: 3.5806 - accuracy: 0.09 - ETA: 1:22 - loss: 3.5809 - accuracy: 0.09 - ETA: 1:22 - loss: 3.5813 - accuracy: 0.09 - ETA: 1:21 - loss: 3.5821 - accuracy: 0.09 - ETA: 1:21 - loss: 3.5816 - accuracy: 0.09 - ETA: 1:20 - loss: 3.5811 - accuracy: 0.09 - ETA: 1:19 - loss: 3.5810 - accuracy: 0.09 - ETA: 1:19 - loss: 3.5809 - accuracy: 0.09 - ETA: 1:18 - loss: 3.5807 - accuracy: 0.09 - ETA: 1:18 - loss: 3.5803 - accuracy: 0.09 - ETA: 1:17 - loss: 3.5790 - accuracy: 0.09 - ETA: 1:17 - loss: 3.5782 - accuracy: 0.09 - ETA: 1:16 - loss: 3.5780 - accuracy: 0.09 - ETA: 1:15 - loss: 3.5784 - accuracy: 0.09 - ETA: 1:15 - loss: 3.5790 - accuracy: 0.09 - ETA: 1:14 - loss: 3.5790 - accuracy: 0.09 - ETA: 1:14 - loss: 3.5798 - accuracy: 0.09 - ETA: 1:13 - loss: 3.5799 - accuracy: 0.09 - ETA: 1:12 - loss: 3.5802 - accuracy: 0.09 - ETA: 1:12 - loss: 3.5801 - accuracy: 0.09 - ETA: 1:11 - loss: 3.5799 - accuracy: 0.09 - ETA: 1:11 - loss: 3.5796 - accuracy: 0.09 - ETA: 1:10 - loss: 3.5791 - accuracy: 0.09 - ETA: 1:10 - loss: 3.5785 - accuracy: 0.09 - ETA: 1:09 - loss: 3.5787 - accuracy: 0.09 - ETA: 1:08 - loss: 3.5780 - accuracy: 0.09 - ETA: 1:08 - loss: 3.5780 - accuracy: 0.09 - ETA: 1:07 - loss: 3.5779 - accuracy: 0.09 - ETA: 1:07 - loss: 3.5778 - accuracy: 0.09 - ETA: 1:06 - loss: 3.5776 - accuracy: 0.09 - ETA: 1:05 - loss: 3.5774 - accuracy: 0.09 - ETA: 1:05 - loss: 3.5766 - accuracy: 0.09 - ETA: 1:04 - loss: 3.5773 - accuracy: 0.09 - ETA: 1:04 - loss: 3.5770 - accuracy: 0.09 - ETA: 1:03 - loss: 3.5768 - accuracy: 0.09 - ETA: 1:02 - loss: 3.5768 - accuracy: 0.09 - ETA: 1:02 - loss: 3.5769 - accuracy: 0.09 - ETA: 1:01 - loss: 3.5768 - accuracy: 0.09 - ETA: 1:01 - loss: 3.5766 - accuracy: 0.09 - ETA: 1:00 - loss: 3.5763 - accuracy: 0.09 - ETA: 1:00 - loss: 3.5755 - accuracy: 0.09 - ETA: 59s - loss: 3.5757 - accuracy: 0.0999 - ETA: 58s - loss: 3.5752 - accuracy: 0.100 - ETA: 58s - loss: 3.5757 - accuracy: 0.099 - ETA: 57s - loss: 3.5758 - accuracy: 0.099 - ETA: 57s - loss: 3.5759 - accuracy: 0.099 - ETA: 56s - loss: 3.5760 - accuracy: 0.099 - ETA: 55s - loss: 3.5765 - accuracy: 0.099 - ETA: 55s - loss: 3.5761 - accuracy: 0.099 - ETA: 54s - loss: 3.5758 - accuracy: 0.099 - ETA: 54s - loss: 3.5755 - accuracy: 0.100 - ETA: 53s - loss: 3.5760 - accuracy: 0.100 - ETA: 52s - loss: 3.5759 - accuracy: 0.100 - ETA: 52s - loss: 3.5754 - accuracy: 0.100 - ETA: 51s - loss: 3.5753 - accuracy: 0.100 - ETA: 51s - loss: 3.5748 - accuracy: 0.100 - ETA: 50s - loss: 3.5747 - accuracy: 0.100 - ETA: 50s - loss: 3.5744 - accuracy: 0.100 - ETA: 49s - loss: 3.5751 - accuracy: 0.100 - ETA: 48s - loss: 3.5749 - accuracy: 0.100 - ETA: 48s - loss: 3.5753 - accuracy: 0.100 - ETA: 47s - loss: 3.5746 - accuracy: 0.100 - ETA: 47s - loss: 3.5746 - accuracy: 0.100 - ETA: 46s - loss: 3.5744 - accuracy: 0.100 - ETA: 45s - loss: 3.5747 - accuracy: 0.100 - ETA: 45s - loss: 3.5751 - accuracy: 0.100 - ETA: 44s - loss: 3.5751 - accuracy: 0.100 - ETA: 44s - loss: 3.5751 - accuracy: 0.100 - ETA: 43s - loss: 3.5746 - accuracy: 0.100 - ETA: 42s - loss: 3.5742 - accuracy: 0.100 - ETA: 42s - loss: 3.5750 - accuracy: 0.100 - ETA: 41s - loss: 3.5757 - accuracy: 0.100 - ETA: 41s - loss: 3.5757 - accuracy: 0.100 - ETA: 40s - loss: 3.5760 - accuracy: 0.100 - ETA: 40s - loss: 3.5759 - accuracy: 0.100 - ETA: 39s - loss: 3.5756 - accuracy: 0.100 - ETA: 38s - loss: 3.5755 - accuracy: 0.100 - ETA: 38s - loss: 3.5758 - accuracy: 0.100 - ETA: 37s - loss: 3.5758 - accuracy: 0.100 - ETA: 37s - loss: 3.5757 - accuracy: 0.100 - ETA: 36s - loss: 3.5750 - accuracy: 0.100 - ETA: 35s - loss: 3.5748 - accuracy: 0.100 - ETA: 35s - loss: 3.5744 - accuracy: 0.100 - ETA: 34s - loss: 3.5747 - accuracy: 0.100 - ETA: 34s - loss: 3.5746 - accuracy: 0.100 - ETA: 33s - loss: 3.5750 - accuracy: 0.100 - ETA: 32s - loss: 3.5755 - accuracy: 0.099 - ETA: 32s - loss: 3.5752 - accuracy: 0.100 - ETA: 31s - loss: 3.5747 - accuracy: 0.100 - ETA: 31s - loss: 3.5741 - accuracy: 0.100 - ETA: 30s - loss: 3.5745 - accuracy: 0.100 - ETA: 30s - loss: 3.5746 - accuracy: 0.100 - ETA: 29s - loss: 3.5742 - accuracy: 0.100 - ETA: 28s - loss: 3.5736 - accuracy: 0.100 - ETA: 28s - loss: 3.5731 - accuracy: 0.100 - ETA: 27s - loss: 3.5731 - accuracy: 0.100 - ETA: 27s - loss: 3.5731 - accuracy: 0.100 - ETA: 26s - loss: 3.5733 - accuracy: 0.100 - ETA: 25s - loss: 3.5728 - accuracy: 0.100 - ETA: 25s - loss: 3.5732 - accuracy: 0.100 - ETA: 24s - loss: 3.5732 - accuracy: 0.100 - ETA: 24s - loss: 3.5731 - accuracy: 0.100 - ETA: 23s - loss: 3.5724 - accuracy: 0.100 - ETA: 22s - loss: 3.5727 - accuracy: 0.100 - ETA: 22s - loss: 3.5729 - accuracy: 0.100 - ETA: 21s - loss: 3.5729 - accuracy: 0.100 - ETA: 21s - loss: 3.5733 - accuracy: 0.100 - ETA: 20s - loss: 3.5730 - accuracy: 0.100 - ETA: 20s - loss: 3.5730 - accuracy: 0.100 - ETA: 19s - loss: 3.5730 - accuracy: 0.100 - ETA: 18s - loss: 3.5728 - accuracy: 0.100 - ETA: 18s - loss: 3.5726 - accuracy: 0.100 - ETA: 17s - loss: 3.5718 - accuracy: 0.100 - ETA: 17s - loss: 3.5714 - accuracy: 0.100 - ETA: 16s - loss: 3.5716 - accuracy: 0.100 - ETA: 15s - loss: 3.5715 - accuracy: 0.100 - ETA: 15s - loss: 3.5719 - accuracy: 0.100 - ETA: 14s - loss: 3.5719 - accuracy: 0.100 - ETA: 14s - loss: 3.5717 - accuracy: 0.100 - ETA: 13s - loss: 3.5718 - accuracy: 0.100 - ETA: 12s - loss: 3.5714 - accuracy: 0.100 - ETA: 12s - loss: 3.5717 - accuracy: 0.100 - ETA: 11s - loss: 3.5713 - accuracy: 0.101 - ETA: 11s - loss: 3.5716 - accuracy: 0.101 - ETA: 10s - loss: 3.5713 - accuracy: 0.101 - ETA: 10s - loss: 3.5713 - accuracy: 0.100 - ETA: 9s - loss: 3.5713 - accuracy: 0.100 - ETA: 8s - loss: 3.5711 - accuracy: 0.10 - ETA: 8s - loss: 3.5712 - accuracy: 0.10 - ETA: 7s - loss: 3.5713 - accuracy: 0.10 - ETA: 7s - loss: 3.5712 - accuracy: 0.10 - ETA: 6s - loss: 3.5709 - accuracy: 0.10 - ETA: 5s - loss: 3.5706 - accuracy: 0.10 - ETA: 5s - loss: 3.5704 - accuracy: 0.10 - ETA: 4s - loss: 3.5699 - accuracy: 0.10 - ETA: 4s - loss: 3.5703 - accuracy: 0.10 - ETA: 3s - loss: 3.5703 - accuracy: 0.10 - ETA: 2s - loss: 3.5700 - accuracy: 0.10 - ETA: 2s - loss: 3.5698 - accuracy: 0.10 - ETA: 1s - loss: 3.5697 - accuracy: 0.10 - ETA: 1s - loss: 3.5694 - accuracy: 0.10 - ETA: 0s - loss: 3.5691 - accuracy: 0.10 - ETA: 0s - loss: 3.5689 - accuracy: 0.10 - 216s 5ms/step - loss: 3.5689 - accuracy: 0.1013 - val_loss: 3.8946 - val_accuracy: 0.0307\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:12 - loss: 3.7114 - accuracy: 0.09 - ETA: 5:09 - loss: 3.7004 - accuracy: 0.07 - ETA: 5:10 - loss: 3.5873 - accuracy: 0.10 - ETA: 5:08 - loss: 3.6097 - accuracy: 0.11 - ETA: 5:04 - loss: 3.5913 - accuracy: 0.11 - ETA: 5:02 - loss: 3.6156 - accuracy: 0.10 - ETA: 5:02 - loss: 3.5972 - accuracy: 0.10 - ETA: 5:00 - loss: 3.6049 - accuracy: 0.10 - ETA: 5:00 - loss: 3.6000 - accuracy: 0.10 - ETA: 5:01 - loss: 3.6038 - accuracy: 0.10 - ETA: 5:01 - loss: 3.6153 - accuracy: 0.10 - ETA: 5:01 - loss: 3.6165 - accuracy: 0.09 - ETA: 5:01 - loss: 3.6171 - accuracy: 0.09 - ETA: 5:02 - loss: 3.6106 - accuracy: 0.10 - ETA: 5:03 - loss: 3.6120 - accuracy: 0.10 - ETA: 5:02 - loss: 3.6117 - accuracy: 0.10 - ETA: 5:08 - loss: 3.5958 - accuracy: 0.10 - ETA: 5:08 - loss: 3.5943 - accuracy: 0.10 - ETA: 5:03 - loss: 3.5948 - accuracy: 0.10 - ETA: 4:57 - loss: 3.5923 - accuracy: 0.10 - ETA: 4:51 - loss: 3.5998 - accuracy: 0.10 - ETA: 4:45 - loss: 3.6012 - accuracy: 0.10 - ETA: 4:40 - loss: 3.6040 - accuracy: 0.09 - ETA: 4:35 - loss: 3.6043 - accuracy: 0.09 - ETA: 4:31 - loss: 3.5994 - accuracy: 0.10 - ETA: 4:27 - loss: 3.5966 - accuracy: 0.10 - ETA: 4:22 - loss: 3.5900 - accuracy: 0.10 - ETA: 4:18 - loss: 3.5833 - accuracy: 0.10 - ETA: 4:15 - loss: 3.5813 - accuracy: 0.10 - ETA: 4:12 - loss: 3.5782 - accuracy: 0.10 - ETA: 4:09 - loss: 3.5764 - accuracy: 0.10 - ETA: 4:06 - loss: 3.5717 - accuracy: 0.10 - ETA: 4:03 - loss: 3.5667 - accuracy: 0.10 - ETA: 4:00 - loss: 3.5637 - accuracy: 0.10 - ETA: 3:58 - loss: 3.5644 - accuracy: 0.10 - ETA: 3:55 - loss: 3.5582 - accuracy: 0.10 - ETA: 3:52 - loss: 3.5601 - accuracy: 0.10 - ETA: 3:50 - loss: 3.5582 - accuracy: 0.10 - ETA: 3:47 - loss: 3.5578 - accuracy: 0.10 - ETA: 3:45 - loss: 3.5571 - accuracy: 0.10 - ETA: 3:43 - loss: 3.5626 - accuracy: 0.10 - ETA: 3:41 - loss: 3.5613 - accuracy: 0.10 - ETA: 3:38 - loss: 3.5601 - accuracy: 0.10 - ETA: 3:36 - loss: 3.5629 - accuracy: 0.10 - ETA: 3:35 - loss: 3.5634 - accuracy: 0.10 - ETA: 3:33 - loss: 3.5618 - accuracy: 0.10 - ETA: 3:31 - loss: 3.5595 - accuracy: 0.10 - ETA: 3:29 - loss: 3.5583 - accuracy: 0.10 - ETA: 3:27 - loss: 3.5610 - accuracy: 0.10 - ETA: 3:26 - loss: 3.5601 - accuracy: 0.10 - ETA: 3:24 - loss: 3.5610 - accuracy: 0.10 - ETA: 3:22 - loss: 3.5616 - accuracy: 0.10 - ETA: 3:21 - loss: 3.5636 - accuracy: 0.10 - ETA: 3:19 - loss: 3.5664 - accuracy: 0.10 - ETA: 3:18 - loss: 3.5645 - accuracy: 0.10 - ETA: 3:16 - loss: 3.5631 - accuracy: 0.10 - ETA: 3:15 - loss: 3.5653 - accuracy: 0.10 - ETA: 3:14 - loss: 3.5642 - accuracy: 0.10 - ETA: 3:12 - loss: 3.5649 - accuracy: 0.10 - ETA: 3:11 - loss: 3.5651 - accuracy: 0.10 - ETA: 3:10 - loss: 3.5640 - accuracy: 0.10 - ETA: 3:08 - loss: 3.5667 - accuracy: 0.10 - ETA: 3:07 - loss: 3.5680 - accuracy: 0.10 - ETA: 3:06 - loss: 3.5685 - accuracy: 0.10 - ETA: 3:05 - loss: 3.5673 - accuracy: 0.10 - ETA: 3:03 - loss: 3.5667 - accuracy: 0.10 - ETA: 3:02 - loss: 3.5666 - accuracy: 0.10 - ETA: 3:01 - loss: 3.5661 - accuracy: 0.10 - ETA: 3:00 - loss: 3.5668 - accuracy: 0.10 - ETA: 2:59 - loss: 3.5659 - accuracy: 0.10 - ETA: 2:58 - loss: 3.5666 - accuracy: 0.10 - ETA: 2:57 - loss: 3.5654 - accuracy: 0.10 - ETA: 2:55 - loss: 3.5655 - accuracy: 0.10 - ETA: 2:54 - loss: 3.5633 - accuracy: 0.10 - ETA: 2:53 - loss: 3.5631 - accuracy: 0.10 - ETA: 2:52 - loss: 3.5629 - accuracy: 0.10 - ETA: 2:51 - loss: 3.5627 - accuracy: 0.10 - ETA: 2:50 - loss: 3.5627 - accuracy: 0.10 - ETA: 2:49 - loss: 3.5616 - accuracy: 0.10 - ETA: 2:48 - loss: 3.5614 - accuracy: 0.10 - ETA: 2:47 - loss: 3.5605 - accuracy: 0.10 - ETA: 2:46 - loss: 3.5617 - accuracy: 0.10 - ETA: 2:45 - loss: 3.5627 - accuracy: 0.10 - ETA: 2:44 - loss: 3.5616 - accuracy: 0.10 - ETA: 2:43 - loss: 3.5594 - accuracy: 0.10 - ETA: 2:42 - loss: 3.5603 - accuracy: 0.10 - ETA: 2:42 - loss: 3.5596 - accuracy: 0.10 - ETA: 2:41 - loss: 3.5600 - accuracy: 0.10 - ETA: 2:40 - loss: 3.5602 - accuracy: 0.10 - ETA: 2:39 - loss: 3.5608 - accuracy: 0.10 - ETA: 2:38 - loss: 3.5603 - accuracy: 0.10 - ETA: 2:37 - loss: 3.5609 - accuracy: 0.10 - ETA: 2:36 - loss: 3.5615 - accuracy: 0.10 - ETA: 2:35 - loss: 3.5598 - accuracy: 0.10 - ETA: 2:35 - loss: 3.5609 - accuracy: 0.10 - ETA: 2:34 - loss: 3.5602 - accuracy: 0.10 - ETA: 2:33 - loss: 3.5610 - accuracy: 0.10 - ETA: 2:32 - loss: 3.5605 - accuracy: 0.10 - ETA: 2:31 - loss: 3.5605 - accuracy: 0.10 - ETA: 2:30 - loss: 3.5587 - accuracy: 0.10 - ETA: 2:30 - loss: 3.5562 - accuracy: 0.10 - ETA: 2:29 - loss: 3.5548 - accuracy: 0.10 - ETA: 2:28 - loss: 3.5551 - accuracy: 0.10 - ETA: 2:27 - loss: 3.5565 - accuracy: 0.10 - ETA: 2:26 - loss: 3.5560 - accuracy: 0.10 - ETA: 2:26 - loss: 3.5545 - accuracy: 0.10 - ETA: 2:25 - loss: 3.5546 - accuracy: 0.10 - ETA: 2:24 - loss: 3.5547 - accuracy: 0.10 - ETA: 2:23 - loss: 3.5552 - accuracy: 0.10 - ETA: 2:22 - loss: 3.5547 - accuracy: 0.10 - ETA: 2:22 - loss: 3.5542 - accuracy: 0.10 - ETA: 2:21 - loss: 3.5550 - accuracy: 0.10 - ETA: 2:20 - loss: 3.5540 - accuracy: 0.10 - ETA: 2:20 - loss: 3.5556 - accuracy: 0.10 - ETA: 2:19 - loss: 3.5565 - accuracy: 0.10 - ETA: 2:18 - loss: 3.5567 - accuracy: 0.10 - ETA: 2:17 - loss: 3.5550 - accuracy: 0.10 - ETA: 2:17 - loss: 3.5555 - accuracy: 0.10 - ETA: 2:16 - loss: 3.5547 - accuracy: 0.10 - ETA: 2:15 - loss: 3.5557 - accuracy: 0.10 - ETA: 2:14 - loss: 3.5557 - accuracy: 0.10 - ETA: 2:14 - loss: 3.5544 - accuracy: 0.10 - ETA: 2:13 - loss: 3.5553 - accuracy: 0.10 - ETA: 2:12 - loss: 3.5558 - accuracy: 0.10 - ETA: 2:11 - loss: 3.5555 - accuracy: 0.10 - ETA: 2:11 - loss: 3.5548 - accuracy: 0.10 - ETA: 2:10 - loss: 3.5543 - accuracy: 0.10 - ETA: 2:09 - loss: 3.5535 - accuracy: 0.10 - ETA: 2:09 - loss: 3.5530 - accuracy: 0.10 - ETA: 2:08 - loss: 3.5538 - accuracy: 0.10 - ETA: 2:07 - loss: 3.5523 - accuracy: 0.10 - ETA: 2:06 - loss: 3.5515 - accuracy: 0.10 - ETA: 2:06 - loss: 3.5510 - accuracy: 0.10 - ETA: 2:05 - loss: 3.5513 - accuracy: 0.10 - ETA: 2:04 - loss: 3.5510 - accuracy: 0.10 - ETA: 2:04 - loss: 3.5503 - accuracy: 0.10 - ETA: 2:03 - loss: 3.5486 - accuracy: 0.10 - ETA: 2:02 - loss: 3.5480 - accuracy: 0.10 - ETA: 2:01 - loss: 3.5480 - accuracy: 0.10 - ETA: 2:01 - loss: 3.5478 - accuracy: 0.10 - ETA: 2:00 - loss: 3.5485 - accuracy: 0.10 - ETA: 2:00 - loss: 3.5471 - accuracy: 0.10 - ETA: 1:59 - loss: 3.5459 - accuracy: 0.10 - ETA: 1:58 - loss: 3.5463 - accuracy: 0.10 - ETA: 1:58 - loss: 3.5451 - accuracy: 0.10 - ETA: 1:57 - loss: 3.5447 - accuracy: 0.10 - ETA: 1:56 - loss: 3.5454 - accuracy: 0.10 - ETA: 1:55 - loss: 3.5449 - accuracy: 0.10 - ETA: 1:55 - loss: 3.5443 - accuracy: 0.10 - ETA: 1:54 - loss: 3.5443 - accuracy: 0.10 - ETA: 1:53 - loss: 3.5443 - accuracy: 0.10 - ETA: 1:53 - loss: 3.5443 - accuracy: 0.10 - ETA: 1:52 - loss: 3.5434 - accuracy: 0.10 - ETA: 1:51 - loss: 3.5433 - accuracy: 0.10 - ETA: 1:51 - loss: 3.5418 - accuracy: 0.10 - ETA: 1:50 - loss: 3.5409 - accuracy: 0.10 - ETA: 1:49 - loss: 3.5404 - accuracy: 0.10 - ETA: 1:49 - loss: 3.5410 - accuracy: 0.10 - ETA: 1:48 - loss: 3.5408 - accuracy: 0.10 - ETA: 1:47 - loss: 3.5392 - accuracy: 0.10 - ETA: 1:47 - loss: 3.5385 - accuracy: 0.10 - ETA: 1:46 - loss: 3.5383 - accuracy: 0.10 - ETA: 1:45 - loss: 3.5388 - accuracy: 0.10 - ETA: 1:45 - loss: 3.5391 - accuracy: 0.10 - ETA: 1:44 - loss: 3.5391 - accuracy: 0.10 - ETA: 1:43 - loss: 3.5380 - accuracy: 0.10 - ETA: 1:43 - loss: 3.5381 - accuracy: 0.10 - ETA: 1:42 - loss: 3.5379 - accuracy: 0.10 - ETA: 1:41 - loss: 3.5374 - accuracy: 0.10 - ETA: 1:41 - loss: 3.5378 - accuracy: 0.10 - ETA: 1:40 - loss: 3.5389 - accuracy: 0.10 - ETA: 1:39 - loss: 3.5388 - accuracy: 0.10 - ETA: 1:39 - loss: 3.5389 - accuracy: 0.10 - ETA: 1:38 - loss: 3.5386 - accuracy: 0.10 - ETA: 1:37 - loss: 3.5392 - accuracy: 0.10 - ETA: 1:37 - loss: 3.5400 - accuracy: 0.10 - ETA: 1:36 - loss: 3.5405 - accuracy: 0.10 - ETA: 1:35 - loss: 3.5405 - accuracy: 0.10 - ETA: 1:35 - loss: 3.5402 - accuracy: 0.10 - ETA: 1:34 - loss: 3.5406 - accuracy: 0.10 - ETA: 1:33 - loss: 3.5405 - accuracy: 0.10 - ETA: 1:33 - loss: 3.5403 - accuracy: 0.10 - ETA: 1:32 - loss: 3.5400 - accuracy: 0.10 - ETA: 1:31 - loss: 3.5398 - accuracy: 0.10 - ETA: 1:31 - loss: 3.5398 - accuracy: 0.10 - ETA: 1:30 - loss: 3.5397 - accuracy: 0.1064"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:29 - loss: 3.5391 - accuracy: 0.10 - ETA: 1:29 - loss: 3.5378 - accuracy: 0.10 - ETA: 1:28 - loss: 3.5384 - accuracy: 0.10 - ETA: 1:27 - loss: 3.5379 - accuracy: 0.10 - ETA: 1:27 - loss: 3.5377 - accuracy: 0.10 - ETA: 1:26 - loss: 3.5388 - accuracy: 0.10 - ETA: 1:25 - loss: 3.5387 - accuracy: 0.10 - ETA: 1:25 - loss: 3.5398 - accuracy: 0.10 - ETA: 1:24 - loss: 3.5395 - accuracy: 0.10 - ETA: 1:23 - loss: 3.5400 - accuracy: 0.10 - ETA: 1:23 - loss: 3.5396 - accuracy: 0.10 - ETA: 1:22 - loss: 3.5395 - accuracy: 0.10 - ETA: 1:22 - loss: 3.5394 - accuracy: 0.10 - ETA: 1:21 - loss: 3.5389 - accuracy: 0.10 - ETA: 1:20 - loss: 3.5379 - accuracy: 0.10 - ETA: 1:20 - loss: 3.5376 - accuracy: 0.10 - ETA: 1:19 - loss: 3.5367 - accuracy: 0.10 - ETA: 1:18 - loss: 3.5370 - accuracy: 0.10 - ETA: 1:18 - loss: 3.5369 - accuracy: 0.10 - ETA: 1:17 - loss: 3.5372 - accuracy: 0.10 - ETA: 1:16 - loss: 3.5361 - accuracy: 0.10 - ETA: 1:16 - loss: 3.5364 - accuracy: 0.10 - ETA: 1:15 - loss: 3.5355 - accuracy: 0.10 - ETA: 1:14 - loss: 3.5359 - accuracy: 0.10 - ETA: 1:14 - loss: 3.5356 - accuracy: 0.10 - ETA: 1:13 - loss: 3.5359 - accuracy: 0.10 - ETA: 1:12 - loss: 3.5354 - accuracy: 0.10 - ETA: 1:12 - loss: 3.5351 - accuracy: 0.10 - ETA: 1:11 - loss: 3.5345 - accuracy: 0.10 - ETA: 1:11 - loss: 3.5343 - accuracy: 0.10 - ETA: 1:10 - loss: 3.5339 - accuracy: 0.10 - ETA: 1:09 - loss: 3.5338 - accuracy: 0.10 - ETA: 1:09 - loss: 3.5327 - accuracy: 0.10 - ETA: 1:08 - loss: 3.5328 - accuracy: 0.10 - ETA: 1:07 - loss: 3.5322 - accuracy: 0.10 - ETA: 1:07 - loss: 3.5314 - accuracy: 0.10 - ETA: 1:06 - loss: 3.5311 - accuracy: 0.10 - ETA: 1:06 - loss: 3.5304 - accuracy: 0.10 - ETA: 1:05 - loss: 3.5297 - accuracy: 0.10 - ETA: 1:04 - loss: 3.5300 - accuracy: 0.10 - ETA: 1:04 - loss: 3.5305 - accuracy: 0.10 - ETA: 1:03 - loss: 3.5307 - accuracy: 0.10 - ETA: 1:02 - loss: 3.5305 - accuracy: 0.10 - ETA: 1:02 - loss: 3.5303 - accuracy: 0.10 - ETA: 1:01 - loss: 3.5299 - accuracy: 0.10 - ETA: 1:00 - loss: 3.5302 - accuracy: 0.10 - ETA: 1:00 - loss: 3.5302 - accuracy: 0.10 - ETA: 59s - loss: 3.5297 - accuracy: 0.1088 - ETA: 59s - loss: 3.5297 - accuracy: 0.109 - ETA: 58s - loss: 3.5293 - accuracy: 0.109 - ETA: 57s - loss: 3.5294 - accuracy: 0.109 - ETA: 57s - loss: 3.5298 - accuracy: 0.108 - ETA: 56s - loss: 3.5300 - accuracy: 0.108 - ETA: 55s - loss: 3.5300 - accuracy: 0.108 - ETA: 55s - loss: 3.5293 - accuracy: 0.108 - ETA: 54s - loss: 3.5292 - accuracy: 0.108 - ETA: 54s - loss: 3.5289 - accuracy: 0.109 - ETA: 53s - loss: 3.5282 - accuracy: 0.109 - ETA: 52s - loss: 3.5282 - accuracy: 0.109 - ETA: 52s - loss: 3.5282 - accuracy: 0.109 - ETA: 51s - loss: 3.5283 - accuracy: 0.109 - ETA: 50s - loss: 3.5290 - accuracy: 0.109 - ETA: 50s - loss: 3.5292 - accuracy: 0.108 - ETA: 49s - loss: 3.5288 - accuracy: 0.109 - ETA: 49s - loss: 3.5282 - accuracy: 0.109 - ETA: 48s - loss: 3.5280 - accuracy: 0.109 - ETA: 47s - loss: 3.5277 - accuracy: 0.109 - ETA: 47s - loss: 3.5274 - accuracy: 0.109 - ETA: 46s - loss: 3.5277 - accuracy: 0.109 - ETA: 46s - loss: 3.5266 - accuracy: 0.109 - ETA: 45s - loss: 3.5268 - accuracy: 0.109 - ETA: 44s - loss: 3.5264 - accuracy: 0.109 - ETA: 44s - loss: 3.5265 - accuracy: 0.109 - ETA: 43s - loss: 3.5262 - accuracy: 0.109 - ETA: 42s - loss: 3.5261 - accuracy: 0.109 - ETA: 42s - loss: 3.5261 - accuracy: 0.109 - ETA: 41s - loss: 3.5263 - accuracy: 0.109 - ETA: 41s - loss: 3.5263 - accuracy: 0.109 - ETA: 40s - loss: 3.5262 - accuracy: 0.109 - ETA: 39s - loss: 3.5257 - accuracy: 0.109 - ETA: 39s - loss: 3.5258 - accuracy: 0.109 - ETA: 38s - loss: 3.5257 - accuracy: 0.109 - ETA: 37s - loss: 3.5249 - accuracy: 0.109 - ETA: 37s - loss: 3.5246 - accuracy: 0.109 - ETA: 36s - loss: 3.5250 - accuracy: 0.109 - ETA: 36s - loss: 3.5251 - accuracy: 0.109 - ETA: 35s - loss: 3.5254 - accuracy: 0.109 - ETA: 34s - loss: 3.5257 - accuracy: 0.109 - ETA: 34s - loss: 3.5257 - accuracy: 0.109 - ETA: 33s - loss: 3.5258 - accuracy: 0.109 - ETA: 33s - loss: 3.5256 - accuracy: 0.109 - ETA: 32s - loss: 3.5257 - accuracy: 0.109 - ETA: 31s - loss: 3.5253 - accuracy: 0.109 - ETA: 31s - loss: 3.5256 - accuracy: 0.109 - ETA: 30s - loss: 3.5261 - accuracy: 0.109 - ETA: 29s - loss: 3.5259 - accuracy: 0.109 - ETA: 29s - loss: 3.5259 - accuracy: 0.109 - ETA: 28s - loss: 3.5261 - accuracy: 0.109 - ETA: 28s - loss: 3.5262 - accuracy: 0.109 - ETA: 27s - loss: 3.5261 - accuracy: 0.109 - ETA: 26s - loss: 3.5262 - accuracy: 0.109 - ETA: 26s - loss: 3.5259 - accuracy: 0.109 - ETA: 25s - loss: 3.5250 - accuracy: 0.110 - ETA: 25s - loss: 3.5249 - accuracy: 0.110 - ETA: 24s - loss: 3.5247 - accuracy: 0.110 - ETA: 23s - loss: 3.5246 - accuracy: 0.110 - ETA: 23s - loss: 3.5250 - accuracy: 0.110 - ETA: 22s - loss: 3.5247 - accuracy: 0.110 - ETA: 22s - loss: 3.5243 - accuracy: 0.110 - ETA: 21s - loss: 3.5246 - accuracy: 0.110 - ETA: 20s - loss: 3.5247 - accuracy: 0.110 - ETA: 20s - loss: 3.5243 - accuracy: 0.110 - ETA: 19s - loss: 3.5244 - accuracy: 0.110 - ETA: 18s - loss: 3.5243 - accuracy: 0.110 - ETA: 18s - loss: 3.5242 - accuracy: 0.110 - ETA: 17s - loss: 3.5241 - accuracy: 0.110 - ETA: 17s - loss: 3.5240 - accuracy: 0.110 - ETA: 16s - loss: 3.5235 - accuracy: 0.110 - ETA: 15s - loss: 3.5235 - accuracy: 0.110 - ETA: 15s - loss: 3.5232 - accuracy: 0.110 - ETA: 14s - loss: 3.5229 - accuracy: 0.110 - ETA: 14s - loss: 3.5229 - accuracy: 0.109 - ETA: 13s - loss: 3.5225 - accuracy: 0.109 - ETA: 12s - loss: 3.5227 - accuracy: 0.109 - ETA: 12s - loss: 3.5225 - accuracy: 0.109 - ETA: 11s - loss: 3.5225 - accuracy: 0.109 - ETA: 10s - loss: 3.5221 - accuracy: 0.110 - ETA: 10s - loss: 3.5222 - accuracy: 0.110 - ETA: 9s - loss: 3.5223 - accuracy: 0.110 - ETA: 9s - loss: 3.5225 - accuracy: 0.10 - ETA: 8s - loss: 3.5221 - accuracy: 0.11 - ETA: 7s - loss: 3.5221 - accuracy: 0.11 - ETA: 7s - loss: 3.5218 - accuracy: 0.11 - ETA: 6s - loss: 3.5214 - accuracy: 0.11 - ETA: 6s - loss: 3.5214 - accuracy: 0.11 - ETA: 5s - loss: 3.5212 - accuracy: 0.11 - ETA: 4s - loss: 3.5215 - accuracy: 0.11 - ETA: 4s - loss: 3.5212 - accuracy: 0.11 - ETA: 3s - loss: 3.5210 - accuracy: 0.11 - ETA: 3s - loss: 3.5214 - accuracy: 0.11 - ETA: 2s - loss: 3.5214 - accuracy: 0.11 - ETA: 1s - loss: 3.5214 - accuracy: 0.10 - ETA: 1s - loss: 3.5215 - accuracy: 0.11 - ETA: 0s - loss: 3.5215 - accuracy: 0.11 - ETA: 0s - loss: 3.5212 - accuracy: 0.11 - 214s 5ms/step - loss: 3.5212 - accuracy: 0.1100 - val_loss: 3.8731 - val_accuracy: 0.0294\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:20 - loss: 3.5171 - accuracy: 0.11 - ETA: 3:19 - loss: 3.4655 - accuracy: 0.13 - ETA: 3:20 - loss: 3.4506 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4413 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4390 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4567 - accuracy: 0.11 - ETA: 3:11 - loss: 3.4499 - accuracy: 0.12 - ETA: 3:10 - loss: 3.4476 - accuracy: 0.12 - ETA: 3:08 - loss: 3.4707 - accuracy: 0.11 - ETA: 3:07 - loss: 3.4847 - accuracy: 0.11 - ETA: 3:07 - loss: 3.4841 - accuracy: 0.11 - ETA: 3:06 - loss: 3.4867 - accuracy: 0.11 - ETA: 3:06 - loss: 3.4861 - accuracy: 0.11 - ETA: 3:05 - loss: 3.4904 - accuracy: 0.11 - ETA: 3:04 - loss: 3.4821 - accuracy: 0.11 - ETA: 3:03 - loss: 3.4940 - accuracy: 0.11 - ETA: 3:02 - loss: 3.5014 - accuracy: 0.11 - ETA: 3:02 - loss: 3.4986 - accuracy: 0.11 - ETA: 3:01 - loss: 3.5030 - accuracy: 0.11 - ETA: 3:01 - loss: 3.4980 - accuracy: 0.11 - ETA: 3:00 - loss: 3.4967 - accuracy: 0.11 - ETA: 2:59 - loss: 3.4992 - accuracy: 0.11 - ETA: 2:59 - loss: 3.5004 - accuracy: 0.11 - ETA: 3:00 - loss: 3.4966 - accuracy: 0.11 - ETA: 3:02 - loss: 3.4994 - accuracy: 0.11 - ETA: 3:03 - loss: 3.5019 - accuracy: 0.11 - ETA: 3:02 - loss: 3.4949 - accuracy: 0.11 - ETA: 3:02 - loss: 3.4911 - accuracy: 0.11 - ETA: 3:02 - loss: 3.4868 - accuracy: 0.11 - ETA: 3:02 - loss: 3.4826 - accuracy: 0.12 - ETA: 3:01 - loss: 3.4860 - accuracy: 0.11 - ETA: 3:00 - loss: 3.4852 - accuracy: 0.11 - ETA: 2:59 - loss: 3.4853 - accuracy: 0.11 - ETA: 2:59 - loss: 3.4870 - accuracy: 0.11 - ETA: 2:58 - loss: 3.4856 - accuracy: 0.11 - ETA: 2:57 - loss: 3.4847 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4820 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4846 - accuracy: 0.12 - ETA: 2:55 - loss: 3.4819 - accuracy: 0.12 - ETA: 2:54 - loss: 3.4851 - accuracy: 0.12 - ETA: 2:53 - loss: 3.4900 - accuracy: 0.12 - ETA: 2:52 - loss: 3.4913 - accuracy: 0.11 - ETA: 2:52 - loss: 3.4920 - accuracy: 0.11 - ETA: 2:51 - loss: 3.4889 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4873 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4840 - accuracy: 0.12 - ETA: 2:49 - loss: 3.4835 - accuracy: 0.12 - ETA: 2:48 - loss: 3.4822 - accuracy: 0.12 - ETA: 2:48 - loss: 3.4800 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4802 - accuracy: 0.12 - ETA: 2:46 - loss: 3.4839 - accuracy: 0.11 - ETA: 2:46 - loss: 3.4849 - accuracy: 0.11 - ETA: 2:45 - loss: 3.4891 - accuracy: 0.11 - ETA: 2:44 - loss: 3.4894 - accuracy: 0.11 - ETA: 2:44 - loss: 3.4903 - accuracy: 0.11 - ETA: 2:43 - loss: 3.4894 - accuracy: 0.11 - ETA: 2:43 - loss: 3.4884 - accuracy: 0.11 - ETA: 2:42 - loss: 3.4848 - accuracy: 0.11 - ETA: 2:42 - loss: 3.4837 - accuracy: 0.11 - ETA: 2:41 - loss: 3.4823 - accuracy: 0.11 - ETA: 2:41 - loss: 3.4818 - accuracy: 0.11 - ETA: 2:40 - loss: 3.4804 - accuracy: 0.11 - ETA: 2:39 - loss: 3.4789 - accuracy: 0.11 - ETA: 2:38 - loss: 3.4797 - accuracy: 0.11 - ETA: 2:38 - loss: 3.4810 - accuracy: 0.11 - ETA: 2:37 - loss: 3.4828 - accuracy: 0.11 - ETA: 2:36 - loss: 3.4841 - accuracy: 0.11 - ETA: 2:36 - loss: 3.4827 - accuracy: 0.11 - ETA: 2:35 - loss: 3.4832 - accuracy: 0.11 - ETA: 2:34 - loss: 3.4842 - accuracy: 0.11 - ETA: 2:34 - loss: 3.4849 - accuracy: 0.11 - ETA: 2:33 - loss: 3.4829 - accuracy: 0.11 - ETA: 2:32 - loss: 3.4838 - accuracy: 0.11 - ETA: 2:32 - loss: 3.4837 - accuracy: 0.11 - ETA: 2:31 - loss: 3.4849 - accuracy: 0.11 - ETA: 2:30 - loss: 3.4883 - accuracy: 0.11 - ETA: 2:30 - loss: 3.4877 - accuracy: 0.11 - ETA: 2:29 - loss: 3.4880 - accuracy: 0.11 - ETA: 2:29 - loss: 3.4876 - accuracy: 0.11 - ETA: 2:28 - loss: 3.4857 - accuracy: 0.11 - ETA: 2:27 - loss: 3.4855 - accuracy: 0.11 - ETA: 2:27 - loss: 3.4853 - accuracy: 0.11 - ETA: 2:26 - loss: 3.4845 - accuracy: 0.11 - ETA: 2:26 - loss: 3.4839 - accuracy: 0.11 - ETA: 2:25 - loss: 3.4851 - accuracy: 0.11 - ETA: 2:25 - loss: 3.4860 - accuracy: 0.11 - ETA: 2:24 - loss: 3.4864 - accuracy: 0.11 - ETA: 2:24 - loss: 3.4846 - accuracy: 0.11 - ETA: 2:23 - loss: 3.4851 - accuracy: 0.11 - ETA: 2:22 - loss: 3.4861 - accuracy: 0.11 - ETA: 2:22 - loss: 3.4859 - accuracy: 0.11 - ETA: 2:21 - loss: 3.4841 - accuracy: 0.11 - ETA: 2:20 - loss: 3.4845 - accuracy: 0.11 - ETA: 2:20 - loss: 3.4853 - accuracy: 0.11 - ETA: 2:19 - loss: 3.4854 - accuracy: 0.11 - ETA: 2:19 - loss: 3.4857 - accuracy: 0.11 - ETA: 2:18 - loss: 3.4847 - accuracy: 0.11 - ETA: 2:17 - loss: 3.4831 - accuracy: 0.11 - ETA: 2:17 - loss: 3.4816 - accuracy: 0.11 - ETA: 2:16 - loss: 3.4816 - accuracy: 0.11 - ETA: 2:15 - loss: 3.4817 - accuracy: 0.11 - ETA: 2:15 - loss: 3.4815 - accuracy: 0.11 - ETA: 2:14 - loss: 3.4830 - accuracy: 0.11 - ETA: 2:14 - loss: 3.4811 - accuracy: 0.11 - ETA: 2:13 - loss: 3.4816 - accuracy: 0.11 - ETA: 2:12 - loss: 3.4816 - accuracy: 0.11 - ETA: 2:12 - loss: 3.4803 - accuracy: 0.11 - ETA: 2:11 - loss: 3.4817 - accuracy: 0.11 - ETA: 2:11 - loss: 3.4820 - accuracy: 0.11 - ETA: 2:10 - loss: 3.4816 - accuracy: 0.11 - ETA: 2:10 - loss: 3.4821 - accuracy: 0.11 - ETA: 2:09 - loss: 3.4804 - accuracy: 0.11 - ETA: 2:09 - loss: 3.4798 - accuracy: 0.11 - ETA: 2:08 - loss: 3.4808 - accuracy: 0.11 - ETA: 2:07 - loss: 3.4818 - accuracy: 0.11 - ETA: 2:07 - loss: 3.4818 - accuracy: 0.11 - ETA: 2:06 - loss: 3.4816 - accuracy: 0.11 - ETA: 2:06 - loss: 3.4817 - accuracy: 0.11 - ETA: 2:05 - loss: 3.4814 - accuracy: 0.11 - ETA: 2:05 - loss: 3.4806 - accuracy: 0.11 - ETA: 2:04 - loss: 3.4794 - accuracy: 0.11 - ETA: 2:03 - loss: 3.4796 - accuracy: 0.11 - ETA: 2:03 - loss: 3.4797 - accuracy: 0.11 - ETA: 2:02 - loss: 3.4794 - accuracy: 0.11 - ETA: 2:02 - loss: 3.4807 - accuracy: 0.11 - ETA: 2:01 - loss: 3.4806 - accuracy: 0.11 - ETA: 2:00 - loss: 3.4821 - accuracy: 0.11 - ETA: 2:00 - loss: 3.4814 - accuracy: 0.11 - ETA: 1:59 - loss: 3.4802 - accuracy: 0.11 - ETA: 1:58 - loss: 3.4803 - accuracy: 0.11 - ETA: 1:58 - loss: 3.4808 - accuracy: 0.11 - ETA: 1:57 - loss: 3.4813 - accuracy: 0.11 - ETA: 1:57 - loss: 3.4797 - accuracy: 0.11 - ETA: 1:56 - loss: 3.4799 - accuracy: 0.11 - ETA: 1:55 - loss: 3.4798 - accuracy: 0.11 - ETA: 1:55 - loss: 3.4806 - accuracy: 0.11 - ETA: 1:54 - loss: 3.4812 - accuracy: 0.11 - ETA: 1:54 - loss: 3.4811 - accuracy: 0.11 - ETA: 1:53 - loss: 3.4810 - accuracy: 0.11 - ETA: 1:53 - loss: 3.4818 - accuracy: 0.11 - ETA: 1:52 - loss: 3.4816 - accuracy: 0.11 - ETA: 1:51 - loss: 3.4813 - accuracy: 0.11 - ETA: 1:51 - loss: 3.4801 - accuracy: 0.11 - ETA: 1:50 - loss: 3.4803 - accuracy: 0.11 - ETA: 1:50 - loss: 3.4806 - accuracy: 0.11 - ETA: 1:49 - loss: 3.4807 - accuracy: 0.11 - ETA: 1:48 - loss: 3.4799 - accuracy: 0.11 - ETA: 1:48 - loss: 3.4785 - accuracy: 0.11 - ETA: 1:47 - loss: 3.4780 - accuracy: 0.11 - ETA: 1:47 - loss: 3.4785 - accuracy: 0.11 - ETA: 1:46 - loss: 3.4786 - accuracy: 0.11 - ETA: 1:45 - loss: 3.4805 - accuracy: 0.11 - ETA: 1:45 - loss: 3.4817 - accuracy: 0.11 - ETA: 1:44 - loss: 3.4823 - accuracy: 0.11 - ETA: 1:44 - loss: 3.4818 - accuracy: 0.11 - ETA: 1:43 - loss: 3.4804 - accuracy: 0.11 - ETA: 1:42 - loss: 3.4809 - accuracy: 0.11 - ETA: 1:42 - loss: 3.4803 - accuracy: 0.11 - ETA: 1:41 - loss: 3.4805 - accuracy: 0.11 - ETA: 1:40 - loss: 3.4809 - accuracy: 0.11 - ETA: 1:40 - loss: 3.4810 - accuracy: 0.11 - ETA: 1:39 - loss: 3.4813 - accuracy: 0.11 - ETA: 1:39 - loss: 3.4815 - accuracy: 0.11 - ETA: 1:38 - loss: 3.4817 - accuracy: 0.11 - ETA: 1:38 - loss: 3.4821 - accuracy: 0.11 - ETA: 1:37 - loss: 3.4830 - accuracy: 0.11 - ETA: 1:36 - loss: 3.4827 - accuracy: 0.11 - ETA: 1:36 - loss: 3.4823 - accuracy: 0.11 - ETA: 1:35 - loss: 3.4822 - accuracy: 0.11 - ETA: 1:35 - loss: 3.4827 - accuracy: 0.11 - ETA: 1:34 - loss: 3.4836 - accuracy: 0.11 - ETA: 1:33 - loss: 3.4829 - accuracy: 0.11 - ETA: 1:33 - loss: 3.4841 - accuracy: 0.11 - ETA: 1:32 - loss: 3.4847 - accuracy: 0.11 - ETA: 1:32 - loss: 3.4853 - accuracy: 0.11 - ETA: 1:31 - loss: 3.4861 - accuracy: 0.11 - ETA: 1:30 - loss: 3.4862 - accuracy: 0.11 - ETA: 1:30 - loss: 3.4860 - accuracy: 0.11 - ETA: 1:29 - loss: 3.4866 - accuracy: 0.11 - ETA: 1:29 - loss: 3.4871 - accuracy: 0.11 - ETA: 1:28 - loss: 3.4871 - accuracy: 0.11 - ETA: 1:27 - loss: 3.4878 - accuracy: 0.11 - ETA: 1:27 - loss: 3.4874 - accuracy: 0.11 - ETA: 1:26 - loss: 3.4878 - accuracy: 0.11 - ETA: 1:26 - loss: 3.4885 - accuracy: 0.11 - ETA: 1:25 - loss: 3.4890 - accuracy: 0.1147"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.4895 - accuracy: 0.11 - ETA: 1:24 - loss: 3.4893 - accuracy: 0.11 - ETA: 1:23 - loss: 3.4888 - accuracy: 0.11 - ETA: 1:23 - loss: 3.4890 - accuracy: 0.11 - ETA: 1:22 - loss: 3.4895 - accuracy: 0.11 - ETA: 1:22 - loss: 3.4895 - accuracy: 0.11 - ETA: 1:21 - loss: 3.4899 - accuracy: 0.11 - ETA: 1:20 - loss: 3.4904 - accuracy: 0.11 - ETA: 1:20 - loss: 3.4905 - accuracy: 0.11 - ETA: 1:19 - loss: 3.4903 - accuracy: 0.11 - ETA: 1:19 - loss: 3.4915 - accuracy: 0.11 - ETA: 1:18 - loss: 3.4925 - accuracy: 0.11 - ETA: 1:17 - loss: 3.4924 - accuracy: 0.11 - ETA: 1:17 - loss: 3.4925 - accuracy: 0.11 - ETA: 1:16 - loss: 3.4925 - accuracy: 0.11 - ETA: 1:16 - loss: 3.4930 - accuracy: 0.11 - ETA: 1:15 - loss: 3.4936 - accuracy: 0.11 - ETA: 1:14 - loss: 3.4933 - accuracy: 0.11 - ETA: 1:14 - loss: 3.4930 - accuracy: 0.11 - ETA: 1:13 - loss: 3.4929 - accuracy: 0.11 - ETA: 1:13 - loss: 3.4930 - accuracy: 0.11 - ETA: 1:12 - loss: 3.4927 - accuracy: 0.11 - ETA: 1:11 - loss: 3.4931 - accuracy: 0.11 - ETA: 1:11 - loss: 3.4938 - accuracy: 0.11 - ETA: 1:10 - loss: 3.4950 - accuracy: 0.11 - ETA: 1:10 - loss: 3.4952 - accuracy: 0.11 - ETA: 1:09 - loss: 3.4952 - accuracy: 0.11 - ETA: 1:09 - loss: 3.4948 - accuracy: 0.11 - ETA: 1:08 - loss: 3.4951 - accuracy: 0.11 - ETA: 1:07 - loss: 3.4948 - accuracy: 0.11 - ETA: 1:07 - loss: 3.4951 - accuracy: 0.11 - ETA: 1:06 - loss: 3.4948 - accuracy: 0.11 - ETA: 1:06 - loss: 3.4947 - accuracy: 0.11 - ETA: 1:05 - loss: 3.4943 - accuracy: 0.11 - ETA: 1:04 - loss: 3.4946 - accuracy: 0.11 - ETA: 1:04 - loss: 3.4949 - accuracy: 0.11 - ETA: 1:03 - loss: 3.4955 - accuracy: 0.11 - ETA: 1:03 - loss: 3.4955 - accuracy: 0.11 - ETA: 1:02 - loss: 3.4955 - accuracy: 0.11 - ETA: 1:01 - loss: 3.4962 - accuracy: 0.11 - ETA: 1:01 - loss: 3.4964 - accuracy: 0.11 - ETA: 1:00 - loss: 3.4964 - accuracy: 0.11 - ETA: 1:00 - loss: 3.4960 - accuracy: 0.11 - ETA: 59s - loss: 3.4963 - accuracy: 0.1140 - ETA: 59s - loss: 3.4967 - accuracy: 0.114 - ETA: 58s - loss: 3.4972 - accuracy: 0.113 - ETA: 57s - loss: 3.4977 - accuracy: 0.113 - ETA: 57s - loss: 3.4979 - accuracy: 0.113 - ETA: 56s - loss: 3.4980 - accuracy: 0.113 - ETA: 56s - loss: 3.4981 - accuracy: 0.113 - ETA: 55s - loss: 3.4975 - accuracy: 0.113 - ETA: 54s - loss: 3.4974 - accuracy: 0.113 - ETA: 54s - loss: 3.4967 - accuracy: 0.113 - ETA: 53s - loss: 3.4968 - accuracy: 0.113 - ETA: 53s - loss: 3.4967 - accuracy: 0.113 - ETA: 52s - loss: 3.4967 - accuracy: 0.113 - ETA: 51s - loss: 3.4969 - accuracy: 0.113 - ETA: 51s - loss: 3.4972 - accuracy: 0.113 - ETA: 50s - loss: 3.4975 - accuracy: 0.113 - ETA: 50s - loss: 3.4976 - accuracy: 0.113 - ETA: 49s - loss: 3.4975 - accuracy: 0.113 - ETA: 49s - loss: 3.4973 - accuracy: 0.113 - ETA: 48s - loss: 3.4971 - accuracy: 0.113 - ETA: 47s - loss: 3.4970 - accuracy: 0.113 - ETA: 47s - loss: 3.4973 - accuracy: 0.113 - ETA: 46s - loss: 3.4972 - accuracy: 0.113 - ETA: 46s - loss: 3.4973 - accuracy: 0.113 - ETA: 45s - loss: 3.4975 - accuracy: 0.112 - ETA: 44s - loss: 3.4977 - accuracy: 0.112 - ETA: 44s - loss: 3.4980 - accuracy: 0.112 - ETA: 43s - loss: 3.4980 - accuracy: 0.112 - ETA: 43s - loss: 3.4979 - accuracy: 0.112 - ETA: 42s - loss: 3.4984 - accuracy: 0.112 - ETA: 41s - loss: 3.4984 - accuracy: 0.112 - ETA: 41s - loss: 3.4979 - accuracy: 0.112 - ETA: 40s - loss: 3.4976 - accuracy: 0.112 - ETA: 40s - loss: 3.4983 - accuracy: 0.112 - ETA: 39s - loss: 3.4978 - accuracy: 0.112 - ETA: 38s - loss: 3.4976 - accuracy: 0.112 - ETA: 38s - loss: 3.4980 - accuracy: 0.112 - ETA: 37s - loss: 3.4979 - accuracy: 0.112 - ETA: 37s - loss: 3.4979 - accuracy: 0.112 - ETA: 36s - loss: 3.4978 - accuracy: 0.112 - ETA: 36s - loss: 3.4976 - accuracy: 0.112 - ETA: 35s - loss: 3.4972 - accuracy: 0.112 - ETA: 34s - loss: 3.4971 - accuracy: 0.112 - ETA: 34s - loss: 3.4970 - accuracy: 0.112 - ETA: 33s - loss: 3.4973 - accuracy: 0.112 - ETA: 33s - loss: 3.4977 - accuracy: 0.112 - ETA: 32s - loss: 3.4974 - accuracy: 0.112 - ETA: 31s - loss: 3.4977 - accuracy: 0.112 - ETA: 31s - loss: 3.4975 - accuracy: 0.112 - ETA: 30s - loss: 3.4980 - accuracy: 0.112 - ETA: 30s - loss: 3.4977 - accuracy: 0.112 - ETA: 29s - loss: 3.4985 - accuracy: 0.112 - ETA: 28s - loss: 3.4973 - accuracy: 0.112 - ETA: 28s - loss: 3.4977 - accuracy: 0.112 - ETA: 27s - loss: 3.4974 - accuracy: 0.112 - ETA: 27s - loss: 3.4965 - accuracy: 0.113 - ETA: 26s - loss: 3.4967 - accuracy: 0.112 - ETA: 25s - loss: 3.4968 - accuracy: 0.112 - ETA: 25s - loss: 3.4974 - accuracy: 0.112 - ETA: 24s - loss: 3.4972 - accuracy: 0.112 - ETA: 24s - loss: 3.4973 - accuracy: 0.112 - ETA: 23s - loss: 3.4973 - accuracy: 0.112 - ETA: 23s - loss: 3.4970 - accuracy: 0.112 - ETA: 22s - loss: 3.4966 - accuracy: 0.112 - ETA: 21s - loss: 3.4965 - accuracy: 0.112 - ETA: 21s - loss: 3.4961 - accuracy: 0.113 - ETA: 20s - loss: 3.4957 - accuracy: 0.113 - ETA: 20s - loss: 3.4958 - accuracy: 0.113 - ETA: 19s - loss: 3.4956 - accuracy: 0.113 - ETA: 18s - loss: 3.4963 - accuracy: 0.113 - ETA: 18s - loss: 3.4963 - accuracy: 0.113 - ETA: 17s - loss: 3.4964 - accuracy: 0.113 - ETA: 17s - loss: 3.4962 - accuracy: 0.113 - ETA: 16s - loss: 3.4958 - accuracy: 0.113 - ETA: 15s - loss: 3.4959 - accuracy: 0.113 - ETA: 15s - loss: 3.4963 - accuracy: 0.113 - ETA: 14s - loss: 3.4956 - accuracy: 0.113 - ETA: 14s - loss: 3.4955 - accuracy: 0.113 - ETA: 13s - loss: 3.4950 - accuracy: 0.113 - ETA: 13s - loss: 3.4949 - accuracy: 0.113 - ETA: 12s - loss: 3.4950 - accuracy: 0.113 - ETA: 11s - loss: 3.4951 - accuracy: 0.113 - ETA: 11s - loss: 3.4948 - accuracy: 0.113 - ETA: 10s - loss: 3.4948 - accuracy: 0.113 - ETA: 10s - loss: 3.4951 - accuracy: 0.113 - ETA: 9s - loss: 3.4947 - accuracy: 0.113 - ETA: 8s - loss: 3.4945 - accuracy: 0.11 - ETA: 8s - loss: 3.4946 - accuracy: 0.11 - ETA: 7s - loss: 3.4947 - accuracy: 0.11 - ETA: 7s - loss: 3.4949 - accuracy: 0.11 - ETA: 6s - loss: 3.4942 - accuracy: 0.11 - ETA: 5s - loss: 3.4943 - accuracy: 0.11 - ETA: 5s - loss: 3.4942 - accuracy: 0.11 - ETA: 4s - loss: 3.4946 - accuracy: 0.11 - ETA: 4s - loss: 3.4948 - accuracy: 0.11 - ETA: 3s - loss: 3.4951 - accuracy: 0.11 - ETA: 2s - loss: 3.4951 - accuracy: 0.11 - ETA: 2s - loss: 3.4953 - accuracy: 0.11 - ETA: 1s - loss: 3.4949 - accuracy: 0.11 - ETA: 1s - loss: 3.4951 - accuracy: 0.11 - ETA: 0s - loss: 3.4956 - accuracy: 0.11 - ETA: 0s - loss: 3.4952 - accuracy: 0.11 - 208s 5ms/step - loss: 3.4950 - accuracy: 0.1135 - val_loss: 3.8503 - val_accuracy: 0.0293\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:15 - loss: 3.5242 - accuracy: 0.10 - ETA: 3:10 - loss: 3.5251 - accuracy: 0.09 - ETA: 3:08 - loss: 3.4957 - accuracy: 0.09 - ETA: 3:09 - loss: 3.4897 - accuracy: 0.10 - ETA: 3:07 - loss: 3.4856 - accuracy: 0.10 - ETA: 3:05 - loss: 3.5038 - accuracy: 0.09 - ETA: 3:05 - loss: 3.4877 - accuracy: 0.10 - ETA: 3:04 - loss: 3.4834 - accuracy: 0.10 - ETA: 3:04 - loss: 3.4682 - accuracy: 0.10 - ETA: 3:03 - loss: 3.4801 - accuracy: 0.10 - ETA: 3:03 - loss: 3.4535 - accuracy: 0.11 - ETA: 3:02 - loss: 3.4568 - accuracy: 0.11 - ETA: 3:01 - loss: 3.4502 - accuracy: 0.11 - ETA: 3:00 - loss: 3.4508 - accuracy: 0.11 - ETA: 3:00 - loss: 3.4397 - accuracy: 0.11 - ETA: 2:59 - loss: 3.4412 - accuracy: 0.11 - ETA: 2:59 - loss: 3.4491 - accuracy: 0.11 - ETA: 2:59 - loss: 3.4486 - accuracy: 0.11 - ETA: 2:58 - loss: 3.4526 - accuracy: 0.11 - ETA: 2:58 - loss: 3.4454 - accuracy: 0.11 - ETA: 2:58 - loss: 3.4443 - accuracy: 0.11 - ETA: 2:57 - loss: 3.4456 - accuracy: 0.11 - ETA: 2:56 - loss: 3.4451 - accuracy: 0.11 - ETA: 2:56 - loss: 3.4485 - accuracy: 0.11 - ETA: 2:56 - loss: 3.4503 - accuracy: 0.11 - ETA: 2:56 - loss: 3.4494 - accuracy: 0.11 - ETA: 2:56 - loss: 3.4461 - accuracy: 0.11 - ETA: 2:56 - loss: 3.4476 - accuracy: 0.11 - ETA: 2:56 - loss: 3.4412 - accuracy: 0.11 - ETA: 2:56 - loss: 3.4436 - accuracy: 0.11 - ETA: 2:55 - loss: 3.4474 - accuracy: 0.11 - ETA: 2:54 - loss: 3.4493 - accuracy: 0.11 - ETA: 2:53 - loss: 3.4508 - accuracy: 0.11 - ETA: 2:53 - loss: 3.4498 - accuracy: 0.11 - ETA: 2:52 - loss: 3.4457 - accuracy: 0.11 - ETA: 2:52 - loss: 3.4449 - accuracy: 0.11 - ETA: 2:51 - loss: 3.4464 - accuracy: 0.11 - ETA: 2:50 - loss: 3.4455 - accuracy: 0.11 - ETA: 2:50 - loss: 3.4472 - accuracy: 0.11 - ETA: 2:49 - loss: 3.4455 - accuracy: 0.11 - ETA: 2:48 - loss: 3.4458 - accuracy: 0.11 - ETA: 2:48 - loss: 3.4426 - accuracy: 0.11 - ETA: 2:47 - loss: 3.4422 - accuracy: 0.11 - ETA: 2:47 - loss: 3.4406 - accuracy: 0.11 - ETA: 2:46 - loss: 3.4397 - accuracy: 0.11 - ETA: 2:45 - loss: 3.4372 - accuracy: 0.11 - ETA: 2:45 - loss: 3.4424 - accuracy: 0.11 - ETA: 2:44 - loss: 3.4444 - accuracy: 0.11 - ETA: 2:44 - loss: 3.4450 - accuracy: 0.11 - ETA: 2:43 - loss: 3.4463 - accuracy: 0.11 - ETA: 2:43 - loss: 3.4453 - accuracy: 0.11 - ETA: 2:42 - loss: 3.4489 - accuracy: 0.11 - ETA: 2:42 - loss: 3.4495 - accuracy: 0.11 - ETA: 2:42 - loss: 3.4487 - accuracy: 0.11 - ETA: 2:41 - loss: 3.4466 - accuracy: 0.11 - ETA: 2:41 - loss: 3.4445 - accuracy: 0.11 - ETA: 2:40 - loss: 3.4441 - accuracy: 0.11 - ETA: 2:39 - loss: 3.4448 - accuracy: 0.11 - ETA: 2:39 - loss: 3.4456 - accuracy: 0.11 - ETA: 2:38 - loss: 3.4457 - accuracy: 0.11 - ETA: 2:38 - loss: 3.4432 - accuracy: 0.11 - ETA: 2:37 - loss: 3.4428 - accuracy: 0.11 - ETA: 2:37 - loss: 3.4413 - accuracy: 0.11 - ETA: 2:37 - loss: 3.4427 - accuracy: 0.11 - ETA: 2:36 - loss: 3.4428 - accuracy: 0.11 - ETA: 2:36 - loss: 3.4440 - accuracy: 0.11 - ETA: 2:36 - loss: 3.4440 - accuracy: 0.11 - ETA: 2:35 - loss: 3.4434 - accuracy: 0.11 - ETA: 2:35 - loss: 3.4425 - accuracy: 0.11 - ETA: 2:34 - loss: 3.4440 - accuracy: 0.11 - ETA: 2:33 - loss: 3.4414 - accuracy: 0.11 - ETA: 2:33 - loss: 3.4403 - accuracy: 0.11 - ETA: 2:32 - loss: 3.4412 - accuracy: 0.11 - ETA: 2:32 - loss: 3.4416 - accuracy: 0.11 - ETA: 2:31 - loss: 3.4407 - accuracy: 0.11 - ETA: 2:31 - loss: 3.4414 - accuracy: 0.11 - ETA: 2:30 - loss: 3.4444 - accuracy: 0.11 - ETA: 2:30 - loss: 3.4458 - accuracy: 0.11 - ETA: 2:30 - loss: 3.4451 - accuracy: 0.11 - ETA: 2:29 - loss: 3.4461 - accuracy: 0.11 - ETA: 2:29 - loss: 3.4483 - accuracy: 0.11 - ETA: 2:29 - loss: 3.4471 - accuracy: 0.11 - ETA: 2:28 - loss: 3.4481 - accuracy: 0.11 - ETA: 2:27 - loss: 3.4484 - accuracy: 0.11 - ETA: 2:27 - loss: 3.4488 - accuracy: 0.11 - ETA: 2:26 - loss: 3.4475 - accuracy: 0.11 - ETA: 2:25 - loss: 3.4488 - accuracy: 0.11 - ETA: 2:25 - loss: 3.4497 - accuracy: 0.11 - ETA: 2:24 - loss: 3.4471 - accuracy: 0.11 - ETA: 2:23 - loss: 3.4477 - accuracy: 0.11 - ETA: 2:23 - loss: 3.4483 - accuracy: 0.11 - ETA: 2:22 - loss: 3.4500 - accuracy: 0.11 - ETA: 2:22 - loss: 3.4513 - accuracy: 0.11 - ETA: 2:21 - loss: 3.4520 - accuracy: 0.11 - ETA: 2:20 - loss: 3.4528 - accuracy: 0.11 - ETA: 2:20 - loss: 3.4541 - accuracy: 0.11 - ETA: 2:19 - loss: 3.4553 - accuracy: 0.11 - ETA: 2:19 - loss: 3.4546 - accuracy: 0.11 - ETA: 2:18 - loss: 3.4551 - accuracy: 0.11 - ETA: 2:18 - loss: 3.4570 - accuracy: 0.11 - ETA: 2:17 - loss: 3.4570 - accuracy: 0.11 - ETA: 2:16 - loss: 3.4572 - accuracy: 0.11 - ETA: 2:16 - loss: 3.4572 - accuracy: 0.11 - ETA: 2:15 - loss: 3.4568 - accuracy: 0.11 - ETA: 2:15 - loss: 3.4570 - accuracy: 0.11 - ETA: 2:14 - loss: 3.4573 - accuracy: 0.11 - ETA: 2:14 - loss: 3.4587 - accuracy: 0.11 - ETA: 2:13 - loss: 3.4593 - accuracy: 0.11 - ETA: 2:13 - loss: 3.4595 - accuracy: 0.11 - ETA: 2:12 - loss: 3.4592 - accuracy: 0.11 - ETA: 2:11 - loss: 3.4600 - accuracy: 0.11 - ETA: 2:11 - loss: 3.4618 - accuracy: 0.11 - ETA: 2:10 - loss: 3.4612 - accuracy: 0.11 - ETA: 2:10 - loss: 3.4625 - accuracy: 0.11 - ETA: 2:09 - loss: 3.4634 - accuracy: 0.11 - ETA: 2:08 - loss: 3.4642 - accuracy: 0.11 - ETA: 2:08 - loss: 3.4652 - accuracy: 0.11 - ETA: 2:07 - loss: 3.4660 - accuracy: 0.11 - ETA: 2:06 - loss: 3.4664 - accuracy: 0.11 - ETA: 2:06 - loss: 3.4649 - accuracy: 0.11 - ETA: 2:05 - loss: 3.4645 - accuracy: 0.11 - ETA: 2:05 - loss: 3.4650 - accuracy: 0.11 - ETA: 2:04 - loss: 3.4658 - accuracy: 0.11 - ETA: 2:03 - loss: 3.4659 - accuracy: 0.11 - ETA: 2:03 - loss: 3.4636 - accuracy: 0.11 - ETA: 2:02 - loss: 3.4664 - accuracy: 0.11 - ETA: 2:02 - loss: 3.4673 - accuracy: 0.11 - ETA: 2:01 - loss: 3.4671 - accuracy: 0.11 - ETA: 2:00 - loss: 3.4667 - accuracy: 0.11 - ETA: 2:00 - loss: 3.4658 - accuracy: 0.11 - ETA: 1:59 - loss: 3.4646 - accuracy: 0.11 - ETA: 1:58 - loss: 3.4648 - accuracy: 0.11 - ETA: 1:58 - loss: 3.4657 - accuracy: 0.11 - ETA: 1:58 - loss: 3.4661 - accuracy: 0.11 - ETA: 1:57 - loss: 3.4668 - accuracy: 0.11 - ETA: 1:56 - loss: 3.4671 - accuracy: 0.11 - ETA: 1:56 - loss: 3.4678 - accuracy: 0.11 - ETA: 1:55 - loss: 3.4676 - accuracy: 0.11 - ETA: 1:55 - loss: 3.4672 - accuracy: 0.11 - ETA: 1:54 - loss: 3.4674 - accuracy: 0.11 - ETA: 1:53 - loss: 3.4669 - accuracy: 0.11 - ETA: 1:53 - loss: 3.4676 - accuracy: 0.11 - ETA: 1:52 - loss: 3.4674 - accuracy: 0.11 - ETA: 1:51 - loss: 3.4654 - accuracy: 0.11 - ETA: 1:51 - loss: 3.4657 - accuracy: 0.11 - ETA: 1:50 - loss: 3.4657 - accuracy: 0.11 - ETA: 1:50 - loss: 3.4650 - accuracy: 0.11 - ETA: 1:49 - loss: 3.4651 - accuracy: 0.11 - ETA: 1:48 - loss: 3.4655 - accuracy: 0.11 - ETA: 1:48 - loss: 3.4648 - accuracy: 0.11 - ETA: 1:47 - loss: 3.4636 - accuracy: 0.11 - ETA: 1:47 - loss: 3.4640 - accuracy: 0.11 - ETA: 1:46 - loss: 3.4649 - accuracy: 0.11 - ETA: 1:45 - loss: 3.4660 - accuracy: 0.11 - ETA: 1:45 - loss: 3.4654 - accuracy: 0.11 - ETA: 1:44 - loss: 3.4656 - accuracy: 0.11 - ETA: 1:43 - loss: 3.4661 - accuracy: 0.11 - ETA: 1:43 - loss: 3.4665 - accuracy: 0.11 - ETA: 1:42 - loss: 3.4664 - accuracy: 0.11 - ETA: 1:42 - loss: 3.4668 - accuracy: 0.11 - ETA: 1:41 - loss: 3.4672 - accuracy: 0.11 - ETA: 1:41 - loss: 3.4668 - accuracy: 0.11 - ETA: 1:40 - loss: 3.4668 - accuracy: 0.11 - ETA: 1:39 - loss: 3.4658 - accuracy: 0.11 - ETA: 1:39 - loss: 3.4659 - accuracy: 0.11 - ETA: 1:38 - loss: 3.4658 - accuracy: 0.11 - ETA: 1:38 - loss: 3.4647 - accuracy: 0.11 - ETA: 1:37 - loss: 3.4658 - accuracy: 0.11 - ETA: 1:36 - loss: 3.4667 - accuracy: 0.11 - ETA: 1:36 - loss: 3.4666 - accuracy: 0.11 - ETA: 1:35 - loss: 3.4668 - accuracy: 0.11 - ETA: 1:35 - loss: 3.4658 - accuracy: 0.11 - ETA: 1:34 - loss: 3.4663 - accuracy: 0.11 - ETA: 1:33 - loss: 3.4661 - accuracy: 0.11 - ETA: 1:33 - loss: 3.4644 - accuracy: 0.11 - ETA: 1:32 - loss: 3.4651 - accuracy: 0.11 - ETA: 1:32 - loss: 3.4657 - accuracy: 0.11 - ETA: 1:31 - loss: 3.4659 - accuracy: 0.11 - ETA: 1:30 - loss: 3.4660 - accuracy: 0.11 - ETA: 1:30 - loss: 3.4656 - accuracy: 0.11 - ETA: 1:29 - loss: 3.4660 - accuracy: 0.11 - ETA: 1:28 - loss: 3.4659 - accuracy: 0.11 - ETA: 1:28 - loss: 3.4659 - accuracy: 0.11 - ETA: 1:27 - loss: 3.4674 - accuracy: 0.11 - ETA: 1:27 - loss: 3.4681 - accuracy: 0.11 - ETA: 1:26 - loss: 3.4677 - accuracy: 0.1144"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:26 - loss: 3.4680 - accuracy: 0.11 - ETA: 1:25 - loss: 3.4682 - accuracy: 0.11 - ETA: 1:24 - loss: 3.4681 - accuracy: 0.11 - ETA: 1:24 - loss: 3.4684 - accuracy: 0.11 - ETA: 1:23 - loss: 3.4682 - accuracy: 0.11 - ETA: 1:23 - loss: 3.4676 - accuracy: 0.11 - ETA: 1:22 - loss: 3.4667 - accuracy: 0.11 - ETA: 1:21 - loss: 3.4671 - accuracy: 0.11 - ETA: 1:21 - loss: 3.4665 - accuracy: 0.11 - ETA: 1:20 - loss: 3.4669 - accuracy: 0.11 - ETA: 1:19 - loss: 3.4676 - accuracy: 0.11 - ETA: 1:19 - loss: 3.4690 - accuracy: 0.11 - ETA: 1:18 - loss: 3.4686 - accuracy: 0.11 - ETA: 1:18 - loss: 3.4682 - accuracy: 0.11 - ETA: 1:17 - loss: 3.4680 - accuracy: 0.11 - ETA: 1:16 - loss: 3.4682 - accuracy: 0.11 - ETA: 1:16 - loss: 3.4674 - accuracy: 0.11 - ETA: 1:15 - loss: 3.4676 - accuracy: 0.11 - ETA: 1:15 - loss: 3.4673 - accuracy: 0.11 - ETA: 1:14 - loss: 3.4675 - accuracy: 0.11 - ETA: 1:13 - loss: 3.4683 - accuracy: 0.11 - ETA: 1:13 - loss: 3.4682 - accuracy: 0.11 - ETA: 1:12 - loss: 3.4685 - accuracy: 0.11 - ETA: 1:12 - loss: 3.4688 - accuracy: 0.11 - ETA: 1:11 - loss: 3.4698 - accuracy: 0.11 - ETA: 1:10 - loss: 3.4697 - accuracy: 0.11 - ETA: 1:10 - loss: 3.4696 - accuracy: 0.11 - ETA: 1:09 - loss: 3.4701 - accuracy: 0.11 - ETA: 1:09 - loss: 3.4696 - accuracy: 0.11 - ETA: 1:08 - loss: 3.4687 - accuracy: 0.11 - ETA: 1:08 - loss: 3.4680 - accuracy: 0.11 - ETA: 1:07 - loss: 3.4680 - accuracy: 0.11 - ETA: 1:06 - loss: 3.4681 - accuracy: 0.11 - ETA: 1:06 - loss: 3.4681 - accuracy: 0.11 - ETA: 1:05 - loss: 3.4680 - accuracy: 0.11 - ETA: 1:05 - loss: 3.4671 - accuracy: 0.11 - ETA: 1:04 - loss: 3.4672 - accuracy: 0.11 - ETA: 1:03 - loss: 3.4677 - accuracy: 0.11 - ETA: 1:03 - loss: 3.4676 - accuracy: 0.11 - ETA: 1:02 - loss: 3.4679 - accuracy: 0.11 - ETA: 1:02 - loss: 3.4675 - accuracy: 0.11 - ETA: 1:01 - loss: 3.4674 - accuracy: 0.11 - ETA: 1:00 - loss: 3.4666 - accuracy: 0.11 - ETA: 1:00 - loss: 3.4673 - accuracy: 0.11 - ETA: 59s - loss: 3.4677 - accuracy: 0.1140 - ETA: 58s - loss: 3.4683 - accuracy: 0.114 - ETA: 58s - loss: 3.4684 - accuracy: 0.114 - ETA: 57s - loss: 3.4686 - accuracy: 0.113 - ETA: 57s - loss: 3.4689 - accuracy: 0.113 - ETA: 56s - loss: 3.4688 - accuracy: 0.113 - ETA: 55s - loss: 3.4687 - accuracy: 0.113 - ETA: 55s - loss: 3.4684 - accuracy: 0.114 - ETA: 54s - loss: 3.4684 - accuracy: 0.114 - ETA: 54s - loss: 3.4678 - accuracy: 0.114 - ETA: 53s - loss: 3.4680 - accuracy: 0.114 - ETA: 53s - loss: 3.4679 - accuracy: 0.114 - ETA: 52s - loss: 3.4678 - accuracy: 0.114 - ETA: 51s - loss: 3.4673 - accuracy: 0.114 - ETA: 51s - loss: 3.4673 - accuracy: 0.114 - ETA: 50s - loss: 3.4672 - accuracy: 0.114 - ETA: 50s - loss: 3.4671 - accuracy: 0.114 - ETA: 49s - loss: 3.4669 - accuracy: 0.114 - ETA: 48s - loss: 3.4669 - accuracy: 0.114 - ETA: 48s - loss: 3.4671 - accuracy: 0.114 - ETA: 47s - loss: 3.4663 - accuracy: 0.114 - ETA: 47s - loss: 3.4664 - accuracy: 0.114 - ETA: 46s - loss: 3.4656 - accuracy: 0.114 - ETA: 45s - loss: 3.4650 - accuracy: 0.114 - ETA: 45s - loss: 3.4650 - accuracy: 0.114 - ETA: 44s - loss: 3.4646 - accuracy: 0.114 - ETA: 44s - loss: 3.4642 - accuracy: 0.114 - ETA: 43s - loss: 3.4644 - accuracy: 0.114 - ETA: 42s - loss: 3.4642 - accuracy: 0.114 - ETA: 42s - loss: 3.4637 - accuracy: 0.114 - ETA: 41s - loss: 3.4634 - accuracy: 0.114 - ETA: 41s - loss: 3.4637 - accuracy: 0.114 - ETA: 40s - loss: 3.4637 - accuracy: 0.114 - ETA: 39s - loss: 3.4633 - accuracy: 0.114 - ETA: 39s - loss: 3.4629 - accuracy: 0.114 - ETA: 38s - loss: 3.4631 - accuracy: 0.114 - ETA: 38s - loss: 3.4632 - accuracy: 0.114 - ETA: 37s - loss: 3.4639 - accuracy: 0.114 - ETA: 36s - loss: 3.4645 - accuracy: 0.114 - ETA: 36s - loss: 3.4643 - accuracy: 0.114 - ETA: 35s - loss: 3.4641 - accuracy: 0.114 - ETA: 35s - loss: 3.4644 - accuracy: 0.114 - ETA: 34s - loss: 3.4646 - accuracy: 0.114 - ETA: 33s - loss: 3.4643 - accuracy: 0.114 - ETA: 33s - loss: 3.4643 - accuracy: 0.114 - ETA: 32s - loss: 3.4639 - accuracy: 0.114 - ETA: 32s - loss: 3.4640 - accuracy: 0.114 - ETA: 31s - loss: 3.4636 - accuracy: 0.114 - ETA: 30s - loss: 3.4630 - accuracy: 0.114 - ETA: 30s - loss: 3.4631 - accuracy: 0.114 - ETA: 29s - loss: 3.4634 - accuracy: 0.114 - ETA: 29s - loss: 3.4635 - accuracy: 0.114 - ETA: 28s - loss: 3.4634 - accuracy: 0.114 - ETA: 27s - loss: 3.4633 - accuracy: 0.114 - ETA: 27s - loss: 3.4630 - accuracy: 0.114 - ETA: 26s - loss: 3.4623 - accuracy: 0.114 - ETA: 26s - loss: 3.4615 - accuracy: 0.114 - ETA: 25s - loss: 3.4611 - accuracy: 0.114 - ETA: 24s - loss: 3.4612 - accuracy: 0.114 - ETA: 24s - loss: 3.4614 - accuracy: 0.114 - ETA: 23s - loss: 3.4614 - accuracy: 0.114 - ETA: 23s - loss: 3.4613 - accuracy: 0.114 - ETA: 22s - loss: 3.4613 - accuracy: 0.114 - ETA: 21s - loss: 3.4613 - accuracy: 0.114 - ETA: 21s - loss: 3.4614 - accuracy: 0.114 - ETA: 20s - loss: 3.4617 - accuracy: 0.114 - ETA: 20s - loss: 3.4621 - accuracy: 0.114 - ETA: 19s - loss: 3.4619 - accuracy: 0.114 - ETA: 19s - loss: 3.4622 - accuracy: 0.114 - ETA: 18s - loss: 3.4626 - accuracy: 0.114 - ETA: 17s - loss: 3.4624 - accuracy: 0.114 - ETA: 17s - loss: 3.4620 - accuracy: 0.114 - ETA: 16s - loss: 3.4616 - accuracy: 0.114 - ETA: 16s - loss: 3.4614 - accuracy: 0.114 - ETA: 15s - loss: 3.4617 - accuracy: 0.114 - ETA: 14s - loss: 3.4615 - accuracy: 0.114 - ETA: 14s - loss: 3.4616 - accuracy: 0.114 - ETA: 13s - loss: 3.4613 - accuracy: 0.114 - ETA: 13s - loss: 3.4615 - accuracy: 0.114 - ETA: 12s - loss: 3.4613 - accuracy: 0.114 - ETA: 11s - loss: 3.4611 - accuracy: 0.114 - ETA: 11s - loss: 3.4614 - accuracy: 0.114 - ETA: 10s - loss: 3.4614 - accuracy: 0.114 - ETA: 10s - loss: 3.4616 - accuracy: 0.114 - ETA: 9s - loss: 3.4620 - accuracy: 0.114 - ETA: 8s - loss: 3.4616 - accuracy: 0.11 - ETA: 8s - loss: 3.4612 - accuracy: 0.11 - ETA: 7s - loss: 3.4613 - accuracy: 0.11 - ETA: 7s - loss: 3.4608 - accuracy: 0.11 - ETA: 6s - loss: 3.4604 - accuracy: 0.11 - ETA: 5s - loss: 3.4603 - accuracy: 0.11 - ETA: 5s - loss: 3.4600 - accuracy: 0.11 - ETA: 4s - loss: 3.4605 - accuracy: 0.11 - ETA: 4s - loss: 3.4608 - accuracy: 0.11 - ETA: 3s - loss: 3.4609 - accuracy: 0.11 - ETA: 3s - loss: 3.4614 - accuracy: 0.11 - ETA: 2s - loss: 3.4614 - accuracy: 0.11 - ETA: 1s - loss: 3.4619 - accuracy: 0.11 - ETA: 1s - loss: 3.4620 - accuracy: 0.11 - ETA: 0s - loss: 3.4621 - accuracy: 0.11 - ETA: 0s - loss: 3.4624 - accuracy: 0.11 - 215s 5ms/step - loss: 3.4623 - accuracy: 0.1143 - val_loss: 3.8735 - val_accuracy: 0.0264\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:08 - loss: 3.3023 - accuracy: 0.16 - ETA: 5:06 - loss: 3.3224 - accuracy: 0.14 - ETA: 5:02 - loss: 3.3367 - accuracy: 0.14 - ETA: 4:59 - loss: 3.3177 - accuracy: 0.14 - ETA: 4:59 - loss: 3.3375 - accuracy: 0.14 - ETA: 5:02 - loss: 3.3790 - accuracy: 0.14 - ETA: 5:03 - loss: 3.3584 - accuracy: 0.14 - ETA: 5:02 - loss: 3.3709 - accuracy: 0.14 - ETA: 5:02 - loss: 3.3652 - accuracy: 0.14 - ETA: 5:02 - loss: 3.3744 - accuracy: 0.14 - ETA: 5:04 - loss: 3.3817 - accuracy: 0.14 - ETA: 5:02 - loss: 3.3821 - accuracy: 0.14 - ETA: 5:01 - loss: 3.3807 - accuracy: 0.14 - ETA: 4:59 - loss: 3.3912 - accuracy: 0.13 - ETA: 4:58 - loss: 3.3890 - accuracy: 0.13 - ETA: 4:57 - loss: 3.4060 - accuracy: 0.13 - ETA: 4:56 - loss: 3.4151 - accuracy: 0.12 - ETA: 4:55 - loss: 3.4139 - accuracy: 0.12 - ETA: 4:54 - loss: 3.4137 - accuracy: 0.12 - ETA: 4:52 - loss: 3.4132 - accuracy: 0.12 - ETA: 4:51 - loss: 3.4200 - accuracy: 0.12 - ETA: 4:50 - loss: 3.4274 - accuracy: 0.12 - ETA: 4:49 - loss: 3.4355 - accuracy: 0.12 - ETA: 4:48 - loss: 3.4415 - accuracy: 0.12 - ETA: 4:47 - loss: 3.4419 - accuracy: 0.12 - ETA: 4:46 - loss: 3.4475 - accuracy: 0.12 - ETA: 4:45 - loss: 3.4491 - accuracy: 0.12 - ETA: 4:44 - loss: 3.4494 - accuracy: 0.12 - ETA: 4:43 - loss: 3.4508 - accuracy: 0.12 - ETA: 4:42 - loss: 3.4473 - accuracy: 0.12 - ETA: 4:41 - loss: 3.4476 - accuracy: 0.12 - ETA: 4:40 - loss: 3.4455 - accuracy: 0.12 - ETA: 4:39 - loss: 3.4425 - accuracy: 0.12 - ETA: 4:38 - loss: 3.4424 - accuracy: 0.12 - ETA: 4:37 - loss: 3.4372 - accuracy: 0.12 - ETA: 4:35 - loss: 3.4343 - accuracy: 0.12 - ETA: 4:34 - loss: 3.4340 - accuracy: 0.12 - ETA: 4:34 - loss: 3.4318 - accuracy: 0.12 - ETA: 4:32 - loss: 3.4327 - accuracy: 0.12 - ETA: 4:32 - loss: 3.4308 - accuracy: 0.12 - ETA: 4:31 - loss: 3.4306 - accuracy: 0.12 - ETA: 4:30 - loss: 3.4326 - accuracy: 0.12 - ETA: 4:29 - loss: 3.4358 - accuracy: 0.12 - ETA: 4:29 - loss: 3.4385 - accuracy: 0.12 - ETA: 4:28 - loss: 3.4367 - accuracy: 0.12 - ETA: 4:27 - loss: 3.4354 - accuracy: 0.12 - ETA: 4:26 - loss: 3.4337 - accuracy: 0.12 - ETA: 4:25 - loss: 3.4351 - accuracy: 0.12 - ETA: 4:25 - loss: 3.4361 - accuracy: 0.12 - ETA: 4:24 - loss: 3.4383 - accuracy: 0.12 - ETA: 4:23 - loss: 3.4371 - accuracy: 0.12 - ETA: 4:22 - loss: 3.4354 - accuracy: 0.12 - ETA: 4:20 - loss: 3.4327 - accuracy: 0.12 - ETA: 4:20 - loss: 3.4352 - accuracy: 0.12 - ETA: 4:19 - loss: 3.4340 - accuracy: 0.12 - ETA: 4:18 - loss: 3.4331 - accuracy: 0.12 - ETA: 4:17 - loss: 3.4317 - accuracy: 0.12 - ETA: 4:17 - loss: 3.4347 - accuracy: 0.12 - ETA: 4:16 - loss: 3.4361 - accuracy: 0.12 - ETA: 4:15 - loss: 3.4353 - accuracy: 0.12 - ETA: 4:13 - loss: 3.4348 - accuracy: 0.12 - ETA: 4:11 - loss: 3.4330 - accuracy: 0.12 - ETA: 4:09 - loss: 3.4331 - accuracy: 0.12 - ETA: 4:07 - loss: 3.4350 - accuracy: 0.12 - ETA: 4:04 - loss: 3.4364 - accuracy: 0.12 - ETA: 4:02 - loss: 3.4348 - accuracy: 0.12 - ETA: 4:00 - loss: 3.4351 - accuracy: 0.12 - ETA: 3:58 - loss: 3.4348 - accuracy: 0.12 - ETA: 3:56 - loss: 3.4331 - accuracy: 0.12 - ETA: 3:53 - loss: 3.4333 - accuracy: 0.12 - ETA: 3:51 - loss: 3.4345 - accuracy: 0.12 - ETA: 3:49 - loss: 3.4368 - accuracy: 0.12 - ETA: 3:47 - loss: 3.4343 - accuracy: 0.12 - ETA: 3:45 - loss: 3.4329 - accuracy: 0.12 - ETA: 3:43 - loss: 3.4341 - accuracy: 0.12 - ETA: 3:42 - loss: 3.4332 - accuracy: 0.12 - ETA: 3:40 - loss: 3.4296 - accuracy: 0.12 - ETA: 3:38 - loss: 3.4304 - accuracy: 0.12 - ETA: 3:36 - loss: 3.4296 - accuracy: 0.12 - ETA: 3:34 - loss: 3.4292 - accuracy: 0.12 - ETA: 3:32 - loss: 3.4273 - accuracy: 0.12 - ETA: 3:31 - loss: 3.4269 - accuracy: 0.12 - ETA: 3:29 - loss: 3.4261 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4251 - accuracy: 0.12 - ETA: 3:26 - loss: 3.4265 - accuracy: 0.12 - ETA: 3:24 - loss: 3.4264 - accuracy: 0.12 - ETA: 3:23 - loss: 3.4268 - accuracy: 0.12 - ETA: 3:21 - loss: 3.4262 - accuracy: 0.12 - ETA: 3:20 - loss: 3.4283 - accuracy: 0.12 - ETA: 3:18 - loss: 3.4279 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4286 - accuracy: 0.12 - ETA: 3:16 - loss: 3.4303 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4315 - accuracy: 0.12 - ETA: 3:13 - loss: 3.4330 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4337 - accuracy: 0.12 - ETA: 3:10 - loss: 3.4335 - accuracy: 0.12 - ETA: 3:08 - loss: 3.4330 - accuracy: 0.12 - ETA: 3:07 - loss: 3.4345 - accuracy: 0.12 - ETA: 3:06 - loss: 3.4349 - accuracy: 0.12 - ETA: 3:04 - loss: 3.4355 - accuracy: 0.12 - ETA: 3:03 - loss: 3.4349 - accuracy: 0.12 - ETA: 3:02 - loss: 3.4363 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4349 - accuracy: 0.12 - ETA: 2:59 - loss: 3.4340 - accuracy: 0.12 - ETA: 2:58 - loss: 3.4336 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4332 - accuracy: 0.12 - ETA: 2:55 - loss: 3.4323 - accuracy: 0.12 - ETA: 2:54 - loss: 3.4319 - accuracy: 0.12 - ETA: 2:53 - loss: 3.4326 - accuracy: 0.12 - ETA: 2:51 - loss: 3.4335 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4325 - accuracy: 0.12 - ETA: 2:49 - loss: 3.4318 - accuracy: 0.12 - ETA: 2:48 - loss: 3.4315 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4305 - accuracy: 0.12 - ETA: 2:46 - loss: 3.4310 - accuracy: 0.12 - ETA: 2:45 - loss: 3.4321 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4309 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4311 - accuracy: 0.12 - ETA: 2:41 - loss: 3.4314 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4305 - accuracy: 0.12 - ETA: 2:39 - loss: 3.4285 - accuracy: 0.12 - ETA: 2:38 - loss: 3.4289 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4288 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4301 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4302 - accuracy: 0.12 - ETA: 2:34 - loss: 3.4290 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4290 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4297 - accuracy: 0.12 - ETA: 2:31 - loss: 3.4283 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4281 - accuracy: 0.12 - ETA: 2:29 - loss: 3.4276 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4278 - accuracy: 0.12 - ETA: 2:27 - loss: 3.4284 - accuracy: 0.12 - ETA: 2:26 - loss: 3.4286 - accuracy: 0.12 - ETA: 2:25 - loss: 3.4282 - accuracy: 0.12 - ETA: 2:24 - loss: 3.4285 - accuracy: 0.12 - ETA: 2:23 - loss: 3.4299 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4292 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4289 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4291 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4295 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4285 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4271 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4267 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4263 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4259 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4249 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4250 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4243 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4234 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4235 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4227 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4227 - accuracy: 0.12 - ETA: 2:08 - loss: 3.4237 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4221 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4223 - accuracy: 0.12 - ETA: 2:05 - loss: 3.4227 - accuracy: 0.12 - ETA: 2:04 - loss: 3.4232 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4240 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4234 - accuracy: 0.12 - ETA: 2:02 - loss: 3.4235 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4236 - accuracy: 0.12 - ETA: 2:00 - loss: 3.4249 - accuracy: 0.12 - ETA: 1:59 - loss: 3.4246 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4253 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4264 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4270 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4278 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4273 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4272 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4271 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4269 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4270 - accuracy: 0.12 - ETA: 1:51 - loss: 3.4272 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4264 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4261 - accuracy: 0.12 - ETA: 1:48 - loss: 3.4257 - accuracy: 0.12 - ETA: 1:48 - loss: 3.4254 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4256 - accuracy: 0.12 - ETA: 1:46 - loss: 3.4260 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4256 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4261 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4268 - accuracy: 0.12 - ETA: 1:43 - loss: 3.4266 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4265 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4261 - accuracy: 0.1232"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:40 - loss: 3.4262 - accuracy: 0.12 - ETA: 1:40 - loss: 3.4269 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4265 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4264 - accuracy: 0.12 - ETA: 1:37 - loss: 3.4264 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4262 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4269 - accuracy: 0.12 - ETA: 1:35 - loss: 3.4271 - accuracy: 0.12 - ETA: 1:34 - loss: 3.4267 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4268 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4254 - accuracy: 0.12 - ETA: 1:32 - loss: 3.4252 - accuracy: 0.12 - ETA: 1:31 - loss: 3.4253 - accuracy: 0.12 - ETA: 1:30 - loss: 3.4257 - accuracy: 0.12 - ETA: 1:30 - loss: 3.4250 - accuracy: 0.12 - ETA: 1:29 - loss: 3.4251 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4255 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4258 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4260 - accuracy: 0.12 - ETA: 1:26 - loss: 3.4252 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4252 - accuracy: 0.12 - ETA: 1:24 - loss: 3.4252 - accuracy: 0.12 - ETA: 1:24 - loss: 3.4258 - accuracy: 0.12 - ETA: 1:23 - loss: 3.4262 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4256 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4255 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4246 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4243 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4244 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4242 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4244 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4241 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4234 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4237 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4238 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4237 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4241 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4239 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4246 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4244 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4239 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4239 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4235 - accuracy: 0.12 - ETA: 1:08 - loss: 3.4236 - accuracy: 0.12 - ETA: 1:07 - loss: 3.4233 - accuracy: 0.12 - ETA: 1:07 - loss: 3.4227 - accuracy: 0.12 - ETA: 1:06 - loss: 3.4222 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4221 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4216 - accuracy: 0.12 - ETA: 1:04 - loss: 3.4207 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4203 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4205 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4210 - accuracy: 0.12 - ETA: 1:01 - loss: 3.4217 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4211 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4212 - accuracy: 0.12 - ETA: 59s - loss: 3.4211 - accuracy: 0.1237 - ETA: 58s - loss: 3.4208 - accuracy: 0.123 - ETA: 57s - loss: 3.4211 - accuracy: 0.123 - ETA: 57s - loss: 3.4219 - accuracy: 0.123 - ETA: 56s - loss: 3.4224 - accuracy: 0.123 - ETA: 55s - loss: 3.4224 - accuracy: 0.123 - ETA: 55s - loss: 3.4226 - accuracy: 0.123 - ETA: 54s - loss: 3.4227 - accuracy: 0.123 - ETA: 53s - loss: 3.4231 - accuracy: 0.123 - ETA: 53s - loss: 3.4230 - accuracy: 0.123 - ETA: 52s - loss: 3.4227 - accuracy: 0.123 - ETA: 51s - loss: 3.4236 - accuracy: 0.123 - ETA: 50s - loss: 3.4241 - accuracy: 0.123 - ETA: 50s - loss: 3.4241 - accuracy: 0.123 - ETA: 49s - loss: 3.4243 - accuracy: 0.123 - ETA: 48s - loss: 3.4247 - accuracy: 0.123 - ETA: 48s - loss: 3.4250 - accuracy: 0.123 - ETA: 47s - loss: 3.4251 - accuracy: 0.123 - ETA: 46s - loss: 3.4254 - accuracy: 0.122 - ETA: 46s - loss: 3.4252 - accuracy: 0.123 - ETA: 45s - loss: 3.4247 - accuracy: 0.123 - ETA: 44s - loss: 3.4248 - accuracy: 0.123 - ETA: 44s - loss: 3.4246 - accuracy: 0.123 - ETA: 43s - loss: 3.4245 - accuracy: 0.123 - ETA: 42s - loss: 3.4239 - accuracy: 0.123 - ETA: 42s - loss: 3.4238 - accuracy: 0.123 - ETA: 41s - loss: 3.4237 - accuracy: 0.123 - ETA: 40s - loss: 3.4237 - accuracy: 0.123 - ETA: 39s - loss: 3.4231 - accuracy: 0.123 - ETA: 39s - loss: 3.4230 - accuracy: 0.123 - ETA: 38s - loss: 3.4224 - accuracy: 0.123 - ETA: 37s - loss: 3.4222 - accuracy: 0.123 - ETA: 37s - loss: 3.4218 - accuracy: 0.123 - ETA: 36s - loss: 3.4212 - accuracy: 0.123 - ETA: 35s - loss: 3.4208 - accuracy: 0.123 - ETA: 35s - loss: 3.4203 - accuracy: 0.123 - ETA: 34s - loss: 3.4207 - accuracy: 0.123 - ETA: 33s - loss: 3.4208 - accuracy: 0.123 - ETA: 33s - loss: 3.4216 - accuracy: 0.123 - ETA: 32s - loss: 3.4212 - accuracy: 0.123 - ETA: 31s - loss: 3.4212 - accuracy: 0.123 - ETA: 31s - loss: 3.4213 - accuracy: 0.123 - ETA: 30s - loss: 3.4216 - accuracy: 0.123 - ETA: 29s - loss: 3.4218 - accuracy: 0.123 - ETA: 29s - loss: 3.4218 - accuracy: 0.123 - ETA: 28s - loss: 3.4216 - accuracy: 0.123 - ETA: 27s - loss: 3.4210 - accuracy: 0.123 - ETA: 27s - loss: 3.4206 - accuracy: 0.123 - ETA: 26s - loss: 3.4208 - accuracy: 0.123 - ETA: 25s - loss: 3.4197 - accuracy: 0.124 - ETA: 25s - loss: 3.4202 - accuracy: 0.124 - ETA: 24s - loss: 3.4199 - accuracy: 0.124 - ETA: 23s - loss: 3.4195 - accuracy: 0.124 - ETA: 23s - loss: 3.4199 - accuracy: 0.124 - ETA: 22s - loss: 3.4201 - accuracy: 0.123 - ETA: 21s - loss: 3.4199 - accuracy: 0.123 - ETA: 21s - loss: 3.4198 - accuracy: 0.123 - ETA: 20s - loss: 3.4201 - accuracy: 0.123 - ETA: 19s - loss: 3.4199 - accuracy: 0.123 - ETA: 19s - loss: 3.4201 - accuracy: 0.123 - ETA: 18s - loss: 3.4207 - accuracy: 0.123 - ETA: 17s - loss: 3.4207 - accuracy: 0.123 - ETA: 17s - loss: 3.4207 - accuracy: 0.123 - ETA: 16s - loss: 3.4206 - accuracy: 0.123 - ETA: 15s - loss: 3.4207 - accuracy: 0.123 - ETA: 15s - loss: 3.4209 - accuracy: 0.123 - ETA: 14s - loss: 3.4211 - accuracy: 0.123 - ETA: 13s - loss: 3.4208 - accuracy: 0.123 - ETA: 13s - loss: 3.4207 - accuracy: 0.123 - ETA: 12s - loss: 3.4205 - accuracy: 0.123 - ETA: 11s - loss: 3.4205 - accuracy: 0.123 - ETA: 11s - loss: 3.4205 - accuracy: 0.123 - ETA: 10s - loss: 3.4201 - accuracy: 0.123 - ETA: 9s - loss: 3.4199 - accuracy: 0.123 - ETA: 9s - loss: 3.4200 - accuracy: 0.12 - ETA: 8s - loss: 3.4196 - accuracy: 0.12 - ETA: 7s - loss: 3.4195 - accuracy: 0.12 - ETA: 7s - loss: 3.4198 - accuracy: 0.12 - ETA: 6s - loss: 3.4199 - accuracy: 0.12 - ETA: 5s - loss: 3.4200 - accuracy: 0.12 - ETA: 5s - loss: 3.4200 - accuracy: 0.12 - ETA: 4s - loss: 3.4202 - accuracy: 0.12 - ETA: 3s - loss: 3.4208 - accuracy: 0.12 - ETA: 3s - loss: 3.4210 - accuracy: 0.12 - ETA: 2s - loss: 3.4213 - accuracy: 0.12 - ETA: 2s - loss: 3.4216 - accuracy: 0.12 - ETA: 1s - loss: 3.4217 - accuracy: 0.12 - ETA: 0s - loss: 3.4215 - accuracy: 0.12 - ETA: 0s - loss: 3.4218 - accuracy: 0.12 - 228s 5ms/step - loss: 3.4218 - accuracy: 0.1229 - val_loss: 3.8727 - val_accuracy: 0.0266\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:31 - loss: 3.4607 - accuracy: 0.10 - ETA: 3:16 - loss: 3.3947 - accuracy: 0.11 - ETA: 3:13 - loss: 3.4144 - accuracy: 0.11 - ETA: 3:12 - loss: 3.4414 - accuracy: 0.11 - ETA: 3:11 - loss: 3.4921 - accuracy: 0.10 - ETA: 3:10 - loss: 3.4838 - accuracy: 0.10 - ETA: 3:10 - loss: 3.4755 - accuracy: 0.10 - ETA: 3:11 - loss: 3.4662 - accuracy: 0.10 - ETA: 3:12 - loss: 3.4690 - accuracy: 0.10 - ETA: 3:13 - loss: 3.4717 - accuracy: 0.10 - ETA: 3:10 - loss: 3.4690 - accuracy: 0.10 - ETA: 3:10 - loss: 3.4617 - accuracy: 0.11 - ETA: 3:09 - loss: 3.4693 - accuracy: 0.11 - ETA: 3:08 - loss: 3.4635 - accuracy: 0.11 - ETA: 3:07 - loss: 3.4632 - accuracy: 0.11 - ETA: 3:06 - loss: 3.4701 - accuracy: 0.11 - ETA: 3:05 - loss: 3.4624 - accuracy: 0.11 - ETA: 3:05 - loss: 3.4527 - accuracy: 0.11 - ETA: 3:05 - loss: 3.4546 - accuracy: 0.11 - ETA: 3:05 - loss: 3.4506 - accuracy: 0.11 - ETA: 3:04 - loss: 3.4532 - accuracy: 0.11 - ETA: 3:03 - loss: 3.4584 - accuracy: 0.11 - ETA: 3:03 - loss: 3.4510 - accuracy: 0.11 - ETA: 3:02 - loss: 3.4487 - accuracy: 0.11 - ETA: 3:01 - loss: 3.4535 - accuracy: 0.11 - ETA: 3:00 - loss: 3.4451 - accuracy: 0.11 - ETA: 3:00 - loss: 3.4473 - accuracy: 0.11 - ETA: 2:59 - loss: 3.4488 - accuracy: 0.11 - ETA: 2:58 - loss: 3.4488 - accuracy: 0.11 - ETA: 2:58 - loss: 3.4459 - accuracy: 0.11 - ETA: 2:57 - loss: 3.4467 - accuracy: 0.11 - ETA: 2:56 - loss: 3.4428 - accuracy: 0.11 - ETA: 2:56 - loss: 3.4387 - accuracy: 0.11 - ETA: 2:55 - loss: 3.4406 - accuracy: 0.11 - ETA: 2:55 - loss: 3.4388 - accuracy: 0.11 - ETA: 2:54 - loss: 3.4394 - accuracy: 0.11 - ETA: 2:54 - loss: 3.4418 - accuracy: 0.11 - ETA: 2:53 - loss: 3.4405 - accuracy: 0.11 - ETA: 2:53 - loss: 3.4353 - accuracy: 0.11 - ETA: 2:52 - loss: 3.4391 - accuracy: 0.11 - ETA: 2:52 - loss: 3.4399 - accuracy: 0.11 - ETA: 2:51 - loss: 3.4358 - accuracy: 0.11 - ETA: 2:50 - loss: 3.4371 - accuracy: 0.11 - ETA: 2:50 - loss: 3.4352 - accuracy: 0.12 - ETA: 2:49 - loss: 3.4351 - accuracy: 0.12 - ETA: 2:48 - loss: 3.4374 - accuracy: 0.12 - ETA: 2:48 - loss: 3.4399 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4381 - accuracy: 0.12 - ETA: 2:46 - loss: 3.4352 - accuracy: 0.12 - ETA: 2:46 - loss: 3.4380 - accuracy: 0.12 - ETA: 2:45 - loss: 3.4370 - accuracy: 0.12 - ETA: 2:44 - loss: 3.4380 - accuracy: 0.12 - ETA: 2:44 - loss: 3.4366 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4343 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4318 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4287 - accuracy: 0.12 - ETA: 2:41 - loss: 3.4292 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4292 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4258 - accuracy: 0.12 - ETA: 2:39 - loss: 3.4229 - accuracy: 0.12 - ETA: 2:38 - loss: 3.4247 - accuracy: 0.12 - ETA: 2:38 - loss: 3.4233 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4264 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4258 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4241 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4239 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4224 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4240 - accuracy: 0.12 - ETA: 2:34 - loss: 3.4229 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4215 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4222 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4218 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4217 - accuracy: 0.12 - ETA: 2:31 - loss: 3.4213 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4192 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4194 - accuracy: 0.12 - ETA: 2:29 - loss: 3.4160 - accuracy: 0.12 - ETA: 2:29 - loss: 3.4152 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4154 - accuracy: 0.12 - ETA: 2:27 - loss: 3.4158 - accuracy: 0.12 - ETA: 2:27 - loss: 3.4161 - accuracy: 0.12 - ETA: 2:26 - loss: 3.4153 - accuracy: 0.12 - ETA: 2:25 - loss: 3.4148 - accuracy: 0.12 - ETA: 2:25 - loss: 3.4126 - accuracy: 0.12 - ETA: 2:24 - loss: 3.4124 - accuracy: 0.12 - ETA: 2:23 - loss: 3.4129 - accuracy: 0.12 - ETA: 2:23 - loss: 3.4154 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4137 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4127 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4110 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4093 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4094 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4091 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4087 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4084 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4082 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4070 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4066 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4077 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4072 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4069 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4075 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4098 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4103 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4094 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4099 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4099 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4104 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4095 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4101 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4092 - accuracy: 0.12 - ETA: 2:08 - loss: 3.4105 - accuracy: 0.12 - ETA: 2:08 - loss: 3.4110 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4104 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4091 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4087 - accuracy: 0.12 - ETA: 2:05 - loss: 3.4090 - accuracy: 0.12 - ETA: 2:05 - loss: 3.4098 - accuracy: 0.12 - ETA: 2:04 - loss: 3.4100 - accuracy: 0.12 - ETA: 2:04 - loss: 3.4099 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4103 - accuracy: 0.12 - ETA: 2:02 - loss: 3.4111 - accuracy: 0.12 - ETA: 2:02 - loss: 3.4119 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4128 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4127 - accuracy: 0.12 - ETA: 2:00 - loss: 3.4119 - accuracy: 0.12 - ETA: 1:59 - loss: 3.4117 - accuracy: 0.12 - ETA: 1:59 - loss: 3.4106 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4109 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4109 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4120 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4123 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4125 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4121 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4110 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4108 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4103 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4103 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4096 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4097 - accuracy: 0.12 - ETA: 1:51 - loss: 3.4095 - accuracy: 0.12 - ETA: 1:51 - loss: 3.4104 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4089 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4097 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4094 - accuracy: 0.12 - ETA: 1:48 - loss: 3.4094 - accuracy: 0.12 - ETA: 1:48 - loss: 3.4095 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4094 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4096 - accuracy: 0.12 - ETA: 1:46 - loss: 3.4098 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4097 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4088 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4082 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4082 - accuracy: 0.12 - ETA: 1:43 - loss: 3.4088 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4088 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4080 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4075 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4079 - accuracy: 0.12 - ETA: 1:40 - loss: 3.4074 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4089 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4086 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4088 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4092 - accuracy: 0.12 - ETA: 1:37 - loss: 3.4093 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4085 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4084 - accuracy: 0.12 - ETA: 1:35 - loss: 3.4078 - accuracy: 0.12 - ETA: 1:35 - loss: 3.4079 - accuracy: 0.12 - ETA: 1:34 - loss: 3.4072 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4068 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4071 - accuracy: 0.12 - ETA: 1:32 - loss: 3.4063 - accuracy: 0.12 - ETA: 1:32 - loss: 3.4063 - accuracy: 0.12 - ETA: 1:31 - loss: 3.4062 - accuracy: 0.12 - ETA: 1:31 - loss: 3.4059 - accuracy: 0.12 - ETA: 1:30 - loss: 3.4058 - accuracy: 0.12 - ETA: 1:30 - loss: 3.4054 - accuracy: 0.12 - ETA: 1:29 - loss: 3.4050 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4055 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4053 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4045 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4046 - accuracy: 0.12 - ETA: 1:26 - loss: 3.4051 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4045 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4045 - accuracy: 0.1257"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.4043 - accuracy: 0.12 - ETA: 1:24 - loss: 3.4045 - accuracy: 0.12 - ETA: 1:23 - loss: 3.4051 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4049 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4038 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4033 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4036 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4036 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4032 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4032 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4034 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4027 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4019 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4016 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4017 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4011 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4018 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4010 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4015 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4011 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4012 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4012 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4014 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4008 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4005 - accuracy: 0.12 - ETA: 1:09 - loss: 3.3998 - accuracy: 0.12 - ETA: 1:09 - loss: 3.3998 - accuracy: 0.12 - ETA: 1:08 - loss: 3.3999 - accuracy: 0.12 - ETA: 1:08 - loss: 3.3997 - accuracy: 0.12 - ETA: 1:07 - loss: 3.3995 - accuracy: 0.12 - ETA: 1:06 - loss: 3.3996 - accuracy: 0.12 - ETA: 1:06 - loss: 3.3997 - accuracy: 0.12 - ETA: 1:05 - loss: 3.3998 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4002 - accuracy: 0.12 - ETA: 1:04 - loss: 3.4005 - accuracy: 0.12 - ETA: 1:04 - loss: 3.4004 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4005 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4003 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4008 - accuracy: 0.12 - ETA: 1:01 - loss: 3.4009 - accuracy: 0.12 - ETA: 1:01 - loss: 3.4014 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4014 - accuracy: 0.12 - ETA: 59s - loss: 3.4019 - accuracy: 0.1252 - ETA: 59s - loss: 3.4018 - accuracy: 0.125 - ETA: 58s - loss: 3.4017 - accuracy: 0.125 - ETA: 58s - loss: 3.4017 - accuracy: 0.125 - ETA: 57s - loss: 3.4019 - accuracy: 0.125 - ETA: 57s - loss: 3.4018 - accuracy: 0.125 - ETA: 56s - loss: 3.4019 - accuracy: 0.125 - ETA: 55s - loss: 3.4021 - accuracy: 0.124 - ETA: 55s - loss: 3.4014 - accuracy: 0.125 - ETA: 54s - loss: 3.4017 - accuracy: 0.125 - ETA: 54s - loss: 3.4016 - accuracy: 0.125 - ETA: 53s - loss: 3.4015 - accuracy: 0.125 - ETA: 52s - loss: 3.4012 - accuracy: 0.125 - ETA: 52s - loss: 3.4005 - accuracy: 0.125 - ETA: 51s - loss: 3.4005 - accuracy: 0.125 - ETA: 51s - loss: 3.4004 - accuracy: 0.125 - ETA: 50s - loss: 3.4003 - accuracy: 0.125 - ETA: 49s - loss: 3.4004 - accuracy: 0.125 - ETA: 49s - loss: 3.4008 - accuracy: 0.125 - ETA: 48s - loss: 3.4009 - accuracy: 0.125 - ETA: 48s - loss: 3.4010 - accuracy: 0.125 - ETA: 47s - loss: 3.4011 - accuracy: 0.125 - ETA: 47s - loss: 3.4006 - accuracy: 0.125 - ETA: 46s - loss: 3.3997 - accuracy: 0.125 - ETA: 45s - loss: 3.4002 - accuracy: 0.125 - ETA: 45s - loss: 3.4006 - accuracy: 0.125 - ETA: 44s - loss: 3.3998 - accuracy: 0.125 - ETA: 44s - loss: 3.4000 - accuracy: 0.125 - ETA: 43s - loss: 3.3996 - accuracy: 0.125 - ETA: 42s - loss: 3.3993 - accuracy: 0.125 - ETA: 42s - loss: 3.3992 - accuracy: 0.125 - ETA: 41s - loss: 3.3991 - accuracy: 0.125 - ETA: 41s - loss: 3.3977 - accuracy: 0.126 - ETA: 40s - loss: 3.3978 - accuracy: 0.126 - ETA: 40s - loss: 3.3977 - accuracy: 0.126 - ETA: 39s - loss: 3.3981 - accuracy: 0.126 - ETA: 38s - loss: 3.3978 - accuracy: 0.126 - ETA: 38s - loss: 3.3978 - accuracy: 0.126 - ETA: 37s - loss: 3.3979 - accuracy: 0.126 - ETA: 37s - loss: 3.3977 - accuracy: 0.126 - ETA: 36s - loss: 3.3973 - accuracy: 0.126 - ETA: 35s - loss: 3.3966 - accuracy: 0.126 - ETA: 35s - loss: 3.3971 - accuracy: 0.126 - ETA: 34s - loss: 3.3971 - accuracy: 0.126 - ETA: 34s - loss: 3.3975 - accuracy: 0.126 - ETA: 33s - loss: 3.3970 - accuracy: 0.126 - ETA: 32s - loss: 3.3971 - accuracy: 0.126 - ETA: 32s - loss: 3.3966 - accuracy: 0.126 - ETA: 31s - loss: 3.3956 - accuracy: 0.126 - ETA: 31s - loss: 3.3961 - accuracy: 0.126 - ETA: 30s - loss: 3.3960 - accuracy: 0.126 - ETA: 30s - loss: 3.3964 - accuracy: 0.126 - ETA: 29s - loss: 3.3957 - accuracy: 0.126 - ETA: 28s - loss: 3.3947 - accuracy: 0.126 - ETA: 28s - loss: 3.3951 - accuracy: 0.126 - ETA: 27s - loss: 3.3950 - accuracy: 0.126 - ETA: 27s - loss: 3.3952 - accuracy: 0.126 - ETA: 26s - loss: 3.3952 - accuracy: 0.126 - ETA: 25s - loss: 3.3957 - accuracy: 0.126 - ETA: 25s - loss: 3.3965 - accuracy: 0.126 - ETA: 24s - loss: 3.3965 - accuracy: 0.126 - ETA: 24s - loss: 3.3972 - accuracy: 0.126 - ETA: 23s - loss: 3.3966 - accuracy: 0.126 - ETA: 22s - loss: 3.3966 - accuracy: 0.126 - ETA: 22s - loss: 3.3966 - accuracy: 0.125 - ETA: 21s - loss: 3.3973 - accuracy: 0.125 - ETA: 21s - loss: 3.3971 - accuracy: 0.125 - ETA: 20s - loss: 3.3976 - accuracy: 0.125 - ETA: 20s - loss: 3.3973 - accuracy: 0.125 - ETA: 19s - loss: 3.3968 - accuracy: 0.126 - ETA: 18s - loss: 3.3968 - accuracy: 0.125 - ETA: 18s - loss: 3.3970 - accuracy: 0.125 - ETA: 17s - loss: 3.3971 - accuracy: 0.125 - ETA: 17s - loss: 3.3970 - accuracy: 0.125 - ETA: 16s - loss: 3.3973 - accuracy: 0.125 - ETA: 15s - loss: 3.3972 - accuracy: 0.125 - ETA: 15s - loss: 3.3970 - accuracy: 0.125 - ETA: 14s - loss: 3.3968 - accuracy: 0.125 - ETA: 14s - loss: 3.3963 - accuracy: 0.125 - ETA: 13s - loss: 3.3959 - accuracy: 0.126 - ETA: 12s - loss: 3.3962 - accuracy: 0.126 - ETA: 12s - loss: 3.3964 - accuracy: 0.126 - ETA: 11s - loss: 3.3966 - accuracy: 0.126 - ETA: 11s - loss: 3.3966 - accuracy: 0.126 - ETA: 10s - loss: 3.3959 - accuracy: 0.126 - ETA: 10s - loss: 3.3960 - accuracy: 0.126 - ETA: 9s - loss: 3.3959 - accuracy: 0.126 - ETA: 8s - loss: 3.3960 - accuracy: 0.12 - ETA: 8s - loss: 3.3954 - accuracy: 0.12 - ETA: 7s - loss: 3.3954 - accuracy: 0.12 - ETA: 7s - loss: 3.3957 - accuracy: 0.12 - ETA: 6s - loss: 3.3951 - accuracy: 0.12 - ETA: 5s - loss: 3.3951 - accuracy: 0.12 - ETA: 5s - loss: 3.3948 - accuracy: 0.12 - ETA: 4s - loss: 3.3947 - accuracy: 0.12 - ETA: 4s - loss: 3.3942 - accuracy: 0.12 - ETA: 3s - loss: 3.3943 - accuracy: 0.12 - ETA: 2s - loss: 3.3943 - accuracy: 0.12 - ETA: 2s - loss: 3.3946 - accuracy: 0.12 - ETA: 1s - loss: 3.3940 - accuracy: 0.12 - ETA: 1s - loss: 3.3939 - accuracy: 0.12 - ETA: 0s - loss: 3.3932 - accuracy: 0.12 - ETA: 0s - loss: 3.3931 - accuracy: 0.12 - 206s 5ms/step - loss: 3.3930 - accuracy: 0.1264 - val_loss: 3.8736 - val_accuracy: 0.0336\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:33 - loss: 3.3558 - accuracy: 0.14 - ETA: 3:21 - loss: 3.3748 - accuracy: 0.15 - ETA: 3:21 - loss: 3.4027 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3864 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4161 - accuracy: 0.11 - ETA: 3:15 - loss: 3.3950 - accuracy: 0.11 - ETA: 3:14 - loss: 3.3955 - accuracy: 0.12 - ETA: 3:13 - loss: 3.3887 - accuracy: 0.12 - ETA: 3:12 - loss: 3.3937 - accuracy: 0.12 - ETA: 3:12 - loss: 3.3710 - accuracy: 0.12 - ETA: 3:12 - loss: 3.3479 - accuracy: 0.12 - ETA: 3:13 - loss: 3.3521 - accuracy: 0.12 - ETA: 3:13 - loss: 3.3494 - accuracy: 0.12 - ETA: 3:12 - loss: 3.3491 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3485 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3421 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3464 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3478 - accuracy: 0.12 - ETA: 3:06 - loss: 3.3516 - accuracy: 0.12 - ETA: 3:05 - loss: 3.3495 - accuracy: 0.12 - ETA: 3:05 - loss: 3.3527 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3523 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3560 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3544 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3532 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3517 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3443 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3452 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3475 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3484 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3456 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3491 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3518 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3574 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3602 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3633 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3665 - accuracy: 0.12 - ETA: 2:52 - loss: 3.3685 - accuracy: 0.12 - ETA: 2:52 - loss: 3.3702 - accuracy: 0.12 - ETA: 2:51 - loss: 3.3686 - accuracy: 0.12 - ETA: 2:51 - loss: 3.3668 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3595 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3612 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3653 - accuracy: 0.12 - ETA: 2:48 - loss: 3.3655 - accuracy: 0.12 - ETA: 2:47 - loss: 3.3670 - accuracy: 0.12 - ETA: 2:47 - loss: 3.3664 - accuracy: 0.12 - ETA: 2:46 - loss: 3.3662 - accuracy: 0.12 - ETA: 2:46 - loss: 3.3691 - accuracy: 0.12 - ETA: 2:45 - loss: 3.3739 - accuracy: 0.12 - ETA: 2:44 - loss: 3.3762 - accuracy: 0.12 - ETA: 2:44 - loss: 3.3794 - accuracy: 0.12 - ETA: 2:43 - loss: 3.3829 - accuracy: 0.12 - ETA: 2:43 - loss: 3.3839 - accuracy: 0.12 - ETA: 2:42 - loss: 3.3872 - accuracy: 0.12 - ETA: 2:41 - loss: 3.3856 - accuracy: 0.12 - ETA: 2:41 - loss: 3.3888 - accuracy: 0.12 - ETA: 2:40 - loss: 3.3873 - accuracy: 0.12 - ETA: 2:39 - loss: 3.3854 - accuracy: 0.12 - ETA: 2:39 - loss: 3.3849 - accuracy: 0.12 - ETA: 2:38 - loss: 3.3839 - accuracy: 0.12 - ETA: 2:38 - loss: 3.3866 - accuracy: 0.12 - ETA: 2:37 - loss: 3.3837 - accuracy: 0.12 - ETA: 2:37 - loss: 3.3817 - accuracy: 0.12 - ETA: 2:36 - loss: 3.3809 - accuracy: 0.12 - ETA: 2:36 - loss: 3.3826 - accuracy: 0.12 - ETA: 2:35 - loss: 3.3857 - accuracy: 0.12 - ETA: 2:35 - loss: 3.3851 - accuracy: 0.12 - ETA: 2:34 - loss: 3.3859 - accuracy: 0.12 - ETA: 2:33 - loss: 3.3869 - accuracy: 0.12 - ETA: 2:33 - loss: 3.3874 - accuracy: 0.12 - ETA: 2:32 - loss: 3.3843 - accuracy: 0.12 - ETA: 2:31 - loss: 3.3843 - accuracy: 0.12 - ETA: 2:31 - loss: 3.3844 - accuracy: 0.12 - ETA: 2:30 - loss: 3.3839 - accuracy: 0.12 - ETA: 2:30 - loss: 3.3817 - accuracy: 0.12 - ETA: 2:29 - loss: 3.3800 - accuracy: 0.12 - ETA: 2:28 - loss: 3.3809 - accuracy: 0.12 - ETA: 2:28 - loss: 3.3809 - accuracy: 0.12 - ETA: 2:27 - loss: 3.3812 - accuracy: 0.12 - ETA: 2:26 - loss: 3.3827 - accuracy: 0.12 - ETA: 2:26 - loss: 3.3852 - accuracy: 0.12 - ETA: 2:25 - loss: 3.3855 - accuracy: 0.12 - ETA: 2:24 - loss: 3.3848 - accuracy: 0.12 - ETA: 2:24 - loss: 3.3856 - accuracy: 0.12 - ETA: 2:23 - loss: 3.3850 - accuracy: 0.12 - ETA: 2:23 - loss: 3.3855 - accuracy: 0.12 - ETA: 2:22 - loss: 3.3867 - accuracy: 0.12 - ETA: 2:21 - loss: 3.3869 - accuracy: 0.12 - ETA: 2:21 - loss: 3.3873 - accuracy: 0.12 - ETA: 2:20 - loss: 3.3865 - accuracy: 0.12 - ETA: 2:19 - loss: 3.3872 - accuracy: 0.12 - ETA: 2:19 - loss: 3.3864 - accuracy: 0.12 - ETA: 2:18 - loss: 3.3857 - accuracy: 0.12 - ETA: 2:18 - loss: 3.3869 - accuracy: 0.12 - ETA: 2:17 - loss: 3.3856 - accuracy: 0.12 - ETA: 2:17 - loss: 3.3847 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3838 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3849 - accuracy: 0.12 - ETA: 2:15 - loss: 3.3855 - accuracy: 0.12 - ETA: 2:14 - loss: 3.3868 - accuracy: 0.12 - ETA: 2:14 - loss: 3.3894 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3895 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3909 - accuracy: 0.12 - ETA: 2:12 - loss: 3.3918 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3912 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3928 - accuracy: 0.12 - ETA: 2:10 - loss: 3.3930 - accuracy: 0.12 - ETA: 2:09 - loss: 3.3905 - accuracy: 0.12 - ETA: 2:09 - loss: 3.3920 - accuracy: 0.12 - ETA: 2:08 - loss: 3.3907 - accuracy: 0.12 - ETA: 2:08 - loss: 3.3901 - accuracy: 0.12 - ETA: 2:07 - loss: 3.3889 - accuracy: 0.12 - ETA: 2:06 - loss: 3.3866 - accuracy: 0.12 - ETA: 2:06 - loss: 3.3868 - accuracy: 0.12 - ETA: 2:05 - loss: 3.3856 - accuracy: 0.12 - ETA: 2:05 - loss: 3.3849 - accuracy: 0.12 - ETA: 2:04 - loss: 3.3853 - accuracy: 0.12 - ETA: 2:04 - loss: 3.3838 - accuracy: 0.12 - ETA: 2:03 - loss: 3.3834 - accuracy: 0.12 - ETA: 2:03 - loss: 3.3827 - accuracy: 0.12 - ETA: 2:02 - loss: 3.3838 - accuracy: 0.12 - ETA: 2:02 - loss: 3.3831 - accuracy: 0.12 - ETA: 2:01 - loss: 3.3830 - accuracy: 0.12 - ETA: 2:01 - loss: 3.3835 - accuracy: 0.12 - ETA: 2:00 - loss: 3.3830 - accuracy: 0.12 - ETA: 1:59 - loss: 3.3827 - accuracy: 0.12 - ETA: 1:59 - loss: 3.3826 - accuracy: 0.12 - ETA: 1:58 - loss: 3.3822 - accuracy: 0.12 - ETA: 1:58 - loss: 3.3828 - accuracy: 0.12 - ETA: 1:57 - loss: 3.3823 - accuracy: 0.12 - ETA: 1:56 - loss: 3.3811 - accuracy: 0.12 - ETA: 1:56 - loss: 3.3809 - accuracy: 0.12 - ETA: 1:55 - loss: 3.3811 - accuracy: 0.12 - ETA: 1:55 - loss: 3.3802 - accuracy: 0.12 - ETA: 1:54 - loss: 3.3798 - accuracy: 0.12 - ETA: 1:53 - loss: 3.3787 - accuracy: 0.12 - ETA: 1:53 - loss: 3.3794 - accuracy: 0.12 - ETA: 1:52 - loss: 3.3799 - accuracy: 0.12 - ETA: 1:52 - loss: 3.3801 - accuracy: 0.12 - ETA: 1:51 - loss: 3.3791 - accuracy: 0.12 - ETA: 1:50 - loss: 3.3789 - accuracy: 0.12 - ETA: 1:50 - loss: 3.3799 - accuracy: 0.12 - ETA: 1:49 - loss: 3.3789 - accuracy: 0.12 - ETA: 1:49 - loss: 3.3781 - accuracy: 0.12 - ETA: 1:48 - loss: 3.3769 - accuracy: 0.12 - ETA: 1:47 - loss: 3.3767 - accuracy: 0.12 - ETA: 1:47 - loss: 3.3770 - accuracy: 0.12 - ETA: 1:46 - loss: 3.3779 - accuracy: 0.12 - ETA: 1:46 - loss: 3.3773 - accuracy: 0.12 - ETA: 1:45 - loss: 3.3773 - accuracy: 0.12 - ETA: 1:45 - loss: 3.3764 - accuracy: 0.12 - ETA: 1:44 - loss: 3.3749 - accuracy: 0.12 - ETA: 1:43 - loss: 3.3751 - accuracy: 0.12 - ETA: 1:43 - loss: 3.3753 - accuracy: 0.12 - ETA: 1:42 - loss: 3.3748 - accuracy: 0.12 - ETA: 1:42 - loss: 3.3745 - accuracy: 0.12 - ETA: 1:41 - loss: 3.3733 - accuracy: 0.12 - ETA: 1:41 - loss: 3.3741 - accuracy: 0.12 - ETA: 1:40 - loss: 3.3745 - accuracy: 0.12 - ETA: 1:39 - loss: 3.3740 - accuracy: 0.12 - ETA: 1:39 - loss: 3.3727 - accuracy: 0.12 - ETA: 1:38 - loss: 3.3718 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3719 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3712 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3711 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3707 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3699 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3689 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3678 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3680 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3681 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3687 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3689 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3683 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3673 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3676 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3680 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3683 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3690 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3698 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3698 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3701 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3702 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3687 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3691 - accuracy: 0.1306"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.3693 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3695 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3700 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3708 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3697 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3703 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3698 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3707 - accuracy: 0.12 - ETA: 1:19 - loss: 3.3703 - accuracy: 0.12 - ETA: 1:19 - loss: 3.3696 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3692 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3697 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3698 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3695 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3686 - accuracy: 0.12 - ETA: 1:15 - loss: 3.3679 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3670 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3666 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3668 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3672 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3672 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3671 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3671 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3672 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3679 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3683 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3675 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3674 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3673 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3679 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3677 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3683 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3678 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3688 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3690 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3689 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3682 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3677 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3667 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3660 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3660 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3670 - accuracy: 0.13 - ETA: 59s - loss: 3.3663 - accuracy: 0.1312 - ETA: 59s - loss: 3.3666 - accuracy: 0.131 - ETA: 58s - loss: 3.3661 - accuracy: 0.131 - ETA: 58s - loss: 3.3661 - accuracy: 0.131 - ETA: 57s - loss: 3.3664 - accuracy: 0.131 - ETA: 56s - loss: 3.3673 - accuracy: 0.131 - ETA: 56s - loss: 3.3674 - accuracy: 0.131 - ETA: 55s - loss: 3.3674 - accuracy: 0.131 - ETA: 55s - loss: 3.3675 - accuracy: 0.131 - ETA: 54s - loss: 3.3663 - accuracy: 0.131 - ETA: 53s - loss: 3.3662 - accuracy: 0.131 - ETA: 53s - loss: 3.3663 - accuracy: 0.131 - ETA: 52s - loss: 3.3665 - accuracy: 0.131 - ETA: 52s - loss: 3.3669 - accuracy: 0.131 - ETA: 51s - loss: 3.3670 - accuracy: 0.131 - ETA: 51s - loss: 3.3674 - accuracy: 0.131 - ETA: 50s - loss: 3.3673 - accuracy: 0.131 - ETA: 49s - loss: 3.3670 - accuracy: 0.131 - ETA: 49s - loss: 3.3673 - accuracy: 0.131 - ETA: 48s - loss: 3.3672 - accuracy: 0.131 - ETA: 48s - loss: 3.3669 - accuracy: 0.131 - ETA: 47s - loss: 3.3670 - accuracy: 0.131 - ETA: 46s - loss: 3.3667 - accuracy: 0.131 - ETA: 46s - loss: 3.3664 - accuracy: 0.131 - ETA: 45s - loss: 3.3663 - accuracy: 0.131 - ETA: 45s - loss: 3.3669 - accuracy: 0.131 - ETA: 44s - loss: 3.3663 - accuracy: 0.131 - ETA: 43s - loss: 3.3661 - accuracy: 0.131 - ETA: 43s - loss: 3.3661 - accuracy: 0.131 - ETA: 42s - loss: 3.3660 - accuracy: 0.131 - ETA: 42s - loss: 3.3660 - accuracy: 0.131 - ETA: 41s - loss: 3.3662 - accuracy: 0.131 - ETA: 41s - loss: 3.3660 - accuracy: 0.131 - ETA: 40s - loss: 3.3667 - accuracy: 0.131 - ETA: 39s - loss: 3.3669 - accuracy: 0.131 - ETA: 39s - loss: 3.3670 - accuracy: 0.131 - ETA: 38s - loss: 3.3669 - accuracy: 0.131 - ETA: 38s - loss: 3.3670 - accuracy: 0.131 - ETA: 37s - loss: 3.3669 - accuracy: 0.131 - ETA: 36s - loss: 3.3676 - accuracy: 0.131 - ETA: 36s - loss: 3.3671 - accuracy: 0.131 - ETA: 35s - loss: 3.3667 - accuracy: 0.131 - ETA: 35s - loss: 3.3663 - accuracy: 0.131 - ETA: 34s - loss: 3.3660 - accuracy: 0.131 - ETA: 34s - loss: 3.3664 - accuracy: 0.131 - ETA: 33s - loss: 3.3656 - accuracy: 0.132 - ETA: 32s - loss: 3.3660 - accuracy: 0.132 - ETA: 32s - loss: 3.3664 - accuracy: 0.132 - ETA: 31s - loss: 3.3663 - accuracy: 0.132 - ETA: 31s - loss: 3.3661 - accuracy: 0.132 - ETA: 30s - loss: 3.3654 - accuracy: 0.132 - ETA: 29s - loss: 3.3655 - accuracy: 0.132 - ETA: 29s - loss: 3.3648 - accuracy: 0.132 - ETA: 28s - loss: 3.3647 - accuracy: 0.132 - ETA: 28s - loss: 3.3647 - accuracy: 0.132 - ETA: 27s - loss: 3.3647 - accuracy: 0.132 - ETA: 27s - loss: 3.3654 - accuracy: 0.132 - ETA: 26s - loss: 3.3648 - accuracy: 0.132 - ETA: 25s - loss: 3.3645 - accuracy: 0.132 - ETA: 25s - loss: 3.3646 - accuracy: 0.132 - ETA: 24s - loss: 3.3649 - accuracy: 0.132 - ETA: 24s - loss: 3.3647 - accuracy: 0.132 - ETA: 23s - loss: 3.3648 - accuracy: 0.132 - ETA: 22s - loss: 3.3653 - accuracy: 0.132 - ETA: 22s - loss: 3.3656 - accuracy: 0.131 - ETA: 21s - loss: 3.3655 - accuracy: 0.131 - ETA: 21s - loss: 3.3658 - accuracy: 0.131 - ETA: 20s - loss: 3.3664 - accuracy: 0.131 - ETA: 19s - loss: 3.3660 - accuracy: 0.131 - ETA: 19s - loss: 3.3660 - accuracy: 0.131 - ETA: 18s - loss: 3.3660 - accuracy: 0.131 - ETA: 18s - loss: 3.3658 - accuracy: 0.131 - ETA: 17s - loss: 3.3655 - accuracy: 0.131 - ETA: 17s - loss: 3.3649 - accuracy: 0.131 - ETA: 16s - loss: 3.3653 - accuracy: 0.131 - ETA: 15s - loss: 3.3649 - accuracy: 0.131 - ETA: 15s - loss: 3.3655 - accuracy: 0.131 - ETA: 14s - loss: 3.3653 - accuracy: 0.131 - ETA: 14s - loss: 3.3646 - accuracy: 0.132 - ETA: 13s - loss: 3.3642 - accuracy: 0.132 - ETA: 12s - loss: 3.3636 - accuracy: 0.132 - ETA: 12s - loss: 3.3635 - accuracy: 0.132 - ETA: 11s - loss: 3.3632 - accuracy: 0.132 - ETA: 11s - loss: 3.3632 - accuracy: 0.132 - ETA: 10s - loss: 3.3636 - accuracy: 0.132 - ETA: 10s - loss: 3.3632 - accuracy: 0.132 - ETA: 9s - loss: 3.3631 - accuracy: 0.132 - ETA: 8s - loss: 3.3630 - accuracy: 0.13 - ETA: 8s - loss: 3.3624 - accuracy: 0.13 - ETA: 7s - loss: 3.3624 - accuracy: 0.13 - ETA: 7s - loss: 3.3620 - accuracy: 0.13 - ETA: 6s - loss: 3.3615 - accuracy: 0.13 - ETA: 5s - loss: 3.3612 - accuracy: 0.13 - ETA: 5s - loss: 3.3612 - accuracy: 0.13 - ETA: 4s - loss: 3.3615 - accuracy: 0.13 - ETA: 4s - loss: 3.3615 - accuracy: 0.13 - ETA: 3s - loss: 3.3621 - accuracy: 0.13 - ETA: 2s - loss: 3.3620 - accuracy: 0.13 - ETA: 2s - loss: 3.3620 - accuracy: 0.13 - ETA: 1s - loss: 3.3615 - accuracy: 0.13 - ETA: 1s - loss: 3.3608 - accuracy: 0.13 - ETA: 0s - loss: 3.3605 - accuracy: 0.13 - ETA: 0s - loss: 3.3600 - accuracy: 0.13 - 207s 5ms/step - loss: 3.3600 - accuracy: 0.1321 - val_loss: 3.8731 - val_accuracy: 0.0306\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:25 - loss: 3.2167 - accuracy: 0.15 - ETA: 3:16 - loss: 3.2106 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2552 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3174 - accuracy: 0.12 - ETA: 3:07 - loss: 3.3092 - accuracy: 0.12 - ETA: 3:06 - loss: 3.3187 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3012 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3293 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3193 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3275 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3265 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3275 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3325 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3263 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3373 - accuracy: 0.12 - ETA: 3:04 - loss: 3.3310 - accuracy: 0.12 - ETA: 3:04 - loss: 3.3357 - accuracy: 0.12 - ETA: 3:03 - loss: 3.3408 - accuracy: 0.12 - ETA: 3:02 - loss: 3.3495 - accuracy: 0.12 - ETA: 3:02 - loss: 3.3436 - accuracy: 0.12 - ETA: 3:01 - loss: 3.3458 - accuracy: 0.12 - ETA: 3:00 - loss: 3.3402 - accuracy: 0.12 - ETA: 2:59 - loss: 3.3458 - accuracy: 0.12 - ETA: 2:59 - loss: 3.3470 - accuracy: 0.12 - ETA: 2:58 - loss: 3.3468 - accuracy: 0.12 - ETA: 2:58 - loss: 3.3452 - accuracy: 0.12 - ETA: 2:57 - loss: 3.3500 - accuracy: 0.12 - ETA: 2:56 - loss: 3.3497 - accuracy: 0.12 - ETA: 2:56 - loss: 3.3488 - accuracy: 0.12 - ETA: 2:55 - loss: 3.3436 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3472 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3476 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3519 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3571 - accuracy: 0.12 - ETA: 2:52 - loss: 3.3543 - accuracy: 0.12 - ETA: 2:51 - loss: 3.3570 - accuracy: 0.12 - ETA: 2:51 - loss: 3.3586 - accuracy: 0.12 - ETA: 2:50 - loss: 3.3521 - accuracy: 0.12 - ETA: 2:49 - loss: 3.3538 - accuracy: 0.12 - ETA: 2:49 - loss: 3.3536 - accuracy: 0.12 - ETA: 2:49 - loss: 3.3541 - accuracy: 0.12 - ETA: 2:48 - loss: 3.3565 - accuracy: 0.12 - ETA: 2:48 - loss: 3.3583 - accuracy: 0.12 - ETA: 2:48 - loss: 3.3573 - accuracy: 0.12 - ETA: 2:47 - loss: 3.3586 - accuracy: 0.12 - ETA: 2:46 - loss: 3.3555 - accuracy: 0.12 - ETA: 2:46 - loss: 3.3525 - accuracy: 0.12 - ETA: 2:45 - loss: 3.3542 - accuracy: 0.12 - ETA: 2:44 - loss: 3.3566 - accuracy: 0.12 - ETA: 2:44 - loss: 3.3581 - accuracy: 0.12 - ETA: 2:43 - loss: 3.3599 - accuracy: 0.12 - ETA: 2:43 - loss: 3.3595 - accuracy: 0.12 - ETA: 2:42 - loss: 3.3588 - accuracy: 0.12 - ETA: 2:41 - loss: 3.3611 - accuracy: 0.12 - ETA: 2:41 - loss: 3.3595 - accuracy: 0.12 - ETA: 2:40 - loss: 3.3596 - accuracy: 0.12 - ETA: 2:39 - loss: 3.3599 - accuracy: 0.12 - ETA: 2:39 - loss: 3.3603 - accuracy: 0.12 - ETA: 2:38 - loss: 3.3595 - accuracy: 0.12 - ETA: 2:37 - loss: 3.3624 - accuracy: 0.12 - ETA: 2:37 - loss: 3.3624 - accuracy: 0.12 - ETA: 2:36 - loss: 3.3579 - accuracy: 0.12 - ETA: 2:36 - loss: 3.3568 - accuracy: 0.12 - ETA: 2:35 - loss: 3.3556 - accuracy: 0.12 - ETA: 2:34 - loss: 3.3552 - accuracy: 0.12 - ETA: 2:34 - loss: 3.3552 - accuracy: 0.12 - ETA: 2:33 - loss: 3.3552 - accuracy: 0.12 - ETA: 2:33 - loss: 3.3533 - accuracy: 0.12 - ETA: 2:32 - loss: 3.3532 - accuracy: 0.12 - ETA: 2:32 - loss: 3.3503 - accuracy: 0.12 - ETA: 2:31 - loss: 3.3524 - accuracy: 0.12 - ETA: 2:31 - loss: 3.3533 - accuracy: 0.12 - ETA: 2:30 - loss: 3.3517 - accuracy: 0.12 - ETA: 2:30 - loss: 3.3509 - accuracy: 0.12 - ETA: 2:29 - loss: 3.3497 - accuracy: 0.12 - ETA: 2:28 - loss: 3.3486 - accuracy: 0.12 - ETA: 2:28 - loss: 3.3497 - accuracy: 0.12 - ETA: 2:27 - loss: 3.3474 - accuracy: 0.12 - ETA: 2:27 - loss: 3.3458 - accuracy: 0.12 - ETA: 2:26 - loss: 3.3480 - accuracy: 0.12 - ETA: 2:25 - loss: 3.3477 - accuracy: 0.12 - ETA: 2:25 - loss: 3.3486 - accuracy: 0.12 - ETA: 2:24 - loss: 3.3505 - accuracy: 0.12 - ETA: 2:23 - loss: 3.3499 - accuracy: 0.12 - ETA: 2:23 - loss: 3.3478 - accuracy: 0.12 - ETA: 2:22 - loss: 3.3475 - accuracy: 0.12 - ETA: 2:22 - loss: 3.3458 - accuracy: 0.12 - ETA: 2:21 - loss: 3.3452 - accuracy: 0.12 - ETA: 2:20 - loss: 3.3448 - accuracy: 0.12 - ETA: 2:20 - loss: 3.3454 - accuracy: 0.12 - ETA: 2:19 - loss: 3.3485 - accuracy: 0.12 - ETA: 2:19 - loss: 3.3480 - accuracy: 0.12 - ETA: 2:18 - loss: 3.3476 - accuracy: 0.12 - ETA: 2:17 - loss: 3.3474 - accuracy: 0.12 - ETA: 2:17 - loss: 3.3469 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3473 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3469 - accuracy: 0.12 - ETA: 2:15 - loss: 3.3463 - accuracy: 0.12 - ETA: 2:15 - loss: 3.3454 - accuracy: 0.12 - ETA: 2:14 - loss: 3.3447 - accuracy: 0.12 - ETA: 2:14 - loss: 3.3440 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3421 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3442 - accuracy: 0.12 - ETA: 2:12 - loss: 3.3451 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3451 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3453 - accuracy: 0.12 - ETA: 2:10 - loss: 3.3445 - accuracy: 0.12 - ETA: 2:10 - loss: 3.3444 - accuracy: 0.12 - ETA: 2:09 - loss: 3.3453 - accuracy: 0.12 - ETA: 2:09 - loss: 3.3469 - accuracy: 0.12 - ETA: 2:08 - loss: 3.3449 - accuracy: 0.12 - ETA: 2:08 - loss: 3.3443 - accuracy: 0.12 - ETA: 2:07 - loss: 3.3436 - accuracy: 0.12 - ETA: 2:06 - loss: 3.3442 - accuracy: 0.12 - ETA: 2:06 - loss: 3.3435 - accuracy: 0.12 - ETA: 2:05 - loss: 3.3421 - accuracy: 0.12 - ETA: 2:05 - loss: 3.3416 - accuracy: 0.12 - ETA: 2:04 - loss: 3.3422 - accuracy: 0.12 - ETA: 2:03 - loss: 3.3447 - accuracy: 0.12 - ETA: 2:03 - loss: 3.3443 - accuracy: 0.12 - ETA: 2:02 - loss: 3.3441 - accuracy: 0.12 - ETA: 2:02 - loss: 3.3418 - accuracy: 0.12 - ETA: 2:01 - loss: 3.3428 - accuracy: 0.12 - ETA: 2:01 - loss: 3.3427 - accuracy: 0.12 - ETA: 2:00 - loss: 3.3422 - accuracy: 0.12 - ETA: 2:00 - loss: 3.3420 - accuracy: 0.12 - ETA: 1:59 - loss: 3.3416 - accuracy: 0.12 - ETA: 1:58 - loss: 3.3418 - accuracy: 0.12 - ETA: 1:58 - loss: 3.3401 - accuracy: 0.12 - ETA: 1:57 - loss: 3.3417 - accuracy: 0.12 - ETA: 1:57 - loss: 3.3435 - accuracy: 0.12 - ETA: 1:56 - loss: 3.3434 - accuracy: 0.12 - ETA: 1:55 - loss: 3.3433 - accuracy: 0.12 - ETA: 1:55 - loss: 3.3436 - accuracy: 0.12 - ETA: 1:54 - loss: 3.3427 - accuracy: 0.12 - ETA: 1:54 - loss: 3.3428 - accuracy: 0.12 - ETA: 1:53 - loss: 3.3431 - accuracy: 0.12 - ETA: 1:52 - loss: 3.3428 - accuracy: 0.12 - ETA: 1:52 - loss: 3.3432 - accuracy: 0.12 - ETA: 1:51 - loss: 3.3432 - accuracy: 0.12 - ETA: 1:51 - loss: 3.3440 - accuracy: 0.12 - ETA: 1:50 - loss: 3.3441 - accuracy: 0.12 - ETA: 1:49 - loss: 3.3449 - accuracy: 0.12 - ETA: 1:49 - loss: 3.3446 - accuracy: 0.12 - ETA: 1:48 - loss: 3.3435 - accuracy: 0.12 - ETA: 1:48 - loss: 3.3431 - accuracy: 0.12 - ETA: 1:47 - loss: 3.3438 - accuracy: 0.12 - ETA: 1:46 - loss: 3.3434 - accuracy: 0.12 - ETA: 1:46 - loss: 3.3424 - accuracy: 0.12 - ETA: 1:45 - loss: 3.3436 - accuracy: 0.12 - ETA: 1:45 - loss: 3.3449 - accuracy: 0.12 - ETA: 1:44 - loss: 3.3450 - accuracy: 0.12 - ETA: 1:44 - loss: 3.3434 - accuracy: 0.12 - ETA: 1:43 - loss: 3.3429 - accuracy: 0.12 - ETA: 1:43 - loss: 3.3425 - accuracy: 0.12 - ETA: 1:42 - loss: 3.3416 - accuracy: 0.12 - ETA: 1:41 - loss: 3.3425 - accuracy: 0.12 - ETA: 1:41 - loss: 3.3418 - accuracy: 0.12 - ETA: 1:40 - loss: 3.3427 - accuracy: 0.12 - ETA: 1:40 - loss: 3.3434 - accuracy: 0.12 - ETA: 1:39 - loss: 3.3432 - accuracy: 0.12 - ETA: 1:38 - loss: 3.3433 - accuracy: 0.12 - ETA: 1:38 - loss: 3.3437 - accuracy: 0.12 - ETA: 1:37 - loss: 3.3445 - accuracy: 0.12 - ETA: 1:37 - loss: 3.3445 - accuracy: 0.12 - ETA: 1:36 - loss: 3.3443 - accuracy: 0.12 - ETA: 1:35 - loss: 3.3440 - accuracy: 0.12 - ETA: 1:35 - loss: 3.3434 - accuracy: 0.12 - ETA: 1:34 - loss: 3.3427 - accuracy: 0.12 - ETA: 1:34 - loss: 3.3416 - accuracy: 0.12 - ETA: 1:33 - loss: 3.3418 - accuracy: 0.12 - ETA: 1:33 - loss: 3.3418 - accuracy: 0.12 - ETA: 1:32 - loss: 3.3423 - accuracy: 0.12 - ETA: 1:31 - loss: 3.3421 - accuracy: 0.12 - ETA: 1:31 - loss: 3.3423 - accuracy: 0.12 - ETA: 1:30 - loss: 3.3419 - accuracy: 0.12 - ETA: 1:30 - loss: 3.3415 - accuracy: 0.12 - ETA: 1:29 - loss: 3.3420 - accuracy: 0.12 - ETA: 1:29 - loss: 3.3417 - accuracy: 0.12 - ETA: 1:28 - loss: 3.3414 - accuracy: 0.12 - ETA: 1:27 - loss: 3.3407 - accuracy: 0.12 - ETA: 1:27 - loss: 3.3409 - accuracy: 0.12 - ETA: 1:26 - loss: 3.3405 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3399 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3405 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3405 - accuracy: 0.1303"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.3405 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3404 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3399 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3407 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3404 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3401 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3401 - accuracy: 0.12 - ETA: 1:20 - loss: 3.3409 - accuracy: 0.12 - ETA: 1:19 - loss: 3.3408 - accuracy: 0.12 - ETA: 1:18 - loss: 3.3414 - accuracy: 0.12 - ETA: 1:18 - loss: 3.3408 - accuracy: 0.12 - ETA: 1:17 - loss: 3.3417 - accuracy: 0.12 - ETA: 1:17 - loss: 3.3414 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3414 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3413 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3416 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3418 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3418 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3425 - accuracy: 0.12 - ETA: 1:13 - loss: 3.3421 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3414 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3415 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3424 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3419 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3416 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3413 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3406 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3402 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3398 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3407 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3407 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3402 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3403 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3394 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3395 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3390 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3398 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3404 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3399 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3398 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3397 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3395 - accuracy: 0.13 - ETA: 59s - loss: 3.3396 - accuracy: 0.1313 - ETA: 59s - loss: 3.3401 - accuracy: 0.131 - ETA: 58s - loss: 3.3389 - accuracy: 0.131 - ETA: 58s - loss: 3.3385 - accuracy: 0.131 - ETA: 57s - loss: 3.3376 - accuracy: 0.132 - ETA: 56s - loss: 3.3368 - accuracy: 0.132 - ETA: 56s - loss: 3.3368 - accuracy: 0.132 - ETA: 55s - loss: 3.3365 - accuracy: 0.132 - ETA: 55s - loss: 3.3363 - accuracy: 0.132 - ETA: 54s - loss: 3.3364 - accuracy: 0.132 - ETA: 53s - loss: 3.3361 - accuracy: 0.132 - ETA: 53s - loss: 3.3357 - accuracy: 0.132 - ETA: 52s - loss: 3.3359 - accuracy: 0.132 - ETA: 52s - loss: 3.3352 - accuracy: 0.132 - ETA: 51s - loss: 3.3350 - accuracy: 0.132 - ETA: 51s - loss: 3.3349 - accuracy: 0.132 - ETA: 50s - loss: 3.3356 - accuracy: 0.132 - ETA: 49s - loss: 3.3358 - accuracy: 0.132 - ETA: 49s - loss: 3.3355 - accuracy: 0.132 - ETA: 48s - loss: 3.3353 - accuracy: 0.132 - ETA: 48s - loss: 3.3350 - accuracy: 0.132 - ETA: 47s - loss: 3.3345 - accuracy: 0.132 - ETA: 46s - loss: 3.3343 - accuracy: 0.133 - ETA: 46s - loss: 3.3342 - accuracy: 0.132 - ETA: 45s - loss: 3.3343 - accuracy: 0.133 - ETA: 45s - loss: 3.3337 - accuracy: 0.133 - ETA: 44s - loss: 3.3344 - accuracy: 0.133 - ETA: 43s - loss: 3.3339 - accuracy: 0.133 - ETA: 43s - loss: 3.3345 - accuracy: 0.133 - ETA: 42s - loss: 3.3344 - accuracy: 0.133 - ETA: 42s - loss: 3.3341 - accuracy: 0.133 - ETA: 41s - loss: 3.3339 - accuracy: 0.133 - ETA: 41s - loss: 3.3338 - accuracy: 0.133 - ETA: 40s - loss: 3.3338 - accuracy: 0.133 - ETA: 39s - loss: 3.3336 - accuracy: 0.133 - ETA: 39s - loss: 3.3330 - accuracy: 0.133 - ETA: 38s - loss: 3.3337 - accuracy: 0.133 - ETA: 38s - loss: 3.3341 - accuracy: 0.133 - ETA: 37s - loss: 3.3337 - accuracy: 0.134 - ETA: 36s - loss: 3.3336 - accuracy: 0.134 - ETA: 36s - loss: 3.3328 - accuracy: 0.134 - ETA: 35s - loss: 3.3331 - accuracy: 0.134 - ETA: 35s - loss: 3.3343 - accuracy: 0.134 - ETA: 34s - loss: 3.3353 - accuracy: 0.134 - ETA: 34s - loss: 3.3356 - accuracy: 0.134 - ETA: 33s - loss: 3.3356 - accuracy: 0.134 - ETA: 32s - loss: 3.3358 - accuracy: 0.134 - ETA: 32s - loss: 3.3357 - accuracy: 0.134 - ETA: 31s - loss: 3.3358 - accuracy: 0.134 - ETA: 31s - loss: 3.3359 - accuracy: 0.134 - ETA: 30s - loss: 3.3363 - accuracy: 0.134 - ETA: 29s - loss: 3.3363 - accuracy: 0.134 - ETA: 29s - loss: 3.3359 - accuracy: 0.135 - ETA: 28s - loss: 3.3360 - accuracy: 0.135 - ETA: 28s - loss: 3.3360 - accuracy: 0.135 - ETA: 27s - loss: 3.3358 - accuracy: 0.135 - ETA: 26s - loss: 3.3362 - accuracy: 0.135 - ETA: 26s - loss: 3.3351 - accuracy: 0.135 - ETA: 25s - loss: 3.3360 - accuracy: 0.135 - ETA: 25s - loss: 3.3357 - accuracy: 0.135 - ETA: 24s - loss: 3.3362 - accuracy: 0.135 - ETA: 24s - loss: 3.3366 - accuracy: 0.135 - ETA: 23s - loss: 3.3366 - accuracy: 0.134 - ETA: 22s - loss: 3.3360 - accuracy: 0.135 - ETA: 22s - loss: 3.3359 - accuracy: 0.135 - ETA: 21s - loss: 3.3361 - accuracy: 0.135 - ETA: 21s - loss: 3.3356 - accuracy: 0.135 - ETA: 20s - loss: 3.3357 - accuracy: 0.135 - ETA: 19s - loss: 3.3361 - accuracy: 0.135 - ETA: 19s - loss: 3.3361 - accuracy: 0.135 - ETA: 18s - loss: 3.3363 - accuracy: 0.135 - ETA: 18s - loss: 3.3365 - accuracy: 0.135 - ETA: 17s - loss: 3.3357 - accuracy: 0.135 - ETA: 17s - loss: 3.3360 - accuracy: 0.135 - ETA: 16s - loss: 3.3366 - accuracy: 0.135 - ETA: 15s - loss: 3.3361 - accuracy: 0.135 - ETA: 15s - loss: 3.3364 - accuracy: 0.135 - ETA: 14s - loss: 3.3365 - accuracy: 0.135 - ETA: 14s - loss: 3.3363 - accuracy: 0.135 - ETA: 13s - loss: 3.3368 - accuracy: 0.134 - ETA: 12s - loss: 3.3367 - accuracy: 0.134 - ETA: 12s - loss: 3.3366 - accuracy: 0.135 - ETA: 11s - loss: 3.3369 - accuracy: 0.134 - ETA: 11s - loss: 3.3371 - accuracy: 0.134 - ETA: 10s - loss: 3.3365 - accuracy: 0.134 - ETA: 9s - loss: 3.3366 - accuracy: 0.134 - ETA: 9s - loss: 3.3374 - accuracy: 0.13 - ETA: 8s - loss: 3.3372 - accuracy: 0.13 - ETA: 8s - loss: 3.3368 - accuracy: 0.13 - ETA: 7s - loss: 3.3367 - accuracy: 0.13 - ETA: 7s - loss: 3.3361 - accuracy: 0.13 - ETA: 6s - loss: 3.3360 - accuracy: 0.13 - ETA: 5s - loss: 3.3359 - accuracy: 0.13 - ETA: 5s - loss: 3.3354 - accuracy: 0.13 - ETA: 4s - loss: 3.3354 - accuracy: 0.13 - ETA: 4s - loss: 3.3351 - accuracy: 0.13 - ETA: 3s - loss: 3.3344 - accuracy: 0.13 - ETA: 2s - loss: 3.3346 - accuracy: 0.13 - ETA: 2s - loss: 3.3348 - accuracy: 0.13 - ETA: 1s - loss: 3.3342 - accuracy: 0.13 - ETA: 1s - loss: 3.3346 - accuracy: 0.13 - ETA: 0s - loss: 3.3344 - accuracy: 0.13 - ETA: 0s - loss: 3.3342 - accuracy: 0.13 - 206s 5ms/step - loss: 3.3344 - accuracy: 0.1352 - val_loss: 3.8544 - val_accuracy: 0.0449\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:24 - loss: 3.2848 - accuracy: 0.14 - ETA: 3:12 - loss: 3.3097 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3045 - accuracy: 0.14 - ETA: 3:10 - loss: 3.3133 - accuracy: 0.14 - ETA: 3:10 - loss: 3.3176 - accuracy: 0.14 - ETA: 3:09 - loss: 3.3022 - accuracy: 0.15 - ETA: 3:09 - loss: 3.3010 - accuracy: 0.14 - ETA: 3:08 - loss: 3.3056 - accuracy: 0.13 - ETA: 3:08 - loss: 3.2879 - accuracy: 0.13 - ETA: 3:08 - loss: 3.2822 - accuracy: 0.13 - ETA: 3:07 - loss: 3.2862 - accuracy: 0.13 - ETA: 3:06 - loss: 3.2853 - accuracy: 0.13 - ETA: 3:05 - loss: 3.2859 - accuracy: 0.13 - ETA: 3:05 - loss: 3.2923 - accuracy: 0.13 - ETA: 3:04 - loss: 3.2856 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3006 - accuracy: 0.13 - ETA: 3:05 - loss: 3.2959 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3085 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3139 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3136 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3109 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3102 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3066 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3067 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3144 - accuracy: 0.12 - ETA: 2:59 - loss: 3.3141 - accuracy: 0.12 - ETA: 2:59 - loss: 3.3180 - accuracy: 0.12 - ETA: 2:58 - loss: 3.3197 - accuracy: 0.12 - ETA: 2:57 - loss: 3.3178 - accuracy: 0.12 - ETA: 2:57 - loss: 3.3174 - accuracy: 0.12 - ETA: 2:56 - loss: 3.3174 - accuracy: 0.12 - ETA: 2:55 - loss: 3.3170 - accuracy: 0.12 - ETA: 2:55 - loss: 3.3091 - accuracy: 0.12 - ETA: 2:54 - loss: 3.3046 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3046 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3066 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3086 - accuracy: 0.12 - ETA: 2:51 - loss: 3.3100 - accuracy: 0.12 - ETA: 2:51 - loss: 3.3126 - accuracy: 0.12 - ETA: 2:50 - loss: 3.3143 - accuracy: 0.12 - ETA: 2:49 - loss: 3.3141 - accuracy: 0.12 - ETA: 2:49 - loss: 3.3173 - accuracy: 0.12 - ETA: 2:48 - loss: 3.3179 - accuracy: 0.12 - ETA: 2:48 - loss: 3.3139 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3118 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3156 - accuracy: 0.12 - ETA: 2:46 - loss: 3.3134 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3157 - accuracy: 0.12 - ETA: 2:45 - loss: 3.3166 - accuracy: 0.12 - ETA: 2:45 - loss: 3.3161 - accuracy: 0.12 - ETA: 2:44 - loss: 3.3185 - accuracy: 0.12 - ETA: 2:43 - loss: 3.3206 - accuracy: 0.12 - ETA: 2:43 - loss: 3.3201 - accuracy: 0.12 - ETA: 2:42 - loss: 3.3203 - accuracy: 0.12 - ETA: 2:41 - loss: 3.3164 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3175 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3169 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3177 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3147 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3145 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3114 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3124 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3114 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3107 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3149 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3156 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3138 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3134 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3143 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3115 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3108 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3113 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3099 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3111 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3060 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3079 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3076 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3078 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3081 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3089 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3094 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3097 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3088 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3068 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3050 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3055 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3068 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3057 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3062 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3065 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3093 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3086 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3069 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3077 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3089 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3086 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3078 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3059 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3046 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3038 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3032 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3031 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3035 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3036 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3038 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3032 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3024 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3032 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3037 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3055 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3042 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3032 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3043 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3036 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3025 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3021 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3008 - accuracy: 0.13 - ETA: 2:04 - loss: 3.2987 - accuracy: 0.13 - ETA: 2:04 - loss: 3.2982 - accuracy: 0.13 - ETA: 2:03 - loss: 3.2989 - accuracy: 0.13 - ETA: 2:03 - loss: 3.2982 - accuracy: 0.13 - ETA: 2:02 - loss: 3.2998 - accuracy: 0.13 - ETA: 2:01 - loss: 3.2997 - accuracy: 0.13 - ETA: 2:01 - loss: 3.2993 - accuracy: 0.13 - ETA: 2:00 - loss: 3.2991 - accuracy: 0.13 - ETA: 2:00 - loss: 3.2981 - accuracy: 0.13 - ETA: 1:59 - loss: 3.2962 - accuracy: 0.13 - ETA: 1:59 - loss: 3.2950 - accuracy: 0.13 - ETA: 1:58 - loss: 3.2954 - accuracy: 0.13 - ETA: 1:57 - loss: 3.2959 - accuracy: 0.13 - ETA: 1:57 - loss: 3.2963 - accuracy: 0.13 - ETA: 1:56 - loss: 3.2967 - accuracy: 0.13 - ETA: 1:56 - loss: 3.2987 - accuracy: 0.13 - ETA: 1:55 - loss: 3.2988 - accuracy: 0.13 - ETA: 1:54 - loss: 3.2977 - accuracy: 0.13 - ETA: 1:54 - loss: 3.2967 - accuracy: 0.13 - ETA: 1:53 - loss: 3.2962 - accuracy: 0.13 - ETA: 1:53 - loss: 3.2948 - accuracy: 0.13 - ETA: 1:52 - loss: 3.2951 - accuracy: 0.13 - ETA: 1:51 - loss: 3.2953 - accuracy: 0.13 - ETA: 1:51 - loss: 3.2944 - accuracy: 0.13 - ETA: 1:50 - loss: 3.2932 - accuracy: 0.13 - ETA: 1:50 - loss: 3.2917 - accuracy: 0.13 - ETA: 1:49 - loss: 3.2924 - accuracy: 0.13 - ETA: 1:49 - loss: 3.2922 - accuracy: 0.13 - ETA: 1:48 - loss: 3.2921 - accuracy: 0.13 - ETA: 1:47 - loss: 3.2908 - accuracy: 0.13 - ETA: 1:47 - loss: 3.2913 - accuracy: 0.13 - ETA: 1:46 - loss: 3.2923 - accuracy: 0.13 - ETA: 1:45 - loss: 3.2921 - accuracy: 0.13 - ETA: 1:45 - loss: 3.2920 - accuracy: 0.13 - ETA: 1:44 - loss: 3.2921 - accuracy: 0.13 - ETA: 1:44 - loss: 3.2919 - accuracy: 0.13 - ETA: 1:43 - loss: 3.2908 - accuracy: 0.13 - ETA: 1:43 - loss: 3.2917 - accuracy: 0.13 - ETA: 1:42 - loss: 3.2922 - accuracy: 0.13 - ETA: 1:42 - loss: 3.2917 - accuracy: 0.13 - ETA: 1:41 - loss: 3.2923 - accuracy: 0.13 - ETA: 1:40 - loss: 3.2930 - accuracy: 0.13 - ETA: 1:40 - loss: 3.2927 - accuracy: 0.13 - ETA: 1:39 - loss: 3.2929 - accuracy: 0.13 - ETA: 1:39 - loss: 3.2929 - accuracy: 0.13 - ETA: 1:38 - loss: 3.2934 - accuracy: 0.13 - ETA: 1:37 - loss: 3.2936 - accuracy: 0.13 - ETA: 1:37 - loss: 3.2936 - accuracy: 0.13 - ETA: 1:36 - loss: 3.2933 - accuracy: 0.13 - ETA: 1:36 - loss: 3.2944 - accuracy: 0.13 - ETA: 1:35 - loss: 3.2934 - accuracy: 0.13 - ETA: 1:34 - loss: 3.2937 - accuracy: 0.13 - ETA: 1:34 - loss: 3.2936 - accuracy: 0.13 - ETA: 1:33 - loss: 3.2934 - accuracy: 0.13 - ETA: 1:33 - loss: 3.2930 - accuracy: 0.13 - ETA: 1:32 - loss: 3.2949 - accuracy: 0.13 - ETA: 1:31 - loss: 3.2956 - accuracy: 0.13 - ETA: 1:31 - loss: 3.2958 - accuracy: 0.13 - ETA: 1:30 - loss: 3.2956 - accuracy: 0.13 - ETA: 1:30 - loss: 3.2958 - accuracy: 0.13 - ETA: 1:29 - loss: 3.2960 - accuracy: 0.13 - ETA: 1:29 - loss: 3.2970 - accuracy: 0.13 - ETA: 1:28 - loss: 3.2977 - accuracy: 0.13 - ETA: 1:27 - loss: 3.2989 - accuracy: 0.13 - ETA: 1:27 - loss: 3.2988 - accuracy: 0.13 - ETA: 1:26 - loss: 3.2991 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3000 - accuracy: 0.13 - ETA: 1:25 - loss: 3.2997 - accuracy: 0.13 - ETA: 1:24 - loss: 3.2994 - accuracy: 0.1393"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.3007 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3010 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3017 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3021 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3022 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3027 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3030 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3030 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3031 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3032 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3025 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3025 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3023 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3024 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3021 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3030 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3029 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3024 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3022 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3030 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3024 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3024 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3028 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3030 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3029 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3031 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3038 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3035 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3032 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3033 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3040 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3037 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3049 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3047 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3046 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3057 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3045 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3046 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3056 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3058 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3063 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3063 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3068 - accuracy: 0.13 - ETA: 59s - loss: 3.3059 - accuracy: 0.1383 - ETA: 58s - loss: 3.3060 - accuracy: 0.138 - ETA: 58s - loss: 3.3059 - accuracy: 0.138 - ETA: 57s - loss: 3.3061 - accuracy: 0.138 - ETA: 57s - loss: 3.3075 - accuracy: 0.138 - ETA: 56s - loss: 3.3083 - accuracy: 0.138 - ETA: 55s - loss: 3.3083 - accuracy: 0.138 - ETA: 55s - loss: 3.3084 - accuracy: 0.138 - ETA: 54s - loss: 3.3079 - accuracy: 0.138 - ETA: 54s - loss: 3.3086 - accuracy: 0.137 - ETA: 53s - loss: 3.3083 - accuracy: 0.138 - ETA: 52s - loss: 3.3086 - accuracy: 0.138 - ETA: 52s - loss: 3.3084 - accuracy: 0.138 - ETA: 51s - loss: 3.3078 - accuracy: 0.138 - ETA: 51s - loss: 3.3081 - accuracy: 0.137 - ETA: 50s - loss: 3.3083 - accuracy: 0.137 - ETA: 50s - loss: 3.3078 - accuracy: 0.138 - ETA: 49s - loss: 3.3084 - accuracy: 0.138 - ETA: 48s - loss: 3.3090 - accuracy: 0.138 - ETA: 48s - loss: 3.3095 - accuracy: 0.138 - ETA: 47s - loss: 3.3098 - accuracy: 0.137 - ETA: 47s - loss: 3.3101 - accuracy: 0.137 - ETA: 46s - loss: 3.3102 - accuracy: 0.137 - ETA: 45s - loss: 3.3105 - accuracy: 0.137 - ETA: 45s - loss: 3.3108 - accuracy: 0.137 - ETA: 44s - loss: 3.3107 - accuracy: 0.138 - ETA: 44s - loss: 3.3109 - accuracy: 0.137 - ETA: 43s - loss: 3.3107 - accuracy: 0.138 - ETA: 42s - loss: 3.3111 - accuracy: 0.137 - ETA: 42s - loss: 3.3114 - accuracy: 0.137 - ETA: 41s - loss: 3.3105 - accuracy: 0.137 - ETA: 41s - loss: 3.3110 - accuracy: 0.137 - ETA: 40s - loss: 3.3114 - accuracy: 0.137 - ETA: 40s - loss: 3.3116 - accuracy: 0.137 - ETA: 39s - loss: 3.3115 - accuracy: 0.137 - ETA: 38s - loss: 3.3109 - accuracy: 0.137 - ETA: 38s - loss: 3.3108 - accuracy: 0.137 - ETA: 37s - loss: 3.3111 - accuracy: 0.137 - ETA: 37s - loss: 3.3108 - accuracy: 0.137 - ETA: 36s - loss: 3.3111 - accuracy: 0.137 - ETA: 35s - loss: 3.3108 - accuracy: 0.137 - ETA: 35s - loss: 3.3106 - accuracy: 0.138 - ETA: 34s - loss: 3.3107 - accuracy: 0.138 - ETA: 34s - loss: 3.3109 - accuracy: 0.138 - ETA: 33s - loss: 3.3109 - accuracy: 0.138 - ETA: 33s - loss: 3.3110 - accuracy: 0.138 - ETA: 32s - loss: 3.3112 - accuracy: 0.138 - ETA: 31s - loss: 3.3116 - accuracy: 0.137 - ETA: 31s - loss: 3.3113 - accuracy: 0.138 - ETA: 30s - loss: 3.3118 - accuracy: 0.138 - ETA: 30s - loss: 3.3118 - accuracy: 0.138 - ETA: 29s - loss: 3.3114 - accuracy: 0.138 - ETA: 28s - loss: 3.3115 - accuracy: 0.138 - ETA: 28s - loss: 3.3114 - accuracy: 0.138 - ETA: 27s - loss: 3.3111 - accuracy: 0.138 - ETA: 27s - loss: 3.3120 - accuracy: 0.138 - ETA: 26s - loss: 3.3117 - accuracy: 0.138 - ETA: 25s - loss: 3.3119 - accuracy: 0.138 - ETA: 25s - loss: 3.3115 - accuracy: 0.138 - ETA: 24s - loss: 3.3121 - accuracy: 0.138 - ETA: 24s - loss: 3.3113 - accuracy: 0.138 - ETA: 23s - loss: 3.3110 - accuracy: 0.138 - ETA: 23s - loss: 3.3111 - accuracy: 0.138 - ETA: 22s - loss: 3.3116 - accuracy: 0.138 - ETA: 21s - loss: 3.3113 - accuracy: 0.138 - ETA: 21s - loss: 3.3111 - accuracy: 0.138 - ETA: 20s - loss: 3.3111 - accuracy: 0.138 - ETA: 20s - loss: 3.3110 - accuracy: 0.138 - ETA: 19s - loss: 3.3110 - accuracy: 0.138 - ETA: 18s - loss: 3.3116 - accuracy: 0.138 - ETA: 18s - loss: 3.3110 - accuracy: 0.138 - ETA: 17s - loss: 3.3101 - accuracy: 0.138 - ETA: 17s - loss: 3.3098 - accuracy: 0.138 - ETA: 16s - loss: 3.3100 - accuracy: 0.138 - ETA: 15s - loss: 3.3102 - accuracy: 0.138 - ETA: 15s - loss: 3.3101 - accuracy: 0.138 - ETA: 14s - loss: 3.3099 - accuracy: 0.138 - ETA: 14s - loss: 3.3098 - accuracy: 0.139 - ETA: 13s - loss: 3.3099 - accuracy: 0.138 - ETA: 13s - loss: 3.3096 - accuracy: 0.139 - ETA: 12s - loss: 3.3105 - accuracy: 0.139 - ETA: 11s - loss: 3.3113 - accuracy: 0.138 - ETA: 11s - loss: 3.3117 - accuracy: 0.138 - ETA: 10s - loss: 3.3116 - accuracy: 0.138 - ETA: 10s - loss: 3.3116 - accuracy: 0.138 - ETA: 9s - loss: 3.3116 - accuracy: 0.138 - ETA: 8s - loss: 3.3119 - accuracy: 0.13 - ETA: 8s - loss: 3.3121 - accuracy: 0.13 - ETA: 7s - loss: 3.3119 - accuracy: 0.13 - ETA: 7s - loss: 3.3115 - accuracy: 0.13 - ETA: 6s - loss: 3.3124 - accuracy: 0.13 - ETA: 5s - loss: 3.3127 - accuracy: 0.13 - ETA: 5s - loss: 3.3127 - accuracy: 0.13 - ETA: 4s - loss: 3.3130 - accuracy: 0.13 - ETA: 4s - loss: 3.3130 - accuracy: 0.13 - ETA: 3s - loss: 3.3130 - accuracy: 0.13 - ETA: 2s - loss: 3.3129 - accuracy: 0.13 - ETA: 2s - loss: 3.3138 - accuracy: 0.13 - ETA: 1s - loss: 3.3141 - accuracy: 0.13 - ETA: 1s - loss: 3.3139 - accuracy: 0.13 - ETA: 0s - loss: 3.3136 - accuracy: 0.13 - ETA: 0s - loss: 3.3141 - accuracy: 0.13 - 207s 5ms/step - loss: 3.3141 - accuracy: 0.1379 - val_loss: 3.9061 - val_accuracy: 0.0375\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:43 - loss: 3.2286 - accuracy: 0.18 - ETA: 3:25 - loss: 3.3375 - accuracy: 0.14 - ETA: 3:20 - loss: 3.3689 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3839 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3539 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3551 - accuracy: 0.14 - ETA: 3:09 - loss: 3.3420 - accuracy: 0.14 - ETA: 3:08 - loss: 3.3225 - accuracy: 0.15 - ETA: 3:07 - loss: 3.3251 - accuracy: 0.15 - ETA: 3:06 - loss: 3.3207 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3126 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3174 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3210 - accuracy: 0.14 - ETA: 3:05 - loss: 3.3315 - accuracy: 0.14 - ETA: 3:04 - loss: 3.3378 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3285 - accuracy: 0.14 - ETA: 3:02 - loss: 3.3259 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3282 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3310 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3369 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3278 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3266 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3298 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3316 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3305 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3247 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3182 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3201 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3183 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3192 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3239 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3253 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3282 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3257 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3259 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3239 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3214 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3189 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3224 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3276 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3305 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3301 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3272 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3267 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3235 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3253 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3256 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3266 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3301 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3299 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3317 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3308 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3327 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3319 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3331 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3344 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3352 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3339 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3342 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3326 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3342 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3337 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3314 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3315 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3322 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3319 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3320 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3321 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3327 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3320 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3297 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3314 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3316 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3308 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3334 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3309 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3313 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3318 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3306 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3308 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3301 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3302 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3305 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3320 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3307 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3290 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3289 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3294 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3289 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3286 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3267 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3257 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3258 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3252 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3229 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3215 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3234 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3227 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3213 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3216 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3198 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3203 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3201 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3206 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3205 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3202 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3199 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3188 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3202 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3202 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3208 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3216 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3217 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3208 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3195 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3182 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3179 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3178 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3191 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3205 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3200 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3195 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3206 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3189 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3181 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3180 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3167 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3159 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3145 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3138 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3130 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3133 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3126 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3118 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3100 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3105 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3106 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3105 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3104 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3101 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3102 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3108 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3109 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3111 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3119 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3103 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3115 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3112 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3113 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3106 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3116 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3109 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3110 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3109 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3108 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3103 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3100 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3106 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3107 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3106 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3105 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3111 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3118 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3109 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3110 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3109 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3110 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3104 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3107 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3101 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3114 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3113 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3109 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3106 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3102 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3111 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3120 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3124 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3124 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3115 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3127 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3131 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3137 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3138 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3129 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3113 - accuracy: 0.1360"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.3105 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3104 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3095 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3087 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3079 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3084 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3080 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3081 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3081 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3074 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3068 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3063 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3052 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3045 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3045 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3052 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3044 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3054 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3049 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3051 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3055 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3063 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3055 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3054 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3051 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3046 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3046 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3042 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3041 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3040 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3032 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3035 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3042 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3046 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3042 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3039 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3034 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3031 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3038 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3039 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3038 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3040 - accuracy: 0.13 - ETA: 59s - loss: 3.3037 - accuracy: 0.1388 - ETA: 59s - loss: 3.3038 - accuracy: 0.138 - ETA: 58s - loss: 3.3038 - accuracy: 0.138 - ETA: 58s - loss: 3.3041 - accuracy: 0.138 - ETA: 57s - loss: 3.3036 - accuracy: 0.139 - ETA: 56s - loss: 3.3026 - accuracy: 0.139 - ETA: 56s - loss: 3.3029 - accuracy: 0.139 - ETA: 55s - loss: 3.3024 - accuracy: 0.139 - ETA: 55s - loss: 3.3024 - accuracy: 0.139 - ETA: 54s - loss: 3.3017 - accuracy: 0.139 - ETA: 53s - loss: 3.3024 - accuracy: 0.139 - ETA: 53s - loss: 3.3023 - accuracy: 0.139 - ETA: 52s - loss: 3.3021 - accuracy: 0.139 - ETA: 52s - loss: 3.3023 - accuracy: 0.139 - ETA: 51s - loss: 3.3026 - accuracy: 0.139 - ETA: 51s - loss: 3.3025 - accuracy: 0.139 - ETA: 50s - loss: 3.3026 - accuracy: 0.139 - ETA: 49s - loss: 3.3025 - accuracy: 0.139 - ETA: 49s - loss: 3.3022 - accuracy: 0.139 - ETA: 48s - loss: 3.3028 - accuracy: 0.139 - ETA: 48s - loss: 3.3030 - accuracy: 0.139 - ETA: 47s - loss: 3.3042 - accuracy: 0.139 - ETA: 46s - loss: 3.3052 - accuracy: 0.139 - ETA: 46s - loss: 3.3052 - accuracy: 0.139 - ETA: 45s - loss: 3.3051 - accuracy: 0.139 - ETA: 45s - loss: 3.3058 - accuracy: 0.138 - ETA: 44s - loss: 3.3056 - accuracy: 0.138 - ETA: 43s - loss: 3.3059 - accuracy: 0.138 - ETA: 43s - loss: 3.3053 - accuracy: 0.139 - ETA: 42s - loss: 3.3054 - accuracy: 0.138 - ETA: 42s - loss: 3.3052 - accuracy: 0.138 - ETA: 41s - loss: 3.3057 - accuracy: 0.138 - ETA: 41s - loss: 3.3065 - accuracy: 0.138 - ETA: 40s - loss: 3.3068 - accuracy: 0.138 - ETA: 39s - loss: 3.3067 - accuracy: 0.138 - ETA: 39s - loss: 3.3068 - accuracy: 0.138 - ETA: 38s - loss: 3.3067 - accuracy: 0.138 - ETA: 38s - loss: 3.3076 - accuracy: 0.138 - ETA: 37s - loss: 3.3078 - accuracy: 0.138 - ETA: 36s - loss: 3.3076 - accuracy: 0.138 - ETA: 36s - loss: 3.3073 - accuracy: 0.138 - ETA: 35s - loss: 3.3075 - accuracy: 0.138 - ETA: 35s - loss: 3.3073 - accuracy: 0.139 - ETA: 34s - loss: 3.3074 - accuracy: 0.139 - ETA: 34s - loss: 3.3077 - accuracy: 0.139 - ETA: 33s - loss: 3.3075 - accuracy: 0.139 - ETA: 32s - loss: 3.3080 - accuracy: 0.139 - ETA: 32s - loss: 3.3084 - accuracy: 0.139 - ETA: 31s - loss: 3.3091 - accuracy: 0.138 - ETA: 31s - loss: 3.3089 - accuracy: 0.138 - ETA: 30s - loss: 3.3087 - accuracy: 0.138 - ETA: 29s - loss: 3.3081 - accuracy: 0.138 - ETA: 29s - loss: 3.3074 - accuracy: 0.139 - ETA: 28s - loss: 3.3082 - accuracy: 0.138 - ETA: 28s - loss: 3.3078 - accuracy: 0.139 - ETA: 27s - loss: 3.3071 - accuracy: 0.139 - ETA: 26s - loss: 3.3074 - accuracy: 0.138 - ETA: 26s - loss: 3.3075 - accuracy: 0.138 - ETA: 25s - loss: 3.3077 - accuracy: 0.138 - ETA: 25s - loss: 3.3080 - accuracy: 0.138 - ETA: 24s - loss: 3.3070 - accuracy: 0.138 - ETA: 24s - loss: 3.3076 - accuracy: 0.138 - ETA: 23s - loss: 3.3078 - accuracy: 0.138 - ETA: 22s - loss: 3.3076 - accuracy: 0.138 - ETA: 22s - loss: 3.3074 - accuracy: 0.138 - ETA: 21s - loss: 3.3076 - accuracy: 0.138 - ETA: 21s - loss: 3.3077 - accuracy: 0.138 - ETA: 20s - loss: 3.3073 - accuracy: 0.138 - ETA: 19s - loss: 3.3069 - accuracy: 0.138 - ETA: 19s - loss: 3.3072 - accuracy: 0.138 - ETA: 18s - loss: 3.3071 - accuracy: 0.138 - ETA: 18s - loss: 3.3072 - accuracy: 0.138 - ETA: 17s - loss: 3.3073 - accuracy: 0.138 - ETA: 17s - loss: 3.3076 - accuracy: 0.138 - ETA: 16s - loss: 3.3079 - accuracy: 0.138 - ETA: 15s - loss: 3.3084 - accuracy: 0.138 - ETA: 15s - loss: 3.3082 - accuracy: 0.138 - ETA: 14s - loss: 3.3078 - accuracy: 0.138 - ETA: 14s - loss: 3.3079 - accuracy: 0.138 - ETA: 13s - loss: 3.3078 - accuracy: 0.138 - ETA: 12s - loss: 3.3076 - accuracy: 0.138 - ETA: 12s - loss: 3.3076 - accuracy: 0.138 - ETA: 11s - loss: 3.3081 - accuracy: 0.138 - ETA: 11s - loss: 3.3085 - accuracy: 0.138 - ETA: 10s - loss: 3.3088 - accuracy: 0.138 - ETA: 10s - loss: 3.3087 - accuracy: 0.138 - ETA: 9s - loss: 3.3082 - accuracy: 0.138 - ETA: 8s - loss: 3.3081 - accuracy: 0.13 - ETA: 8s - loss: 3.3081 - accuracy: 0.13 - ETA: 7s - loss: 3.3083 - accuracy: 0.13 - ETA: 7s - loss: 3.3075 - accuracy: 0.13 - ETA: 6s - loss: 3.3079 - accuracy: 0.13 - ETA: 5s - loss: 3.3080 - accuracy: 0.13 - ETA: 5s - loss: 3.3079 - accuracy: 0.13 - ETA: 4s - loss: 3.3077 - accuracy: 0.13 - ETA: 4s - loss: 3.3080 - accuracy: 0.13 - ETA: 3s - loss: 3.3083 - accuracy: 0.13 - ETA: 2s - loss: 3.3085 - accuracy: 0.13 - ETA: 2s - loss: 3.3075 - accuracy: 0.13 - ETA: 1s - loss: 3.3072 - accuracy: 0.13 - ETA: 1s - loss: 3.3071 - accuracy: 0.13 - ETA: 0s - loss: 3.3066 - accuracy: 0.13 - ETA: 0s - loss: 3.3070 - accuracy: 0.13 - 206s 5ms/step - loss: 3.3070 - accuracy: 0.1388 - val_loss: 3.9422 - val_accuracy: 0.0390\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:25 - loss: 3.1966 - accuracy: 0.18 - ETA: 3:16 - loss: 3.2133 - accuracy: 0.17 - ETA: 3:13 - loss: 3.2093 - accuracy: 0.17 - ETA: 3:12 - loss: 3.2407 - accuracy: 0.17 - ETA: 3:13 - loss: 3.2300 - accuracy: 0.16 - ETA: 3:13 - loss: 3.2448 - accuracy: 0.16 - ETA: 3:11 - loss: 3.2691 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2605 - accuracy: 0.16 - ETA: 3:09 - loss: 3.2651 - accuracy: 0.15 - ETA: 3:08 - loss: 3.2606 - accuracy: 0.16 - ETA: 3:07 - loss: 3.2578 - accuracy: 0.15 - ETA: 3:07 - loss: 3.2522 - accuracy: 0.15 - ETA: 3:08 - loss: 3.2764 - accuracy: 0.16 - ETA: 3:08 - loss: 3.2795 - accuracy: 0.16 - ETA: 3:08 - loss: 3.2821 - accuracy: 0.16 - ETA: 3:07 - loss: 3.2775 - accuracy: 0.16 - ETA: 3:06 - loss: 3.2856 - accuracy: 0.15 - ETA: 3:05 - loss: 3.2822 - accuracy: 0.15 - ETA: 3:05 - loss: 3.2939 - accuracy: 0.15 - ETA: 3:05 - loss: 3.2884 - accuracy: 0.15 - ETA: 3:05 - loss: 3.2829 - accuracy: 0.16 - ETA: 3:06 - loss: 3.2837 - accuracy: 0.16 - ETA: 3:06 - loss: 3.2868 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2881 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2960 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2975 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2938 - accuracy: 0.15 - ETA: 3:05 - loss: 3.2869 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2863 - accuracy: 0.15 - ETA: 3:03 - loss: 3.2951 - accuracy: 0.15 - ETA: 3:02 - loss: 3.2905 - accuracy: 0.15 - ETA: 3:01 - loss: 3.2862 - accuracy: 0.15 - ETA: 3:00 - loss: 3.2892 - accuracy: 0.15 - ETA: 2:59 - loss: 3.2866 - accuracy: 0.15 - ETA: 2:59 - loss: 3.2910 - accuracy: 0.15 - ETA: 2:58 - loss: 3.2945 - accuracy: 0.15 - ETA: 2:57 - loss: 3.2934 - accuracy: 0.15 - ETA: 2:56 - loss: 3.2946 - accuracy: 0.15 - ETA: 2:55 - loss: 3.2943 - accuracy: 0.15 - ETA: 2:55 - loss: 3.2926 - accuracy: 0.15 - ETA: 2:54 - loss: 3.2936 - accuracy: 0.15 - ETA: 2:53 - loss: 3.2949 - accuracy: 0.15 - ETA: 2:53 - loss: 3.2939 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2949 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2935 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2927 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2908 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2899 - accuracy: 0.15 - ETA: 2:48 - loss: 3.2905 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2913 - accuracy: 0.15 - ETA: 2:48 - loss: 3.2923 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2928 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2918 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2887 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2858 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2819 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2789 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2801 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2805 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2779 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2785 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2804 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2806 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2817 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2819 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2820 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2845 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2859 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2868 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2877 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2894 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2891 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2884 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2876 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2873 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2866 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2849 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2846 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2838 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2819 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2825 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2807 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2803 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2794 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2780 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2780 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2783 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2772 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2783 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2773 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2781 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2770 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2781 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2801 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2774 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2771 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2785 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2792 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2790 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2777 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2774 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2783 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2778 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2782 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2794 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2796 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2787 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2780 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2782 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2781 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2789 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2790 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2785 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2779 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2777 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2764 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2751 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2756 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2759 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2751 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2744 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2743 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2752 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2755 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2756 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2761 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2758 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2773 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2769 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2758 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2763 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2770 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2774 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2759 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2771 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2764 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2757 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2750 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2746 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2753 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2748 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2747 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2732 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2717 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2716 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2725 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2725 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2740 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2747 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2765 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2774 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2773 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2775 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2776 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2777 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2775 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2776 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2772 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2769 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2768 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2767 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2768 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2750 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2744 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2751 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2762 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2759 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2755 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2759 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2748 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2747 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2756 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2754 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2760 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2759 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2760 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2761 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2773 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2774 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2776 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2778 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2785 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2782 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2788 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2793 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2794 - accuracy: 0.1464"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.2783 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2781 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2789 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2790 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2795 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2794 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2788 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2777 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2778 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2771 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2760 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2754 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2746 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2748 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2746 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2746 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2751 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2753 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2754 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2757 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2771 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2766 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2764 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2756 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2756 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2750 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2742 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2742 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2743 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2743 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2736 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2735 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2733 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2737 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2740 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2756 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2762 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2761 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2762 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2758 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2755 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2756 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2752 - accuracy: 0.14 - ETA: 59s - loss: 3.2755 - accuracy: 0.1465 - ETA: 58s - loss: 3.2748 - accuracy: 0.146 - ETA: 58s - loss: 3.2744 - accuracy: 0.146 - ETA: 57s - loss: 3.2742 - accuracy: 0.146 - ETA: 57s - loss: 3.2747 - accuracy: 0.146 - ETA: 56s - loss: 3.2753 - accuracy: 0.146 - ETA: 55s - loss: 3.2764 - accuracy: 0.146 - ETA: 55s - loss: 3.2756 - accuracy: 0.146 - ETA: 54s - loss: 3.2759 - accuracy: 0.146 - ETA: 54s - loss: 3.2755 - accuracy: 0.146 - ETA: 53s - loss: 3.2743 - accuracy: 0.146 - ETA: 52s - loss: 3.2749 - accuracy: 0.146 - ETA: 52s - loss: 3.2752 - accuracy: 0.146 - ETA: 51s - loss: 3.2750 - accuracy: 0.146 - ETA: 51s - loss: 3.2749 - accuracy: 0.146 - ETA: 50s - loss: 3.2747 - accuracy: 0.146 - ETA: 50s - loss: 3.2740 - accuracy: 0.146 - ETA: 49s - loss: 3.2738 - accuracy: 0.146 - ETA: 48s - loss: 3.2741 - accuracy: 0.146 - ETA: 48s - loss: 3.2743 - accuracy: 0.146 - ETA: 47s - loss: 3.2747 - accuracy: 0.146 - ETA: 47s - loss: 3.2740 - accuracy: 0.146 - ETA: 46s - loss: 3.2738 - accuracy: 0.146 - ETA: 45s - loss: 3.2739 - accuracy: 0.146 - ETA: 45s - loss: 3.2747 - accuracy: 0.146 - ETA: 44s - loss: 3.2741 - accuracy: 0.146 - ETA: 44s - loss: 3.2743 - accuracy: 0.146 - ETA: 43s - loss: 3.2748 - accuracy: 0.146 - ETA: 42s - loss: 3.2753 - accuracy: 0.146 - ETA: 42s - loss: 3.2753 - accuracy: 0.146 - ETA: 41s - loss: 3.2753 - accuracy: 0.146 - ETA: 41s - loss: 3.2752 - accuracy: 0.146 - ETA: 40s - loss: 3.2755 - accuracy: 0.146 - ETA: 39s - loss: 3.2750 - accuracy: 0.146 - ETA: 39s - loss: 3.2743 - accuracy: 0.146 - ETA: 38s - loss: 3.2746 - accuracy: 0.146 - ETA: 38s - loss: 3.2741 - accuracy: 0.146 - ETA: 37s - loss: 3.2740 - accuracy: 0.146 - ETA: 37s - loss: 3.2745 - accuracy: 0.146 - ETA: 36s - loss: 3.2745 - accuracy: 0.146 - ETA: 35s - loss: 3.2746 - accuracy: 0.146 - ETA: 35s - loss: 3.2740 - accuracy: 0.146 - ETA: 34s - loss: 3.2734 - accuracy: 0.146 - ETA: 34s - loss: 3.2733 - accuracy: 0.146 - ETA: 33s - loss: 3.2731 - accuracy: 0.146 - ETA: 32s - loss: 3.2728 - accuracy: 0.146 - ETA: 32s - loss: 3.2726 - accuracy: 0.146 - ETA: 31s - loss: 3.2733 - accuracy: 0.146 - ETA: 31s - loss: 3.2729 - accuracy: 0.147 - ETA: 30s - loss: 3.2733 - accuracy: 0.146 - ETA: 29s - loss: 3.2732 - accuracy: 0.146 - ETA: 29s - loss: 3.2739 - accuracy: 0.146 - ETA: 28s - loss: 3.2741 - accuracy: 0.146 - ETA: 28s - loss: 3.2744 - accuracy: 0.146 - ETA: 27s - loss: 3.2747 - accuracy: 0.146 - ETA: 27s - loss: 3.2741 - accuracy: 0.146 - ETA: 26s - loss: 3.2740 - accuracy: 0.146 - ETA: 25s - loss: 3.2746 - accuracy: 0.146 - ETA: 25s - loss: 3.2747 - accuracy: 0.146 - ETA: 24s - loss: 3.2746 - accuracy: 0.146 - ETA: 24s - loss: 3.2747 - accuracy: 0.146 - ETA: 23s - loss: 3.2750 - accuracy: 0.146 - ETA: 22s - loss: 3.2751 - accuracy: 0.146 - ETA: 22s - loss: 3.2744 - accuracy: 0.146 - ETA: 21s - loss: 3.2738 - accuracy: 0.146 - ETA: 21s - loss: 3.2745 - accuracy: 0.146 - ETA: 20s - loss: 3.2747 - accuracy: 0.146 - ETA: 20s - loss: 3.2751 - accuracy: 0.146 - ETA: 19s - loss: 3.2754 - accuracy: 0.146 - ETA: 18s - loss: 3.2758 - accuracy: 0.146 - ETA: 18s - loss: 3.2758 - accuracy: 0.146 - ETA: 17s - loss: 3.2763 - accuracy: 0.146 - ETA: 17s - loss: 3.2764 - accuracy: 0.146 - ETA: 16s - loss: 3.2766 - accuracy: 0.146 - ETA: 15s - loss: 3.2770 - accuracy: 0.145 - ETA: 15s - loss: 3.2767 - accuracy: 0.146 - ETA: 14s - loss: 3.2768 - accuracy: 0.145 - ETA: 14s - loss: 3.2768 - accuracy: 0.145 - ETA: 13s - loss: 3.2770 - accuracy: 0.145 - ETA: 12s - loss: 3.2770 - accuracy: 0.145 - ETA: 12s - loss: 3.2770 - accuracy: 0.145 - ETA: 11s - loss: 3.2768 - accuracy: 0.145 - ETA: 11s - loss: 3.2767 - accuracy: 0.145 - ETA: 10s - loss: 3.2767 - accuracy: 0.145 - ETA: 10s - loss: 3.2766 - accuracy: 0.145 - ETA: 9s - loss: 3.2764 - accuracy: 0.145 - ETA: 8s - loss: 3.2771 - accuracy: 0.14 - ETA: 8s - loss: 3.2770 - accuracy: 0.14 - ETA: 7s - loss: 3.2773 - accuracy: 0.14 - ETA: 7s - loss: 3.2772 - accuracy: 0.14 - ETA: 6s - loss: 3.2765 - accuracy: 0.14 - ETA: 5s - loss: 3.2767 - accuracy: 0.14 - ETA: 5s - loss: 3.2769 - accuracy: 0.14 - ETA: 4s - loss: 3.2766 - accuracy: 0.14 - ETA: 4s - loss: 3.2758 - accuracy: 0.14 - ETA: 3s - loss: 3.2761 - accuracy: 0.14 - ETA: 2s - loss: 3.2760 - accuracy: 0.14 - ETA: 2s - loss: 3.2758 - accuracy: 0.14 - ETA: 1s - loss: 3.2756 - accuracy: 0.14 - ETA: 1s - loss: 3.2759 - accuracy: 0.14 - ETA: 0s - loss: 3.2760 - accuracy: 0.14 - ETA: 0s - loss: 3.2762 - accuracy: 0.14 - 206s 5ms/step - loss: 3.2761 - accuracy: 0.1461 - val_loss: 3.9570 - val_accuracy: 0.0431\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:28 - loss: 3.3487 - accuracy: 0.10 - ETA: 3:12 - loss: 3.3663 - accuracy: 0.12 - ETA: 3:10 - loss: 3.2961 - accuracy: 0.13 - ETA: 3:09 - loss: 3.2870 - accuracy: 0.13 - ETA: 3:09 - loss: 3.2633 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2553 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2363 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2428 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2399 - accuracy: 0.15 - ETA: 3:05 - loss: 3.2498 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2465 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2454 - accuracy: 0.15 - ETA: 3:03 - loss: 3.2439 - accuracy: 0.15 - ETA: 3:03 - loss: 3.2555 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2542 - accuracy: 0.15 - ETA: 3:03 - loss: 3.2535 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2573 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2512 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2486 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2424 - accuracy: 0.15 - ETA: 3:01 - loss: 3.2450 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2445 - accuracy: 0.15 - ETA: 3:00 - loss: 3.2448 - accuracy: 0.15 - ETA: 2:59 - loss: 3.2438 - accuracy: 0.15 - ETA: 2:58 - loss: 3.2508 - accuracy: 0.15 - ETA: 2:59 - loss: 3.2511 - accuracy: 0.15 - ETA: 2:59 - loss: 3.2540 - accuracy: 0.15 - ETA: 2:58 - loss: 3.2462 - accuracy: 0.15 - ETA: 2:58 - loss: 3.2494 - accuracy: 0.15 - ETA: 2:57 - loss: 3.2559 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2504 - accuracy: 0.15 - ETA: 2:55 - loss: 3.2483 - accuracy: 0.15 - ETA: 2:55 - loss: 3.2523 - accuracy: 0.15 - ETA: 2:54 - loss: 3.2604 - accuracy: 0.15 - ETA: 2:53 - loss: 3.2620 - accuracy: 0.15 - ETA: 2:53 - loss: 3.2655 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2700 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2680 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2687 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2719 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2741 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2734 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2729 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2706 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2692 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2673 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2714 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2713 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2745 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2739 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2731 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2745 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2798 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2811 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2812 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2774 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2783 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2773 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2787 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2765 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2770 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2759 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2729 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2722 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2706 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2702 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2695 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2690 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2703 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2747 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2749 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2744 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2727 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2718 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2698 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2705 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2670 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2659 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2660 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2667 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2668 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2647 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2652 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2660 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2659 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2685 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2673 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2687 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2648 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2660 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2664 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2648 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2640 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2638 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2651 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2641 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2621 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2627 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2596 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2598 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2581 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2607 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2595 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2568 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2585 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2566 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2546 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2538 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2560 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2569 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2575 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2551 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2554 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2546 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2538 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2521 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2506 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2510 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2503 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2512 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2518 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2516 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2517 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2521 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2508 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2508 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2501 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2498 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2504 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2511 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2519 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2508 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2514 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2507 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2511 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2520 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2523 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2524 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2529 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2542 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2555 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2558 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2552 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2535 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2514 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2504 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2497 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2499 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2500 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2517 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2528 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2528 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2544 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2543 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2551 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2552 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2542 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2551 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2551 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2563 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2572 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2578 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2581 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2578 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2577 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2584 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2574 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2573 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2565 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2564 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2568 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2560 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2559 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2566 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2571 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2576 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2584 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2591 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2589 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2585 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2570 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2565 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2570 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2569 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2568 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2551 - accuracy: 0.1505"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.2536 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2540 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2547 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2549 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2552 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2562 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2562 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2564 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2562 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2558 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2560 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2565 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2559 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2553 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2555 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2555 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2556 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2558 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2566 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2561 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2566 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2571 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2577 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2577 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2573 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2569 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2576 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2579 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2578 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2578 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2575 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2574 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2581 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2586 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2580 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2577 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2581 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2572 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2562 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2561 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2549 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2557 - accuracy: 0.15 - ETA: 59s - loss: 3.2564 - accuracy: 0.1502 - ETA: 59s - loss: 3.2570 - accuracy: 0.150 - ETA: 58s - loss: 3.2563 - accuracy: 0.150 - ETA: 58s - loss: 3.2560 - accuracy: 0.150 - ETA: 57s - loss: 3.2568 - accuracy: 0.149 - ETA: 56s - loss: 3.2567 - accuracy: 0.149 - ETA: 56s - loss: 3.2571 - accuracy: 0.149 - ETA: 55s - loss: 3.2557 - accuracy: 0.150 - ETA: 55s - loss: 3.2559 - accuracy: 0.150 - ETA: 54s - loss: 3.2558 - accuracy: 0.150 - ETA: 53s - loss: 3.2559 - accuracy: 0.150 - ETA: 53s - loss: 3.2563 - accuracy: 0.150 - ETA: 52s - loss: 3.2570 - accuracy: 0.150 - ETA: 52s - loss: 3.2567 - accuracy: 0.150 - ETA: 51s - loss: 3.2564 - accuracy: 0.150 - ETA: 50s - loss: 3.2562 - accuracy: 0.150 - ETA: 50s - loss: 3.2561 - accuracy: 0.150 - ETA: 49s - loss: 3.2549 - accuracy: 0.150 - ETA: 49s - loss: 3.2555 - accuracy: 0.150 - ETA: 48s - loss: 3.2556 - accuracy: 0.150 - ETA: 48s - loss: 3.2553 - accuracy: 0.150 - ETA: 47s - loss: 3.2556 - accuracy: 0.150 - ETA: 46s - loss: 3.2556 - accuracy: 0.150 - ETA: 46s - loss: 3.2556 - accuracy: 0.150 - ETA: 45s - loss: 3.2557 - accuracy: 0.150 - ETA: 45s - loss: 3.2558 - accuracy: 0.150 - ETA: 44s - loss: 3.2565 - accuracy: 0.150 - ETA: 43s - loss: 3.2561 - accuracy: 0.150 - ETA: 43s - loss: 3.2570 - accuracy: 0.150 - ETA: 42s - loss: 3.2562 - accuracy: 0.150 - ETA: 42s - loss: 3.2562 - accuracy: 0.150 - ETA: 41s - loss: 3.2565 - accuracy: 0.150 - ETA: 41s - loss: 3.2564 - accuracy: 0.150 - ETA: 40s - loss: 3.2562 - accuracy: 0.150 - ETA: 39s - loss: 3.2562 - accuracy: 0.150 - ETA: 39s - loss: 3.2567 - accuracy: 0.149 - ETA: 38s - loss: 3.2562 - accuracy: 0.149 - ETA: 38s - loss: 3.2566 - accuracy: 0.149 - ETA: 37s - loss: 3.2566 - accuracy: 0.149 - ETA: 36s - loss: 3.2565 - accuracy: 0.150 - ETA: 36s - loss: 3.2568 - accuracy: 0.149 - ETA: 35s - loss: 3.2567 - accuracy: 0.149 - ETA: 35s - loss: 3.2570 - accuracy: 0.149 - ETA: 34s - loss: 3.2566 - accuracy: 0.149 - ETA: 34s - loss: 3.2567 - accuracy: 0.149 - ETA: 33s - loss: 3.2570 - accuracy: 0.149 - ETA: 32s - loss: 3.2568 - accuracy: 0.149 - ETA: 32s - loss: 3.2569 - accuracy: 0.149 - ETA: 31s - loss: 3.2565 - accuracy: 0.149 - ETA: 31s - loss: 3.2563 - accuracy: 0.149 - ETA: 30s - loss: 3.2564 - accuracy: 0.149 - ETA: 29s - loss: 3.2553 - accuracy: 0.150 - ETA: 29s - loss: 3.2556 - accuracy: 0.150 - ETA: 28s - loss: 3.2557 - accuracy: 0.150 - ETA: 28s - loss: 3.2559 - accuracy: 0.150 - ETA: 27s - loss: 3.2562 - accuracy: 0.150 - ETA: 27s - loss: 3.2561 - accuracy: 0.150 - ETA: 26s - loss: 3.2561 - accuracy: 0.150 - ETA: 25s - loss: 3.2559 - accuracy: 0.150 - ETA: 25s - loss: 3.2559 - accuracy: 0.150 - ETA: 24s - loss: 3.2558 - accuracy: 0.150 - ETA: 24s - loss: 3.2551 - accuracy: 0.150 - ETA: 23s - loss: 3.2550 - accuracy: 0.150 - ETA: 22s - loss: 3.2545 - accuracy: 0.151 - ETA: 22s - loss: 3.2546 - accuracy: 0.150 - ETA: 21s - loss: 3.2541 - accuracy: 0.151 - ETA: 21s - loss: 3.2544 - accuracy: 0.150 - ETA: 20s - loss: 3.2543 - accuracy: 0.150 - ETA: 19s - loss: 3.2538 - accuracy: 0.151 - ETA: 19s - loss: 3.2539 - accuracy: 0.150 - ETA: 18s - loss: 3.2533 - accuracy: 0.151 - ETA: 18s - loss: 3.2530 - accuracy: 0.151 - ETA: 17s - loss: 3.2531 - accuracy: 0.151 - ETA: 17s - loss: 3.2532 - accuracy: 0.151 - ETA: 16s - loss: 3.2531 - accuracy: 0.151 - ETA: 15s - loss: 3.2529 - accuracy: 0.151 - ETA: 15s - loss: 3.2523 - accuracy: 0.151 - ETA: 14s - loss: 3.2523 - accuracy: 0.151 - ETA: 14s - loss: 3.2526 - accuracy: 0.151 - ETA: 13s - loss: 3.2530 - accuracy: 0.150 - ETA: 12s - loss: 3.2526 - accuracy: 0.150 - ETA: 12s - loss: 3.2528 - accuracy: 0.150 - ETA: 11s - loss: 3.2529 - accuracy: 0.150 - ETA: 11s - loss: 3.2528 - accuracy: 0.150 - ETA: 10s - loss: 3.2532 - accuracy: 0.150 - ETA: 10s - loss: 3.2528 - accuracy: 0.150 - ETA: 9s - loss: 3.2528 - accuracy: 0.150 - ETA: 8s - loss: 3.2522 - accuracy: 0.15 - ETA: 8s - loss: 3.2527 - accuracy: 0.15 - ETA: 7s - loss: 3.2529 - accuracy: 0.15 - ETA: 7s - loss: 3.2528 - accuracy: 0.15 - ETA: 6s - loss: 3.2531 - accuracy: 0.15 - ETA: 5s - loss: 3.2536 - accuracy: 0.15 - ETA: 5s - loss: 3.2533 - accuracy: 0.15 - ETA: 4s - loss: 3.2530 - accuracy: 0.15 - ETA: 4s - loss: 3.2527 - accuracy: 0.15 - ETA: 3s - loss: 3.2524 - accuracy: 0.15 - ETA: 2s - loss: 3.2525 - accuracy: 0.15 - ETA: 2s - loss: 3.2523 - accuracy: 0.15 - ETA: 1s - loss: 3.2528 - accuracy: 0.15 - ETA: 1s - loss: 3.2531 - accuracy: 0.15 - ETA: 0s - loss: 3.2529 - accuracy: 0.15 - ETA: 0s - loss: 3.2530 - accuracy: 0.15 - 206s 5ms/step - loss: 3.2531 - accuracy: 0.1508 - val_loss: 3.9307 - val_accuracy: 0.0394\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:30 - loss: 3.3698 - accuracy: 0.10 - ETA: 3:27 - loss: 3.3132 - accuracy: 0.17 - ETA: 3:24 - loss: 3.1694 - accuracy: 0.19 - ETA: 3:21 - loss: 3.1973 - accuracy: 0.19 - ETA: 3:19 - loss: 3.2042 - accuracy: 0.18 - ETA: 3:17 - loss: 3.1957 - accuracy: 0.18 - ETA: 3:15 - loss: 3.1997 - accuracy: 0.18 - ETA: 3:13 - loss: 3.1823 - accuracy: 0.18 - ETA: 3:12 - loss: 3.1985 - accuracy: 0.17 - ETA: 3:11 - loss: 3.2049 - accuracy: 0.17 - ETA: 3:10 - loss: 3.2082 - accuracy: 0.16 - ETA: 3:08 - loss: 3.2147 - accuracy: 0.16 - ETA: 3:07 - loss: 3.2194 - accuracy: 0.16 - ETA: 3:06 - loss: 3.2276 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2356 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2386 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2389 - accuracy: 0.15 - ETA: 3:03 - loss: 3.2411 - accuracy: 0.15 - ETA: 3:02 - loss: 3.2252 - accuracy: 0.15 - ETA: 3:01 - loss: 3.2278 - accuracy: 0.15 - ETA: 3:00 - loss: 3.2284 - accuracy: 0.15 - ETA: 2:59 - loss: 3.2268 - accuracy: 0.15 - ETA: 2:59 - loss: 3.2182 - accuracy: 0.15 - ETA: 2:58 - loss: 3.2143 - accuracy: 0.15 - ETA: 2:57 - loss: 3.2155 - accuracy: 0.15 - ETA: 2:57 - loss: 3.2118 - accuracy: 0.15 - ETA: 2:56 - loss: 3.2061 - accuracy: 0.16 - ETA: 2:56 - loss: 3.2011 - accuracy: 0.16 - ETA: 2:55 - loss: 3.2047 - accuracy: 0.16 - ETA: 2:56 - loss: 3.2067 - accuracy: 0.16 - ETA: 2:55 - loss: 3.2052 - accuracy: 0.16 - ETA: 2:55 - loss: 3.2054 - accuracy: 0.16 - ETA: 2:55 - loss: 3.2006 - accuracy: 0.16 - ETA: 2:55 - loss: 3.1950 - accuracy: 0.16 - ETA: 2:54 - loss: 3.1937 - accuracy: 0.16 - ETA: 2:53 - loss: 3.1934 - accuracy: 0.16 - ETA: 2:53 - loss: 3.1941 - accuracy: 0.16 - ETA: 2:52 - loss: 3.1957 - accuracy: 0.16 - ETA: 2:51 - loss: 3.1959 - accuracy: 0.16 - ETA: 2:51 - loss: 3.1941 - accuracy: 0.16 - ETA: 2:50 - loss: 3.1966 - accuracy: 0.16 - ETA: 2:49 - loss: 3.1982 - accuracy: 0.16 - ETA: 2:49 - loss: 3.1968 - accuracy: 0.16 - ETA: 2:48 - loss: 3.2009 - accuracy: 0.16 - ETA: 2:47 - loss: 3.2026 - accuracy: 0.16 - ETA: 2:47 - loss: 3.2036 - accuracy: 0.16 - ETA: 2:46 - loss: 3.2054 - accuracy: 0.16 - ETA: 2:45 - loss: 3.2080 - accuracy: 0.16 - ETA: 2:45 - loss: 3.2087 - accuracy: 0.16 - ETA: 2:44 - loss: 3.2100 - accuracy: 0.16 - ETA: 2:43 - loss: 3.2097 - accuracy: 0.16 - ETA: 2:43 - loss: 3.2089 - accuracy: 0.16 - ETA: 2:43 - loss: 3.2114 - accuracy: 0.16 - ETA: 2:43 - loss: 3.2154 - accuracy: 0.16 - ETA: 2:43 - loss: 3.2148 - accuracy: 0.16 - ETA: 2:43 - loss: 3.2118 - accuracy: 0.16 - ETA: 2:44 - loss: 3.2095 - accuracy: 0.16 - ETA: 2:44 - loss: 3.2083 - accuracy: 0.16 - ETA: 2:43 - loss: 3.2087 - accuracy: 0.16 - ETA: 2:43 - loss: 3.2086 - accuracy: 0.16 - ETA: 2:42 - loss: 3.2111 - accuracy: 0.16 - ETA: 2:41 - loss: 3.2099 - accuracy: 0.16 - ETA: 2:41 - loss: 3.2108 - accuracy: 0.16 - ETA: 2:40 - loss: 3.2127 - accuracy: 0.16 - ETA: 2:40 - loss: 3.2116 - accuracy: 0.16 - ETA: 2:39 - loss: 3.2123 - accuracy: 0.16 - ETA: 2:38 - loss: 3.2115 - accuracy: 0.16 - ETA: 2:37 - loss: 3.2116 - accuracy: 0.16 - ETA: 2:37 - loss: 3.2119 - accuracy: 0.16 - ETA: 2:36 - loss: 3.2112 - accuracy: 0.16 - ETA: 2:35 - loss: 3.2119 - accuracy: 0.16 - ETA: 2:34 - loss: 3.2156 - accuracy: 0.16 - ETA: 2:34 - loss: 3.2150 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2156 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2168 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2191 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2198 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2204 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2226 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2218 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2206 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2214 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2213 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2226 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2227 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2218 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2237 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2224 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2252 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2244 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2257 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2241 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2248 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2251 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2254 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2245 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2239 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2234 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2215 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2205 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2196 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2195 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2191 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2191 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2204 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2215 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2238 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2239 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2253 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2237 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2252 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2263 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2257 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2257 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2249 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2253 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2240 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2220 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2211 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2208 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2208 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2210 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2212 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2228 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2242 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2245 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2232 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2230 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2224 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2196 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2199 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2198 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2198 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2216 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2211 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2211 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2212 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2207 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2200 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2210 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2202 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2194 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2192 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2193 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2192 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2183 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2178 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2181 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2192 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2182 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2191 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2196 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2199 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2196 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2200 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2203 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2207 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2206 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2216 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2208 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2214 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2216 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2201 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2202 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2211 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2216 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2215 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2213 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2222 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2218 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2229 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2230 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2231 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2224 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2226 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2219 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2217 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2220 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2214 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2225 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2226 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2218 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2220 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2211 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2208 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2210 - accuracy: 0.1569"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:25 - loss: 3.2208 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2214 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2209 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2205 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2206 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2204 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2207 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2218 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2225 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2227 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2222 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2219 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2222 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2224 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2218 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2215 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2203 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2196 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2195 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2203 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2198 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2206 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2210 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2208 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2211 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2208 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2210 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2207 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2203 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2193 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2192 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2192 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2186 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2185 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2195 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2180 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2172 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2164 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2167 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2159 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2155 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2157 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2162 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2166 - accuracy: 0.15 - ETA: 59s - loss: 3.2167 - accuracy: 0.1585 - ETA: 58s - loss: 3.2161 - accuracy: 0.158 - ETA: 58s - loss: 3.2162 - accuracy: 0.158 - ETA: 57s - loss: 3.2171 - accuracy: 0.158 - ETA: 57s - loss: 3.2178 - accuracy: 0.157 - ETA: 56s - loss: 3.2181 - accuracy: 0.158 - ETA: 55s - loss: 3.2182 - accuracy: 0.157 - ETA: 55s - loss: 3.2186 - accuracy: 0.157 - ETA: 54s - loss: 3.2184 - accuracy: 0.157 - ETA: 54s - loss: 3.2182 - accuracy: 0.157 - ETA: 53s - loss: 3.2188 - accuracy: 0.157 - ETA: 52s - loss: 3.2183 - accuracy: 0.157 - ETA: 52s - loss: 3.2185 - accuracy: 0.157 - ETA: 51s - loss: 3.2178 - accuracy: 0.157 - ETA: 51s - loss: 3.2185 - accuracy: 0.157 - ETA: 50s - loss: 3.2182 - accuracy: 0.157 - ETA: 49s - loss: 3.2182 - accuracy: 0.157 - ETA: 49s - loss: 3.2176 - accuracy: 0.157 - ETA: 48s - loss: 3.2174 - accuracy: 0.157 - ETA: 48s - loss: 3.2171 - accuracy: 0.157 - ETA: 47s - loss: 3.2167 - accuracy: 0.157 - ETA: 46s - loss: 3.2165 - accuracy: 0.157 - ETA: 46s - loss: 3.2169 - accuracy: 0.157 - ETA: 45s - loss: 3.2179 - accuracy: 0.157 - ETA: 45s - loss: 3.2187 - accuracy: 0.157 - ETA: 44s - loss: 3.2182 - accuracy: 0.157 - ETA: 43s - loss: 3.2181 - accuracy: 0.157 - ETA: 43s - loss: 3.2186 - accuracy: 0.157 - ETA: 42s - loss: 3.2187 - accuracy: 0.157 - ETA: 42s - loss: 3.2185 - accuracy: 0.157 - ETA: 41s - loss: 3.2189 - accuracy: 0.157 - ETA: 41s - loss: 3.2191 - accuracy: 0.156 - ETA: 40s - loss: 3.2192 - accuracy: 0.156 - ETA: 39s - loss: 3.2192 - accuracy: 0.156 - ETA: 39s - loss: 3.2187 - accuracy: 0.157 - ETA: 38s - loss: 3.2188 - accuracy: 0.157 - ETA: 38s - loss: 3.2194 - accuracy: 0.157 - ETA: 37s - loss: 3.2190 - accuracy: 0.157 - ETA: 36s - loss: 3.2196 - accuracy: 0.156 - ETA: 36s - loss: 3.2194 - accuracy: 0.156 - ETA: 35s - loss: 3.2194 - accuracy: 0.156 - ETA: 35s - loss: 3.2193 - accuracy: 0.156 - ETA: 34s - loss: 3.2190 - accuracy: 0.156 - ETA: 33s - loss: 3.2188 - accuracy: 0.156 - ETA: 33s - loss: 3.2194 - accuracy: 0.156 - ETA: 32s - loss: 3.2197 - accuracy: 0.156 - ETA: 32s - loss: 3.2190 - accuracy: 0.157 - ETA: 31s - loss: 3.2191 - accuracy: 0.157 - ETA: 30s - loss: 3.2193 - accuracy: 0.157 - ETA: 30s - loss: 3.2181 - accuracy: 0.157 - ETA: 29s - loss: 3.2176 - accuracy: 0.157 - ETA: 29s - loss: 3.2176 - accuracy: 0.157 - ETA: 28s - loss: 3.2176 - accuracy: 0.157 - ETA: 27s - loss: 3.2176 - accuracy: 0.157 - ETA: 27s - loss: 3.2174 - accuracy: 0.157 - ETA: 26s - loss: 3.2170 - accuracy: 0.157 - ETA: 26s - loss: 3.2171 - accuracy: 0.157 - ETA: 25s - loss: 3.2170 - accuracy: 0.157 - ETA: 24s - loss: 3.2171 - accuracy: 0.157 - ETA: 24s - loss: 3.2170 - accuracy: 0.157 - ETA: 23s - loss: 3.2171 - accuracy: 0.157 - ETA: 23s - loss: 3.2177 - accuracy: 0.157 - ETA: 22s - loss: 3.2176 - accuracy: 0.157 - ETA: 21s - loss: 3.2183 - accuracy: 0.157 - ETA: 21s - loss: 3.2189 - accuracy: 0.157 - ETA: 20s - loss: 3.2195 - accuracy: 0.157 - ETA: 20s - loss: 3.2195 - accuracy: 0.157 - ETA: 19s - loss: 3.2199 - accuracy: 0.157 - ETA: 19s - loss: 3.2197 - accuracy: 0.157 - ETA: 18s - loss: 3.2199 - accuracy: 0.157 - ETA: 17s - loss: 3.2199 - accuracy: 0.157 - ETA: 17s - loss: 3.2195 - accuracy: 0.157 - ETA: 16s - loss: 3.2199 - accuracy: 0.157 - ETA: 16s - loss: 3.2204 - accuracy: 0.156 - ETA: 15s - loss: 3.2201 - accuracy: 0.157 - ETA: 14s - loss: 3.2196 - accuracy: 0.157 - ETA: 14s - loss: 3.2189 - accuracy: 0.157 - ETA: 13s - loss: 3.2195 - accuracy: 0.157 - ETA: 13s - loss: 3.2190 - accuracy: 0.157 - ETA: 12s - loss: 3.2188 - accuracy: 0.157 - ETA: 11s - loss: 3.2193 - accuracy: 0.157 - ETA: 11s - loss: 3.2197 - accuracy: 0.157 - ETA: 10s - loss: 3.2199 - accuracy: 0.157 - ETA: 10s - loss: 3.2193 - accuracy: 0.157 - ETA: 9s - loss: 3.2195 - accuracy: 0.157 - ETA: 8s - loss: 3.2192 - accuracy: 0.15 - ETA: 8s - loss: 3.2196 - accuracy: 0.15 - ETA: 7s - loss: 3.2196 - accuracy: 0.15 - ETA: 7s - loss: 3.2200 - accuracy: 0.15 - ETA: 6s - loss: 3.2204 - accuracy: 0.15 - ETA: 5s - loss: 3.2201 - accuracy: 0.15 - ETA: 5s - loss: 3.2200 - accuracy: 0.15 - ETA: 4s - loss: 3.2205 - accuracy: 0.15 - ETA: 4s - loss: 3.2206 - accuracy: 0.15 - ETA: 3s - loss: 3.2210 - accuracy: 0.15 - ETA: 3s - loss: 3.2209 - accuracy: 0.15 - ETA: 2s - loss: 3.2206 - accuracy: 0.15 - ETA: 1s - loss: 3.2207 - accuracy: 0.15 - ETA: 1s - loss: 3.2206 - accuracy: 0.15 - ETA: 0s - loss: 3.2209 - accuracy: 0.15 - ETA: 0s - loss: 3.2208 - accuracy: 0.15 - 208s 5ms/step - loss: 3.2209 - accuracy: 0.1568 - val_loss: 3.9903 - val_accuracy: 0.0391\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:39 - loss: 3.3055 - accuracy: 0.15 - ETA: 3:31 - loss: 3.1906 - accuracy: 0.18 - ETA: 3:25 - loss: 3.1844 - accuracy: 0.18 - ETA: 3:23 - loss: 3.1588 - accuracy: 0.17 - ETA: 3:20 - loss: 3.1967 - accuracy: 0.16 - ETA: 3:17 - loss: 3.1939 - accuracy: 0.16 - ETA: 3:15 - loss: 3.2155 - accuracy: 0.15 - ETA: 3:13 - loss: 3.2118 - accuracy: 0.15 - ETA: 3:12 - loss: 3.2141 - accuracy: 0.15 - ETA: 3:11 - loss: 3.2237 - accuracy: 0.15 - ETA: 3:11 - loss: 3.2167 - accuracy: 0.16 - ETA: 3:12 - loss: 3.2100 - accuracy: 0.16 - ETA: 3:11 - loss: 3.2127 - accuracy: 0.16 - ETA: 3:11 - loss: 3.2370 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2381 - accuracy: 0.15 - ETA: 3:09 - loss: 3.2391 - accuracy: 0.15 - ETA: 3:08 - loss: 3.2343 - accuracy: 0.15 - ETA: 3:07 - loss: 3.2355 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2361 - accuracy: 0.15 - ETA: 3:05 - loss: 3.2376 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2315 - accuracy: 0.15 - ETA: 3:03 - loss: 3.2306 - accuracy: 0.15 - ETA: 3:02 - loss: 3.2418 - accuracy: 0.15 - ETA: 3:01 - loss: 3.2398 - accuracy: 0.15 - ETA: 3:00 - loss: 3.2410 - accuracy: 0.15 - ETA: 2:59 - loss: 3.2352 - accuracy: 0.15 - ETA: 2:59 - loss: 3.2363 - accuracy: 0.15 - ETA: 2:58 - loss: 3.2329 - accuracy: 0.15 - ETA: 2:58 - loss: 3.2256 - accuracy: 0.15 - ETA: 2:58 - loss: 3.2249 - accuracy: 0.15 - ETA: 2:57 - loss: 3.2219 - accuracy: 0.16 - ETA: 2:57 - loss: 3.2170 - accuracy: 0.16 - ETA: 2:56 - loss: 3.2238 - accuracy: 0.16 - ETA: 2:55 - loss: 3.2278 - accuracy: 0.15 - ETA: 2:55 - loss: 3.2260 - accuracy: 0.16 - ETA: 2:54 - loss: 3.2306 - accuracy: 0.15 - ETA: 2:53 - loss: 3.2287 - accuracy: 0.15 - ETA: 2:53 - loss: 3.2260 - accuracy: 0.15 - ETA: 2:52 - loss: 3.2310 - accuracy: 0.15 - ETA: 2:51 - loss: 3.2343 - accuracy: 0.15 - ETA: 2:50 - loss: 3.2364 - accuracy: 0.15 - ETA: 2:50 - loss: 3.2370 - accuracy: 0.15 - ETA: 2:50 - loss: 3.2331 - accuracy: 0.15 - ETA: 2:49 - loss: 3.2362 - accuracy: 0.15 - ETA: 2:48 - loss: 3.2364 - accuracy: 0.15 - ETA: 2:48 - loss: 3.2389 - accuracy: 0.15 - ETA: 2:47 - loss: 3.2385 - accuracy: 0.15 - ETA: 2:46 - loss: 3.2396 - accuracy: 0.15 - ETA: 2:46 - loss: 3.2367 - accuracy: 0.15 - ETA: 2:45 - loss: 3.2329 - accuracy: 0.15 - ETA: 2:44 - loss: 3.2340 - accuracy: 0.15 - ETA: 2:44 - loss: 3.2334 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2321 - accuracy: 0.15 - ETA: 2:42 - loss: 3.2327 - accuracy: 0.15 - ETA: 2:42 - loss: 3.2288 - accuracy: 0.15 - ETA: 2:42 - loss: 3.2284 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2301 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2265 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2271 - accuracy: 0.16 - ETA: 2:40 - loss: 3.2284 - accuracy: 0.15 - ETA: 2:40 - loss: 3.2323 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2346 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2366 - accuracy: 0.15 - ETA: 2:38 - loss: 3.2376 - accuracy: 0.15 - ETA: 2:38 - loss: 3.2389 - accuracy: 0.15 - ETA: 2:37 - loss: 3.2372 - accuracy: 0.15 - ETA: 2:37 - loss: 3.2348 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2364 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2372 - accuracy: 0.15 - ETA: 2:35 - loss: 3.2374 - accuracy: 0.15 - ETA: 2:34 - loss: 3.2374 - accuracy: 0.15 - ETA: 2:34 - loss: 3.2369 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2367 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2350 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2342 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2347 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2338 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2330 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2340 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2335 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2326 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2335 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2324 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2313 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2322 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2328 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2293 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2289 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2265 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2273 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2254 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2254 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2267 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2275 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2289 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2288 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2297 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2302 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2294 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2297 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2295 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2297 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2305 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2313 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2320 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2309 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2288 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2299 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2303 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2310 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2290 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2293 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2278 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2282 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2274 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2266 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2269 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2281 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2271 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2275 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2276 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2282 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2290 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2294 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2274 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2270 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2279 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2286 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2300 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2316 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2312 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2289 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2287 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2287 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2305 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2311 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2316 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2332 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2332 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2342 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2343 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2336 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2319 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2317 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2317 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2325 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2319 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2316 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2316 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2316 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2324 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2328 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2334 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2341 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2334 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2337 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2342 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2338 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2336 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2336 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2345 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2339 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2341 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2346 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2347 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2358 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2361 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2356 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2359 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2355 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2341 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2340 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2347 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2334 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2332 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2334 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2338 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2340 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2345 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2338 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2340 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2349 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2343 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2333 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2328 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2325 - accuracy: 0.1566"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.2328 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2322 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2315 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2310 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2314 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2300 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2310 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2314 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2312 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2304 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2302 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2304 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2310 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2296 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2298 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2294 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2294 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2282 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2277 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2280 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2269 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2267 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2267 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2270 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2266 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2256 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2261 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2283 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2289 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2295 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2296 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2301 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2304 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2302 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2298 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2303 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2306 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2298 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2302 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2306 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2300 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2302 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2308 - accuracy: 0.15 - ETA: 59s - loss: 3.2309 - accuracy: 0.1584 - ETA: 58s - loss: 3.2306 - accuracy: 0.158 - ETA: 58s - loss: 3.2311 - accuracy: 0.158 - ETA: 57s - loss: 3.2317 - accuracy: 0.158 - ETA: 57s - loss: 3.2327 - accuracy: 0.157 - ETA: 56s - loss: 3.2333 - accuracy: 0.157 - ETA: 55s - loss: 3.2335 - accuracy: 0.157 - ETA: 55s - loss: 3.2344 - accuracy: 0.157 - ETA: 54s - loss: 3.2351 - accuracy: 0.157 - ETA: 54s - loss: 3.2357 - accuracy: 0.157 - ETA: 53s - loss: 3.2359 - accuracy: 0.157 - ETA: 53s - loss: 3.2363 - accuracy: 0.157 - ETA: 52s - loss: 3.2360 - accuracy: 0.157 - ETA: 51s - loss: 3.2361 - accuracy: 0.157 - ETA: 51s - loss: 3.2358 - accuracy: 0.157 - ETA: 50s - loss: 3.2363 - accuracy: 0.157 - ETA: 50s - loss: 3.2364 - accuracy: 0.156 - ETA: 49s - loss: 3.2368 - accuracy: 0.156 - ETA: 48s - loss: 3.2372 - accuracy: 0.156 - ETA: 48s - loss: 3.2379 - accuracy: 0.156 - ETA: 47s - loss: 3.2375 - accuracy: 0.156 - ETA: 47s - loss: 3.2372 - accuracy: 0.156 - ETA: 46s - loss: 3.2377 - accuracy: 0.156 - ETA: 46s - loss: 3.2381 - accuracy: 0.156 - ETA: 45s - loss: 3.2387 - accuracy: 0.156 - ETA: 44s - loss: 3.2389 - accuracy: 0.156 - ETA: 44s - loss: 3.2395 - accuracy: 0.156 - ETA: 43s - loss: 3.2397 - accuracy: 0.156 - ETA: 43s - loss: 3.2406 - accuracy: 0.156 - ETA: 42s - loss: 3.2402 - accuracy: 0.156 - ETA: 41s - loss: 3.2405 - accuracy: 0.156 - ETA: 41s - loss: 3.2402 - accuracy: 0.156 - ETA: 40s - loss: 3.2406 - accuracy: 0.156 - ETA: 40s - loss: 3.2411 - accuracy: 0.156 - ETA: 39s - loss: 3.2414 - accuracy: 0.156 - ETA: 38s - loss: 3.2423 - accuracy: 0.156 - ETA: 38s - loss: 3.2430 - accuracy: 0.156 - ETA: 37s - loss: 3.2431 - accuracy: 0.156 - ETA: 37s - loss: 3.2430 - accuracy: 0.156 - ETA: 36s - loss: 3.2428 - accuracy: 0.156 - ETA: 35s - loss: 3.2427 - accuracy: 0.156 - ETA: 35s - loss: 3.2432 - accuracy: 0.156 - ETA: 34s - loss: 3.2432 - accuracy: 0.155 - ETA: 34s - loss: 3.2437 - accuracy: 0.155 - ETA: 33s - loss: 3.2439 - accuracy: 0.155 - ETA: 33s - loss: 3.2442 - accuracy: 0.155 - ETA: 32s - loss: 3.2444 - accuracy: 0.155 - ETA: 31s - loss: 3.2451 - accuracy: 0.155 - ETA: 31s - loss: 3.2446 - accuracy: 0.155 - ETA: 30s - loss: 3.2444 - accuracy: 0.155 - ETA: 30s - loss: 3.2437 - accuracy: 0.156 - ETA: 29s - loss: 3.2432 - accuracy: 0.156 - ETA: 28s - loss: 3.2427 - accuracy: 0.156 - ETA: 28s - loss: 3.2431 - accuracy: 0.156 - ETA: 27s - loss: 3.2433 - accuracy: 0.156 - ETA: 27s - loss: 3.2435 - accuracy: 0.156 - ETA: 26s - loss: 3.2435 - accuracy: 0.156 - ETA: 25s - loss: 3.2440 - accuracy: 0.155 - ETA: 25s - loss: 3.2435 - accuracy: 0.155 - ETA: 24s - loss: 3.2442 - accuracy: 0.155 - ETA: 24s - loss: 3.2450 - accuracy: 0.155 - ETA: 23s - loss: 3.2449 - accuracy: 0.155 - ETA: 22s - loss: 3.2449 - accuracy: 0.155 - ETA: 22s - loss: 3.2452 - accuracy: 0.155 - ETA: 21s - loss: 3.2458 - accuracy: 0.155 - ETA: 21s - loss: 3.2458 - accuracy: 0.155 - ETA: 20s - loss: 3.2462 - accuracy: 0.155 - ETA: 20s - loss: 3.2466 - accuracy: 0.155 - ETA: 19s - loss: 3.2467 - accuracy: 0.154 - ETA: 18s - loss: 3.2472 - accuracy: 0.154 - ETA: 18s - loss: 3.2468 - accuracy: 0.154 - ETA: 17s - loss: 3.2469 - accuracy: 0.154 - ETA: 17s - loss: 3.2477 - accuracy: 0.154 - ETA: 16s - loss: 3.2480 - accuracy: 0.154 - ETA: 15s - loss: 3.2476 - accuracy: 0.154 - ETA: 15s - loss: 3.2481 - accuracy: 0.154 - ETA: 14s - loss: 3.2475 - accuracy: 0.154 - ETA: 14s - loss: 3.2474 - accuracy: 0.154 - ETA: 13s - loss: 3.2479 - accuracy: 0.154 - ETA: 12s - loss: 3.2482 - accuracy: 0.154 - ETA: 12s - loss: 3.2486 - accuracy: 0.154 - ETA: 11s - loss: 3.2481 - accuracy: 0.154 - ETA: 11s - loss: 3.2480 - accuracy: 0.154 - ETA: 10s - loss: 3.2478 - accuracy: 0.154 - ETA: 10s - loss: 3.2477 - accuracy: 0.154 - ETA: 9s - loss: 3.2472 - accuracy: 0.154 - ETA: 8s - loss: 3.2474 - accuracy: 0.15 - ETA: 8s - loss: 3.2473 - accuracy: 0.15 - ETA: 7s - loss: 3.2482 - accuracy: 0.15 - ETA: 7s - loss: 3.2481 - accuracy: 0.15 - ETA: 6s - loss: 3.2484 - accuracy: 0.15 - ETA: 5s - loss: 3.2481 - accuracy: 0.15 - ETA: 5s - loss: 3.2483 - accuracy: 0.15 - ETA: 4s - loss: 3.2488 - accuracy: 0.15 - ETA: 4s - loss: 3.2492 - accuracy: 0.15 - ETA: 3s - loss: 3.2493 - accuracy: 0.15 - ETA: 2s - loss: 3.2494 - accuracy: 0.15 - ETA: 2s - loss: 3.2493 - accuracy: 0.15 - ETA: 1s - loss: 3.2499 - accuracy: 0.15 - ETA: 1s - loss: 3.2500 - accuracy: 0.15 - ETA: 0s - loss: 3.2505 - accuracy: 0.15 - ETA: 0s - loss: 3.2504 - accuracy: 0.15 - 207s 5ms/step - loss: 3.2505 - accuracy: 0.1541 - val_loss: 3.9352 - val_accuracy: 0.0395\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:18 - loss: 3.1737 - accuracy: 0.18 - ETA: 3:12 - loss: 3.2235 - accuracy: 0.16 - ETA: 3:17 - loss: 3.1887 - accuracy: 0.17 - ETA: 3:20 - loss: 3.2225 - accuracy: 0.16 - ETA: 3:19 - loss: 3.2308 - accuracy: 0.17 - ETA: 3:20 - loss: 3.2617 - accuracy: 0.15 - ETA: 3:16 - loss: 3.2728 - accuracy: 0.15 - ETA: 3:15 - loss: 3.2633 - accuracy: 0.15 - ETA: 3:14 - loss: 3.2793 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2794 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2669 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2701 - accuracy: 0.15 - ETA: 3:09 - loss: 3.2662 - accuracy: 0.15 - ETA: 3:09 - loss: 3.2771 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2845 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2812 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2828 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2884 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2945 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2958 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2777 - accuracy: 0.15 - ETA: 3:01 - loss: 3.2739 - accuracy: 0.15 - ETA: 3:01 - loss: 3.2729 - accuracy: 0.15 - ETA: 3:00 - loss: 3.2674 - accuracy: 0.15 - ETA: 2:59 - loss: 3.2688 - accuracy: 0.15 - ETA: 2:59 - loss: 3.2687 - accuracy: 0.15 - ETA: 2:58 - loss: 3.2702 - accuracy: 0.15 - ETA: 2:57 - loss: 3.2687 - accuracy: 0.15 - ETA: 2:57 - loss: 3.2712 - accuracy: 0.15 - ETA: 2:56 - loss: 3.2688 - accuracy: 0.15 - ETA: 2:56 - loss: 3.2698 - accuracy: 0.15 - ETA: 2:56 - loss: 3.2672 - accuracy: 0.15 - ETA: 2:55 - loss: 3.2677 - accuracy: 0.15 - ETA: 2:55 - loss: 3.2611 - accuracy: 0.15 - ETA: 2:54 - loss: 3.2615 - accuracy: 0.15 - ETA: 2:53 - loss: 3.2659 - accuracy: 0.15 - ETA: 2:53 - loss: 3.2695 - accuracy: 0.15 - ETA: 2:52 - loss: 3.2673 - accuracy: 0.15 - ETA: 2:51 - loss: 3.2651 - accuracy: 0.15 - ETA: 2:51 - loss: 3.2622 - accuracy: 0.15 - ETA: 2:50 - loss: 3.2578 - accuracy: 0.15 - ETA: 2:49 - loss: 3.2554 - accuracy: 0.15 - ETA: 2:49 - loss: 3.2557 - accuracy: 0.15 - ETA: 2:48 - loss: 3.2549 - accuracy: 0.15 - ETA: 2:47 - loss: 3.2541 - accuracy: 0.15 - ETA: 2:47 - loss: 3.2540 - accuracy: 0.15 - ETA: 2:46 - loss: 3.2519 - accuracy: 0.15 - ETA: 2:45 - loss: 3.2527 - accuracy: 0.15 - ETA: 2:45 - loss: 3.2550 - accuracy: 0.15 - ETA: 2:44 - loss: 3.2525 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2540 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2554 - accuracy: 0.15 - ETA: 2:42 - loss: 3.2530 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2521 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2561 - accuracy: 0.15 - ETA: 2:40 - loss: 3.2550 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2533 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2557 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2539 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2535 - accuracy: 0.15 - ETA: 2:38 - loss: 3.2515 - accuracy: 0.15 - ETA: 2:37 - loss: 3.2522 - accuracy: 0.15 - ETA: 2:37 - loss: 3.2523 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2511 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2505 - accuracy: 0.15 - ETA: 2:35 - loss: 3.2476 - accuracy: 0.15 - ETA: 2:34 - loss: 3.2461 - accuracy: 0.15 - ETA: 2:34 - loss: 3.2460 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2444 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2422 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2397 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2416 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2403 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2413 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2411 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2408 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2410 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2394 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2396 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2403 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2397 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2399 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2392 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2373 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2400 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2404 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2401 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2397 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2399 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2421 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2410 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2412 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2409 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2419 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2416 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2404 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2390 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2387 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2385 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2378 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2372 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2367 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2376 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2368 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2358 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2382 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2387 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2406 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2417 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2413 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2425 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2401 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2399 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2390 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2390 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2373 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2358 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2360 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2356 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2364 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2364 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2378 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2370 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2375 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2356 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2358 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2354 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2358 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2354 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2347 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2351 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2362 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2364 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2369 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2375 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2366 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2363 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2362 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2360 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2358 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2347 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2334 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2333 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2337 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2331 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2327 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2336 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2329 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2349 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2353 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2347 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2352 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2350 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2340 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2345 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2358 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2366 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2363 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2358 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2363 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2367 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2368 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2363 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2352 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2360 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2358 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2354 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2348 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2351 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2351 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2339 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2335 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2322 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2321 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2308 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2309 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2304 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2297 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2294 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2297 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2297 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2301 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2296 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2292 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2292 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2301 - accuracy: 0.1568"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.2297 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2294 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2298 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2295 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2291 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2288 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2287 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2277 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2276 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2279 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2271 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2272 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2281 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2276 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2275 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2272 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2274 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2276 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2286 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2276 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2279 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2276 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2277 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2276 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2280 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2287 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2285 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2288 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2283 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2274 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2268 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2261 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2257 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2256 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2255 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2250 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2239 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2238 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2241 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2244 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2242 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2245 - accuracy: 0.15 - ETA: 59s - loss: 3.2241 - accuracy: 0.1569 - ETA: 59s - loss: 3.2233 - accuracy: 0.157 - ETA: 58s - loss: 3.2229 - accuracy: 0.157 - ETA: 58s - loss: 3.2228 - accuracy: 0.157 - ETA: 57s - loss: 3.2221 - accuracy: 0.157 - ETA: 56s - loss: 3.2223 - accuracy: 0.157 - ETA: 56s - loss: 3.2216 - accuracy: 0.157 - ETA: 55s - loss: 3.2204 - accuracy: 0.158 - ETA: 55s - loss: 3.2207 - accuracy: 0.157 - ETA: 54s - loss: 3.2199 - accuracy: 0.158 - ETA: 53s - loss: 3.2199 - accuracy: 0.158 - ETA: 53s - loss: 3.2192 - accuracy: 0.158 - ETA: 52s - loss: 3.2193 - accuracy: 0.158 - ETA: 52s - loss: 3.2194 - accuracy: 0.158 - ETA: 51s - loss: 3.2192 - accuracy: 0.158 - ETA: 50s - loss: 3.2190 - accuracy: 0.158 - ETA: 50s - loss: 3.2183 - accuracy: 0.158 - ETA: 49s - loss: 3.2178 - accuracy: 0.158 - ETA: 49s - loss: 3.2182 - accuracy: 0.158 - ETA: 48s - loss: 3.2178 - accuracy: 0.158 - ETA: 48s - loss: 3.2184 - accuracy: 0.158 - ETA: 47s - loss: 3.2189 - accuracy: 0.158 - ETA: 46s - loss: 3.2195 - accuracy: 0.158 - ETA: 46s - loss: 3.2193 - accuracy: 0.157 - ETA: 45s - loss: 3.2192 - accuracy: 0.157 - ETA: 45s - loss: 3.2189 - accuracy: 0.157 - ETA: 44s - loss: 3.2190 - accuracy: 0.157 - ETA: 43s - loss: 3.2197 - accuracy: 0.157 - ETA: 43s - loss: 3.2194 - accuracy: 0.157 - ETA: 42s - loss: 3.2198 - accuracy: 0.157 - ETA: 42s - loss: 3.2194 - accuracy: 0.158 - ETA: 41s - loss: 3.2196 - accuracy: 0.158 - ETA: 41s - loss: 3.2198 - accuracy: 0.157 - ETA: 40s - loss: 3.2206 - accuracy: 0.157 - ETA: 39s - loss: 3.2208 - accuracy: 0.157 - ETA: 39s - loss: 3.2212 - accuracy: 0.157 - ETA: 38s - loss: 3.2208 - accuracy: 0.157 - ETA: 38s - loss: 3.2210 - accuracy: 0.157 - ETA: 37s - loss: 3.2213 - accuracy: 0.157 - ETA: 36s - loss: 3.2213 - accuracy: 0.157 - ETA: 36s - loss: 3.2210 - accuracy: 0.157 - ETA: 35s - loss: 3.2207 - accuracy: 0.157 - ETA: 35s - loss: 3.2210 - accuracy: 0.157 - ETA: 34s - loss: 3.2211 - accuracy: 0.157 - ETA: 33s - loss: 3.2210 - accuracy: 0.157 - ETA: 33s - loss: 3.2211 - accuracy: 0.157 - ETA: 32s - loss: 3.2213 - accuracy: 0.157 - ETA: 32s - loss: 3.2203 - accuracy: 0.157 - ETA: 31s - loss: 3.2206 - accuracy: 0.157 - ETA: 31s - loss: 3.2205 - accuracy: 0.157 - ETA: 30s - loss: 3.2205 - accuracy: 0.157 - ETA: 29s - loss: 3.2197 - accuracy: 0.157 - ETA: 29s - loss: 3.2201 - accuracy: 0.157 - ETA: 28s - loss: 3.2202 - accuracy: 0.157 - ETA: 28s - loss: 3.2195 - accuracy: 0.157 - ETA: 27s - loss: 3.2193 - accuracy: 0.157 - ETA: 26s - loss: 3.2193 - accuracy: 0.157 - ETA: 26s - loss: 3.2194 - accuracy: 0.157 - ETA: 25s - loss: 3.2193 - accuracy: 0.157 - ETA: 25s - loss: 3.2194 - accuracy: 0.157 - ETA: 24s - loss: 3.2193 - accuracy: 0.157 - ETA: 24s - loss: 3.2191 - accuracy: 0.157 - ETA: 23s - loss: 3.2187 - accuracy: 0.157 - ETA: 22s - loss: 3.2185 - accuracy: 0.158 - ETA: 22s - loss: 3.2186 - accuracy: 0.157 - ETA: 21s - loss: 3.2186 - accuracy: 0.158 - ETA: 21s - loss: 3.2189 - accuracy: 0.157 - ETA: 20s - loss: 3.2183 - accuracy: 0.158 - ETA: 19s - loss: 3.2178 - accuracy: 0.158 - ETA: 19s - loss: 3.2177 - accuracy: 0.158 - ETA: 18s - loss: 3.2174 - accuracy: 0.158 - ETA: 18s - loss: 3.2176 - accuracy: 0.158 - ETA: 17s - loss: 3.2177 - accuracy: 0.158 - ETA: 17s - loss: 3.2178 - accuracy: 0.157 - ETA: 16s - loss: 3.2179 - accuracy: 0.158 - ETA: 15s - loss: 3.2175 - accuracy: 0.157 - ETA: 15s - loss: 3.2171 - accuracy: 0.158 - ETA: 14s - loss: 3.2165 - accuracy: 0.158 - ETA: 14s - loss: 3.2167 - accuracy: 0.158 - ETA: 13s - loss: 3.2172 - accuracy: 0.157 - ETA: 12s - loss: 3.2178 - accuracy: 0.157 - ETA: 12s - loss: 3.2174 - accuracy: 0.157 - ETA: 11s - loss: 3.2167 - accuracy: 0.158 - ETA: 11s - loss: 3.2160 - accuracy: 0.158 - ETA: 10s - loss: 3.2158 - accuracy: 0.158 - ETA: 10s - loss: 3.2168 - accuracy: 0.158 - ETA: 9s - loss: 3.2170 - accuracy: 0.158 - ETA: 8s - loss: 3.2177 - accuracy: 0.15 - ETA: 8s - loss: 3.2178 - accuracy: 0.15 - ETA: 7s - loss: 3.2175 - accuracy: 0.15 - ETA: 7s - loss: 3.2175 - accuracy: 0.15 - ETA: 6s - loss: 3.2175 - accuracy: 0.15 - ETA: 5s - loss: 3.2175 - accuracy: 0.15 - ETA: 5s - loss: 3.2180 - accuracy: 0.15 - ETA: 4s - loss: 3.2180 - accuracy: 0.15 - ETA: 4s - loss: 3.2181 - accuracy: 0.15 - ETA: 3s - loss: 3.2181 - accuracy: 0.15 - ETA: 2s - loss: 3.2179 - accuracy: 0.15 - ETA: 2s - loss: 3.2179 - accuracy: 0.15 - ETA: 1s - loss: 3.2182 - accuracy: 0.15 - ETA: 1s - loss: 3.2182 - accuracy: 0.15 - ETA: 0s - loss: 3.2185 - accuracy: 0.15 - ETA: 0s - loss: 3.2184 - accuracy: 0.15 - 206s 5ms/step - loss: 3.2186 - accuracy: 0.1573 - val_loss: 3.8875 - val_accuracy: 0.0398\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:21 - loss: 3.1874 - accuracy: 0.20 - ETA: 3:14 - loss: 3.2488 - accuracy: 0.17 - ETA: 3:11 - loss: 3.2950 - accuracy: 0.16 - ETA: 3:11 - loss: 3.2757 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2173 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2212 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2055 - accuracy: 0.15 - ETA: 3:11 - loss: 3.2085 - accuracy: 0.15 - ETA: 3:12 - loss: 3.1978 - accuracy: 0.15 - ETA: 3:11 - loss: 3.1774 - accuracy: 0.16 - ETA: 3:10 - loss: 3.2093 - accuracy: 0.16 - ETA: 3:09 - loss: 3.2165 - accuracy: 0.15 - ETA: 3:08 - loss: 3.2189 - accuracy: 0.15 - ETA: 3:07 - loss: 3.2141 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2025 - accuracy: 0.16 - ETA: 3:05 - loss: 3.1916 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1926 - accuracy: 0.16 - ETA: 3:03 - loss: 3.1827 - accuracy: 0.16 - ETA: 3:03 - loss: 3.1856 - accuracy: 0.16 - ETA: 3:02 - loss: 3.1880 - accuracy: 0.16 - ETA: 3:02 - loss: 3.1807 - accuracy: 0.16 - ETA: 3:01 - loss: 3.1702 - accuracy: 0.16 - ETA: 3:00 - loss: 3.1759 - accuracy: 0.16 - ETA: 2:59 - loss: 3.1814 - accuracy: 0.16 - ETA: 2:58 - loss: 3.1796 - accuracy: 0.16 - ETA: 2:58 - loss: 3.1757 - accuracy: 0.16 - ETA: 2:57 - loss: 3.1728 - accuracy: 0.16 - ETA: 2:56 - loss: 3.1691 - accuracy: 0.16 - ETA: 2:56 - loss: 3.1757 - accuracy: 0.16 - ETA: 2:55 - loss: 3.1750 - accuracy: 0.16 - ETA: 2:54 - loss: 3.1800 - accuracy: 0.16 - ETA: 2:54 - loss: 3.1792 - accuracy: 0.16 - ETA: 2:53 - loss: 3.1755 - accuracy: 0.16 - ETA: 2:53 - loss: 3.1724 - accuracy: 0.16 - ETA: 2:52 - loss: 3.1748 - accuracy: 0.16 - ETA: 2:52 - loss: 3.1766 - accuracy: 0.16 - ETA: 2:52 - loss: 3.1787 - accuracy: 0.16 - ETA: 2:51 - loss: 3.1819 - accuracy: 0.15 - ETA: 2:51 - loss: 3.1813 - accuracy: 0.15 - ETA: 2:50 - loss: 3.1845 - accuracy: 0.15 - ETA: 2:49 - loss: 3.1859 - accuracy: 0.15 - ETA: 2:48 - loss: 3.1853 - accuracy: 0.15 - ETA: 2:48 - loss: 3.1837 - accuracy: 0.15 - ETA: 2:47 - loss: 3.1842 - accuracy: 0.15 - ETA: 2:47 - loss: 3.1891 - accuracy: 0.15 - ETA: 2:46 - loss: 3.1874 - accuracy: 0.15 - ETA: 2:46 - loss: 3.1903 - accuracy: 0.15 - ETA: 2:45 - loss: 3.1863 - accuracy: 0.15 - ETA: 2:44 - loss: 3.1844 - accuracy: 0.15 - ETA: 2:43 - loss: 3.1837 - accuracy: 0.15 - ETA: 2:43 - loss: 3.1836 - accuracy: 0.15 - ETA: 2:42 - loss: 3.1795 - accuracy: 0.15 - ETA: 2:42 - loss: 3.1798 - accuracy: 0.15 - ETA: 2:41 - loss: 3.1778 - accuracy: 0.15 - ETA: 2:40 - loss: 3.1757 - accuracy: 0.15 - ETA: 2:40 - loss: 3.1768 - accuracy: 0.15 - ETA: 2:39 - loss: 3.1778 - accuracy: 0.15 - ETA: 2:39 - loss: 3.1780 - accuracy: 0.15 - ETA: 2:38 - loss: 3.1755 - accuracy: 0.16 - ETA: 2:38 - loss: 3.1775 - accuracy: 0.16 - ETA: 2:37 - loss: 3.1757 - accuracy: 0.16 - ETA: 2:37 - loss: 3.1783 - accuracy: 0.16 - ETA: 2:36 - loss: 3.1757 - accuracy: 0.16 - ETA: 2:36 - loss: 3.1765 - accuracy: 0.16 - ETA: 2:35 - loss: 3.1766 - accuracy: 0.16 - ETA: 2:34 - loss: 3.1772 - accuracy: 0.16 - ETA: 2:34 - loss: 3.1794 - accuracy: 0.15 - ETA: 2:33 - loss: 3.1812 - accuracy: 0.15 - ETA: 2:33 - loss: 3.1819 - accuracy: 0.15 - ETA: 2:32 - loss: 3.1817 - accuracy: 0.15 - ETA: 2:31 - loss: 3.1790 - accuracy: 0.16 - ETA: 2:31 - loss: 3.1800 - accuracy: 0.16 - ETA: 2:30 - loss: 3.1788 - accuracy: 0.16 - ETA: 2:30 - loss: 3.1795 - accuracy: 0.16 - ETA: 2:29 - loss: 3.1821 - accuracy: 0.16 - ETA: 2:28 - loss: 3.1800 - accuracy: 0.16 - ETA: 2:28 - loss: 3.1799 - accuracy: 0.16 - ETA: 2:27 - loss: 3.1790 - accuracy: 0.16 - ETA: 2:27 - loss: 3.1795 - accuracy: 0.16 - ETA: 2:26 - loss: 3.1794 - accuracy: 0.16 - ETA: 2:25 - loss: 3.1795 - accuracy: 0.15 - ETA: 2:25 - loss: 3.1785 - accuracy: 0.15 - ETA: 2:24 - loss: 3.1818 - accuracy: 0.15 - ETA: 2:24 - loss: 3.1824 - accuracy: 0.15 - ETA: 2:23 - loss: 3.1838 - accuracy: 0.15 - ETA: 2:22 - loss: 3.1837 - accuracy: 0.15 - ETA: 2:22 - loss: 3.1840 - accuracy: 0.15 - ETA: 2:21 - loss: 3.1844 - accuracy: 0.15 - ETA: 2:21 - loss: 3.1835 - accuracy: 0.15 - ETA: 2:20 - loss: 3.1851 - accuracy: 0.15 - ETA: 2:20 - loss: 3.1836 - accuracy: 0.15 - ETA: 2:19 - loss: 3.1830 - accuracy: 0.15 - ETA: 2:19 - loss: 3.1840 - accuracy: 0.15 - ETA: 2:18 - loss: 3.1842 - accuracy: 0.15 - ETA: 2:18 - loss: 3.1855 - accuracy: 0.16 - ETA: 2:17 - loss: 3.1853 - accuracy: 0.16 - ETA: 2:17 - loss: 3.1852 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1857 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1861 - accuracy: 0.16 - ETA: 2:15 - loss: 3.1838 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1845 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1842 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1839 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1815 - accuracy: 0.16 - ETA: 2:12 - loss: 3.1837 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1849 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1860 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1850 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1847 - accuracy: 0.16 - ETA: 2:09 - loss: 3.1849 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1830 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1830 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1818 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1825 - accuracy: 0.16 - ETA: 2:06 - loss: 3.1831 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1854 - accuracy: 0.15 - ETA: 2:05 - loss: 3.1842 - accuracy: 0.15 - ETA: 2:04 - loss: 3.1850 - accuracy: 0.15 - ETA: 2:04 - loss: 3.1865 - accuracy: 0.15 - ETA: 2:03 - loss: 3.1856 - accuracy: 0.15 - ETA: 2:03 - loss: 3.1843 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1841 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1841 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1843 - accuracy: 0.15 - ETA: 2:00 - loss: 3.1841 - accuracy: 0.15 - ETA: 2:00 - loss: 3.1836 - accuracy: 0.15 - ETA: 1:59 - loss: 3.1845 - accuracy: 0.15 - ETA: 1:58 - loss: 3.1858 - accuracy: 0.15 - ETA: 1:58 - loss: 3.1850 - accuracy: 0.15 - ETA: 1:57 - loss: 3.1844 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1848 - accuracy: 0.15 - ETA: 1:56 - loss: 3.1867 - accuracy: 0.15 - ETA: 1:55 - loss: 3.1866 - accuracy: 0.15 - ETA: 1:55 - loss: 3.1882 - accuracy: 0.15 - ETA: 1:54 - loss: 3.1891 - accuracy: 0.15 - ETA: 1:54 - loss: 3.1882 - accuracy: 0.15 - ETA: 1:53 - loss: 3.1879 - accuracy: 0.15 - ETA: 1:52 - loss: 3.1865 - accuracy: 0.15 - ETA: 1:52 - loss: 3.1857 - accuracy: 0.15 - ETA: 1:51 - loss: 3.1863 - accuracy: 0.15 - ETA: 1:51 - loss: 3.1863 - accuracy: 0.15 - ETA: 1:50 - loss: 3.1862 - accuracy: 0.15 - ETA: 1:50 - loss: 3.1854 - accuracy: 0.15 - ETA: 1:49 - loss: 3.1859 - accuracy: 0.15 - ETA: 1:49 - loss: 3.1866 - accuracy: 0.15 - ETA: 1:48 - loss: 3.1854 - accuracy: 0.15 - ETA: 1:47 - loss: 3.1855 - accuracy: 0.15 - ETA: 1:47 - loss: 3.1849 - accuracy: 0.15 - ETA: 1:46 - loss: 3.1847 - accuracy: 0.15 - ETA: 1:46 - loss: 3.1854 - accuracy: 0.15 - ETA: 1:45 - loss: 3.1870 - accuracy: 0.15 - ETA: 1:44 - loss: 3.1869 - accuracy: 0.15 - ETA: 1:44 - loss: 3.1853 - accuracy: 0.15 - ETA: 1:43 - loss: 3.1840 - accuracy: 0.15 - ETA: 1:43 - loss: 3.1845 - accuracy: 0.15 - ETA: 1:42 - loss: 3.1857 - accuracy: 0.15 - ETA: 1:42 - loss: 3.1845 - accuracy: 0.15 - ETA: 1:41 - loss: 3.1845 - accuracy: 0.15 - ETA: 1:40 - loss: 3.1855 - accuracy: 0.15 - ETA: 1:40 - loss: 3.1852 - accuracy: 0.15 - ETA: 1:39 - loss: 3.1850 - accuracy: 0.15 - ETA: 1:39 - loss: 3.1853 - accuracy: 0.15 - ETA: 1:38 - loss: 3.1855 - accuracy: 0.15 - ETA: 1:37 - loss: 3.1853 - accuracy: 0.15 - ETA: 1:37 - loss: 3.1860 - accuracy: 0.15 - ETA: 1:36 - loss: 3.1859 - accuracy: 0.15 - ETA: 1:36 - loss: 3.1856 - accuracy: 0.15 - ETA: 1:35 - loss: 3.1852 - accuracy: 0.15 - ETA: 1:34 - loss: 3.1862 - accuracy: 0.15 - ETA: 1:34 - loss: 3.1851 - accuracy: 0.15 - ETA: 1:33 - loss: 3.1850 - accuracy: 0.15 - ETA: 1:33 - loss: 3.1843 - accuracy: 0.15 - ETA: 1:32 - loss: 3.1848 - accuracy: 0.15 - ETA: 1:32 - loss: 3.1850 - accuracy: 0.15 - ETA: 1:31 - loss: 3.1844 - accuracy: 0.15 - ETA: 1:30 - loss: 3.1846 - accuracy: 0.15 - ETA: 1:30 - loss: 3.1845 - accuracy: 0.15 - ETA: 1:29 - loss: 3.1843 - accuracy: 0.15 - ETA: 1:29 - loss: 3.1851 - accuracy: 0.15 - ETA: 1:28 - loss: 3.1848 - accuracy: 0.15 - ETA: 1:27 - loss: 3.1844 - accuracy: 0.15 - ETA: 1:27 - loss: 3.1834 - accuracy: 0.15 - ETA: 1:26 - loss: 3.1826 - accuracy: 0.16 - ETA: 1:26 - loss: 3.1822 - accuracy: 0.16 - ETA: 1:25 - loss: 3.1832 - accuracy: 0.16 - ETA: 1:24 - loss: 3.1829 - accuracy: 0.1600"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1828 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1814 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1824 - accuracy: 0.16 - ETA: 1:22 - loss: 3.1812 - accuracy: 0.16 - ETA: 1:22 - loss: 3.1812 - accuracy: 0.16 - ETA: 1:21 - loss: 3.1803 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1818 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1827 - accuracy: 0.16 - ETA: 1:19 - loss: 3.1824 - accuracy: 0.16 - ETA: 1:19 - loss: 3.1834 - accuracy: 0.16 - ETA: 1:18 - loss: 3.1828 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1821 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1830 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1834 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1831 - accuracy: 0.16 - ETA: 1:15 - loss: 3.1836 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1839 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1840 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1834 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1844 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1838 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1839 - accuracy: 0.16 - ETA: 1:11 - loss: 3.1831 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1838 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1834 - accuracy: 0.16 - ETA: 1:09 - loss: 3.1843 - accuracy: 0.16 - ETA: 1:09 - loss: 3.1840 - accuracy: 0.16 - ETA: 1:08 - loss: 3.1838 - accuracy: 0.16 - ETA: 1:07 - loss: 3.1840 - accuracy: 0.16 - ETA: 1:07 - loss: 3.1844 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1849 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1855 - accuracy: 0.16 - ETA: 1:05 - loss: 3.1864 - accuracy: 0.16 - ETA: 1:05 - loss: 3.1874 - accuracy: 0.16 - ETA: 1:04 - loss: 3.1867 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1868 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1873 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1876 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1865 - accuracy: 0.16 - ETA: 1:01 - loss: 3.1868 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1866 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1863 - accuracy: 0.16 - ETA: 59s - loss: 3.1861 - accuracy: 0.1613 - ETA: 59s - loss: 3.1867 - accuracy: 0.161 - ETA: 58s - loss: 3.1874 - accuracy: 0.160 - ETA: 58s - loss: 3.1869 - accuracy: 0.160 - ETA: 57s - loss: 3.1871 - accuracy: 0.160 - ETA: 56s - loss: 3.1871 - accuracy: 0.160 - ETA: 56s - loss: 3.1864 - accuracy: 0.160 - ETA: 55s - loss: 3.1864 - accuracy: 0.160 - ETA: 55s - loss: 3.1862 - accuracy: 0.160 - ETA: 54s - loss: 3.1862 - accuracy: 0.160 - ETA: 53s - loss: 3.1858 - accuracy: 0.160 - ETA: 53s - loss: 3.1854 - accuracy: 0.160 - ETA: 52s - loss: 3.1856 - accuracy: 0.160 - ETA: 52s - loss: 3.1855 - accuracy: 0.160 - ETA: 51s - loss: 3.1851 - accuracy: 0.160 - ETA: 50s - loss: 3.1857 - accuracy: 0.160 - ETA: 50s - loss: 3.1850 - accuracy: 0.160 - ETA: 49s - loss: 3.1847 - accuracy: 0.160 - ETA: 49s - loss: 3.1849 - accuracy: 0.160 - ETA: 48s - loss: 3.1843 - accuracy: 0.160 - ETA: 48s - loss: 3.1845 - accuracy: 0.161 - ETA: 47s - loss: 3.1846 - accuracy: 0.161 - ETA: 46s - loss: 3.1841 - accuracy: 0.160 - ETA: 46s - loss: 3.1843 - accuracy: 0.161 - ETA: 45s - loss: 3.1848 - accuracy: 0.161 - ETA: 45s - loss: 3.1849 - accuracy: 0.161 - ETA: 44s - loss: 3.1844 - accuracy: 0.161 - ETA: 43s - loss: 3.1838 - accuracy: 0.161 - ETA: 43s - loss: 3.1836 - accuracy: 0.161 - ETA: 42s - loss: 3.1843 - accuracy: 0.161 - ETA: 42s - loss: 3.1836 - accuracy: 0.161 - ETA: 41s - loss: 3.1833 - accuracy: 0.161 - ETA: 41s - loss: 3.1831 - accuracy: 0.161 - ETA: 40s - loss: 3.1832 - accuracy: 0.161 - ETA: 39s - loss: 3.1830 - accuracy: 0.161 - ETA: 39s - loss: 3.1830 - accuracy: 0.161 - ETA: 38s - loss: 3.1827 - accuracy: 0.161 - ETA: 38s - loss: 3.1830 - accuracy: 0.161 - ETA: 37s - loss: 3.1822 - accuracy: 0.161 - ETA: 36s - loss: 3.1822 - accuracy: 0.161 - ETA: 36s - loss: 3.1830 - accuracy: 0.161 - ETA: 35s - loss: 3.1839 - accuracy: 0.160 - ETA: 35s - loss: 3.1841 - accuracy: 0.160 - ETA: 34s - loss: 3.1841 - accuracy: 0.160 - ETA: 33s - loss: 3.1846 - accuracy: 0.160 - ETA: 33s - loss: 3.1844 - accuracy: 0.160 - ETA: 32s - loss: 3.1849 - accuracy: 0.160 - ETA: 32s - loss: 3.1849 - accuracy: 0.160 - ETA: 31s - loss: 3.1855 - accuracy: 0.160 - ETA: 31s - loss: 3.1856 - accuracy: 0.160 - ETA: 30s - loss: 3.1851 - accuracy: 0.160 - ETA: 29s - loss: 3.1851 - accuracy: 0.160 - ETA: 29s - loss: 3.1849 - accuracy: 0.160 - ETA: 28s - loss: 3.1854 - accuracy: 0.160 - ETA: 28s - loss: 3.1857 - accuracy: 0.160 - ETA: 27s - loss: 3.1849 - accuracy: 0.161 - ETA: 26s - loss: 3.1846 - accuracy: 0.161 - ETA: 26s - loss: 3.1842 - accuracy: 0.161 - ETA: 25s - loss: 3.1842 - accuracy: 0.161 - ETA: 25s - loss: 3.1839 - accuracy: 0.161 - ETA: 24s - loss: 3.1836 - accuracy: 0.161 - ETA: 24s - loss: 3.1842 - accuracy: 0.161 - ETA: 23s - loss: 3.1847 - accuracy: 0.161 - ETA: 22s - loss: 3.1851 - accuracy: 0.161 - ETA: 22s - loss: 3.1853 - accuracy: 0.160 - ETA: 21s - loss: 3.1854 - accuracy: 0.160 - ETA: 21s - loss: 3.1856 - accuracy: 0.160 - ETA: 20s - loss: 3.1851 - accuracy: 0.161 - ETA: 19s - loss: 3.1852 - accuracy: 0.161 - ETA: 19s - loss: 3.1848 - accuracy: 0.161 - ETA: 18s - loss: 3.1853 - accuracy: 0.161 - ETA: 18s - loss: 3.1854 - accuracy: 0.161 - ETA: 17s - loss: 3.1859 - accuracy: 0.161 - ETA: 17s - loss: 3.1862 - accuracy: 0.161 - ETA: 16s - loss: 3.1856 - accuracy: 0.161 - ETA: 15s - loss: 3.1854 - accuracy: 0.161 - ETA: 15s - loss: 3.1863 - accuracy: 0.161 - ETA: 14s - loss: 3.1862 - accuracy: 0.161 - ETA: 14s - loss: 3.1853 - accuracy: 0.161 - ETA: 13s - loss: 3.1849 - accuracy: 0.161 - ETA: 12s - loss: 3.1855 - accuracy: 0.161 - ETA: 12s - loss: 3.1848 - accuracy: 0.161 - ETA: 11s - loss: 3.1848 - accuracy: 0.161 - ETA: 11s - loss: 3.1847 - accuracy: 0.161 - ETA: 10s - loss: 3.1839 - accuracy: 0.161 - ETA: 9s - loss: 3.1840 - accuracy: 0.161 - ETA: 9s - loss: 3.1851 - accuracy: 0.16 - ETA: 8s - loss: 3.1854 - accuracy: 0.16 - ETA: 8s - loss: 3.1854 - accuracy: 0.16 - ETA: 7s - loss: 3.1862 - accuracy: 0.16 - ETA: 7s - loss: 3.1866 - accuracy: 0.16 - ETA: 6s - loss: 3.1863 - accuracy: 0.16 - ETA: 5s - loss: 3.1865 - accuracy: 0.16 - ETA: 5s - loss: 3.1861 - accuracy: 0.16 - ETA: 4s - loss: 3.1857 - accuracy: 0.16 - ETA: 4s - loss: 3.1860 - accuracy: 0.16 - ETA: 3s - loss: 3.1866 - accuracy: 0.16 - ETA: 2s - loss: 3.1858 - accuracy: 0.16 - ETA: 2s - loss: 3.1861 - accuracy: 0.16 - ETA: 1s - loss: 3.1861 - accuracy: 0.16 - ETA: 1s - loss: 3.1853 - accuracy: 0.16 - ETA: 0s - loss: 3.1854 - accuracy: 0.16 - ETA: 0s - loss: 3.1852 - accuracy: 0.16 - 205s 5ms/step - loss: 3.1852 - accuracy: 0.1612 - val_loss: 3.8881 - val_accuracy: 0.0445\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:11 - loss: 3.1326 - accuracy: 0.15 - ETA: 3:07 - loss: 3.1805 - accuracy: 0.16 - ETA: 3:09 - loss: 3.2214 - accuracy: 0.15 - ETA: 3:07 - loss: 3.1844 - accuracy: 0.16 - ETA: 3:07 - loss: 3.1506 - accuracy: 0.16 - ETA: 3:06 - loss: 3.1478 - accuracy: 0.16 - ETA: 3:05 - loss: 3.1645 - accuracy: 0.15 - ETA: 3:05 - loss: 3.1706 - accuracy: 0.15 - ETA: 3:04 - loss: 3.1636 - accuracy: 0.15 - ETA: 3:04 - loss: 3.1771 - accuracy: 0.15 - ETA: 3:06 - loss: 3.1668 - accuracy: 0.15 - ETA: 3:07 - loss: 3.1712 - accuracy: 0.15 - ETA: 3:07 - loss: 3.1768 - accuracy: 0.15 - ETA: 3:07 - loss: 3.1786 - accuracy: 0.15 - ETA: 3:07 - loss: 3.1714 - accuracy: 0.15 - ETA: 3:06 - loss: 3.1712 - accuracy: 0.15 - ETA: 3:05 - loss: 3.1784 - accuracy: 0.15 - ETA: 3:04 - loss: 3.1843 - accuracy: 0.15 - ETA: 3:03 - loss: 3.1869 - accuracy: 0.15 - ETA: 3:03 - loss: 3.1782 - accuracy: 0.15 - ETA: 3:02 - loss: 3.1638 - accuracy: 0.15 - ETA: 3:01 - loss: 3.1678 - accuracy: 0.15 - ETA: 3:00 - loss: 3.1647 - accuracy: 0.15 - ETA: 3:00 - loss: 3.1676 - accuracy: 0.15 - ETA: 2:59 - loss: 3.1649 - accuracy: 0.15 - ETA: 2:58 - loss: 3.1604 - accuracy: 0.15 - ETA: 2:58 - loss: 3.1638 - accuracy: 0.15 - ETA: 2:57 - loss: 3.1654 - accuracy: 0.15 - ETA: 2:56 - loss: 3.1666 - accuracy: 0.15 - ETA: 2:56 - loss: 3.1654 - accuracy: 0.15 - ETA: 2:55 - loss: 3.1709 - accuracy: 0.15 - ETA: 2:54 - loss: 3.1725 - accuracy: 0.15 - ETA: 2:54 - loss: 3.1734 - accuracy: 0.15 - ETA: 2:53 - loss: 3.1744 - accuracy: 0.15 - ETA: 2:52 - loss: 3.1760 - accuracy: 0.15 - ETA: 2:52 - loss: 3.1740 - accuracy: 0.15 - ETA: 2:51 - loss: 3.1724 - accuracy: 0.15 - ETA: 2:50 - loss: 3.1730 - accuracy: 0.15 - ETA: 2:50 - loss: 3.1706 - accuracy: 0.15 - ETA: 2:50 - loss: 3.1738 - accuracy: 0.15 - ETA: 2:49 - loss: 3.1710 - accuracy: 0.15 - ETA: 2:49 - loss: 3.1689 - accuracy: 0.15 - ETA: 2:48 - loss: 3.1687 - accuracy: 0.15 - ETA: 2:47 - loss: 3.1686 - accuracy: 0.15 - ETA: 2:47 - loss: 3.1710 - accuracy: 0.15 - ETA: 2:46 - loss: 3.1734 - accuracy: 0.15 - ETA: 2:46 - loss: 3.1695 - accuracy: 0.15 - ETA: 2:45 - loss: 3.1713 - accuracy: 0.15 - ETA: 2:45 - loss: 3.1687 - accuracy: 0.15 - ETA: 2:44 - loss: 3.1628 - accuracy: 0.15 - ETA: 2:43 - loss: 3.1615 - accuracy: 0.15 - ETA: 2:43 - loss: 3.1602 - accuracy: 0.15 - ETA: 2:42 - loss: 3.1650 - accuracy: 0.15 - ETA: 2:41 - loss: 3.1654 - accuracy: 0.15 - ETA: 2:41 - loss: 3.1658 - accuracy: 0.15 - ETA: 2:40 - loss: 3.1682 - accuracy: 0.15 - ETA: 2:39 - loss: 3.1731 - accuracy: 0.15 - ETA: 2:39 - loss: 3.1729 - accuracy: 0.15 - ETA: 2:38 - loss: 3.1736 - accuracy: 0.15 - ETA: 2:38 - loss: 3.1741 - accuracy: 0.15 - ETA: 2:37 - loss: 3.1758 - accuracy: 0.15 - ETA: 2:36 - loss: 3.1752 - accuracy: 0.15 - ETA: 2:36 - loss: 3.1749 - accuracy: 0.15 - ETA: 2:35 - loss: 3.1747 - accuracy: 0.15 - ETA: 2:34 - loss: 3.1752 - accuracy: 0.15 - ETA: 2:34 - loss: 3.1736 - accuracy: 0.15 - ETA: 2:34 - loss: 3.1751 - accuracy: 0.15 - ETA: 2:33 - loss: 3.1736 - accuracy: 0.15 - ETA: 2:33 - loss: 3.1723 - accuracy: 0.15 - ETA: 2:32 - loss: 3.1725 - accuracy: 0.15 - ETA: 2:31 - loss: 3.1720 - accuracy: 0.15 - ETA: 2:31 - loss: 3.1711 - accuracy: 0.15 - ETA: 2:30 - loss: 3.1710 - accuracy: 0.15 - ETA: 2:30 - loss: 3.1722 - accuracy: 0.15 - ETA: 2:29 - loss: 3.1708 - accuracy: 0.15 - ETA: 2:28 - loss: 3.1721 - accuracy: 0.15 - ETA: 2:28 - loss: 3.1719 - accuracy: 0.15 - ETA: 2:27 - loss: 3.1737 - accuracy: 0.15 - ETA: 2:27 - loss: 3.1742 - accuracy: 0.15 - ETA: 2:26 - loss: 3.1750 - accuracy: 0.15 - ETA: 2:25 - loss: 3.1753 - accuracy: 0.15 - ETA: 2:25 - loss: 3.1759 - accuracy: 0.15 - ETA: 2:24 - loss: 3.1746 - accuracy: 0.15 - ETA: 2:24 - loss: 3.1744 - accuracy: 0.15 - ETA: 2:23 - loss: 3.1755 - accuracy: 0.15 - ETA: 2:23 - loss: 3.1754 - accuracy: 0.15 - ETA: 2:22 - loss: 3.1760 - accuracy: 0.15 - ETA: 2:22 - loss: 3.1741 - accuracy: 0.15 - ETA: 2:21 - loss: 3.1731 - accuracy: 0.15 - ETA: 2:20 - loss: 3.1716 - accuracy: 0.15 - ETA: 2:20 - loss: 3.1738 - accuracy: 0.15 - ETA: 2:19 - loss: 3.1730 - accuracy: 0.15 - ETA: 2:19 - loss: 3.1718 - accuracy: 0.15 - ETA: 2:18 - loss: 3.1707 - accuracy: 0.15 - ETA: 2:18 - loss: 3.1688 - accuracy: 0.15 - ETA: 2:17 - loss: 3.1663 - accuracy: 0.15 - ETA: 2:17 - loss: 3.1653 - accuracy: 0.15 - ETA: 2:16 - loss: 3.1648 - accuracy: 0.15 - ETA: 2:15 - loss: 3.1634 - accuracy: 0.16 - ETA: 2:15 - loss: 3.1639 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1634 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1621 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1590 - accuracy: 0.16 - ETA: 2:12 - loss: 3.1573 - accuracy: 0.16 - ETA: 2:12 - loss: 3.1563 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1560 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1571 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1561 - accuracy: 0.16 - ETA: 2:09 - loss: 3.1576 - accuracy: 0.16 - ETA: 2:09 - loss: 3.1577 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1569 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1555 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1556 - accuracy: 0.16 - ETA: 2:06 - loss: 3.1550 - accuracy: 0.16 - ETA: 2:06 - loss: 3.1547 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1549 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1537 - accuracy: 0.16 - ETA: 2:04 - loss: 3.1532 - accuracy: 0.16 - ETA: 2:03 - loss: 3.1522 - accuracy: 0.16 - ETA: 2:03 - loss: 3.1533 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1542 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1542 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1548 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1550 - accuracy: 0.16 - ETA: 2:00 - loss: 3.1538 - accuracy: 0.16 - ETA: 2:00 - loss: 3.1534 - accuracy: 0.16 - ETA: 1:59 - loss: 3.1517 - accuracy: 0.16 - ETA: 1:58 - loss: 3.1528 - accuracy: 0.16 - ETA: 1:58 - loss: 3.1533 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1571 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1578 - accuracy: 0.16 - ETA: 1:56 - loss: 3.1586 - accuracy: 0.16 - ETA: 1:55 - loss: 3.1600 - accuracy: 0.16 - ETA: 1:55 - loss: 3.1612 - accuracy: 0.16 - ETA: 1:54 - loss: 3.1627 - accuracy: 0.16 - ETA: 1:54 - loss: 3.1629 - accuracy: 0.16 - ETA: 1:53 - loss: 3.1625 - accuracy: 0.16 - ETA: 1:52 - loss: 3.1633 - accuracy: 0.16 - ETA: 1:52 - loss: 3.1632 - accuracy: 0.16 - ETA: 1:51 - loss: 3.1638 - accuracy: 0.16 - ETA: 1:51 - loss: 3.1646 - accuracy: 0.16 - ETA: 1:50 - loss: 3.1638 - accuracy: 0.16 - ETA: 1:49 - loss: 3.1657 - accuracy: 0.16 - ETA: 1:49 - loss: 3.1672 - accuracy: 0.16 - ETA: 1:48 - loss: 3.1683 - accuracy: 0.16 - ETA: 1:48 - loss: 3.1680 - accuracy: 0.16 - ETA: 1:47 - loss: 3.1686 - accuracy: 0.16 - ETA: 1:46 - loss: 3.1704 - accuracy: 0.16 - ETA: 1:46 - loss: 3.1705 - accuracy: 0.16 - ETA: 1:45 - loss: 3.1719 - accuracy: 0.16 - ETA: 1:45 - loss: 3.1720 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1722 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1723 - accuracy: 0.16 - ETA: 1:43 - loss: 3.1714 - accuracy: 0.16 - ETA: 1:42 - loss: 3.1722 - accuracy: 0.16 - ETA: 1:42 - loss: 3.1737 - accuracy: 0.16 - ETA: 1:41 - loss: 3.1743 - accuracy: 0.16 - ETA: 1:41 - loss: 3.1757 - accuracy: 0.16 - ETA: 1:40 - loss: 3.1777 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1781 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1786 - accuracy: 0.16 - ETA: 1:38 - loss: 3.1794 - accuracy: 0.16 - ETA: 1:38 - loss: 3.1803 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1808 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1815 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1821 - accuracy: 0.16 - ETA: 1:35 - loss: 3.1812 - accuracy: 0.16 - ETA: 1:35 - loss: 3.1809 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1803 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1804 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1802 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1794 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1784 - accuracy: 0.16 - ETA: 1:31 - loss: 3.1789 - accuracy: 0.16 - ETA: 1:31 - loss: 3.1775 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1772 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1779 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1771 - accuracy: 0.16 - ETA: 1:28 - loss: 3.1763 - accuracy: 0.16 - ETA: 1:28 - loss: 3.1768 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1772 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1766 - accuracy: 0.16 - ETA: 1:26 - loss: 3.1772 - accuracy: 0.16 - ETA: 1:25 - loss: 3.1785 - accuracy: 0.16 - ETA: 1:25 - loss: 3.1777 - accuracy: 0.16 - ETA: 1:24 - loss: 3.1779 - accuracy: 0.1610"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1783 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1784 - accuracy: 0.16 - ETA: 1:22 - loss: 3.1783 - accuracy: 0.16 - ETA: 1:22 - loss: 3.1779 - accuracy: 0.16 - ETA: 1:21 - loss: 3.1783 - accuracy: 0.16 - ETA: 1:21 - loss: 3.1778 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1773 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1776 - accuracy: 0.16 - ETA: 1:19 - loss: 3.1772 - accuracy: 0.16 - ETA: 1:18 - loss: 3.1776 - accuracy: 0.16 - ETA: 1:18 - loss: 3.1767 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1760 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1766 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1765 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1770 - accuracy: 0.16 - ETA: 1:15 - loss: 3.1763 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1775 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1782 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1773 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1780 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1779 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1776 - accuracy: 0.16 - ETA: 1:11 - loss: 3.1781 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1785 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1785 - accuracy: 0.16 - ETA: 1:09 - loss: 3.1784 - accuracy: 0.16 - ETA: 1:09 - loss: 3.1790 - accuracy: 0.16 - ETA: 1:08 - loss: 3.1783 - accuracy: 0.16 - ETA: 1:07 - loss: 3.1779 - accuracy: 0.16 - ETA: 1:07 - loss: 3.1775 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1774 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1767 - accuracy: 0.16 - ETA: 1:05 - loss: 3.1769 - accuracy: 0.16 - ETA: 1:05 - loss: 3.1771 - accuracy: 0.16 - ETA: 1:04 - loss: 3.1766 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1765 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1774 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1770 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1773 - accuracy: 0.16 - ETA: 1:01 - loss: 3.1774 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1779 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1778 - accuracy: 0.16 - ETA: 59s - loss: 3.1777 - accuracy: 0.1616 - ETA: 59s - loss: 3.1773 - accuracy: 0.161 - ETA: 58s - loss: 3.1775 - accuracy: 0.161 - ETA: 58s - loss: 3.1775 - accuracy: 0.161 - ETA: 57s - loss: 3.1778 - accuracy: 0.161 - ETA: 56s - loss: 3.1766 - accuracy: 0.161 - ETA: 56s - loss: 3.1770 - accuracy: 0.161 - ETA: 55s - loss: 3.1763 - accuracy: 0.161 - ETA: 55s - loss: 3.1769 - accuracy: 0.161 - ETA: 54s - loss: 3.1771 - accuracy: 0.161 - ETA: 53s - loss: 3.1772 - accuracy: 0.161 - ETA: 53s - loss: 3.1777 - accuracy: 0.161 - ETA: 52s - loss: 3.1778 - accuracy: 0.161 - ETA: 52s - loss: 3.1770 - accuracy: 0.161 - ETA: 51s - loss: 3.1770 - accuracy: 0.161 - ETA: 50s - loss: 3.1772 - accuracy: 0.161 - ETA: 50s - loss: 3.1773 - accuracy: 0.161 - ETA: 49s - loss: 3.1780 - accuracy: 0.161 - ETA: 49s - loss: 3.1777 - accuracy: 0.161 - ETA: 48s - loss: 3.1781 - accuracy: 0.161 - ETA: 48s - loss: 3.1786 - accuracy: 0.161 - ETA: 47s - loss: 3.1797 - accuracy: 0.161 - ETA: 46s - loss: 3.1798 - accuracy: 0.161 - ETA: 46s - loss: 3.1800 - accuracy: 0.161 - ETA: 45s - loss: 3.1814 - accuracy: 0.161 - ETA: 45s - loss: 3.1815 - accuracy: 0.160 - ETA: 44s - loss: 3.1814 - accuracy: 0.160 - ETA: 43s - loss: 3.1813 - accuracy: 0.160 - ETA: 43s - loss: 3.1818 - accuracy: 0.160 - ETA: 42s - loss: 3.1820 - accuracy: 0.160 - ETA: 42s - loss: 3.1822 - accuracy: 0.160 - ETA: 41s - loss: 3.1826 - accuracy: 0.160 - ETA: 41s - loss: 3.1826 - accuracy: 0.160 - ETA: 40s - loss: 3.1819 - accuracy: 0.160 - ETA: 39s - loss: 3.1813 - accuracy: 0.160 - ETA: 39s - loss: 3.1815 - accuracy: 0.160 - ETA: 38s - loss: 3.1810 - accuracy: 0.160 - ETA: 38s - loss: 3.1810 - accuracy: 0.160 - ETA: 37s - loss: 3.1802 - accuracy: 0.160 - ETA: 36s - loss: 3.1794 - accuracy: 0.160 - ETA: 36s - loss: 3.1792 - accuracy: 0.160 - ETA: 35s - loss: 3.1796 - accuracy: 0.160 - ETA: 35s - loss: 3.1796 - accuracy: 0.161 - ETA: 34s - loss: 3.1794 - accuracy: 0.161 - ETA: 33s - loss: 3.1788 - accuracy: 0.161 - ETA: 33s - loss: 3.1792 - accuracy: 0.161 - ETA: 32s - loss: 3.1794 - accuracy: 0.161 - ETA: 32s - loss: 3.1792 - accuracy: 0.161 - ETA: 31s - loss: 3.1789 - accuracy: 0.161 - ETA: 31s - loss: 3.1792 - accuracy: 0.161 - ETA: 30s - loss: 3.1786 - accuracy: 0.161 - ETA: 29s - loss: 3.1797 - accuracy: 0.161 - ETA: 29s - loss: 3.1798 - accuracy: 0.160 - ETA: 28s - loss: 3.1804 - accuracy: 0.160 - ETA: 28s - loss: 3.1798 - accuracy: 0.161 - ETA: 27s - loss: 3.1793 - accuracy: 0.161 - ETA: 26s - loss: 3.1798 - accuracy: 0.161 - ETA: 26s - loss: 3.1795 - accuracy: 0.161 - ETA: 25s - loss: 3.1801 - accuracy: 0.161 - ETA: 25s - loss: 3.1803 - accuracy: 0.161 - ETA: 24s - loss: 3.1806 - accuracy: 0.161 - ETA: 24s - loss: 3.1808 - accuracy: 0.161 - ETA: 23s - loss: 3.1802 - accuracy: 0.161 - ETA: 22s - loss: 3.1805 - accuracy: 0.161 - ETA: 22s - loss: 3.1807 - accuracy: 0.161 - ETA: 21s - loss: 3.1814 - accuracy: 0.161 - ETA: 21s - loss: 3.1813 - accuracy: 0.161 - ETA: 20s - loss: 3.1813 - accuracy: 0.161 - ETA: 19s - loss: 3.1816 - accuracy: 0.161 - ETA: 19s - loss: 3.1814 - accuracy: 0.161 - ETA: 18s - loss: 3.1804 - accuracy: 0.161 - ETA: 18s - loss: 3.1803 - accuracy: 0.161 - ETA: 17s - loss: 3.1804 - accuracy: 0.161 - ETA: 17s - loss: 3.1805 - accuracy: 0.161 - ETA: 16s - loss: 3.1808 - accuracy: 0.161 - ETA: 15s - loss: 3.1810 - accuracy: 0.161 - ETA: 15s - loss: 3.1806 - accuracy: 0.161 - ETA: 14s - loss: 3.1806 - accuracy: 0.161 - ETA: 14s - loss: 3.1811 - accuracy: 0.161 - ETA: 13s - loss: 3.1807 - accuracy: 0.161 - ETA: 12s - loss: 3.1805 - accuracy: 0.161 - ETA: 12s - loss: 3.1796 - accuracy: 0.162 - ETA: 11s - loss: 3.1794 - accuracy: 0.162 - ETA: 11s - loss: 3.1795 - accuracy: 0.162 - ETA: 10s - loss: 3.1797 - accuracy: 0.162 - ETA: 10s - loss: 3.1795 - accuracy: 0.162 - ETA: 9s - loss: 3.1791 - accuracy: 0.162 - ETA: 8s - loss: 3.1801 - accuracy: 0.16 - ETA: 8s - loss: 3.1804 - accuracy: 0.16 - ETA: 7s - loss: 3.1810 - accuracy: 0.16 - ETA: 7s - loss: 3.1808 - accuracy: 0.16 - ETA: 6s - loss: 3.1807 - accuracy: 0.16 - ETA: 5s - loss: 3.1806 - accuracy: 0.16 - ETA: 5s - loss: 3.1807 - accuracy: 0.16 - ETA: 4s - loss: 3.1817 - accuracy: 0.16 - ETA: 4s - loss: 3.1817 - accuracy: 0.16 - ETA: 3s - loss: 3.1815 - accuracy: 0.16 - ETA: 2s - loss: 3.1818 - accuracy: 0.16 - ETA: 2s - loss: 3.1813 - accuracy: 0.16 - ETA: 1s - loss: 3.1813 - accuracy: 0.16 - ETA: 1s - loss: 3.1812 - accuracy: 0.16 - ETA: 0s - loss: 3.1815 - accuracy: 0.16 - ETA: 0s - loss: 3.1812 - accuracy: 0.16 - 206s 5ms/step - loss: 3.1811 - accuracy: 0.1619 - val_loss: 3.8708 - val_accuracy: 0.0418\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:34 - loss: 3.2862 - accuracy: 0.12 - ETA: 3:19 - loss: 3.1105 - accuracy: 0.17 - ETA: 3:13 - loss: 3.1872 - accuracy: 0.15 - ETA: 3:11 - loss: 3.2029 - accuracy: 0.15 - ETA: 3:11 - loss: 3.1980 - accuracy: 0.15 - ETA: 3:10 - loss: 3.1808 - accuracy: 0.15 - ETA: 3:09 - loss: 3.1608 - accuracy: 0.15 - ETA: 3:08 - loss: 3.1742 - accuracy: 0.15 - ETA: 3:10 - loss: 3.1478 - accuracy: 0.16 - ETA: 3:09 - loss: 3.1505 - accuracy: 0.16 - ETA: 3:09 - loss: 3.1609 - accuracy: 0.16 - ETA: 3:08 - loss: 3.1661 - accuracy: 0.15 - ETA: 3:07 - loss: 3.1696 - accuracy: 0.15 - ETA: 3:07 - loss: 3.1739 - accuracy: 0.16 - ETA: 3:08 - loss: 3.1732 - accuracy: 0.16 - ETA: 3:07 - loss: 3.1629 - accuracy: 0.16 - ETA: 3:07 - loss: 3.1655 - accuracy: 0.16 - ETA: 3:06 - loss: 3.1683 - accuracy: 0.16 - ETA: 3:05 - loss: 3.1784 - accuracy: 0.15 - ETA: 3:04 - loss: 3.1865 - accuracy: 0.15 - ETA: 3:03 - loss: 3.1890 - accuracy: 0.15 - ETA: 3:02 - loss: 3.1851 - accuracy: 0.15 - ETA: 3:01 - loss: 3.1906 - accuracy: 0.15 - ETA: 3:00 - loss: 3.1969 - accuracy: 0.15 - ETA: 3:00 - loss: 3.1969 - accuracy: 0.15 - ETA: 2:59 - loss: 3.1849 - accuracy: 0.15 - ETA: 2:58 - loss: 3.1831 - accuracy: 0.15 - ETA: 2:57 - loss: 3.1926 - accuracy: 0.15 - ETA: 2:57 - loss: 3.1880 - accuracy: 0.15 - ETA: 2:56 - loss: 3.1880 - accuracy: 0.15 - ETA: 2:55 - loss: 3.1838 - accuracy: 0.15 - ETA: 2:55 - loss: 3.1852 - accuracy: 0.15 - ETA: 2:54 - loss: 3.1941 - accuracy: 0.15 - ETA: 2:53 - loss: 3.1934 - accuracy: 0.15 - ETA: 2:53 - loss: 3.1894 - accuracy: 0.15 - ETA: 2:52 - loss: 3.1881 - accuracy: 0.15 - ETA: 2:52 - loss: 3.1879 - accuracy: 0.15 - ETA: 2:51 - loss: 3.1970 - accuracy: 0.15 - ETA: 2:50 - loss: 3.1993 - accuracy: 0.15 - ETA: 2:49 - loss: 3.2002 - accuracy: 0.15 - ETA: 2:49 - loss: 3.1985 - accuracy: 0.15 - ETA: 2:49 - loss: 3.1977 - accuracy: 0.15 - ETA: 2:48 - loss: 3.1966 - accuracy: 0.15 - ETA: 2:48 - loss: 3.1922 - accuracy: 0.15 - ETA: 2:47 - loss: 3.1876 - accuracy: 0.15 - ETA: 2:47 - loss: 3.1852 - accuracy: 0.15 - ETA: 2:46 - loss: 3.1868 - accuracy: 0.15 - ETA: 2:45 - loss: 3.1846 - accuracy: 0.15 - ETA: 2:45 - loss: 3.1841 - accuracy: 0.15 - ETA: 2:44 - loss: 3.1849 - accuracy: 0.15 - ETA: 2:44 - loss: 3.1833 - accuracy: 0.15 - ETA: 2:43 - loss: 3.1810 - accuracy: 0.15 - ETA: 2:42 - loss: 3.1835 - accuracy: 0.15 - ETA: 2:42 - loss: 3.1815 - accuracy: 0.15 - ETA: 2:41 - loss: 3.1797 - accuracy: 0.15 - ETA: 2:41 - loss: 3.1786 - accuracy: 0.15 - ETA: 2:41 - loss: 3.1798 - accuracy: 0.15 - ETA: 2:41 - loss: 3.1784 - accuracy: 0.15 - ETA: 2:40 - loss: 3.1802 - accuracy: 0.15 - ETA: 2:39 - loss: 3.1811 - accuracy: 0.15 - ETA: 2:39 - loss: 3.1804 - accuracy: 0.15 - ETA: 2:38 - loss: 3.1807 - accuracy: 0.15 - ETA: 2:38 - loss: 3.1806 - accuracy: 0.15 - ETA: 2:37 - loss: 3.1759 - accuracy: 0.15 - ETA: 2:36 - loss: 3.1729 - accuracy: 0.15 - ETA: 2:36 - loss: 3.1727 - accuracy: 0.15 - ETA: 2:35 - loss: 3.1729 - accuracy: 0.15 - ETA: 2:35 - loss: 3.1719 - accuracy: 0.15 - ETA: 2:35 - loss: 3.1722 - accuracy: 0.15 - ETA: 2:34 - loss: 3.1749 - accuracy: 0.15 - ETA: 2:34 - loss: 3.1756 - accuracy: 0.15 - ETA: 2:33 - loss: 3.1748 - accuracy: 0.15 - ETA: 2:33 - loss: 3.1758 - accuracy: 0.15 - ETA: 2:32 - loss: 3.1772 - accuracy: 0.16 - ETA: 2:32 - loss: 3.1798 - accuracy: 0.16 - ETA: 2:31 - loss: 3.1803 - accuracy: 0.16 - ETA: 2:30 - loss: 3.1824 - accuracy: 0.15 - ETA: 2:30 - loss: 3.1819 - accuracy: 0.15 - ETA: 2:29 - loss: 3.1805 - accuracy: 0.15 - ETA: 2:28 - loss: 3.1807 - accuracy: 0.16 - ETA: 2:28 - loss: 3.1821 - accuracy: 0.15 - ETA: 2:27 - loss: 3.1830 - accuracy: 0.15 - ETA: 2:26 - loss: 3.1828 - accuracy: 0.15 - ETA: 2:26 - loss: 3.1822 - accuracy: 0.15 - ETA: 2:25 - loss: 3.1836 - accuracy: 0.15 - ETA: 2:25 - loss: 3.1821 - accuracy: 0.15 - ETA: 2:24 - loss: 3.1795 - accuracy: 0.16 - ETA: 2:23 - loss: 3.1770 - accuracy: 0.16 - ETA: 2:23 - loss: 3.1775 - accuracy: 0.15 - ETA: 2:22 - loss: 3.1746 - accuracy: 0.16 - ETA: 2:21 - loss: 3.1746 - accuracy: 0.16 - ETA: 2:21 - loss: 3.1741 - accuracy: 0.16 - ETA: 2:20 - loss: 3.1735 - accuracy: 0.16 - ETA: 2:19 - loss: 3.1758 - accuracy: 0.16 - ETA: 2:19 - loss: 3.1756 - accuracy: 0.16 - ETA: 2:18 - loss: 3.1730 - accuracy: 0.16 - ETA: 2:18 - loss: 3.1734 - accuracy: 0.16 - ETA: 2:17 - loss: 3.1728 - accuracy: 0.16 - ETA: 2:17 - loss: 3.1719 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1710 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1718 - accuracy: 0.16 - ETA: 2:15 - loss: 3.1701 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1714 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1719 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1711 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1718 - accuracy: 0.16 - ETA: 2:12 - loss: 3.1723 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1712 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1712 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1706 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1697 - accuracy: 0.16 - ETA: 2:09 - loss: 3.1712 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1705 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1697 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1705 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1709 - accuracy: 0.16 - ETA: 2:06 - loss: 3.1709 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1711 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1715 - accuracy: 0.16 - ETA: 2:04 - loss: 3.1702 - accuracy: 0.16 - ETA: 2:04 - loss: 3.1723 - accuracy: 0.16 - ETA: 2:03 - loss: 3.1735 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1735 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1727 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1732 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1708 - accuracy: 0.16 - ETA: 2:00 - loss: 3.1719 - accuracy: 0.16 - ETA: 2:00 - loss: 3.1706 - accuracy: 0.16 - ETA: 1:59 - loss: 3.1707 - accuracy: 0.16 - ETA: 1:58 - loss: 3.1704 - accuracy: 0.16 - ETA: 1:58 - loss: 3.1712 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1701 - accuracy: 0.16 - ETA: 1:56 - loss: 3.1701 - accuracy: 0.16 - ETA: 1:56 - loss: 3.1713 - accuracy: 0.16 - ETA: 1:55 - loss: 3.1713 - accuracy: 0.16 - ETA: 1:55 - loss: 3.1723 - accuracy: 0.16 - ETA: 1:54 - loss: 3.1708 - accuracy: 0.16 - ETA: 1:54 - loss: 3.1692 - accuracy: 0.16 - ETA: 1:53 - loss: 3.1698 - accuracy: 0.16 - ETA: 1:52 - loss: 3.1693 - accuracy: 0.16 - ETA: 1:52 - loss: 3.1702 - accuracy: 0.16 - ETA: 1:51 - loss: 3.1696 - accuracy: 0.16 - ETA: 1:50 - loss: 3.1690 - accuracy: 0.16 - ETA: 1:50 - loss: 3.1691 - accuracy: 0.16 - ETA: 1:49 - loss: 3.1695 - accuracy: 0.16 - ETA: 1:49 - loss: 3.1688 - accuracy: 0.16 - ETA: 1:48 - loss: 3.1692 - accuracy: 0.16 - ETA: 1:47 - loss: 3.1680 - accuracy: 0.16 - ETA: 1:47 - loss: 3.1681 - accuracy: 0.16 - ETA: 1:46 - loss: 3.1671 - accuracy: 0.16 - ETA: 1:46 - loss: 3.1667 - accuracy: 0.16 - ETA: 1:45 - loss: 3.1675 - accuracy: 0.16 - ETA: 1:45 - loss: 3.1681 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1683 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1669 - accuracy: 0.16 - ETA: 1:43 - loss: 3.1677 - accuracy: 0.16 - ETA: 1:42 - loss: 3.1667 - accuracy: 0.16 - ETA: 1:42 - loss: 3.1668 - accuracy: 0.16 - ETA: 1:41 - loss: 3.1664 - accuracy: 0.16 - ETA: 1:41 - loss: 3.1662 - accuracy: 0.16 - ETA: 1:40 - loss: 3.1658 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1651 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1667 - accuracy: 0.16 - ETA: 1:38 - loss: 3.1667 - accuracy: 0.16 - ETA: 1:38 - loss: 3.1649 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1647 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1650 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1651 - accuracy: 0.16 - ETA: 1:35 - loss: 3.1652 - accuracy: 0.16 - ETA: 1:35 - loss: 3.1650 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1653 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1664 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1665 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1659 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1670 - accuracy: 0.16 - ETA: 1:31 - loss: 3.1669 - accuracy: 0.16 - ETA: 1:31 - loss: 3.1673 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1660 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1647 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1646 - accuracy: 0.16 - ETA: 1:28 - loss: 3.1644 - accuracy: 0.16 - ETA: 1:28 - loss: 3.1640 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1647 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1643 - accuracy: 0.16 - ETA: 1:26 - loss: 3.1644 - accuracy: 0.16 - ETA: 1:25 - loss: 3.1646 - accuracy: 0.1644"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:25 - loss: 3.1637 - accuracy: 0.16 - ETA: 1:24 - loss: 3.1637 - accuracy: 0.16 - ETA: 1:24 - loss: 3.1640 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1635 - accuracy: 0.16 - ETA: 1:22 - loss: 3.1626 - accuracy: 0.16 - ETA: 1:22 - loss: 3.1617 - accuracy: 0.16 - ETA: 1:21 - loss: 3.1611 - accuracy: 0.16 - ETA: 1:21 - loss: 3.1609 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1614 - accuracy: 0.16 - ETA: 1:19 - loss: 3.1621 - accuracy: 0.16 - ETA: 1:19 - loss: 3.1613 - accuracy: 0.16 - ETA: 1:18 - loss: 3.1605 - accuracy: 0.16 - ETA: 1:18 - loss: 3.1599 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1591 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1597 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1603 - accuracy: 0.16 - ETA: 1:15 - loss: 3.1598 - accuracy: 0.16 - ETA: 1:15 - loss: 3.1598 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1606 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1602 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1601 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1593 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1594 - accuracy: 0.16 - ETA: 1:11 - loss: 3.1587 - accuracy: 0.16 - ETA: 1:11 - loss: 3.1586 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1587 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1589 - accuracy: 0.16 - ETA: 1:09 - loss: 3.1588 - accuracy: 0.16 - ETA: 1:09 - loss: 3.1598 - accuracy: 0.16 - ETA: 1:08 - loss: 3.1590 - accuracy: 0.16 - ETA: 1:07 - loss: 3.1588 - accuracy: 0.16 - ETA: 1:07 - loss: 3.1589 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1588 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1591 - accuracy: 0.16 - ETA: 1:05 - loss: 3.1584 - accuracy: 0.16 - ETA: 1:04 - loss: 3.1582 - accuracy: 0.16 - ETA: 1:04 - loss: 3.1584 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1592 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1597 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1589 - accuracy: 0.16 - ETA: 1:01 - loss: 3.1590 - accuracy: 0.16 - ETA: 1:01 - loss: 3.1583 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1585 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1583 - accuracy: 0.16 - ETA: 59s - loss: 3.1594 - accuracy: 0.1660 - ETA: 58s - loss: 3.1587 - accuracy: 0.166 - ETA: 58s - loss: 3.1590 - accuracy: 0.166 - ETA: 57s - loss: 3.1585 - accuracy: 0.165 - ETA: 57s - loss: 3.1583 - accuracy: 0.165 - ETA: 56s - loss: 3.1587 - accuracy: 0.165 - ETA: 55s - loss: 3.1584 - accuracy: 0.165 - ETA: 55s - loss: 3.1585 - accuracy: 0.166 - ETA: 54s - loss: 3.1582 - accuracy: 0.166 - ETA: 54s - loss: 3.1578 - accuracy: 0.166 - ETA: 53s - loss: 3.1582 - accuracy: 0.166 - ETA: 52s - loss: 3.1590 - accuracy: 0.166 - ETA: 52s - loss: 3.1585 - accuracy: 0.166 - ETA: 51s - loss: 3.1590 - accuracy: 0.166 - ETA: 51s - loss: 3.1590 - accuracy: 0.166 - ETA: 50s - loss: 3.1583 - accuracy: 0.166 - ETA: 49s - loss: 3.1587 - accuracy: 0.166 - ETA: 49s - loss: 3.1583 - accuracy: 0.166 - ETA: 48s - loss: 3.1582 - accuracy: 0.166 - ETA: 48s - loss: 3.1586 - accuracy: 0.166 - ETA: 47s - loss: 3.1589 - accuracy: 0.166 - ETA: 46s - loss: 3.1588 - accuracy: 0.166 - ETA: 46s - loss: 3.1591 - accuracy: 0.166 - ETA: 45s - loss: 3.1600 - accuracy: 0.166 - ETA: 45s - loss: 3.1606 - accuracy: 0.166 - ETA: 44s - loss: 3.1598 - accuracy: 0.166 - ETA: 43s - loss: 3.1598 - accuracy: 0.166 - ETA: 43s - loss: 3.1598 - accuracy: 0.166 - ETA: 42s - loss: 3.1600 - accuracy: 0.166 - ETA: 42s - loss: 3.1612 - accuracy: 0.166 - ETA: 41s - loss: 3.1606 - accuracy: 0.166 - ETA: 41s - loss: 3.1609 - accuracy: 0.166 - ETA: 40s - loss: 3.1613 - accuracy: 0.165 - ETA: 39s - loss: 3.1609 - accuracy: 0.165 - ETA: 39s - loss: 3.1609 - accuracy: 0.165 - ETA: 38s - loss: 3.1607 - accuracy: 0.165 - ETA: 38s - loss: 3.1605 - accuracy: 0.166 - ETA: 37s - loss: 3.1608 - accuracy: 0.165 - ETA: 36s - loss: 3.1606 - accuracy: 0.166 - ETA: 36s - loss: 3.1609 - accuracy: 0.165 - ETA: 35s - loss: 3.1611 - accuracy: 0.165 - ETA: 35s - loss: 3.1615 - accuracy: 0.165 - ETA: 34s - loss: 3.1612 - accuracy: 0.165 - ETA: 33s - loss: 3.1617 - accuracy: 0.165 - ETA: 33s - loss: 3.1620 - accuracy: 0.165 - ETA: 32s - loss: 3.1615 - accuracy: 0.165 - ETA: 32s - loss: 3.1614 - accuracy: 0.165 - ETA: 31s - loss: 3.1611 - accuracy: 0.165 - ETA: 30s - loss: 3.1617 - accuracy: 0.165 - ETA: 30s - loss: 3.1623 - accuracy: 0.165 - ETA: 29s - loss: 3.1624 - accuracy: 0.165 - ETA: 29s - loss: 3.1622 - accuracy: 0.165 - ETA: 28s - loss: 3.1620 - accuracy: 0.165 - ETA: 27s - loss: 3.1621 - accuracy: 0.165 - ETA: 27s - loss: 3.1620 - accuracy: 0.165 - ETA: 26s - loss: 3.1620 - accuracy: 0.165 - ETA: 26s - loss: 3.1614 - accuracy: 0.165 - ETA: 25s - loss: 3.1616 - accuracy: 0.165 - ETA: 24s - loss: 3.1616 - accuracy: 0.165 - ETA: 24s - loss: 3.1615 - accuracy: 0.165 - ETA: 23s - loss: 3.1615 - accuracy: 0.165 - ETA: 23s - loss: 3.1615 - accuracy: 0.165 - ETA: 22s - loss: 3.1616 - accuracy: 0.165 - ETA: 21s - loss: 3.1607 - accuracy: 0.165 - ETA: 21s - loss: 3.1607 - accuracy: 0.165 - ETA: 20s - loss: 3.1612 - accuracy: 0.165 - ETA: 20s - loss: 3.1621 - accuracy: 0.165 - ETA: 19s - loss: 3.1627 - accuracy: 0.164 - ETA: 19s - loss: 3.1624 - accuracy: 0.164 - ETA: 18s - loss: 3.1623 - accuracy: 0.164 - ETA: 17s - loss: 3.1617 - accuracy: 0.165 - ETA: 17s - loss: 3.1617 - accuracy: 0.165 - ETA: 16s - loss: 3.1608 - accuracy: 0.165 - ETA: 16s - loss: 3.1599 - accuracy: 0.165 - ETA: 15s - loss: 3.1603 - accuracy: 0.165 - ETA: 14s - loss: 3.1603 - accuracy: 0.165 - ETA: 14s - loss: 3.1601 - accuracy: 0.165 - ETA: 13s - loss: 3.1605 - accuracy: 0.165 - ETA: 13s - loss: 3.1602 - accuracy: 0.165 - ETA: 12s - loss: 3.1603 - accuracy: 0.165 - ETA: 11s - loss: 3.1597 - accuracy: 0.165 - ETA: 11s - loss: 3.1598 - accuracy: 0.165 - ETA: 10s - loss: 3.1595 - accuracy: 0.166 - ETA: 10s - loss: 3.1595 - accuracy: 0.166 - ETA: 9s - loss: 3.1596 - accuracy: 0.166 - ETA: 8s - loss: 3.1595 - accuracy: 0.16 - ETA: 8s - loss: 3.1598 - accuracy: 0.16 - ETA: 7s - loss: 3.1592 - accuracy: 0.16 - ETA: 7s - loss: 3.1591 - accuracy: 0.16 - ETA: 6s - loss: 3.1599 - accuracy: 0.16 - ETA: 5s - loss: 3.1603 - accuracy: 0.16 - ETA: 5s - loss: 3.1600 - accuracy: 0.16 - ETA: 4s - loss: 3.1597 - accuracy: 0.16 - ETA: 4s - loss: 3.1597 - accuracy: 0.16 - ETA: 3s - loss: 3.1591 - accuracy: 0.16 - ETA: 3s - loss: 3.1591 - accuracy: 0.16 - ETA: 2s - loss: 3.1596 - accuracy: 0.16 - ETA: 1s - loss: 3.1604 - accuracy: 0.16 - ETA: 1s - loss: 3.1606 - accuracy: 0.16 - ETA: 0s - loss: 3.1602 - accuracy: 0.16 - ETA: 0s - loss: 3.1597 - accuracy: 0.16 - 208s 5ms/step - loss: 3.1596 - accuracy: 0.1664 - val_loss: 3.9509 - val_accuracy: 0.0467\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:24 - loss: 3.2918 - accuracy: 0.12 - ETA: 3:13 - loss: 3.2073 - accuracy: 0.15 - ETA: 3:09 - loss: 3.2413 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2395 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2327 - accuracy: 0.14 - ETA: 3:09 - loss: 3.1911 - accuracy: 0.14 - ETA: 3:09 - loss: 3.1954 - accuracy: 0.14 - ETA: 3:07 - loss: 3.1780 - accuracy: 0.15 - ETA: 3:06 - loss: 3.1640 - accuracy: 0.15 - ETA: 3:06 - loss: 3.1530 - accuracy: 0.15 - ETA: 3:05 - loss: 3.1668 - accuracy: 0.15 - ETA: 3:04 - loss: 3.1554 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1351 - accuracy: 0.16 - ETA: 3:03 - loss: 3.1391 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1349 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1331 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1302 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1348 - accuracy: 0.16 - ETA: 3:03 - loss: 3.1355 - accuracy: 0.16 - ETA: 3:03 - loss: 3.1337 - accuracy: 0.16 - ETA: 3:02 - loss: 3.1331 - accuracy: 0.16 - ETA: 3:01 - loss: 3.1275 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1281 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1277 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1181 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1122 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1167 - accuracy: 0.16 - ETA: 2:57 - loss: 3.1140 - accuracy: 0.16 - ETA: 2:56 - loss: 3.1114 - accuracy: 0.16 - ETA: 2:56 - loss: 3.1189 - accuracy: 0.16 - ETA: 2:55 - loss: 3.1242 - accuracy: 0.16 - ETA: 2:54 - loss: 3.1274 - accuracy: 0.16 - ETA: 2:54 - loss: 3.1311 - accuracy: 0.16 - ETA: 2:53 - loss: 3.1331 - accuracy: 0.16 - ETA: 2:52 - loss: 3.1349 - accuracy: 0.16 - ETA: 2:52 - loss: 3.1361 - accuracy: 0.16 - ETA: 2:51 - loss: 3.1345 - accuracy: 0.16 - ETA: 2:50 - loss: 3.1446 - accuracy: 0.16 - ETA: 2:50 - loss: 3.1416 - accuracy: 0.16 - ETA: 2:49 - loss: 3.1378 - accuracy: 0.16 - ETA: 2:49 - loss: 3.1390 - accuracy: 0.16 - ETA: 2:48 - loss: 3.1398 - accuracy: 0.16 - ETA: 2:48 - loss: 3.1375 - accuracy: 0.16 - ETA: 2:48 - loss: 3.1417 - accuracy: 0.16 - ETA: 2:47 - loss: 3.1424 - accuracy: 0.16 - ETA: 2:47 - loss: 3.1455 - accuracy: 0.16 - ETA: 2:46 - loss: 3.1456 - accuracy: 0.16 - ETA: 2:45 - loss: 3.1430 - accuracy: 0.16 - ETA: 2:45 - loss: 3.1401 - accuracy: 0.16 - ETA: 2:44 - loss: 3.1403 - accuracy: 0.16 - ETA: 2:44 - loss: 3.1387 - accuracy: 0.16 - ETA: 2:43 - loss: 3.1384 - accuracy: 0.16 - ETA: 2:43 - loss: 3.1378 - accuracy: 0.16 - ETA: 2:42 - loss: 3.1382 - accuracy: 0.16 - ETA: 2:42 - loss: 3.1430 - accuracy: 0.16 - ETA: 2:41 - loss: 3.1473 - accuracy: 0.16 - ETA: 2:41 - loss: 3.1463 - accuracy: 0.16 - ETA: 2:40 - loss: 3.1470 - accuracy: 0.16 - ETA: 2:39 - loss: 3.1463 - accuracy: 0.16 - ETA: 2:39 - loss: 3.1505 - accuracy: 0.16 - ETA: 2:38 - loss: 3.1512 - accuracy: 0.16 - ETA: 2:37 - loss: 3.1529 - accuracy: 0.16 - ETA: 2:37 - loss: 3.1517 - accuracy: 0.16 - ETA: 2:36 - loss: 3.1547 - accuracy: 0.16 - ETA: 2:35 - loss: 3.1532 - accuracy: 0.16 - ETA: 2:35 - loss: 3.1531 - accuracy: 0.16 - ETA: 2:34 - loss: 3.1555 - accuracy: 0.16 - ETA: 2:34 - loss: 3.1553 - accuracy: 0.16 - ETA: 2:33 - loss: 3.1590 - accuracy: 0.16 - ETA: 2:33 - loss: 3.1613 - accuracy: 0.16 - ETA: 2:32 - loss: 3.1602 - accuracy: 0.16 - ETA: 2:32 - loss: 3.1610 - accuracy: 0.16 - ETA: 2:31 - loss: 3.1631 - accuracy: 0.16 - ETA: 2:31 - loss: 3.1655 - accuracy: 0.16 - ETA: 2:30 - loss: 3.1652 - accuracy: 0.16 - ETA: 2:29 - loss: 3.1670 - accuracy: 0.16 - ETA: 2:29 - loss: 3.1707 - accuracy: 0.16 - ETA: 2:28 - loss: 3.1728 - accuracy: 0.16 - ETA: 2:27 - loss: 3.1752 - accuracy: 0.16 - ETA: 2:27 - loss: 3.1765 - accuracy: 0.16 - ETA: 2:26 - loss: 3.1757 - accuracy: 0.16 - ETA: 2:26 - loss: 3.1773 - accuracy: 0.16 - ETA: 2:25 - loss: 3.1757 - accuracy: 0.16 - ETA: 2:24 - loss: 3.1761 - accuracy: 0.16 - ETA: 2:24 - loss: 3.1749 - accuracy: 0.16 - ETA: 2:23 - loss: 3.1725 - accuracy: 0.16 - ETA: 2:22 - loss: 3.1748 - accuracy: 0.16 - ETA: 2:22 - loss: 3.1738 - accuracy: 0.16 - ETA: 2:21 - loss: 3.1723 - accuracy: 0.16 - ETA: 2:21 - loss: 3.1747 - accuracy: 0.16 - ETA: 2:20 - loss: 3.1742 - accuracy: 0.16 - ETA: 2:19 - loss: 3.1732 - accuracy: 0.16 - ETA: 2:19 - loss: 3.1727 - accuracy: 0.16 - ETA: 2:18 - loss: 3.1742 - accuracy: 0.16 - ETA: 2:18 - loss: 3.1756 - accuracy: 0.16 - ETA: 2:17 - loss: 3.1739 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1740 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1731 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1720 - accuracy: 0.16 - ETA: 2:15 - loss: 3.1712 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1710 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1707 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1689 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1702 - accuracy: 0.16 - ETA: 2:12 - loss: 3.1710 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1718 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1717 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1727 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1732 - accuracy: 0.16 - ETA: 2:09 - loss: 3.1738 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1728 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1733 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1735 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1733 - accuracy: 0.16 - ETA: 2:06 - loss: 3.1713 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1722 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1721 - accuracy: 0.16 - ETA: 2:04 - loss: 3.1724 - accuracy: 0.16 - ETA: 2:04 - loss: 3.1723 - accuracy: 0.16 - ETA: 2:03 - loss: 3.1726 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1717 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1697 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1692 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1699 - accuracy: 0.16 - ETA: 2:00 - loss: 3.1697 - accuracy: 0.16 - ETA: 2:00 - loss: 3.1697 - accuracy: 0.16 - ETA: 1:59 - loss: 3.1697 - accuracy: 0.16 - ETA: 1:59 - loss: 3.1698 - accuracy: 0.16 - ETA: 1:58 - loss: 3.1710 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1710 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1707 - accuracy: 0.16 - ETA: 1:56 - loss: 3.1710 - accuracy: 0.16 - ETA: 1:56 - loss: 3.1723 - accuracy: 0.16 - ETA: 1:55 - loss: 3.1734 - accuracy: 0.16 - ETA: 1:54 - loss: 3.1743 - accuracy: 0.16 - ETA: 1:54 - loss: 3.1744 - accuracy: 0.16 - ETA: 1:53 - loss: 3.1740 - accuracy: 0.16 - ETA: 1:53 - loss: 3.1731 - accuracy: 0.16 - ETA: 1:52 - loss: 3.1724 - accuracy: 0.16 - ETA: 1:51 - loss: 3.1718 - accuracy: 0.16 - ETA: 1:51 - loss: 3.1696 - accuracy: 0.16 - ETA: 1:50 - loss: 3.1701 - accuracy: 0.16 - ETA: 1:50 - loss: 3.1695 - accuracy: 0.16 - ETA: 1:49 - loss: 3.1698 - accuracy: 0.16 - ETA: 1:48 - loss: 3.1698 - accuracy: 0.16 - ETA: 1:48 - loss: 3.1688 - accuracy: 0.16 - ETA: 1:47 - loss: 3.1685 - accuracy: 0.16 - ETA: 1:47 - loss: 3.1685 - accuracy: 0.16 - ETA: 1:46 - loss: 3.1699 - accuracy: 0.16 - ETA: 1:45 - loss: 3.1705 - accuracy: 0.16 - ETA: 1:45 - loss: 3.1689 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1712 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1721 - accuracy: 0.16 - ETA: 1:43 - loss: 3.1714 - accuracy: 0.16 - ETA: 1:43 - loss: 3.1721 - accuracy: 0.16 - ETA: 1:42 - loss: 3.1721 - accuracy: 0.16 - ETA: 1:41 - loss: 3.1723 - accuracy: 0.16 - ETA: 1:41 - loss: 3.1729 - accuracy: 0.16 - ETA: 1:40 - loss: 3.1709 - accuracy: 0.16 - ETA: 1:40 - loss: 3.1698 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1697 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1693 - accuracy: 0.16 - ETA: 1:38 - loss: 3.1692 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1706 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1699 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1694 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1682 - accuracy: 0.16 - ETA: 1:35 - loss: 3.1681 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1681 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1698 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1706 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1701 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1699 - accuracy: 0.16 - ETA: 1:31 - loss: 3.1699 - accuracy: 0.16 - ETA: 1:31 - loss: 3.1706 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1709 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1698 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1697 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1698 - accuracy: 0.16 - ETA: 1:28 - loss: 3.1709 - accuracy: 0.16 - ETA: 1:28 - loss: 3.1708 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1715 - accuracy: 0.16 - ETA: 1:26 - loss: 3.1720 - accuracy: 0.16 - ETA: 1:26 - loss: 3.1718 - accuracy: 0.16 - ETA: 1:25 - loss: 3.1723 - accuracy: 0.16 - ETA: 1:25 - loss: 3.1726 - accuracy: 0.1661"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1730 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1744 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1749 - accuracy: 0.16 - ETA: 1:22 - loss: 3.1750 - accuracy: 0.16 - ETA: 1:22 - loss: 3.1754 - accuracy: 0.16 - ETA: 1:21 - loss: 3.1747 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1751 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1752 - accuracy: 0.16 - ETA: 1:19 - loss: 3.1750 - accuracy: 0.16 - ETA: 1:19 - loss: 3.1753 - accuracy: 0.16 - ETA: 1:18 - loss: 3.1757 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1765 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1775 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1778 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1773 - accuracy: 0.16 - ETA: 1:15 - loss: 3.1770 - accuracy: 0.16 - ETA: 1:15 - loss: 3.1777 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1764 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1764 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1768 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1770 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1777 - accuracy: 0.16 - ETA: 1:11 - loss: 3.1777 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1775 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1766 - accuracy: 0.16 - ETA: 1:09 - loss: 3.1773 - accuracy: 0.16 - ETA: 1:09 - loss: 3.1767 - accuracy: 0.16 - ETA: 1:08 - loss: 3.1762 - accuracy: 0.16 - ETA: 1:08 - loss: 3.1766 - accuracy: 0.16 - ETA: 1:07 - loss: 3.1757 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1750 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1752 - accuracy: 0.16 - ETA: 1:05 - loss: 3.1739 - accuracy: 0.16 - ETA: 1:05 - loss: 3.1734 - accuracy: 0.16 - ETA: 1:04 - loss: 3.1731 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1730 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1729 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1733 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1737 - accuracy: 0.16 - ETA: 1:01 - loss: 3.1740 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1739 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1738 - accuracy: 0.16 - ETA: 59s - loss: 3.1732 - accuracy: 0.1666 - ETA: 59s - loss: 3.1735 - accuracy: 0.166 - ETA: 58s - loss: 3.1748 - accuracy: 0.166 - ETA: 58s - loss: 3.1755 - accuracy: 0.166 - ETA: 57s - loss: 3.1756 - accuracy: 0.165 - ETA: 56s - loss: 3.1761 - accuracy: 0.165 - ETA: 56s - loss: 3.1767 - accuracy: 0.165 - ETA: 55s - loss: 3.1770 - accuracy: 0.165 - ETA: 55s - loss: 3.1774 - accuracy: 0.165 - ETA: 54s - loss: 3.1777 - accuracy: 0.165 - ETA: 53s - loss: 3.1774 - accuracy: 0.165 - ETA: 53s - loss: 3.1771 - accuracy: 0.165 - ETA: 52s - loss: 3.1764 - accuracy: 0.165 - ETA: 52s - loss: 3.1769 - accuracy: 0.165 - ETA: 51s - loss: 3.1775 - accuracy: 0.165 - ETA: 50s - loss: 3.1780 - accuracy: 0.164 - ETA: 50s - loss: 3.1777 - accuracy: 0.165 - ETA: 49s - loss: 3.1779 - accuracy: 0.165 - ETA: 49s - loss: 3.1772 - accuracy: 0.165 - ETA: 48s - loss: 3.1772 - accuracy: 0.165 - ETA: 48s - loss: 3.1775 - accuracy: 0.165 - ETA: 47s - loss: 3.1777 - accuracy: 0.165 - ETA: 46s - loss: 3.1779 - accuracy: 0.165 - ETA: 46s - loss: 3.1781 - accuracy: 0.164 - ETA: 45s - loss: 3.1785 - accuracy: 0.164 - ETA: 45s - loss: 3.1777 - accuracy: 0.165 - ETA: 44s - loss: 3.1772 - accuracy: 0.165 - ETA: 43s - loss: 3.1779 - accuracy: 0.165 - ETA: 43s - loss: 3.1774 - accuracy: 0.165 - ETA: 42s - loss: 3.1774 - accuracy: 0.165 - ETA: 42s - loss: 3.1774 - accuracy: 0.165 - ETA: 41s - loss: 3.1774 - accuracy: 0.165 - ETA: 41s - loss: 3.1775 - accuracy: 0.165 - ETA: 40s - loss: 3.1777 - accuracy: 0.165 - ETA: 39s - loss: 3.1778 - accuracy: 0.165 - ETA: 39s - loss: 3.1777 - accuracy: 0.165 - ETA: 38s - loss: 3.1776 - accuracy: 0.165 - ETA: 38s - loss: 3.1775 - accuracy: 0.165 - ETA: 37s - loss: 3.1776 - accuracy: 0.165 - ETA: 36s - loss: 3.1772 - accuracy: 0.165 - ETA: 36s - loss: 3.1774 - accuracy: 0.165 - ETA: 35s - loss: 3.1767 - accuracy: 0.165 - ETA: 35s - loss: 3.1770 - accuracy: 0.165 - ETA: 34s - loss: 3.1763 - accuracy: 0.165 - ETA: 34s - loss: 3.1766 - accuracy: 0.165 - ETA: 33s - loss: 3.1764 - accuracy: 0.165 - ETA: 32s - loss: 3.1765 - accuracy: 0.165 - ETA: 32s - loss: 3.1762 - accuracy: 0.165 - ETA: 31s - loss: 3.1763 - accuracy: 0.165 - ETA: 31s - loss: 3.1765 - accuracy: 0.165 - ETA: 30s - loss: 3.1761 - accuracy: 0.165 - ETA: 29s - loss: 3.1772 - accuracy: 0.164 - ETA: 29s - loss: 3.1763 - accuracy: 0.165 - ETA: 28s - loss: 3.1757 - accuracy: 0.165 - ETA: 28s - loss: 3.1764 - accuracy: 0.165 - ETA: 27s - loss: 3.1764 - accuracy: 0.165 - ETA: 27s - loss: 3.1764 - accuracy: 0.165 - ETA: 26s - loss: 3.1763 - accuracy: 0.165 - ETA: 25s - loss: 3.1756 - accuracy: 0.165 - ETA: 25s - loss: 3.1755 - accuracy: 0.165 - ETA: 24s - loss: 3.1753 - accuracy: 0.165 - ETA: 24s - loss: 3.1757 - accuracy: 0.165 - ETA: 23s - loss: 3.1753 - accuracy: 0.165 - ETA: 22s - loss: 3.1753 - accuracy: 0.165 - ETA: 22s - loss: 3.1752 - accuracy: 0.165 - ETA: 21s - loss: 3.1750 - accuracy: 0.165 - ETA: 21s - loss: 3.1739 - accuracy: 0.165 - ETA: 20s - loss: 3.1736 - accuracy: 0.165 - ETA: 19s - loss: 3.1735 - accuracy: 0.165 - ETA: 19s - loss: 3.1734 - accuracy: 0.165 - ETA: 18s - loss: 3.1739 - accuracy: 0.165 - ETA: 18s - loss: 3.1739 - accuracy: 0.165 - ETA: 17s - loss: 3.1738 - accuracy: 0.165 - ETA: 17s - loss: 3.1728 - accuracy: 0.165 - ETA: 16s - loss: 3.1727 - accuracy: 0.165 - ETA: 15s - loss: 3.1728 - accuracy: 0.165 - ETA: 15s - loss: 3.1729 - accuracy: 0.165 - ETA: 14s - loss: 3.1735 - accuracy: 0.165 - ETA: 14s - loss: 3.1736 - accuracy: 0.165 - ETA: 13s - loss: 3.1734 - accuracy: 0.165 - ETA: 12s - loss: 3.1733 - accuracy: 0.165 - ETA: 12s - loss: 3.1736 - accuracy: 0.164 - ETA: 11s - loss: 3.1736 - accuracy: 0.164 - ETA: 11s - loss: 3.1736 - accuracy: 0.164 - ETA: 10s - loss: 3.1740 - accuracy: 0.164 - ETA: 9s - loss: 3.1740 - accuracy: 0.164 - ETA: 9s - loss: 3.1743 - accuracy: 0.16 - ETA: 8s - loss: 3.1746 - accuracy: 0.16 - ETA: 8s - loss: 3.1744 - accuracy: 0.16 - ETA: 7s - loss: 3.1740 - accuracy: 0.16 - ETA: 7s - loss: 3.1737 - accuracy: 0.16 - ETA: 6s - loss: 3.1741 - accuracy: 0.16 - ETA: 5s - loss: 3.1736 - accuracy: 0.16 - ETA: 5s - loss: 3.1737 - accuracy: 0.16 - ETA: 4s - loss: 3.1734 - accuracy: 0.16 - ETA: 4s - loss: 3.1729 - accuracy: 0.16 - ETA: 3s - loss: 3.1729 - accuracy: 0.16 - ETA: 2s - loss: 3.1729 - accuracy: 0.16 - ETA: 2s - loss: 3.1732 - accuracy: 0.16 - ETA: 1s - loss: 3.1731 - accuracy: 0.16 - ETA: 1s - loss: 3.1732 - accuracy: 0.16 - ETA: 0s - loss: 3.1733 - accuracy: 0.16 - ETA: 0s - loss: 3.1734 - accuracy: 0.16 - 207s 5ms/step - loss: 3.1734 - accuracy: 0.1648 - val_loss: 4.0687 - val_accuracy: 0.0478\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:33 - loss: 3.1028 - accuracy: 0.22 - ETA: 3:22 - loss: 3.1958 - accuracy: 0.18 - ETA: 3:15 - loss: 3.1603 - accuracy: 0.18 - ETA: 3:13 - loss: 3.1338 - accuracy: 0.18 - ETA: 3:11 - loss: 3.1136 - accuracy: 0.18 - ETA: 3:10 - loss: 3.1257 - accuracy: 0.18 - ETA: 3:10 - loss: 3.1253 - accuracy: 0.18 - ETA: 3:09 - loss: 3.1093 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0925 - accuracy: 0.18 - ETA: 3:08 - loss: 3.1037 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0995 - accuracy: 0.18 - ETA: 3:07 - loss: 3.1001 - accuracy: 0.17 - ETA: 3:06 - loss: 3.1045 - accuracy: 0.17 - ETA: 3:05 - loss: 3.1073 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1091 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1073 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1154 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1278 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1261 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1280 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1281 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1334 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1261 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1234 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1345 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1386 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1388 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1400 - accuracy: 0.16 - ETA: 2:56 - loss: 3.1446 - accuracy: 0.16 - ETA: 2:55 - loss: 3.1426 - accuracy: 0.16 - ETA: 2:55 - loss: 3.1375 - accuracy: 0.16 - ETA: 2:54 - loss: 3.1349 - accuracy: 0.16 - ETA: 2:53 - loss: 3.1297 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1261 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1266 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1253 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1322 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1306 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1258 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1279 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1278 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1256 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1217 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1230 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1219 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1218 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1176 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1201 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1229 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1245 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1252 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1249 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1253 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1253 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1261 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1268 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1245 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1231 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1228 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1260 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1285 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1295 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1303 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1260 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1238 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1229 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1249 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1247 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1250 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1247 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1240 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1268 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1273 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1288 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1265 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1267 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1261 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1293 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1296 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1292 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1291 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1285 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1304 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1291 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1318 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1321 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1334 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1361 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1345 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1343 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1333 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1333 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1321 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1326 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1330 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1334 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1346 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1363 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1360 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1342 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1331 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1345 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1333 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1317 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1308 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1292 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1291 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1285 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1268 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1262 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1262 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1273 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1278 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1261 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1250 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1247 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1235 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1234 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1221 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1225 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1213 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1197 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1188 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1199 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1193 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1190 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1207 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1222 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1221 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1230 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1232 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1241 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1228 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1233 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1241 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1248 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1255 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1266 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1261 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1257 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1254 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1244 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1255 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1265 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1270 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1270 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1262 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1268 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1268 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1271 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1283 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1288 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1294 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1298 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1297 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1307 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1316 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1317 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1319 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1326 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1334 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1339 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1374 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1375 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1374 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1387 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1403 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1406 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1403 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1415 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1405 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1416 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1422 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1433 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1435 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1437 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1441 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1449 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1466 - accuracy: 0.16 - ETA: 1:28 - loss: 3.1465 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1472 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1474 - accuracy: 0.16 - ETA: 1:26 - loss: 3.1477 - accuracy: 0.16 - ETA: 1:26 - loss: 3.1468 - accuracy: 0.16 - ETA: 1:25 - loss: 3.1477 - accuracy: 0.16 - ETA: 1:24 - loss: 3.1480 - accuracy: 0.1697"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1476 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1475 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1481 - accuracy: 0.16 - ETA: 1:22 - loss: 3.1475 - accuracy: 0.16 - ETA: 1:22 - loss: 3.1467 - accuracy: 0.16 - ETA: 1:21 - loss: 3.1471 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1469 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1480 - accuracy: 0.16 - ETA: 1:19 - loss: 3.1473 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1476 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1479 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1487 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1491 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1491 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1490 - accuracy: 0.16 - ETA: 1:15 - loss: 3.1484 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1486 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1483 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1493 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1490 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1487 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1491 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1502 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1498 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1493 - accuracy: 0.16 - ETA: 1:09 - loss: 3.1497 - accuracy: 0.16 - ETA: 1:09 - loss: 3.1507 - accuracy: 0.16 - ETA: 1:08 - loss: 3.1506 - accuracy: 0.16 - ETA: 1:07 - loss: 3.1505 - accuracy: 0.16 - ETA: 1:07 - loss: 3.1509 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1495 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1493 - accuracy: 0.16 - ETA: 1:05 - loss: 3.1488 - accuracy: 0.16 - ETA: 1:05 - loss: 3.1487 - accuracy: 0.16 - ETA: 1:04 - loss: 3.1481 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1486 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1490 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1497 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1493 - accuracy: 0.16 - ETA: 1:01 - loss: 3.1491 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1494 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1496 - accuracy: 0.16 - ETA: 59s - loss: 3.1491 - accuracy: 0.1693 - ETA: 59s - loss: 3.1484 - accuracy: 0.169 - ETA: 58s - loss: 3.1485 - accuracy: 0.169 - ETA: 57s - loss: 3.1478 - accuracy: 0.169 - ETA: 57s - loss: 3.1480 - accuracy: 0.169 - ETA: 56s - loss: 3.1477 - accuracy: 0.169 - ETA: 56s - loss: 3.1477 - accuracy: 0.169 - ETA: 55s - loss: 3.1481 - accuracy: 0.169 - ETA: 55s - loss: 3.1481 - accuracy: 0.169 - ETA: 54s - loss: 3.1483 - accuracy: 0.169 - ETA: 53s - loss: 3.1490 - accuracy: 0.169 - ETA: 53s - loss: 3.1493 - accuracy: 0.169 - ETA: 52s - loss: 3.1485 - accuracy: 0.169 - ETA: 52s - loss: 3.1489 - accuracy: 0.169 - ETA: 51s - loss: 3.1488 - accuracy: 0.169 - ETA: 50s - loss: 3.1485 - accuracy: 0.169 - ETA: 50s - loss: 3.1490 - accuracy: 0.169 - ETA: 49s - loss: 3.1491 - accuracy: 0.169 - ETA: 49s - loss: 3.1492 - accuracy: 0.169 - ETA: 48s - loss: 3.1495 - accuracy: 0.169 - ETA: 48s - loss: 3.1497 - accuracy: 0.169 - ETA: 47s - loss: 3.1496 - accuracy: 0.169 - ETA: 46s - loss: 3.1495 - accuracy: 0.169 - ETA: 46s - loss: 3.1499 - accuracy: 0.169 - ETA: 45s - loss: 3.1506 - accuracy: 0.168 - ETA: 45s - loss: 3.1499 - accuracy: 0.169 - ETA: 44s - loss: 3.1504 - accuracy: 0.168 - ETA: 43s - loss: 3.1500 - accuracy: 0.168 - ETA: 43s - loss: 3.1501 - accuracy: 0.168 - ETA: 42s - loss: 3.1503 - accuracy: 0.168 - ETA: 42s - loss: 3.1505 - accuracy: 0.168 - ETA: 41s - loss: 3.1514 - accuracy: 0.168 - ETA: 41s - loss: 3.1517 - accuracy: 0.168 - ETA: 40s - loss: 3.1524 - accuracy: 0.168 - ETA: 39s - loss: 3.1522 - accuracy: 0.168 - ETA: 39s - loss: 3.1516 - accuracy: 0.168 - ETA: 38s - loss: 3.1518 - accuracy: 0.168 - ETA: 38s - loss: 3.1525 - accuracy: 0.168 - ETA: 37s - loss: 3.1530 - accuracy: 0.168 - ETA: 36s - loss: 3.1532 - accuracy: 0.168 - ETA: 36s - loss: 3.1528 - accuracy: 0.168 - ETA: 35s - loss: 3.1530 - accuracy: 0.168 - ETA: 35s - loss: 3.1531 - accuracy: 0.168 - ETA: 34s - loss: 3.1536 - accuracy: 0.167 - ETA: 34s - loss: 3.1539 - accuracy: 0.167 - ETA: 33s - loss: 3.1539 - accuracy: 0.167 - ETA: 32s - loss: 3.1543 - accuracy: 0.167 - ETA: 32s - loss: 3.1545 - accuracy: 0.167 - ETA: 31s - loss: 3.1547 - accuracy: 0.167 - ETA: 31s - loss: 3.1544 - accuracy: 0.167 - ETA: 30s - loss: 3.1539 - accuracy: 0.167 - ETA: 29s - loss: 3.1543 - accuracy: 0.167 - ETA: 29s - loss: 3.1541 - accuracy: 0.167 - ETA: 28s - loss: 3.1534 - accuracy: 0.167 - ETA: 28s - loss: 3.1540 - accuracy: 0.167 - ETA: 27s - loss: 3.1541 - accuracy: 0.167 - ETA: 26s - loss: 3.1541 - accuracy: 0.167 - ETA: 26s - loss: 3.1536 - accuracy: 0.167 - ETA: 25s - loss: 3.1535 - accuracy: 0.167 - ETA: 25s - loss: 3.1533 - accuracy: 0.167 - ETA: 24s - loss: 3.1535 - accuracy: 0.167 - ETA: 24s - loss: 3.1531 - accuracy: 0.167 - ETA: 23s - loss: 3.1526 - accuracy: 0.167 - ETA: 22s - loss: 3.1525 - accuracy: 0.167 - ETA: 22s - loss: 3.1527 - accuracy: 0.167 - ETA: 21s - loss: 3.1530 - accuracy: 0.167 - ETA: 21s - loss: 3.1525 - accuracy: 0.167 - ETA: 20s - loss: 3.1528 - accuracy: 0.167 - ETA: 19s - loss: 3.1531 - accuracy: 0.167 - ETA: 19s - loss: 3.1535 - accuracy: 0.167 - ETA: 18s - loss: 3.1537 - accuracy: 0.167 - ETA: 18s - loss: 3.1538 - accuracy: 0.167 - ETA: 17s - loss: 3.1538 - accuracy: 0.167 - ETA: 17s - loss: 3.1539 - accuracy: 0.167 - ETA: 16s - loss: 3.1541 - accuracy: 0.167 - ETA: 15s - loss: 3.1540 - accuracy: 0.167 - ETA: 15s - loss: 3.1543 - accuracy: 0.167 - ETA: 14s - loss: 3.1540 - accuracy: 0.167 - ETA: 14s - loss: 3.1534 - accuracy: 0.167 - ETA: 13s - loss: 3.1534 - accuracy: 0.167 - ETA: 12s - loss: 3.1537 - accuracy: 0.167 - ETA: 12s - loss: 3.1533 - accuracy: 0.167 - ETA: 11s - loss: 3.1532 - accuracy: 0.167 - ETA: 11s - loss: 3.1534 - accuracy: 0.167 - ETA: 10s - loss: 3.1533 - accuracy: 0.167 - ETA: 9s - loss: 3.1534 - accuracy: 0.167 - ETA: 9s - loss: 3.1529 - accuracy: 0.16 - ETA: 8s - loss: 3.1533 - accuracy: 0.16 - ETA: 8s - loss: 3.1542 - accuracy: 0.16 - ETA: 7s - loss: 3.1540 - accuracy: 0.16 - ETA: 7s - loss: 3.1544 - accuracy: 0.16 - ETA: 6s - loss: 3.1544 - accuracy: 0.16 - ETA: 5s - loss: 3.1545 - accuracy: 0.16 - ETA: 5s - loss: 3.1543 - accuracy: 0.16 - ETA: 4s - loss: 3.1542 - accuracy: 0.16 - ETA: 4s - loss: 3.1539 - accuracy: 0.16 - ETA: 3s - loss: 3.1532 - accuracy: 0.16 - ETA: 2s - loss: 3.1530 - accuracy: 0.16 - ETA: 2s - loss: 3.1529 - accuracy: 0.16 - ETA: 1s - loss: 3.1528 - accuracy: 0.16 - ETA: 1s - loss: 3.1528 - accuracy: 0.16 - ETA: 0s - loss: 3.1524 - accuracy: 0.16 - ETA: 0s - loss: 3.1519 - accuracy: 0.16 - 206s 5ms/step - loss: 3.1519 - accuracy: 0.1679 - val_loss: 3.9356 - val_accuracy: 0.0382\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:45 - loss: 3.0445 - accuracy: 0.15 - ETA: 3:32 - loss: 3.1181 - accuracy: 0.13 - ETA: 3:28 - loss: 3.1091 - accuracy: 0.15 - ETA: 3:24 - loss: 3.1297 - accuracy: 0.16 - ETA: 3:19 - loss: 3.1862 - accuracy: 0.15 - ETA: 3:18 - loss: 3.1935 - accuracy: 0.15 - ETA: 3:16 - loss: 3.1726 - accuracy: 0.15 - ETA: 3:14 - loss: 3.1500 - accuracy: 0.15 - ETA: 3:13 - loss: 3.1463 - accuracy: 0.15 - ETA: 3:11 - loss: 3.1527 - accuracy: 0.14 - ETA: 3:10 - loss: 3.1369 - accuracy: 0.15 - ETA: 3:09 - loss: 3.1369 - accuracy: 0.15 - ETA: 3:08 - loss: 3.1204 - accuracy: 0.16 - ETA: 3:07 - loss: 3.1078 - accuracy: 0.16 - ETA: 3:06 - loss: 3.1054 - accuracy: 0.16 - ETA: 3:05 - loss: 3.0950 - accuracy: 0.17 - ETA: 3:04 - loss: 3.0975 - accuracy: 0.17 - ETA: 3:04 - loss: 3.0942 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1042 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1038 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1031 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1069 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1082 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1126 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1089 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1098 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1177 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1239 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1269 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1273 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1255 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1297 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1346 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1346 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1292 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1290 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1311 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1320 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1362 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1391 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1387 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1352 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1302 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1288 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1310 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1306 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1325 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1376 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1398 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1385 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1379 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1414 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1423 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1422 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1429 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1398 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1393 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1400 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1393 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1374 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1386 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1376 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1390 - accuracy: 0.16 - ETA: 2:37 - loss: 3.1401 - accuracy: 0.16 - ETA: 2:36 - loss: 3.1401 - accuracy: 0.16 - ETA: 2:35 - loss: 3.1402 - accuracy: 0.16 - ETA: 2:35 - loss: 3.1366 - accuracy: 0.16 - ETA: 2:34 - loss: 3.1374 - accuracy: 0.16 - ETA: 2:33 - loss: 3.1400 - accuracy: 0.16 - ETA: 2:33 - loss: 3.1407 - accuracy: 0.16 - ETA: 2:32 - loss: 3.1402 - accuracy: 0.16 - ETA: 2:31 - loss: 3.1398 - accuracy: 0.16 - ETA: 2:31 - loss: 3.1428 - accuracy: 0.16 - ETA: 2:30 - loss: 3.1443 - accuracy: 0.16 - ETA: 2:30 - loss: 3.1432 - accuracy: 0.16 - ETA: 2:29 - loss: 3.1422 - accuracy: 0.16 - ETA: 2:29 - loss: 3.1446 - accuracy: 0.16 - ETA: 2:28 - loss: 3.1444 - accuracy: 0.16 - ETA: 2:28 - loss: 3.1467 - accuracy: 0.16 - ETA: 2:27 - loss: 3.1488 - accuracy: 0.16 - ETA: 2:26 - loss: 3.1480 - accuracy: 0.16 - ETA: 2:26 - loss: 3.1473 - accuracy: 0.16 - ETA: 2:25 - loss: 3.1465 - accuracy: 0.16 - ETA: 2:25 - loss: 3.1461 - accuracy: 0.16 - ETA: 2:24 - loss: 3.1458 - accuracy: 0.16 - ETA: 2:23 - loss: 3.1506 - accuracy: 0.16 - ETA: 2:23 - loss: 3.1501 - accuracy: 0.16 - ETA: 2:22 - loss: 3.1497 - accuracy: 0.16 - ETA: 2:22 - loss: 3.1494 - accuracy: 0.16 - ETA: 2:21 - loss: 3.1487 - accuracy: 0.16 - ETA: 2:20 - loss: 3.1510 - accuracy: 0.16 - ETA: 2:20 - loss: 3.1521 - accuracy: 0.16 - ETA: 2:19 - loss: 3.1550 - accuracy: 0.16 - ETA: 2:18 - loss: 3.1572 - accuracy: 0.16 - ETA: 2:18 - loss: 3.1555 - accuracy: 0.16 - ETA: 2:17 - loss: 3.1548 - accuracy: 0.16 - ETA: 2:17 - loss: 3.1559 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1569 - accuracy: 0.16 - ETA: 2:15 - loss: 3.1566 - accuracy: 0.16 - ETA: 2:15 - loss: 3.1547 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1540 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1538 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1523 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1517 - accuracy: 0.16 - ETA: 2:12 - loss: 3.1506 - accuracy: 0.16 - ETA: 2:12 - loss: 3.1485 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1489 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1504 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1534 - accuracy: 0.16 - ETA: 2:09 - loss: 3.1558 - accuracy: 0.16 - ETA: 2:09 - loss: 3.1565 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1573 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1582 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1579 - accuracy: 0.16 - ETA: 2:06 - loss: 3.1556 - accuracy: 0.16 - ETA: 2:06 - loss: 3.1566 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1559 - accuracy: 0.16 - ETA: 2:04 - loss: 3.1573 - accuracy: 0.16 - ETA: 2:04 - loss: 3.1577 - accuracy: 0.16 - ETA: 2:03 - loss: 3.1559 - accuracy: 0.16 - ETA: 2:03 - loss: 3.1550 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1552 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1545 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1533 - accuracy: 0.16 - ETA: 2:00 - loss: 3.1524 - accuracy: 0.16 - ETA: 2:00 - loss: 3.1513 - accuracy: 0.16 - ETA: 1:59 - loss: 3.1531 - accuracy: 0.16 - ETA: 1:58 - loss: 3.1542 - accuracy: 0.16 - ETA: 1:58 - loss: 3.1535 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1531 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1544 - accuracy: 0.16 - ETA: 1:56 - loss: 3.1551 - accuracy: 0.16 - ETA: 1:56 - loss: 3.1554 - accuracy: 0.16 - ETA: 1:55 - loss: 3.1531 - accuracy: 0.16 - ETA: 1:55 - loss: 3.1535 - accuracy: 0.16 - ETA: 1:54 - loss: 3.1541 - accuracy: 0.16 - ETA: 1:53 - loss: 3.1529 - accuracy: 0.16 - ETA: 1:53 - loss: 3.1528 - accuracy: 0.16 - ETA: 1:52 - loss: 3.1526 - accuracy: 0.16 - ETA: 1:52 - loss: 3.1538 - accuracy: 0.16 - ETA: 1:51 - loss: 3.1525 - accuracy: 0.16 - ETA: 1:50 - loss: 3.1538 - accuracy: 0.16 - ETA: 1:50 - loss: 3.1543 - accuracy: 0.16 - ETA: 1:49 - loss: 3.1547 - accuracy: 0.16 - ETA: 1:49 - loss: 3.1533 - accuracy: 0.16 - ETA: 1:48 - loss: 3.1527 - accuracy: 0.16 - ETA: 1:47 - loss: 3.1522 - accuracy: 0.16 - ETA: 1:47 - loss: 3.1526 - accuracy: 0.16 - ETA: 1:46 - loss: 3.1532 - accuracy: 0.16 - ETA: 1:46 - loss: 3.1521 - accuracy: 0.16 - ETA: 1:45 - loss: 3.1527 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1512 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1517 - accuracy: 0.16 - ETA: 1:43 - loss: 3.1513 - accuracy: 0.16 - ETA: 1:43 - loss: 3.1508 - accuracy: 0.16 - ETA: 1:42 - loss: 3.1503 - accuracy: 0.16 - ETA: 1:41 - loss: 3.1496 - accuracy: 0.16 - ETA: 1:41 - loss: 3.1499 - accuracy: 0.16 - ETA: 1:40 - loss: 3.1505 - accuracy: 0.16 - ETA: 1:40 - loss: 3.1510 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1513 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1510 - accuracy: 0.16 - ETA: 1:38 - loss: 3.1503 - accuracy: 0.16 - ETA: 1:38 - loss: 3.1512 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1505 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1498 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1519 - accuracy: 0.16 - ETA: 1:35 - loss: 3.1526 - accuracy: 0.16 - ETA: 1:35 - loss: 3.1534 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1536 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1550 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1552 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1551 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1551 - accuracy: 0.16 - ETA: 1:31 - loss: 3.1547 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1553 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1557 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1569 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1569 - accuracy: 0.16 - ETA: 1:28 - loss: 3.1577 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1583 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1587 - accuracy: 0.16 - ETA: 1:26 - loss: 3.1576 - accuracy: 0.16 - ETA: 1:26 - loss: 3.1581 - accuracy: 0.16 - ETA: 1:25 - loss: 3.1588 - accuracy: 0.16 - ETA: 1:24 - loss: 3.1578 - accuracy: 0.1674"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1570 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1573 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1565 - accuracy: 0.16 - ETA: 1:22 - loss: 3.1557 - accuracy: 0.16 - ETA: 1:22 - loss: 3.1559 - accuracy: 0.16 - ETA: 1:21 - loss: 3.1560 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1560 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1562 - accuracy: 0.16 - ETA: 1:19 - loss: 3.1560 - accuracy: 0.16 - ETA: 1:19 - loss: 3.1549 - accuracy: 0.16 - ETA: 1:18 - loss: 3.1544 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1541 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1535 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1532 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1526 - accuracy: 0.16 - ETA: 1:15 - loss: 3.1534 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1529 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1527 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1516 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1511 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1504 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1503 - accuracy: 0.16 - ETA: 1:11 - loss: 3.1499 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1499 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1497 - accuracy: 0.16 - ETA: 1:09 - loss: 3.1490 - accuracy: 0.16 - ETA: 1:09 - loss: 3.1493 - accuracy: 0.16 - ETA: 1:08 - loss: 3.1488 - accuracy: 0.16 - ETA: 1:07 - loss: 3.1481 - accuracy: 0.16 - ETA: 1:07 - loss: 3.1476 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1477 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1473 - accuracy: 0.16 - ETA: 1:05 - loss: 3.1466 - accuracy: 0.16 - ETA: 1:05 - loss: 3.1465 - accuracy: 0.16 - ETA: 1:04 - loss: 3.1459 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1467 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1468 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1465 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1469 - accuracy: 0.16 - ETA: 1:01 - loss: 3.1470 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1481 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1484 - accuracy: 0.16 - ETA: 59s - loss: 3.1481 - accuracy: 0.1691 - ETA: 59s - loss: 3.1477 - accuracy: 0.169 - ETA: 58s - loss: 3.1471 - accuracy: 0.169 - ETA: 57s - loss: 3.1465 - accuracy: 0.169 - ETA: 57s - loss: 3.1462 - accuracy: 0.169 - ETA: 56s - loss: 3.1458 - accuracy: 0.169 - ETA: 56s - loss: 3.1459 - accuracy: 0.169 - ETA: 55s - loss: 3.1456 - accuracy: 0.169 - ETA: 55s - loss: 3.1461 - accuracy: 0.169 - ETA: 54s - loss: 3.1462 - accuracy: 0.169 - ETA: 53s - loss: 3.1459 - accuracy: 0.169 - ETA: 53s - loss: 3.1457 - accuracy: 0.169 - ETA: 52s - loss: 3.1459 - accuracy: 0.169 - ETA: 52s - loss: 3.1453 - accuracy: 0.169 - ETA: 51s - loss: 3.1458 - accuracy: 0.169 - ETA: 50s - loss: 3.1454 - accuracy: 0.169 - ETA: 50s - loss: 3.1457 - accuracy: 0.169 - ETA: 49s - loss: 3.1460 - accuracy: 0.169 - ETA: 49s - loss: 3.1459 - accuracy: 0.169 - ETA: 48s - loss: 3.1455 - accuracy: 0.169 - ETA: 48s - loss: 3.1451 - accuracy: 0.169 - ETA: 47s - loss: 3.1452 - accuracy: 0.169 - ETA: 46s - loss: 3.1454 - accuracy: 0.169 - ETA: 46s - loss: 3.1447 - accuracy: 0.169 - ETA: 45s - loss: 3.1458 - accuracy: 0.168 - ETA: 45s - loss: 3.1449 - accuracy: 0.169 - ETA: 44s - loss: 3.1449 - accuracy: 0.169 - ETA: 44s - loss: 3.1456 - accuracy: 0.169 - ETA: 43s - loss: 3.1447 - accuracy: 0.169 - ETA: 42s - loss: 3.1443 - accuracy: 0.169 - ETA: 42s - loss: 3.1445 - accuracy: 0.169 - ETA: 41s - loss: 3.1445 - accuracy: 0.169 - ETA: 41s - loss: 3.1441 - accuracy: 0.168 - ETA: 40s - loss: 3.1446 - accuracy: 0.168 - ETA: 39s - loss: 3.1451 - accuracy: 0.168 - ETA: 39s - loss: 3.1448 - accuracy: 0.168 - ETA: 38s - loss: 3.1449 - accuracy: 0.168 - ETA: 38s - loss: 3.1456 - accuracy: 0.168 - ETA: 37s - loss: 3.1456 - accuracy: 0.168 - ETA: 36s - loss: 3.1451 - accuracy: 0.168 - ETA: 36s - loss: 3.1460 - accuracy: 0.168 - ETA: 35s - loss: 3.1458 - accuracy: 0.168 - ETA: 35s - loss: 3.1447 - accuracy: 0.168 - ETA: 34s - loss: 3.1439 - accuracy: 0.168 - ETA: 34s - loss: 3.1439 - accuracy: 0.168 - ETA: 33s - loss: 3.1438 - accuracy: 0.168 - ETA: 32s - loss: 3.1438 - accuracy: 0.168 - ETA: 32s - loss: 3.1435 - accuracy: 0.168 - ETA: 31s - loss: 3.1435 - accuracy: 0.168 - ETA: 31s - loss: 3.1433 - accuracy: 0.168 - ETA: 30s - loss: 3.1429 - accuracy: 0.168 - ETA: 29s - loss: 3.1432 - accuracy: 0.168 - ETA: 29s - loss: 3.1432 - accuracy: 0.168 - ETA: 28s - loss: 3.1437 - accuracy: 0.168 - ETA: 28s - loss: 3.1437 - accuracy: 0.168 - ETA: 27s - loss: 3.1433 - accuracy: 0.168 - ETA: 26s - loss: 3.1433 - accuracy: 0.168 - ETA: 26s - loss: 3.1428 - accuracy: 0.168 - ETA: 25s - loss: 3.1429 - accuracy: 0.168 - ETA: 25s - loss: 3.1436 - accuracy: 0.168 - ETA: 24s - loss: 3.1437 - accuracy: 0.168 - ETA: 24s - loss: 3.1440 - accuracy: 0.168 - ETA: 23s - loss: 3.1438 - accuracy: 0.168 - ETA: 22s - loss: 3.1440 - accuracy: 0.168 - ETA: 22s - loss: 3.1442 - accuracy: 0.168 - ETA: 21s - loss: 3.1433 - accuracy: 0.168 - ETA: 21s - loss: 3.1439 - accuracy: 0.168 - ETA: 20s - loss: 3.1440 - accuracy: 0.168 - ETA: 19s - loss: 3.1439 - accuracy: 0.168 - ETA: 19s - loss: 3.1445 - accuracy: 0.168 - ETA: 18s - loss: 3.1449 - accuracy: 0.168 - ETA: 18s - loss: 3.1446 - accuracy: 0.168 - ETA: 17s - loss: 3.1444 - accuracy: 0.168 - ETA: 17s - loss: 3.1440 - accuracy: 0.168 - ETA: 16s - loss: 3.1445 - accuracy: 0.168 - ETA: 15s - loss: 3.1442 - accuracy: 0.168 - ETA: 15s - loss: 3.1445 - accuracy: 0.168 - ETA: 14s - loss: 3.1446 - accuracy: 0.168 - ETA: 14s - loss: 3.1449 - accuracy: 0.168 - ETA: 13s - loss: 3.1447 - accuracy: 0.168 - ETA: 12s - loss: 3.1452 - accuracy: 0.168 - ETA: 12s - loss: 3.1449 - accuracy: 0.168 - ETA: 11s - loss: 3.1447 - accuracy: 0.168 - ETA: 11s - loss: 3.1446 - accuracy: 0.168 - ETA: 10s - loss: 3.1451 - accuracy: 0.168 - ETA: 10s - loss: 3.1449 - accuracy: 0.168 - ETA: 9s - loss: 3.1454 - accuracy: 0.168 - ETA: 8s - loss: 3.1450 - accuracy: 0.16 - ETA: 8s - loss: 3.1454 - accuracy: 0.16 - ETA: 7s - loss: 3.1456 - accuracy: 0.16 - ETA: 7s - loss: 3.1455 - accuracy: 0.16 - ETA: 6s - loss: 3.1454 - accuracy: 0.16 - ETA: 5s - loss: 3.1448 - accuracy: 0.16 - ETA: 5s - loss: 3.1443 - accuracy: 0.16 - ETA: 4s - loss: 3.1441 - accuracy: 0.16 - ETA: 4s - loss: 3.1442 - accuracy: 0.16 - ETA: 3s - loss: 3.1447 - accuracy: 0.16 - ETA: 2s - loss: 3.1440 - accuracy: 0.16 - ETA: 2s - loss: 3.1445 - accuracy: 0.16 - ETA: 1s - loss: 3.1447 - accuracy: 0.16 - ETA: 1s - loss: 3.1448 - accuracy: 0.16 - ETA: 0s - loss: 3.1439 - accuracy: 0.16 - ETA: 0s - loss: 3.1438 - accuracy: 0.16 - 206s 5ms/step - loss: 3.1439 - accuracy: 0.1689 - val_loss: 4.1188 - val_accuracy: 0.0413\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:28 - loss: 3.2920 - accuracy: 0.15 - ETA: 3:17 - loss: 3.1075 - accuracy: 0.17 - ETA: 3:11 - loss: 3.0594 - accuracy: 0.17 - ETA: 3:10 - loss: 3.0870 - accuracy: 0.16 - ETA: 3:10 - loss: 3.0984 - accuracy: 0.15 - ETA: 3:10 - loss: 3.0892 - accuracy: 0.15 - ETA: 3:08 - loss: 3.1160 - accuracy: 0.15 - ETA: 3:11 - loss: 3.0963 - accuracy: 0.16 - ETA: 3:09 - loss: 3.1129 - accuracy: 0.16 - ETA: 3:08 - loss: 3.1088 - accuracy: 0.16 - ETA: 3:08 - loss: 3.1195 - accuracy: 0.16 - ETA: 3:07 - loss: 3.1472 - accuracy: 0.15 - ETA: 3:06 - loss: 3.1486 - accuracy: 0.16 - ETA: 3:05 - loss: 3.1457 - accuracy: 0.16 - ETA: 3:05 - loss: 3.1427 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1318 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1378 - accuracy: 0.16 - ETA: 3:03 - loss: 3.1387 - accuracy: 0.16 - ETA: 3:02 - loss: 3.1359 - accuracy: 0.16 - ETA: 3:02 - loss: 3.1361 - accuracy: 0.16 - ETA: 3:01 - loss: 3.1332 - accuracy: 0.16 - ETA: 3:00 - loss: 3.1262 - accuracy: 0.16 - ETA: 2:59 - loss: 3.1290 - accuracy: 0.16 - ETA: 2:58 - loss: 3.1244 - accuracy: 0.16 - ETA: 2:58 - loss: 3.1155 - accuracy: 0.16 - ETA: 2:58 - loss: 3.1157 - accuracy: 0.16 - ETA: 2:58 - loss: 3.1135 - accuracy: 0.16 - ETA: 2:57 - loss: 3.1196 - accuracy: 0.16 - ETA: 2:57 - loss: 3.1220 - accuracy: 0.16 - ETA: 2:56 - loss: 3.1254 - accuracy: 0.16 - ETA: 2:55 - loss: 3.1215 - accuracy: 0.16 - ETA: 2:55 - loss: 3.1290 - accuracy: 0.16 - ETA: 2:54 - loss: 3.1273 - accuracy: 0.16 - ETA: 2:54 - loss: 3.1229 - accuracy: 0.16 - ETA: 2:53 - loss: 3.1282 - accuracy: 0.16 - ETA: 2:52 - loss: 3.1315 - accuracy: 0.16 - ETA: 2:51 - loss: 3.1325 - accuracy: 0.16 - ETA: 2:51 - loss: 3.1306 - accuracy: 0.16 - ETA: 2:50 - loss: 3.1295 - accuracy: 0.16 - ETA: 2:49 - loss: 3.1333 - accuracy: 0.16 - ETA: 2:49 - loss: 3.1315 - accuracy: 0.16 - ETA: 2:48 - loss: 3.1372 - accuracy: 0.16 - ETA: 2:48 - loss: 3.1424 - accuracy: 0.16 - ETA: 2:47 - loss: 3.1461 - accuracy: 0.16 - ETA: 2:46 - loss: 3.1456 - accuracy: 0.16 - ETA: 2:46 - loss: 3.1446 - accuracy: 0.16 - ETA: 2:45 - loss: 3.1438 - accuracy: 0.16 - ETA: 2:44 - loss: 3.1422 - accuracy: 0.16 - ETA: 2:44 - loss: 3.1486 - accuracy: 0.16 - ETA: 2:43 - loss: 3.1509 - accuracy: 0.16 - ETA: 2:43 - loss: 3.1466 - accuracy: 0.16 - ETA: 2:42 - loss: 3.1455 - accuracy: 0.16 - ETA: 2:42 - loss: 3.1465 - accuracy: 0.16 - ETA: 2:41 - loss: 3.1495 - accuracy: 0.16 - ETA: 2:41 - loss: 3.1478 - accuracy: 0.16 - ETA: 2:40 - loss: 3.1465 - accuracy: 0.16 - ETA: 2:40 - loss: 3.1455 - accuracy: 0.16 - ETA: 2:39 - loss: 3.1479 - accuracy: 0.16 - ETA: 2:39 - loss: 3.1452 - accuracy: 0.16 - ETA: 2:38 - loss: 3.1459 - accuracy: 0.16 - ETA: 2:38 - loss: 3.1437 - accuracy: 0.16 - ETA: 2:37 - loss: 3.1422 - accuracy: 0.16 - ETA: 2:36 - loss: 3.1433 - accuracy: 0.16 - ETA: 2:36 - loss: 3.1462 - accuracy: 0.16 - ETA: 2:35 - loss: 3.1448 - accuracy: 0.16 - ETA: 2:34 - loss: 3.1453 - accuracy: 0.16 - ETA: 2:34 - loss: 3.1438 - accuracy: 0.16 - ETA: 2:33 - loss: 3.1445 - accuracy: 0.16 - ETA: 2:33 - loss: 3.1457 - accuracy: 0.16 - ETA: 2:32 - loss: 3.1459 - accuracy: 0.16 - ETA: 2:31 - loss: 3.1471 - accuracy: 0.16 - ETA: 2:31 - loss: 3.1475 - accuracy: 0.16 - ETA: 2:30 - loss: 3.1475 - accuracy: 0.16 - ETA: 2:29 - loss: 3.1481 - accuracy: 0.16 - ETA: 2:29 - loss: 3.1499 - accuracy: 0.16 - ETA: 2:28 - loss: 3.1490 - accuracy: 0.16 - ETA: 2:28 - loss: 3.1548 - accuracy: 0.16 - ETA: 2:27 - loss: 3.1550 - accuracy: 0.16 - ETA: 2:27 - loss: 3.1553 - accuracy: 0.16 - ETA: 2:26 - loss: 3.1560 - accuracy: 0.16 - ETA: 2:25 - loss: 3.1537 - accuracy: 0.16 - ETA: 2:25 - loss: 3.1525 - accuracy: 0.16 - ETA: 2:25 - loss: 3.1521 - accuracy: 0.16 - ETA: 2:24 - loss: 3.1534 - accuracy: 0.16 - ETA: 2:23 - loss: 3.1539 - accuracy: 0.16 - ETA: 2:23 - loss: 3.1536 - accuracy: 0.16 - ETA: 2:22 - loss: 3.1545 - accuracy: 0.16 - ETA: 2:22 - loss: 3.1526 - accuracy: 0.16 - ETA: 2:21 - loss: 3.1523 - accuracy: 0.16 - ETA: 2:20 - loss: 3.1513 - accuracy: 0.16 - ETA: 2:20 - loss: 3.1524 - accuracy: 0.16 - ETA: 2:19 - loss: 3.1532 - accuracy: 0.16 - ETA: 2:19 - loss: 3.1543 - accuracy: 0.16 - ETA: 2:18 - loss: 3.1545 - accuracy: 0.16 - ETA: 2:18 - loss: 3.1539 - accuracy: 0.16 - ETA: 2:17 - loss: 3.1567 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1554 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1579 - accuracy: 0.16 - ETA: 2:15 - loss: 3.1597 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1623 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1659 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1643 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1633 - accuracy: 0.16 - ETA: 2:12 - loss: 3.1647 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1656 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1662 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1636 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1645 - accuracy: 0.16 - ETA: 2:09 - loss: 3.1629 - accuracy: 0.16 - ETA: 2:09 - loss: 3.1625 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1615 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1608 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1610 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1614 - accuracy: 0.16 - ETA: 2:06 - loss: 3.1615 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1612 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1601 - accuracy: 0.16 - ETA: 2:04 - loss: 3.1609 - accuracy: 0.16 - ETA: 2:04 - loss: 3.1607 - accuracy: 0.16 - ETA: 2:03 - loss: 3.1605 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1603 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1607 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1606 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1615 - accuracy: 0.16 - ETA: 2:00 - loss: 3.1599 - accuracy: 0.16 - ETA: 2:00 - loss: 3.1605 - accuracy: 0.16 - ETA: 1:59 - loss: 3.1619 - accuracy: 0.16 - ETA: 1:58 - loss: 3.1617 - accuracy: 0.16 - ETA: 1:58 - loss: 3.1618 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1610 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1609 - accuracy: 0.16 - ETA: 1:56 - loss: 3.1600 - accuracy: 0.16 - ETA: 1:55 - loss: 3.1596 - accuracy: 0.16 - ETA: 1:55 - loss: 3.1607 - accuracy: 0.16 - ETA: 1:54 - loss: 3.1606 - accuracy: 0.16 - ETA: 1:54 - loss: 3.1675 - accuracy: 0.16 - ETA: 1:53 - loss: 3.1668 - accuracy: 0.16 - ETA: 1:53 - loss: 3.1656 - accuracy: 0.16 - ETA: 1:52 - loss: 3.1666 - accuracy: 0.16 - ETA: 1:52 - loss: 3.1672 - accuracy: 0.16 - ETA: 1:51 - loss: 3.1672 - accuracy: 0.16 - ETA: 1:50 - loss: 3.1663 - accuracy: 0.16 - ETA: 1:50 - loss: 3.1659 - accuracy: 0.16 - ETA: 1:49 - loss: 3.1648 - accuracy: 0.16 - ETA: 1:49 - loss: 3.1653 - accuracy: 0.16 - ETA: 1:48 - loss: 3.1652 - accuracy: 0.16 - ETA: 1:47 - loss: 3.1652 - accuracy: 0.16 - ETA: 1:47 - loss: 3.1649 - accuracy: 0.16 - ETA: 1:46 - loss: 3.1644 - accuracy: 0.16 - ETA: 1:46 - loss: 3.1650 - accuracy: 0.16 - ETA: 1:45 - loss: 3.1660 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1651 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1647 - accuracy: 0.16 - ETA: 1:43 - loss: 3.1652 - accuracy: 0.16 - ETA: 1:43 - loss: 3.1648 - accuracy: 0.16 - ETA: 1:42 - loss: 3.1643 - accuracy: 0.16 - ETA: 1:41 - loss: 3.1642 - accuracy: 0.16 - ETA: 1:41 - loss: 3.1626 - accuracy: 0.16 - ETA: 1:40 - loss: 3.1623 - accuracy: 0.16 - ETA: 1:40 - loss: 3.1627 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1620 - accuracy: 0.16 - ETA: 1:38 - loss: 3.1619 - accuracy: 0.16 - ETA: 1:38 - loss: 3.1608 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1605 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1597 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1586 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1584 - accuracy: 0.16 - ETA: 1:35 - loss: 3.1581 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1589 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1581 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1594 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1599 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1589 - accuracy: 0.16 - ETA: 1:31 - loss: 3.1589 - accuracy: 0.16 - ETA: 1:31 - loss: 3.1587 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1561 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1551 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1546 - accuracy: 0.16 - ETA: 1:28 - loss: 3.1542 - accuracy: 0.16 - ETA: 1:28 - loss: 3.1538 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1533 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1536 - accuracy: 0.16 - ETA: 1:26 - loss: 3.1532 - accuracy: 0.16 - ETA: 1:25 - loss: 3.1523 - accuracy: 0.16 - ETA: 1:25 - loss: 3.1515 - accuracy: 0.16 - ETA: 1:24 - loss: 3.1509 - accuracy: 0.1686"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1506 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1492 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1497 - accuracy: 0.16 - ETA: 1:22 - loss: 3.1497 - accuracy: 0.16 - ETA: 1:21 - loss: 3.1500 - accuracy: 0.16 - ETA: 1:21 - loss: 3.1507 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1504 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1506 - accuracy: 0.16 - ETA: 1:19 - loss: 3.1509 - accuracy: 0.16 - ETA: 1:19 - loss: 3.1502 - accuracy: 0.16 - ETA: 1:18 - loss: 3.1504 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1504 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1504 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1508 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1507 - accuracy: 0.16 - ETA: 1:15 - loss: 3.1523 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1511 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1512 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1513 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1515 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1512 - accuracy: 0.16 - ETA: 1:11 - loss: 3.1521 - accuracy: 0.16 - ETA: 1:11 - loss: 3.1522 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1534 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1524 - accuracy: 0.16 - ETA: 1:09 - loss: 3.1524 - accuracy: 0.16 - ETA: 1:08 - loss: 3.1523 - accuracy: 0.16 - ETA: 1:08 - loss: 3.1520 - accuracy: 0.16 - ETA: 1:07 - loss: 3.1517 - accuracy: 0.16 - ETA: 1:07 - loss: 3.1518 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1509 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1510 - accuracy: 0.16 - ETA: 1:05 - loss: 3.1508 - accuracy: 0.16 - ETA: 1:04 - loss: 3.1508 - accuracy: 0.16 - ETA: 1:04 - loss: 3.1506 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1505 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1509 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1515 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1515 - accuracy: 0.16 - ETA: 1:01 - loss: 3.1526 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1521 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1518 - accuracy: 0.16 - ETA: 59s - loss: 3.1512 - accuracy: 0.1682 - ETA: 59s - loss: 3.1518 - accuracy: 0.168 - ETA: 58s - loss: 3.1519 - accuracy: 0.167 - ETA: 57s - loss: 3.1520 - accuracy: 0.167 - ETA: 57s - loss: 3.1519 - accuracy: 0.167 - ETA: 56s - loss: 3.1521 - accuracy: 0.167 - ETA: 56s - loss: 3.1517 - accuracy: 0.167 - ETA: 55s - loss: 3.1516 - accuracy: 0.168 - ETA: 55s - loss: 3.1515 - accuracy: 0.168 - ETA: 54s - loss: 3.1512 - accuracy: 0.168 - ETA: 53s - loss: 3.1515 - accuracy: 0.168 - ETA: 53s - loss: 3.1519 - accuracy: 0.168 - ETA: 52s - loss: 3.1519 - accuracy: 0.168 - ETA: 52s - loss: 3.1527 - accuracy: 0.167 - ETA: 51s - loss: 3.1527 - accuracy: 0.167 - ETA: 50s - loss: 3.1527 - accuracy: 0.167 - ETA: 50s - loss: 3.1529 - accuracy: 0.167 - ETA: 49s - loss: 3.1533 - accuracy: 0.167 - ETA: 49s - loss: 3.1532 - accuracy: 0.167 - ETA: 48s - loss: 3.1533 - accuracy: 0.167 - ETA: 48s - loss: 3.1534 - accuracy: 0.167 - ETA: 47s - loss: 3.1542 - accuracy: 0.167 - ETA: 47s - loss: 3.1552 - accuracy: 0.167 - ETA: 46s - loss: 3.1557 - accuracy: 0.167 - ETA: 45s - loss: 3.1557 - accuracy: 0.167 - ETA: 45s - loss: 3.1559 - accuracy: 0.167 - ETA: 44s - loss: 3.1563 - accuracy: 0.167 - ETA: 44s - loss: 3.1560 - accuracy: 0.167 - ETA: 43s - loss: 3.1557 - accuracy: 0.167 - ETA: 42s - loss: 3.1558 - accuracy: 0.167 - ETA: 42s - loss: 3.1559 - accuracy: 0.167 - ETA: 41s - loss: 3.1563 - accuracy: 0.167 - ETA: 41s - loss: 3.1563 - accuracy: 0.167 - ETA: 40s - loss: 3.1559 - accuracy: 0.167 - ETA: 40s - loss: 3.1562 - accuracy: 0.167 - ETA: 39s - loss: 3.1561 - accuracy: 0.167 - ETA: 38s - loss: 3.1563 - accuracy: 0.167 - ETA: 38s - loss: 3.1563 - accuracy: 0.167 - ETA: 37s - loss: 3.1569 - accuracy: 0.167 - ETA: 37s - loss: 3.1576 - accuracy: 0.167 - ETA: 36s - loss: 3.1580 - accuracy: 0.167 - ETA: 35s - loss: 3.1574 - accuracy: 0.167 - ETA: 35s - loss: 3.1574 - accuracy: 0.167 - ETA: 34s - loss: 3.1572 - accuracy: 0.167 - ETA: 34s - loss: 3.1574 - accuracy: 0.167 - ETA: 33s - loss: 3.1575 - accuracy: 0.167 - ETA: 33s - loss: 3.1582 - accuracy: 0.167 - ETA: 32s - loss: 3.1585 - accuracy: 0.167 - ETA: 31s - loss: 3.1582 - accuracy: 0.167 - ETA: 31s - loss: 3.1585 - accuracy: 0.167 - ETA: 30s - loss: 3.1588 - accuracy: 0.167 - ETA: 30s - loss: 3.1588 - accuracy: 0.167 - ETA: 29s - loss: 3.1592 - accuracy: 0.167 - ETA: 28s - loss: 3.1602 - accuracy: 0.167 - ETA: 28s - loss: 3.1597 - accuracy: 0.167 - ETA: 27s - loss: 3.1596 - accuracy: 0.167 - ETA: 27s - loss: 3.1601 - accuracy: 0.167 - ETA: 26s - loss: 3.1600 - accuracy: 0.167 - ETA: 25s - loss: 3.1601 - accuracy: 0.167 - ETA: 25s - loss: 3.1605 - accuracy: 0.167 - ETA: 24s - loss: 3.1607 - accuracy: 0.167 - ETA: 24s - loss: 3.1608 - accuracy: 0.167 - ETA: 23s - loss: 3.1606 - accuracy: 0.167 - ETA: 22s - loss: 3.1601 - accuracy: 0.167 - ETA: 22s - loss: 3.1612 - accuracy: 0.167 - ETA: 21s - loss: 3.1614 - accuracy: 0.167 - ETA: 21s - loss: 3.1611 - accuracy: 0.167 - ETA: 20s - loss: 3.1603 - accuracy: 0.167 - ETA: 20s - loss: 3.1598 - accuracy: 0.167 - ETA: 19s - loss: 3.1598 - accuracy: 0.167 - ETA: 18s - loss: 3.1593 - accuracy: 0.167 - ETA: 18s - loss: 3.1590 - accuracy: 0.167 - ETA: 17s - loss: 3.1591 - accuracy: 0.167 - ETA: 17s - loss: 3.1588 - accuracy: 0.167 - ETA: 16s - loss: 3.1591 - accuracy: 0.167 - ETA: 15s - loss: 3.1590 - accuracy: 0.167 - ETA: 15s - loss: 3.1589 - accuracy: 0.167 - ETA: 14s - loss: 3.1583 - accuracy: 0.167 - ETA: 14s - loss: 3.1585 - accuracy: 0.167 - ETA: 13s - loss: 3.1581 - accuracy: 0.167 - ETA: 12s - loss: 3.1581 - accuracy: 0.167 - ETA: 12s - loss: 3.1572 - accuracy: 0.167 - ETA: 11s - loss: 3.1577 - accuracy: 0.167 - ETA: 11s - loss: 3.1570 - accuracy: 0.167 - ETA: 10s - loss: 3.1565 - accuracy: 0.168 - ETA: 10s - loss: 3.1566 - accuracy: 0.168 - ETA: 9s - loss: 3.1563 - accuracy: 0.168 - ETA: 8s - loss: 3.1566 - accuracy: 0.16 - ETA: 8s - loss: 3.1563 - accuracy: 0.16 - ETA: 7s - loss: 3.1566 - accuracy: 0.16 - ETA: 7s - loss: 3.1565 - accuracy: 0.16 - ETA: 6s - loss: 3.1566 - accuracy: 0.16 - ETA: 5s - loss: 3.1568 - accuracy: 0.16 - ETA: 5s - loss: 3.1565 - accuracy: 0.16 - ETA: 4s - loss: 3.1565 - accuracy: 0.16 - ETA: 4s - loss: 3.1562 - accuracy: 0.16 - ETA: 3s - loss: 3.1562 - accuracy: 0.16 - ETA: 2s - loss: 3.1558 - accuracy: 0.16 - ETA: 2s - loss: 3.1555 - accuracy: 0.16 - ETA: 1s - loss: 3.1558 - accuracy: 0.16 - ETA: 1s - loss: 3.1552 - accuracy: 0.16 - ETA: 0s - loss: 3.1544 - accuracy: 0.16 - ETA: 0s - loss: 3.1541 - accuracy: 0.16 - 207s 5ms/step - loss: 3.1542 - accuracy: 0.1682 - val_loss: 4.0294 - val_accuracy: 0.0454\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:44 - loss: 3.0851 - accuracy: 0.14 - ETA: 3:37 - loss: 3.1483 - accuracy: 0.15 - ETA: 3:35 - loss: 3.1486 - accuracy: 0.16 - ETA: 3:32 - loss: 3.3264 - accuracy: 0.16 - ETA: 3:28 - loss: 3.2940 - accuracy: 0.17 - ETA: 3:26 - loss: 3.2920 - accuracy: 0.16 - ETA: 3:24 - loss: 3.2742 - accuracy: 0.16 - ETA: 3:23 - loss: 3.2536 - accuracy: 0.16 - ETA: 3:22 - loss: 3.2570 - accuracy: 0.16 - ETA: 3:21 - loss: 3.2515 - accuracy: 0.16 - ETA: 3:20 - loss: 3.2445 - accuracy: 0.15 - ETA: 3:19 - loss: 3.2461 - accuracy: 0.15 - ETA: 3:18 - loss: 3.2294 - accuracy: 0.15 - ETA: 3:16 - loss: 3.2304 - accuracy: 0.15 - ETA: 3:16 - loss: 3.2317 - accuracy: 0.15 - ETA: 3:15 - loss: 3.2293 - accuracy: 0.15 - ETA: 3:13 - loss: 3.2283 - accuracy: 0.15 - ETA: 3:12 - loss: 3.2237 - accuracy: 0.16 - ETA: 3:11 - loss: 3.2268 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2266 - accuracy: 0.15 - ETA: 3:09 - loss: 3.2198 - accuracy: 0.15 - ETA: 3:08 - loss: 3.2080 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2059 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2040 - accuracy: 0.15 - ETA: 3:04 - loss: 3.1977 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1912 - accuracy: 0.16 - ETA: 3:03 - loss: 3.1884 - accuracy: 0.16 - ETA: 3:03 - loss: 3.1805 - accuracy: 0.16 - ETA: 3:03 - loss: 3.1736 - accuracy: 0.16 - ETA: 3:02 - loss: 3.1746 - accuracy: 0.16 - ETA: 3:01 - loss: 3.1742 - accuracy: 0.16 - ETA: 3:01 - loss: 3.1692 - accuracy: 0.16 - ETA: 3:00 - loss: 3.1627 - accuracy: 0.16 - ETA: 2:59 - loss: 3.1631 - accuracy: 0.16 - ETA: 2:58 - loss: 3.1618 - accuracy: 0.16 - ETA: 2:57 - loss: 3.1608 - accuracy: 0.16 - ETA: 2:56 - loss: 3.1584 - accuracy: 0.16 - ETA: 2:56 - loss: 3.1651 - accuracy: 0.16 - ETA: 2:55 - loss: 3.1616 - accuracy: 0.16 - ETA: 2:54 - loss: 3.1615 - accuracy: 0.16 - ETA: 2:53 - loss: 3.1646 - accuracy: 0.16 - ETA: 2:52 - loss: 3.1623 - accuracy: 0.16 - ETA: 2:51 - loss: 3.1599 - accuracy: 0.16 - ETA: 2:51 - loss: 3.1556 - accuracy: 0.16 - ETA: 2:50 - loss: 3.1566 - accuracy: 0.16 - ETA: 2:50 - loss: 3.1574 - accuracy: 0.16 - ETA: 2:49 - loss: 3.1591 - accuracy: 0.16 - ETA: 2:48 - loss: 3.1620 - accuracy: 0.16 - ETA: 2:47 - loss: 3.1623 - accuracy: 0.16 - ETA: 2:46 - loss: 3.1614 - accuracy: 0.16 - ETA: 2:46 - loss: 3.1598 - accuracy: 0.16 - ETA: 2:45 - loss: 3.1574 - accuracy: 0.16 - ETA: 2:44 - loss: 3.1519 - accuracy: 0.16 - ETA: 2:44 - loss: 3.1572 - accuracy: 0.16 - ETA: 2:44 - loss: 3.1553 - accuracy: 0.16 - ETA: 2:43 - loss: 3.1582 - accuracy: 0.16 - ETA: 2:43 - loss: 3.1571 - accuracy: 0.16 - ETA: 2:42 - loss: 3.1595 - accuracy: 0.16 - ETA: 2:41 - loss: 3.1600 - accuracy: 0.16 - ETA: 2:40 - loss: 3.1619 - accuracy: 0.16 - ETA: 2:40 - loss: 3.1621 - accuracy: 0.16 - ETA: 2:39 - loss: 3.1614 - accuracy: 0.16 - ETA: 2:39 - loss: 3.1616 - accuracy: 0.16 - ETA: 2:38 - loss: 3.1625 - accuracy: 0.16 - ETA: 2:37 - loss: 3.1608 - accuracy: 0.16 - ETA: 2:37 - loss: 3.1631 - accuracy: 0.16 - ETA: 2:36 - loss: 3.1625 - accuracy: 0.16 - ETA: 2:35 - loss: 3.1606 - accuracy: 0.16 - ETA: 2:34 - loss: 3.1597 - accuracy: 0.16 - ETA: 2:34 - loss: 3.1598 - accuracy: 0.16 - ETA: 2:33 - loss: 3.1618 - accuracy: 0.16 - ETA: 2:33 - loss: 3.1637 - accuracy: 0.16 - ETA: 2:32 - loss: 3.1655 - accuracy: 0.16 - ETA: 2:31 - loss: 3.1642 - accuracy: 0.16 - ETA: 2:31 - loss: 3.1647 - accuracy: 0.16 - ETA: 2:30 - loss: 3.1626 - accuracy: 0.16 - ETA: 2:29 - loss: 3.1630 - accuracy: 0.16 - ETA: 2:29 - loss: 3.1619 - accuracy: 0.16 - ETA: 2:28 - loss: 3.1607 - accuracy: 0.16 - ETA: 2:28 - loss: 3.1601 - accuracy: 0.16 - ETA: 2:27 - loss: 3.1630 - accuracy: 0.16 - ETA: 2:27 - loss: 3.1621 - accuracy: 0.16 - ETA: 2:26 - loss: 3.1605 - accuracy: 0.16 - ETA: 2:26 - loss: 3.1596 - accuracy: 0.16 - ETA: 2:25 - loss: 3.1622 - accuracy: 0.16 - ETA: 2:24 - loss: 3.1633 - accuracy: 0.16 - ETA: 2:24 - loss: 3.1629 - accuracy: 0.16 - ETA: 2:23 - loss: 3.1595 - accuracy: 0.16 - ETA: 2:22 - loss: 3.1604 - accuracy: 0.16 - ETA: 2:22 - loss: 3.1601 - accuracy: 0.16 - ETA: 2:21 - loss: 3.1594 - accuracy: 0.16 - ETA: 2:21 - loss: 3.1597 - accuracy: 0.16 - ETA: 2:20 - loss: 3.1603 - accuracy: 0.16 - ETA: 2:19 - loss: 3.1602 - accuracy: 0.16 - ETA: 2:19 - loss: 3.1615 - accuracy: 0.16 - ETA: 2:18 - loss: 3.1628 - accuracy: 0.16 - ETA: 2:17 - loss: 3.1624 - accuracy: 0.16 - ETA: 2:17 - loss: 3.1646 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1625 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1630 - accuracy: 0.16 - ETA: 2:15 - loss: 3.1624 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1619 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1610 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1605 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1596 - accuracy: 0.16 - ETA: 2:12 - loss: 3.1590 - accuracy: 0.16 - ETA: 2:12 - loss: 3.1603 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1600 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1592 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1588 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1595 - accuracy: 0.16 - ETA: 2:09 - loss: 3.1602 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1610 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1601 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1597 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1578 - accuracy: 0.16 - ETA: 2:06 - loss: 3.1583 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1582 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1574 - accuracy: 0.16 - ETA: 2:04 - loss: 3.1576 - accuracy: 0.16 - ETA: 2:03 - loss: 3.1575 - accuracy: 0.16 - ETA: 2:03 - loss: 3.1587 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1582 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1574 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1582 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1582 - accuracy: 0.16 - ETA: 2:00 - loss: 3.1590 - accuracy: 0.16 - ETA: 1:59 - loss: 3.1583 - accuracy: 0.16 - ETA: 1:59 - loss: 3.1579 - accuracy: 0.16 - ETA: 1:58 - loss: 3.1567 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1559 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1560 - accuracy: 0.16 - ETA: 1:56 - loss: 3.1554 - accuracy: 0.16 - ETA: 1:56 - loss: 3.1541 - accuracy: 0.16 - ETA: 1:55 - loss: 3.1555 - accuracy: 0.16 - ETA: 1:54 - loss: 3.1560 - accuracy: 0.16 - ETA: 1:54 - loss: 3.1556 - accuracy: 0.16 - ETA: 1:53 - loss: 3.1562 - accuracy: 0.16 - ETA: 1:53 - loss: 3.1558 - accuracy: 0.16 - ETA: 1:52 - loss: 3.1557 - accuracy: 0.16 - ETA: 1:52 - loss: 3.1562 - accuracy: 0.16 - ETA: 1:51 - loss: 3.1569 - accuracy: 0.16 - ETA: 1:50 - loss: 3.1565 - accuracy: 0.16 - ETA: 1:50 - loss: 3.1574 - accuracy: 0.16 - ETA: 1:49 - loss: 3.1588 - accuracy: 0.16 - ETA: 1:49 - loss: 3.1595 - accuracy: 0.16 - ETA: 1:48 - loss: 3.1578 - accuracy: 0.16 - ETA: 1:48 - loss: 3.1571 - accuracy: 0.16 - ETA: 1:47 - loss: 3.1559 - accuracy: 0.16 - ETA: 1:46 - loss: 3.1567 - accuracy: 0.16 - ETA: 1:46 - loss: 3.1557 - accuracy: 0.16 - ETA: 1:45 - loss: 3.1557 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1548 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1533 - accuracy: 0.16 - ETA: 1:43 - loss: 3.1539 - accuracy: 0.16 - ETA: 1:43 - loss: 3.1535 - accuracy: 0.16 - ETA: 1:42 - loss: 3.1525 - accuracy: 0.16 - ETA: 1:41 - loss: 3.1515 - accuracy: 0.16 - ETA: 1:41 - loss: 3.1514 - accuracy: 0.16 - ETA: 1:40 - loss: 3.1514 - accuracy: 0.16 - ETA: 1:40 - loss: 3.1506 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1516 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1505 - accuracy: 0.16 - ETA: 1:38 - loss: 3.1494 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1491 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1485 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1474 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1470 - accuracy: 0.16 - ETA: 1:35 - loss: 3.1471 - accuracy: 0.16 - ETA: 1:35 - loss: 3.1471 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1474 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1485 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1479 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1480 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1474 - accuracy: 0.16 - ETA: 1:31 - loss: 3.1476 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1470 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1454 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1471 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1454 - accuracy: 0.16 - ETA: 1:28 - loss: 3.1445 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1436 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1423 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1425 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1415 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1419 - accuracy: 0.1704"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1417 - accuracy: 0.17 - ETA: 1:24 - loss: 3.1418 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1426 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1431 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1439 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1464 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1469 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1476 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1476 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1492 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1505 - accuracy: 0.16 - ETA: 1:18 - loss: 3.1507 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1521 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1521 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1519 - accuracy: 0.16 - ETA: 1:15 - loss: 3.1531 - accuracy: 0.16 - ETA: 1:15 - loss: 3.1536 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1534 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1538 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1532 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1528 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1529 - accuracy: 0.16 - ETA: 1:11 - loss: 3.1525 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1515 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1510 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1512 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1516 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1516 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1511 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1514 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1514 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1519 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1533 - accuracy: 0.16 - ETA: 1:05 - loss: 3.1538 - accuracy: 0.16 - ETA: 1:04 - loss: 3.1547 - accuracy: 0.16 - ETA: 1:04 - loss: 3.1550 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1546 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1549 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1560 - accuracy: 0.16 - ETA: 1:01 - loss: 3.1558 - accuracy: 0.16 - ETA: 1:01 - loss: 3.1558 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1557 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1558 - accuracy: 0.16 - ETA: 59s - loss: 3.1556 - accuracy: 0.1694 - ETA: 58s - loss: 3.1554 - accuracy: 0.169 - ETA: 58s - loss: 3.1562 - accuracy: 0.169 - ETA: 57s - loss: 3.1568 - accuracy: 0.169 - ETA: 57s - loss: 3.1569 - accuracy: 0.168 - ETA: 56s - loss: 3.1569 - accuracy: 0.168 - ETA: 55s - loss: 3.1571 - accuracy: 0.168 - ETA: 55s - loss: 3.1565 - accuracy: 0.169 - ETA: 54s - loss: 3.1557 - accuracy: 0.169 - ETA: 54s - loss: 3.1554 - accuracy: 0.169 - ETA: 53s - loss: 3.1554 - accuracy: 0.169 - ETA: 52s - loss: 3.1549 - accuracy: 0.169 - ETA: 52s - loss: 3.1551 - accuracy: 0.169 - ETA: 51s - loss: 3.1548 - accuracy: 0.169 - ETA: 51s - loss: 3.1551 - accuracy: 0.169 - ETA: 50s - loss: 3.1559 - accuracy: 0.169 - ETA: 50s - loss: 3.1563 - accuracy: 0.169 - ETA: 49s - loss: 3.1571 - accuracy: 0.168 - ETA: 48s - loss: 3.1578 - accuracy: 0.168 - ETA: 48s - loss: 3.1581 - accuracy: 0.168 - ETA: 47s - loss: 3.1583 - accuracy: 0.168 - ETA: 47s - loss: 3.1582 - accuracy: 0.168 - ETA: 46s - loss: 3.1586 - accuracy: 0.168 - ETA: 45s - loss: 3.1575 - accuracy: 0.168 - ETA: 45s - loss: 3.1575 - accuracy: 0.168 - ETA: 44s - loss: 3.1569 - accuracy: 0.168 - ETA: 44s - loss: 3.1572 - accuracy: 0.168 - ETA: 43s - loss: 3.1568 - accuracy: 0.168 - ETA: 42s - loss: 3.1568 - accuracy: 0.168 - ETA: 42s - loss: 3.1559 - accuracy: 0.168 - ETA: 41s - loss: 3.1560 - accuracy: 0.168 - ETA: 41s - loss: 3.1567 - accuracy: 0.168 - ETA: 40s - loss: 3.1564 - accuracy: 0.168 - ETA: 40s - loss: 3.1565 - accuracy: 0.168 - ETA: 39s - loss: 3.1564 - accuracy: 0.168 - ETA: 38s - loss: 3.1564 - accuracy: 0.168 - ETA: 38s - loss: 3.1564 - accuracy: 0.168 - ETA: 37s - loss: 3.1567 - accuracy: 0.168 - ETA: 37s - loss: 3.1573 - accuracy: 0.168 - ETA: 36s - loss: 3.1575 - accuracy: 0.168 - ETA: 35s - loss: 3.1576 - accuracy: 0.168 - ETA: 35s - loss: 3.1571 - accuracy: 0.168 - ETA: 34s - loss: 3.1564 - accuracy: 0.168 - ETA: 34s - loss: 3.1560 - accuracy: 0.168 - ETA: 33s - loss: 3.1561 - accuracy: 0.168 - ETA: 32s - loss: 3.1566 - accuracy: 0.168 - ETA: 32s - loss: 3.1572 - accuracy: 0.168 - ETA: 31s - loss: 3.1566 - accuracy: 0.168 - ETA: 31s - loss: 3.1562 - accuracy: 0.168 - ETA: 30s - loss: 3.1559 - accuracy: 0.168 - ETA: 30s - loss: 3.1555 - accuracy: 0.168 - ETA: 29s - loss: 3.1558 - accuracy: 0.168 - ETA: 28s - loss: 3.1551 - accuracy: 0.168 - ETA: 28s - loss: 3.1551 - accuracy: 0.168 - ETA: 27s - loss: 3.1546 - accuracy: 0.169 - ETA: 27s - loss: 3.1541 - accuracy: 0.169 - ETA: 26s - loss: 3.1539 - accuracy: 0.169 - ETA: 25s - loss: 3.1533 - accuracy: 0.169 - ETA: 25s - loss: 3.1532 - accuracy: 0.169 - ETA: 24s - loss: 3.1530 - accuracy: 0.169 - ETA: 24s - loss: 3.1534 - accuracy: 0.169 - ETA: 23s - loss: 3.1535 - accuracy: 0.169 - ETA: 22s - loss: 3.1527 - accuracy: 0.169 - ETA: 22s - loss: 3.1524 - accuracy: 0.169 - ETA: 21s - loss: 3.1531 - accuracy: 0.169 - ETA: 21s - loss: 3.1534 - accuracy: 0.169 - ETA: 20s - loss: 3.1535 - accuracy: 0.169 - ETA: 20s - loss: 3.1537 - accuracy: 0.169 - ETA: 19s - loss: 3.1531 - accuracy: 0.169 - ETA: 18s - loss: 3.1524 - accuracy: 0.169 - ETA: 18s - loss: 3.1521 - accuracy: 0.169 - ETA: 17s - loss: 3.1525 - accuracy: 0.169 - ETA: 17s - loss: 3.1524 - accuracy: 0.169 - ETA: 16s - loss: 3.1514 - accuracy: 0.169 - ETA: 15s - loss: 3.1507 - accuracy: 0.169 - ETA: 15s - loss: 3.1508 - accuracy: 0.169 - ETA: 14s - loss: 3.1509 - accuracy: 0.169 - ETA: 14s - loss: 3.1514 - accuracy: 0.169 - ETA: 13s - loss: 3.1510 - accuracy: 0.169 - ETA: 12s - loss: 3.1507 - accuracy: 0.169 - ETA: 12s - loss: 3.1504 - accuracy: 0.169 - ETA: 11s - loss: 3.1503 - accuracy: 0.169 - ETA: 11s - loss: 3.1500 - accuracy: 0.169 - ETA: 10s - loss: 3.1499 - accuracy: 0.169 - ETA: 10s - loss: 3.1497 - accuracy: 0.169 - ETA: 9s - loss: 3.1491 - accuracy: 0.169 - ETA: 8s - loss: 3.1489 - accuracy: 0.16 - ETA: 8s - loss: 3.1491 - accuracy: 0.16 - ETA: 7s - loss: 3.1485 - accuracy: 0.17 - ETA: 7s - loss: 3.1481 - accuracy: 0.17 - ETA: 6s - loss: 3.1484 - accuracy: 0.16 - ETA: 5s - loss: 3.1480 - accuracy: 0.17 - ETA: 5s - loss: 3.1482 - accuracy: 0.16 - ETA: 4s - loss: 3.1485 - accuracy: 0.16 - ETA: 4s - loss: 3.1488 - accuracy: 0.16 - ETA: 3s - loss: 3.1487 - accuracy: 0.16 - ETA: 2s - loss: 3.1481 - accuracy: 0.16 - ETA: 2s - loss: 3.1485 - accuracy: 0.16 - ETA: 1s - loss: 3.1487 - accuracy: 0.16 - ETA: 1s - loss: 3.1487 - accuracy: 0.16 - ETA: 0s - loss: 3.1486 - accuracy: 0.16 - ETA: 0s - loss: 3.1489 - accuracy: 0.16 - 206s 5ms/step - loss: 3.1488 - accuracy: 0.1695 - val_loss: 3.9047 - val_accuracy: 0.0387\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:42 - loss: 3.3854 - accuracy: 0.10 - ETA: 3:25 - loss: 3.2165 - accuracy: 0.16 - ETA: 3:22 - loss: 3.1882 - accuracy: 0.16 - ETA: 3:23 - loss: 3.1911 - accuracy: 0.16 - ETA: 3:23 - loss: 3.2305 - accuracy: 0.15 - ETA: 3:22 - loss: 3.1993 - accuracy: 0.15 - ETA: 3:19 - loss: 3.1929 - accuracy: 0.15 - ETA: 3:16 - loss: 3.1835 - accuracy: 0.16 - ETA: 3:15 - loss: 3.1612 - accuracy: 0.16 - ETA: 3:14 - loss: 3.1688 - accuracy: 0.16 - ETA: 3:13 - loss: 3.1539 - accuracy: 0.16 - ETA: 3:12 - loss: 3.1456 - accuracy: 0.16 - ETA: 3:10 - loss: 3.1669 - accuracy: 0.16 - ETA: 3:09 - loss: 3.1574 - accuracy: 0.16 - ETA: 3:09 - loss: 3.1617 - accuracy: 0.16 - ETA: 3:07 - loss: 3.1458 - accuracy: 0.16 - ETA: 3:07 - loss: 3.1462 - accuracy: 0.17 - ETA: 3:06 - loss: 3.1387 - accuracy: 0.17 - ETA: 3:05 - loss: 3.1358 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1410 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1344 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1342 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1444 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1409 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1405 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1414 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1434 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1424 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1431 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1438 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1410 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1375 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1433 - accuracy: 0.16 - ETA: 2:54 - loss: 3.1460 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1409 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1473 - accuracy: 0.16 - ETA: 2:52 - loss: 3.1449 - accuracy: 0.16 - ETA: 2:52 - loss: 3.1400 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1396 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1431 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1365 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1377 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1403 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1372 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1354 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1331 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1340 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1329 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1349 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1343 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1376 - accuracy: 0.16 - ETA: 2:43 - loss: 3.1353 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1363 - accuracy: 0.16 - ETA: 2:42 - loss: 3.1395 - accuracy: 0.16 - ETA: 2:41 - loss: 3.1369 - accuracy: 0.16 - ETA: 2:41 - loss: 3.1375 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1393 - accuracy: 0.16 - ETA: 2:40 - loss: 3.1379 - accuracy: 0.16 - ETA: 2:39 - loss: 3.1398 - accuracy: 0.16 - ETA: 2:39 - loss: 3.1402 - accuracy: 0.16 - ETA: 2:38 - loss: 3.1407 - accuracy: 0.16 - ETA: 2:38 - loss: 3.1412 - accuracy: 0.16 - ETA: 2:37 - loss: 3.1398 - accuracy: 0.16 - ETA: 2:36 - loss: 3.1408 - accuracy: 0.16 - ETA: 2:36 - loss: 3.1405 - accuracy: 0.16 - ETA: 2:35 - loss: 3.1409 - accuracy: 0.16 - ETA: 2:35 - loss: 3.1422 - accuracy: 0.16 - ETA: 2:34 - loss: 3.1430 - accuracy: 0.16 - ETA: 2:33 - loss: 3.1436 - accuracy: 0.16 - ETA: 2:33 - loss: 3.1443 - accuracy: 0.16 - ETA: 2:32 - loss: 3.1417 - accuracy: 0.16 - ETA: 2:32 - loss: 3.1407 - accuracy: 0.16 - ETA: 2:31 - loss: 3.1400 - accuracy: 0.16 - ETA: 2:30 - loss: 3.1405 - accuracy: 0.16 - ETA: 2:30 - loss: 3.1391 - accuracy: 0.16 - ETA: 2:29 - loss: 3.1374 - accuracy: 0.16 - ETA: 2:28 - loss: 3.1385 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1385 - accuracy: 0.16 - ETA: 2:27 - loss: 3.1362 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1389 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1416 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1414 - accuracy: 0.16 - ETA: 2:25 - loss: 3.1432 - accuracy: 0.16 - ETA: 2:24 - loss: 3.1430 - accuracy: 0.16 - ETA: 2:24 - loss: 3.1467 - accuracy: 0.16 - ETA: 2:23 - loss: 3.1455 - accuracy: 0.16 - ETA: 2:23 - loss: 3.1468 - accuracy: 0.16 - ETA: 2:22 - loss: 3.1473 - accuracy: 0.16 - ETA: 2:22 - loss: 3.1455 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1470 - accuracy: 0.16 - ETA: 2:20 - loss: 3.1448 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1441 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1457 - accuracy: 0.16 - ETA: 2:19 - loss: 3.1453 - accuracy: 0.16 - ETA: 2:18 - loss: 3.1468 - accuracy: 0.16 - ETA: 2:18 - loss: 3.1453 - accuracy: 0.16 - ETA: 2:17 - loss: 3.1437 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1450 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1452 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1443 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1440 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1448 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1434 - accuracy: 0.16 - ETA: 2:12 - loss: 3.1423 - accuracy: 0.16 - ETA: 2:12 - loss: 3.1414 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1422 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1429 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1423 - accuracy: 0.16 - ETA: 2:09 - loss: 3.1433 - accuracy: 0.16 - ETA: 2:09 - loss: 3.1450 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1438 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1453 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1437 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1446 - accuracy: 0.16 - ETA: 2:06 - loss: 3.1449 - accuracy: 0.16 - ETA: 2:06 - loss: 3.1444 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1434 - accuracy: 0.16 - ETA: 2:04 - loss: 3.1434 - accuracy: 0.16 - ETA: 2:04 - loss: 3.1435 - accuracy: 0.16 - ETA: 2:03 - loss: 3.1447 - accuracy: 0.16 - ETA: 2:03 - loss: 3.1440 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1431 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1418 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1416 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1415 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1429 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1429 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1443 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1429 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1431 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1425 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1429 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1419 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1409 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1402 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1394 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1404 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1410 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1415 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1433 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1416 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1404 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1407 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1399 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1406 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1392 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1401 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1390 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1398 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1406 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1405 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1410 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1412 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1411 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1426 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1422 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1424 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1431 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1433 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1445 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1451 - accuracy: 0.16 - ETA: 1:38 - loss: 3.1466 - accuracy: 0.16 - ETA: 1:38 - loss: 3.1461 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1466 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1459 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1474 - accuracy: 0.16 - ETA: 1:35 - loss: 3.1478 - accuracy: 0.16 - ETA: 1:35 - loss: 3.1483 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1492 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1499 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1499 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1507 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1505 - accuracy: 0.16 - ETA: 1:31 - loss: 3.1509 - accuracy: 0.16 - ETA: 1:31 - loss: 3.1503 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1509 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1510 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1515 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1516 - accuracy: 0.16 - ETA: 1:28 - loss: 3.1523 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1521 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1505 - accuracy: 0.16 - ETA: 1:26 - loss: 3.1510 - accuracy: 0.16 - ETA: 1:26 - loss: 3.1515 - accuracy: 0.16 - ETA: 1:25 - loss: 3.1512 - accuracy: 0.16 - ETA: 1:24 - loss: 3.1525 - accuracy: 0.1681"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1535 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1533 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1539 - accuracy: 0.16 - ETA: 1:22 - loss: 3.1540 - accuracy: 0.16 - ETA: 1:21 - loss: 3.1544 - accuracy: 0.16 - ETA: 1:21 - loss: 3.1542 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1552 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1559 - accuracy: 0.16 - ETA: 1:19 - loss: 3.1558 - accuracy: 0.16 - ETA: 1:19 - loss: 3.1554 - accuracy: 0.16 - ETA: 1:18 - loss: 3.1560 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1564 - accuracy: 0.16 - ETA: 1:17 - loss: 3.1563 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1570 - accuracy: 0.16 - ETA: 1:16 - loss: 3.1565 - accuracy: 0.16 - ETA: 1:15 - loss: 3.1571 - accuracy: 0.16 - ETA: 1:15 - loss: 3.1566 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1563 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1555 - accuracy: 0.16 - ETA: 1:13 - loss: 3.1557 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1559 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1563 - accuracy: 0.16 - ETA: 1:11 - loss: 3.1555 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1553 - accuracy: 0.16 - ETA: 1:10 - loss: 3.1554 - accuracy: 0.16 - ETA: 1:09 - loss: 3.1557 - accuracy: 0.16 - ETA: 1:09 - loss: 3.1564 - accuracy: 0.16 - ETA: 1:08 - loss: 3.1570 - accuracy: 0.16 - ETA: 1:08 - loss: 3.1572 - accuracy: 0.16 - ETA: 1:07 - loss: 3.1570 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1570 - accuracy: 0.16 - ETA: 1:06 - loss: 3.1572 - accuracy: 0.16 - ETA: 1:05 - loss: 3.1565 - accuracy: 0.16 - ETA: 1:05 - loss: 3.1567 - accuracy: 0.16 - ETA: 1:04 - loss: 3.1568 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1558 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1556 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1561 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1558 - accuracy: 0.16 - ETA: 1:01 - loss: 3.1550 - accuracy: 0.16 - ETA: 1:01 - loss: 3.1551 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1556 - accuracy: 0.16 - ETA: 59s - loss: 3.1554 - accuracy: 0.1678 - ETA: 59s - loss: 3.1556 - accuracy: 0.167 - ETA: 58s - loss: 3.1560 - accuracy: 0.167 - ETA: 58s - loss: 3.1546 - accuracy: 0.168 - ETA: 57s - loss: 3.1544 - accuracy: 0.168 - ETA: 56s - loss: 3.1542 - accuracy: 0.168 - ETA: 56s - loss: 3.1545 - accuracy: 0.168 - ETA: 55s - loss: 3.1550 - accuracy: 0.167 - ETA: 55s - loss: 3.1543 - accuracy: 0.167 - ETA: 54s - loss: 3.1533 - accuracy: 0.168 - ETA: 53s - loss: 3.1523 - accuracy: 0.168 - ETA: 53s - loss: 3.1526 - accuracy: 0.168 - ETA: 52s - loss: 3.1526 - accuracy: 0.167 - ETA: 52s - loss: 3.1524 - accuracy: 0.167 - ETA: 51s - loss: 3.1529 - accuracy: 0.167 - ETA: 50s - loss: 3.1524 - accuracy: 0.167 - ETA: 50s - loss: 3.1519 - accuracy: 0.167 - ETA: 49s - loss: 3.1510 - accuracy: 0.167 - ETA: 49s - loss: 3.1504 - accuracy: 0.168 - ETA: 48s - loss: 3.1510 - accuracy: 0.167 - ETA: 48s - loss: 3.1521 - accuracy: 0.167 - ETA: 47s - loss: 3.1524 - accuracy: 0.167 - ETA: 46s - loss: 3.1522 - accuracy: 0.167 - ETA: 46s - loss: 3.1516 - accuracy: 0.167 - ETA: 45s - loss: 3.1514 - accuracy: 0.167 - ETA: 45s - loss: 3.1520 - accuracy: 0.167 - ETA: 44s - loss: 3.1521 - accuracy: 0.167 - ETA: 43s - loss: 3.1514 - accuracy: 0.168 - ETA: 43s - loss: 3.1514 - accuracy: 0.168 - ETA: 42s - loss: 3.1521 - accuracy: 0.167 - ETA: 42s - loss: 3.1517 - accuracy: 0.167 - ETA: 41s - loss: 3.1516 - accuracy: 0.167 - ETA: 41s - loss: 3.1513 - accuracy: 0.168 - ETA: 40s - loss: 3.1512 - accuracy: 0.167 - ETA: 39s - loss: 3.1513 - accuracy: 0.167 - ETA: 39s - loss: 3.1512 - accuracy: 0.168 - ETA: 38s - loss: 3.1517 - accuracy: 0.167 - ETA: 38s - loss: 3.1519 - accuracy: 0.167 - ETA: 37s - loss: 3.1517 - accuracy: 0.167 - ETA: 36s - loss: 3.1516 - accuracy: 0.167 - ETA: 36s - loss: 3.1516 - accuracy: 0.167 - ETA: 35s - loss: 3.1517 - accuracy: 0.167 - ETA: 35s - loss: 3.1513 - accuracy: 0.167 - ETA: 34s - loss: 3.1508 - accuracy: 0.167 - ETA: 33s - loss: 3.1497 - accuracy: 0.168 - ETA: 33s - loss: 3.1495 - accuracy: 0.168 - ETA: 32s - loss: 3.1496 - accuracy: 0.168 - ETA: 32s - loss: 3.1496 - accuracy: 0.168 - ETA: 31s - loss: 3.1496 - accuracy: 0.168 - ETA: 31s - loss: 3.1499 - accuracy: 0.168 - ETA: 30s - loss: 3.1497 - accuracy: 0.168 - ETA: 29s - loss: 3.1501 - accuracy: 0.168 - ETA: 29s - loss: 3.1507 - accuracy: 0.167 - ETA: 28s - loss: 3.1509 - accuracy: 0.167 - ETA: 28s - loss: 3.1502 - accuracy: 0.168 - ETA: 27s - loss: 3.1499 - accuracy: 0.168 - ETA: 26s - loss: 3.1497 - accuracy: 0.168 - ETA: 26s - loss: 3.1491 - accuracy: 0.168 - ETA: 25s - loss: 3.1493 - accuracy: 0.168 - ETA: 25s - loss: 3.1486 - accuracy: 0.168 - ETA: 24s - loss: 3.1483 - accuracy: 0.168 - ETA: 24s - loss: 3.1478 - accuracy: 0.168 - ETA: 23s - loss: 3.1479 - accuracy: 0.169 - ETA: 22s - loss: 3.1476 - accuracy: 0.169 - ETA: 22s - loss: 3.1474 - accuracy: 0.169 - ETA: 21s - loss: 3.1470 - accuracy: 0.169 - ETA: 21s - loss: 3.1467 - accuracy: 0.169 - ETA: 20s - loss: 3.1469 - accuracy: 0.169 - ETA: 19s - loss: 3.1464 - accuracy: 0.169 - ETA: 19s - loss: 3.1468 - accuracy: 0.169 - ETA: 18s - loss: 3.1468 - accuracy: 0.169 - ETA: 18s - loss: 3.1467 - accuracy: 0.169 - ETA: 17s - loss: 3.1465 - accuracy: 0.169 - ETA: 17s - loss: 3.1466 - accuracy: 0.169 - ETA: 16s - loss: 3.1466 - accuracy: 0.169 - ETA: 15s - loss: 3.1468 - accuracy: 0.169 - ETA: 15s - loss: 3.1469 - accuracy: 0.169 - ETA: 14s - loss: 3.1464 - accuracy: 0.169 - ETA: 14s - loss: 3.1465 - accuracy: 0.169 - ETA: 13s - loss: 3.1459 - accuracy: 0.169 - ETA: 12s - loss: 3.1461 - accuracy: 0.169 - ETA: 12s - loss: 3.1461 - accuracy: 0.169 - ETA: 11s - loss: 3.1460 - accuracy: 0.169 - ETA: 11s - loss: 3.1456 - accuracy: 0.169 - ETA: 10s - loss: 3.1454 - accuracy: 0.169 - ETA: 10s - loss: 3.1451 - accuracy: 0.169 - ETA: 9s - loss: 3.1453 - accuracy: 0.168 - ETA: 8s - loss: 3.1455 - accuracy: 0.16 - ETA: 8s - loss: 3.1451 - accuracy: 0.16 - ETA: 7s - loss: 3.1446 - accuracy: 0.16 - ETA: 7s - loss: 3.1450 - accuracy: 0.16 - ETA: 6s - loss: 3.1452 - accuracy: 0.16 - ETA: 5s - loss: 3.1457 - accuracy: 0.16 - ETA: 5s - loss: 3.1463 - accuracy: 0.16 - ETA: 4s - loss: 3.1460 - accuracy: 0.16 - ETA: 4s - loss: 3.1456 - accuracy: 0.16 - ETA: 3s - loss: 3.1456 - accuracy: 0.16 - ETA: 2s - loss: 3.1454 - accuracy: 0.16 - ETA: 2s - loss: 3.1460 - accuracy: 0.16 - ETA: 1s - loss: 3.1462 - accuracy: 0.16 - ETA: 1s - loss: 3.1461 - accuracy: 0.16 - ETA: 0s - loss: 3.1462 - accuracy: 0.16 - ETA: 0s - loss: 3.1464 - accuracy: 0.16 - 205s 5ms/step - loss: 3.1464 - accuracy: 0.1690 - val_loss: 3.9618 - val_accuracy: 0.0327\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:32 - loss: 2.9898 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0523 - accuracy: 0.16 - ETA: 3:15 - loss: 3.0584 - accuracy: 0.17 - ETA: 3:11 - loss: 3.0398 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0176 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0530 - accuracy: 0.17 - ETA: 3:08 - loss: 3.0692 - accuracy: 0.17 - ETA: 3:09 - loss: 3.0635 - accuracy: 0.17 - ETA: 3:10 - loss: 3.0729 - accuracy: 0.17 - ETA: 3:11 - loss: 3.0693 - accuracy: 0.17 - ETA: 3:11 - loss: 3.0630 - accuracy: 0.17 - ETA: 3:09 - loss: 3.0750 - accuracy: 0.17 - ETA: 3:09 - loss: 3.0828 - accuracy: 0.17 - ETA: 3:08 - loss: 3.0986 - accuracy: 0.17 - ETA: 3:06 - loss: 3.0902 - accuracy: 0.17 - ETA: 3:05 - loss: 3.0834 - accuracy: 0.17 - ETA: 3:05 - loss: 3.0837 - accuracy: 0.17 - ETA: 3:04 - loss: 3.0820 - accuracy: 0.17 - ETA: 3:03 - loss: 3.0950 - accuracy: 0.17 - ETA: 3:02 - loss: 3.0992 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1100 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1043 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1086 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1041 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1030 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1005 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1066 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1028 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1040 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1012 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1022 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1055 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1075 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1069 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1069 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1018 - accuracy: 0.17 - ETA: 2:52 - loss: 3.0999 - accuracy: 0.17 - ETA: 2:52 - loss: 3.0974 - accuracy: 0.17 - ETA: 2:51 - loss: 3.0996 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1021 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1038 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1055 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1078 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1086 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1099 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1098 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1066 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1012 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1019 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1037 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1023 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1019 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1014 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1010 - accuracy: 0.17 - ETA: 2:41 - loss: 3.0990 - accuracy: 0.17 - ETA: 2:40 - loss: 3.0982 - accuracy: 0.17 - ETA: 2:40 - loss: 3.0981 - accuracy: 0.17 - ETA: 2:39 - loss: 3.0991 - accuracy: 0.17 - ETA: 2:38 - loss: 3.0987 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1019 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1011 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1005 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1032 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1035 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1018 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1017 - accuracy: 0.17 - ETA: 2:34 - loss: 3.0991 - accuracy: 0.17 - ETA: 2:34 - loss: 3.0993 - accuracy: 0.17 - ETA: 2:33 - loss: 3.0992 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1011 - accuracy: 0.17 - ETA: 2:32 - loss: 3.0991 - accuracy: 0.17 - ETA: 2:31 - loss: 3.0990 - accuracy: 0.17 - ETA: 2:31 - loss: 3.0974 - accuracy: 0.17 - ETA: 2:30 - loss: 3.0990 - accuracy: 0.17 - ETA: 2:30 - loss: 3.0992 - accuracy: 0.17 - ETA: 2:29 - loss: 3.0979 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1012 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1027 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1042 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1032 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1012 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1021 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1029 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1037 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1035 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1016 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1015 - accuracy: 0.17 - ETA: 2:22 - loss: 3.0976 - accuracy: 0.17 - ETA: 2:22 - loss: 3.0988 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1032 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1050 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1058 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1063 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1051 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1076 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1079 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1083 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1076 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1087 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1089 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1088 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1073 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1083 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1062 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1059 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1050 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1040 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1052 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1050 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1045 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1037 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1046 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1053 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1042 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1030 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1034 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1038 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1028 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1016 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1016 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1019 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1011 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1002 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1012 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1017 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1039 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1048 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1060 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1056 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1072 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1061 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1062 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1059 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1050 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1059 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1068 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1049 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1050 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1052 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1048 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1044 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1048 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1065 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1066 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1068 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1082 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1082 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1076 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1087 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1072 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1072 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1074 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1088 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1096 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1090 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1090 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1092 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1089 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1093 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1101 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1107 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1108 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1116 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1108 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1103 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1106 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1107 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1096 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1104 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1118 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1109 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1117 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1113 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1112 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1108 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1118 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1124 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1133 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1129 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1122 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1124 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1130 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1133 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1137 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1140 - accuracy: 0.17 - ETA: 1:24 - loss: 3.1139 - accuracy: 0.1766"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1145 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1146 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1148 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1153 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1152 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1154 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1150 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1148 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1144 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1135 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1139 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1135 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1132 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1134 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1124 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1130 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1128 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1119 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1116 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1112 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1129 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1127 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1130 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1123 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1120 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1124 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1124 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1123 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1123 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1119 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1128 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1132 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1131 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1132 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1141 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1139 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1133 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1138 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1132 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1130 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1126 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1118 - accuracy: 0.17 - ETA: 59s - loss: 3.1108 - accuracy: 0.1765 - ETA: 59s - loss: 3.1104 - accuracy: 0.176 - ETA: 58s - loss: 3.1117 - accuracy: 0.176 - ETA: 58s - loss: 3.1123 - accuracy: 0.176 - ETA: 57s - loss: 3.1122 - accuracy: 0.176 - ETA: 56s - loss: 3.1122 - accuracy: 0.176 - ETA: 56s - loss: 3.1127 - accuracy: 0.175 - ETA: 55s - loss: 3.1129 - accuracy: 0.175 - ETA: 55s - loss: 3.1128 - accuracy: 0.175 - ETA: 54s - loss: 3.1136 - accuracy: 0.175 - ETA: 53s - loss: 3.1134 - accuracy: 0.175 - ETA: 53s - loss: 3.1128 - accuracy: 0.175 - ETA: 52s - loss: 3.1120 - accuracy: 0.175 - ETA: 52s - loss: 3.1117 - accuracy: 0.175 - ETA: 51s - loss: 3.1123 - accuracy: 0.175 - ETA: 50s - loss: 3.1114 - accuracy: 0.175 - ETA: 50s - loss: 3.1116 - accuracy: 0.175 - ETA: 49s - loss: 3.1122 - accuracy: 0.175 - ETA: 49s - loss: 3.1115 - accuracy: 0.176 - ETA: 48s - loss: 3.1118 - accuracy: 0.175 - ETA: 48s - loss: 3.1124 - accuracy: 0.175 - ETA: 47s - loss: 3.1121 - accuracy: 0.175 - ETA: 46s - loss: 3.1115 - accuracy: 0.175 - ETA: 46s - loss: 3.1107 - accuracy: 0.176 - ETA: 45s - loss: 3.1107 - accuracy: 0.175 - ETA: 45s - loss: 3.1094 - accuracy: 0.176 - ETA: 44s - loss: 3.1095 - accuracy: 0.176 - ETA: 43s - loss: 3.1095 - accuracy: 0.176 - ETA: 43s - loss: 3.1091 - accuracy: 0.176 - ETA: 42s - loss: 3.1089 - accuracy: 0.176 - ETA: 42s - loss: 3.1091 - accuracy: 0.176 - ETA: 41s - loss: 3.1089 - accuracy: 0.176 - ETA: 41s - loss: 3.1090 - accuracy: 0.176 - ETA: 40s - loss: 3.1091 - accuracy: 0.176 - ETA: 39s - loss: 3.1085 - accuracy: 0.176 - ETA: 39s - loss: 3.1090 - accuracy: 0.176 - ETA: 38s - loss: 3.1088 - accuracy: 0.176 - ETA: 38s - loss: 3.1095 - accuracy: 0.176 - ETA: 37s - loss: 3.1097 - accuracy: 0.176 - ETA: 36s - loss: 3.1093 - accuracy: 0.176 - ETA: 36s - loss: 3.1093 - accuracy: 0.176 - ETA: 35s - loss: 3.1092 - accuracy: 0.176 - ETA: 35s - loss: 3.1099 - accuracy: 0.176 - ETA: 34s - loss: 3.1104 - accuracy: 0.176 - ETA: 33s - loss: 3.1101 - accuracy: 0.176 - ETA: 33s - loss: 3.1094 - accuracy: 0.176 - ETA: 32s - loss: 3.1084 - accuracy: 0.176 - ETA: 32s - loss: 3.1080 - accuracy: 0.176 - ETA: 31s - loss: 3.1077 - accuracy: 0.176 - ETA: 31s - loss: 3.1067 - accuracy: 0.177 - ETA: 30s - loss: 3.1072 - accuracy: 0.176 - ETA: 29s - loss: 3.1078 - accuracy: 0.176 - ETA: 29s - loss: 3.1081 - accuracy: 0.176 - ETA: 28s - loss: 3.1080 - accuracy: 0.176 - ETA: 28s - loss: 3.1085 - accuracy: 0.176 - ETA: 27s - loss: 3.1088 - accuracy: 0.176 - ETA: 26s - loss: 3.1078 - accuracy: 0.176 - ETA: 26s - loss: 3.1085 - accuracy: 0.176 - ETA: 25s - loss: 3.1091 - accuracy: 0.175 - ETA: 25s - loss: 3.1085 - accuracy: 0.176 - ETA: 24s - loss: 3.1084 - accuracy: 0.176 - ETA: 24s - loss: 3.1086 - accuracy: 0.176 - ETA: 23s - loss: 3.1095 - accuracy: 0.176 - ETA: 22s - loss: 3.1093 - accuracy: 0.176 - ETA: 22s - loss: 3.1083 - accuracy: 0.176 - ETA: 21s - loss: 3.1083 - accuracy: 0.176 - ETA: 21s - loss: 3.1084 - accuracy: 0.176 - ETA: 20s - loss: 3.1087 - accuracy: 0.176 - ETA: 19s - loss: 3.1085 - accuracy: 0.176 - ETA: 19s - loss: 3.1084 - accuracy: 0.176 - ETA: 18s - loss: 3.1084 - accuracy: 0.176 - ETA: 18s - loss: 3.1082 - accuracy: 0.176 - ETA: 17s - loss: 3.1085 - accuracy: 0.176 - ETA: 17s - loss: 3.1085 - accuracy: 0.176 - ETA: 16s - loss: 3.1090 - accuracy: 0.176 - ETA: 15s - loss: 3.1088 - accuracy: 0.176 - ETA: 15s - loss: 3.1091 - accuracy: 0.176 - ETA: 14s - loss: 3.1093 - accuracy: 0.176 - ETA: 14s - loss: 3.1098 - accuracy: 0.176 - ETA: 13s - loss: 3.1094 - accuracy: 0.176 - ETA: 12s - loss: 3.1091 - accuracy: 0.176 - ETA: 12s - loss: 3.1093 - accuracy: 0.176 - ETA: 11s - loss: 3.1087 - accuracy: 0.176 - ETA: 11s - loss: 3.1081 - accuracy: 0.176 - ETA: 10s - loss: 3.1083 - accuracy: 0.176 - ETA: 10s - loss: 3.1082 - accuracy: 0.176 - ETA: 9s - loss: 3.1082 - accuracy: 0.176 - ETA: 8s - loss: 3.1082 - accuracy: 0.17 - ETA: 8s - loss: 3.1083 - accuracy: 0.17 - ETA: 7s - loss: 3.1080 - accuracy: 0.17 - ETA: 7s - loss: 3.1083 - accuracy: 0.17 - ETA: 6s - loss: 3.1086 - accuracy: 0.17 - ETA: 5s - loss: 3.1095 - accuracy: 0.17 - ETA: 5s - loss: 3.1091 - accuracy: 0.17 - ETA: 4s - loss: 3.1092 - accuracy: 0.17 - ETA: 4s - loss: 3.1092 - accuracy: 0.17 - ETA: 3s - loss: 3.1086 - accuracy: 0.17 - ETA: 2s - loss: 3.1086 - accuracy: 0.17 - ETA: 2s - loss: 3.1087 - accuracy: 0.17 - ETA: 1s - loss: 3.1084 - accuracy: 0.17 - ETA: 1s - loss: 3.1080 - accuracy: 0.17 - ETA: 0s - loss: 3.1081 - accuracy: 0.17 - ETA: 0s - loss: 3.1079 - accuracy: 0.17 - 206s 5ms/step - loss: 3.1078 - accuracy: 0.1767 - val_loss: 4.0019 - val_accuracy: 0.0391\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:15 - loss: 3.3965 - accuracy: 0.14 - ETA: 3:07 - loss: 3.1967 - accuracy: 0.17 - ETA: 3:06 - loss: 3.2024 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1752 - accuracy: 0.17 - ETA: 3:05 - loss: 3.1874 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1386 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1151 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1363 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1222 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1281 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1176 - accuracy: 0.17 - ETA: 3:03 - loss: 3.0970 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1002 - accuracy: 0.17 - ETA: 3:04 - loss: 3.0997 - accuracy: 0.17 - ETA: 3:04 - loss: 3.0903 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0873 - accuracy: 0.18 - ETA: 3:03 - loss: 3.1010 - accuracy: 0.17 - ETA: 3:02 - loss: 3.0886 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0914 - accuracy: 0.18 - ETA: 3:01 - loss: 3.1082 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1065 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1027 - accuracy: 0.17 - ETA: 2:59 - loss: 3.0993 - accuracy: 0.17 - ETA: 2:58 - loss: 3.0970 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1031 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1016 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1069 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1083 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1081 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1105 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1111 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1131 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1112 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1100 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1089 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1066 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1061 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1061 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1097 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1132 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1092 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1108 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1127 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1151 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1184 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1192 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1193 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1166 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1164 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1167 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1190 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1184 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1175 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1159 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1132 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1117 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1096 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1059 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1057 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1027 - accuracy: 0.17 - ETA: 2:37 - loss: 3.0982 - accuracy: 0.17 - ETA: 2:37 - loss: 3.0993 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1023 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1062 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1072 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1087 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1083 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1091 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1071 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1089 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1091 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1096 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1096 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1114 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1083 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1090 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1092 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1080 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1074 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1051 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1035 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1043 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1021 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1038 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1014 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1020 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1023 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1043 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1048 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1047 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1059 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1066 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1071 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1044 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1023 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1020 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1013 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1022 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1028 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1021 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1008 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1013 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1004 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1001 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1024 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1034 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1036 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1031 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1018 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1025 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1009 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1010 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1004 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1018 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1011 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1005 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1005 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1002 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1002 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1009 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1013 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1021 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1039 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1036 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1049 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1050 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1046 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1054 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1083 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1082 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1089 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1086 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1096 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1095 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1104 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1106 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1140 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1138 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1136 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1129 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1141 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1130 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1146 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1160 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1158 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1169 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1175 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1179 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1176 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1180 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1185 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1183 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1198 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1205 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1218 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1224 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1227 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1232 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1231 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1237 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1239 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1247 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1255 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1254 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1246 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1247 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1248 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1247 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1246 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1252 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1255 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1260 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1249 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1246 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1245 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1245 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1244 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1238 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1240 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1233 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1234 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1231 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1219 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1208 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1217 - accuracy: 0.17 - ETA: 1:24 - loss: 3.1207 - accuracy: 0.1773"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1203 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1203 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1205 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1201 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1204 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1201 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1201 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1205 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1208 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1209 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1210 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1200 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1208 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1207 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1211 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1211 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1213 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1207 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1215 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1209 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1212 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1208 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1206 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1230 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1222 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1216 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1211 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1210 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1208 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1205 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1208 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1210 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1209 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1212 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1208 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1203 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1198 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1205 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1208 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1210 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1209 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1208 - accuracy: 0.17 - ETA: 59s - loss: 3.1207 - accuracy: 0.1776 - ETA: 59s - loss: 3.1202 - accuracy: 0.177 - ETA: 58s - loss: 3.1202 - accuracy: 0.177 - ETA: 58s - loss: 3.1196 - accuracy: 0.177 - ETA: 57s - loss: 3.1193 - accuracy: 0.177 - ETA: 56s - loss: 3.1196 - accuracy: 0.177 - ETA: 56s - loss: 3.1196 - accuracy: 0.177 - ETA: 55s - loss: 3.1196 - accuracy: 0.177 - ETA: 55s - loss: 3.1200 - accuracy: 0.177 - ETA: 54s - loss: 3.1198 - accuracy: 0.177 - ETA: 54s - loss: 3.1190 - accuracy: 0.177 - ETA: 53s - loss: 3.1197 - accuracy: 0.177 - ETA: 52s - loss: 3.1190 - accuracy: 0.177 - ETA: 52s - loss: 3.1185 - accuracy: 0.177 - ETA: 51s - loss: 3.1185 - accuracy: 0.177 - ETA: 51s - loss: 3.1194 - accuracy: 0.177 - ETA: 50s - loss: 3.1195 - accuracy: 0.177 - ETA: 49s - loss: 3.1199 - accuracy: 0.177 - ETA: 49s - loss: 3.1199 - accuracy: 0.177 - ETA: 48s - loss: 3.1190 - accuracy: 0.177 - ETA: 48s - loss: 3.1186 - accuracy: 0.177 - ETA: 47s - loss: 3.1188 - accuracy: 0.177 - ETA: 46s - loss: 3.1185 - accuracy: 0.177 - ETA: 46s - loss: 3.1187 - accuracy: 0.177 - ETA: 45s - loss: 3.1183 - accuracy: 0.177 - ETA: 45s - loss: 3.1190 - accuracy: 0.177 - ETA: 44s - loss: 3.1183 - accuracy: 0.177 - ETA: 44s - loss: 3.1180 - accuracy: 0.177 - ETA: 43s - loss: 3.1182 - accuracy: 0.177 - ETA: 42s - loss: 3.1175 - accuracy: 0.177 - ETA: 42s - loss: 3.1171 - accuracy: 0.177 - ETA: 41s - loss: 3.1166 - accuracy: 0.177 - ETA: 41s - loss: 3.1168 - accuracy: 0.177 - ETA: 40s - loss: 3.1176 - accuracy: 0.176 - ETA: 39s - loss: 3.1183 - accuracy: 0.176 - ETA: 39s - loss: 3.1192 - accuracy: 0.176 - ETA: 38s - loss: 3.1194 - accuracy: 0.176 - ETA: 38s - loss: 3.1197 - accuracy: 0.176 - ETA: 37s - loss: 3.1194 - accuracy: 0.176 - ETA: 37s - loss: 3.1194 - accuracy: 0.176 - ETA: 36s - loss: 3.1196 - accuracy: 0.175 - ETA: 35s - loss: 3.1194 - accuracy: 0.175 - ETA: 35s - loss: 3.1192 - accuracy: 0.175 - ETA: 34s - loss: 3.1187 - accuracy: 0.175 - ETA: 34s - loss: 3.1191 - accuracy: 0.175 - ETA: 33s - loss: 3.1186 - accuracy: 0.175 - ETA: 32s - loss: 3.1186 - accuracy: 0.175 - ETA: 32s - loss: 3.1187 - accuracy: 0.175 - ETA: 31s - loss: 3.1189 - accuracy: 0.175 - ETA: 31s - loss: 3.1197 - accuracy: 0.175 - ETA: 30s - loss: 3.1189 - accuracy: 0.175 - ETA: 29s - loss: 3.1187 - accuracy: 0.175 - ETA: 29s - loss: 3.1184 - accuracy: 0.175 - ETA: 28s - loss: 3.1182 - accuracy: 0.175 - ETA: 28s - loss: 3.1181 - accuracy: 0.175 - ETA: 27s - loss: 3.1175 - accuracy: 0.175 - ETA: 27s - loss: 3.1176 - accuracy: 0.175 - ETA: 26s - loss: 3.1180 - accuracy: 0.175 - ETA: 25s - loss: 3.1176 - accuracy: 0.175 - ETA: 25s - loss: 3.1182 - accuracy: 0.175 - ETA: 24s - loss: 3.1179 - accuracy: 0.175 - ETA: 24s - loss: 3.1179 - accuracy: 0.175 - ETA: 23s - loss: 3.1179 - accuracy: 0.175 - ETA: 22s - loss: 3.1178 - accuracy: 0.175 - ETA: 22s - loss: 3.1179 - accuracy: 0.175 - ETA: 21s - loss: 3.1180 - accuracy: 0.175 - ETA: 21s - loss: 3.1180 - accuracy: 0.175 - ETA: 20s - loss: 3.1184 - accuracy: 0.175 - ETA: 19s - loss: 3.1187 - accuracy: 0.175 - ETA: 19s - loss: 3.1180 - accuracy: 0.175 - ETA: 18s - loss: 3.1180 - accuracy: 0.175 - ETA: 18s - loss: 3.1186 - accuracy: 0.175 - ETA: 17s - loss: 3.1193 - accuracy: 0.175 - ETA: 17s - loss: 3.1200 - accuracy: 0.175 - ETA: 16s - loss: 3.1197 - accuracy: 0.175 - ETA: 15s - loss: 3.1200 - accuracy: 0.175 - ETA: 15s - loss: 3.1202 - accuracy: 0.175 - ETA: 14s - loss: 3.1202 - accuracy: 0.175 - ETA: 14s - loss: 3.1197 - accuracy: 0.175 - ETA: 13s - loss: 3.1202 - accuracy: 0.175 - ETA: 12s - loss: 3.1199 - accuracy: 0.175 - ETA: 12s - loss: 3.1203 - accuracy: 0.175 - ETA: 11s - loss: 3.1206 - accuracy: 0.175 - ETA: 11s - loss: 3.1212 - accuracy: 0.174 - ETA: 10s - loss: 3.1207 - accuracy: 0.175 - ETA: 10s - loss: 3.1211 - accuracy: 0.174 - ETA: 9s - loss: 3.1216 - accuracy: 0.174 - ETA: 8s - loss: 3.1211 - accuracy: 0.17 - ETA: 8s - loss: 3.1208 - accuracy: 0.17 - ETA: 7s - loss: 3.1209 - accuracy: 0.17 - ETA: 7s - loss: 3.1207 - accuracy: 0.17 - ETA: 6s - loss: 3.1206 - accuracy: 0.17 - ETA: 5s - loss: 3.1210 - accuracy: 0.17 - ETA: 5s - loss: 3.1205 - accuracy: 0.17 - ETA: 4s - loss: 3.1205 - accuracy: 0.17 - ETA: 4s - loss: 3.1201 - accuracy: 0.17 - ETA: 3s - loss: 3.1206 - accuracy: 0.17 - ETA: 2s - loss: 3.1213 - accuracy: 0.17 - ETA: 2s - loss: 3.1213 - accuracy: 0.17 - ETA: 1s - loss: 3.1221 - accuracy: 0.17 - ETA: 1s - loss: 3.1225 - accuracy: 0.17 - ETA: 0s - loss: 3.1227 - accuracy: 0.17 - ETA: 0s - loss: 3.1220 - accuracy: 0.17 - 206s 5ms/step - loss: 3.1222 - accuracy: 0.1746 - val_loss: 3.9812 - val_accuracy: 0.0417\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:17 - loss: 3.1774 - accuracy: 0.17 - ETA: 3:12 - loss: 3.1770 - accuracy: 0.17 - ETA: 3:11 - loss: 3.1461 - accuracy: 0.18 - ETA: 3:08 - loss: 3.1528 - accuracy: 0.16 - ETA: 3:08 - loss: 3.1732 - accuracy: 0.16 - ETA: 3:07 - loss: 3.1605 - accuracy: 0.16 - ETA: 3:07 - loss: 3.1625 - accuracy: 0.16 - ETA: 3:07 - loss: 3.1771 - accuracy: 0.15 - ETA: 3:06 - loss: 3.1627 - accuracy: 0.15 - ETA: 3:05 - loss: 3.1663 - accuracy: 0.15 - ETA: 3:04 - loss: 3.1658 - accuracy: 0.15 - ETA: 3:04 - loss: 3.1613 - accuracy: 0.16 - ETA: 3:03 - loss: 3.1542 - accuracy: 0.16 - ETA: 3:03 - loss: 3.1575 - accuracy: 0.16 - ETA: 3:03 - loss: 3.1681 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1735 - accuracy: 0.15 - ETA: 3:04 - loss: 3.1623 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1620 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1509 - accuracy: 0.16 - ETA: 3:03 - loss: 3.1441 - accuracy: 0.16 - ETA: 3:02 - loss: 3.1343 - accuracy: 0.16 - ETA: 3:01 - loss: 3.1361 - accuracy: 0.16 - ETA: 3:00 - loss: 3.1352 - accuracy: 0.16 - ETA: 3:00 - loss: 3.1268 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1285 - accuracy: 0.16 - ETA: 2:58 - loss: 3.1227 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1250 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1300 - accuracy: 0.16 - ETA: 2:57 - loss: 3.1285 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1289 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1275 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1289 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1272 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1207 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1240 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1277 - accuracy: 0.16 - ETA: 2:51 - loss: 3.1315 - accuracy: 0.16 - ETA: 2:51 - loss: 3.1313 - accuracy: 0.16 - ETA: 2:50 - loss: 3.1316 - accuracy: 0.16 - ETA: 2:49 - loss: 3.1314 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1313 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1289 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1286 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1305 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1316 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1293 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1295 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1310 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1287 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1290 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1290 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1287 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1261 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1270 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1251 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1229 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1226 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1250 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1230 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1231 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1191 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1156 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1133 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1130 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1176 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1190 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1171 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1162 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1162 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1148 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1144 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1135 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1139 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1140 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1149 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1163 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1158 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1143 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1134 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1110 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1127 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1153 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1138 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1129 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1115 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1129 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1114 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1111 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1092 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1085 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1080 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1090 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1092 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1098 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1087 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1087 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1086 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1085 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1106 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1123 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1111 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1098 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1092 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1097 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1100 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1094 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1093 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1080 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1056 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1061 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1076 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1070 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1037 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1039 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1042 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1032 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1023 - accuracy: 0.18 - ETA: 2:04 - loss: 3.1025 - accuracy: 0.18 - ETA: 2:04 - loss: 3.1021 - accuracy: 0.18 - ETA: 2:03 - loss: 3.1034 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1020 - accuracy: 0.18 - ETA: 2:02 - loss: 3.1016 - accuracy: 0.18 - ETA: 2:01 - loss: 3.1005 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0999 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0993 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0994 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0998 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0992 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0982 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0985 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0983 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0985 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0998 - accuracy: 0.18 - ETA: 1:55 - loss: 3.1007 - accuracy: 0.18 - ETA: 1:54 - loss: 3.1002 - accuracy: 0.18 - ETA: 1:54 - loss: 3.1004 - accuracy: 0.18 - ETA: 1:53 - loss: 3.1003 - accuracy: 0.18 - ETA: 1:52 - loss: 3.1005 - accuracy: 0.18 - ETA: 1:52 - loss: 3.1009 - accuracy: 0.18 - ETA: 1:51 - loss: 3.1010 - accuracy: 0.18 - ETA: 1:51 - loss: 3.1006 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0997 - accuracy: 0.18 - ETA: 1:49 - loss: 3.1000 - accuracy: 0.18 - ETA: 1:49 - loss: 3.1005 - accuracy: 0.18 - ETA: 1:48 - loss: 3.1004 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0996 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0992 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0998 - accuracy: 0.18 - ETA: 1:46 - loss: 3.1002 - accuracy: 0.18 - ETA: 1:45 - loss: 3.1004 - accuracy: 0.18 - ETA: 1:45 - loss: 3.1024 - accuracy: 0.18 - ETA: 1:44 - loss: 3.1029 - accuracy: 0.18 - ETA: 1:44 - loss: 3.1029 - accuracy: 0.18 - ETA: 1:43 - loss: 3.1024 - accuracy: 0.18 - ETA: 1:43 - loss: 3.1029 - accuracy: 0.18 - ETA: 1:42 - loss: 3.1028 - accuracy: 0.18 - ETA: 1:42 - loss: 3.1033 - accuracy: 0.18 - ETA: 1:41 - loss: 3.1039 - accuracy: 0.18 - ETA: 1:41 - loss: 3.1039 - accuracy: 0.18 - ETA: 1:40 - loss: 3.1036 - accuracy: 0.18 - ETA: 1:40 - loss: 3.1046 - accuracy: 0.18 - ETA: 1:39 - loss: 3.1047 - accuracy: 0.18 - ETA: 1:38 - loss: 3.1054 - accuracy: 0.18 - ETA: 1:38 - loss: 3.1060 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1070 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1075 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1076 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1077 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1075 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1072 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1065 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1062 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1070 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1071 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1071 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1064 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1069 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1063 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1071 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1066 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1067 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1064 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1069 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1071 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1074 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1077 - accuracy: 0.1788"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:25 - loss: 3.1067 - accuracy: 0.17 - ETA: 1:24 - loss: 3.1065 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1072 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1069 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1069 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1061 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1063 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1062 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1073 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1075 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1076 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1084 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1075 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1077 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1065 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1069 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1070 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1069 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1061 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1048 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1036 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1035 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1030 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1037 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1039 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1038 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1035 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1032 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1024 - accuracy: 0.18 - ETA: 1:07 - loss: 3.1031 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1034 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1063 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1067 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1066 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1069 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1066 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1077 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1078 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1081 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1086 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1090 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1086 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1084 - accuracy: 0.17 - ETA: 59s - loss: 3.1075 - accuracy: 0.1796 - ETA: 58s - loss: 3.1070 - accuracy: 0.179 - ETA: 58s - loss: 3.1074 - accuracy: 0.179 - ETA: 57s - loss: 3.1068 - accuracy: 0.179 - ETA: 57s - loss: 3.1065 - accuracy: 0.179 - ETA: 56s - loss: 3.1069 - accuracy: 0.179 - ETA: 56s - loss: 3.1072 - accuracy: 0.179 - ETA: 55s - loss: 3.1071 - accuracy: 0.179 - ETA: 54s - loss: 3.1065 - accuracy: 0.179 - ETA: 54s - loss: 3.1075 - accuracy: 0.179 - ETA: 53s - loss: 3.1080 - accuracy: 0.179 - ETA: 53s - loss: 3.1086 - accuracy: 0.179 - ETA: 52s - loss: 3.1088 - accuracy: 0.179 - ETA: 51s - loss: 3.1089 - accuracy: 0.179 - ETA: 51s - loss: 3.1093 - accuracy: 0.179 - ETA: 50s - loss: 3.1110 - accuracy: 0.179 - ETA: 50s - loss: 3.1112 - accuracy: 0.179 - ETA: 49s - loss: 3.1107 - accuracy: 0.179 - ETA: 48s - loss: 3.1107 - accuracy: 0.179 - ETA: 48s - loss: 3.1111 - accuracy: 0.178 - ETA: 47s - loss: 3.1121 - accuracy: 0.178 - ETA: 47s - loss: 3.1124 - accuracy: 0.178 - ETA: 46s - loss: 3.1127 - accuracy: 0.178 - ETA: 45s - loss: 3.1122 - accuracy: 0.178 - ETA: 45s - loss: 3.1121 - accuracy: 0.178 - ETA: 44s - loss: 3.1125 - accuracy: 0.178 - ETA: 44s - loss: 3.1121 - accuracy: 0.178 - ETA: 43s - loss: 3.1121 - accuracy: 0.178 - ETA: 42s - loss: 3.1123 - accuracy: 0.178 - ETA: 42s - loss: 3.1129 - accuracy: 0.178 - ETA: 41s - loss: 3.1134 - accuracy: 0.178 - ETA: 41s - loss: 3.1138 - accuracy: 0.178 - ETA: 40s - loss: 3.1146 - accuracy: 0.178 - ETA: 40s - loss: 3.1147 - accuracy: 0.178 - ETA: 39s - loss: 3.1147 - accuracy: 0.178 - ETA: 38s - loss: 3.1152 - accuracy: 0.178 - ETA: 38s - loss: 3.1151 - accuracy: 0.178 - ETA: 37s - loss: 3.1145 - accuracy: 0.178 - ETA: 37s - loss: 3.1149 - accuracy: 0.178 - ETA: 36s - loss: 3.1149 - accuracy: 0.178 - ETA: 35s - loss: 3.1144 - accuracy: 0.178 - ETA: 35s - loss: 3.1142 - accuracy: 0.178 - ETA: 34s - loss: 3.1143 - accuracy: 0.178 - ETA: 34s - loss: 3.1148 - accuracy: 0.178 - ETA: 33s - loss: 3.1153 - accuracy: 0.178 - ETA: 32s - loss: 3.1160 - accuracy: 0.178 - ETA: 32s - loss: 3.1159 - accuracy: 0.177 - ETA: 31s - loss: 3.1160 - accuracy: 0.177 - ETA: 31s - loss: 3.1167 - accuracy: 0.177 - ETA: 30s - loss: 3.1168 - accuracy: 0.177 - ETA: 30s - loss: 3.1175 - accuracy: 0.177 - ETA: 29s - loss: 3.1173 - accuracy: 0.177 - ETA: 28s - loss: 3.1175 - accuracy: 0.177 - ETA: 28s - loss: 3.1189 - accuracy: 0.177 - ETA: 27s - loss: 3.1191 - accuracy: 0.177 - ETA: 27s - loss: 3.1194 - accuracy: 0.177 - ETA: 26s - loss: 3.1196 - accuracy: 0.177 - ETA: 25s - loss: 3.1199 - accuracy: 0.177 - ETA: 25s - loss: 3.1203 - accuracy: 0.177 - ETA: 24s - loss: 3.1203 - accuracy: 0.177 - ETA: 24s - loss: 3.1201 - accuracy: 0.177 - ETA: 23s - loss: 3.1193 - accuracy: 0.177 - ETA: 22s - loss: 3.1197 - accuracy: 0.177 - ETA: 22s - loss: 3.1201 - accuracy: 0.177 - ETA: 21s - loss: 3.1203 - accuracy: 0.177 - ETA: 21s - loss: 3.1210 - accuracy: 0.177 - ETA: 20s - loss: 3.1206 - accuracy: 0.177 - ETA: 20s - loss: 3.1205 - accuracy: 0.177 - ETA: 19s - loss: 3.1206 - accuracy: 0.177 - ETA: 18s - loss: 3.1204 - accuracy: 0.177 - ETA: 18s - loss: 3.1204 - accuracy: 0.177 - ETA: 17s - loss: 3.1206 - accuracy: 0.177 - ETA: 17s - loss: 3.1202 - accuracy: 0.177 - ETA: 16s - loss: 3.1207 - accuracy: 0.177 - ETA: 15s - loss: 3.1210 - accuracy: 0.177 - ETA: 15s - loss: 3.1207 - accuracy: 0.177 - ETA: 14s - loss: 3.1209 - accuracy: 0.177 - ETA: 14s - loss: 3.1210 - accuracy: 0.177 - ETA: 13s - loss: 3.1212 - accuracy: 0.176 - ETA: 12s - loss: 3.1208 - accuracy: 0.176 - ETA: 12s - loss: 3.1199 - accuracy: 0.176 - ETA: 11s - loss: 3.1204 - accuracy: 0.176 - ETA: 11s - loss: 3.1213 - accuracy: 0.176 - ETA: 10s - loss: 3.1219 - accuracy: 0.176 - ETA: 10s - loss: 3.1227 - accuracy: 0.176 - ETA: 9s - loss: 3.1228 - accuracy: 0.176 - ETA: 8s - loss: 3.1224 - accuracy: 0.17 - ETA: 8s - loss: 3.1223 - accuracy: 0.17 - ETA: 7s - loss: 3.1230 - accuracy: 0.17 - ETA: 7s - loss: 3.1223 - accuracy: 0.17 - ETA: 6s - loss: 3.1224 - accuracy: 0.17 - ETA: 5s - loss: 3.1222 - accuracy: 0.17 - ETA: 5s - loss: 3.1219 - accuracy: 0.17 - ETA: 4s - loss: 3.1221 - accuracy: 0.17 - ETA: 4s - loss: 3.1227 - accuracy: 0.17 - ETA: 3s - loss: 3.1228 - accuracy: 0.17 - ETA: 2s - loss: 3.1222 - accuracy: 0.17 - ETA: 2s - loss: 3.1221 - accuracy: 0.17 - ETA: 1s - loss: 3.1224 - accuracy: 0.17 - ETA: 1s - loss: 3.1218 - accuracy: 0.17 - ETA: 0s - loss: 3.1218 - accuracy: 0.17 - ETA: 0s - loss: 3.1212 - accuracy: 0.17 - 206s 5ms/step - loss: 3.1211 - accuracy: 0.1768 - val_loss: 4.0677 - val_accuracy: 0.0394\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:25 - loss: 2.8540 - accuracy: 0.21 - ETA: 3:16 - loss: 2.9897 - accuracy: 0.19 - ETA: 3:13 - loss: 2.9975 - accuracy: 0.19 - ETA: 3:11 - loss: 2.9956 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0216 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0424 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0359 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0229 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0162 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0244 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0201 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0333 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0569 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0601 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0811 - accuracy: 0.17 - ETA: 3:01 - loss: 3.0805 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0893 - accuracy: 0.17 - ETA: 3:01 - loss: 3.0907 - accuracy: 0.17 - ETA: 3:01 - loss: 3.0933 - accuracy: 0.17 - ETA: 3:02 - loss: 3.0993 - accuracy: 0.17 - ETA: 3:01 - loss: 3.0929 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0960 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0894 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0912 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0995 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0942 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0866 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0838 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0856 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0887 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0958 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0947 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0929 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0951 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0943 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0946 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0965 - accuracy: 0.18 - ETA: 2:51 - loss: 3.1018 - accuracy: 0.18 - ETA: 2:51 - loss: 3.1020 - accuracy: 0.18 - ETA: 2:50 - loss: 3.1016 - accuracy: 0.18 - ETA: 2:50 - loss: 3.1003 - accuracy: 0.18 - ETA: 2:49 - loss: 3.1023 - accuracy: 0.18 - ETA: 2:48 - loss: 3.1038 - accuracy: 0.18 - ETA: 2:48 - loss: 3.1012 - accuracy: 0.18 - ETA: 2:47 - loss: 3.1038 - accuracy: 0.18 - ETA: 2:47 - loss: 3.1095 - accuracy: 0.18 - ETA: 2:47 - loss: 3.1165 - accuracy: 0.18 - ETA: 2:47 - loss: 3.1154 - accuracy: 0.18 - ETA: 2:46 - loss: 3.1153 - accuracy: 0.18 - ETA: 2:45 - loss: 3.1149 - accuracy: 0.18 - ETA: 2:44 - loss: 3.1185 - accuracy: 0.18 - ETA: 2:44 - loss: 3.1180 - accuracy: 0.18 - ETA: 2:43 - loss: 3.1174 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1158 - accuracy: 0.18 - ETA: 2:42 - loss: 3.1198 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1211 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1252 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1296 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1333 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1338 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1357 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1359 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1377 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1378 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1379 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1389 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1400 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1408 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1388 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1401 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1392 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1389 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1417 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1431 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1432 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1419 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1424 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1424 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1441 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1480 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1479 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1484 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1490 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1488 - accuracy: 0.16 - ETA: 2:24 - loss: 3.1462 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1478 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1473 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1480 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1491 - accuracy: 0.16 - ETA: 2:21 - loss: 3.1497 - accuracy: 0.16 - ETA: 2:20 - loss: 3.1487 - accuracy: 0.16 - ETA: 2:20 - loss: 3.1476 - accuracy: 0.16 - ETA: 2:19 - loss: 3.1498 - accuracy: 0.16 - ETA: 2:18 - loss: 3.1513 - accuracy: 0.16 - ETA: 2:18 - loss: 3.1514 - accuracy: 0.16 - ETA: 2:17 - loss: 3.1518 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1530 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1543 - accuracy: 0.16 - ETA: 2:15 - loss: 3.1524 - accuracy: 0.16 - ETA: 2:15 - loss: 3.1533 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1535 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1537 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1531 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1507 - accuracy: 0.16 - ETA: 2:12 - loss: 3.1509 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1509 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1510 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1527 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1535 - accuracy: 0.16 - ETA: 2:09 - loss: 3.1520 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1513 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1510 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1514 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1513 - accuracy: 0.16 - ETA: 2:06 - loss: 3.1506 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1511 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1517 - accuracy: 0.16 - ETA: 2:04 - loss: 3.1525 - accuracy: 0.16 - ETA: 2:04 - loss: 3.1514 - accuracy: 0.16 - ETA: 2:03 - loss: 3.1502 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1500 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1498 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1483 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1498 - accuracy: 0.16 - ETA: 2:00 - loss: 3.1499 - accuracy: 0.16 - ETA: 1:59 - loss: 3.1521 - accuracy: 0.16 - ETA: 1:59 - loss: 3.1516 - accuracy: 0.16 - ETA: 1:58 - loss: 3.1519 - accuracy: 0.16 - ETA: 1:58 - loss: 3.1527 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1528 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1529 - accuracy: 0.16 - ETA: 1:56 - loss: 3.1517 - accuracy: 0.16 - ETA: 1:56 - loss: 3.1522 - accuracy: 0.16 - ETA: 1:55 - loss: 3.1523 - accuracy: 0.16 - ETA: 1:54 - loss: 3.1514 - accuracy: 0.16 - ETA: 1:54 - loss: 3.1513 - accuracy: 0.16 - ETA: 1:53 - loss: 3.1512 - accuracy: 0.16 - ETA: 1:53 - loss: 3.1503 - accuracy: 0.16 - ETA: 1:52 - loss: 3.1495 - accuracy: 0.16 - ETA: 1:51 - loss: 3.1492 - accuracy: 0.16 - ETA: 1:51 - loss: 3.1499 - accuracy: 0.16 - ETA: 1:50 - loss: 3.1489 - accuracy: 0.16 - ETA: 1:50 - loss: 3.1490 - accuracy: 0.16 - ETA: 1:49 - loss: 3.1493 - accuracy: 0.16 - ETA: 1:48 - loss: 3.1490 - accuracy: 0.16 - ETA: 1:48 - loss: 3.1488 - accuracy: 0.16 - ETA: 1:47 - loss: 3.1480 - accuracy: 0.16 - ETA: 1:47 - loss: 3.1462 - accuracy: 0.16 - ETA: 1:46 - loss: 3.1479 - accuracy: 0.16 - ETA: 1:46 - loss: 3.1488 - accuracy: 0.16 - ETA: 1:45 - loss: 3.1464 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1462 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1456 - accuracy: 0.16 - ETA: 1:43 - loss: 3.1464 - accuracy: 0.16 - ETA: 1:43 - loss: 3.1473 - accuracy: 0.16 - ETA: 1:42 - loss: 3.1463 - accuracy: 0.16 - ETA: 1:42 - loss: 3.1462 - accuracy: 0.16 - ETA: 1:41 - loss: 3.1462 - accuracy: 0.16 - ETA: 1:40 - loss: 3.1467 - accuracy: 0.16 - ETA: 1:40 - loss: 3.1462 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1463 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1463 - accuracy: 0.16 - ETA: 1:38 - loss: 3.1463 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1459 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1477 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1477 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1524 - accuracy: 0.16 - ETA: 1:35 - loss: 3.1524 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1521 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1522 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1514 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1521 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1510 - accuracy: 0.16 - ETA: 1:31 - loss: 3.1507 - accuracy: 0.16 - ETA: 1:31 - loss: 3.1499 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1492 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1494 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1485 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1495 - accuracy: 0.16 - ETA: 1:28 - loss: 3.1496 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1494 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1482 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1480 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1477 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1478 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1474 - accuracy: 0.1703"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1473 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1480 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1486 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1484 - accuracy: 0.16 - ETA: 1:22 - loss: 3.1482 - accuracy: 0.16 - ETA: 1:21 - loss: 3.1480 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1478 - accuracy: 0.16 - ETA: 1:20 - loss: 3.1470 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1479 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1474 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1473 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1463 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1462 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1469 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1471 - accuracy: 0.16 - ETA: 1:15 - loss: 3.1470 - accuracy: 0.16 - ETA: 1:14 - loss: 3.1455 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1451 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1444 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1444 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1450 - accuracy: 0.16 - ETA: 1:12 - loss: 3.1445 - accuracy: 0.16 - ETA: 1:11 - loss: 3.1443 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1436 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1437 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1425 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1427 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1433 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1432 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1438 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1442 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1452 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1445 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1446 - accuracy: 0.16 - ETA: 1:04 - loss: 3.1452 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1460 - accuracy: 0.16 - ETA: 1:03 - loss: 3.1461 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1454 - accuracy: 0.16 - ETA: 1:02 - loss: 3.1465 - accuracy: 0.16 - ETA: 1:01 - loss: 3.1469 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1466 - accuracy: 0.16 - ETA: 1:00 - loss: 3.1470 - accuracy: 0.16 - ETA: 59s - loss: 3.1471 - accuracy: 0.1696 - ETA: 59s - loss: 3.1476 - accuracy: 0.169 - ETA: 58s - loss: 3.1475 - accuracy: 0.169 - ETA: 57s - loss: 3.1473 - accuracy: 0.169 - ETA: 57s - loss: 3.1473 - accuracy: 0.169 - ETA: 56s - loss: 3.1480 - accuracy: 0.169 - ETA: 56s - loss: 3.1480 - accuracy: 0.169 - ETA: 55s - loss: 3.1471 - accuracy: 0.169 - ETA: 55s - loss: 3.1467 - accuracy: 0.169 - ETA: 54s - loss: 3.1488 - accuracy: 0.169 - ETA: 53s - loss: 3.1488 - accuracy: 0.169 - ETA: 53s - loss: 3.1489 - accuracy: 0.169 - ETA: 52s - loss: 3.1481 - accuracy: 0.169 - ETA: 52s - loss: 3.1482 - accuracy: 0.169 - ETA: 51s - loss: 3.1481 - accuracy: 0.169 - ETA: 50s - loss: 3.1483 - accuracy: 0.170 - ETA: 50s - loss: 3.1481 - accuracy: 0.170 - ETA: 49s - loss: 3.1482 - accuracy: 0.170 - ETA: 49s - loss: 3.1478 - accuracy: 0.170 - ETA: 48s - loss: 3.1487 - accuracy: 0.170 - ETA: 48s - loss: 3.1480 - accuracy: 0.170 - ETA: 47s - loss: 3.1488 - accuracy: 0.170 - ETA: 46s - loss: 3.1498 - accuracy: 0.170 - ETA: 46s - loss: 3.1495 - accuracy: 0.170 - ETA: 45s - loss: 3.1497 - accuracy: 0.170 - ETA: 45s - loss: 3.1493 - accuracy: 0.170 - ETA: 44s - loss: 3.1492 - accuracy: 0.170 - ETA: 43s - loss: 3.1487 - accuracy: 0.170 - ETA: 43s - loss: 3.1485 - accuracy: 0.170 - ETA: 42s - loss: 3.1494 - accuracy: 0.170 - ETA: 42s - loss: 3.1493 - accuracy: 0.170 - ETA: 41s - loss: 3.1492 - accuracy: 0.170 - ETA: 41s - loss: 3.1497 - accuracy: 0.170 - ETA: 40s - loss: 3.1500 - accuracy: 0.170 - ETA: 39s - loss: 3.1502 - accuracy: 0.170 - ETA: 39s - loss: 3.1492 - accuracy: 0.170 - ETA: 38s - loss: 3.1490 - accuracy: 0.170 - ETA: 38s - loss: 3.1490 - accuracy: 0.170 - ETA: 37s - loss: 3.1486 - accuracy: 0.170 - ETA: 36s - loss: 3.1497 - accuracy: 0.169 - ETA: 36s - loss: 3.1495 - accuracy: 0.170 - ETA: 35s - loss: 3.1501 - accuracy: 0.169 - ETA: 35s - loss: 3.1497 - accuracy: 0.169 - ETA: 34s - loss: 3.1494 - accuracy: 0.169 - ETA: 34s - loss: 3.1491 - accuracy: 0.169 - ETA: 33s - loss: 3.1489 - accuracy: 0.169 - ETA: 32s - loss: 3.1488 - accuracy: 0.169 - ETA: 32s - loss: 3.1483 - accuracy: 0.169 - ETA: 31s - loss: 3.1482 - accuracy: 0.169 - ETA: 31s - loss: 3.1482 - accuracy: 0.169 - ETA: 30s - loss: 3.1487 - accuracy: 0.169 - ETA: 29s - loss: 3.1489 - accuracy: 0.169 - ETA: 29s - loss: 3.1491 - accuracy: 0.169 - ETA: 28s - loss: 3.1492 - accuracy: 0.169 - ETA: 28s - loss: 3.1487 - accuracy: 0.169 - ETA: 27s - loss: 3.1485 - accuracy: 0.169 - ETA: 26s - loss: 3.1485 - accuracy: 0.169 - ETA: 26s - loss: 3.1482 - accuracy: 0.169 - ETA: 25s - loss: 3.1474 - accuracy: 0.169 - ETA: 25s - loss: 3.1466 - accuracy: 0.169 - ETA: 24s - loss: 3.1467 - accuracy: 0.169 - ETA: 24s - loss: 3.1467 - accuracy: 0.169 - ETA: 23s - loss: 3.1462 - accuracy: 0.169 - ETA: 22s - loss: 3.1461 - accuracy: 0.169 - ETA: 22s - loss: 3.1464 - accuracy: 0.169 - ETA: 21s - loss: 3.1463 - accuracy: 0.170 - ETA: 21s - loss: 3.1467 - accuracy: 0.169 - ETA: 20s - loss: 3.1462 - accuracy: 0.169 - ETA: 19s - loss: 3.1461 - accuracy: 0.169 - ETA: 19s - loss: 3.1461 - accuracy: 0.169 - ETA: 18s - loss: 3.1456 - accuracy: 0.170 - ETA: 18s - loss: 3.1452 - accuracy: 0.170 - ETA: 17s - loss: 3.1451 - accuracy: 0.170 - ETA: 17s - loss: 3.1454 - accuracy: 0.170 - ETA: 16s - loss: 3.1449 - accuracy: 0.170 - ETA: 15s - loss: 3.1445 - accuracy: 0.170 - ETA: 15s - loss: 3.1444 - accuracy: 0.170 - ETA: 14s - loss: 3.1447 - accuracy: 0.170 - ETA: 14s - loss: 3.1445 - accuracy: 0.170 - ETA: 13s - loss: 3.1442 - accuracy: 0.170 - ETA: 12s - loss: 3.1450 - accuracy: 0.170 - ETA: 12s - loss: 3.1451 - accuracy: 0.170 - ETA: 11s - loss: 3.1452 - accuracy: 0.169 - ETA: 11s - loss: 3.1450 - accuracy: 0.170 - ETA: 10s - loss: 3.1456 - accuracy: 0.169 - ETA: 9s - loss: 3.1459 - accuracy: 0.169 - ETA: 9s - loss: 3.1457 - accuracy: 0.16 - ETA: 8s - loss: 3.1455 - accuracy: 0.16 - ETA: 8s - loss: 3.1462 - accuracy: 0.16 - ETA: 7s - loss: 3.1464 - accuracy: 0.16 - ETA: 7s - loss: 3.1466 - accuracy: 0.16 - ETA: 6s - loss: 3.1464 - accuracy: 0.16 - ETA: 5s - loss: 3.1463 - accuracy: 0.16 - ETA: 5s - loss: 3.1458 - accuracy: 0.16 - ETA: 4s - loss: 3.1459 - accuracy: 0.16 - ETA: 4s - loss: 3.1462 - accuracy: 0.16 - ETA: 3s - loss: 3.1469 - accuracy: 0.16 - ETA: 2s - loss: 3.1468 - accuracy: 0.16 - ETA: 2s - loss: 3.1467 - accuracy: 0.16 - ETA: 1s - loss: 3.1465 - accuracy: 0.16 - ETA: 1s - loss: 3.1460 - accuracy: 0.16 - ETA: 0s - loss: 3.1465 - accuracy: 0.16 - ETA: 0s - loss: 3.1469 - accuracy: 0.16 - 206s 5ms/step - loss: 3.1468 - accuracy: 0.1695 - val_loss: 4.0855 - val_accuracy: 0.0416\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:33 - loss: 2.9288 - accuracy: 0.24 - ETA: 3:19 - loss: 3.0524 - accuracy: 0.21 - ETA: 3:16 - loss: 3.1754 - accuracy: 0.19 - ETA: 3:14 - loss: 3.1856 - accuracy: 0.18 - ETA: 3:12 - loss: 3.1752 - accuracy: 0.18 - ETA: 3:09 - loss: 3.1495 - accuracy: 0.18 - ETA: 3:09 - loss: 3.1483 - accuracy: 0.17 - ETA: 3:09 - loss: 3.1704 - accuracy: 0.16 - ETA: 3:07 - loss: 3.1483 - accuracy: 0.17 - ETA: 3:06 - loss: 3.1519 - accuracy: 0.17 - ETA: 3:08 - loss: 3.1373 - accuracy: 0.17 - ETA: 3:07 - loss: 3.1533 - accuracy: 0.17 - ETA: 3:06 - loss: 3.1326 - accuracy: 0.17 - ETA: 3:06 - loss: 3.1479 - accuracy: 0.17 - ETA: 3:05 - loss: 3.1461 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1358 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1348 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1378 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1428 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1432 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1486 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1452 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1424 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1452 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1449 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1341 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1364 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1395 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1413 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1317 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1270 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1251 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1276 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1231 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1184 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1157 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1125 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1136 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1149 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1151 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1071 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1061 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1069 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1107 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1124 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1099 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1062 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1085 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1093 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1101 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1096 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1109 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1106 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1115 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1113 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1148 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1160 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1154 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1183 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1214 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1208 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1181 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1200 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1229 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1208 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1205 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1207 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1254 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1267 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1257 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1256 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1266 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1274 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1277 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1288 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1297 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1325 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1356 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1338 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1343 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1317 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1340 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1333 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1321 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1322 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1311 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1306 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1327 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1343 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1340 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1335 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1350 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1341 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1333 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1326 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1333 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1313 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1318 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1302 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1311 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1306 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1326 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1326 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1304 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1313 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1313 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1322 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1336 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1333 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1338 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1329 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1323 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1328 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1313 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1313 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1309 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1304 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1295 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1300 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1293 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1292 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1356 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1366 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1387 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1395 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1391 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1394 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1397 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1412 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1403 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1404 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1415 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1426 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1423 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1423 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1433 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1426 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1445 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1450 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1446 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1447 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1446 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1444 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1447 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1434 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1428 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1410 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1405 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1405 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1416 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1412 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1429 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1436 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1424 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1424 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1424 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1426 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1427 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1434 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1431 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1429 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1432 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1456 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1467 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1467 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1469 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1463 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1472 - accuracy: 0.16 - ETA: 1:35 - loss: 3.1478 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1457 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1441 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1448 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1449 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1441 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1438 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1435 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1443 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1443 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1437 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1433 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1435 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1440 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1445 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1448 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1447 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1451 - accuracy: 0.1701"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1452 - accuracy: 0.17 - ETA: 1:24 - loss: 3.1445 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1439 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1436 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1439 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1435 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1425 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1425 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1428 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1431 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1421 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1416 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1410 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1405 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1404 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1398 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1401 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1389 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1386 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1379 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1379 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1369 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1373 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1380 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1378 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1378 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1379 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1374 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1372 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1374 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1381 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1391 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1396 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1410 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1408 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1406 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1406 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1409 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1406 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1399 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1403 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1402 - accuracy: 0.17 - ETA: 59s - loss: 3.1404 - accuracy: 0.1715 - ETA: 59s - loss: 3.1398 - accuracy: 0.171 - ETA: 58s - loss: 3.1388 - accuracy: 0.172 - ETA: 58s - loss: 3.1384 - accuracy: 0.172 - ETA: 57s - loss: 3.1391 - accuracy: 0.172 - ETA: 56s - loss: 3.1390 - accuracy: 0.172 - ETA: 56s - loss: 3.1392 - accuracy: 0.172 - ETA: 55s - loss: 3.1394 - accuracy: 0.172 - ETA: 55s - loss: 3.1391 - accuracy: 0.172 - ETA: 54s - loss: 3.1387 - accuracy: 0.172 - ETA: 53s - loss: 3.1390 - accuracy: 0.172 - ETA: 53s - loss: 3.1396 - accuracy: 0.171 - ETA: 52s - loss: 3.1404 - accuracy: 0.171 - ETA: 52s - loss: 3.1403 - accuracy: 0.171 - ETA: 51s - loss: 3.1403 - accuracy: 0.171 - ETA: 51s - loss: 3.1400 - accuracy: 0.171 - ETA: 50s - loss: 3.1398 - accuracy: 0.171 - ETA: 49s - loss: 3.1398 - accuracy: 0.171 - ETA: 49s - loss: 3.1392 - accuracy: 0.171 - ETA: 48s - loss: 3.1386 - accuracy: 0.171 - ETA: 48s - loss: 3.1387 - accuracy: 0.171 - ETA: 47s - loss: 3.1394 - accuracy: 0.171 - ETA: 47s - loss: 3.1400 - accuracy: 0.171 - ETA: 46s - loss: 3.1403 - accuracy: 0.171 - ETA: 45s - loss: 3.1402 - accuracy: 0.171 - ETA: 45s - loss: 3.1397 - accuracy: 0.171 - ETA: 44s - loss: 3.1395 - accuracy: 0.171 - ETA: 44s - loss: 3.1397 - accuracy: 0.171 - ETA: 43s - loss: 3.1399 - accuracy: 0.171 - ETA: 42s - loss: 3.1403 - accuracy: 0.171 - ETA: 42s - loss: 3.1406 - accuracy: 0.171 - ETA: 41s - loss: 3.1397 - accuracy: 0.171 - ETA: 41s - loss: 3.1398 - accuracy: 0.171 - ETA: 40s - loss: 3.1405 - accuracy: 0.171 - ETA: 39s - loss: 3.1408 - accuracy: 0.171 - ETA: 39s - loss: 3.1406 - accuracy: 0.171 - ETA: 38s - loss: 3.1408 - accuracy: 0.171 - ETA: 38s - loss: 3.1408 - accuracy: 0.171 - ETA: 37s - loss: 3.1406 - accuracy: 0.171 - ETA: 37s - loss: 3.1406 - accuracy: 0.171 - ETA: 36s - loss: 3.1401 - accuracy: 0.171 - ETA: 35s - loss: 3.1399 - accuracy: 0.171 - ETA: 35s - loss: 3.1393 - accuracy: 0.171 - ETA: 34s - loss: 3.1422 - accuracy: 0.171 - ETA: 34s - loss: 3.1422 - accuracy: 0.171 - ETA: 33s - loss: 3.1418 - accuracy: 0.171 - ETA: 32s - loss: 3.1415 - accuracy: 0.171 - ETA: 32s - loss: 3.1412 - accuracy: 0.171 - ETA: 31s - loss: 3.1407 - accuracy: 0.171 - ETA: 31s - loss: 3.1398 - accuracy: 0.171 - ETA: 30s - loss: 3.1400 - accuracy: 0.171 - ETA: 29s - loss: 3.1401 - accuracy: 0.171 - ETA: 29s - loss: 3.1399 - accuracy: 0.171 - ETA: 28s - loss: 3.1401 - accuracy: 0.171 - ETA: 28s - loss: 3.1408 - accuracy: 0.171 - ETA: 27s - loss: 3.1407 - accuracy: 0.171 - ETA: 27s - loss: 3.1408 - accuracy: 0.171 - ETA: 26s - loss: 3.1410 - accuracy: 0.171 - ETA: 25s - loss: 3.1407 - accuracy: 0.171 - ETA: 25s - loss: 3.1405 - accuracy: 0.171 - ETA: 24s - loss: 3.1400 - accuracy: 0.171 - ETA: 24s - loss: 3.1401 - accuracy: 0.171 - ETA: 23s - loss: 3.1409 - accuracy: 0.171 - ETA: 22s - loss: 3.1414 - accuracy: 0.171 - ETA: 22s - loss: 3.1414 - accuracy: 0.171 - ETA: 21s - loss: 3.1414 - accuracy: 0.171 - ETA: 21s - loss: 3.1412 - accuracy: 0.171 - ETA: 20s - loss: 3.1407 - accuracy: 0.171 - ETA: 19s - loss: 3.1406 - accuracy: 0.171 - ETA: 19s - loss: 3.1397 - accuracy: 0.171 - ETA: 18s - loss: 3.1392 - accuracy: 0.171 - ETA: 18s - loss: 3.1396 - accuracy: 0.171 - ETA: 17s - loss: 3.1396 - accuracy: 0.171 - ETA: 17s - loss: 3.1402 - accuracy: 0.171 - ETA: 16s - loss: 3.1400 - accuracy: 0.171 - ETA: 15s - loss: 3.1404 - accuracy: 0.170 - ETA: 15s - loss: 3.1394 - accuracy: 0.171 - ETA: 14s - loss: 3.1397 - accuracy: 0.171 - ETA: 14s - loss: 3.1394 - accuracy: 0.171 - ETA: 13s - loss: 3.1394 - accuracy: 0.170 - ETA: 12s - loss: 3.1389 - accuracy: 0.171 - ETA: 12s - loss: 3.1388 - accuracy: 0.171 - ETA: 11s - loss: 3.1392 - accuracy: 0.171 - ETA: 11s - loss: 3.1389 - accuracy: 0.171 - ETA: 10s - loss: 3.1387 - accuracy: 0.171 - ETA: 10s - loss: 3.1387 - accuracy: 0.171 - ETA: 9s - loss: 3.1390 - accuracy: 0.171 - ETA: 8s - loss: 3.1390 - accuracy: 0.17 - ETA: 8s - loss: 3.1393 - accuracy: 0.17 - ETA: 7s - loss: 3.1393 - accuracy: 0.17 - ETA: 7s - loss: 3.1392 - accuracy: 0.17 - ETA: 6s - loss: 3.1388 - accuracy: 0.17 - ETA: 5s - loss: 3.1383 - accuracy: 0.17 - ETA: 5s - loss: 3.1380 - accuracy: 0.17 - ETA: 4s - loss: 3.1378 - accuracy: 0.17 - ETA: 4s - loss: 3.1374 - accuracy: 0.17 - ETA: 3s - loss: 3.1373 - accuracy: 0.17 - ETA: 2s - loss: 3.1375 - accuracy: 0.17 - ETA: 2s - loss: 3.1372 - accuracy: 0.17 - ETA: 1s - loss: 3.1376 - accuracy: 0.17 - ETA: 1s - loss: 3.1379 - accuracy: 0.17 - ETA: 0s - loss: 3.1385 - accuracy: 0.17 - ETA: 0s - loss: 3.1383 - accuracy: 0.17 - 206s 5ms/step - loss: 3.1383 - accuracy: 0.1715 - val_loss: 4.1176 - val_accuracy: 0.0365\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:17 - loss: 3.1464 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0786 - accuracy: 0.21 - ETA: 3:18 - loss: 3.1299 - accuracy: 0.19 - ETA: 3:19 - loss: 3.1416 - accuracy: 0.18 - ETA: 3:16 - loss: 3.1190 - accuracy: 0.18 - ETA: 3:15 - loss: 3.1081 - accuracy: 0.18 - ETA: 3:13 - loss: 3.1097 - accuracy: 0.17 - ETA: 3:12 - loss: 3.1079 - accuracy: 0.17 - ETA: 3:13 - loss: 3.1102 - accuracy: 0.17 - ETA: 3:14 - loss: 3.1254 - accuracy: 0.17 - ETA: 3:13 - loss: 3.1373 - accuracy: 0.16 - ETA: 3:11 - loss: 3.1497 - accuracy: 0.16 - ETA: 3:10 - loss: 3.1573 - accuracy: 0.16 - ETA: 3:09 - loss: 3.1597 - accuracy: 0.16 - ETA: 3:08 - loss: 3.1508 - accuracy: 0.16 - ETA: 3:07 - loss: 3.1544 - accuracy: 0.16 - ETA: 3:06 - loss: 3.1326 - accuracy: 0.16 - ETA: 3:06 - loss: 3.1356 - accuracy: 0.16 - ETA: 3:05 - loss: 3.1326 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1355 - accuracy: 0.16 - ETA: 3:03 - loss: 3.1482 - accuracy: 0.16 - ETA: 3:02 - loss: 3.1357 - accuracy: 0.16 - ETA: 3:01 - loss: 3.1314 - accuracy: 0.16 - ETA: 3:00 - loss: 3.1298 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1280 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1311 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1275 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1296 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1296 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1250 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1175 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1156 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1061 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1128 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1101 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1118 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1174 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1157 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1119 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1135 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1110 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1117 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1081 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1120 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1136 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1131 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1108 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1167 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1201 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1270 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1281 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1240 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1229 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1207 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1194 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1180 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1218 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1212 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1182 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1178 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1192 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1185 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1174 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1180 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1151 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1154 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1166 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1202 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1262 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1245 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1252 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1237 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1223 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1239 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1242 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1237 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1234 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1221 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1256 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1240 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1245 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1236 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1239 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1232 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1236 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1214 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1193 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1174 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1156 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1146 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1140 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1118 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1116 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1101 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1096 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1101 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1108 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1131 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1140 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1128 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1128 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1114 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1110 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1125 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1100 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1084 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1067 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1063 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1043 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1056 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1056 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1060 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1064 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1053 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1048 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1075 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1077 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1083 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1099 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1104 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1110 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1123 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1096 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1103 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1120 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1109 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1101 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1101 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1101 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1097 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1090 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1089 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1106 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1107 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1107 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1104 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1117 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1100 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1111 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1116 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1120 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1124 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1129 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1114 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1114 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1126 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1132 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1122 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1120 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1142 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1129 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1143 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1142 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1133 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1125 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1126 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1131 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1118 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1133 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1142 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1126 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1116 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1112 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1120 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1119 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1122 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1116 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1114 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1110 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1113 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1105 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1104 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1108 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1088 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1083 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1075 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1067 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1066 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1068 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1060 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1062 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1055 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1060 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1059 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1065 - accuracy: 0.17 - ETA: 1:24 - loss: 3.1057 - accuracy: 0.1762"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1054 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1055 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1052 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1058 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1051 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1065 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1065 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1064 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1063 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1053 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1049 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1042 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1041 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1038 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1039 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1043 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1052 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1055 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1060 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1064 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1064 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1059 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1062 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1067 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1067 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1070 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1076 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1071 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1066 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1068 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1061 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1040 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1032 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1038 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1035 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1043 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1035 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1038 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1040 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1041 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1033 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1030 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1033 - accuracy: 0.17 - ETA: 59s - loss: 3.1027 - accuracy: 0.1762 - ETA: 58s - loss: 3.1029 - accuracy: 0.176 - ETA: 58s - loss: 3.1020 - accuracy: 0.176 - ETA: 57s - loss: 3.1051 - accuracy: 0.176 - ETA: 57s - loss: 3.1056 - accuracy: 0.176 - ETA: 56s - loss: 3.1051 - accuracy: 0.176 - ETA: 56s - loss: 3.1050 - accuracy: 0.176 - ETA: 55s - loss: 3.1046 - accuracy: 0.176 - ETA: 54s - loss: 3.1039 - accuracy: 0.176 - ETA: 54s - loss: 3.1033 - accuracy: 0.176 - ETA: 53s - loss: 3.1033 - accuracy: 0.176 - ETA: 53s - loss: 3.1037 - accuracy: 0.176 - ETA: 52s - loss: 3.1039 - accuracy: 0.176 - ETA: 51s - loss: 3.1042 - accuracy: 0.176 - ETA: 51s - loss: 3.1039 - accuracy: 0.176 - ETA: 50s - loss: 3.1038 - accuracy: 0.176 - ETA: 50s - loss: 3.1043 - accuracy: 0.176 - ETA: 49s - loss: 3.1040 - accuracy: 0.176 - ETA: 48s - loss: 3.1044 - accuracy: 0.176 - ETA: 48s - loss: 3.1047 - accuracy: 0.176 - ETA: 47s - loss: 3.1052 - accuracy: 0.176 - ETA: 47s - loss: 3.1051 - accuracy: 0.177 - ETA: 46s - loss: 3.1042 - accuracy: 0.177 - ETA: 45s - loss: 3.1030 - accuracy: 0.177 - ETA: 45s - loss: 3.1035 - accuracy: 0.177 - ETA: 44s - loss: 3.1035 - accuracy: 0.177 - ETA: 44s - loss: 3.1029 - accuracy: 0.177 - ETA: 43s - loss: 3.1026 - accuracy: 0.177 - ETA: 43s - loss: 3.1027 - accuracy: 0.177 - ETA: 42s - loss: 3.1023 - accuracy: 0.177 - ETA: 41s - loss: 3.1020 - accuracy: 0.177 - ETA: 41s - loss: 3.1022 - accuracy: 0.177 - ETA: 40s - loss: 3.1023 - accuracy: 0.177 - ETA: 40s - loss: 3.1030 - accuracy: 0.177 - ETA: 39s - loss: 3.1027 - accuracy: 0.177 - ETA: 38s - loss: 3.1030 - accuracy: 0.177 - ETA: 38s - loss: 3.1034 - accuracy: 0.177 - ETA: 37s - loss: 3.1029 - accuracy: 0.177 - ETA: 37s - loss: 3.1029 - accuracy: 0.177 - ETA: 36s - loss: 3.1022 - accuracy: 0.177 - ETA: 35s - loss: 3.1025 - accuracy: 0.177 - ETA: 35s - loss: 3.1025 - accuracy: 0.177 - ETA: 34s - loss: 3.1025 - accuracy: 0.177 - ETA: 34s - loss: 3.1018 - accuracy: 0.177 - ETA: 33s - loss: 3.1023 - accuracy: 0.177 - ETA: 33s - loss: 3.1022 - accuracy: 0.177 - ETA: 32s - loss: 3.1026 - accuracy: 0.177 - ETA: 31s - loss: 3.1023 - accuracy: 0.177 - ETA: 31s - loss: 3.1025 - accuracy: 0.177 - ETA: 30s - loss: 3.1024 - accuracy: 0.177 - ETA: 30s - loss: 3.1016 - accuracy: 0.177 - ETA: 29s - loss: 3.1018 - accuracy: 0.177 - ETA: 28s - loss: 3.1019 - accuracy: 0.177 - ETA: 28s - loss: 3.1021 - accuracy: 0.177 - ETA: 27s - loss: 3.1025 - accuracy: 0.177 - ETA: 27s - loss: 3.1024 - accuracy: 0.177 - ETA: 26s - loss: 3.1019 - accuracy: 0.177 - ETA: 25s - loss: 3.1016 - accuracy: 0.177 - ETA: 25s - loss: 3.1009 - accuracy: 0.177 - ETA: 24s - loss: 3.1013 - accuracy: 0.177 - ETA: 24s - loss: 3.1014 - accuracy: 0.177 - ETA: 23s - loss: 3.1010 - accuracy: 0.177 - ETA: 22s - loss: 3.1011 - accuracy: 0.177 - ETA: 22s - loss: 3.1009 - accuracy: 0.177 - ETA: 21s - loss: 3.1018 - accuracy: 0.177 - ETA: 21s - loss: 3.1019 - accuracy: 0.177 - ETA: 20s - loss: 3.1019 - accuracy: 0.177 - ETA: 20s - loss: 3.1013 - accuracy: 0.177 - ETA: 19s - loss: 3.1008 - accuracy: 0.177 - ETA: 18s - loss: 3.1007 - accuracy: 0.177 - ETA: 18s - loss: 3.1012 - accuracy: 0.177 - ETA: 17s - loss: 3.1013 - accuracy: 0.177 - ETA: 17s - loss: 3.1007 - accuracy: 0.177 - ETA: 16s - loss: 3.1004 - accuracy: 0.177 - ETA: 15s - loss: 3.1001 - accuracy: 0.177 - ETA: 15s - loss: 3.1001 - accuracy: 0.177 - ETA: 14s - loss: 3.1004 - accuracy: 0.177 - ETA: 14s - loss: 3.1008 - accuracy: 0.177 - ETA: 13s - loss: 3.1010 - accuracy: 0.177 - ETA: 12s - loss: 3.1012 - accuracy: 0.177 - ETA: 12s - loss: 3.1011 - accuracy: 0.177 - ETA: 11s - loss: 3.1013 - accuracy: 0.177 - ETA: 11s - loss: 3.1013 - accuracy: 0.177 - ETA: 10s - loss: 3.1009 - accuracy: 0.177 - ETA: 10s - loss: 3.1010 - accuracy: 0.177 - ETA: 9s - loss: 3.1005 - accuracy: 0.177 - ETA: 8s - loss: 3.1013 - accuracy: 0.17 - ETA: 8s - loss: 3.1016 - accuracy: 0.17 - ETA: 7s - loss: 3.1016 - accuracy: 0.17 - ETA: 7s - loss: 3.1015 - accuracy: 0.17 - ETA: 6s - loss: 3.1013 - accuracy: 0.17 - ETA: 5s - loss: 3.1014 - accuracy: 0.17 - ETA: 5s - loss: 3.1013 - accuracy: 0.17 - ETA: 4s - loss: 3.1016 - accuracy: 0.17 - ETA: 4s - loss: 3.1015 - accuracy: 0.17 - ETA: 3s - loss: 3.1013 - accuracy: 0.17 - ETA: 2s - loss: 3.1008 - accuracy: 0.17 - ETA: 2s - loss: 3.1008 - accuracy: 0.17 - ETA: 1s - loss: 3.1006 - accuracy: 0.17 - ETA: 1s - loss: 3.1007 - accuracy: 0.17 - ETA: 0s - loss: 3.1003 - accuracy: 0.17 - ETA: 0s - loss: 3.1002 - accuracy: 0.17 - 207s 5ms/step - loss: 3.1003 - accuracy: 0.1781 - val_loss: 4.1365 - val_accuracy: 0.0420\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:23 - loss: 3.1163 - accuracy: 0.17 - ETA: 3:14 - loss: 3.0400 - accuracy: 0.18 - ETA: 3:11 - loss: 3.1114 - accuracy: 0.17 - ETA: 3:08 - loss: 3.0847 - accuracy: 0.18 - ETA: 3:08 - loss: 3.1224 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0934 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0775 - accuracy: 0.20 - ETA: 3:06 - loss: 3.0584 - accuracy: 0.20 - ETA: 3:05 - loss: 3.0594 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0628 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0768 - accuracy: 0.19 - ETA: 3:04 - loss: 3.1023 - accuracy: 0.19 - ETA: 3:03 - loss: 3.1014 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0933 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0844 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0829 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0834 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0900 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0828 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0884 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0890 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0801 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0894 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0763 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0810 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0782 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0747 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0785 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0808 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0868 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0884 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0988 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0972 - accuracy: 0.17 - ETA: 2:53 - loss: 3.0946 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0943 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0940 - accuracy: 0.17 - ETA: 2:51 - loss: 3.0928 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0953 - accuracy: 0.17 - ETA: 2:50 - loss: 3.0979 - accuracy: 0.17 - ETA: 2:50 - loss: 3.0967 - accuracy: 0.17 - ETA: 2:50 - loss: 3.0983 - accuracy: 0.17 - ETA: 2:49 - loss: 3.0927 - accuracy: 0.17 - ETA: 2:48 - loss: 3.0917 - accuracy: 0.17 - ETA: 2:48 - loss: 3.0943 - accuracy: 0.17 - ETA: 2:47 - loss: 3.0903 - accuracy: 0.17 - ETA: 2:46 - loss: 3.0900 - accuracy: 0.17 - ETA: 2:45 - loss: 3.0874 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0879 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0866 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0924 - accuracy: 0.17 - ETA: 2:43 - loss: 3.0895 - accuracy: 0.17 - ETA: 2:43 - loss: 3.0877 - accuracy: 0.17 - ETA: 2:42 - loss: 3.0916 - accuracy: 0.17 - ETA: 2:42 - loss: 3.0924 - accuracy: 0.17 - ETA: 2:41 - loss: 3.0965 - accuracy: 0.17 - ETA: 2:41 - loss: 3.0986 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1023 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1043 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1042 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1026 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1067 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1062 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1089 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1046 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1032 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1015 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1002 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1013 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1000 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1006 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1007 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1029 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1051 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1028 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1014 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1022 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1029 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1036 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1021 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1005 - accuracy: 0.17 - ETA: 2:26 - loss: 3.0997 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1000 - accuracy: 0.17 - ETA: 2:25 - loss: 3.0988 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1000 - accuracy: 0.17 - ETA: 2:24 - loss: 3.0979 - accuracy: 0.17 - ETA: 2:23 - loss: 3.0976 - accuracy: 0.17 - ETA: 2:23 - loss: 3.0956 - accuracy: 0.17 - ETA: 2:22 - loss: 3.0957 - accuracy: 0.17 - ETA: 2:21 - loss: 3.0950 - accuracy: 0.17 - ETA: 2:21 - loss: 3.0964 - accuracy: 0.17 - ETA: 2:20 - loss: 3.0960 - accuracy: 0.17 - ETA: 2:20 - loss: 3.0998 - accuracy: 0.17 - ETA: 2:19 - loss: 3.0994 - accuracy: 0.17 - ETA: 2:18 - loss: 3.0993 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1021 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1028 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1066 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1062 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1062 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1052 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1075 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1075 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1092 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1100 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1100 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1102 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1087 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1099 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1109 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1103 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1108 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1114 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1110 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1115 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1131 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1132 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1136 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1124 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1114 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1097 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1100 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1113 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1111 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1104 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1105 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1110 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1082 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1100 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1091 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1101 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1106 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1095 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1085 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1095 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1106 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1099 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1096 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1076 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1067 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1058 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1056 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1058 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1050 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1052 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1040 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1036 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1042 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1059 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1042 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1045 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1044 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1043 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1035 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1023 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1019 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1012 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1019 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1020 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1012 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1006 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1002 - accuracy: 0.17 - ETA: 1:38 - loss: 3.0992 - accuracy: 0.17 - ETA: 1:38 - loss: 3.0976 - accuracy: 0.17 - ETA: 1:37 - loss: 3.0972 - accuracy: 0.17 - ETA: 1:37 - loss: 3.0979 - accuracy: 0.17 - ETA: 1:36 - loss: 3.0983 - accuracy: 0.17 - ETA: 1:36 - loss: 3.0989 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1005 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1003 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1000 - accuracy: 0.17 - ETA: 1:33 - loss: 3.0992 - accuracy: 0.17 - ETA: 1:33 - loss: 3.0997 - accuracy: 0.17 - ETA: 1:32 - loss: 3.0996 - accuracy: 0.17 - ETA: 1:32 - loss: 3.0989 - accuracy: 0.17 - ETA: 1:31 - loss: 3.0994 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1004 - accuracy: 0.17 - ETA: 1:30 - loss: 3.0988 - accuracy: 0.17 - ETA: 1:29 - loss: 3.0981 - accuracy: 0.17 - ETA: 1:29 - loss: 3.0974 - accuracy: 0.17 - ETA: 1:28 - loss: 3.0982 - accuracy: 0.17 - ETA: 1:27 - loss: 3.0992 - accuracy: 0.17 - ETA: 1:27 - loss: 3.0990 - accuracy: 0.17 - ETA: 1:26 - loss: 3.0989 - accuracy: 0.17 - ETA: 1:26 - loss: 3.0984 - accuracy: 0.17 - ETA: 1:25 - loss: 3.0982 - accuracy: 0.17 - ETA: 1:24 - loss: 3.0994 - accuracy: 0.1781"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.0993 - accuracy: 0.17 - ETA: 1:23 - loss: 3.0992 - accuracy: 0.17 - ETA: 1:23 - loss: 3.0999 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1005 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1006 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1001 - accuracy: 0.17 - ETA: 1:20 - loss: 3.0998 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1001 - accuracy: 0.17 - ETA: 1:19 - loss: 3.0997 - accuracy: 0.17 - ETA: 1:19 - loss: 3.0997 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1001 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1003 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1009 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1006 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1017 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1022 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1021 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1033 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1039 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1029 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1029 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1034 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1035 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1034 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1035 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1044 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1041 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1041 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1027 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1031 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1029 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1034 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1037 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1028 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1030 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1028 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1020 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1031 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1030 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1039 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1038 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1034 - accuracy: 0.17 - ETA: 59s - loss: 3.1037 - accuracy: 0.1775 - ETA: 59s - loss: 3.1033 - accuracy: 0.177 - ETA: 58s - loss: 3.1027 - accuracy: 0.177 - ETA: 58s - loss: 3.1032 - accuracy: 0.177 - ETA: 57s - loss: 3.1030 - accuracy: 0.177 - ETA: 56s - loss: 3.1038 - accuracy: 0.177 - ETA: 56s - loss: 3.1049 - accuracy: 0.177 - ETA: 55s - loss: 3.1048 - accuracy: 0.177 - ETA: 55s - loss: 3.1043 - accuracy: 0.177 - ETA: 54s - loss: 3.1038 - accuracy: 0.177 - ETA: 53s - loss: 3.1036 - accuracy: 0.177 - ETA: 53s - loss: 3.1028 - accuracy: 0.178 - ETA: 52s - loss: 3.1024 - accuracy: 0.178 - ETA: 52s - loss: 3.1026 - accuracy: 0.178 - ETA: 51s - loss: 3.1024 - accuracy: 0.178 - ETA: 51s - loss: 3.1020 - accuracy: 0.178 - ETA: 50s - loss: 3.1025 - accuracy: 0.177 - ETA: 49s - loss: 3.1027 - accuracy: 0.177 - ETA: 49s - loss: 3.1022 - accuracy: 0.177 - ETA: 48s - loss: 3.1029 - accuracy: 0.177 - ETA: 48s - loss: 3.1023 - accuracy: 0.177 - ETA: 47s - loss: 3.1018 - accuracy: 0.177 - ETA: 46s - loss: 3.1015 - accuracy: 0.177 - ETA: 46s - loss: 3.1017 - accuracy: 0.177 - ETA: 45s - loss: 3.1015 - accuracy: 0.177 - ETA: 45s - loss: 3.1016 - accuracy: 0.177 - ETA: 44s - loss: 3.1017 - accuracy: 0.177 - ETA: 44s - loss: 3.1018 - accuracy: 0.178 - ETA: 43s - loss: 3.1021 - accuracy: 0.177 - ETA: 42s - loss: 3.1023 - accuracy: 0.177 - ETA: 42s - loss: 3.1029 - accuracy: 0.177 - ETA: 41s - loss: 3.1031 - accuracy: 0.177 - ETA: 41s - loss: 3.1024 - accuracy: 0.177 - ETA: 40s - loss: 3.1025 - accuracy: 0.177 - ETA: 39s - loss: 3.1013 - accuracy: 0.177 - ETA: 39s - loss: 3.1015 - accuracy: 0.177 - ETA: 38s - loss: 3.1019 - accuracy: 0.177 - ETA: 38s - loss: 3.1018 - accuracy: 0.177 - ETA: 37s - loss: 3.1014 - accuracy: 0.177 - ETA: 36s - loss: 3.1011 - accuracy: 0.178 - ETA: 36s - loss: 3.1017 - accuracy: 0.177 - ETA: 35s - loss: 3.1010 - accuracy: 0.178 - ETA: 35s - loss: 3.1009 - accuracy: 0.178 - ETA: 34s - loss: 3.1004 - accuracy: 0.178 - ETA: 34s - loss: 3.1012 - accuracy: 0.177 - ETA: 33s - loss: 3.1015 - accuracy: 0.177 - ETA: 32s - loss: 3.1018 - accuracy: 0.177 - ETA: 32s - loss: 3.1018 - accuracy: 0.177 - ETA: 31s - loss: 3.1015 - accuracy: 0.177 - ETA: 31s - loss: 3.1019 - accuracy: 0.177 - ETA: 30s - loss: 3.1018 - accuracy: 0.177 - ETA: 29s - loss: 3.1016 - accuracy: 0.177 - ETA: 29s - loss: 3.1018 - accuracy: 0.177 - ETA: 28s - loss: 3.1015 - accuracy: 0.177 - ETA: 28s - loss: 3.1016 - accuracy: 0.177 - ETA: 27s - loss: 3.1012 - accuracy: 0.177 - ETA: 27s - loss: 3.1012 - accuracy: 0.177 - ETA: 26s - loss: 3.1013 - accuracy: 0.177 - ETA: 25s - loss: 3.1014 - accuracy: 0.177 - ETA: 25s - loss: 3.1018 - accuracy: 0.177 - ETA: 24s - loss: 3.1018 - accuracy: 0.177 - ETA: 24s - loss: 3.1012 - accuracy: 0.177 - ETA: 23s - loss: 3.1020 - accuracy: 0.177 - ETA: 22s - loss: 3.1024 - accuracy: 0.177 - ETA: 22s - loss: 3.1022 - accuracy: 0.177 - ETA: 21s - loss: 3.1025 - accuracy: 0.177 - ETA: 21s - loss: 3.1017 - accuracy: 0.177 - ETA: 20s - loss: 3.1016 - accuracy: 0.177 - ETA: 19s - loss: 3.1020 - accuracy: 0.177 - ETA: 19s - loss: 3.1024 - accuracy: 0.177 - ETA: 18s - loss: 3.1017 - accuracy: 0.177 - ETA: 18s - loss: 3.1016 - accuracy: 0.177 - ETA: 17s - loss: 3.1016 - accuracy: 0.177 - ETA: 17s - loss: 3.1020 - accuracy: 0.177 - ETA: 16s - loss: 3.1013 - accuracy: 0.177 - ETA: 15s - loss: 3.1008 - accuracy: 0.177 - ETA: 15s - loss: 3.1007 - accuracy: 0.177 - ETA: 14s - loss: 3.1007 - accuracy: 0.177 - ETA: 14s - loss: 3.1000 - accuracy: 0.177 - ETA: 13s - loss: 3.0998 - accuracy: 0.177 - ETA: 12s - loss: 3.0995 - accuracy: 0.177 - ETA: 12s - loss: 3.1002 - accuracy: 0.177 - ETA: 11s - loss: 3.1002 - accuracy: 0.177 - ETA: 11s - loss: 3.1006 - accuracy: 0.177 - ETA: 10s - loss: 3.1006 - accuracy: 0.177 - ETA: 10s - loss: 3.1006 - accuracy: 0.177 - ETA: 9s - loss: 3.1007 - accuracy: 0.177 - ETA: 8s - loss: 3.1003 - accuracy: 0.17 - ETA: 8s - loss: 3.0996 - accuracy: 0.17 - ETA: 7s - loss: 3.0999 - accuracy: 0.17 - ETA: 7s - loss: 3.1006 - accuracy: 0.17 - ETA: 6s - loss: 3.1008 - accuracy: 0.17 - ETA: 5s - loss: 3.1006 - accuracy: 0.17 - ETA: 5s - loss: 3.1006 - accuracy: 0.17 - ETA: 4s - loss: 3.1003 - accuracy: 0.17 - ETA: 4s - loss: 3.1001 - accuracy: 0.17 - ETA: 3s - loss: 3.0997 - accuracy: 0.17 - ETA: 2s - loss: 3.1000 - accuracy: 0.17 - ETA: 2s - loss: 3.1004 - accuracy: 0.17 - ETA: 1s - loss: 3.1006 - accuracy: 0.17 - ETA: 1s - loss: 3.1003 - accuracy: 0.17 - ETA: 0s - loss: 3.0998 - accuracy: 0.17 - ETA: 0s - loss: 3.0995 - accuracy: 0.17 - 207s 5ms/step - loss: 3.0993 - accuracy: 0.1776 - val_loss: 4.2307 - val_accuracy: 0.0385\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:37 - loss: 3.0381 - accuracy: 0.20 - ETA: 3:27 - loss: 3.0164 - accuracy: 0.17 - ETA: 3:27 - loss: 3.0973 - accuracy: 0.17 - ETA: 3:26 - loss: 3.0135 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0637 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0822 - accuracy: 0.18 - ETA: 3:18 - loss: 3.1001 - accuracy: 0.17 - ETA: 3:16 - loss: 3.0967 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0882 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0926 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0824 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0848 - accuracy: 0.18 - ETA: 3:09 - loss: 3.1040 - accuracy: 0.17 - ETA: 3:08 - loss: 3.0881 - accuracy: 0.17 - ETA: 3:07 - loss: 3.0985 - accuracy: 0.17 - ETA: 3:06 - loss: 3.1034 - accuracy: 0.17 - ETA: 3:05 - loss: 3.0973 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1118 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1129 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1104 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1201 - accuracy: 0.16 - ETA: 3:02 - loss: 3.1180 - accuracy: 0.16 - ETA: 3:01 - loss: 3.1120 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1141 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1197 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1153 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1200 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1196 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1258 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1318 - accuracy: 0.16 - ETA: 2:56 - loss: 3.1314 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1311 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1297 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1370 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1335 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1307 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1293 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1272 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1223 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1215 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1243 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1233 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1148 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1188 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1211 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1217 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1208 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1201 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1158 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1132 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1135 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1118 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1084 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1090 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1037 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1048 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1076 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1068 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1083 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1091 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1095 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1109 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1114 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1154 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1161 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1147 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1132 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1143 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1148 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1121 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1118 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1108 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1122 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1113 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1107 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1069 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1091 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1086 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1107 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1092 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1094 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1082 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1096 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1113 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1111 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1121 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1129 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1126 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1135 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1124 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1145 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1157 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1134 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1139 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1146 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1158 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1147 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1137 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1118 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1095 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1105 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1106 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1115 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1118 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1123 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1136 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1123 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1123 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1118 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1104 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1106 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1087 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1078 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1072 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1083 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1082 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1077 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1061 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1042 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1046 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1048 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1052 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1067 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1068 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1070 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1072 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1061 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1056 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1063 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1067 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1065 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1057 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1050 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1045 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1032 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1037 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1031 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1024 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1025 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1026 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1019 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1019 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1008 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1016 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1020 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1006 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1003 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1012 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1003 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1010 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1009 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1009 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1009 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1016 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1010 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1016 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1017 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1024 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1027 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1015 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1019 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1020 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1013 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1006 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1019 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1012 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1018 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1013 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1013 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1020 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1016 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1017 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1000 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1001 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1004 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1011 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1009 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1013 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1018 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1014 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1015 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1001 - accuracy: 0.17 - ETA: 1:26 - loss: 3.0997 - accuracy: 0.17 - ETA: 1:26 - loss: 3.0993 - accuracy: 0.17 - ETA: 1:25 - loss: 3.0988 - accuracy: 0.17 - ETA: 1:24 - loss: 3.0989 - accuracy: 0.1765"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.0996 - accuracy: 0.17 - ETA: 1:23 - loss: 3.0998 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1002 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1007 - accuracy: 0.17 - ETA: 1:21 - loss: 3.0999 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1005 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1009 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1003 - accuracy: 0.17 - ETA: 1:19 - loss: 3.0998 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1001 - accuracy: 0.17 - ETA: 1:18 - loss: 3.0997 - accuracy: 0.17 - ETA: 1:17 - loss: 3.0995 - accuracy: 0.17 - ETA: 1:17 - loss: 3.0998 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1007 - accuracy: 0.17 - ETA: 1:16 - loss: 3.0988 - accuracy: 0.17 - ETA: 1:15 - loss: 3.0987 - accuracy: 0.17 - ETA: 1:15 - loss: 3.0981 - accuracy: 0.17 - ETA: 1:14 - loss: 3.0976 - accuracy: 0.17 - ETA: 1:13 - loss: 3.0971 - accuracy: 0.17 - ETA: 1:13 - loss: 3.0962 - accuracy: 0.17 - ETA: 1:12 - loss: 3.0964 - accuracy: 0.17 - ETA: 1:12 - loss: 3.0968 - accuracy: 0.17 - ETA: 1:11 - loss: 3.0977 - accuracy: 0.17 - ETA: 1:10 - loss: 3.0982 - accuracy: 0.17 - ETA: 1:10 - loss: 3.0981 - accuracy: 0.17 - ETA: 1:09 - loss: 3.0966 - accuracy: 0.17 - ETA: 1:09 - loss: 3.0972 - accuracy: 0.17 - ETA: 1:08 - loss: 3.0969 - accuracy: 0.17 - ETA: 1:07 - loss: 3.0979 - accuracy: 0.17 - ETA: 1:07 - loss: 3.0982 - accuracy: 0.17 - ETA: 1:06 - loss: 3.0986 - accuracy: 0.17 - ETA: 1:06 - loss: 3.0989 - accuracy: 0.17 - ETA: 1:05 - loss: 3.0989 - accuracy: 0.17 - ETA: 1:05 - loss: 3.0982 - accuracy: 0.17 - ETA: 1:04 - loss: 3.0975 - accuracy: 0.17 - ETA: 1:03 - loss: 3.0974 - accuracy: 0.17 - ETA: 1:03 - loss: 3.0975 - accuracy: 0.17 - ETA: 1:02 - loss: 3.0973 - accuracy: 0.17 - ETA: 1:02 - loss: 3.0974 - accuracy: 0.17 - ETA: 1:01 - loss: 3.0985 - accuracy: 0.17 - ETA: 1:00 - loss: 3.0985 - accuracy: 0.17 - ETA: 1:00 - loss: 3.0985 - accuracy: 0.17 - ETA: 59s - loss: 3.0982 - accuracy: 0.1769 - ETA: 59s - loss: 3.0987 - accuracy: 0.176 - ETA: 58s - loss: 3.0989 - accuracy: 0.176 - ETA: 58s - loss: 3.0994 - accuracy: 0.176 - ETA: 57s - loss: 3.0990 - accuracy: 0.176 - ETA: 56s - loss: 3.0975 - accuracy: 0.176 - ETA: 56s - loss: 3.0975 - accuracy: 0.176 - ETA: 55s - loss: 3.0978 - accuracy: 0.176 - ETA: 55s - loss: 3.0972 - accuracy: 0.176 - ETA: 54s - loss: 3.0967 - accuracy: 0.177 - ETA: 53s - loss: 3.0969 - accuracy: 0.176 - ETA: 53s - loss: 3.0973 - accuracy: 0.176 - ETA: 52s - loss: 3.0974 - accuracy: 0.176 - ETA: 52s - loss: 3.0986 - accuracy: 0.176 - ETA: 51s - loss: 3.0999 - accuracy: 0.176 - ETA: 50s - loss: 3.1008 - accuracy: 0.176 - ETA: 50s - loss: 3.1014 - accuracy: 0.175 - ETA: 49s - loss: 3.1003 - accuracy: 0.176 - ETA: 49s - loss: 3.1011 - accuracy: 0.176 - ETA: 48s - loss: 3.1017 - accuracy: 0.176 - ETA: 48s - loss: 3.1015 - accuracy: 0.176 - ETA: 47s - loss: 3.1017 - accuracy: 0.176 - ETA: 46s - loss: 3.1015 - accuracy: 0.176 - ETA: 46s - loss: 3.1017 - accuracy: 0.176 - ETA: 45s - loss: 3.1023 - accuracy: 0.176 - ETA: 45s - loss: 3.1022 - accuracy: 0.176 - ETA: 44s - loss: 3.1013 - accuracy: 0.176 - ETA: 43s - loss: 3.1016 - accuracy: 0.176 - ETA: 43s - loss: 3.1014 - accuracy: 0.176 - ETA: 42s - loss: 3.1014 - accuracy: 0.176 - ETA: 42s - loss: 3.1021 - accuracy: 0.176 - ETA: 41s - loss: 3.1022 - accuracy: 0.176 - ETA: 41s - loss: 3.1030 - accuracy: 0.176 - ETA: 40s - loss: 3.1018 - accuracy: 0.176 - ETA: 39s - loss: 3.1017 - accuracy: 0.176 - ETA: 39s - loss: 3.1024 - accuracy: 0.176 - ETA: 38s - loss: 3.1017 - accuracy: 0.176 - ETA: 38s - loss: 3.1020 - accuracy: 0.176 - ETA: 37s - loss: 3.1020 - accuracy: 0.176 - ETA: 36s - loss: 3.1017 - accuracy: 0.176 - ETA: 36s - loss: 3.1021 - accuracy: 0.176 - ETA: 35s - loss: 3.1040 - accuracy: 0.176 - ETA: 35s - loss: 3.1041 - accuracy: 0.176 - ETA: 34s - loss: 3.1045 - accuracy: 0.176 - ETA: 33s - loss: 3.1055 - accuracy: 0.176 - ETA: 33s - loss: 3.1053 - accuracy: 0.176 - ETA: 32s - loss: 3.1051 - accuracy: 0.176 - ETA: 32s - loss: 3.1049 - accuracy: 0.176 - ETA: 31s - loss: 3.1051 - accuracy: 0.176 - ETA: 31s - loss: 3.1055 - accuracy: 0.176 - ETA: 30s - loss: 3.1051 - accuracy: 0.176 - ETA: 29s - loss: 3.1047 - accuracy: 0.176 - ETA: 29s - loss: 3.1041 - accuracy: 0.176 - ETA: 28s - loss: 3.1042 - accuracy: 0.176 - ETA: 28s - loss: 3.1038 - accuracy: 0.176 - ETA: 27s - loss: 3.1041 - accuracy: 0.176 - ETA: 26s - loss: 3.1036 - accuracy: 0.176 - ETA: 26s - loss: 3.1031 - accuracy: 0.176 - ETA: 25s - loss: 3.1031 - accuracy: 0.176 - ETA: 25s - loss: 3.1032 - accuracy: 0.176 - ETA: 24s - loss: 3.1034 - accuracy: 0.177 - ETA: 24s - loss: 3.1040 - accuracy: 0.177 - ETA: 23s - loss: 3.1040 - accuracy: 0.176 - ETA: 22s - loss: 3.1042 - accuracy: 0.176 - ETA: 22s - loss: 3.1048 - accuracy: 0.176 - ETA: 21s - loss: 3.1047 - accuracy: 0.176 - ETA: 21s - loss: 3.1045 - accuracy: 0.176 - ETA: 20s - loss: 3.1043 - accuracy: 0.176 - ETA: 19s - loss: 3.1047 - accuracy: 0.176 - ETA: 19s - loss: 3.1045 - accuracy: 0.176 - ETA: 18s - loss: 3.1048 - accuracy: 0.176 - ETA: 18s - loss: 3.1047 - accuracy: 0.176 - ETA: 17s - loss: 3.1051 - accuracy: 0.176 - ETA: 17s - loss: 3.1051 - accuracy: 0.176 - ETA: 16s - loss: 3.1054 - accuracy: 0.176 - ETA: 15s - loss: 3.1051 - accuracy: 0.176 - ETA: 15s - loss: 3.1048 - accuracy: 0.176 - ETA: 14s - loss: 3.1051 - accuracy: 0.176 - ETA: 14s - loss: 3.1055 - accuracy: 0.176 - ETA: 13s - loss: 3.1054 - accuracy: 0.176 - ETA: 12s - loss: 3.1051 - accuracy: 0.176 - ETA: 12s - loss: 3.1057 - accuracy: 0.176 - ETA: 11s - loss: 3.1061 - accuracy: 0.176 - ETA: 11s - loss: 3.1058 - accuracy: 0.176 - ETA: 10s - loss: 3.1053 - accuracy: 0.176 - ETA: 10s - loss: 3.1053 - accuracy: 0.176 - ETA: 9s - loss: 3.1049 - accuracy: 0.176 - ETA: 8s - loss: 3.1043 - accuracy: 0.17 - ETA: 8s - loss: 3.1040 - accuracy: 0.17 - ETA: 7s - loss: 3.1038 - accuracy: 0.17 - ETA: 7s - loss: 3.1031 - accuracy: 0.17 - ETA: 6s - loss: 3.1027 - accuracy: 0.17 - ETA: 5s - loss: 3.1023 - accuracy: 0.17 - ETA: 5s - loss: 3.1020 - accuracy: 0.17 - ETA: 4s - loss: 3.1021 - accuracy: 0.17 - ETA: 4s - loss: 3.1025 - accuracy: 0.17 - ETA: 3s - loss: 3.1026 - accuracy: 0.17 - ETA: 2s - loss: 3.1032 - accuracy: 0.17 - ETA: 2s - loss: 3.1038 - accuracy: 0.17 - ETA: 1s - loss: 3.1038 - accuracy: 0.17 - ETA: 1s - loss: 3.1039 - accuracy: 0.17 - ETA: 0s - loss: 3.1039 - accuracy: 0.17 - ETA: 0s - loss: 3.1041 - accuracy: 0.17 - 205s 5ms/step - loss: 3.1039 - accuracy: 0.1769 - val_loss: 4.0277 - val_accuracy: 0.0369\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:29 - loss: 3.2378 - accuracy: 0.14 - ETA: 3:18 - loss: 3.1526 - accuracy: 0.19 - ETA: 3:13 - loss: 3.1212 - accuracy: 0.18 - ETA: 3:13 - loss: 3.1169 - accuracy: 0.18 - ETA: 3:11 - loss: 3.1092 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0860 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0900 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0764 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0559 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0612 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0697 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0657 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0580 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0549 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0534 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0641 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0607 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0551 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0529 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0451 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0437 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0484 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0552 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0549 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0550 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0521 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0548 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0524 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0588 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0674 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0675 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0587 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0567 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0579 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0618 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0708 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0672 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0683 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0625 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0623 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0686 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0714 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0714 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0704 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0686 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0714 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0720 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0711 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0733 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0734 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0762 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0775 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0809 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0832 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0866 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0880 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0841 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0822 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0848 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0834 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0821 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0831 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0811 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0798 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0826 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0827 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0850 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0889 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0922 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0917 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0947 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0934 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0934 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0940 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0955 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0963 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0948 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0975 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0978 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0989 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0988 - accuracy: 0.18 - ETA: 2:26 - loss: 3.1014 - accuracy: 0.18 - ETA: 2:25 - loss: 3.1025 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0998 - accuracy: 0.18 - ETA: 2:24 - loss: 3.1020 - accuracy: 0.18 - ETA: 2:24 - loss: 3.1043 - accuracy: 0.18 - ETA: 2:23 - loss: 3.1061 - accuracy: 0.18 - ETA: 2:23 - loss: 3.1061 - accuracy: 0.18 - ETA: 2:22 - loss: 3.1066 - accuracy: 0.18 - ETA: 2:22 - loss: 3.1056 - accuracy: 0.18 - ETA: 2:21 - loss: 3.1068 - accuracy: 0.18 - ETA: 2:21 - loss: 3.1078 - accuracy: 0.18 - ETA: 2:20 - loss: 3.1101 - accuracy: 0.18 - ETA: 2:19 - loss: 3.1107 - accuracy: 0.18 - ETA: 2:19 - loss: 3.1128 - accuracy: 0.18 - ETA: 2:18 - loss: 3.1112 - accuracy: 0.18 - ETA: 2:17 - loss: 3.1097 - accuracy: 0.18 - ETA: 2:17 - loss: 3.1085 - accuracy: 0.18 - ETA: 2:16 - loss: 3.1072 - accuracy: 0.18 - ETA: 2:16 - loss: 3.1057 - accuracy: 0.18 - ETA: 2:15 - loss: 3.1049 - accuracy: 0.18 - ETA: 2:14 - loss: 3.1058 - accuracy: 0.18 - ETA: 2:14 - loss: 3.1056 - accuracy: 0.18 - ETA: 2:13 - loss: 3.1057 - accuracy: 0.18 - ETA: 2:13 - loss: 3.1047 - accuracy: 0.18 - ETA: 2:12 - loss: 3.1032 - accuracy: 0.18 - ETA: 2:11 - loss: 3.1011 - accuracy: 0.18 - ETA: 2:11 - loss: 3.1011 - accuracy: 0.18 - ETA: 2:10 - loss: 3.1008 - accuracy: 0.18 - ETA: 2:09 - loss: 3.1015 - accuracy: 0.18 - ETA: 2:09 - loss: 3.1029 - accuracy: 0.18 - ETA: 2:08 - loss: 3.1033 - accuracy: 0.18 - ETA: 2:08 - loss: 3.1030 - accuracy: 0.18 - ETA: 2:07 - loss: 3.1023 - accuracy: 0.18 - ETA: 2:07 - loss: 3.1024 - accuracy: 0.18 - ETA: 2:06 - loss: 3.1020 - accuracy: 0.18 - ETA: 2:06 - loss: 3.1000 - accuracy: 0.18 - ETA: 2:05 - loss: 3.1003 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0978 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0979 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0970 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0965 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0984 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0973 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0956 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0977 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0987 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0993 - accuracy: 0.18 - ETA: 1:59 - loss: 3.1010 - accuracy: 0.18 - ETA: 1:58 - loss: 3.1009 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0997 - accuracy: 0.18 - ETA: 1:57 - loss: 3.1007 - accuracy: 0.18 - ETA: 1:57 - loss: 3.1009 - accuracy: 0.18 - ETA: 1:56 - loss: 3.1002 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0989 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0986 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0985 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0979 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0985 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0975 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0980 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0976 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0968 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0945 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0940 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0942 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0948 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0935 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0940 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0932 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0935 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0928 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0939 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0928 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0919 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0920 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0934 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0930 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0920 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0922 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0927 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0940 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0934 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0939 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0946 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0947 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0950 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0960 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0968 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0963 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0965 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0964 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0967 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0971 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0969 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0966 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0969 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0972 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0981 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0973 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0971 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0963 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0969 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0972 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0960 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0965 - accuracy: 0.1808"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.0969 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0963 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0968 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0978 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0975 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0975 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0974 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0981 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0983 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0990 - accuracy: 0.17 - ETA: 1:19 - loss: 3.0991 - accuracy: 0.17 - ETA: 1:18 - loss: 3.0991 - accuracy: 0.17 - ETA: 1:18 - loss: 3.0990 - accuracy: 0.17 - ETA: 1:17 - loss: 3.0993 - accuracy: 0.17 - ETA: 1:16 - loss: 3.0994 - accuracy: 0.17 - ETA: 1:16 - loss: 3.0995 - accuracy: 0.17 - ETA: 1:15 - loss: 3.0982 - accuracy: 0.17 - ETA: 1:15 - loss: 3.0985 - accuracy: 0.17 - ETA: 1:14 - loss: 3.0981 - accuracy: 0.17 - ETA: 1:13 - loss: 3.0972 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0976 - accuracy: 0.17 - ETA: 1:12 - loss: 3.0981 - accuracy: 0.17 - ETA: 1:12 - loss: 3.0979 - accuracy: 0.17 - ETA: 1:11 - loss: 3.0978 - accuracy: 0.17 - ETA: 1:10 - loss: 3.0976 - accuracy: 0.17 - ETA: 1:10 - loss: 3.0982 - accuracy: 0.17 - ETA: 1:09 - loss: 3.0981 - accuracy: 0.17 - ETA: 1:09 - loss: 3.0980 - accuracy: 0.17 - ETA: 1:08 - loss: 3.0990 - accuracy: 0.17 - ETA: 1:07 - loss: 3.0988 - accuracy: 0.17 - ETA: 1:07 - loss: 3.0995 - accuracy: 0.17 - ETA: 1:06 - loss: 3.0995 - accuracy: 0.17 - ETA: 1:06 - loss: 3.0996 - accuracy: 0.17 - ETA: 1:05 - loss: 3.0989 - accuracy: 0.17 - ETA: 1:04 - loss: 3.0984 - accuracy: 0.17 - ETA: 1:04 - loss: 3.0983 - accuracy: 0.17 - ETA: 1:03 - loss: 3.0984 - accuracy: 0.17 - ETA: 1:03 - loss: 3.0990 - accuracy: 0.17 - ETA: 1:02 - loss: 3.0988 - accuracy: 0.17 - ETA: 1:01 - loss: 3.0991 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1003 - accuracy: 0.17 - ETA: 1:00 - loss: 3.0999 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1000 - accuracy: 0.17 - ETA: 59s - loss: 3.1000 - accuracy: 0.1787 - ETA: 59s - loss: 3.0992 - accuracy: 0.178 - ETA: 58s - loss: 3.0988 - accuracy: 0.179 - ETA: 57s - loss: 3.0989 - accuracy: 0.179 - ETA: 57s - loss: 3.0988 - accuracy: 0.179 - ETA: 56s - loss: 3.0981 - accuracy: 0.179 - ETA: 56s - loss: 3.0978 - accuracy: 0.179 - ETA: 55s - loss: 3.0969 - accuracy: 0.179 - ETA: 54s - loss: 3.0970 - accuracy: 0.179 - ETA: 54s - loss: 3.0966 - accuracy: 0.179 - ETA: 53s - loss: 3.0968 - accuracy: 0.179 - ETA: 53s - loss: 3.0970 - accuracy: 0.179 - ETA: 52s - loss: 3.0970 - accuracy: 0.179 - ETA: 51s - loss: 3.0968 - accuracy: 0.179 - ETA: 51s - loss: 3.0967 - accuracy: 0.179 - ETA: 50s - loss: 3.0961 - accuracy: 0.179 - ETA: 50s - loss: 3.0965 - accuracy: 0.179 - ETA: 49s - loss: 3.0970 - accuracy: 0.179 - ETA: 48s - loss: 3.0967 - accuracy: 0.179 - ETA: 48s - loss: 3.0967 - accuracy: 0.179 - ETA: 47s - loss: 3.0970 - accuracy: 0.179 - ETA: 47s - loss: 3.0957 - accuracy: 0.180 - ETA: 46s - loss: 3.0958 - accuracy: 0.179 - ETA: 46s - loss: 3.0954 - accuracy: 0.180 - ETA: 45s - loss: 3.0954 - accuracy: 0.180 - ETA: 44s - loss: 3.0956 - accuracy: 0.180 - ETA: 44s - loss: 3.0949 - accuracy: 0.180 - ETA: 43s - loss: 3.0956 - accuracy: 0.180 - ETA: 43s - loss: 3.0958 - accuracy: 0.179 - ETA: 42s - loss: 3.0958 - accuracy: 0.179 - ETA: 41s - loss: 3.0961 - accuracy: 0.179 - ETA: 41s - loss: 3.0967 - accuracy: 0.179 - ETA: 40s - loss: 3.0977 - accuracy: 0.179 - ETA: 40s - loss: 3.0981 - accuracy: 0.179 - ETA: 39s - loss: 3.0980 - accuracy: 0.179 - ETA: 38s - loss: 3.0982 - accuracy: 0.179 - ETA: 38s - loss: 3.0978 - accuracy: 0.179 - ETA: 37s - loss: 3.0978 - accuracy: 0.179 - ETA: 37s - loss: 3.0969 - accuracy: 0.179 - ETA: 36s - loss: 3.0969 - accuracy: 0.179 - ETA: 35s - loss: 3.0978 - accuracy: 0.179 - ETA: 35s - loss: 3.0976 - accuracy: 0.179 - ETA: 34s - loss: 3.0971 - accuracy: 0.179 - ETA: 34s - loss: 3.0977 - accuracy: 0.179 - ETA: 33s - loss: 3.0974 - accuracy: 0.179 - ETA: 33s - loss: 3.0975 - accuracy: 0.179 - ETA: 32s - loss: 3.0975 - accuracy: 0.179 - ETA: 31s - loss: 3.0977 - accuracy: 0.179 - ETA: 31s - loss: 3.0978 - accuracy: 0.179 - ETA: 30s - loss: 3.0977 - accuracy: 0.179 - ETA: 30s - loss: 3.0976 - accuracy: 0.179 - ETA: 29s - loss: 3.0981 - accuracy: 0.179 - ETA: 28s - loss: 3.0985 - accuracy: 0.179 - ETA: 28s - loss: 3.0986 - accuracy: 0.179 - ETA: 27s - loss: 3.0985 - accuracy: 0.179 - ETA: 27s - loss: 3.0986 - accuracy: 0.179 - ETA: 26s - loss: 3.0996 - accuracy: 0.179 - ETA: 25s - loss: 3.0989 - accuracy: 0.179 - ETA: 25s - loss: 3.0998 - accuracy: 0.179 - ETA: 24s - loss: 3.1004 - accuracy: 0.179 - ETA: 24s - loss: 3.1005 - accuracy: 0.179 - ETA: 23s - loss: 3.1003 - accuracy: 0.179 - ETA: 23s - loss: 3.0999 - accuracy: 0.179 - ETA: 22s - loss: 3.1002 - accuracy: 0.179 - ETA: 21s - loss: 3.1010 - accuracy: 0.179 - ETA: 21s - loss: 3.1018 - accuracy: 0.179 - ETA: 20s - loss: 3.1020 - accuracy: 0.178 - ETA: 20s - loss: 3.1017 - accuracy: 0.179 - ETA: 19s - loss: 3.1017 - accuracy: 0.179 - ETA: 18s - loss: 3.1020 - accuracy: 0.179 - ETA: 18s - loss: 3.1019 - accuracy: 0.179 - ETA: 17s - loss: 3.1015 - accuracy: 0.179 - ETA: 17s - loss: 3.1010 - accuracy: 0.179 - ETA: 16s - loss: 3.1012 - accuracy: 0.179 - ETA: 15s - loss: 3.1019 - accuracy: 0.179 - ETA: 15s - loss: 3.1017 - accuracy: 0.179 - ETA: 14s - loss: 3.1016 - accuracy: 0.179 - ETA: 14s - loss: 3.1015 - accuracy: 0.179 - ETA: 13s - loss: 3.1010 - accuracy: 0.179 - ETA: 13s - loss: 3.1015 - accuracy: 0.179 - ETA: 12s - loss: 3.1011 - accuracy: 0.179 - ETA: 11s - loss: 3.1012 - accuracy: 0.179 - ETA: 11s - loss: 3.1013 - accuracy: 0.179 - ETA: 10s - loss: 3.1014 - accuracy: 0.179 - ETA: 10s - loss: 3.1015 - accuracy: 0.179 - ETA: 9s - loss: 3.1016 - accuracy: 0.179 - ETA: 8s - loss: 3.1012 - accuracy: 0.17 - ETA: 8s - loss: 3.1006 - accuracy: 0.17 - ETA: 7s - loss: 3.1006 - accuracy: 0.17 - ETA: 7s - loss: 3.1007 - accuracy: 0.17 - ETA: 6s - loss: 3.1001 - accuracy: 0.17 - ETA: 5s - loss: 3.1010 - accuracy: 0.17 - ETA: 5s - loss: 3.1007 - accuracy: 0.17 - ETA: 4s - loss: 3.1013 - accuracy: 0.17 - ETA: 4s - loss: 3.1011 - accuracy: 0.17 - ETA: 3s - loss: 3.1015 - accuracy: 0.17 - ETA: 2s - loss: 3.1016 - accuracy: 0.17 - ETA: 2s - loss: 3.1023 - accuracy: 0.17 - ETA: 1s - loss: 3.1021 - accuracy: 0.17 - ETA: 1s - loss: 3.1026 - accuracy: 0.17 - ETA: 0s - loss: 3.1028 - accuracy: 0.17 - ETA: 0s - loss: 3.1021 - accuracy: 0.17 - 207s 5ms/step - loss: 3.1021 - accuracy: 0.1791 - val_loss: 4.0966 - val_accuracy: 0.0368\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:14 - loss: 2.9469 - accuracy: 0.20 - ETA: 3:07 - loss: 2.9426 - accuracy: 0.22 - ETA: 3:07 - loss: 2.9689 - accuracy: 0.22 - ETA: 3:05 - loss: 3.0059 - accuracy: 0.21 - ETA: 3:05 - loss: 3.0205 - accuracy: 0.20 - ETA: 3:05 - loss: 3.0224 - accuracy: 0.20 - ETA: 3:05 - loss: 3.0436 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0793 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0763 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0976 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0954 - accuracy: 0.18 - ETA: 3:08 - loss: 3.1311 - accuracy: 0.18 - ETA: 3:08 - loss: 3.1328 - accuracy: 0.18 - ETA: 3:07 - loss: 3.1263 - accuracy: 0.18 - ETA: 3:06 - loss: 3.1409 - accuracy: 0.18 - ETA: 3:06 - loss: 3.1392 - accuracy: 0.18 - ETA: 3:05 - loss: 3.1296 - accuracy: 0.18 - ETA: 3:04 - loss: 3.1262 - accuracy: 0.18 - ETA: 3:03 - loss: 3.1248 - accuracy: 0.18 - ETA: 3:03 - loss: 3.1246 - accuracy: 0.18 - ETA: 3:02 - loss: 3.1192 - accuracy: 0.18 - ETA: 3:01 - loss: 3.1222 - accuracy: 0.18 - ETA: 3:00 - loss: 3.1091 - accuracy: 0.18 - ETA: 3:00 - loss: 3.1069 - accuracy: 0.18 - ETA: 2:59 - loss: 3.1046 - accuracy: 0.18 - ETA: 2:59 - loss: 3.1175 - accuracy: 0.18 - ETA: 2:58 - loss: 3.1114 - accuracy: 0.18 - ETA: 2:57 - loss: 3.1104 - accuracy: 0.18 - ETA: 2:56 - loss: 3.1154 - accuracy: 0.18 - ETA: 2:56 - loss: 3.1149 - accuracy: 0.18 - ETA: 2:55 - loss: 3.1110 - accuracy: 0.18 - ETA: 2:54 - loss: 3.1156 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1155 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1175 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1210 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1185 - accuracy: 0.18 - ETA: 2:52 - loss: 3.1168 - accuracy: 0.18 - ETA: 2:52 - loss: 3.1150 - accuracy: 0.18 - ETA: 2:51 - loss: 3.1126 - accuracy: 0.18 - ETA: 2:50 - loss: 3.1201 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1191 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1224 - accuracy: 0.18 - ETA: 2:49 - loss: 3.1225 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1260 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1285 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1310 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1343 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1342 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1306 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1320 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1346 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1382 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1355 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1346 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1338 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1328 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1325 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1352 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1336 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1330 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1323 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1348 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1336 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1345 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1359 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1409 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1410 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1402 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1425 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1429 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1453 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1462 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1439 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1427 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1431 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1413 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1429 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1426 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1416 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1399 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1389 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1380 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1396 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1425 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1399 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1424 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1411 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1407 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1411 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1406 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1410 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1392 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1398 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1394 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1397 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1407 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1391 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1389 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1394 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1397 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1383 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1362 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1372 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1366 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1359 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1352 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1352 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1367 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1363 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1347 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1340 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1330 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1325 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1331 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1318 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1301 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1305 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1306 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1292 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1281 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1287 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1288 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1287 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1295 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1282 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1275 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1280 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1282 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1286 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1276 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1271 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1269 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1254 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1247 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1240 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1229 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1224 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1224 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1226 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1226 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1229 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1233 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1236 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1242 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1253 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1247 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1250 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1246 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1244 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1235 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1225 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1220 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1228 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1225 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1234 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1232 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1216 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1206 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1204 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1201 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1206 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1211 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1212 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1208 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1203 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1206 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1203 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1200 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1214 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1217 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1214 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1199 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1193 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1201 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1198 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1194 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1197 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1191 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1195 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1199 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1188 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1179 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1192 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1180 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1175 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1174 - accuracy: 0.1738"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1168 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1163 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1162 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1167 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1173 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1187 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1184 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1173 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1172 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1171 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1160 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1169 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1168 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1168 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1175 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1179 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1179 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1179 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1189 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1200 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1194 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1190 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1191 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1195 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1200 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1202 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1201 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1204 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1210 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1207 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1202 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1208 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1210 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1199 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1203 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1208 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1205 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1201 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1200 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1191 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1190 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1192 - accuracy: 0.17 - ETA: 59s - loss: 3.1205 - accuracy: 0.1735 - ETA: 59s - loss: 3.1205 - accuracy: 0.173 - ETA: 58s - loss: 3.1212 - accuracy: 0.173 - ETA: 58s - loss: 3.1209 - accuracy: 0.173 - ETA: 57s - loss: 3.1216 - accuracy: 0.173 - ETA: 56s - loss: 3.1221 - accuracy: 0.173 - ETA: 56s - loss: 3.1226 - accuracy: 0.172 - ETA: 55s - loss: 3.1227 - accuracy: 0.172 - ETA: 55s - loss: 3.1229 - accuracy: 0.172 - ETA: 54s - loss: 3.1224 - accuracy: 0.172 - ETA: 54s - loss: 3.1228 - accuracy: 0.172 - ETA: 53s - loss: 3.1227 - accuracy: 0.172 - ETA: 52s - loss: 3.1224 - accuracy: 0.172 - ETA: 52s - loss: 3.1230 - accuracy: 0.172 - ETA: 51s - loss: 3.1233 - accuracy: 0.172 - ETA: 51s - loss: 3.1233 - accuracy: 0.172 - ETA: 50s - loss: 3.1230 - accuracy: 0.172 - ETA: 49s - loss: 3.1226 - accuracy: 0.172 - ETA: 49s - loss: 3.1226 - accuracy: 0.172 - ETA: 48s - loss: 3.1230 - accuracy: 0.172 - ETA: 48s - loss: 3.1234 - accuracy: 0.172 - ETA: 47s - loss: 3.1233 - accuracy: 0.172 - ETA: 46s - loss: 3.1234 - accuracy: 0.172 - ETA: 46s - loss: 3.1229 - accuracy: 0.172 - ETA: 45s - loss: 3.1232 - accuracy: 0.172 - ETA: 45s - loss: 3.1231 - accuracy: 0.172 - ETA: 44s - loss: 3.1232 - accuracy: 0.172 - ETA: 44s - loss: 3.1225 - accuracy: 0.172 - ETA: 43s - loss: 3.1220 - accuracy: 0.172 - ETA: 42s - loss: 3.1221 - accuracy: 0.172 - ETA: 42s - loss: 3.1216 - accuracy: 0.172 - ETA: 41s - loss: 3.1216 - accuracy: 0.172 - ETA: 41s - loss: 3.1216 - accuracy: 0.172 - ETA: 40s - loss: 3.1217 - accuracy: 0.172 - ETA: 39s - loss: 3.1216 - accuracy: 0.172 - ETA: 39s - loss: 3.1208 - accuracy: 0.172 - ETA: 38s - loss: 3.1204 - accuracy: 0.172 - ETA: 38s - loss: 3.1202 - accuracy: 0.172 - ETA: 37s - loss: 3.1202 - accuracy: 0.172 - ETA: 36s - loss: 3.1211 - accuracy: 0.172 - ETA: 36s - loss: 3.1213 - accuracy: 0.172 - ETA: 35s - loss: 3.1207 - accuracy: 0.172 - ETA: 35s - loss: 3.1199 - accuracy: 0.172 - ETA: 34s - loss: 3.1200 - accuracy: 0.172 - ETA: 34s - loss: 3.1202 - accuracy: 0.172 - ETA: 33s - loss: 3.1200 - accuracy: 0.172 - ETA: 32s - loss: 3.1201 - accuracy: 0.172 - ETA: 32s - loss: 3.1195 - accuracy: 0.172 - ETA: 31s - loss: 3.1185 - accuracy: 0.173 - ETA: 31s - loss: 3.1186 - accuracy: 0.173 - ETA: 30s - loss: 3.1182 - accuracy: 0.173 - ETA: 29s - loss: 3.1187 - accuracy: 0.173 - ETA: 29s - loss: 3.1189 - accuracy: 0.172 - ETA: 28s - loss: 3.1187 - accuracy: 0.172 - ETA: 28s - loss: 3.1187 - accuracy: 0.172 - ETA: 27s - loss: 3.1182 - accuracy: 0.172 - ETA: 27s - loss: 3.1180 - accuracy: 0.172 - ETA: 26s - loss: 3.1180 - accuracy: 0.173 - ETA: 25s - loss: 3.1181 - accuracy: 0.173 - ETA: 25s - loss: 3.1184 - accuracy: 0.172 - ETA: 24s - loss: 3.1183 - accuracy: 0.172 - ETA: 24s - loss: 3.1182 - accuracy: 0.172 - ETA: 23s - loss: 3.1174 - accuracy: 0.173 - ETA: 22s - loss: 3.1174 - accuracy: 0.173 - ETA: 22s - loss: 3.1166 - accuracy: 0.173 - ETA: 21s - loss: 3.1164 - accuracy: 0.173 - ETA: 21s - loss: 3.1165 - accuracy: 0.173 - ETA: 20s - loss: 3.1161 - accuracy: 0.173 - ETA: 19s - loss: 3.1162 - accuracy: 0.173 - ETA: 19s - loss: 3.1158 - accuracy: 0.173 - ETA: 18s - loss: 3.1156 - accuracy: 0.173 - ETA: 18s - loss: 3.1153 - accuracy: 0.173 - ETA: 17s - loss: 3.1150 - accuracy: 0.173 - ETA: 17s - loss: 3.1151 - accuracy: 0.173 - ETA: 16s - loss: 3.1155 - accuracy: 0.173 - ETA: 15s - loss: 3.1155 - accuracy: 0.173 - ETA: 15s - loss: 3.1158 - accuracy: 0.173 - ETA: 14s - loss: 3.1156 - accuracy: 0.173 - ETA: 14s - loss: 3.1156 - accuracy: 0.173 - ETA: 13s - loss: 3.1153 - accuracy: 0.173 - ETA: 12s - loss: 3.1148 - accuracy: 0.173 - ETA: 12s - loss: 3.1146 - accuracy: 0.173 - ETA: 11s - loss: 3.1145 - accuracy: 0.173 - ETA: 11s - loss: 3.1132 - accuracy: 0.174 - ETA: 10s - loss: 3.1122 - accuracy: 0.174 - ETA: 10s - loss: 3.1116 - accuracy: 0.174 - ETA: 9s - loss: 3.1114 - accuracy: 0.174 - ETA: 8s - loss: 3.1112 - accuracy: 0.17 - ETA: 8s - loss: 3.1112 - accuracy: 0.17 - ETA: 7s - loss: 3.1113 - accuracy: 0.17 - ETA: 7s - loss: 3.1111 - accuracy: 0.17 - ETA: 6s - loss: 3.1115 - accuracy: 0.17 - ETA: 5s - loss: 3.1125 - accuracy: 0.17 - ETA: 5s - loss: 3.1128 - accuracy: 0.17 - ETA: 4s - loss: 3.1129 - accuracy: 0.17 - ETA: 4s - loss: 3.1131 - accuracy: 0.17 - ETA: 3s - loss: 3.1127 - accuracy: 0.17 - ETA: 2s - loss: 3.1125 - accuracy: 0.17 - ETA: 2s - loss: 3.1128 - accuracy: 0.17 - ETA: 1s - loss: 3.1129 - accuracy: 0.17 - ETA: 1s - loss: 3.1131 - accuracy: 0.17 - ETA: 0s - loss: 3.1129 - accuracy: 0.17 - ETA: 0s - loss: 3.1122 - accuracy: 0.17 - 205s 5ms/step - loss: 3.1122 - accuracy: 0.1744 - val_loss: 4.1189 - val_accuracy: 0.0391\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:18 - loss: 2.9334 - accuracy: 0.20 - ETA: 3:09 - loss: 2.9808 - accuracy: 0.20 - ETA: 3:09 - loss: 3.0328 - accuracy: 0.19 - ETA: 3:10 - loss: 2.9771 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0178 - accuracy: 0.17 - ETA: 3:08 - loss: 3.0508 - accuracy: 0.17 - ETA: 3:08 - loss: 3.0522 - accuracy: 0.17 - ETA: 3:07 - loss: 3.0397 - accuracy: 0.17 - ETA: 3:06 - loss: 3.0331 - accuracy: 0.17 - ETA: 3:06 - loss: 3.0231 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0274 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0354 - accuracy: 0.17 - ETA: 3:04 - loss: 3.0345 - accuracy: 0.17 - ETA: 3:05 - loss: 3.0385 - accuracy: 0.17 - ETA: 3:05 - loss: 3.0373 - accuracy: 0.17 - ETA: 3:05 - loss: 3.0425 - accuracy: 0.17 - ETA: 3:05 - loss: 3.0371 - accuracy: 0.17 - ETA: 3:04 - loss: 3.0471 - accuracy: 0.17 - ETA: 3:03 - loss: 3.0572 - accuracy: 0.17 - ETA: 3:02 - loss: 3.0506 - accuracy: 0.17 - ETA: 3:01 - loss: 3.0450 - accuracy: 0.17 - ETA: 3:01 - loss: 3.0398 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0495 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0525 - accuracy: 0.17 - ETA: 3:00 - loss: 3.0566 - accuracy: 0.17 - ETA: 2:59 - loss: 3.0579 - accuracy: 0.17 - ETA: 2:58 - loss: 3.0601 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0591 - accuracy: 0.17 - ETA: 2:57 - loss: 3.0644 - accuracy: 0.17 - ETA: 2:56 - loss: 3.0611 - accuracy: 0.17 - ETA: 2:55 - loss: 3.0671 - accuracy: 0.17 - ETA: 2:55 - loss: 3.0659 - accuracy: 0.17 - ETA: 2:54 - loss: 3.0726 - accuracy: 0.17 - ETA: 2:54 - loss: 3.0808 - accuracy: 0.17 - ETA: 2:53 - loss: 3.0795 - accuracy: 0.17 - ETA: 2:52 - loss: 3.0829 - accuracy: 0.17 - ETA: 2:52 - loss: 3.0914 - accuracy: 0.17 - ETA: 2:51 - loss: 3.0898 - accuracy: 0.17 - ETA: 2:50 - loss: 3.0874 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0997 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1053 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1055 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1076 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1071 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1086 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1102 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1122 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1114 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1108 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1104 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1123 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1146 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1111 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1152 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1165 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1170 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1160 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1199 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1187 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1252 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1301 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1301 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1272 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1281 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1274 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1279 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1283 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1294 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1292 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1293 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1311 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1314 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1334 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1325 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1302 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1317 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1306 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1320 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1320 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1324 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1320 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1302 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1312 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1307 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1313 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1323 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1353 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1351 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1373 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1366 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1379 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1361 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1355 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1349 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1362 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1369 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1380 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1357 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1355 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1369 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1365 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1348 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1342 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1327 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1338 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1350 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1356 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1362 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1361 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1338 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1340 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1344 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1354 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1351 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1340 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1337 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1317 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1326 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1317 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1319 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1311 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1323 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1306 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1301 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1323 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1325 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1323 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1309 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1309 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1295 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1316 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1315 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1329 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1318 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1313 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1321 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1323 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1329 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1345 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1326 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1317 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1318 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1322 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1331 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1340 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1328 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1334 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1341 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1344 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1364 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1362 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1364 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1375 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1374 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1360 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1365 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1377 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1371 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1377 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1377 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1372 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1374 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1377 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1383 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1389 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1398 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1400 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1411 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1420 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1410 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1419 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1422 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1426 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1428 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1409 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1393 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1398 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1398 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1407 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1412 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1411 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1413 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1411 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1406 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1405 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1411 - accuracy: 0.1723"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1407 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1413 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1403 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1412 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1416 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1413 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1410 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1405 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1407 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1410 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1411 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1416 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1416 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1412 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1410 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1405 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1399 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1392 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1391 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1394 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1398 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1406 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1401 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1396 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1409 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1406 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1391 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1379 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1371 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1372 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1360 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1365 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1363 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1361 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1363 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1369 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1360 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1354 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1351 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1361 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1360 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1366 - accuracy: 0.17 - ETA: 59s - loss: 3.1367 - accuracy: 0.1727 - ETA: 59s - loss: 3.1366 - accuracy: 0.172 - ETA: 58s - loss: 3.1363 - accuracy: 0.172 - ETA: 58s - loss: 3.1362 - accuracy: 0.172 - ETA: 57s - loss: 3.1361 - accuracy: 0.172 - ETA: 56s - loss: 3.1364 - accuracy: 0.172 - ETA: 56s - loss: 3.1351 - accuracy: 0.173 - ETA: 55s - loss: 3.1349 - accuracy: 0.173 - ETA: 55s - loss: 3.1349 - accuracy: 0.173 - ETA: 54s - loss: 3.1351 - accuracy: 0.173 - ETA: 54s - loss: 3.1349 - accuracy: 0.173 - ETA: 53s - loss: 3.1349 - accuracy: 0.173 - ETA: 52s - loss: 3.1351 - accuracy: 0.173 - ETA: 52s - loss: 3.1360 - accuracy: 0.173 - ETA: 51s - loss: 3.1359 - accuracy: 0.172 - ETA: 51s - loss: 3.1356 - accuracy: 0.173 - ETA: 50s - loss: 3.1362 - accuracy: 0.172 - ETA: 49s - loss: 3.1364 - accuracy: 0.172 - ETA: 49s - loss: 3.1364 - accuracy: 0.172 - ETA: 48s - loss: 3.1367 - accuracy: 0.172 - ETA: 48s - loss: 3.1365 - accuracy: 0.173 - ETA: 47s - loss: 3.1366 - accuracy: 0.173 - ETA: 46s - loss: 3.1361 - accuracy: 0.173 - ETA: 46s - loss: 3.1365 - accuracy: 0.173 - ETA: 45s - loss: 3.1364 - accuracy: 0.172 - ETA: 45s - loss: 3.1353 - accuracy: 0.173 - ETA: 44s - loss: 3.1345 - accuracy: 0.173 - ETA: 43s - loss: 3.1356 - accuracy: 0.172 - ETA: 43s - loss: 3.1358 - accuracy: 0.172 - ETA: 42s - loss: 3.1357 - accuracy: 0.172 - ETA: 42s - loss: 3.1355 - accuracy: 0.173 - ETA: 41s - loss: 3.1349 - accuracy: 0.173 - ETA: 41s - loss: 3.1347 - accuracy: 0.173 - ETA: 40s - loss: 3.1354 - accuracy: 0.172 - ETA: 39s - loss: 3.1352 - accuracy: 0.172 - ETA: 39s - loss: 3.1348 - accuracy: 0.173 - ETA: 38s - loss: 3.1350 - accuracy: 0.172 - ETA: 38s - loss: 3.1350 - accuracy: 0.172 - ETA: 37s - loss: 3.1347 - accuracy: 0.173 - ETA: 36s - loss: 3.1344 - accuracy: 0.172 - ETA: 36s - loss: 3.1339 - accuracy: 0.172 - ETA: 35s - loss: 3.1342 - accuracy: 0.172 - ETA: 35s - loss: 3.1340 - accuracy: 0.172 - ETA: 34s - loss: 3.1343 - accuracy: 0.172 - ETA: 34s - loss: 3.1331 - accuracy: 0.172 - ETA: 33s - loss: 3.1326 - accuracy: 0.172 - ETA: 32s - loss: 3.1325 - accuracy: 0.172 - ETA: 32s - loss: 3.1323 - accuracy: 0.172 - ETA: 31s - loss: 3.1327 - accuracy: 0.172 - ETA: 31s - loss: 3.1326 - accuracy: 0.172 - ETA: 30s - loss: 3.1318 - accuracy: 0.172 - ETA: 29s - loss: 3.1327 - accuracy: 0.172 - ETA: 29s - loss: 3.1320 - accuracy: 0.172 - ETA: 28s - loss: 3.1317 - accuracy: 0.172 - ETA: 28s - loss: 3.1319 - accuracy: 0.172 - ETA: 27s - loss: 3.1323 - accuracy: 0.172 - ETA: 27s - loss: 3.1327 - accuracy: 0.172 - ETA: 26s - loss: 3.1321 - accuracy: 0.172 - ETA: 25s - loss: 3.1323 - accuracy: 0.172 - ETA: 25s - loss: 3.1311 - accuracy: 0.172 - ETA: 24s - loss: 3.1320 - accuracy: 0.172 - ETA: 24s - loss: 3.1319 - accuracy: 0.172 - ETA: 23s - loss: 3.1316 - accuracy: 0.172 - ETA: 22s - loss: 3.1310 - accuracy: 0.172 - ETA: 22s - loss: 3.1317 - accuracy: 0.172 - ETA: 21s - loss: 3.1318 - accuracy: 0.172 - ETA: 21s - loss: 3.1321 - accuracy: 0.172 - ETA: 20s - loss: 3.1317 - accuracy: 0.172 - ETA: 19s - loss: 3.1318 - accuracy: 0.172 - ETA: 19s - loss: 3.1316 - accuracy: 0.172 - ETA: 18s - loss: 3.1315 - accuracy: 0.172 - ETA: 18s - loss: 3.1310 - accuracy: 0.172 - ETA: 17s - loss: 3.1312 - accuracy: 0.172 - ETA: 17s - loss: 3.1313 - accuracy: 0.172 - ETA: 16s - loss: 3.1314 - accuracy: 0.172 - ETA: 15s - loss: 3.1311 - accuracy: 0.172 - ETA: 15s - loss: 3.1306 - accuracy: 0.172 - ETA: 14s - loss: 3.1312 - accuracy: 0.172 - ETA: 14s - loss: 3.1314 - accuracy: 0.172 - ETA: 13s - loss: 3.1302 - accuracy: 0.172 - ETA: 12s - loss: 3.1297 - accuracy: 0.172 - ETA: 12s - loss: 3.1294 - accuracy: 0.172 - ETA: 11s - loss: 3.1290 - accuracy: 0.172 - ETA: 11s - loss: 3.1290 - accuracy: 0.172 - ETA: 10s - loss: 3.1284 - accuracy: 0.172 - ETA: 10s - loss: 3.1279 - accuracy: 0.172 - ETA: 9s - loss: 3.1277 - accuracy: 0.172 - ETA: 8s - loss: 3.1281 - accuracy: 0.17 - ETA: 8s - loss: 3.1278 - accuracy: 0.17 - ETA: 7s - loss: 3.1274 - accuracy: 0.17 - ETA: 7s - loss: 3.1274 - accuracy: 0.17 - ETA: 6s - loss: 3.1271 - accuracy: 0.17 - ETA: 5s - loss: 3.1277 - accuracy: 0.17 - ETA: 5s - loss: 3.1274 - accuracy: 0.17 - ETA: 4s - loss: 3.1274 - accuracy: 0.17 - ETA: 4s - loss: 3.1276 - accuracy: 0.17 - ETA: 3s - loss: 3.1275 - accuracy: 0.17 - ETA: 2s - loss: 3.1273 - accuracy: 0.17 - ETA: 2s - loss: 3.1272 - accuracy: 0.17 - ETA: 1s - loss: 3.1272 - accuracy: 0.17 - ETA: 1s - loss: 3.1266 - accuracy: 0.17 - ETA: 0s - loss: 3.1265 - accuracy: 0.17 - ETA: 0s - loss: 3.1271 - accuracy: 0.17 - 206s 5ms/step - loss: 3.1271 - accuracy: 0.1730 - val_loss: 4.1671 - val_accuracy: 0.0398\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:25 - loss: 3.1453 - accuracy: 0.17 - ETA: 3:13 - loss: 3.1158 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0724 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0526 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0469 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0625 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0861 - accuracy: 0.17 - ETA: 3:05 - loss: 3.0540 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0985 - accuracy: 0.17 - ETA: 3:04 - loss: 3.0985 - accuracy: 0.17 - ETA: 3:03 - loss: 3.0968 - accuracy: 0.17 - ETA: 3:03 - loss: 3.0880 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0808 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0655 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0636 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0669 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0683 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0830 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0893 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0862 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0930 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0889 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0816 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0841 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0921 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0968 - accuracy: 0.18 - ETA: 2:57 - loss: 3.1040 - accuracy: 0.18 - ETA: 2:56 - loss: 3.1030 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1001 - accuracy: 0.18 - ETA: 2:54 - loss: 3.1038 - accuracy: 0.18 - ETA: 2:54 - loss: 3.1031 - accuracy: 0.18 - ETA: 2:54 - loss: 3.1057 - accuracy: 0.17 - ETA: 2:54 - loss: 3.0964 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0975 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0965 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0996 - accuracy: 0.18 - ETA: 2:51 - loss: 3.1006 - accuracy: 0.18 - ETA: 2:51 - loss: 3.1072 - accuracy: 0.18 - ETA: 2:50 - loss: 3.1087 - accuracy: 0.18 - ETA: 2:50 - loss: 3.1084 - accuracy: 0.18 - ETA: 2:49 - loss: 3.1083 - accuracy: 0.18 - ETA: 2:48 - loss: 3.1047 - accuracy: 0.18 - ETA: 2:48 - loss: 3.1074 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1043 - accuracy: 0.18 - ETA: 2:47 - loss: 3.1043 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1018 - accuracy: 0.18 - ETA: 2:46 - loss: 3.1000 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0998 - accuracy: 0.18 - ETA: 2:45 - loss: 3.1014 - accuracy: 0.18 - ETA: 2:44 - loss: 3.1033 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0994 - accuracy: 0.18 - ETA: 2:43 - loss: 3.1012 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0977 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0974 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0967 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0992 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0998 - accuracy: 0.17 - ETA: 2:39 - loss: 3.0987 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0948 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0953 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0971 - accuracy: 0.18 - ETA: 2:37 - loss: 3.1057 - accuracy: 0.18 - ETA: 2:36 - loss: 3.1053 - accuracy: 0.18 - ETA: 2:36 - loss: 3.1027 - accuracy: 0.18 - ETA: 2:35 - loss: 3.1043 - accuracy: 0.18 - ETA: 2:35 - loss: 3.1036 - accuracy: 0.18 - ETA: 2:34 - loss: 3.1043 - accuracy: 0.18 - ETA: 2:33 - loss: 3.1035 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1012 - accuracy: 0.18 - ETA: 2:32 - loss: 3.1011 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0978 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0965 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0943 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0914 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0890 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0897 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0907 - accuracy: 0.18 - ETA: 2:28 - loss: 3.1076 - accuracy: 0.18 - ETA: 2:27 - loss: 3.1076 - accuracy: 0.18 - ETA: 2:26 - loss: 3.1070 - accuracy: 0.18 - ETA: 2:26 - loss: 3.1047 - accuracy: 0.18 - ETA: 2:26 - loss: 3.1051 - accuracy: 0.18 - ETA: 2:25 - loss: 3.1055 - accuracy: 0.18 - ETA: 2:24 - loss: 3.1046 - accuracy: 0.18 - ETA: 2:24 - loss: 3.1056 - accuracy: 0.18 - ETA: 2:23 - loss: 3.1069 - accuracy: 0.18 - ETA: 2:22 - loss: 3.1082 - accuracy: 0.18 - ETA: 2:22 - loss: 3.1069 - accuracy: 0.18 - ETA: 2:21 - loss: 3.1057 - accuracy: 0.18 - ETA: 2:21 - loss: 3.1068 - accuracy: 0.18 - ETA: 2:20 - loss: 3.1082 - accuracy: 0.18 - ETA: 2:19 - loss: 3.1078 - accuracy: 0.18 - ETA: 2:19 - loss: 3.1084 - accuracy: 0.18 - ETA: 2:18 - loss: 3.1082 - accuracy: 0.18 - ETA: 2:18 - loss: 3.1073 - accuracy: 0.18 - ETA: 2:17 - loss: 3.1065 - accuracy: 0.18 - ETA: 2:17 - loss: 3.1087 - accuracy: 0.18 - ETA: 2:16 - loss: 3.1085 - accuracy: 0.18 - ETA: 2:16 - loss: 3.1085 - accuracy: 0.18 - ETA: 2:15 - loss: 3.1092 - accuracy: 0.18 - ETA: 2:15 - loss: 3.1100 - accuracy: 0.18 - ETA: 2:14 - loss: 3.1121 - accuracy: 0.18 - ETA: 2:14 - loss: 3.1123 - accuracy: 0.18 - ETA: 2:13 - loss: 3.1115 - accuracy: 0.18 - ETA: 2:13 - loss: 3.1117 - accuracy: 0.18 - ETA: 2:12 - loss: 3.1119 - accuracy: 0.18 - ETA: 2:12 - loss: 3.1115 - accuracy: 0.18 - ETA: 2:11 - loss: 3.1121 - accuracy: 0.18 - ETA: 2:11 - loss: 3.1113 - accuracy: 0.18 - ETA: 2:10 - loss: 3.1108 - accuracy: 0.18 - ETA: 2:09 - loss: 3.1112 - accuracy: 0.18 - ETA: 2:09 - loss: 3.1108 - accuracy: 0.18 - ETA: 2:08 - loss: 3.1102 - accuracy: 0.18 - ETA: 2:08 - loss: 3.1116 - accuracy: 0.18 - ETA: 2:07 - loss: 3.1120 - accuracy: 0.18 - ETA: 2:07 - loss: 3.1132 - accuracy: 0.18 - ETA: 2:06 - loss: 3.1123 - accuracy: 0.18 - ETA: 2:05 - loss: 3.1123 - accuracy: 0.18 - ETA: 2:05 - loss: 3.1131 - accuracy: 0.18 - ETA: 2:04 - loss: 3.1133 - accuracy: 0.18 - ETA: 2:03 - loss: 3.1131 - accuracy: 0.18 - ETA: 2:03 - loss: 3.1125 - accuracy: 0.18 - ETA: 2:02 - loss: 3.1106 - accuracy: 0.18 - ETA: 2:02 - loss: 3.1111 - accuracy: 0.18 - ETA: 2:01 - loss: 3.1119 - accuracy: 0.18 - ETA: 2:01 - loss: 3.1121 - accuracy: 0.18 - ETA: 2:00 - loss: 3.1113 - accuracy: 0.18 - ETA: 1:59 - loss: 3.1095 - accuracy: 0.18 - ETA: 1:59 - loss: 3.1091 - accuracy: 0.18 - ETA: 1:58 - loss: 3.1080 - accuracy: 0.18 - ETA: 1:58 - loss: 3.1075 - accuracy: 0.18 - ETA: 1:57 - loss: 3.1086 - accuracy: 0.18 - ETA: 1:56 - loss: 3.1084 - accuracy: 0.18 - ETA: 1:56 - loss: 3.1065 - accuracy: 0.18 - ETA: 1:55 - loss: 3.1053 - accuracy: 0.18 - ETA: 1:55 - loss: 3.1041 - accuracy: 0.18 - ETA: 1:54 - loss: 3.1035 - accuracy: 0.18 - ETA: 1:53 - loss: 3.1029 - accuracy: 0.18 - ETA: 1:53 - loss: 3.1031 - accuracy: 0.18 - ETA: 1:52 - loss: 3.1044 - accuracy: 0.18 - ETA: 1:52 - loss: 3.1036 - accuracy: 0.18 - ETA: 1:51 - loss: 3.1037 - accuracy: 0.18 - ETA: 1:50 - loss: 3.1034 - accuracy: 0.18 - ETA: 1:50 - loss: 3.1045 - accuracy: 0.18 - ETA: 1:49 - loss: 3.1030 - accuracy: 0.18 - ETA: 1:49 - loss: 3.1030 - accuracy: 0.18 - ETA: 1:48 - loss: 3.1014 - accuracy: 0.18 - ETA: 1:48 - loss: 3.1009 - accuracy: 0.18 - ETA: 1:47 - loss: 3.1002 - accuracy: 0.18 - ETA: 1:46 - loss: 3.1000 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0997 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0990 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0995 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0976 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0973 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0958 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0950 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0955 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0968 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0966 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0957 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0949 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0948 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0923 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0916 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0908 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0902 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0910 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0912 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0904 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0891 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0881 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0882 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0895 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0904 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0910 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0919 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0913 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0918 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0905 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0899 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0893 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0894 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0892 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0879 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0871 - accuracy: 0.1835"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.0866 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0871 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0872 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0872 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0879 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0879 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0878 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0870 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0866 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0867 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0872 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0873 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0872 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0871 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0883 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0886 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0891 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0901 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0899 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0904 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0901 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0894 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0901 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0893 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0900 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0903 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0903 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0907 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0898 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0899 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0909 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0906 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0905 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0903 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0895 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0897 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0904 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0903 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0907 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0907 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0906 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0908 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0901 - accuracy: 0.18 - ETA: 59s - loss: 3.0893 - accuracy: 0.1811 - ETA: 58s - loss: 3.0893 - accuracy: 0.181 - ETA: 58s - loss: 3.0886 - accuracy: 0.181 - ETA: 57s - loss: 3.0889 - accuracy: 0.181 - ETA: 57s - loss: 3.0891 - accuracy: 0.181 - ETA: 56s - loss: 3.0898 - accuracy: 0.181 - ETA: 55s - loss: 3.0895 - accuracy: 0.181 - ETA: 55s - loss: 3.0891 - accuracy: 0.181 - ETA: 54s - loss: 3.0888 - accuracy: 0.181 - ETA: 54s - loss: 3.0879 - accuracy: 0.181 - ETA: 53s - loss: 3.0875 - accuracy: 0.181 - ETA: 52s - loss: 3.0878 - accuracy: 0.181 - ETA: 52s - loss: 3.0874 - accuracy: 0.181 - ETA: 51s - loss: 3.0874 - accuracy: 0.181 - ETA: 51s - loss: 3.0873 - accuracy: 0.181 - ETA: 50s - loss: 3.0873 - accuracy: 0.182 - ETA: 49s - loss: 3.0875 - accuracy: 0.182 - ETA: 49s - loss: 3.0878 - accuracy: 0.182 - ETA: 48s - loss: 3.0883 - accuracy: 0.181 - ETA: 48s - loss: 3.0878 - accuracy: 0.181 - ETA: 47s - loss: 3.0873 - accuracy: 0.182 - ETA: 47s - loss: 3.0873 - accuracy: 0.182 - ETA: 46s - loss: 3.0876 - accuracy: 0.182 - ETA: 45s - loss: 3.0884 - accuracy: 0.182 - ETA: 45s - loss: 3.0882 - accuracy: 0.181 - ETA: 44s - loss: 3.0884 - accuracy: 0.182 - ETA: 44s - loss: 3.0886 - accuracy: 0.181 - ETA: 43s - loss: 3.0885 - accuracy: 0.181 - ETA: 42s - loss: 3.0891 - accuracy: 0.181 - ETA: 42s - loss: 3.0897 - accuracy: 0.181 - ETA: 41s - loss: 3.0892 - accuracy: 0.181 - ETA: 41s - loss: 3.0885 - accuracy: 0.181 - ETA: 40s - loss: 3.0886 - accuracy: 0.181 - ETA: 40s - loss: 3.0883 - accuracy: 0.181 - ETA: 39s - loss: 3.0878 - accuracy: 0.181 - ETA: 38s - loss: 3.0871 - accuracy: 0.182 - ETA: 38s - loss: 3.0866 - accuracy: 0.182 - ETA: 37s - loss: 3.0861 - accuracy: 0.182 - ETA: 37s - loss: 3.0859 - accuracy: 0.182 - ETA: 36s - loss: 3.0860 - accuracy: 0.182 - ETA: 35s - loss: 3.0865 - accuracy: 0.182 - ETA: 35s - loss: 3.0858 - accuracy: 0.182 - ETA: 34s - loss: 3.0856 - accuracy: 0.182 - ETA: 34s - loss: 3.0862 - accuracy: 0.182 - ETA: 33s - loss: 3.0862 - accuracy: 0.182 - ETA: 32s - loss: 3.0854 - accuracy: 0.182 - ETA: 32s - loss: 3.0852 - accuracy: 0.182 - ETA: 31s - loss: 3.0850 - accuracy: 0.182 - ETA: 31s - loss: 3.0849 - accuracy: 0.182 - ETA: 30s - loss: 3.0850 - accuracy: 0.182 - ETA: 30s - loss: 3.0850 - accuracy: 0.182 - ETA: 29s - loss: 3.0847 - accuracy: 0.182 - ETA: 28s - loss: 3.0844 - accuracy: 0.182 - ETA: 28s - loss: 3.0842 - accuracy: 0.182 - ETA: 27s - loss: 3.0839 - accuracy: 0.182 - ETA: 27s - loss: 3.0831 - accuracy: 0.182 - ETA: 26s - loss: 3.0835 - accuracy: 0.182 - ETA: 25s - loss: 3.0835 - accuracy: 0.182 - ETA: 25s - loss: 3.0832 - accuracy: 0.182 - ETA: 24s - loss: 3.0832 - accuracy: 0.182 - ETA: 24s - loss: 3.0829 - accuracy: 0.182 - ETA: 23s - loss: 3.0829 - accuracy: 0.182 - ETA: 22s - loss: 3.0826 - accuracy: 0.182 - ETA: 22s - loss: 3.0822 - accuracy: 0.182 - ETA: 21s - loss: 3.0830 - accuracy: 0.182 - ETA: 21s - loss: 3.0835 - accuracy: 0.182 - ETA: 20s - loss: 3.0836 - accuracy: 0.182 - ETA: 20s - loss: 3.0844 - accuracy: 0.182 - ETA: 19s - loss: 3.0843 - accuracy: 0.182 - ETA: 18s - loss: 3.0841 - accuracy: 0.182 - ETA: 18s - loss: 3.0846 - accuracy: 0.182 - ETA: 17s - loss: 3.0850 - accuracy: 0.182 - ETA: 17s - loss: 3.0850 - accuracy: 0.182 - ETA: 16s - loss: 3.0847 - accuracy: 0.182 - ETA: 15s - loss: 3.0840 - accuracy: 0.182 - ETA: 15s - loss: 3.0839 - accuracy: 0.182 - ETA: 14s - loss: 3.0838 - accuracy: 0.182 - ETA: 14s - loss: 3.0839 - accuracy: 0.182 - ETA: 13s - loss: 3.0839 - accuracy: 0.182 - ETA: 12s - loss: 3.0839 - accuracy: 0.182 - ETA: 12s - loss: 3.0840 - accuracy: 0.182 - ETA: 11s - loss: 3.0845 - accuracy: 0.182 - ETA: 11s - loss: 3.0849 - accuracy: 0.182 - ETA: 10s - loss: 3.0851 - accuracy: 0.181 - ETA: 10s - loss: 3.0856 - accuracy: 0.182 - ETA: 9s - loss: 3.0860 - accuracy: 0.182 - ETA: 8s - loss: 3.0861 - accuracy: 0.18 - ETA: 8s - loss: 3.0863 - accuracy: 0.18 - ETA: 7s - loss: 3.0869 - accuracy: 0.18 - ETA: 7s - loss: 3.0860 - accuracy: 0.18 - ETA: 6s - loss: 3.0860 - accuracy: 0.18 - ETA: 5s - loss: 3.0858 - accuracy: 0.18 - ETA: 5s - loss: 3.0864 - accuracy: 0.18 - ETA: 4s - loss: 3.0863 - accuracy: 0.18 - ETA: 4s - loss: 3.0863 - accuracy: 0.18 - ETA: 3s - loss: 3.0862 - accuracy: 0.18 - ETA: 2s - loss: 3.0869 - accuracy: 0.18 - ETA: 2s - loss: 3.0874 - accuracy: 0.18 - ETA: 1s - loss: 3.0875 - accuracy: 0.18 - ETA: 1s - loss: 3.0884 - accuracy: 0.18 - ETA: 0s - loss: 3.0886 - accuracy: 0.18 - ETA: 0s - loss: 3.0896 - accuracy: 0.18 - 206s 5ms/step - loss: 3.0896 - accuracy: 0.1816 - val_loss: 4.0903 - val_accuracy: 0.0343\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:05 - loss: 2.9564 - accuracy: 0.21 - ETA: 3:02 - loss: 2.9628 - accuracy: 0.23 - ETA: 3:08 - loss: 3.0038 - accuracy: 0.22 - ETA: 3:07 - loss: 3.0413 - accuracy: 0.20 - ETA: 3:07 - loss: 3.0955 - accuracy: 0.19 - ETA: 3:06 - loss: 3.1392 - accuracy: 0.18 - ETA: 3:06 - loss: 3.1630 - accuracy: 0.17 - ETA: 3:05 - loss: 3.1745 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1658 - accuracy: 0.18 - ETA: 3:04 - loss: 3.1459 - accuracy: 0.18 - ETA: 3:03 - loss: 3.1524 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1486 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1598 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1587 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1545 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1392 - accuracy: 0.18 - ETA: 3:02 - loss: 3.1282 - accuracy: 0.18 - ETA: 3:02 - loss: 3.1281 - accuracy: 0.18 - ETA: 3:01 - loss: 3.1253 - accuracy: 0.18 - ETA: 3:01 - loss: 3.1194 - accuracy: 0.18 - ETA: 3:02 - loss: 3.1178 - accuracy: 0.18 - ETA: 3:02 - loss: 3.1189 - accuracy: 0.18 - ETA: 3:01 - loss: 3.1220 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1176 - accuracy: 0.18 - ETA: 3:00 - loss: 3.1206 - accuracy: 0.18 - ETA: 2:59 - loss: 3.1169 - accuracy: 0.18 - ETA: 2:58 - loss: 3.1201 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1198 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1151 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1130 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1135 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1231 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1153 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1089 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1051 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1047 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1063 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1053 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1091 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1025 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1052 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1108 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1082 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1110 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1113 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1112 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1098 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1096 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1103 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1067 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1076 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1039 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1047 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1038 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1017 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1026 - accuracy: 0.17 - ETA: 2:40 - loss: 3.0978 - accuracy: 0.17 - ETA: 2:40 - loss: 3.0978 - accuracy: 0.17 - ETA: 2:39 - loss: 3.0980 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1009 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1004 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1006 - accuracy: 0.17 - ETA: 2:36 - loss: 3.0980 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1010 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1013 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1025 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1043 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1028 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1042 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1047 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1034 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1031 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1011 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1036 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1025 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1005 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1008 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1014 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1016 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1008 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1002 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1009 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1021 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1020 - accuracy: 0.17 - ETA: 2:24 - loss: 3.0991 - accuracy: 0.17 - ETA: 2:23 - loss: 3.0997 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1024 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1047 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1068 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1062 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1057 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1076 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1070 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1065 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1048 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1064 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1063 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1064 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1072 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1031 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1041 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1049 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1059 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1045 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1055 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1035 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1033 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1016 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1028 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1021 - accuracy: 0.17 - ETA: 2:09 - loss: 3.0992 - accuracy: 0.17 - ETA: 2:08 - loss: 3.0983 - accuracy: 0.17 - ETA: 2:08 - loss: 3.0968 - accuracy: 0.17 - ETA: 2:07 - loss: 3.0962 - accuracy: 0.17 - ETA: 2:06 - loss: 3.0953 - accuracy: 0.17 - ETA: 2:06 - loss: 3.0936 - accuracy: 0.17 - ETA: 2:05 - loss: 3.0930 - accuracy: 0.17 - ETA: 2:04 - loss: 3.0935 - accuracy: 0.17 - ETA: 2:04 - loss: 3.0915 - accuracy: 0.17 - ETA: 2:03 - loss: 3.0901 - accuracy: 0.17 - ETA: 2:03 - loss: 3.0908 - accuracy: 0.17 - ETA: 2:02 - loss: 3.0913 - accuracy: 0.17 - ETA: 2:02 - loss: 3.0893 - accuracy: 0.17 - ETA: 2:01 - loss: 3.0896 - accuracy: 0.17 - ETA: 2:00 - loss: 3.0886 - accuracy: 0.17 - ETA: 2:00 - loss: 3.0888 - accuracy: 0.17 - ETA: 1:59 - loss: 3.0882 - accuracy: 0.17 - ETA: 1:59 - loss: 3.0890 - accuracy: 0.17 - ETA: 1:58 - loss: 3.0882 - accuracy: 0.17 - ETA: 1:57 - loss: 3.0880 - accuracy: 0.17 - ETA: 1:57 - loss: 3.0880 - accuracy: 0.17 - ETA: 1:57 - loss: 3.0893 - accuracy: 0.17 - ETA: 1:56 - loss: 3.0900 - accuracy: 0.17 - ETA: 1:55 - loss: 3.0894 - accuracy: 0.17 - ETA: 1:55 - loss: 3.0891 - accuracy: 0.17 - ETA: 1:54 - loss: 3.0928 - accuracy: 0.17 - ETA: 1:54 - loss: 3.0922 - accuracy: 0.17 - ETA: 1:53 - loss: 3.0936 - accuracy: 0.17 - ETA: 1:53 - loss: 3.0931 - accuracy: 0.17 - ETA: 1:52 - loss: 3.0930 - accuracy: 0.17 - ETA: 1:51 - loss: 3.0925 - accuracy: 0.17 - ETA: 1:51 - loss: 3.0942 - accuracy: 0.17 - ETA: 1:50 - loss: 3.0951 - accuracy: 0.17 - ETA: 1:49 - loss: 3.0951 - accuracy: 0.17 - ETA: 1:49 - loss: 3.0951 - accuracy: 0.17 - ETA: 1:48 - loss: 3.0957 - accuracy: 0.17 - ETA: 1:48 - loss: 3.0972 - accuracy: 0.17 - ETA: 1:47 - loss: 3.0965 - accuracy: 0.17 - ETA: 1:46 - loss: 3.0969 - accuracy: 0.17 - ETA: 1:46 - loss: 3.0973 - accuracy: 0.17 - ETA: 1:45 - loss: 3.0980 - accuracy: 0.17 - ETA: 1:45 - loss: 3.0985 - accuracy: 0.17 - ETA: 1:44 - loss: 3.0983 - accuracy: 0.17 - ETA: 1:43 - loss: 3.0982 - accuracy: 0.17 - ETA: 1:43 - loss: 3.0983 - accuracy: 0.17 - ETA: 1:42 - loss: 3.0984 - accuracy: 0.17 - ETA: 1:42 - loss: 3.0979 - accuracy: 0.17 - ETA: 1:41 - loss: 3.0986 - accuracy: 0.17 - ETA: 1:41 - loss: 3.0994 - accuracy: 0.17 - ETA: 1:40 - loss: 3.0994 - accuracy: 0.17 - ETA: 1:39 - loss: 3.0991 - accuracy: 0.17 - ETA: 1:39 - loss: 3.0988 - accuracy: 0.17 - ETA: 1:38 - loss: 3.0974 - accuracy: 0.17 - ETA: 1:38 - loss: 3.0954 - accuracy: 0.17 - ETA: 1:37 - loss: 3.0966 - accuracy: 0.17 - ETA: 1:37 - loss: 3.0976 - accuracy: 0.17 - ETA: 1:36 - loss: 3.0983 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1002 - accuracy: 0.17 - ETA: 1:35 - loss: 3.0996 - accuracy: 0.17 - ETA: 1:34 - loss: 3.0984 - accuracy: 0.17 - ETA: 1:33 - loss: 3.0988 - accuracy: 0.17 - ETA: 1:33 - loss: 3.0992 - accuracy: 0.17 - ETA: 1:32 - loss: 3.0994 - accuracy: 0.17 - ETA: 1:32 - loss: 3.0996 - accuracy: 0.17 - ETA: 1:31 - loss: 3.0990 - accuracy: 0.17 - ETA: 1:31 - loss: 3.0987 - accuracy: 0.17 - ETA: 1:30 - loss: 3.0977 - accuracy: 0.17 - ETA: 1:29 - loss: 3.0980 - accuracy: 0.17 - ETA: 1:29 - loss: 3.0972 - accuracy: 0.17 - ETA: 1:28 - loss: 3.0977 - accuracy: 0.17 - ETA: 1:28 - loss: 3.0976 - accuracy: 0.17 - ETA: 1:27 - loss: 3.0980 - accuracy: 0.17 - ETA: 1:27 - loss: 3.0978 - accuracy: 0.17 - ETA: 1:26 - loss: 3.0976 - accuracy: 0.17 - ETA: 1:25 - loss: 3.0977 - accuracy: 0.17 - ETA: 1:25 - loss: 3.0974 - accuracy: 0.1789"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.0971 - accuracy: 0.17 - ETA: 1:24 - loss: 3.0966 - accuracy: 0.17 - ETA: 1:23 - loss: 3.0958 - accuracy: 0.17 - ETA: 1:22 - loss: 3.0952 - accuracy: 0.17 - ETA: 1:22 - loss: 3.0961 - accuracy: 0.17 - ETA: 1:21 - loss: 3.0970 - accuracy: 0.17 - ETA: 1:21 - loss: 3.0975 - accuracy: 0.17 - ETA: 1:20 - loss: 3.0974 - accuracy: 0.17 - ETA: 1:20 - loss: 3.0953 - accuracy: 0.17 - ETA: 1:19 - loss: 3.0955 - accuracy: 0.17 - ETA: 1:18 - loss: 3.0956 - accuracy: 0.17 - ETA: 1:18 - loss: 3.0951 - accuracy: 0.17 - ETA: 1:17 - loss: 3.0954 - accuracy: 0.17 - ETA: 1:17 - loss: 3.0959 - accuracy: 0.17 - ETA: 1:16 - loss: 3.0966 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0961 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0972 - accuracy: 0.17 - ETA: 1:14 - loss: 3.0966 - accuracy: 0.17 - ETA: 1:14 - loss: 3.0966 - accuracy: 0.17 - ETA: 1:13 - loss: 3.0966 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0972 - accuracy: 0.17 - ETA: 1:12 - loss: 3.0983 - accuracy: 0.17 - ETA: 1:11 - loss: 3.0988 - accuracy: 0.17 - ETA: 1:11 - loss: 3.0984 - accuracy: 0.17 - ETA: 1:10 - loss: 3.0986 - accuracy: 0.17 - ETA: 1:10 - loss: 3.0986 - accuracy: 0.17 - ETA: 1:09 - loss: 3.0993 - accuracy: 0.17 - ETA: 1:08 - loss: 3.0983 - accuracy: 0.17 - ETA: 1:08 - loss: 3.0978 - accuracy: 0.17 - ETA: 1:07 - loss: 3.0972 - accuracy: 0.17 - ETA: 1:07 - loss: 3.0968 - accuracy: 0.17 - ETA: 1:06 - loss: 3.0966 - accuracy: 0.17 - ETA: 1:05 - loss: 3.0962 - accuracy: 0.17 - ETA: 1:05 - loss: 3.0959 - accuracy: 0.17 - ETA: 1:04 - loss: 3.0963 - accuracy: 0.17 - ETA: 1:04 - loss: 3.0964 - accuracy: 0.17 - ETA: 1:03 - loss: 3.0959 - accuracy: 0.17 - ETA: 1:02 - loss: 3.0954 - accuracy: 0.17 - ETA: 1:02 - loss: 3.0952 - accuracy: 0.17 - ETA: 1:01 - loss: 3.0944 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0954 - accuracy: 0.17 - ETA: 1:00 - loss: 3.0946 - accuracy: 0.18 - ETA: 59s - loss: 3.0945 - accuracy: 0.1801 - ETA: 59s - loss: 3.0947 - accuracy: 0.180 - ETA: 58s - loss: 3.0948 - accuracy: 0.180 - ETA: 58s - loss: 3.0945 - accuracy: 0.180 - ETA: 57s - loss: 3.0946 - accuracy: 0.180 - ETA: 57s - loss: 3.0951 - accuracy: 0.179 - ETA: 56s - loss: 3.0948 - accuracy: 0.180 - ETA: 55s - loss: 3.0953 - accuracy: 0.179 - ETA: 55s - loss: 3.0944 - accuracy: 0.180 - ETA: 54s - loss: 3.0944 - accuracy: 0.180 - ETA: 54s - loss: 3.0945 - accuracy: 0.180 - ETA: 53s - loss: 3.0944 - accuracy: 0.180 - ETA: 52s - loss: 3.0938 - accuracy: 0.180 - ETA: 52s - loss: 3.0934 - accuracy: 0.180 - ETA: 51s - loss: 3.0932 - accuracy: 0.180 - ETA: 51s - loss: 3.0938 - accuracy: 0.180 - ETA: 50s - loss: 3.0929 - accuracy: 0.180 - ETA: 50s - loss: 3.0942 - accuracy: 0.180 - ETA: 49s - loss: 3.0936 - accuracy: 0.179 - ETA: 48s - loss: 3.0934 - accuracy: 0.179 - ETA: 48s - loss: 3.0930 - accuracy: 0.179 - ETA: 47s - loss: 3.0927 - accuracy: 0.179 - ETA: 47s - loss: 3.0927 - accuracy: 0.179 - ETA: 46s - loss: 3.0917 - accuracy: 0.180 - ETA: 45s - loss: 3.0913 - accuracy: 0.180 - ETA: 45s - loss: 3.0905 - accuracy: 0.180 - ETA: 44s - loss: 3.0904 - accuracy: 0.180 - ETA: 44s - loss: 3.0905 - accuracy: 0.180 - ETA: 43s - loss: 3.0899 - accuracy: 0.180 - ETA: 42s - loss: 3.0897 - accuracy: 0.180 - ETA: 42s - loss: 3.0899 - accuracy: 0.180 - ETA: 41s - loss: 3.0902 - accuracy: 0.180 - ETA: 41s - loss: 3.0903 - accuracy: 0.180 - ETA: 40s - loss: 3.0899 - accuracy: 0.180 - ETA: 40s - loss: 3.0895 - accuracy: 0.180 - ETA: 39s - loss: 3.0904 - accuracy: 0.180 - ETA: 38s - loss: 3.0902 - accuracy: 0.180 - ETA: 38s - loss: 3.0907 - accuracy: 0.180 - ETA: 37s - loss: 3.0911 - accuracy: 0.180 - ETA: 37s - loss: 3.0913 - accuracy: 0.180 - ETA: 36s - loss: 3.0915 - accuracy: 0.180 - ETA: 35s - loss: 3.0914 - accuracy: 0.180 - ETA: 35s - loss: 3.0906 - accuracy: 0.180 - ETA: 34s - loss: 3.0908 - accuracy: 0.180 - ETA: 34s - loss: 3.0903 - accuracy: 0.180 - ETA: 33s - loss: 3.0900 - accuracy: 0.180 - ETA: 32s - loss: 3.0902 - accuracy: 0.180 - ETA: 32s - loss: 3.0901 - accuracy: 0.180 - ETA: 31s - loss: 3.0894 - accuracy: 0.180 - ETA: 31s - loss: 3.0895 - accuracy: 0.180 - ETA: 30s - loss: 3.0893 - accuracy: 0.180 - ETA: 30s - loss: 3.0892 - accuracy: 0.180 - ETA: 29s - loss: 3.0885 - accuracy: 0.180 - ETA: 28s - loss: 3.0892 - accuracy: 0.180 - ETA: 28s - loss: 3.0887 - accuracy: 0.180 - ETA: 27s - loss: 3.0891 - accuracy: 0.180 - ETA: 27s - loss: 3.0888 - accuracy: 0.179 - ETA: 26s - loss: 3.0892 - accuracy: 0.179 - ETA: 25s - loss: 3.0886 - accuracy: 0.180 - ETA: 25s - loss: 3.0885 - accuracy: 0.180 - ETA: 24s - loss: 3.0884 - accuracy: 0.180 - ETA: 24s - loss: 3.0883 - accuracy: 0.180 - ETA: 23s - loss: 3.0886 - accuracy: 0.180 - ETA: 22s - loss: 3.0889 - accuracy: 0.179 - ETA: 22s - loss: 3.0886 - accuracy: 0.179 - ETA: 21s - loss: 3.0884 - accuracy: 0.179 - ETA: 21s - loss: 3.0885 - accuracy: 0.179 - ETA: 20s - loss: 3.0887 - accuracy: 0.179 - ETA: 20s - loss: 3.0881 - accuracy: 0.179 - ETA: 19s - loss: 3.0879 - accuracy: 0.179 - ETA: 18s - loss: 3.0882 - accuracy: 0.179 - ETA: 18s - loss: 3.0880 - accuracy: 0.179 - ETA: 17s - loss: 3.0874 - accuracy: 0.179 - ETA: 17s - loss: 3.0874 - accuracy: 0.179 - ETA: 16s - loss: 3.0873 - accuracy: 0.179 - ETA: 15s - loss: 3.0872 - accuracy: 0.179 - ETA: 15s - loss: 3.0915 - accuracy: 0.179 - ETA: 14s - loss: 3.0916 - accuracy: 0.179 - ETA: 14s - loss: 3.0949 - accuracy: 0.179 - ETA: 13s - loss: 3.0945 - accuracy: 0.180 - ETA: 12s - loss: 3.0945 - accuracy: 0.180 - ETA: 12s - loss: 3.0944 - accuracy: 0.180 - ETA: 11s - loss: 3.0945 - accuracy: 0.180 - ETA: 11s - loss: 3.0944 - accuracy: 0.180 - ETA: 10s - loss: 3.0938 - accuracy: 0.180 - ETA: 10s - loss: 3.0931 - accuracy: 0.180 - ETA: 9s - loss: 3.0925 - accuracy: 0.180 - ETA: 8s - loss: 3.0923 - accuracy: 0.18 - ETA: 8s - loss: 3.0923 - accuracy: 0.18 - ETA: 7s - loss: 3.0926 - accuracy: 0.18 - ETA: 7s - loss: 3.0931 - accuracy: 0.18 - ETA: 6s - loss: 3.0929 - accuracy: 0.18 - ETA: 5s - loss: 3.0930 - accuracy: 0.18 - ETA: 5s - loss: 3.0932 - accuracy: 0.18 - ETA: 4s - loss: 3.0931 - accuracy: 0.18 - ETA: 4s - loss: 3.0932 - accuracy: 0.18 - ETA: 3s - loss: 3.0933 - accuracy: 0.18 - ETA: 2s - loss: 3.0932 - accuracy: 0.17 - ETA: 2s - loss: 3.0936 - accuracy: 0.18 - ETA: 1s - loss: 3.0929 - accuracy: 0.18 - ETA: 1s - loss: 3.0931 - accuracy: 0.18 - ETA: 0s - loss: 3.0924 - accuracy: 0.18 - ETA: 0s - loss: 3.0919 - accuracy: 0.18 - 207s 5ms/step - loss: 3.0921 - accuracy: 0.1803 - val_loss: 4.1291 - val_accuracy: 0.0383\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:24 - loss: 3.1398 - accuracy: 0.14 - ETA: 3:19 - loss: 3.1853 - accuracy: 0.16 - ETA: 3:17 - loss: 3.1464 - accuracy: 0.16 - ETA: 3:16 - loss: 3.1275 - accuracy: 0.16 - ETA: 3:14 - loss: 3.1258 - accuracy: 0.17 - ETA: 3:13 - loss: 3.1219 - accuracy: 0.17 - ETA: 3:11 - loss: 3.1254 - accuracy: 0.17 - ETA: 3:10 - loss: 3.1184 - accuracy: 0.17 - ETA: 3:10 - loss: 3.1064 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0938 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0872 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0776 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0853 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0827 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0866 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0791 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0818 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0782 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0727 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0824 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0817 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0808 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0842 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0860 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0803 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0799 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0756 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0765 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0828 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0824 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0856 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0874 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0845 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0828 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0804 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0753 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0733 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0740 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0769 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0748 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0777 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0771 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0737 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0719 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0677 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0664 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0672 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0642 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0659 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0645 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0642 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0573 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0562 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0559 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0607 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0584 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0602 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0618 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0612 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0600 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0579 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0564 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0575 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0582 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0584 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0598 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0601 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0617 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0591 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0593 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0578 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0607 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0600 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0614 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0601 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0592 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0580 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0570 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0549 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0554 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0557 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0542 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0538 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0528 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0539 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0544 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0571 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0582 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0581 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0602 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0593 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0589 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0561 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0550 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0556 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0566 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0568 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0566 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0572 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0558 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0550 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0546 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0541 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0543 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0533 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0547 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0561 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0562 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0570 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0562 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0552 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0557 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0568 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0556 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0553 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0552 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0560 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0561 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0565 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0570 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0580 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0583 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0574 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0578 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0561 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0558 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0577 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0578 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0595 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0595 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0609 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0611 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0621 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0622 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0628 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0617 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0621 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0627 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0611 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0624 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0622 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0627 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0611 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0610 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0626 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0622 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0627 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0623 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0616 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0618 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0617 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0611 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0621 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0620 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0619 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0623 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0633 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0631 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0636 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0628 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0638 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0632 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0633 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0638 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0642 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0632 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0636 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0648 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0642 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0643 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0639 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0628 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0620 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0627 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0632 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0630 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0628 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0632 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0645 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0635 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0629 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0625 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0612 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0617 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0619 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0629 - accuracy: 0.1854"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.0625 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0626 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0630 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0648 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0648 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0652 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0656 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0651 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0652 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0656 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0652 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0646 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0640 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0649 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0650 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0646 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0639 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0637 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0641 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0646 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0648 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0653 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0653 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0658 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0657 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0653 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0641 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0646 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0650 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0655 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0656 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0659 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0667 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0680 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0669 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0674 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0673 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0668 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0671 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0675 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0671 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0683 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0683 - accuracy: 0.18 - ETA: 59s - loss: 3.0677 - accuracy: 0.1847 - ETA: 58s - loss: 3.0679 - accuracy: 0.184 - ETA: 58s - loss: 3.0673 - accuracy: 0.184 - ETA: 57s - loss: 3.0681 - accuracy: 0.184 - ETA: 57s - loss: 3.0682 - accuracy: 0.184 - ETA: 56s - loss: 3.0684 - accuracy: 0.184 - ETA: 55s - loss: 3.0679 - accuracy: 0.184 - ETA: 55s - loss: 3.0689 - accuracy: 0.184 - ETA: 54s - loss: 3.0688 - accuracy: 0.184 - ETA: 54s - loss: 3.0685 - accuracy: 0.184 - ETA: 53s - loss: 3.0690 - accuracy: 0.184 - ETA: 52s - loss: 3.0697 - accuracy: 0.184 - ETA: 52s - loss: 3.0702 - accuracy: 0.184 - ETA: 51s - loss: 3.0692 - accuracy: 0.184 - ETA: 51s - loss: 3.0696 - accuracy: 0.184 - ETA: 50s - loss: 3.0695 - accuracy: 0.184 - ETA: 50s - loss: 3.0694 - accuracy: 0.184 - ETA: 49s - loss: 3.0694 - accuracy: 0.184 - ETA: 48s - loss: 3.0701 - accuracy: 0.184 - ETA: 48s - loss: 3.0696 - accuracy: 0.184 - ETA: 47s - loss: 3.0695 - accuracy: 0.184 - ETA: 47s - loss: 3.0693 - accuracy: 0.184 - ETA: 46s - loss: 3.0697 - accuracy: 0.184 - ETA: 45s - loss: 3.0697 - accuracy: 0.184 - ETA: 45s - loss: 3.0694 - accuracy: 0.184 - ETA: 44s - loss: 3.0691 - accuracy: 0.184 - ETA: 44s - loss: 3.0696 - accuracy: 0.184 - ETA: 43s - loss: 3.0690 - accuracy: 0.184 - ETA: 42s - loss: 3.0696 - accuracy: 0.184 - ETA: 42s - loss: 3.0703 - accuracy: 0.184 - ETA: 41s - loss: 3.0708 - accuracy: 0.184 - ETA: 41s - loss: 3.0705 - accuracy: 0.184 - ETA: 40s - loss: 3.0702 - accuracy: 0.184 - ETA: 40s - loss: 3.0691 - accuracy: 0.184 - ETA: 39s - loss: 3.0697 - accuracy: 0.184 - ETA: 38s - loss: 3.0690 - accuracy: 0.184 - ETA: 38s - loss: 3.0686 - accuracy: 0.184 - ETA: 37s - loss: 3.0689 - accuracy: 0.184 - ETA: 37s - loss: 3.0692 - accuracy: 0.184 - ETA: 36s - loss: 3.0695 - accuracy: 0.184 - ETA: 35s - loss: 3.0696 - accuracy: 0.184 - ETA: 35s - loss: 3.0701 - accuracy: 0.184 - ETA: 34s - loss: 3.0697 - accuracy: 0.184 - ETA: 34s - loss: 3.0707 - accuracy: 0.184 - ETA: 33s - loss: 3.0707 - accuracy: 0.184 - ETA: 32s - loss: 3.0705 - accuracy: 0.184 - ETA: 32s - loss: 3.0710 - accuracy: 0.184 - ETA: 31s - loss: 3.0714 - accuracy: 0.184 - ETA: 31s - loss: 3.0713 - accuracy: 0.184 - ETA: 30s - loss: 3.0714 - accuracy: 0.184 - ETA: 30s - loss: 3.0718 - accuracy: 0.184 - ETA: 29s - loss: 3.0721 - accuracy: 0.184 - ETA: 28s - loss: 3.0721 - accuracy: 0.184 - ETA: 28s - loss: 3.0715 - accuracy: 0.184 - ETA: 27s - loss: 3.0709 - accuracy: 0.184 - ETA: 27s - loss: 3.0708 - accuracy: 0.184 - ETA: 26s - loss: 3.0703 - accuracy: 0.184 - ETA: 25s - loss: 3.0706 - accuracy: 0.184 - ETA: 25s - loss: 3.0701 - accuracy: 0.184 - ETA: 24s - loss: 3.0699 - accuracy: 0.184 - ETA: 24s - loss: 3.0691 - accuracy: 0.184 - ETA: 23s - loss: 3.0693 - accuracy: 0.184 - ETA: 22s - loss: 3.0691 - accuracy: 0.184 - ETA: 22s - loss: 3.0692 - accuracy: 0.184 - ETA: 21s - loss: 3.0691 - accuracy: 0.184 - ETA: 21s - loss: 3.0680 - accuracy: 0.184 - ETA: 20s - loss: 3.0675 - accuracy: 0.184 - ETA: 20s - loss: 3.0675 - accuracy: 0.184 - ETA: 19s - loss: 3.0680 - accuracy: 0.184 - ETA: 18s - loss: 3.0673 - accuracy: 0.184 - ETA: 18s - loss: 3.0677 - accuracy: 0.184 - ETA: 17s - loss: 3.0672 - accuracy: 0.185 - ETA: 17s - loss: 3.0673 - accuracy: 0.185 - ETA: 16s - loss: 3.0664 - accuracy: 0.185 - ETA: 15s - loss: 3.0669 - accuracy: 0.185 - ETA: 15s - loss: 3.0666 - accuracy: 0.185 - ETA: 14s - loss: 3.0665 - accuracy: 0.185 - ETA: 14s - loss: 3.0665 - accuracy: 0.185 - ETA: 13s - loss: 3.0661 - accuracy: 0.185 - ETA: 12s - loss: 3.0662 - accuracy: 0.184 - ETA: 12s - loss: 3.0665 - accuracy: 0.184 - ETA: 11s - loss: 3.0661 - accuracy: 0.185 - ETA: 11s - loss: 3.0660 - accuracy: 0.185 - ETA: 10s - loss: 3.0660 - accuracy: 0.185 - ETA: 10s - loss: 3.0662 - accuracy: 0.184 - ETA: 9s - loss: 3.0660 - accuracy: 0.184 - ETA: 8s - loss: 3.0661 - accuracy: 0.18 - ETA: 8s - loss: 3.0665 - accuracy: 0.18 - ETA: 7s - loss: 3.0667 - accuracy: 0.18 - ETA: 7s - loss: 3.0665 - accuracy: 0.18 - ETA: 6s - loss: 3.0659 - accuracy: 0.18 - ETA: 5s - loss: 3.0658 - accuracy: 0.18 - ETA: 5s - loss: 3.0662 - accuracy: 0.18 - ETA: 4s - loss: 3.0659 - accuracy: 0.18 - ETA: 4s - loss: 3.0651 - accuracy: 0.18 - ETA: 3s - loss: 3.0656 - accuracy: 0.18 - ETA: 2s - loss: 3.0657 - accuracy: 0.18 - ETA: 2s - loss: 3.0660 - accuracy: 0.18 - ETA: 1s - loss: 3.0662 - accuracy: 0.18 - ETA: 1s - loss: 3.0661 - accuracy: 0.18 - ETA: 0s - loss: 3.0661 - accuracy: 0.18 - ETA: 0s - loss: 3.0659 - accuracy: 0.18 - 207s 5ms/step - loss: 3.0661 - accuracy: 0.1845 - val_loss: 4.1760 - val_accuracy: 0.0389\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:22 - loss: 3.3065 - accuracy: 0.15 - ETA: 3:14 - loss: 3.5535 - accuracy: 0.14 - ETA: 3:11 - loss: 3.4486 - accuracy: 0.14 - ETA: 3:10 - loss: 3.3715 - accuracy: 0.14 - ETA: 3:10 - loss: 3.3331 - accuracy: 0.15 - ETA: 3:09 - loss: 3.2935 - accuracy: 0.15 - ETA: 3:07 - loss: 3.2978 - accuracy: 0.15 - ETA: 3:07 - loss: 3.2909 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2879 - accuracy: 0.15 - ETA: 3:05 - loss: 3.2885 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2575 - accuracy: 0.15 - ETA: 3:03 - loss: 3.2640 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2635 - accuracy: 0.15 - ETA: 3:03 - loss: 3.2654 - accuracy: 0.15 - ETA: 3:02 - loss: 3.2646 - accuracy: 0.15 - ETA: 3:01 - loss: 3.2612 - accuracy: 0.15 - ETA: 3:01 - loss: 3.2546 - accuracy: 0.15 - ETA: 3:00 - loss: 3.2338 - accuracy: 0.15 - ETA: 3:00 - loss: 3.2266 - accuracy: 0.16 - ETA: 2:59 - loss: 3.2203 - accuracy: 0.16 - ETA: 2:58 - loss: 3.2196 - accuracy: 0.16 - ETA: 2:59 - loss: 3.2065 - accuracy: 0.16 - ETA: 2:59 - loss: 3.2038 - accuracy: 0.16 - ETA: 2:59 - loss: 3.2011 - accuracy: 0.16 - ETA: 2:58 - loss: 3.2012 - accuracy: 0.16 - ETA: 2:58 - loss: 3.1972 - accuracy: 0.16 - ETA: 2:57 - loss: 3.1984 - accuracy: 0.16 - ETA: 2:56 - loss: 3.1974 - accuracy: 0.16 - ETA: 2:56 - loss: 3.1945 - accuracy: 0.16 - ETA: 2:55 - loss: 3.1940 - accuracy: 0.16 - ETA: 2:55 - loss: 3.1949 - accuracy: 0.16 - ETA: 2:54 - loss: 3.1962 - accuracy: 0.16 - ETA: 2:53 - loss: 3.2007 - accuracy: 0.16 - ETA: 2:53 - loss: 3.2042 - accuracy: 0.15 - ETA: 2:52 - loss: 3.2085 - accuracy: 0.15 - ETA: 2:51 - loss: 3.2118 - accuracy: 0.15 - ETA: 2:51 - loss: 3.2150 - accuracy: 0.15 - ETA: 2:50 - loss: 3.2160 - accuracy: 0.15 - ETA: 2:50 - loss: 3.2158 - accuracy: 0.15 - ETA: 2:49 - loss: 3.2148 - accuracy: 0.15 - ETA: 2:48 - loss: 3.2151 - accuracy: 0.15 - ETA: 2:48 - loss: 3.2148 - accuracy: 0.15 - ETA: 2:47 - loss: 3.2138 - accuracy: 0.15 - ETA: 2:46 - loss: 3.2171 - accuracy: 0.15 - ETA: 2:46 - loss: 3.2160 - accuracy: 0.15 - ETA: 2:46 - loss: 3.2172 - accuracy: 0.15 - ETA: 2:45 - loss: 3.2156 - accuracy: 0.15 - ETA: 2:45 - loss: 3.2173 - accuracy: 0.15 - ETA: 2:44 - loss: 3.2176 - accuracy: 0.15 - ETA: 2:44 - loss: 3.2215 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2209 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2239 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2283 - accuracy: 0.15 - ETA: 2:42 - loss: 3.2274 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2295 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2301 - accuracy: 0.15 - ETA: 2:40 - loss: 3.2330 - accuracy: 0.15 - ETA: 2:40 - loss: 3.2341 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2303 - accuracy: 0.15 - ETA: 2:38 - loss: 3.2270 - accuracy: 0.15 - ETA: 2:38 - loss: 3.2257 - accuracy: 0.15 - ETA: 2:37 - loss: 3.2247 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2247 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2251 - accuracy: 0.15 - ETA: 2:35 - loss: 3.2250 - accuracy: 0.15 - ETA: 2:35 - loss: 3.2263 - accuracy: 0.15 - ETA: 2:34 - loss: 3.2256 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2244 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2221 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2209 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2176 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2180 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2197 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2191 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2199 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2180 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2164 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2149 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2185 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2163 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2189 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2156 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2147 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2096 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2073 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2063 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2063 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2049 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2041 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2022 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2003 - accuracy: 0.16 - ETA: 2:20 - loss: 3.1978 - accuracy: 0.16 - ETA: 2:19 - loss: 3.1954 - accuracy: 0.16 - ETA: 2:18 - loss: 3.1938 - accuracy: 0.16 - ETA: 2:18 - loss: 3.1925 - accuracy: 0.16 - ETA: 2:17 - loss: 3.1926 - accuracy: 0.16 - ETA: 2:17 - loss: 3.1916 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1897 - accuracy: 0.16 - ETA: 2:16 - loss: 3.1873 - accuracy: 0.16 - ETA: 2:15 - loss: 3.1880 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1876 - accuracy: 0.16 - ETA: 2:14 - loss: 3.1855 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1832 - accuracy: 0.16 - ETA: 2:13 - loss: 3.1832 - accuracy: 0.16 - ETA: 2:12 - loss: 3.1821 - accuracy: 0.16 - ETA: 2:12 - loss: 3.1825 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1829 - accuracy: 0.16 - ETA: 2:11 - loss: 3.1840 - accuracy: 0.16 - ETA: 2:10 - loss: 3.1838 - accuracy: 0.16 - ETA: 2:09 - loss: 3.1811 - accuracy: 0.16 - ETA: 2:09 - loss: 3.1823 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1810 - accuracy: 0.16 - ETA: 2:08 - loss: 3.1813 - accuracy: 0.16 - ETA: 2:07 - loss: 3.1815 - accuracy: 0.16 - ETA: 2:06 - loss: 3.1809 - accuracy: 0.16 - ETA: 2:06 - loss: 3.1813 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1805 - accuracy: 0.16 - ETA: 2:05 - loss: 3.1799 - accuracy: 0.16 - ETA: 2:04 - loss: 3.1808 - accuracy: 0.16 - ETA: 2:03 - loss: 3.1799 - accuracy: 0.16 - ETA: 2:03 - loss: 3.1806 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1783 - accuracy: 0.16 - ETA: 2:02 - loss: 3.1788 - accuracy: 0.16 - ETA: 2:01 - loss: 3.1789 - accuracy: 0.16 - ETA: 2:00 - loss: 3.1801 - accuracy: 0.16 - ETA: 2:00 - loss: 3.1792 - accuracy: 0.16 - ETA: 1:59 - loss: 3.1772 - accuracy: 0.16 - ETA: 1:59 - loss: 3.1761 - accuracy: 0.16 - ETA: 1:58 - loss: 3.1756 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1742 - accuracy: 0.16 - ETA: 1:57 - loss: 3.1738 - accuracy: 0.16 - ETA: 1:56 - loss: 3.1744 - accuracy: 0.16 - ETA: 1:56 - loss: 3.1739 - accuracy: 0.16 - ETA: 1:55 - loss: 3.1733 - accuracy: 0.16 - ETA: 1:55 - loss: 3.1739 - accuracy: 0.16 - ETA: 1:54 - loss: 3.1729 - accuracy: 0.16 - ETA: 1:53 - loss: 3.1731 - accuracy: 0.16 - ETA: 1:53 - loss: 3.1726 - accuracy: 0.16 - ETA: 1:52 - loss: 3.1721 - accuracy: 0.16 - ETA: 1:52 - loss: 3.1708 - accuracy: 0.16 - ETA: 1:51 - loss: 3.1688 - accuracy: 0.16 - ETA: 1:50 - loss: 3.1683 - accuracy: 0.16 - ETA: 1:50 - loss: 3.1680 - accuracy: 0.16 - ETA: 1:49 - loss: 3.1689 - accuracy: 0.16 - ETA: 1:49 - loss: 3.1689 - accuracy: 0.16 - ETA: 1:48 - loss: 3.1678 - accuracy: 0.16 - ETA: 1:47 - loss: 3.1671 - accuracy: 0.16 - ETA: 1:47 - loss: 3.1656 - accuracy: 0.16 - ETA: 1:46 - loss: 3.1655 - accuracy: 0.16 - ETA: 1:46 - loss: 3.1644 - accuracy: 0.16 - ETA: 1:45 - loss: 3.1641 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1637 - accuracy: 0.16 - ETA: 1:44 - loss: 3.1641 - accuracy: 0.16 - ETA: 1:43 - loss: 3.1648 - accuracy: 0.16 - ETA: 1:43 - loss: 3.1648 - accuracy: 0.16 - ETA: 1:42 - loss: 3.1656 - accuracy: 0.16 - ETA: 1:41 - loss: 3.1664 - accuracy: 0.16 - ETA: 1:41 - loss: 3.1665 - accuracy: 0.16 - ETA: 1:40 - loss: 3.1666 - accuracy: 0.16 - ETA: 1:40 - loss: 3.1660 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1661 - accuracy: 0.16 - ETA: 1:39 - loss: 3.1668 - accuracy: 0.16 - ETA: 1:38 - loss: 3.1663 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1652 - accuracy: 0.16 - ETA: 1:37 - loss: 3.1645 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1638 - accuracy: 0.16 - ETA: 1:36 - loss: 3.1638 - accuracy: 0.16 - ETA: 1:35 - loss: 3.1633 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1628 - accuracy: 0.16 - ETA: 1:34 - loss: 3.1618 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1616 - accuracy: 0.16 - ETA: 1:33 - loss: 3.1597 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1593 - accuracy: 0.16 - ETA: 1:32 - loss: 3.1598 - accuracy: 0.16 - ETA: 1:31 - loss: 3.1590 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1588 - accuracy: 0.16 - ETA: 1:30 - loss: 3.1578 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1582 - accuracy: 0.16 - ETA: 1:29 - loss: 3.1578 - accuracy: 0.16 - ETA: 1:28 - loss: 3.1575 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1568 - accuracy: 0.16 - ETA: 1:27 - loss: 3.1566 - accuracy: 0.16 - ETA: 1:26 - loss: 3.1560 - accuracy: 0.16 - ETA: 1:26 - loss: 3.1556 - accuracy: 0.16 - ETA: 1:25 - loss: 3.1545 - accuracy: 0.16 - ETA: 1:24 - loss: 3.1530 - accuracy: 0.1697"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1527 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1532 - accuracy: 0.16 - ETA: 1:23 - loss: 3.1517 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1514 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1499 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1493 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1487 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1484 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1479 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1479 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1479 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1479 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1475 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1466 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1463 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1465 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1475 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1473 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1478 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1477 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1474 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1474 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1480 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1472 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1474 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1459 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1461 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1457 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1455 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1451 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1444 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1442 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1438 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1428 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1436 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1424 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1422 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1423 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1417 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1419 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1412 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1413 - accuracy: 0.17 - ETA: 59s - loss: 3.1413 - accuracy: 0.1718 - ETA: 59s - loss: 3.1407 - accuracy: 0.171 - ETA: 58s - loss: 3.1406 - accuracy: 0.171 - ETA: 58s - loss: 3.1400 - accuracy: 0.171 - ETA: 57s - loss: 3.1393 - accuracy: 0.171 - ETA: 57s - loss: 3.1390 - accuracy: 0.172 - ETA: 56s - loss: 3.1382 - accuracy: 0.172 - ETA: 55s - loss: 3.1384 - accuracy: 0.172 - ETA: 55s - loss: 3.1388 - accuracy: 0.172 - ETA: 54s - loss: 3.1384 - accuracy: 0.172 - ETA: 54s - loss: 3.1378 - accuracy: 0.172 - ETA: 53s - loss: 3.1375 - accuracy: 0.172 - ETA: 52s - loss: 3.1387 - accuracy: 0.172 - ETA: 52s - loss: 3.1382 - accuracy: 0.172 - ETA: 51s - loss: 3.1387 - accuracy: 0.172 - ETA: 51s - loss: 3.1382 - accuracy: 0.172 - ETA: 50s - loss: 3.1370 - accuracy: 0.172 - ETA: 49s - loss: 3.1369 - accuracy: 0.172 - ETA: 49s - loss: 3.1368 - accuracy: 0.172 - ETA: 48s - loss: 3.1361 - accuracy: 0.172 - ETA: 48s - loss: 3.1360 - accuracy: 0.172 - ETA: 47s - loss: 3.1354 - accuracy: 0.172 - ETA: 47s - loss: 3.1353 - accuracy: 0.172 - ETA: 46s - loss: 3.1355 - accuracy: 0.172 - ETA: 45s - loss: 3.1353 - accuracy: 0.172 - ETA: 45s - loss: 3.1349 - accuracy: 0.172 - ETA: 44s - loss: 3.1344 - accuracy: 0.172 - ETA: 44s - loss: 3.1343 - accuracy: 0.172 - ETA: 43s - loss: 3.1337 - accuracy: 0.172 - ETA: 42s - loss: 3.1346 - accuracy: 0.172 - ETA: 42s - loss: 3.1349 - accuracy: 0.172 - ETA: 41s - loss: 3.1347 - accuracy: 0.172 - ETA: 41s - loss: 3.1345 - accuracy: 0.172 - ETA: 40s - loss: 3.1336 - accuracy: 0.172 - ETA: 39s - loss: 3.1340 - accuracy: 0.172 - ETA: 39s - loss: 3.1337 - accuracy: 0.172 - ETA: 38s - loss: 3.1337 - accuracy: 0.172 - ETA: 38s - loss: 3.1336 - accuracy: 0.172 - ETA: 37s - loss: 3.1331 - accuracy: 0.172 - ETA: 37s - loss: 3.1330 - accuracy: 0.172 - ETA: 36s - loss: 3.1328 - accuracy: 0.172 - ETA: 35s - loss: 3.1330 - accuracy: 0.172 - ETA: 35s - loss: 3.1324 - accuracy: 0.172 - ETA: 34s - loss: 3.1327 - accuracy: 0.172 - ETA: 34s - loss: 3.1325 - accuracy: 0.172 - ETA: 33s - loss: 3.1317 - accuracy: 0.172 - ETA: 32s - loss: 3.1321 - accuracy: 0.172 - ETA: 32s - loss: 3.1323 - accuracy: 0.172 - ETA: 31s - loss: 3.1316 - accuracy: 0.172 - ETA: 31s - loss: 3.1315 - accuracy: 0.172 - ETA: 30s - loss: 3.1310 - accuracy: 0.173 - ETA: 29s - loss: 3.1307 - accuracy: 0.172 - ETA: 29s - loss: 3.1301 - accuracy: 0.173 - ETA: 28s - loss: 3.1296 - accuracy: 0.173 - ETA: 28s - loss: 3.1295 - accuracy: 0.173 - ETA: 27s - loss: 3.1295 - accuracy: 0.172 - ETA: 27s - loss: 3.1290 - accuracy: 0.172 - ETA: 26s - loss: 3.1289 - accuracy: 0.173 - ETA: 25s - loss: 3.1283 - accuracy: 0.173 - ETA: 25s - loss: 3.1276 - accuracy: 0.173 - ETA: 24s - loss: 3.1277 - accuracy: 0.172 - ETA: 24s - loss: 3.1279 - accuracy: 0.172 - ETA: 23s - loss: 3.1275 - accuracy: 0.172 - ETA: 22s - loss: 3.1274 - accuracy: 0.172 - ETA: 22s - loss: 3.1269 - accuracy: 0.172 - ETA: 21s - loss: 3.1271 - accuracy: 0.172 - ETA: 21s - loss: 3.1267 - accuracy: 0.173 - ETA: 20s - loss: 3.1260 - accuracy: 0.173 - ETA: 19s - loss: 3.1256 - accuracy: 0.173 - ETA: 19s - loss: 3.1257 - accuracy: 0.173 - ETA: 18s - loss: 3.1256 - accuracy: 0.173 - ETA: 18s - loss: 3.1246 - accuracy: 0.173 - ETA: 17s - loss: 3.1247 - accuracy: 0.173 - ETA: 17s - loss: 3.1240 - accuracy: 0.173 - ETA: 16s - loss: 3.1239 - accuracy: 0.173 - ETA: 15s - loss: 3.1229 - accuracy: 0.173 - ETA: 15s - loss: 3.1225 - accuracy: 0.173 - ETA: 14s - loss: 3.1222 - accuracy: 0.173 - ETA: 14s - loss: 3.1222 - accuracy: 0.173 - ETA: 13s - loss: 3.1224 - accuracy: 0.173 - ETA: 12s - loss: 3.1222 - accuracy: 0.173 - ETA: 12s - loss: 3.1221 - accuracy: 0.173 - ETA: 11s - loss: 3.1223 - accuracy: 0.173 - ETA: 11s - loss: 3.1227 - accuracy: 0.173 - ETA: 10s - loss: 3.1230 - accuracy: 0.173 - ETA: 10s - loss: 3.1229 - accuracy: 0.173 - ETA: 9s - loss: 3.1221 - accuracy: 0.173 - ETA: 8s - loss: 3.1216 - accuracy: 0.17 - ETA: 8s - loss: 3.1220 - accuracy: 0.17 - ETA: 7s - loss: 3.1222 - accuracy: 0.17 - ETA: 7s - loss: 3.1227 - accuracy: 0.17 - ETA: 6s - loss: 3.1225 - accuracy: 0.17 - ETA: 5s - loss: 3.1219 - accuracy: 0.17 - ETA: 5s - loss: 3.1215 - accuracy: 0.17 - ETA: 4s - loss: 3.1212 - accuracy: 0.17 - ETA: 4s - loss: 3.1204 - accuracy: 0.17 - ETA: 3s - loss: 3.1204 - accuracy: 0.17 - ETA: 2s - loss: 3.1205 - accuracy: 0.17 - ETA: 2s - loss: 3.1204 - accuracy: 0.17 - ETA: 1s - loss: 3.1207 - accuracy: 0.17 - ETA: 1s - loss: 3.1203 - accuracy: 0.17 - ETA: 0s - loss: 3.1204 - accuracy: 0.17 - ETA: 0s - loss: 3.1204 - accuracy: 0.17 - 206s 5ms/step - loss: 3.1203 - accuracy: 0.1740 - val_loss: 4.1315 - val_accuracy: 0.0325\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:21 - loss: 3.0201 - accuracy: 0.14 - ETA: 3:17 - loss: 3.0237 - accuracy: 0.16 - ETA: 3:17 - loss: 2.9867 - accuracy: 0.16 - ETA: 3:13 - loss: 3.0183 - accuracy: 0.17 - ETA: 3:12 - loss: 3.0011 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0152 - accuracy: 0.17 - ETA: 3:09 - loss: 3.0430 - accuracy: 0.17 - ETA: 3:08 - loss: 3.0600 - accuracy: 0.17 - ETA: 3:07 - loss: 3.0686 - accuracy: 0.17 - ETA: 3:08 - loss: 3.0516 - accuracy: 0.17 - ETA: 3:08 - loss: 3.0808 - accuracy: 0.16 - ETA: 3:07 - loss: 3.0781 - accuracy: 0.16 - ETA: 3:07 - loss: 3.0728 - accuracy: 0.16 - ETA: 3:06 - loss: 3.0869 - accuracy: 0.16 - ETA: 3:05 - loss: 3.0787 - accuracy: 0.16 - ETA: 3:04 - loss: 3.0723 - accuracy: 0.16 - ETA: 3:03 - loss: 3.0790 - accuracy: 0.16 - ETA: 3:02 - loss: 3.0780 - accuracy: 0.17 - ETA: 3:02 - loss: 3.0717 - accuracy: 0.17 - ETA: 3:01 - loss: 3.0782 - accuracy: 0.17 - ETA: 3:01 - loss: 3.0829 - accuracy: 0.17 - ETA: 3:00 - loss: 3.0912 - accuracy: 0.16 - ETA: 2:59 - loss: 3.0847 - accuracy: 0.17 - ETA: 2:59 - loss: 3.0842 - accuracy: 0.17 - ETA: 2:58 - loss: 3.0817 - accuracy: 0.17 - ETA: 2:58 - loss: 3.0826 - accuracy: 0.17 - ETA: 2:58 - loss: 3.0827 - accuracy: 0.17 - ETA: 2:58 - loss: 3.0794 - accuracy: 0.17 - ETA: 2:58 - loss: 3.0797 - accuracy: 0.17 - ETA: 2:57 - loss: 3.0806 - accuracy: 0.17 - ETA: 2:56 - loss: 3.0824 - accuracy: 0.17 - ETA: 2:55 - loss: 3.0893 - accuracy: 0.17 - ETA: 2:55 - loss: 3.0949 - accuracy: 0.17 - ETA: 2:54 - loss: 3.0997 - accuracy: 0.17 - ETA: 2:53 - loss: 3.0999 - accuracy: 0.16 - ETA: 2:53 - loss: 3.1025 - accuracy: 0.16 - ETA: 2:52 - loss: 3.1073 - accuracy: 0.16 - ETA: 2:51 - loss: 3.1030 - accuracy: 0.16 - ETA: 2:51 - loss: 3.1055 - accuracy: 0.16 - ETA: 2:50 - loss: 3.1022 - accuracy: 0.16 - ETA: 2:49 - loss: 3.1069 - accuracy: 0.16 - ETA: 2:49 - loss: 3.1095 - accuracy: 0.16 - ETA: 2:48 - loss: 3.1096 - accuracy: 0.16 - ETA: 2:47 - loss: 3.1104 - accuracy: 0.16 - ETA: 2:47 - loss: 3.1087 - accuracy: 0.16 - ETA: 2:46 - loss: 3.1016 - accuracy: 0.16 - ETA: 2:46 - loss: 3.1002 - accuracy: 0.16 - ETA: 2:45 - loss: 3.1027 - accuracy: 0.16 - ETA: 2:45 - loss: 3.1061 - accuracy: 0.16 - ETA: 2:44 - loss: 3.1043 - accuracy: 0.16 - ETA: 2:43 - loss: 3.1022 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1061 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1090 - accuracy: 0.16 - ETA: 2:42 - loss: 3.1128 - accuracy: 0.16 - ETA: 2:42 - loss: 3.1094 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1100 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1051 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1048 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1034 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1005 - accuracy: 0.17 - ETA: 2:38 - loss: 3.0977 - accuracy: 0.17 - ETA: 2:38 - loss: 3.0969 - accuracy: 0.17 - ETA: 2:37 - loss: 3.0984 - accuracy: 0.17 - ETA: 2:37 - loss: 3.0987 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1018 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1053 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1033 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1040 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1016 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1049 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1072 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1083 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1061 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1057 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1041 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1043 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1056 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1053 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1064 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1059 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1074 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1095 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1077 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1084 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1085 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1091 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1091 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1119 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1099 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1101 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1105 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1120 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1138 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1103 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1108 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1137 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1130 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1123 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1118 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1115 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1119 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1099 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1116 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1105 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1096 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1092 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1089 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1089 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1097 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1096 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1086 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1084 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1088 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1098 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1090 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1110 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1119 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1115 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1125 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1121 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1110 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1125 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1131 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1145 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1138 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1147 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1147 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1142 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1142 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1139 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1130 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1135 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1116 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1097 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1090 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1098 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1096 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1077 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1089 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1095 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1080 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1083 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1095 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1096 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1102 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1091 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1100 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1095 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1091 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1087 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1084 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1091 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1089 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1072 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1071 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1072 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1074 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1061 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1052 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1052 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1058 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1059 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1050 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1048 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1051 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1050 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1052 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1060 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1066 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1072 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1081 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1083 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1073 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1083 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1085 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1089 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1101 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1102 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1105 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1103 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1106 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1094 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1094 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1091 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1084 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1083 - accuracy: 0.1745"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.1082 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1080 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1080 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1088 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1082 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1094 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1099 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1091 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1090 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1078 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1083 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1081 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1076 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1075 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1076 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1075 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1066 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1070 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1067 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1070 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1080 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1085 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1085 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1087 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1077 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1087 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1101 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1097 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1096 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1099 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1095 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1085 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1073 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1073 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1067 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1070 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1063 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1064 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1062 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1062 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1067 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1066 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1056 - accuracy: 0.17 - ETA: 59s - loss: 3.1064 - accuracy: 0.1759 - ETA: 58s - loss: 3.1062 - accuracy: 0.176 - ETA: 58s - loss: 3.1061 - accuracy: 0.175 - ETA: 57s - loss: 3.1053 - accuracy: 0.176 - ETA: 57s - loss: 3.1049 - accuracy: 0.176 - ETA: 56s - loss: 3.1051 - accuracy: 0.176 - ETA: 55s - loss: 3.1057 - accuracy: 0.176 - ETA: 55s - loss: 3.1054 - accuracy: 0.176 - ETA: 54s - loss: 3.1052 - accuracy: 0.176 - ETA: 54s - loss: 3.1055 - accuracy: 0.176 - ETA: 53s - loss: 3.1047 - accuracy: 0.176 - ETA: 52s - loss: 3.1047 - accuracy: 0.176 - ETA: 52s - loss: 3.1050 - accuracy: 0.176 - ETA: 51s - loss: 3.1049 - accuracy: 0.176 - ETA: 51s - loss: 3.1040 - accuracy: 0.176 - ETA: 50s - loss: 3.1037 - accuracy: 0.176 - ETA: 50s - loss: 3.1037 - accuracy: 0.176 - ETA: 49s - loss: 3.1041 - accuracy: 0.176 - ETA: 48s - loss: 3.1033 - accuracy: 0.177 - ETA: 48s - loss: 3.1027 - accuracy: 0.177 - ETA: 47s - loss: 3.1024 - accuracy: 0.177 - ETA: 47s - loss: 3.1026 - accuracy: 0.177 - ETA: 46s - loss: 3.1024 - accuracy: 0.177 - ETA: 46s - loss: 3.1026 - accuracy: 0.177 - ETA: 45s - loss: 3.1030 - accuracy: 0.177 - ETA: 44s - loss: 3.1034 - accuracy: 0.177 - ETA: 44s - loss: 3.1029 - accuracy: 0.177 - ETA: 43s - loss: 3.1034 - accuracy: 0.177 - ETA: 43s - loss: 3.1034 - accuracy: 0.177 - ETA: 42s - loss: 3.1026 - accuracy: 0.177 - ETA: 41s - loss: 3.1019 - accuracy: 0.177 - ETA: 41s - loss: 3.1015 - accuracy: 0.177 - ETA: 40s - loss: 3.1010 - accuracy: 0.177 - ETA: 40s - loss: 3.1009 - accuracy: 0.177 - ETA: 39s - loss: 3.1004 - accuracy: 0.178 - ETA: 38s - loss: 3.1002 - accuracy: 0.178 - ETA: 38s - loss: 3.1007 - accuracy: 0.178 - ETA: 37s - loss: 3.1008 - accuracy: 0.178 - ETA: 37s - loss: 3.1018 - accuracy: 0.177 - ETA: 36s - loss: 3.1029 - accuracy: 0.177 - ETA: 35s - loss: 3.1036 - accuracy: 0.177 - ETA: 35s - loss: 3.1040 - accuracy: 0.177 - ETA: 34s - loss: 3.1043 - accuracy: 0.177 - ETA: 34s - loss: 3.1046 - accuracy: 0.177 - ETA: 33s - loss: 3.1047 - accuracy: 0.177 - ETA: 33s - loss: 3.1045 - accuracy: 0.177 - ETA: 32s - loss: 3.1040 - accuracy: 0.177 - ETA: 31s - loss: 3.1044 - accuracy: 0.177 - ETA: 31s - loss: 3.1037 - accuracy: 0.177 - ETA: 30s - loss: 3.1042 - accuracy: 0.177 - ETA: 30s - loss: 3.1042 - accuracy: 0.177 - ETA: 29s - loss: 3.1035 - accuracy: 0.177 - ETA: 28s - loss: 3.1036 - accuracy: 0.177 - ETA: 28s - loss: 3.1039 - accuracy: 0.177 - ETA: 27s - loss: 3.1042 - accuracy: 0.177 - ETA: 27s - loss: 3.1042 - accuracy: 0.177 - ETA: 26s - loss: 3.1033 - accuracy: 0.177 - ETA: 25s - loss: 3.1027 - accuracy: 0.177 - ETA: 25s - loss: 3.1026 - accuracy: 0.177 - ETA: 24s - loss: 3.1024 - accuracy: 0.177 - ETA: 24s - loss: 3.1029 - accuracy: 0.177 - ETA: 23s - loss: 3.1026 - accuracy: 0.177 - ETA: 23s - loss: 3.1025 - accuracy: 0.177 - ETA: 22s - loss: 3.1023 - accuracy: 0.178 - ETA: 21s - loss: 3.1022 - accuracy: 0.178 - ETA: 21s - loss: 3.1030 - accuracy: 0.178 - ETA: 20s - loss: 3.1026 - accuracy: 0.178 - ETA: 20s - loss: 3.1024 - accuracy: 0.178 - ETA: 19s - loss: 3.1026 - accuracy: 0.178 - ETA: 18s - loss: 3.1018 - accuracy: 0.178 - ETA: 18s - loss: 3.1023 - accuracy: 0.178 - ETA: 17s - loss: 3.1022 - accuracy: 0.178 - ETA: 17s - loss: 3.1023 - accuracy: 0.178 - ETA: 16s - loss: 3.1019 - accuracy: 0.178 - ETA: 15s - loss: 3.1020 - accuracy: 0.178 - ETA: 15s - loss: 3.1019 - accuracy: 0.178 - ETA: 14s - loss: 3.1019 - accuracy: 0.178 - ETA: 14s - loss: 3.1018 - accuracy: 0.178 - ETA: 13s - loss: 3.1016 - accuracy: 0.178 - ETA: 13s - loss: 3.1008 - accuracy: 0.178 - ETA: 12s - loss: 3.1011 - accuracy: 0.178 - ETA: 11s - loss: 3.1009 - accuracy: 0.178 - ETA: 11s - loss: 3.1007 - accuracy: 0.178 - ETA: 10s - loss: 3.1007 - accuracy: 0.178 - ETA: 10s - loss: 3.1001 - accuracy: 0.179 - ETA: 9s - loss: 3.1000 - accuracy: 0.179 - ETA: 8s - loss: 3.0998 - accuracy: 0.17 - ETA: 8s - loss: 3.0999 - accuracy: 0.17 - ETA: 7s - loss: 3.0994 - accuracy: 0.17 - ETA: 7s - loss: 3.0996 - accuracy: 0.17 - ETA: 6s - loss: 3.0991 - accuracy: 0.17 - ETA: 5s - loss: 3.0991 - accuracy: 0.17 - ETA: 5s - loss: 3.0987 - accuracy: 0.17 - ETA: 4s - loss: 3.0982 - accuracy: 0.17 - ETA: 4s - loss: 3.0984 - accuracy: 0.17 - ETA: 3s - loss: 3.0985 - accuracy: 0.17 - ETA: 2s - loss: 3.0991 - accuracy: 0.17 - ETA: 2s - loss: 3.0993 - accuracy: 0.17 - ETA: 1s - loss: 3.0995 - accuracy: 0.17 - ETA: 1s - loss: 3.0998 - accuracy: 0.17 - ETA: 0s - loss: 3.1001 - accuracy: 0.17 - ETA: 0s - loss: 3.1004 - accuracy: 0.17 - 207s 5ms/step - loss: 3.1004 - accuracy: 0.1792 - val_loss: 3.9846 - val_accuracy: 0.0346\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:33 - loss: 2.7712 - accuracy: 0.24 - ETA: 3:26 - loss: 2.8541 - accuracy: 0.21 - ETA: 3:24 - loss: 2.9003 - accuracy: 0.21 - ETA: 3:19 - loss: 2.9178 - accuracy: 0.21 - ETA: 3:17 - loss: 2.9310 - accuracy: 0.21 - ETA: 3:17 - loss: 2.9458 - accuracy: 0.21 - ETA: 3:14 - loss: 2.9870 - accuracy: 0.21 - ETA: 3:14 - loss: 2.9923 - accuracy: 0.21 - ETA: 3:12 - loss: 2.9797 - accuracy: 0.21 - ETA: 3:11 - loss: 2.9962 - accuracy: 0.21 - ETA: 3:10 - loss: 3.0124 - accuracy: 0.20 - ETA: 3:08 - loss: 3.0092 - accuracy: 0.20 - ETA: 3:07 - loss: 3.0213 - accuracy: 0.20 - ETA: 3:06 - loss: 3.0077 - accuracy: 0.20 - ETA: 3:05 - loss: 3.0159 - accuracy: 0.20 - ETA: 3:05 - loss: 3.0365 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0367 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0507 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0603 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0603 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0605 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0628 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0618 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0638 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0706 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0761 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0741 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0760 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0749 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0739 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0776 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0769 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0718 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0766 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0675 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0717 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0707 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0670 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0601 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0567 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0574 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0641 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0639 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0647 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0669 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0717 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0723 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0672 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0646 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0679 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0664 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0674 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0621 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0636 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0610 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0591 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0612 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0612 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0601 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0610 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0621 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0626 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0645 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0632 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0641 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0692 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0677 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0678 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0682 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0646 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0626 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0616 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0631 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0616 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0606 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0595 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0605 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0591 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0586 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0613 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0593 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0600 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0599 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0586 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0594 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0590 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0618 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0630 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0630 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0630 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0659 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0674 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0681 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0695 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0693 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0706 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0703 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0702 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0736 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0727 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0743 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0762 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0782 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0808 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0808 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0822 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0832 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0852 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0861 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0853 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0856 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0877 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0878 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0890 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0897 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0914 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0929 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0939 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0953 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0953 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0949 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0945 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0951 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0951 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0941 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0946 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0946 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0940 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0930 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0915 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0917 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0900 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0903 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0893 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0885 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0897 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0897 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0889 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0888 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0869 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0869 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0854 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0863 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0860 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0851 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0851 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0843 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0854 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0865 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0851 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0846 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0854 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0849 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0870 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0870 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0865 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0872 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0872 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0877 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0872 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0860 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0867 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0870 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0868 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0876 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0887 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0884 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0881 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0881 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0875 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0882 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0893 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0891 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0892 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0876 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0875 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0872 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0872 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0874 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0884 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0877 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0873 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0875 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0866 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0874 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0882 - accuracy: 0.1819"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.0879 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0885 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0889 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0895 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0899 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0900 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0897 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0888 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0883 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0890 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0886 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0892 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0890 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0892 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0889 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0882 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0882 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0878 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0874 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0879 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0880 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0871 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0871 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0868 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0876 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0876 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0876 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0886 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0884 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0891 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0899 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0904 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0910 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0908 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0900 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0903 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0895 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0889 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0889 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0887 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0888 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0888 - accuracy: 0.18 - ETA: 59s - loss: 3.0889 - accuracy: 0.1821 - ETA: 59s - loss: 3.0888 - accuracy: 0.182 - ETA: 58s - loss: 3.0882 - accuracy: 0.182 - ETA: 58s - loss: 3.0878 - accuracy: 0.182 - ETA: 57s - loss: 3.0877 - accuracy: 0.182 - ETA: 56s - loss: 3.0875 - accuracy: 0.182 - ETA: 56s - loss: 3.0873 - accuracy: 0.182 - ETA: 55s - loss: 3.0872 - accuracy: 0.182 - ETA: 55s - loss: 3.0870 - accuracy: 0.182 - ETA: 54s - loss: 3.0868 - accuracy: 0.182 - ETA: 53s - loss: 3.0871 - accuracy: 0.182 - ETA: 53s - loss: 3.0874 - accuracy: 0.182 - ETA: 52s - loss: 3.0878 - accuracy: 0.182 - ETA: 52s - loss: 3.0883 - accuracy: 0.182 - ETA: 51s - loss: 3.0882 - accuracy: 0.182 - ETA: 51s - loss: 3.0881 - accuracy: 0.182 - ETA: 50s - loss: 3.0881 - accuracy: 0.182 - ETA: 49s - loss: 3.0874 - accuracy: 0.182 - ETA: 49s - loss: 3.0872 - accuracy: 0.182 - ETA: 48s - loss: 3.0873 - accuracy: 0.182 - ETA: 48s - loss: 3.0874 - accuracy: 0.182 - ETA: 47s - loss: 3.0878 - accuracy: 0.182 - ETA: 46s - loss: 3.0881 - accuracy: 0.182 - ETA: 46s - loss: 3.0873 - accuracy: 0.182 - ETA: 45s - loss: 3.0878 - accuracy: 0.182 - ETA: 45s - loss: 3.0876 - accuracy: 0.182 - ETA: 44s - loss: 3.0881 - accuracy: 0.182 - ETA: 44s - loss: 3.0879 - accuracy: 0.182 - ETA: 43s - loss: 3.0878 - accuracy: 0.182 - ETA: 42s - loss: 3.0884 - accuracy: 0.182 - ETA: 42s - loss: 3.0888 - accuracy: 0.182 - ETA: 41s - loss: 3.0891 - accuracy: 0.182 - ETA: 41s - loss: 3.0891 - accuracy: 0.182 - ETA: 40s - loss: 3.0886 - accuracy: 0.182 - ETA: 39s - loss: 3.0888 - accuracy: 0.182 - ETA: 39s - loss: 3.0887 - accuracy: 0.182 - ETA: 38s - loss: 3.0878 - accuracy: 0.182 - ETA: 38s - loss: 3.0878 - accuracy: 0.182 - ETA: 37s - loss: 3.0886 - accuracy: 0.182 - ETA: 36s - loss: 3.0889 - accuracy: 0.182 - ETA: 36s - loss: 3.0891 - accuracy: 0.182 - ETA: 35s - loss: 3.0884 - accuracy: 0.182 - ETA: 35s - loss: 3.0879 - accuracy: 0.182 - ETA: 34s - loss: 3.0876 - accuracy: 0.182 - ETA: 34s - loss: 3.0872 - accuracy: 0.182 - ETA: 33s - loss: 3.0878 - accuracy: 0.182 - ETA: 32s - loss: 3.0877 - accuracy: 0.182 - ETA: 32s - loss: 3.0877 - accuracy: 0.182 - ETA: 31s - loss: 3.0886 - accuracy: 0.182 - ETA: 31s - loss: 3.0884 - accuracy: 0.182 - ETA: 30s - loss: 3.0883 - accuracy: 0.182 - ETA: 29s - loss: 3.0892 - accuracy: 0.182 - ETA: 29s - loss: 3.0899 - accuracy: 0.182 - ETA: 28s - loss: 3.0898 - accuracy: 0.182 - ETA: 28s - loss: 3.0899 - accuracy: 0.182 - ETA: 27s - loss: 3.0896 - accuracy: 0.182 - ETA: 27s - loss: 3.0887 - accuracy: 0.182 - ETA: 26s - loss: 3.0885 - accuracy: 0.182 - ETA: 25s - loss: 3.0886 - accuracy: 0.182 - ETA: 25s - loss: 3.0881 - accuracy: 0.183 - ETA: 24s - loss: 3.0881 - accuracy: 0.183 - ETA: 24s - loss: 3.0881 - accuracy: 0.182 - ETA: 23s - loss: 3.0880 - accuracy: 0.182 - ETA: 22s - loss: 3.0875 - accuracy: 0.182 - ETA: 22s - loss: 3.0875 - accuracy: 0.182 - ETA: 21s - loss: 3.0860 - accuracy: 0.183 - ETA: 21s - loss: 3.0861 - accuracy: 0.183 - ETA: 20s - loss: 3.0861 - accuracy: 0.183 - ETA: 19s - loss: 3.0858 - accuracy: 0.183 - ETA: 19s - loss: 3.0865 - accuracy: 0.183 - ETA: 18s - loss: 3.0864 - accuracy: 0.183 - ETA: 18s - loss: 3.0870 - accuracy: 0.183 - ETA: 17s - loss: 3.0870 - accuracy: 0.183 - ETA: 17s - loss: 3.0869 - accuracy: 0.183 - ETA: 16s - loss: 3.0871 - accuracy: 0.183 - ETA: 15s - loss: 3.0865 - accuracy: 0.183 - ETA: 15s - loss: 3.0875 - accuracy: 0.182 - ETA: 14s - loss: 3.0870 - accuracy: 0.182 - ETA: 14s - loss: 3.0861 - accuracy: 0.183 - ETA: 13s - loss: 3.0855 - accuracy: 0.183 - ETA: 12s - loss: 3.0848 - accuracy: 0.183 - ETA: 12s - loss: 3.0847 - accuracy: 0.183 - ETA: 11s - loss: 3.0838 - accuracy: 0.183 - ETA: 11s - loss: 3.0842 - accuracy: 0.183 - ETA: 10s - loss: 3.0834 - accuracy: 0.183 - ETA: 10s - loss: 3.0830 - accuracy: 0.183 - ETA: 9s - loss: 3.0830 - accuracy: 0.183 - ETA: 8s - loss: 3.0835 - accuracy: 0.18 - ETA: 8s - loss: 3.0832 - accuracy: 0.18 - ETA: 7s - loss: 3.0833 - accuracy: 0.18 - ETA: 7s - loss: 3.0838 - accuracy: 0.18 - ETA: 6s - loss: 3.0832 - accuracy: 0.18 - ETA: 5s - loss: 3.0832 - accuracy: 0.18 - ETA: 5s - loss: 3.0831 - accuracy: 0.18 - ETA: 4s - loss: 3.0830 - accuracy: 0.18 - ETA: 4s - loss: 3.0832 - accuracy: 0.18 - ETA: 3s - loss: 3.0836 - accuracy: 0.18 - ETA: 2s - loss: 3.0837 - accuracy: 0.18 - ETA: 2s - loss: 3.0834 - accuracy: 0.18 - ETA: 1s - loss: 3.0834 - accuracy: 0.18 - ETA: 1s - loss: 3.0836 - accuracy: 0.18 - ETA: 0s - loss: 3.0834 - accuracy: 0.18 - ETA: 0s - loss: 3.0827 - accuracy: 0.18 - 206s 5ms/step - loss: 3.0824 - accuracy: 0.1837 - val_loss: 4.0160 - val_accuracy: 0.0401\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:29 - loss: 3.2412 - accuracy: 0.14 - ETA: 3:15 - loss: 3.1873 - accuracy: 0.15 - ETA: 3:11 - loss: 3.1978 - accuracy: 0.15 - ETA: 3:12 - loss: 3.1659 - accuracy: 0.16 - ETA: 3:14 - loss: 3.1412 - accuracy: 0.17 - ETA: 3:16 - loss: 3.1364 - accuracy: 0.16 - ETA: 3:17 - loss: 3.1125 - accuracy: 0.17 - ETA: 3:16 - loss: 3.0918 - accuracy: 0.17 - ETA: 3:14 - loss: 3.0905 - accuracy: 0.17 - ETA: 3:12 - loss: 3.1062 - accuracy: 0.17 - ETA: 3:10 - loss: 3.1088 - accuracy: 0.17 - ETA: 3:09 - loss: 3.1325 - accuracy: 0.17 - ETA: 3:08 - loss: 3.1333 - accuracy: 0.17 - ETA: 3:10 - loss: 3.1293 - accuracy: 0.17 - ETA: 3:09 - loss: 3.1239 - accuracy: 0.17 - ETA: 3:08 - loss: 3.1219 - accuracy: 0.17 - ETA: 3:07 - loss: 3.1276 - accuracy: 0.17 - ETA: 3:06 - loss: 3.1206 - accuracy: 0.17 - ETA: 3:05 - loss: 3.1336 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1395 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1309 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1229 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1274 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1224 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1130 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1209 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1195 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1205 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1168 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1163 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1206 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1191 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1195 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1199 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1174 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1166 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1159 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1089 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1079 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1071 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1031 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1069 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1067 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1035 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1009 - accuracy: 0.17 - ETA: 2:48 - loss: 3.0986 - accuracy: 0.17 - ETA: 2:47 - loss: 3.0972 - accuracy: 0.17 - ETA: 2:47 - loss: 3.0967 - accuracy: 0.17 - ETA: 2:46 - loss: 3.0977 - accuracy: 0.17 - ETA: 2:45 - loss: 3.0963 - accuracy: 0.17 - ETA: 2:45 - loss: 3.0911 - accuracy: 0.17 - ETA: 2:44 - loss: 3.0898 - accuracy: 0.17 - ETA: 2:44 - loss: 3.0866 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0832 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0835 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0825 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0789 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0824 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0824 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0784 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0763 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0772 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0799 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0790 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0780 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0778 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0763 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0702 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0722 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0712 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0701 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0689 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0676 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0692 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0703 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0707 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0711 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0707 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0705 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0701 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0701 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0708 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0714 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0756 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0765 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0742 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0742 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0724 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0739 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0753 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0783 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0801 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0796 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0791 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0789 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0763 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0764 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0762 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0760 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0782 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0774 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0769 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0765 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0774 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0782 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0771 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0781 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0774 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0768 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0753 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0737 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0738 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0743 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0726 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0715 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0703 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0698 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0684 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0682 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0678 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0685 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0688 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0701 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0690 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0712 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0704 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0718 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0717 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0722 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0719 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0721 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0728 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0718 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0709 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0693 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0702 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0705 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0705 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0710 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0702 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0704 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0703 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0711 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0705 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0697 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0692 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0697 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0707 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0705 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0695 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0690 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0694 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0698 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0695 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0695 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0676 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0682 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0684 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0690 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0688 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0693 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0699 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0698 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0694 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0696 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0699 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0710 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0721 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0721 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0723 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0717 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0724 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0718 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0716 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0721 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0727 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0720 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0724 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0720 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0714 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0715 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0720 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0716 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0717 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0724 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0723 - accuracy: 0.1841"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.0728 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0729 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0729 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0710 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0708 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0704 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0704 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0711 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0712 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0715 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0704 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0710 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0715 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0719 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0722 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0717 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0711 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0706 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0707 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0718 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0721 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0714 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0714 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0716 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0719 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0716 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0729 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0732 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0731 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0734 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0732 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0733 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0727 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0717 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0722 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0713 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0718 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0715 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0719 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0717 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0720 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0717 - accuracy: 0.18 - ETA: 59s - loss: 3.0725 - accuracy: 0.1833 - ETA: 58s - loss: 3.0718 - accuracy: 0.183 - ETA: 58s - loss: 3.0724 - accuracy: 0.183 - ETA: 57s - loss: 3.0719 - accuracy: 0.183 - ETA: 57s - loss: 3.0713 - accuracy: 0.183 - ETA: 56s - loss: 3.0715 - accuracy: 0.182 - ETA: 55s - loss: 3.0711 - accuracy: 0.183 - ETA: 55s - loss: 3.0711 - accuracy: 0.183 - ETA: 54s - loss: 3.0715 - accuracy: 0.183 - ETA: 54s - loss: 3.0712 - accuracy: 0.183 - ETA: 53s - loss: 3.0718 - accuracy: 0.183 - ETA: 52s - loss: 3.0715 - accuracy: 0.182 - ETA: 52s - loss: 3.0707 - accuracy: 0.182 - ETA: 51s - loss: 3.0702 - accuracy: 0.183 - ETA: 51s - loss: 3.0700 - accuracy: 0.183 - ETA: 50s - loss: 3.0692 - accuracy: 0.183 - ETA: 50s - loss: 3.0695 - accuracy: 0.183 - ETA: 49s - loss: 3.0698 - accuracy: 0.183 - ETA: 48s - loss: 3.0701 - accuracy: 0.183 - ETA: 48s - loss: 3.0699 - accuracy: 0.182 - ETA: 47s - loss: 3.0705 - accuracy: 0.182 - ETA: 47s - loss: 3.0702 - accuracy: 0.182 - ETA: 46s - loss: 3.0706 - accuracy: 0.182 - ETA: 45s - loss: 3.0702 - accuracy: 0.182 - ETA: 45s - loss: 3.0703 - accuracy: 0.182 - ETA: 44s - loss: 3.0701 - accuracy: 0.182 - ETA: 44s - loss: 3.0698 - accuracy: 0.182 - ETA: 43s - loss: 3.0697 - accuracy: 0.182 - ETA: 42s - loss: 3.0695 - accuracy: 0.182 - ETA: 42s - loss: 3.0692 - accuracy: 0.182 - ETA: 41s - loss: 3.0683 - accuracy: 0.182 - ETA: 41s - loss: 3.0690 - accuracy: 0.182 - ETA: 40s - loss: 3.0684 - accuracy: 0.182 - ETA: 40s - loss: 3.0680 - accuracy: 0.183 - ETA: 39s - loss: 3.0680 - accuracy: 0.183 - ETA: 38s - loss: 3.0687 - accuracy: 0.182 - ETA: 38s - loss: 3.0680 - accuracy: 0.182 - ETA: 37s - loss: 3.0680 - accuracy: 0.182 - ETA: 37s - loss: 3.0682 - accuracy: 0.183 - ETA: 36s - loss: 3.0679 - accuracy: 0.183 - ETA: 35s - loss: 3.0686 - accuracy: 0.182 - ETA: 35s - loss: 3.0685 - accuracy: 0.182 - ETA: 34s - loss: 3.0689 - accuracy: 0.182 - ETA: 34s - loss: 3.0691 - accuracy: 0.182 - ETA: 33s - loss: 3.0692 - accuracy: 0.182 - ETA: 32s - loss: 3.0700 - accuracy: 0.182 - ETA: 32s - loss: 3.0713 - accuracy: 0.182 - ETA: 31s - loss: 3.0719 - accuracy: 0.182 - ETA: 31s - loss: 3.0711 - accuracy: 0.182 - ETA: 30s - loss: 3.0717 - accuracy: 0.182 - ETA: 30s - loss: 3.0707 - accuracy: 0.182 - ETA: 29s - loss: 3.0697 - accuracy: 0.182 - ETA: 28s - loss: 3.0698 - accuracy: 0.182 - ETA: 28s - loss: 3.0697 - accuracy: 0.182 - ETA: 27s - loss: 3.0698 - accuracy: 0.182 - ETA: 27s - loss: 3.0697 - accuracy: 0.182 - ETA: 26s - loss: 3.0697 - accuracy: 0.182 - ETA: 25s - loss: 3.0693 - accuracy: 0.182 - ETA: 25s - loss: 3.0694 - accuracy: 0.182 - ETA: 24s - loss: 3.0694 - accuracy: 0.182 - ETA: 24s - loss: 3.0702 - accuracy: 0.182 - ETA: 23s - loss: 3.0701 - accuracy: 0.182 - ETA: 22s - loss: 3.0698 - accuracy: 0.183 - ETA: 22s - loss: 3.0710 - accuracy: 0.183 - ETA: 21s - loss: 3.0704 - accuracy: 0.183 - ETA: 21s - loss: 3.0709 - accuracy: 0.183 - ETA: 20s - loss: 3.0710 - accuracy: 0.183 - ETA: 20s - loss: 3.0712 - accuracy: 0.182 - ETA: 19s - loss: 3.0718 - accuracy: 0.182 - ETA: 18s - loss: 3.0717 - accuracy: 0.182 - ETA: 18s - loss: 3.0717 - accuracy: 0.182 - ETA: 17s - loss: 3.0729 - accuracy: 0.182 - ETA: 17s - loss: 3.0736 - accuracy: 0.182 - ETA: 16s - loss: 3.0736 - accuracy: 0.182 - ETA: 15s - loss: 3.0746 - accuracy: 0.182 - ETA: 15s - loss: 3.0755 - accuracy: 0.182 - ETA: 14s - loss: 3.0759 - accuracy: 0.182 - ETA: 14s - loss: 3.0766 - accuracy: 0.181 - ETA: 13s - loss: 3.0765 - accuracy: 0.182 - ETA: 12s - loss: 3.0764 - accuracy: 0.182 - ETA: 12s - loss: 3.0770 - accuracy: 0.182 - ETA: 11s - loss: 3.0768 - accuracy: 0.182 - ETA: 11s - loss: 3.0770 - accuracy: 0.182 - ETA: 10s - loss: 3.0774 - accuracy: 0.182 - ETA: 10s - loss: 3.0773 - accuracy: 0.182 - ETA: 9s - loss: 3.0773 - accuracy: 0.182 - ETA: 8s - loss: 3.0767 - accuracy: 0.18 - ETA: 8s - loss: 3.0769 - accuracy: 0.18 - ETA: 7s - loss: 3.0776 - accuracy: 0.18 - ETA: 7s - loss: 3.0776 - accuracy: 0.18 - ETA: 6s - loss: 3.0781 - accuracy: 0.18 - ETA: 5s - loss: 3.0786 - accuracy: 0.18 - ETA: 5s - loss: 3.0783 - accuracy: 0.18 - ETA: 4s - loss: 3.0784 - accuracy: 0.18 - ETA: 4s - loss: 3.0782 - accuracy: 0.18 - ETA: 3s - loss: 3.0783 - accuracy: 0.18 - ETA: 2s - loss: 3.0787 - accuracy: 0.18 - ETA: 2s - loss: 3.0785 - accuracy: 0.18 - ETA: 1s - loss: 3.0782 - accuracy: 0.18 - ETA: 1s - loss: 3.0784 - accuracy: 0.18 - ETA: 0s - loss: 3.0781 - accuracy: 0.18 - ETA: 0s - loss: 3.0786 - accuracy: 0.18 - 206s 5ms/step - loss: 3.0783 - accuracy: 0.1821 - val_loss: 4.0061 - val_accuracy: 0.0368\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:32 - loss: 3.1596 - accuracy: 0.15 - ETA: 3:17 - loss: 3.1221 - accuracy: 0.16 - ETA: 3:14 - loss: 3.1173 - accuracy: 0.18 - ETA: 3:14 - loss: 3.1269 - accuracy: 0.19 - ETA: 3:13 - loss: 3.1430 - accuracy: 0.18 - ETA: 3:11 - loss: 3.1573 - accuracy: 0.17 - ETA: 3:09 - loss: 3.1749 - accuracy: 0.17 - ETA: 3:11 - loss: 3.1751 - accuracy: 0.17 - ETA: 3:12 - loss: 3.1661 - accuracy: 0.17 - ETA: 3:12 - loss: 3.1514 - accuracy: 0.17 - ETA: 3:11 - loss: 3.1421 - accuracy: 0.17 - ETA: 3:10 - loss: 3.1269 - accuracy: 0.17 - ETA: 3:09 - loss: 3.1404 - accuracy: 0.17 - ETA: 3:08 - loss: 3.1219 - accuracy: 0.17 - ETA: 3:07 - loss: 3.1194 - accuracy: 0.17 - ETA: 3:07 - loss: 3.1172 - accuracy: 0.17 - ETA: 3:05 - loss: 3.1084 - accuracy: 0.17 - ETA: 3:05 - loss: 3.1115 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1091 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1074 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1119 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1096 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1130 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1058 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1010 - accuracy: 0.18 - ETA: 2:58 - loss: 3.1077 - accuracy: 0.17 - ETA: 2:58 - loss: 3.0997 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1018 - accuracy: 0.18 - ETA: 2:56 - loss: 3.1049 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1023 - accuracy: 0.18 - ETA: 2:56 - loss: 3.1014 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0999 - accuracy: 0.18 - ETA: 2:54 - loss: 3.1012 - accuracy: 0.18 - ETA: 2:54 - loss: 3.1002 - accuracy: 0.18 - ETA: 2:53 - loss: 3.1007 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0979 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0988 - accuracy: 0.17 - ETA: 2:53 - loss: 3.0971 - accuracy: 0.17 - ETA: 2:52 - loss: 3.0950 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0961 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0953 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0951 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0930 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0935 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0919 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0909 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0906 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0875 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0831 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0858 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0830 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0846 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0840 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0856 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0850 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0822 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0795 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0795 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0799 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0786 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0808 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0807 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0826 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0853 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0829 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0829 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0845 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0869 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0869 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0897 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0903 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0934 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0941 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0964 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0979 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0974 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0979 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0995 - accuracy: 0.18 - ETA: 2:28 - loss: 3.1011 - accuracy: 0.18 - ETA: 2:27 - loss: 3.1048 - accuracy: 0.18 - ETA: 2:27 - loss: 3.1026 - accuracy: 0.18 - ETA: 2:26 - loss: 3.1042 - accuracy: 0.18 - ETA: 2:25 - loss: 3.1040 - accuracy: 0.18 - ETA: 2:25 - loss: 3.1044 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1026 - accuracy: 0.18 - ETA: 2:23 - loss: 3.1020 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1018 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1006 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0992 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0975 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0961 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0942 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0960 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0979 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0983 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0972 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0969 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0967 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0989 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0984 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0975 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0972 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0962 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0955 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0945 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0960 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0943 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0943 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0953 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0934 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0939 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0928 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0935 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0927 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0913 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0907 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0911 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0924 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0934 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0923 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0926 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0951 - accuracy: 0.17 - ETA: 2:02 - loss: 3.0943 - accuracy: 0.17 - ETA: 2:01 - loss: 3.0948 - accuracy: 0.17 - ETA: 2:00 - loss: 3.0955 - accuracy: 0.17 - ETA: 2:00 - loss: 3.0951 - accuracy: 0.17 - ETA: 1:59 - loss: 3.0941 - accuracy: 0.17 - ETA: 1:59 - loss: 3.0951 - accuracy: 0.17 - ETA: 1:58 - loss: 3.0937 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0937 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0939 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0936 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0928 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0927 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0928 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0922 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0915 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0920 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0913 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0917 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0914 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0932 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0933 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0919 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0912 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0904 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0890 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0901 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0901 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0893 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0900 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0877 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0881 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0873 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0874 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0883 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0877 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0884 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0861 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0850 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0840 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0842 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0839 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0828 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0830 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0825 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0820 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0819 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0813 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0823 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0817 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0818 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0808 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0814 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0807 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0809 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0812 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0810 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0800 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0792 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0791 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0790 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0792 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0793 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0792 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0799 - accuracy: 0.1840"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.0788 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0777 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0791 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0795 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0792 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0795 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0802 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0794 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0795 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0810 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0812 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0794 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0796 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0784 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0788 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0790 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0789 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0791 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0783 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0780 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0780 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0776 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0767 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0763 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0761 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0759 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0766 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0765 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0764 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0774 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0784 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0788 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0793 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0795 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0806 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0816 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0827 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0824 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0829 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0822 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0824 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0832 - accuracy: 0.18 - ETA: 59s - loss: 3.0829 - accuracy: 0.1846 - ETA: 59s - loss: 3.0832 - accuracy: 0.184 - ETA: 58s - loss: 3.0844 - accuracy: 0.184 - ETA: 58s - loss: 3.0844 - accuracy: 0.184 - ETA: 57s - loss: 3.0847 - accuracy: 0.184 - ETA: 56s - loss: 3.0861 - accuracy: 0.183 - ETA: 56s - loss: 3.0865 - accuracy: 0.183 - ETA: 55s - loss: 3.0875 - accuracy: 0.183 - ETA: 55s - loss: 3.0874 - accuracy: 0.183 - ETA: 54s - loss: 3.0877 - accuracy: 0.183 - ETA: 54s - loss: 3.0879 - accuracy: 0.183 - ETA: 53s - loss: 3.0878 - accuracy: 0.183 - ETA: 52s - loss: 3.0880 - accuracy: 0.183 - ETA: 52s - loss: 3.0879 - accuracy: 0.183 - ETA: 51s - loss: 3.0884 - accuracy: 0.183 - ETA: 51s - loss: 3.0883 - accuracy: 0.183 - ETA: 50s - loss: 3.0885 - accuracy: 0.183 - ETA: 49s - loss: 3.0891 - accuracy: 0.183 - ETA: 49s - loss: 3.0888 - accuracy: 0.183 - ETA: 48s - loss: 3.0888 - accuracy: 0.183 - ETA: 48s - loss: 3.0886 - accuracy: 0.183 - ETA: 47s - loss: 3.0890 - accuracy: 0.183 - ETA: 46s - loss: 3.0888 - accuracy: 0.183 - ETA: 46s - loss: 3.0892 - accuracy: 0.183 - ETA: 45s - loss: 3.0895 - accuracy: 0.183 - ETA: 45s - loss: 3.0892 - accuracy: 0.183 - ETA: 44s - loss: 3.0891 - accuracy: 0.183 - ETA: 44s - loss: 3.0896 - accuracy: 0.183 - ETA: 43s - loss: 3.0898 - accuracy: 0.183 - ETA: 42s - loss: 3.0891 - accuracy: 0.183 - ETA: 42s - loss: 3.0899 - accuracy: 0.183 - ETA: 41s - loss: 3.0903 - accuracy: 0.183 - ETA: 41s - loss: 3.0901 - accuracy: 0.183 - ETA: 40s - loss: 3.0903 - accuracy: 0.183 - ETA: 39s - loss: 3.0900 - accuracy: 0.183 - ETA: 39s - loss: 3.0905 - accuracy: 0.183 - ETA: 38s - loss: 3.0900 - accuracy: 0.183 - ETA: 38s - loss: 3.0906 - accuracy: 0.183 - ETA: 37s - loss: 3.0910 - accuracy: 0.183 - ETA: 36s - loss: 3.0902 - accuracy: 0.183 - ETA: 36s - loss: 3.0910 - accuracy: 0.183 - ETA: 35s - loss: 3.0909 - accuracy: 0.183 - ETA: 35s - loss: 3.0914 - accuracy: 0.183 - ETA: 34s - loss: 3.0912 - accuracy: 0.183 - ETA: 34s - loss: 3.0914 - accuracy: 0.183 - ETA: 33s - loss: 3.0910 - accuracy: 0.183 - ETA: 32s - loss: 3.0911 - accuracy: 0.183 - ETA: 32s - loss: 3.0915 - accuracy: 0.183 - ETA: 31s - loss: 3.0922 - accuracy: 0.183 - ETA: 31s - loss: 3.0927 - accuracy: 0.182 - ETA: 30s - loss: 3.0933 - accuracy: 0.182 - ETA: 29s - loss: 3.0934 - accuracy: 0.182 - ETA: 29s - loss: 3.0933 - accuracy: 0.182 - ETA: 28s - loss: 3.0936 - accuracy: 0.182 - ETA: 28s - loss: 3.0941 - accuracy: 0.182 - ETA: 27s - loss: 3.0938 - accuracy: 0.182 - ETA: 27s - loss: 3.0940 - accuracy: 0.182 - ETA: 26s - loss: 3.0942 - accuracy: 0.182 - ETA: 25s - loss: 3.0946 - accuracy: 0.182 - ETA: 25s - loss: 3.0954 - accuracy: 0.182 - ETA: 24s - loss: 3.0947 - accuracy: 0.182 - ETA: 24s - loss: 3.0945 - accuracy: 0.182 - ETA: 23s - loss: 3.0947 - accuracy: 0.182 - ETA: 22s - loss: 3.0951 - accuracy: 0.182 - ETA: 22s - loss: 3.0951 - accuracy: 0.182 - ETA: 21s - loss: 3.0951 - accuracy: 0.182 - ETA: 21s - loss: 3.0950 - accuracy: 0.182 - ETA: 20s - loss: 3.0950 - accuracy: 0.181 - ETA: 19s - loss: 3.0950 - accuracy: 0.181 - ETA: 19s - loss: 3.0953 - accuracy: 0.182 - ETA: 18s - loss: 3.0962 - accuracy: 0.181 - ETA: 18s - loss: 3.0961 - accuracy: 0.181 - ETA: 17s - loss: 3.0957 - accuracy: 0.181 - ETA: 17s - loss: 3.0961 - accuracy: 0.181 - ETA: 16s - loss: 3.0959 - accuracy: 0.181 - ETA: 15s - loss: 3.0957 - accuracy: 0.181 - ETA: 15s - loss: 3.0952 - accuracy: 0.181 - ETA: 14s - loss: 3.0950 - accuracy: 0.181 - ETA: 14s - loss: 3.0946 - accuracy: 0.181 - ETA: 13s - loss: 3.0942 - accuracy: 0.182 - ETA: 12s - loss: 3.0946 - accuracy: 0.182 - ETA: 12s - loss: 3.0940 - accuracy: 0.182 - ETA: 11s - loss: 3.0938 - accuracy: 0.182 - ETA: 11s - loss: 3.0933 - accuracy: 0.182 - ETA: 10s - loss: 3.0936 - accuracy: 0.182 - ETA: 10s - loss: 3.0934 - accuracy: 0.182 - ETA: 9s - loss: 3.0930 - accuracy: 0.182 - ETA: 8s - loss: 3.0932 - accuracy: 0.18 - ETA: 8s - loss: 3.0927 - accuracy: 0.18 - ETA: 7s - loss: 3.0932 - accuracy: 0.18 - ETA: 7s - loss: 3.0935 - accuracy: 0.18 - ETA: 6s - loss: 3.0933 - accuracy: 0.18 - ETA: 5s - loss: 3.0933 - accuracy: 0.18 - ETA: 5s - loss: 3.0934 - accuracy: 0.18 - ETA: 4s - loss: 3.0936 - accuracy: 0.18 - ETA: 4s - loss: 3.0936 - accuracy: 0.18 - ETA: 3s - loss: 3.0930 - accuracy: 0.18 - ETA: 2s - loss: 3.0931 - accuracy: 0.18 - ETA: 2s - loss: 3.0926 - accuracy: 0.18 - ETA: 1s - loss: 3.0925 - accuracy: 0.18 - ETA: 1s - loss: 3.0928 - accuracy: 0.18 - ETA: 0s - loss: 3.0927 - accuracy: 0.18 - ETA: 0s - loss: 3.0927 - accuracy: 0.18 - 206s 5ms/step - loss: 3.0926 - accuracy: 0.1831 - val_loss: 4.1042 - val_accuracy: 0.0373\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:34 - loss: 3.0663 - accuracy: 0.19 - ETA: 3:23 - loss: 3.1975 - accuracy: 0.18 - ETA: 3:20 - loss: 3.2301 - accuracy: 0.16 - ETA: 3:16 - loss: 3.2118 - accuracy: 0.16 - ETA: 3:14 - loss: 3.1847 - accuracy: 0.16 - ETA: 3:12 - loss: 3.1787 - accuracy: 0.16 - ETA: 3:11 - loss: 3.1576 - accuracy: 0.17 - ETA: 3:10 - loss: 3.1711 - accuracy: 0.16 - ETA: 3:09 - loss: 3.1661 - accuracy: 0.16 - ETA: 3:08 - loss: 3.1825 - accuracy: 0.16 - ETA: 3:10 - loss: 3.1936 - accuracy: 0.16 - ETA: 3:10 - loss: 3.2009 - accuracy: 0.16 - ETA: 3:10 - loss: 3.1885 - accuracy: 0.16 - ETA: 3:09 - loss: 3.1646 - accuracy: 0.16 - ETA: 3:09 - loss: 3.1555 - accuracy: 0.16 - ETA: 3:07 - loss: 3.1405 - accuracy: 0.17 - ETA: 3:06 - loss: 3.1482 - accuracy: 0.17 - ETA: 3:05 - loss: 3.1369 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1375 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1291 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1225 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1234 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1155 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1145 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1121 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1077 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1021 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1056 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1050 - accuracy: 0.17 - ETA: 2:58 - loss: 3.0969 - accuracy: 0.17 - ETA: 2:57 - loss: 3.0915 - accuracy: 0.17 - ETA: 2:57 - loss: 3.0888 - accuracy: 0.17 - ETA: 2:56 - loss: 3.0896 - accuracy: 0.17 - ETA: 2:55 - loss: 3.0889 - accuracy: 0.17 - ETA: 2:54 - loss: 3.0887 - accuracy: 0.17 - ETA: 2:54 - loss: 3.0871 - accuracy: 0.17 - ETA: 2:53 - loss: 3.0834 - accuracy: 0.17 - ETA: 2:53 - loss: 3.0861 - accuracy: 0.17 - ETA: 2:52 - loss: 3.0875 - accuracy: 0.17 - ETA: 2:52 - loss: 3.0903 - accuracy: 0.17 - ETA: 2:51 - loss: 3.0878 - accuracy: 0.17 - ETA: 2:50 - loss: 3.0844 - accuracy: 0.17 - ETA: 2:50 - loss: 3.0884 - accuracy: 0.17 - ETA: 2:49 - loss: 3.0840 - accuracy: 0.17 - ETA: 2:48 - loss: 3.0812 - accuracy: 0.17 - ETA: 2:48 - loss: 3.0827 - accuracy: 0.17 - ETA: 2:47 - loss: 3.0830 - accuracy: 0.17 - ETA: 2:46 - loss: 3.0846 - accuracy: 0.17 - ETA: 2:46 - loss: 3.0862 - accuracy: 0.17 - ETA: 2:45 - loss: 3.0852 - accuracy: 0.17 - ETA: 2:44 - loss: 3.0830 - accuracy: 0.17 - ETA: 2:44 - loss: 3.0834 - accuracy: 0.17 - ETA: 2:43 - loss: 3.0828 - accuracy: 0.17 - ETA: 2:42 - loss: 3.0818 - accuracy: 0.17 - ETA: 2:42 - loss: 3.0839 - accuracy: 0.17 - ETA: 2:41 - loss: 3.0840 - accuracy: 0.17 - ETA: 2:40 - loss: 3.0862 - accuracy: 0.17 - ETA: 2:40 - loss: 3.0854 - accuracy: 0.17 - ETA: 2:39 - loss: 3.0898 - accuracy: 0.17 - ETA: 2:38 - loss: 3.0882 - accuracy: 0.17 - ETA: 2:38 - loss: 3.0871 - accuracy: 0.17 - ETA: 2:37 - loss: 3.0870 - accuracy: 0.17 - ETA: 2:37 - loss: 3.0858 - accuracy: 0.17 - ETA: 2:36 - loss: 3.0855 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0883 - accuracy: 0.17 - ETA: 2:35 - loss: 3.0900 - accuracy: 0.17 - ETA: 2:35 - loss: 3.0867 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0851 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0854 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0856 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0839 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0848 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0882 - accuracy: 0.17 - ETA: 2:31 - loss: 3.0885 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0869 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0868 - accuracy: 0.17 - ETA: 2:29 - loss: 3.0878 - accuracy: 0.17 - ETA: 2:28 - loss: 3.0859 - accuracy: 0.17 - ETA: 2:27 - loss: 3.0851 - accuracy: 0.17 - ETA: 2:27 - loss: 3.0822 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0882 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0864 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0868 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0862 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0859 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0876 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0890 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0893 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0879 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0886 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0872 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0864 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0870 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0856 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0866 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0890 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0886 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0890 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0909 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0896 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0875 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0871 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0877 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0872 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0875 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0884 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0875 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0885 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0863 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0851 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0847 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0823 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0843 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0841 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0850 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0833 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0817 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0836 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0838 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0824 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0807 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0833 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0846 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0855 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0845 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0845 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0841 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0822 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0816 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0818 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0818 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0807 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0795 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0783 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0781 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0774 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0771 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0768 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0766 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0760 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0731 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0744 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0745 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0755 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0760 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0761 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0752 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0754 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0753 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0753 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0749 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0752 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0735 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0734 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0729 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0737 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0747 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0757 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0747 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0742 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0741 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0729 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0719 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0706 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0708 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0703 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0707 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0711 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0704 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0710 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0704 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0712 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0711 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0700 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0694 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0683 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0676 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0676 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0667 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0669 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0668 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0674 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0683 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0681 - accuracy: 0.1854"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:24 - loss: 3.0691 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0688 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0684 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0679 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0680 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0681 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0689 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0702 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0714 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0704 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0707 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0713 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0712 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0704 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0705 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0710 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0705 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0711 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0704 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0707 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0715 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0712 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0720 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0731 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0742 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0749 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0745 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0747 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0758 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0757 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0750 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0752 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0751 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0753 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0738 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0751 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0753 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0746 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0751 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0742 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0745 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0747 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0745 - accuracy: 0.18 - ETA: 59s - loss: 3.0743 - accuracy: 0.1853 - ETA: 58s - loss: 3.0751 - accuracy: 0.185 - ETA: 58s - loss: 3.0756 - accuracy: 0.185 - ETA: 57s - loss: 3.0754 - accuracy: 0.185 - ETA: 57s - loss: 3.0755 - accuracy: 0.185 - ETA: 56s - loss: 3.0746 - accuracy: 0.185 - ETA: 55s - loss: 3.0749 - accuracy: 0.185 - ETA: 55s - loss: 3.0750 - accuracy: 0.185 - ETA: 54s - loss: 3.0754 - accuracy: 0.185 - ETA: 54s - loss: 3.0751 - accuracy: 0.185 - ETA: 53s - loss: 3.0749 - accuracy: 0.185 - ETA: 53s - loss: 3.0749 - accuracy: 0.185 - ETA: 52s - loss: 3.0740 - accuracy: 0.186 - ETA: 51s - loss: 3.0736 - accuracy: 0.186 - ETA: 51s - loss: 3.0738 - accuracy: 0.186 - ETA: 50s - loss: 3.0742 - accuracy: 0.185 - ETA: 50s - loss: 3.0742 - accuracy: 0.186 - ETA: 49s - loss: 3.0739 - accuracy: 0.185 - ETA: 48s - loss: 3.0736 - accuracy: 0.186 - ETA: 48s - loss: 3.0732 - accuracy: 0.186 - ETA: 47s - loss: 3.0736 - accuracy: 0.186 - ETA: 47s - loss: 3.0733 - accuracy: 0.185 - ETA: 46s - loss: 3.0730 - accuracy: 0.186 - ETA: 45s - loss: 3.0729 - accuracy: 0.185 - ETA: 45s - loss: 3.0724 - accuracy: 0.186 - ETA: 44s - loss: 3.0725 - accuracy: 0.186 - ETA: 44s - loss: 3.0727 - accuracy: 0.185 - ETA: 43s - loss: 3.0733 - accuracy: 0.185 - ETA: 43s - loss: 3.0731 - accuracy: 0.185 - ETA: 42s - loss: 3.0723 - accuracy: 0.185 - ETA: 41s - loss: 3.0731 - accuracy: 0.185 - ETA: 41s - loss: 3.0732 - accuracy: 0.185 - ETA: 40s - loss: 3.0736 - accuracy: 0.185 - ETA: 40s - loss: 3.0730 - accuracy: 0.185 - ETA: 39s - loss: 3.0727 - accuracy: 0.185 - ETA: 38s - loss: 3.0728 - accuracy: 0.185 - ETA: 38s - loss: 3.0733 - accuracy: 0.185 - ETA: 37s - loss: 3.0738 - accuracy: 0.185 - ETA: 37s - loss: 3.0743 - accuracy: 0.185 - ETA: 36s - loss: 3.0744 - accuracy: 0.185 - ETA: 35s - loss: 3.0737 - accuracy: 0.185 - ETA: 35s - loss: 3.0729 - accuracy: 0.186 - ETA: 34s - loss: 3.0733 - accuracy: 0.185 - ETA: 34s - loss: 3.0725 - accuracy: 0.186 - ETA: 33s - loss: 3.0729 - accuracy: 0.185 - ETA: 32s - loss: 3.0727 - accuracy: 0.185 - ETA: 32s - loss: 3.0724 - accuracy: 0.185 - ETA: 31s - loss: 3.0726 - accuracy: 0.185 - ETA: 31s - loss: 3.0725 - accuracy: 0.185 - ETA: 30s - loss: 3.0719 - accuracy: 0.185 - ETA: 30s - loss: 3.0712 - accuracy: 0.185 - ETA: 29s - loss: 3.0707 - accuracy: 0.185 - ETA: 28s - loss: 3.0699 - accuracy: 0.185 - ETA: 28s - loss: 3.0703 - accuracy: 0.185 - ETA: 27s - loss: 3.0708 - accuracy: 0.185 - ETA: 27s - loss: 3.0710 - accuracy: 0.185 - ETA: 26s - loss: 3.0706 - accuracy: 0.185 - ETA: 25s - loss: 3.0710 - accuracy: 0.185 - ETA: 25s - loss: 3.0708 - accuracy: 0.185 - ETA: 24s - loss: 3.0715 - accuracy: 0.185 - ETA: 24s - loss: 3.0708 - accuracy: 0.185 - ETA: 23s - loss: 3.0713 - accuracy: 0.185 - ETA: 23s - loss: 3.0714 - accuracy: 0.185 - ETA: 22s - loss: 3.0709 - accuracy: 0.185 - ETA: 21s - loss: 3.0705 - accuracy: 0.185 - ETA: 21s - loss: 3.0710 - accuracy: 0.185 - ETA: 20s - loss: 3.0711 - accuracy: 0.185 - ETA: 20s - loss: 3.0709 - accuracy: 0.185 - ETA: 19s - loss: 3.0708 - accuracy: 0.185 - ETA: 18s - loss: 3.0708 - accuracy: 0.185 - ETA: 18s - loss: 3.0710 - accuracy: 0.185 - ETA: 17s - loss: 3.0706 - accuracy: 0.185 - ETA: 17s - loss: 3.0704 - accuracy: 0.185 - ETA: 16s - loss: 3.0696 - accuracy: 0.185 - ETA: 15s - loss: 3.0700 - accuracy: 0.185 - ETA: 15s - loss: 3.0698 - accuracy: 0.185 - ETA: 14s - loss: 3.0704 - accuracy: 0.185 - ETA: 14s - loss: 3.0709 - accuracy: 0.185 - ETA: 13s - loss: 3.0703 - accuracy: 0.185 - ETA: 12s - loss: 3.0705 - accuracy: 0.184 - ETA: 12s - loss: 3.0696 - accuracy: 0.185 - ETA: 11s - loss: 3.0697 - accuracy: 0.185 - ETA: 11s - loss: 3.0704 - accuracy: 0.185 - ETA: 10s - loss: 3.0699 - accuracy: 0.185 - ETA: 10s - loss: 3.0698 - accuracy: 0.185 - ETA: 9s - loss: 3.0700 - accuracy: 0.185 - ETA: 8s - loss: 3.0707 - accuracy: 0.18 - ETA: 8s - loss: 3.0706 - accuracy: 0.18 - ETA: 7s - loss: 3.0704 - accuracy: 0.18 - ETA: 7s - loss: 3.0702 - accuracy: 0.18 - ETA: 6s - loss: 3.0698 - accuracy: 0.18 - ETA: 5s - loss: 3.0703 - accuracy: 0.18 - ETA: 5s - loss: 3.0707 - accuracy: 0.18 - ETA: 4s - loss: 3.0708 - accuracy: 0.18 - ETA: 4s - loss: 3.0705 - accuracy: 0.18 - ETA: 3s - loss: 3.0706 - accuracy: 0.18 - ETA: 2s - loss: 3.0702 - accuracy: 0.18 - ETA: 2s - loss: 3.0700 - accuracy: 0.18 - ETA: 1s - loss: 3.0702 - accuracy: 0.18 - ETA: 1s - loss: 3.0704 - accuracy: 0.18 - ETA: 0s - loss: 3.0704 - accuracy: 0.18 - ETA: 0s - loss: 3.0703 - accuracy: 0.18 - 207s 5ms/step - loss: 3.0701 - accuracy: 0.1847 - val_loss: 4.0615 - val_accuracy: 0.0375\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:18 - loss: 3.0568 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0168 - accuracy: 0.20 - ETA: 3:11 - loss: 3.2408 - accuracy: 0.20 - ETA: 3:13 - loss: 3.2103 - accuracy: 0.19 - ETA: 3:14 - loss: 3.1737 - accuracy: 0.19 - ETA: 3:13 - loss: 3.1785 - accuracy: 0.18 - ETA: 3:12 - loss: 3.1434 - accuracy: 0.18 - ETA: 3:13 - loss: 3.1274 - accuracy: 0.18 - ETA: 3:14 - loss: 3.1148 - accuracy: 0.18 - ETA: 3:12 - loss: 3.1047 - accuracy: 0.18 - ETA: 3:12 - loss: 3.1107 - accuracy: 0.18 - ETA: 3:11 - loss: 3.1054 - accuracy: 0.18 - ETA: 3:12 - loss: 3.1127 - accuracy: 0.18 - ETA: 3:12 - loss: 3.1146 - accuracy: 0.18 - ETA: 3:11 - loss: 3.1290 - accuracy: 0.17 - ETA: 3:10 - loss: 3.1282 - accuracy: 0.17 - ETA: 3:09 - loss: 3.1210 - accuracy: 0.17 - ETA: 3:08 - loss: 3.1212 - accuracy: 0.17 - ETA: 3:07 - loss: 3.1124 - accuracy: 0.18 - ETA: 3:06 - loss: 3.1056 - accuracy: 0.18 - ETA: 3:05 - loss: 3.1035 - accuracy: 0.18 - ETA: 3:04 - loss: 3.1026 - accuracy: 0.18 - ETA: 3:03 - loss: 3.1115 - accuracy: 0.18 - ETA: 3:02 - loss: 3.1152 - accuracy: 0.18 - ETA: 3:02 - loss: 3.1166 - accuracy: 0.18 - ETA: 3:01 - loss: 3.1205 - accuracy: 0.18 - ETA: 3:00 - loss: 3.1193 - accuracy: 0.18 - ETA: 2:59 - loss: 3.1208 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1209 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1234 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1182 - accuracy: 0.18 - ETA: 2:56 - loss: 3.1190 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1133 - accuracy: 0.18 - ETA: 2:55 - loss: 3.1099 - accuracy: 0.18 - ETA: 2:54 - loss: 3.1098 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1122 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1104 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1141 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1145 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1132 - accuracy: 0.18 - ETA: 2:53 - loss: 3.1152 - accuracy: 0.18 - ETA: 2:53 - loss: 3.1138 - accuracy: 0.18 - ETA: 2:52 - loss: 3.1135 - accuracy: 0.18 - ETA: 2:52 - loss: 3.1070 - accuracy: 0.18 - ETA: 2:51 - loss: 3.1045 - accuracy: 0.18 - ETA: 2:51 - loss: 3.1079 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1110 - accuracy: 0.18 - ETA: 2:50 - loss: 3.1123 - accuracy: 0.18 - ETA: 2:49 - loss: 3.1108 - accuracy: 0.18 - ETA: 2:49 - loss: 3.1083 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1055 - accuracy: 0.18 - ETA: 2:47 - loss: 3.1019 - accuracy: 0.18 - ETA: 2:47 - loss: 3.1025 - accuracy: 0.18 - ETA: 2:46 - loss: 3.1034 - accuracy: 0.18 - ETA: 2:45 - loss: 3.1007 - accuracy: 0.17 - ETA: 2:45 - loss: 3.0955 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0939 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0903 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0909 - accuracy: 0.17 - ETA: 2:44 - loss: 3.0902 - accuracy: 0.17 - ETA: 2:46 - loss: 3.0873 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0850 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0875 - accuracy: 0.17 - ETA: 2:50 - loss: 3.0862 - accuracy: 0.17 - ETA: 2:50 - loss: 3.0838 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0816 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0816 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0821 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0826 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0847 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0822 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0832 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0866 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0856 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0839 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0849 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0869 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0870 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0868 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0864 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0873 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0870 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0864 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0834 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0819 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0801 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0837 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0860 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0858 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0828 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0826 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0823 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0819 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0801 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0798 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0778 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0766 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0767 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0763 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0756 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0749 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0729 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0729 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0715 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0711 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0697 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0701 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0691 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0706 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0708 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0721 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0727 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0749 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0743 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0735 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0720 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0722 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0730 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0733 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0730 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0714 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0700 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0685 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0686 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0688 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0685 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0687 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0674 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0670 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0678 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0681 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0682 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0676 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0691 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0690 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0693 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0682 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0682 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0688 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0674 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0658 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0668 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0675 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0678 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0670 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0668 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0676 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0671 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0673 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0679 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0669 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0675 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0678 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0672 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0675 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0662 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0663 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0655 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0659 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0662 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0659 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0658 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0667 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0668 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0676 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0664 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0668 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0681 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0678 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0670 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0669 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0672 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0676 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0678 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0682 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0691 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0683 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0678 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0675 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0669 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0677 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0678 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0693 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0691 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0694 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0694 - accuracy: 0.1846"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:59 - loss: 3.0701 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0704 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0703 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0704 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0693 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0694 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0707 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0712 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0726 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0726 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0721 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0720 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0713 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0714 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0723 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0734 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0738 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0738 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0740 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0745 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0754 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0747 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0763 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0763 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0757 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0749 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0760 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0766 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0785 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0777 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0778 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0776 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0772 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0774 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0786 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0773 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0778 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0783 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0781 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0784 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0781 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0788 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0787 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0780 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0784 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0773 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0768 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0764 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0767 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0770 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0768 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0770 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0765 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0766 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0763 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0775 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0762 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0753 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0751 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0756 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0753 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0751 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0747 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0750 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0748 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0748 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0750 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0752 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0758 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0761 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0767 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0778 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0784 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0782 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0789 - accuracy: 0.18 - ETA: 59s - loss: 3.0796 - accuracy: 0.1823 - ETA: 58s - loss: 3.0801 - accuracy: 0.182 - ETA: 57s - loss: 3.0796 - accuracy: 0.182 - ETA: 56s - loss: 3.0803 - accuracy: 0.182 - ETA: 56s - loss: 3.0801 - accuracy: 0.182 - ETA: 55s - loss: 3.0804 - accuracy: 0.182 - ETA: 54s - loss: 3.0809 - accuracy: 0.182 - ETA: 53s - loss: 3.0812 - accuracy: 0.182 - ETA: 52s - loss: 3.0820 - accuracy: 0.182 - ETA: 51s - loss: 3.0825 - accuracy: 0.181 - ETA: 50s - loss: 3.0822 - accuracy: 0.182 - ETA: 50s - loss: 3.0818 - accuracy: 0.182 - ETA: 49s - loss: 3.0818 - accuracy: 0.182 - ETA: 48s - loss: 3.0818 - accuracy: 0.182 - ETA: 47s - loss: 3.0819 - accuracy: 0.182 - ETA: 46s - loss: 3.0819 - accuracy: 0.182 - ETA: 45s - loss: 3.0822 - accuracy: 0.182 - ETA: 45s - loss: 3.0824 - accuracy: 0.182 - ETA: 44s - loss: 3.0828 - accuracy: 0.182 - ETA: 43s - loss: 3.0838 - accuracy: 0.182 - ETA: 42s - loss: 3.0847 - accuracy: 0.182 - ETA: 41s - loss: 3.0854 - accuracy: 0.181 - ETA: 40s - loss: 3.0861 - accuracy: 0.181 - ETA: 39s - loss: 3.0856 - accuracy: 0.181 - ETA: 39s - loss: 3.0853 - accuracy: 0.181 - ETA: 38s - loss: 3.0858 - accuracy: 0.181 - ETA: 37s - loss: 3.0859 - accuracy: 0.181 - ETA: 36s - loss: 3.0863 - accuracy: 0.181 - ETA: 35s - loss: 3.0870 - accuracy: 0.181 - ETA: 34s - loss: 3.0870 - accuracy: 0.181 - ETA: 33s - loss: 3.0866 - accuracy: 0.181 - ETA: 33s - loss: 3.0865 - accuracy: 0.181 - ETA: 32s - loss: 3.0873 - accuracy: 0.181 - ETA: 31s - loss: 3.0874 - accuracy: 0.181 - ETA: 30s - loss: 3.0875 - accuracy: 0.181 - ETA: 29s - loss: 3.0882 - accuracy: 0.181 - ETA: 28s - loss: 3.0884 - accuracy: 0.181 - ETA: 27s - loss: 3.0885 - accuracy: 0.181 - ETA: 27s - loss: 3.0878 - accuracy: 0.181 - ETA: 26s - loss: 3.0874 - accuracy: 0.181 - ETA: 25s - loss: 3.0874 - accuracy: 0.181 - ETA: 24s - loss: 3.0870 - accuracy: 0.181 - ETA: 23s - loss: 3.0874 - accuracy: 0.181 - ETA: 22s - loss: 3.0875 - accuracy: 0.181 - ETA: 21s - loss: 3.0872 - accuracy: 0.181 - ETA: 20s - loss: 3.0872 - accuracy: 0.181 - ETA: 20s - loss: 3.0873 - accuracy: 0.181 - ETA: 19s - loss: 3.0873 - accuracy: 0.181 - ETA: 18s - loss: 3.0876 - accuracy: 0.181 - ETA: 17s - loss: 3.0879 - accuracy: 0.181 - ETA: 16s - loss: 3.0882 - accuracy: 0.181 - ETA: 15s - loss: 3.0872 - accuracy: 0.181 - ETA: 14s - loss: 3.0870 - accuracy: 0.181 - ETA: 14s - loss: 3.0870 - accuracy: 0.181 - ETA: 13s - loss: 3.0871 - accuracy: 0.181 - ETA: 12s - loss: 3.0872 - accuracy: 0.181 - ETA: 11s - loss: 3.0878 - accuracy: 0.181 - ETA: 10s - loss: 3.0880 - accuracy: 0.181 - ETA: 9s - loss: 3.0882 - accuracy: 0.181 - ETA: 8s - loss: 3.0879 - accuracy: 0.18 - ETA: 7s - loss: 3.0881 - accuracy: 0.18 - ETA: 7s - loss: 3.0874 - accuracy: 0.18 - ETA: 6s - loss: 3.0875 - accuracy: 0.18 - ETA: 5s - loss: 3.0877 - accuracy: 0.18 - ETA: 4s - loss: 3.0880 - accuracy: 0.18 - ETA: 3s - loss: 3.0883 - accuracy: 0.18 - ETA: 2s - loss: 3.0885 - accuracy: 0.18 - ETA: 1s - loss: 3.0881 - accuracy: 0.18 - ETA: 0s - loss: 3.0880 - accuracy: 0.18 - ETA: 0s - loss: 3.0877 - accuracy: 0.18 - 312s 7ms/step - loss: 3.0876 - accuracy: 0.1818 - val_loss: 3.9780 - val_accuracy: 0.0354\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:17 - loss: 3.0501 - accuracy: 0.21 - ETA: 5:04 - loss: 3.0497 - accuracy: 0.20 - ETA: 5:06 - loss: 3.0114 - accuracy: 0.21 - ETA: 5:04 - loss: 3.0292 - accuracy: 0.21 - ETA: 4:59 - loss: 3.0137 - accuracy: 0.22 - ETA: 4:57 - loss: 3.0325 - accuracy: 0.21 - ETA: 4:57 - loss: 3.0417 - accuracy: 0.21 - ETA: 4:56 - loss: 3.0473 - accuracy: 0.20 - ETA: 4:57 - loss: 3.0228 - accuracy: 0.21 - ETA: 4:56 - loss: 3.0331 - accuracy: 0.20 - ETA: 4:56 - loss: 3.0330 - accuracy: 0.21 - ETA: 4:55 - loss: 3.0445 - accuracy: 0.20 - ETA: 4:55 - loss: 3.0487 - accuracy: 0.20 - ETA: 4:54 - loss: 3.0539 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0566 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0405 - accuracy: 0.20 - ETA: 4:52 - loss: 3.0379 - accuracy: 0.20 - ETA: 4:51 - loss: 3.0546 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0599 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0615 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0750 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0725 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0763 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0658 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0628 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0626 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0461 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0498 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0509 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0526 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0530 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0505 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0472 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0442 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0423 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0463 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0477 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0430 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0419 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0448 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0452 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0424 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0471 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0444 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0476 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0499 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0474 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0516 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0495 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0447 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0442 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0444 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0463 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0468 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0475 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0515 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0517 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0572 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0591 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0554 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0578 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0605 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0590 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0626 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0623 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0648 - accuracy: 0.18 - ETA: 4:10 - loss: 3.0624 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0607 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0633 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0597 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0608 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0580 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0579 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0583 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0607 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0620 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0601 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0606 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0593 - accuracy: 0.18 - ETA: 3:57 - loss: 3.0581 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0543 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0532 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0521 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0532 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0528 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0521 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0500 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0522 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0508 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0504 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0484 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0468 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0470 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0464 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0465 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0472 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0475 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0466 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0465 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0480 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0481 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0486 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0490 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0488 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0472 - accuracy: 0.18 - ETA: 3:33 - loss: 3.0482 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0481 - accuracy: 0.18 - ETA: 3:31 - loss: 3.0464 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0459 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0469 - accuracy: 0.18 - ETA: 3:28 - loss: 3.0464 - accuracy: 0.18 - ETA: 3:27 - loss: 3.0460 - accuracy: 0.18 - ETA: 3:26 - loss: 3.0461 - accuracy: 0.18 - ETA: 3:25 - loss: 3.0463 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0462 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0446 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0459 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0461 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0448 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0445 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0455 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0448 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0430 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0422 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0435 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0439 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0446 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0508 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0519 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0524 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0514 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0517 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0518 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0540 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0546 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0553 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0528 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0527 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0527 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0526 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0515 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0520 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0531 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0531 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0537 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0544 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0545 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0542 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0554 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0566 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0575 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0572 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0574 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0578 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0572 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0578 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0567 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0563 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0563 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0567 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0575 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0579 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0581 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0600 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0604 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0609 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0614 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0614 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0620 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0620 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0626 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0630 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0632 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0639 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0637 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0644 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0648 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0655 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0664 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0665 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0679 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0691 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0698 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0703 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0706 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0711 - accuracy: 0.1856"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0722 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0733 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0733 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0735 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0728 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0730 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0731 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0734 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0735 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0736 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0732 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0731 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0743 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0753 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0760 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0755 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0748 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0750 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0756 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0756 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0747 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0746 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0745 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0739 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0738 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0741 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0735 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0725 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0726 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0734 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0742 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0749 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0752 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0758 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0756 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0762 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0760 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0753 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0744 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0752 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0743 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0736 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0735 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0737 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0739 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0734 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0737 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0731 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0719 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0721 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0723 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0726 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0725 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0720 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0728 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0723 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0712 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0717 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0715 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0711 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0716 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0719 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0723 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0722 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0725 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0729 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0727 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0729 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0725 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0732 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0734 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0733 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0729 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0742 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0740 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0733 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0738 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0737 - accuracy: 0.18 - ETA: 59s - loss: 3.0736 - accuracy: 0.1847 - ETA: 58s - loss: 3.0741 - accuracy: 0.184 - ETA: 57s - loss: 3.0745 - accuracy: 0.184 - ETA: 56s - loss: 3.0746 - accuracy: 0.184 - ETA: 55s - loss: 3.0749 - accuracy: 0.184 - ETA: 54s - loss: 3.0756 - accuracy: 0.184 - ETA: 53s - loss: 3.0750 - accuracy: 0.184 - ETA: 52s - loss: 3.0745 - accuracy: 0.184 - ETA: 51s - loss: 3.0744 - accuracy: 0.184 - ETA: 50s - loss: 3.0740 - accuracy: 0.184 - ETA: 49s - loss: 3.0739 - accuracy: 0.184 - ETA: 48s - loss: 3.0736 - accuracy: 0.184 - ETA: 47s - loss: 3.0735 - accuracy: 0.184 - ETA: 46s - loss: 3.0725 - accuracy: 0.184 - ETA: 45s - loss: 3.0722 - accuracy: 0.184 - ETA: 44s - loss: 3.0721 - accuracy: 0.184 - ETA: 43s - loss: 3.0727 - accuracy: 0.184 - ETA: 42s - loss: 3.0723 - accuracy: 0.184 - ETA: 41s - loss: 3.0718 - accuracy: 0.184 - ETA: 40s - loss: 3.0719 - accuracy: 0.185 - ETA: 40s - loss: 3.0717 - accuracy: 0.184 - ETA: 39s - loss: 3.0716 - accuracy: 0.184 - ETA: 38s - loss: 3.0716 - accuracy: 0.184 - ETA: 37s - loss: 3.0718 - accuracy: 0.184 - ETA: 36s - loss: 3.0715 - accuracy: 0.184 - ETA: 35s - loss: 3.0710 - accuracy: 0.184 - ETA: 34s - loss: 3.0714 - accuracy: 0.185 - ETA: 33s - loss: 3.0717 - accuracy: 0.184 - ETA: 32s - loss: 3.0712 - accuracy: 0.184 - ETA: 31s - loss: 3.0708 - accuracy: 0.185 - ETA: 30s - loss: 3.0709 - accuracy: 0.185 - ETA: 29s - loss: 3.0709 - accuracy: 0.185 - ETA: 28s - loss: 3.0709 - accuracy: 0.185 - ETA: 27s - loss: 3.0709 - accuracy: 0.185 - ETA: 26s - loss: 3.0711 - accuracy: 0.185 - ETA: 25s - loss: 3.0706 - accuracy: 0.185 - ETA: 24s - loss: 3.0705 - accuracy: 0.185 - ETA: 23s - loss: 3.0703 - accuracy: 0.185 - ETA: 22s - loss: 3.0708 - accuracy: 0.185 - ETA: 21s - loss: 3.0700 - accuracy: 0.185 - ETA: 21s - loss: 3.0698 - accuracy: 0.185 - ETA: 20s - loss: 3.0701 - accuracy: 0.185 - ETA: 19s - loss: 3.0701 - accuracy: 0.185 - ETA: 18s - loss: 3.0699 - accuracy: 0.185 - ETA: 17s - loss: 3.0694 - accuracy: 0.185 - ETA: 16s - loss: 3.0696 - accuracy: 0.185 - ETA: 15s - loss: 3.0689 - accuracy: 0.185 - ETA: 14s - loss: 3.0690 - accuracy: 0.185 - ETA: 13s - loss: 3.0689 - accuracy: 0.185 - ETA: 12s - loss: 3.0692 - accuracy: 0.185 - ETA: 11s - loss: 3.0691 - accuracy: 0.185 - ETA: 10s - loss: 3.0688 - accuracy: 0.185 - ETA: 9s - loss: 3.0685 - accuracy: 0.185 - ETA: 8s - loss: 3.0682 - accuracy: 0.18 - ETA: 7s - loss: 3.0673 - accuracy: 0.18 - ETA: 6s - loss: 3.0673 - accuracy: 0.18 - ETA: 5s - loss: 3.0674 - accuracy: 0.18 - ETA: 4s - loss: 3.0670 - accuracy: 0.18 - ETA: 3s - loss: 3.0667 - accuracy: 0.18 - ETA: 2s - loss: 3.0674 - accuracy: 0.18 - ETA: 1s - loss: 3.0674 - accuracy: 0.18 - ETA: 1s - loss: 3.0669 - accuracy: 0.18 - ETA: 0s - loss: 3.0666 - accuracy: 0.18 - 336s 8ms/step - loss: 3.0666 - accuracy: 0.1855 - val_loss: 4.0505 - val_accuracy: 0.0348\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:02 - loss: 2.9300 - accuracy: 0.24 - ETA: 5:16 - loss: 2.9153 - accuracy: 0.24 - ETA: 5:12 - loss: 2.9480 - accuracy: 0.22 - ETA: 5:13 - loss: 3.0157 - accuracy: 0.20 - ETA: 5:13 - loss: 2.9876 - accuracy: 0.21 - ETA: 5:11 - loss: 3.0357 - accuracy: 0.20 - ETA: 5:09 - loss: 3.0403 - accuracy: 0.20 - ETA: 5:08 - loss: 3.0303 - accuracy: 0.20 - ETA: 5:10 - loss: 3.0324 - accuracy: 0.20 - ETA: 5:08 - loss: 3.0427 - accuracy: 0.20 - ETA: 5:06 - loss: 3.0423 - accuracy: 0.20 - ETA: 5:05 - loss: 3.0258 - accuracy: 0.20 - ETA: 5:00 - loss: 3.0346 - accuracy: 0.20 - ETA: 5:01 - loss: 3.0219 - accuracy: 0.20 - ETA: 5:02 - loss: 3.0184 - accuracy: 0.20 - ETA: 5:01 - loss: 3.0231 - accuracy: 0.20 - ETA: 5:01 - loss: 3.0186 - accuracy: 0.20 - ETA: 5:00 - loss: 3.0257 - accuracy: 0.20 - ETA: 4:58 - loss: 3.0264 - accuracy: 0.19 - ETA: 4:57 - loss: 3.0229 - accuracy: 0.20 - ETA: 4:56 - loss: 3.0204 - accuracy: 0.20 - ETA: 4:55 - loss: 3.0227 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0277 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0223 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0296 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0154 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0188 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0210 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0214 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0222 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0372 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0357 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0380 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0388 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0410 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0381 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0353 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0406 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0352 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0343 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0543 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0507 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0540 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0581 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0712 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0760 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0789 - accuracy: 0.18 - ETA: 4:31 - loss: 3.0746 - accuracy: 0.18 - ETA: 4:30 - loss: 3.0739 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0737 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0758 - accuracy: 0.18 - ETA: 4:27 - loss: 3.0716 - accuracy: 0.18 - ETA: 4:26 - loss: 3.0719 - accuracy: 0.18 - ETA: 4:25 - loss: 3.0736 - accuracy: 0.18 - ETA: 4:24 - loss: 3.0696 - accuracy: 0.18 - ETA: 4:23 - loss: 3.0716 - accuracy: 0.18 - ETA: 4:22 - loss: 3.0704 - accuracy: 0.18 - ETA: 4:21 - loss: 3.0677 - accuracy: 0.18 - ETA: 4:20 - loss: 3.0665 - accuracy: 0.18 - ETA: 4:19 - loss: 3.0642 - accuracy: 0.18 - ETA: 4:18 - loss: 3.0639 - accuracy: 0.18 - ETA: 4:17 - loss: 3.0641 - accuracy: 0.18 - ETA: 4:17 - loss: 3.0635 - accuracy: 0.18 - ETA: 4:16 - loss: 3.0607 - accuracy: 0.18 - ETA: 4:15 - loss: 3.0601 - accuracy: 0.18 - ETA: 4:14 - loss: 3.0600 - accuracy: 0.18 - ETA: 4:13 - loss: 3.0606 - accuracy: 0.18 - ETA: 4:12 - loss: 3.0582 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0578 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0584 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0563 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0575 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0552 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0545 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0540 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0540 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0543 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0571 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0578 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0580 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0564 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0566 - accuracy: 0.18 - ETA: 3:57 - loss: 3.0572 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0589 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0587 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0584 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0579 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0591 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0586 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0581 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0574 - accuracy: 0.18 - ETA: 3:48 - loss: 3.0572 - accuracy: 0.18 - ETA: 3:47 - loss: 3.0627 - accuracy: 0.18 - ETA: 3:46 - loss: 3.0647 - accuracy: 0.18 - ETA: 3:45 - loss: 3.0639 - accuracy: 0.18 - ETA: 3:44 - loss: 3.0615 - accuracy: 0.18 - ETA: 3:44 - loss: 3.0617 - accuracy: 0.18 - ETA: 3:42 - loss: 3.0619 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0635 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0640 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0647 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0641 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0644 - accuracy: 0.18 - ETA: 3:37 - loss: 3.0631 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0635 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0635 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0628 - accuracy: 0.18 - ETA: 3:33 - loss: 3.0626 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0632 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0606 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0602 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0619 - accuracy: 0.18 - ETA: 3:28 - loss: 3.0623 - accuracy: 0.18 - ETA: 3:27 - loss: 3.0627 - accuracy: 0.18 - ETA: 3:26 - loss: 3.0624 - accuracy: 0.18 - ETA: 3:25 - loss: 3.0638 - accuracy: 0.18 - ETA: 3:24 - loss: 3.0633 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0621 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0630 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0620 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0637 - accuracy: 0.18 - ETA: 3:19 - loss: 3.0629 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0632 - accuracy: 0.18 - ETA: 3:17 - loss: 3.0623 - accuracy: 0.18 - ETA: 3:16 - loss: 3.0622 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0632 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0634 - accuracy: 0.18 - ETA: 3:13 - loss: 3.0643 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0648 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0655 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0663 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0663 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0674 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0681 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0675 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0671 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0677 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0669 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0664 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0671 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0676 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0676 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0680 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0678 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0688 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0687 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0693 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0688 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0697 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0721 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0719 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0726 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0734 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0739 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0755 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0759 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0754 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0765 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0764 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0762 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0765 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0753 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0765 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0772 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0772 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0772 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0774 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0793 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0792 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0788 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0786 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0775 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0777 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0778 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0781 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0765 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0762 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0771 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0780 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0775 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0776 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0775 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0777 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0782 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0776 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0765 - accuracy: 0.1836"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0762 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0770 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0772 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0779 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0774 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0782 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0770 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0779 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0786 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0789 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0793 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0790 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0801 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0794 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0797 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0805 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0808 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0815 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0810 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0810 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0811 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0815 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0810 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0812 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0803 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0820 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0823 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0834 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0844 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0849 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0850 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0852 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0847 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0845 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0841 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0842 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0845 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0843 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0838 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0835 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0836 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0834 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0830 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0830 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0828 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0832 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0828 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0821 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0821 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0825 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0819 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0812 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0815 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0813 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0811 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0805 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0801 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0807 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0812 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0814 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0816 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0820 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0818 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0815 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0817 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0819 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0822 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0830 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0833 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0836 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0838 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0838 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0844 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0844 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0846 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0838 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0841 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0841 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0838 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0838 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0837 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0839 - accuracy: 0.18 - ETA: 59s - loss: 3.0843 - accuracy: 0.1820 - ETA: 58s - loss: 3.0838 - accuracy: 0.182 - ETA: 57s - loss: 3.0836 - accuracy: 0.182 - ETA: 56s - loss: 3.0837 - accuracy: 0.182 - ETA: 55s - loss: 3.0826 - accuracy: 0.182 - ETA: 54s - loss: 3.0820 - accuracy: 0.182 - ETA: 53s - loss: 3.0817 - accuracy: 0.182 - ETA: 52s - loss: 3.0824 - accuracy: 0.181 - ETA: 51s - loss: 3.0821 - accuracy: 0.181 - ETA: 50s - loss: 3.0824 - accuracy: 0.181 - ETA: 49s - loss: 3.0822 - accuracy: 0.181 - ETA: 48s - loss: 3.0820 - accuracy: 0.181 - ETA: 47s - loss: 3.0816 - accuracy: 0.182 - ETA: 46s - loss: 3.0810 - accuracy: 0.182 - ETA: 45s - loss: 3.0810 - accuracy: 0.182 - ETA: 44s - loss: 3.0812 - accuracy: 0.182 - ETA: 43s - loss: 3.0810 - accuracy: 0.182 - ETA: 42s - loss: 3.0812 - accuracy: 0.182 - ETA: 42s - loss: 3.0815 - accuracy: 0.182 - ETA: 41s - loss: 3.0815 - accuracy: 0.182 - ETA: 40s - loss: 3.0807 - accuracy: 0.182 - ETA: 39s - loss: 3.0813 - accuracy: 0.182 - ETA: 38s - loss: 3.0816 - accuracy: 0.182 - ETA: 37s - loss: 3.0816 - accuracy: 0.182 - ETA: 36s - loss: 3.0808 - accuracy: 0.182 - ETA: 35s - loss: 3.0803 - accuracy: 0.182 - ETA: 34s - loss: 3.0806 - accuracy: 0.182 - ETA: 33s - loss: 3.0803 - accuracy: 0.182 - ETA: 32s - loss: 3.0806 - accuracy: 0.182 - ETA: 31s - loss: 3.0810 - accuracy: 0.181 - ETA: 30s - loss: 3.0809 - accuracy: 0.181 - ETA: 29s - loss: 3.0809 - accuracy: 0.181 - ETA: 28s - loss: 3.0813 - accuracy: 0.181 - ETA: 27s - loss: 3.0816 - accuracy: 0.181 - ETA: 26s - loss: 3.0813 - accuracy: 0.181 - ETA: 25s - loss: 3.0819 - accuracy: 0.181 - ETA: 24s - loss: 3.0818 - accuracy: 0.181 - ETA: 23s - loss: 3.0815 - accuracy: 0.181 - ETA: 22s - loss: 3.0812 - accuracy: 0.181 - ETA: 22s - loss: 3.0819 - accuracy: 0.181 - ETA: 21s - loss: 3.0816 - accuracy: 0.181 - ETA: 20s - loss: 3.0817 - accuracy: 0.181 - ETA: 19s - loss: 3.0808 - accuracy: 0.181 - ETA: 18s - loss: 3.0830 - accuracy: 0.181 - ETA: 17s - loss: 3.0831 - accuracy: 0.181 - ETA: 16s - loss: 3.0829 - accuracy: 0.181 - ETA: 15s - loss: 3.0828 - accuracy: 0.181 - ETA: 14s - loss: 3.0830 - accuracy: 0.181 - ETA: 13s - loss: 3.0832 - accuracy: 0.181 - ETA: 12s - loss: 3.0832 - accuracy: 0.181 - ETA: 11s - loss: 3.0836 - accuracy: 0.181 - ETA: 10s - loss: 3.0831 - accuracy: 0.181 - ETA: 9s - loss: 3.0833 - accuracy: 0.181 - ETA: 8s - loss: 3.0829 - accuracy: 0.18 - ETA: 7s - loss: 3.0832 - accuracy: 0.18 - ETA: 6s - loss: 3.0839 - accuracy: 0.18 - ETA: 5s - loss: 3.0838 - accuracy: 0.18 - ETA: 4s - loss: 3.0837 - accuracy: 0.18 - ETA: 3s - loss: 3.0833 - accuracy: 0.18 - ETA: 2s - loss: 3.0831 - accuracy: 0.18 - ETA: 1s - loss: 3.0830 - accuracy: 0.18 - ETA: 1s - loss: 3.0831 - accuracy: 0.18 - ETA: 0s - loss: 3.0831 - accuracy: 0.18 - 339s 8ms/step - loss: 3.0829 - accuracy: 0.1815 - val_loss: 4.0353 - val_accuracy: 0.0362\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:08 - loss: 2.9646 - accuracy: 0.24 - ETA: 5:22 - loss: 3.0738 - accuracy: 0.19 - ETA: 5:27 - loss: 3.1252 - accuracy: 0.16 - ETA: 5:28 - loss: 3.0919 - accuracy: 0.18 - ETA: 5:31 - loss: 3.1291 - accuracy: 0.17 - ETA: 5:31 - loss: 3.1255 - accuracy: 0.16 - ETA: 5:27 - loss: 3.1556 - accuracy: 0.16 - ETA: 5:25 - loss: 3.1538 - accuracy: 0.16 - ETA: 5:21 - loss: 3.1780 - accuracy: 0.15 - ETA: 5:18 - loss: 3.1749 - accuracy: 0.16 - ETA: 5:17 - loss: 3.1698 - accuracy: 0.16 - ETA: 5:16 - loss: 3.1690 - accuracy: 0.16 - ETA: 5:15 - loss: 3.1559 - accuracy: 0.16 - ETA: 5:12 - loss: 3.1707 - accuracy: 0.16 - ETA: 5:10 - loss: 3.1589 - accuracy: 0.16 - ETA: 5:08 - loss: 3.1557 - accuracy: 0.16 - ETA: 5:06 - loss: 3.1636 - accuracy: 0.16 - ETA: 5:04 - loss: 3.1683 - accuracy: 0.16 - ETA: 5:02 - loss: 3.1655 - accuracy: 0.16 - ETA: 5:00 - loss: 3.1665 - accuracy: 0.16 - ETA: 4:58 - loss: 3.1678 - accuracy: 0.16 - ETA: 4:57 - loss: 3.1688 - accuracy: 0.16 - ETA: 4:55 - loss: 3.1639 - accuracy: 0.16 - ETA: 4:55 - loss: 3.1671 - accuracy: 0.16 - ETA: 4:53 - loss: 3.1711 - accuracy: 0.16 - ETA: 4:52 - loss: 3.1636 - accuracy: 0.16 - ETA: 4:50 - loss: 3.1648 - accuracy: 0.16 - ETA: 4:50 - loss: 3.1639 - accuracy: 0.15 - ETA: 4:49 - loss: 3.1616 - accuracy: 0.16 - ETA: 4:49 - loss: 3.1580 - accuracy: 0.16 - ETA: 4:47 - loss: 3.1552 - accuracy: 0.16 - ETA: 4:46 - loss: 3.1539 - accuracy: 0.16 - ETA: 4:45 - loss: 3.1535 - accuracy: 0.16 - ETA: 4:44 - loss: 3.1491 - accuracy: 0.16 - ETA: 4:43 - loss: 3.1505 - accuracy: 0.16 - ETA: 4:42 - loss: 3.1521 - accuracy: 0.16 - ETA: 4:41 - loss: 3.1510 - accuracy: 0.16 - ETA: 4:40 - loss: 3.1479 - accuracy: 0.16 - ETA: 4:39 - loss: 3.1458 - accuracy: 0.16 - ETA: 4:38 - loss: 3.1417 - accuracy: 0.16 - ETA: 4:36 - loss: 3.1423 - accuracy: 0.16 - ETA: 4:35 - loss: 3.1384 - accuracy: 0.16 - ETA: 4:34 - loss: 3.1350 - accuracy: 0.16 - ETA: 4:33 - loss: 3.1347 - accuracy: 0.16 - ETA: 4:32 - loss: 3.1349 - accuracy: 0.16 - ETA: 4:31 - loss: 3.1299 - accuracy: 0.16 - ETA: 4:30 - loss: 3.1318 - accuracy: 0.16 - ETA: 4:29 - loss: 3.1306 - accuracy: 0.16 - ETA: 4:28 - loss: 3.1333 - accuracy: 0.16 - ETA: 4:27 - loss: 3.1330 - accuracy: 0.16 - ETA: 4:26 - loss: 3.1245 - accuracy: 0.16 - ETA: 4:25 - loss: 3.1235 - accuracy: 0.16 - ETA: 4:24 - loss: 3.1193 - accuracy: 0.16 - ETA: 4:22 - loss: 3.1172 - accuracy: 0.16 - ETA: 4:21 - loss: 3.1188 - accuracy: 0.16 - ETA: 4:21 - loss: 3.1188 - accuracy: 0.16 - ETA: 4:20 - loss: 3.1190 - accuracy: 0.16 - ETA: 4:19 - loss: 3.1182 - accuracy: 0.16 - ETA: 4:18 - loss: 3.1154 - accuracy: 0.16 - ETA: 4:17 - loss: 3.1152 - accuracy: 0.16 - ETA: 4:16 - loss: 3.1126 - accuracy: 0.16 - ETA: 4:15 - loss: 3.1100 - accuracy: 0.17 - ETA: 4:14 - loss: 3.1117 - accuracy: 0.16 - ETA: 4:13 - loss: 3.1120 - accuracy: 0.16 - ETA: 4:12 - loss: 3.1116 - accuracy: 0.16 - ETA: 4:11 - loss: 3.1105 - accuracy: 0.16 - ETA: 4:10 - loss: 3.1131 - accuracy: 0.16 - ETA: 4:09 - loss: 3.1116 - accuracy: 0.16 - ETA: 4:08 - loss: 3.1107 - accuracy: 0.16 - ETA: 4:07 - loss: 3.1107 - accuracy: 0.16 - ETA: 4:06 - loss: 3.1090 - accuracy: 0.16 - ETA: 4:05 - loss: 3.1086 - accuracy: 0.17 - ETA: 4:04 - loss: 3.1078 - accuracy: 0.16 - ETA: 4:04 - loss: 3.1082 - accuracy: 0.16 - ETA: 4:03 - loss: 3.1079 - accuracy: 0.16 - ETA: 4:02 - loss: 3.1067 - accuracy: 0.17 - ETA: 4:01 - loss: 3.1077 - accuracy: 0.17 - ETA: 4:00 - loss: 3.1126 - accuracy: 0.17 - ETA: 3:59 - loss: 3.1148 - accuracy: 0.17 - ETA: 3:58 - loss: 3.1190 - accuracy: 0.17 - ETA: 3:57 - loss: 3.1170 - accuracy: 0.17 - ETA: 3:56 - loss: 3.1191 - accuracy: 0.17 - ETA: 3:55 - loss: 3.1206 - accuracy: 0.17 - ETA: 3:54 - loss: 3.1199 - accuracy: 0.17 - ETA: 3:53 - loss: 3.1187 - accuracy: 0.17 - ETA: 3:52 - loss: 3.1162 - accuracy: 0.17 - ETA: 3:51 - loss: 3.1183 - accuracy: 0.17 - ETA: 3:50 - loss: 3.1180 - accuracy: 0.17 - ETA: 3:49 - loss: 3.1178 - accuracy: 0.17 - ETA: 3:48 - loss: 3.1206 - accuracy: 0.17 - ETA: 3:47 - loss: 3.1202 - accuracy: 0.17 - ETA: 3:46 - loss: 3.1224 - accuracy: 0.17 - ETA: 3:45 - loss: 3.1251 - accuracy: 0.17 - ETA: 3:44 - loss: 3.1249 - accuracy: 0.17 - ETA: 3:44 - loss: 3.1263 - accuracy: 0.17 - ETA: 3:43 - loss: 3.1263 - accuracy: 0.17 - ETA: 3:42 - loss: 3.1257 - accuracy: 0.17 - ETA: 3:41 - loss: 3.1245 - accuracy: 0.17 - ETA: 3:40 - loss: 3.1247 - accuracy: 0.17 - ETA: 3:39 - loss: 3.1242 - accuracy: 0.17 - ETA: 3:38 - loss: 3.1251 - accuracy: 0.17 - ETA: 3:37 - loss: 3.1254 - accuracy: 0.17 - ETA: 3:36 - loss: 3.1253 - accuracy: 0.17 - ETA: 3:35 - loss: 3.1271 - accuracy: 0.17 - ETA: 3:34 - loss: 3.1245 - accuracy: 0.17 - ETA: 3:33 - loss: 3.1230 - accuracy: 0.17 - ETA: 3:32 - loss: 3.1224 - accuracy: 0.17 - ETA: 3:31 - loss: 3.1227 - accuracy: 0.17 - ETA: 3:30 - loss: 3.1237 - accuracy: 0.17 - ETA: 3:29 - loss: 3.1255 - accuracy: 0.17 - ETA: 3:28 - loss: 3.1249 - accuracy: 0.17 - ETA: 3:28 - loss: 3.1229 - accuracy: 0.17 - ETA: 3:27 - loss: 3.1239 - accuracy: 0.17 - ETA: 3:26 - loss: 3.1243 - accuracy: 0.17 - ETA: 3:25 - loss: 3.1238 - accuracy: 0.17 - ETA: 3:24 - loss: 3.1257 - accuracy: 0.17 - ETA: 3:23 - loss: 3.1249 - accuracy: 0.17 - ETA: 3:22 - loss: 3.1266 - accuracy: 0.17 - ETA: 3:21 - loss: 3.1265 - accuracy: 0.17 - ETA: 3:20 - loss: 3.1280 - accuracy: 0.17 - ETA: 3:19 - loss: 3.1281 - accuracy: 0.17 - ETA: 3:18 - loss: 3.1296 - accuracy: 0.17 - ETA: 3:17 - loss: 3.1296 - accuracy: 0.17 - ETA: 3:16 - loss: 3.1320 - accuracy: 0.17 - ETA: 3:16 - loss: 3.1325 - accuracy: 0.17 - ETA: 3:15 - loss: 3.1322 - accuracy: 0.17 - ETA: 3:14 - loss: 3.1333 - accuracy: 0.16 - ETA: 3:13 - loss: 3.1342 - accuracy: 0.16 - ETA: 3:12 - loss: 3.1357 - accuracy: 0.16 - ETA: 3:11 - loss: 3.1347 - accuracy: 0.16 - ETA: 3:10 - loss: 3.1337 - accuracy: 0.16 - ETA: 3:09 - loss: 3.1332 - accuracy: 0.16 - ETA: 3:08 - loss: 3.1346 - accuracy: 0.16 - ETA: 3:07 - loss: 3.1348 - accuracy: 0.16 - ETA: 3:06 - loss: 3.1358 - accuracy: 0.16 - ETA: 3:05 - loss: 3.1355 - accuracy: 0.16 - ETA: 3:04 - loss: 3.1373 - accuracy: 0.16 - ETA: 3:03 - loss: 3.1364 - accuracy: 0.16 - ETA: 3:02 - loss: 3.1352 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1358 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1362 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1390 - accuracy: 0.16 - ETA: 2:59 - loss: 3.1399 - accuracy: 0.16 - ETA: 2:58 - loss: 3.1387 - accuracy: 0.16 - ETA: 2:57 - loss: 3.1386 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1403 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1415 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1417 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1451 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1457 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1460 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1455 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1459 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1465 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1487 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1486 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1507 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1510 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1519 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1525 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1533 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1540 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1550 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1554 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1560 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1558 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1560 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1558 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1569 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1570 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1577 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1570 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1570 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1577 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1585 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1586 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1587 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1592 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1603 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1603 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1603 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1596 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1586 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1595 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1591 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1592 - accuracy: 0.1708"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.1596 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1591 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1590 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1586 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1584 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1581 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1579 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1572 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1568 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1563 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1568 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1572 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1584 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1589 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1585 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1583 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1597 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1590 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1588 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1590 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1586 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1583 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1580 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1585 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1577 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1576 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1568 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1571 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1597 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1632 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1639 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1643 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1635 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1643 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1644 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1642 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1640 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1639 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1646 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1654 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1665 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1712 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1715 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1718 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1719 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1706 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1715 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1721 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1718 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1717 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1715 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1719 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1719 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1717 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1718 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1714 - accuracy: 0.17 - ETA: 1:24 - loss: 3.1720 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1721 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1718 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1743 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1743 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1739 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1743 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1752 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1753 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1752 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1750 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1754 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1757 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1755 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1751 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1744 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1745 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1737 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1742 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1738 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1737 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1747 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1756 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1752 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1755 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1752 - accuracy: 0.17 - ETA: 59s - loss: 3.1743 - accuracy: 0.1707 - ETA: 58s - loss: 3.1737 - accuracy: 0.170 - ETA: 57s - loss: 3.1742 - accuracy: 0.170 - ETA: 56s - loss: 3.1739 - accuracy: 0.170 - ETA: 55s - loss: 3.1739 - accuracy: 0.170 - ETA: 54s - loss: 3.1740 - accuracy: 0.170 - ETA: 53s - loss: 3.1740 - accuracy: 0.170 - ETA: 52s - loss: 3.1742 - accuracy: 0.170 - ETA: 51s - loss: 3.1740 - accuracy: 0.170 - ETA: 50s - loss: 3.1734 - accuracy: 0.170 - ETA: 49s - loss: 3.1734 - accuracy: 0.170 - ETA: 48s - loss: 3.1725 - accuracy: 0.170 - ETA: 47s - loss: 3.1719 - accuracy: 0.170 - ETA: 46s - loss: 3.1716 - accuracy: 0.170 - ETA: 45s - loss: 3.1717 - accuracy: 0.170 - ETA: 44s - loss: 3.1716 - accuracy: 0.170 - ETA: 43s - loss: 3.1720 - accuracy: 0.170 - ETA: 43s - loss: 3.1729 - accuracy: 0.170 - ETA: 42s - loss: 3.1722 - accuracy: 0.170 - ETA: 41s - loss: 3.1725 - accuracy: 0.170 - ETA: 40s - loss: 3.1716 - accuracy: 0.170 - ETA: 39s - loss: 3.1719 - accuracy: 0.170 - ETA: 38s - loss: 3.1720 - accuracy: 0.170 - ETA: 37s - loss: 3.1724 - accuracy: 0.170 - ETA: 36s - loss: 3.1729 - accuracy: 0.170 - ETA: 35s - loss: 3.1724 - accuracy: 0.170 - ETA: 34s - loss: 3.1720 - accuracy: 0.170 - ETA: 33s - loss: 3.1717 - accuracy: 0.170 - ETA: 32s - loss: 3.1722 - accuracy: 0.170 - ETA: 31s - loss: 3.1727 - accuracy: 0.170 - ETA: 30s - loss: 3.1723 - accuracy: 0.170 - ETA: 29s - loss: 3.1720 - accuracy: 0.170 - ETA: 28s - loss: 3.1714 - accuracy: 0.170 - ETA: 27s - loss: 3.1708 - accuracy: 0.170 - ETA: 26s - loss: 3.1709 - accuracy: 0.170 - ETA: 25s - loss: 3.1704 - accuracy: 0.170 - ETA: 24s - loss: 3.1704 - accuracy: 0.170 - ETA: 23s - loss: 3.1702 - accuracy: 0.170 - ETA: 22s - loss: 3.1700 - accuracy: 0.170 - ETA: 22s - loss: 3.1702 - accuracy: 0.170 - ETA: 21s - loss: 3.1697 - accuracy: 0.170 - ETA: 20s - loss: 3.1694 - accuracy: 0.170 - ETA: 19s - loss: 3.1692 - accuracy: 0.170 - ETA: 18s - loss: 3.1691 - accuracy: 0.170 - ETA: 17s - loss: 3.1692 - accuracy: 0.170 - ETA: 16s - loss: 3.1694 - accuracy: 0.170 - ETA: 15s - loss: 3.1692 - accuracy: 0.170 - ETA: 14s - loss: 3.1685 - accuracy: 0.170 - ETA: 13s - loss: 3.1680 - accuracy: 0.170 - ETA: 12s - loss: 3.1681 - accuracy: 0.170 - ETA: 11s - loss: 3.1674 - accuracy: 0.170 - ETA: 10s - loss: 3.1674 - accuracy: 0.170 - ETA: 9s - loss: 3.1669 - accuracy: 0.170 - ETA: 8s - loss: 3.1670 - accuracy: 0.17 - ETA: 7s - loss: 3.1661 - accuracy: 0.17 - ETA: 6s - loss: 3.1655 - accuracy: 0.17 - ETA: 5s - loss: 3.1648 - accuracy: 0.17 - ETA: 4s - loss: 3.1648 - accuracy: 0.17 - ETA: 3s - loss: 3.1642 - accuracy: 0.17 - ETA: 2s - loss: 3.1644 - accuracy: 0.17 - ETA: 1s - loss: 3.1641 - accuracy: 0.17 - ETA: 1s - loss: 3.1641 - accuracy: 0.17 - ETA: 0s - loss: 3.1641 - accuracy: 0.17 - 339s 8ms/step - loss: 3.1641 - accuracy: 0.1708 - val_loss: 4.0826 - val_accuracy: 0.0351\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:02 - loss: 3.2531 - accuracy: 0.12 - ETA: 5:12 - loss: 3.1243 - accuracy: 0.15 - ETA: 5:05 - loss: 3.0831 - accuracy: 0.16 - ETA: 5:04 - loss: 3.0962 - accuracy: 0.16 - ETA: 5:03 - loss: 3.1347 - accuracy: 0.16 - ETA: 5:01 - loss: 3.1004 - accuracy: 0.17 - ETA: 5:00 - loss: 3.1056 - accuracy: 0.17 - ETA: 4:58 - loss: 3.0718 - accuracy: 0.18 - ETA: 5:00 - loss: 3.0400 - accuracy: 0.18 - ETA: 5:01 - loss: 3.0444 - accuracy: 0.18 - ETA: 4:59 - loss: 3.0348 - accuracy: 0.19 - ETA: 4:59 - loss: 3.0304 - accuracy: 0.19 - ETA: 4:59 - loss: 3.0315 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0343 - accuracy: 0.19 - ETA: 4:57 - loss: 3.0451 - accuracy: 0.19 - ETA: 4:57 - loss: 3.0564 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0613 - accuracy: 0.19 - ETA: 4:55 - loss: 3.0626 - accuracy: 0.18 - ETA: 4:55 - loss: 3.0677 - accuracy: 0.18 - ETA: 4:54 - loss: 3.0568 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0485 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0538 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0523 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0514 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0589 - accuracy: 0.18 - ETA: 4:47 - loss: 3.0547 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0578 - accuracy: 0.18 - ETA: 4:46 - loss: 3.0643 - accuracy: 0.18 - ETA: 4:45 - loss: 3.0609 - accuracy: 0.18 - ETA: 4:44 - loss: 3.0572 - accuracy: 0.18 - ETA: 4:43 - loss: 3.0609 - accuracy: 0.18 - ETA: 4:41 - loss: 3.0678 - accuracy: 0.18 - ETA: 4:41 - loss: 3.0734 - accuracy: 0.18 - ETA: 4:39 - loss: 3.0676 - accuracy: 0.18 - ETA: 4:39 - loss: 3.0667 - accuracy: 0.18 - ETA: 4:38 - loss: 3.0641 - accuracy: 0.18 - ETA: 4:37 - loss: 3.0625 - accuracy: 0.18 - ETA: 4:36 - loss: 3.0529 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0467 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0452 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0433 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0417 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0437 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0408 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0501 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0522 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0501 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0544 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0548 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0574 - accuracy: 0.18 - ETA: 4:24 - loss: 3.0558 - accuracy: 0.18 - ETA: 4:23 - loss: 3.0582 - accuracy: 0.18 - ETA: 4:22 - loss: 3.0603 - accuracy: 0.18 - ETA: 4:21 - loss: 3.0626 - accuracy: 0.18 - ETA: 4:20 - loss: 3.0620 - accuracy: 0.18 - ETA: 4:19 - loss: 3.0648 - accuracy: 0.18 - ETA: 4:18 - loss: 3.0670 - accuracy: 0.18 - ETA: 4:17 - loss: 3.0677 - accuracy: 0.18 - ETA: 4:16 - loss: 3.0664 - accuracy: 0.18 - ETA: 4:15 - loss: 3.0665 - accuracy: 0.18 - ETA: 4:15 - loss: 3.0672 - accuracy: 0.18 - ETA: 4:14 - loss: 3.0670 - accuracy: 0.18 - ETA: 4:14 - loss: 3.0705 - accuracy: 0.18 - ETA: 4:13 - loss: 3.0679 - accuracy: 0.18 - ETA: 4:12 - loss: 3.0685 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0679 - accuracy: 0.18 - ETA: 4:10 - loss: 3.0705 - accuracy: 0.18 - ETA: 4:09 - loss: 3.0686 - accuracy: 0.18 - ETA: 4:08 - loss: 3.0680 - accuracy: 0.18 - ETA: 4:07 - loss: 3.0689 - accuracy: 0.18 - ETA: 4:06 - loss: 3.0682 - accuracy: 0.18 - ETA: 4:06 - loss: 3.0670 - accuracy: 0.18 - ETA: 4:05 - loss: 3.0698 - accuracy: 0.18 - ETA: 4:04 - loss: 3.0680 - accuracy: 0.18 - ETA: 4:03 - loss: 3.0671 - accuracy: 0.18 - ETA: 4:01 - loss: 3.0634 - accuracy: 0.18 - ETA: 4:01 - loss: 3.0625 - accuracy: 0.18 - ETA: 4:00 - loss: 3.0623 - accuracy: 0.18 - ETA: 3:59 - loss: 3.0642 - accuracy: 0.18 - ETA: 3:58 - loss: 3.0647 - accuracy: 0.18 - ETA: 3:57 - loss: 3.0643 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0651 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0601 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0618 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0621 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0608 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0624 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0634 - accuracy: 0.18 - ETA: 3:49 - loss: 3.0618 - accuracy: 0.18 - ETA: 3:48 - loss: 3.0597 - accuracy: 0.18 - ETA: 3:47 - loss: 3.0587 - accuracy: 0.18 - ETA: 3:46 - loss: 3.0578 - accuracy: 0.18 - ETA: 3:45 - loss: 3.0573 - accuracy: 0.18 - ETA: 3:44 - loss: 3.0558 - accuracy: 0.18 - ETA: 3:43 - loss: 3.0557 - accuracy: 0.18 - ETA: 3:43 - loss: 3.0551 - accuracy: 0.18 - ETA: 3:42 - loss: 3.0551 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0551 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0551 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0537 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0521 - accuracy: 0.18 - ETA: 3:37 - loss: 3.0530 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0531 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0543 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0546 - accuracy: 0.18 - ETA: 3:33 - loss: 3.0552 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0555 - accuracy: 0.18 - ETA: 3:31 - loss: 3.0553 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0553 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0556 - accuracy: 0.18 - ETA: 3:28 - loss: 3.0579 - accuracy: 0.18 - ETA: 3:27 - loss: 3.0585 - accuracy: 0.18 - ETA: 3:26 - loss: 3.0575 - accuracy: 0.18 - ETA: 3:25 - loss: 3.0583 - accuracy: 0.18 - ETA: 3:24 - loss: 3.0587 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0596 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0597 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0598 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0591 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0588 - accuracy: 0.18 - ETA: 3:19 - loss: 3.0595 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0601 - accuracy: 0.18 - ETA: 3:17 - loss: 3.0616 - accuracy: 0.18 - ETA: 3:16 - loss: 3.0602 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0617 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0616 - accuracy: 0.18 - ETA: 3:13 - loss: 3.0622 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0624 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0617 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0590 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0610 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0599 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0594 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0596 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0596 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0594 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0590 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0582 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0583 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0568 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0568 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0555 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0533 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0517 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0518 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0515 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0514 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0496 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0493 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0499 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0514 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0516 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0513 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0513 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0520 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0517 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0511 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0512 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0506 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0510 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0514 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0509 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0502 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0498 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0501 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0495 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0501 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0489 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0487 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0484 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0474 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0474 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0475 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0475 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0472 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0479 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0487 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0490 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0494 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0485 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0485 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0482 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0482 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0475 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0475 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0481 - accuracy: 0.1890"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0478 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0486 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0482 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0473 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0465 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0474 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0469 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0474 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0474 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0466 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0461 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0460 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0448 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0451 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0464 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0463 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0458 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0462 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0455 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0463 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0476 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0471 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0473 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0478 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0483 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0478 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0481 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0482 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0486 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0485 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0494 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0491 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0486 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0487 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0492 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0491 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0494 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0487 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0485 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0478 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0479 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0484 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0486 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0475 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0481 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0476 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0475 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0472 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0472 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0467 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0464 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0459 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0456 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0453 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0445 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0449 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0447 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0450 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0460 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0466 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0457 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0464 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0484 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0481 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0477 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0475 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0470 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0465 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0468 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0468 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0462 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0465 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0466 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0468 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0468 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0475 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0479 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0480 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0478 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0470 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0469 - accuracy: 0.18 - ETA: 59s - loss: 3.0472 - accuracy: 0.1879 - ETA: 59s - loss: 3.0469 - accuracy: 0.187 - ETA: 58s - loss: 3.0470 - accuracy: 0.187 - ETA: 57s - loss: 3.0468 - accuracy: 0.188 - ETA: 56s - loss: 3.0460 - accuracy: 0.188 - ETA: 55s - loss: 3.0458 - accuracy: 0.188 - ETA: 54s - loss: 3.0459 - accuracy: 0.188 - ETA: 53s - loss: 3.0470 - accuracy: 0.188 - ETA: 52s - loss: 3.0473 - accuracy: 0.187 - ETA: 51s - loss: 3.0481 - accuracy: 0.187 - ETA: 50s - loss: 3.0477 - accuracy: 0.187 - ETA: 49s - loss: 3.0480 - accuracy: 0.187 - ETA: 48s - loss: 3.0476 - accuracy: 0.187 - ETA: 47s - loss: 3.0473 - accuracy: 0.187 - ETA: 46s - loss: 3.0472 - accuracy: 0.187 - ETA: 45s - loss: 3.0466 - accuracy: 0.187 - ETA: 44s - loss: 3.0463 - accuracy: 0.187 - ETA: 43s - loss: 3.0459 - accuracy: 0.187 - ETA: 42s - loss: 3.0459 - accuracy: 0.187 - ETA: 41s - loss: 3.0461 - accuracy: 0.187 - ETA: 40s - loss: 3.0460 - accuracy: 0.187 - ETA: 40s - loss: 3.0457 - accuracy: 0.187 - ETA: 39s - loss: 3.0464 - accuracy: 0.187 - ETA: 38s - loss: 3.0459 - accuracy: 0.187 - ETA: 37s - loss: 3.0490 - accuracy: 0.187 - ETA: 36s - loss: 3.0484 - accuracy: 0.187 - ETA: 35s - loss: 3.0483 - accuracy: 0.187 - ETA: 34s - loss: 3.0481 - accuracy: 0.187 - ETA: 33s - loss: 3.0483 - accuracy: 0.187 - ETA: 32s - loss: 3.0486 - accuracy: 0.187 - ETA: 31s - loss: 3.0486 - accuracy: 0.187 - ETA: 30s - loss: 3.0488 - accuracy: 0.187 - ETA: 29s - loss: 3.0492 - accuracy: 0.187 - ETA: 28s - loss: 3.0493 - accuracy: 0.187 - ETA: 27s - loss: 3.0497 - accuracy: 0.187 - ETA: 26s - loss: 3.0496 - accuracy: 0.187 - ETA: 25s - loss: 3.0497 - accuracy: 0.187 - ETA: 24s - loss: 3.0498 - accuracy: 0.187 - ETA: 23s - loss: 3.0503 - accuracy: 0.186 - ETA: 22s - loss: 3.0502 - accuracy: 0.186 - ETA: 21s - loss: 3.0506 - accuracy: 0.186 - ETA: 20s - loss: 3.0507 - accuracy: 0.186 - ETA: 20s - loss: 3.0505 - accuracy: 0.186 - ETA: 19s - loss: 3.0504 - accuracy: 0.187 - ETA: 18s - loss: 3.0505 - accuracy: 0.187 - ETA: 17s - loss: 3.0506 - accuracy: 0.186 - ETA: 16s - loss: 3.0510 - accuracy: 0.186 - ETA: 15s - loss: 3.0519 - accuracy: 0.186 - ETA: 14s - loss: 3.0520 - accuracy: 0.186 - ETA: 13s - loss: 3.0526 - accuracy: 0.186 - ETA: 12s - loss: 3.0523 - accuracy: 0.186 - ETA: 11s - loss: 3.0522 - accuracy: 0.186 - ETA: 10s - loss: 3.0519 - accuracy: 0.186 - ETA: 9s - loss: 3.0522 - accuracy: 0.186 - ETA: 8s - loss: 3.0523 - accuracy: 0.18 - ETA: 7s - loss: 3.0522 - accuracy: 0.18 - ETA: 6s - loss: 3.0529 - accuracy: 0.18 - ETA: 5s - loss: 3.0524 - accuracy: 0.18 - ETA: 4s - loss: 3.0524 - accuracy: 0.18 - ETA: 3s - loss: 3.0531 - accuracy: 0.18 - ETA: 2s - loss: 3.0532 - accuracy: 0.18 - ETA: 1s - loss: 3.0532 - accuracy: 0.18 - ETA: 1s - loss: 3.0533 - accuracy: 0.18 - ETA: 0s - loss: 3.0539 - accuracy: 0.18 - 338s 8ms/step - loss: 3.0539 - accuracy: 0.1865 - val_loss: 4.2431 - val_accuracy: 0.0344\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:08 - loss: 2.9235 - accuracy: 0.18 - ETA: 5:08 - loss: 3.0449 - accuracy: 0.18 - ETA: 5:05 - loss: 3.1183 - accuracy: 0.17 - ETA: 5:13 - loss: 3.0880 - accuracy: 0.18 - ETA: 5:09 - loss: 3.0641 - accuracy: 0.18 - ETA: 5:09 - loss: 3.1015 - accuracy: 0.17 - ETA: 5:08 - loss: 3.1187 - accuracy: 0.17 - ETA: 5:06 - loss: 3.1115 - accuracy: 0.17 - ETA: 5:04 - loss: 3.1066 - accuracy: 0.18 - ETA: 5:04 - loss: 3.0990 - accuracy: 0.17 - ETA: 5:04 - loss: 3.0878 - accuracy: 0.18 - ETA: 5:02 - loss: 3.0732 - accuracy: 0.18 - ETA: 5:02 - loss: 3.0628 - accuracy: 0.18 - ETA: 5:01 - loss: 3.0731 - accuracy: 0.18 - ETA: 5:01 - loss: 3.0820 - accuracy: 0.18 - ETA: 5:00 - loss: 3.0774 - accuracy: 0.18 - ETA: 4:59 - loss: 3.0638 - accuracy: 0.18 - ETA: 4:58 - loss: 3.0586 - accuracy: 0.18 - ETA: 4:57 - loss: 3.0538 - accuracy: 0.18 - ETA: 4:55 - loss: 3.0513 - accuracy: 0.19 - ETA: 4:54 - loss: 3.0442 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0451 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0364 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0344 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0335 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0352 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0385 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0410 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0420 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0419 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0451 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0464 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0438 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0413 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0411 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0456 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0387 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0375 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0393 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0444 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0448 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0468 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0496 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0513 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0484 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0465 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0449 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0494 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0497 - accuracy: 0.18 - ETA: 4:27 - loss: 3.0519 - accuracy: 0.18 - ETA: 4:26 - loss: 3.0519 - accuracy: 0.18 - ETA: 4:25 - loss: 3.0481 - accuracy: 0.18 - ETA: 4:25 - loss: 3.0470 - accuracy: 0.18 - ETA: 4:24 - loss: 3.0449 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0458 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0477 - accuracy: 0.18 - ETA: 4:21 - loss: 3.0494 - accuracy: 0.18 - ETA: 4:20 - loss: 3.0475 - accuracy: 0.18 - ETA: 4:19 - loss: 3.0456 - accuracy: 0.18 - ETA: 4:19 - loss: 3.0425 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0433 - accuracy: 0.18 - ETA: 4:17 - loss: 3.0414 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0413 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0412 - accuracy: 0.18 - ETA: 4:13 - loss: 3.0421 - accuracy: 0.18 - ETA: 4:12 - loss: 3.0449 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0421 - accuracy: 0.18 - ETA: 4:10 - loss: 3.0446 - accuracy: 0.18 - ETA: 4:10 - loss: 3.0450 - accuracy: 0.18 - ETA: 4:08 - loss: 3.0421 - accuracy: 0.18 - ETA: 4:07 - loss: 3.0404 - accuracy: 0.18 - ETA: 4:06 - loss: 3.0385 - accuracy: 0.18 - ETA: 4:05 - loss: 3.0373 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0357 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0355 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0354 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0365 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0371 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0369 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0351 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0352 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0374 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0380 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0359 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0364 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0339 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0337 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0346 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0322 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0328 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0329 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0354 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0344 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0339 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0328 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0324 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0307 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0314 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0306 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0299 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0271 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0251 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0261 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0264 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0277 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0275 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0298 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0285 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0283 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0281 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0296 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0289 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0294 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0286 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0288 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0301 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0320 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0316 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0315 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0323 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0323 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0323 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0311 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0336 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0340 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0345 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0352 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0342 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0355 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0350 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0344 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0354 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0364 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0377 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0382 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0379 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0378 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0384 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0389 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0393 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0398 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0401 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0401 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0409 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0415 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0399 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0399 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0474 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0486 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0488 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0470 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0476 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0470 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0458 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0460 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0462 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0468 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0467 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0465 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0467 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0458 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0461 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0450 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0440 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0425 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0422 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0420 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0419 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0413 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0410 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0415 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0510 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0510 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0508 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0508 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0498 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0504 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0503 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0509 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0515 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0527 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0539 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0541 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0549 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0561 - accuracy: 0.19 - ETA: 2:17 - loss: 3.0565 - accuracy: 0.1902"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0561 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0561 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0560 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0569 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0581 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0586 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0601 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0602 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0589 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0596 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0604 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0617 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0618 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0620 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0619 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0618 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0614 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0621 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0639 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0642 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0646 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0639 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0637 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0645 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0659 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0655 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0650 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0655 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0660 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0662 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0662 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0648 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0648 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0641 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0634 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0636 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0642 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0642 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0647 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0642 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0647 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0655 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0653 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0653 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0654 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0653 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0651 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0650 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0652 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0650 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0655 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0649 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0649 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0663 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0675 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0682 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0675 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0688 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0684 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0687 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0688 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0687 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0696 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0696 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0700 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0703 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0710 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0717 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0718 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0718 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0726 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0726 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0734 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0741 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0739 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0749 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0755 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0752 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0755 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0759 - accuracy: 0.18 - ETA: 59s - loss: 3.0768 - accuracy: 0.1852 - ETA: 58s - loss: 3.0775 - accuracy: 0.185 - ETA: 57s - loss: 3.0777 - accuracy: 0.185 - ETA: 56s - loss: 3.0782 - accuracy: 0.184 - ETA: 55s - loss: 3.0785 - accuracy: 0.185 - ETA: 54s - loss: 3.0791 - accuracy: 0.184 - ETA: 53s - loss: 3.0798 - accuracy: 0.184 - ETA: 52s - loss: 3.0796 - accuracy: 0.184 - ETA: 51s - loss: 3.0800 - accuracy: 0.184 - ETA: 50s - loss: 3.0808 - accuracy: 0.184 - ETA: 49s - loss: 3.0810 - accuracy: 0.184 - ETA: 48s - loss: 3.0805 - accuracy: 0.184 - ETA: 47s - loss: 3.0807 - accuracy: 0.184 - ETA: 46s - loss: 3.0813 - accuracy: 0.184 - ETA: 45s - loss: 3.0819 - accuracy: 0.184 - ETA: 44s - loss: 3.0826 - accuracy: 0.183 - ETA: 43s - loss: 3.0823 - accuracy: 0.183 - ETA: 42s - loss: 3.0827 - accuracy: 0.183 - ETA: 42s - loss: 3.0827 - accuracy: 0.183 - ETA: 41s - loss: 3.0829 - accuracy: 0.183 - ETA: 40s - loss: 3.0827 - accuracy: 0.183 - ETA: 39s - loss: 3.0830 - accuracy: 0.183 - ETA: 38s - loss: 3.0831 - accuracy: 0.183 - ETA: 37s - loss: 3.0837 - accuracy: 0.183 - ETA: 36s - loss: 3.0843 - accuracy: 0.183 - ETA: 35s - loss: 3.0840 - accuracy: 0.183 - ETA: 34s - loss: 3.0849 - accuracy: 0.183 - ETA: 33s - loss: 3.0854 - accuracy: 0.183 - ETA: 32s - loss: 3.0858 - accuracy: 0.182 - ETA: 31s - loss: 3.0858 - accuracy: 0.183 - ETA: 30s - loss: 3.0852 - accuracy: 0.183 - ETA: 29s - loss: 3.0858 - accuracy: 0.183 - ETA: 28s - loss: 3.0859 - accuracy: 0.183 - ETA: 27s - loss: 3.0864 - accuracy: 0.183 - ETA: 26s - loss: 3.0858 - accuracy: 0.183 - ETA: 25s - loss: 3.0864 - accuracy: 0.183 - ETA: 24s - loss: 3.0863 - accuracy: 0.183 - ETA: 23s - loss: 3.0864 - accuracy: 0.182 - ETA: 22s - loss: 3.0863 - accuracy: 0.182 - ETA: 21s - loss: 3.0866 - accuracy: 0.182 - ETA: 21s - loss: 3.0861 - accuracy: 0.182 - ETA: 20s - loss: 3.0865 - accuracy: 0.182 - ETA: 19s - loss: 3.0859 - accuracy: 0.182 - ETA: 18s - loss: 3.0853 - accuracy: 0.182 - ETA: 17s - loss: 3.0851 - accuracy: 0.182 - ETA: 16s - loss: 3.0854 - accuracy: 0.182 - ETA: 15s - loss: 3.0854 - accuracy: 0.182 - ETA: 14s - loss: 3.0851 - accuracy: 0.182 - ETA: 13s - loss: 3.0850 - accuracy: 0.182 - ETA: 12s - loss: 3.0846 - accuracy: 0.183 - ETA: 11s - loss: 3.0850 - accuracy: 0.183 - ETA: 10s - loss: 3.0850 - accuracy: 0.183 - ETA: 9s - loss: 3.0850 - accuracy: 0.183 - ETA: 8s - loss: 3.0851 - accuracy: 0.18 - ETA: 7s - loss: 3.0853 - accuracy: 0.18 - ETA: 6s - loss: 3.0849 - accuracy: 0.18 - ETA: 5s - loss: 3.0851 - accuracy: 0.18 - ETA: 4s - loss: 3.0846 - accuracy: 0.18 - ETA: 3s - loss: 3.0851 - accuracy: 0.18 - ETA: 2s - loss: 3.0854 - accuracy: 0.18 - ETA: 1s - loss: 3.0853 - accuracy: 0.18 - ETA: 1s - loss: 3.0854 - accuracy: 0.18 - ETA: 0s - loss: 3.0854 - accuracy: 0.18 - 338s 8ms/step - loss: 3.0857 - accuracy: 0.1832 - val_loss: 4.0478 - val_accuracy: 0.0357\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:17 - loss: 2.9734 - accuracy: 0.18 - ETA: 5:14 - loss: 3.1622 - accuracy: 0.15 - ETA: 5:15 - loss: 3.0984 - accuracy: 0.18 - ETA: 5:10 - loss: 3.0498 - accuracy: 0.19 - ETA: 5:10 - loss: 3.0375 - accuracy: 0.19 - ETA: 5:10 - loss: 3.0410 - accuracy: 0.19 - ETA: 5:09 - loss: 3.0305 - accuracy: 0.20 - ETA: 5:08 - loss: 3.0678 - accuracy: 0.19 - ETA: 5:07 - loss: 3.0760 - accuracy: 0.19 - ETA: 5:04 - loss: 3.0735 - accuracy: 0.19 - ETA: 5:06 - loss: 3.0970 - accuracy: 0.18 - ETA: 5:05 - loss: 3.1101 - accuracy: 0.18 - ETA: 5:04 - loss: 3.1098 - accuracy: 0.18 - ETA: 5:02 - loss: 3.1188 - accuracy: 0.18 - ETA: 5:03 - loss: 3.1136 - accuracy: 0.18 - ETA: 5:02 - loss: 3.1166 - accuracy: 0.18 - ETA: 5:00 - loss: 3.1154 - accuracy: 0.17 - ETA: 4:59 - loss: 3.1261 - accuracy: 0.17 - ETA: 4:57 - loss: 3.1379 - accuracy: 0.17 - ETA: 4:56 - loss: 3.1425 - accuracy: 0.17 - ETA: 4:54 - loss: 3.1304 - accuracy: 0.17 - ETA: 4:54 - loss: 3.1383 - accuracy: 0.17 - ETA: 4:53 - loss: 3.1285 - accuracy: 0.17 - ETA: 4:52 - loss: 3.1220 - accuracy: 0.17 - ETA: 4:51 - loss: 3.1200 - accuracy: 0.18 - ETA: 4:49 - loss: 3.1419 - accuracy: 0.17 - ETA: 4:49 - loss: 3.1442 - accuracy: 0.17 - ETA: 4:48 - loss: 3.1401 - accuracy: 0.17 - ETA: 4:47 - loss: 3.1340 - accuracy: 0.17 - ETA: 4:45 - loss: 3.1338 - accuracy: 0.17 - ETA: 4:44 - loss: 3.1699 - accuracy: 0.17 - ETA: 4:43 - loss: 3.1676 - accuracy: 0.17 - ETA: 4:42 - loss: 3.1678 - accuracy: 0.17 - ETA: 4:41 - loss: 3.1674 - accuracy: 0.17 - ETA: 4:40 - loss: 3.1688 - accuracy: 0.17 - ETA: 4:39 - loss: 3.1655 - accuracy: 0.17 - ETA: 4:38 - loss: 3.1694 - accuracy: 0.17 - ETA: 4:37 - loss: 3.1686 - accuracy: 0.17 - ETA: 4:36 - loss: 3.1653 - accuracy: 0.17 - ETA: 4:35 - loss: 3.1601 - accuracy: 0.17 - ETA: 4:34 - loss: 3.1597 - accuracy: 0.17 - ETA: 4:34 - loss: 3.1603 - accuracy: 0.17 - ETA: 4:33 - loss: 3.1586 - accuracy: 0.17 - ETA: 4:32 - loss: 3.1592 - accuracy: 0.17 - ETA: 4:32 - loss: 3.1533 - accuracy: 0.17 - ETA: 4:31 - loss: 3.1539 - accuracy: 0.17 - ETA: 4:30 - loss: 3.1536 - accuracy: 0.17 - ETA: 4:28 - loss: 3.1499 - accuracy: 0.17 - ETA: 4:27 - loss: 3.1515 - accuracy: 0.17 - ETA: 4:26 - loss: 3.1498 - accuracy: 0.17 - ETA: 4:25 - loss: 3.1504 - accuracy: 0.17 - ETA: 4:24 - loss: 3.1501 - accuracy: 0.17 - ETA: 4:23 - loss: 3.1507 - accuracy: 0.17 - ETA: 4:22 - loss: 3.1504 - accuracy: 0.17 - ETA: 4:21 - loss: 3.1473 - accuracy: 0.17 - ETA: 4:20 - loss: 3.1586 - accuracy: 0.17 - ETA: 4:19 - loss: 3.1593 - accuracy: 0.17 - ETA: 4:19 - loss: 3.1630 - accuracy: 0.17 - ETA: 4:18 - loss: 3.1620 - accuracy: 0.17 - ETA: 4:17 - loss: 3.1564 - accuracy: 0.17 - ETA: 4:16 - loss: 3.1555 - accuracy: 0.17 - ETA: 4:15 - loss: 3.1551 - accuracy: 0.17 - ETA: 4:14 - loss: 3.1530 - accuracy: 0.17 - ETA: 4:13 - loss: 3.1518 - accuracy: 0.17 - ETA: 4:12 - loss: 3.1479 - accuracy: 0.17 - ETA: 4:11 - loss: 3.1453 - accuracy: 0.17 - ETA: 4:10 - loss: 3.1414 - accuracy: 0.17 - ETA: 4:09 - loss: 3.1408 - accuracy: 0.17 - ETA: 4:08 - loss: 3.1404 - accuracy: 0.17 - ETA: 4:07 - loss: 3.1406 - accuracy: 0.17 - ETA: 4:06 - loss: 3.1407 - accuracy: 0.17 - ETA: 4:05 - loss: 3.1393 - accuracy: 0.17 - ETA: 4:04 - loss: 3.1414 - accuracy: 0.17 - ETA: 4:03 - loss: 3.1416 - accuracy: 0.17 - ETA: 4:02 - loss: 3.1404 - accuracy: 0.17 - ETA: 4:01 - loss: 3.1413 - accuracy: 0.17 - ETA: 4:00 - loss: 3.1403 - accuracy: 0.17 - ETA: 3:59 - loss: 3.1397 - accuracy: 0.17 - ETA: 3:58 - loss: 3.1408 - accuracy: 0.17 - ETA: 3:57 - loss: 3.1398 - accuracy: 0.17 - ETA: 3:57 - loss: 3.1416 - accuracy: 0.17 - ETA: 3:55 - loss: 3.1435 - accuracy: 0.17 - ETA: 3:54 - loss: 3.1401 - accuracy: 0.17 - ETA: 3:54 - loss: 3.1400 - accuracy: 0.17 - ETA: 3:53 - loss: 3.1372 - accuracy: 0.17 - ETA: 3:52 - loss: 3.1340 - accuracy: 0.17 - ETA: 3:51 - loss: 3.1362 - accuracy: 0.17 - ETA: 3:50 - loss: 3.1366 - accuracy: 0.17 - ETA: 3:48 - loss: 3.1361 - accuracy: 0.17 - ETA: 3:48 - loss: 3.1359 - accuracy: 0.17 - ETA: 3:47 - loss: 3.1377 - accuracy: 0.17 - ETA: 3:46 - loss: 3.1350 - accuracy: 0.17 - ETA: 3:45 - loss: 3.1364 - accuracy: 0.17 - ETA: 3:44 - loss: 3.1347 - accuracy: 0.17 - ETA: 3:43 - loss: 3.1351 - accuracy: 0.17 - ETA: 3:42 - loss: 3.1363 - accuracy: 0.17 - ETA: 3:41 - loss: 3.1364 - accuracy: 0.17 - ETA: 3:40 - loss: 3.1348 - accuracy: 0.17 - ETA: 3:39 - loss: 3.1351 - accuracy: 0.17 - ETA: 3:38 - loss: 3.1350 - accuracy: 0.17 - ETA: 3:37 - loss: 3.1357 - accuracy: 0.17 - ETA: 3:36 - loss: 3.1331 - accuracy: 0.17 - ETA: 3:35 - loss: 3.1329 - accuracy: 0.17 - ETA: 3:34 - loss: 3.1332 - accuracy: 0.17 - ETA: 3:33 - loss: 3.1333 - accuracy: 0.17 - ETA: 3:32 - loss: 3.1324 - accuracy: 0.17 - ETA: 3:31 - loss: 3.1323 - accuracy: 0.17 - ETA: 3:30 - loss: 3.1322 - accuracy: 0.17 - ETA: 3:30 - loss: 3.1338 - accuracy: 0.17 - ETA: 3:29 - loss: 3.1333 - accuracy: 0.17 - ETA: 3:28 - loss: 3.1328 - accuracy: 0.17 - ETA: 3:27 - loss: 3.1314 - accuracy: 0.17 - ETA: 3:26 - loss: 3.1296 - accuracy: 0.17 - ETA: 3:25 - loss: 3.1303 - accuracy: 0.17 - ETA: 3:24 - loss: 3.1264 - accuracy: 0.17 - ETA: 3:24 - loss: 3.1270 - accuracy: 0.17 - ETA: 3:23 - loss: 3.1259 - accuracy: 0.17 - ETA: 3:22 - loss: 3.1244 - accuracy: 0.17 - ETA: 3:21 - loss: 3.1240 - accuracy: 0.17 - ETA: 3:20 - loss: 3.1244 - accuracy: 0.17 - ETA: 3:19 - loss: 3.1245 - accuracy: 0.17 - ETA: 3:18 - loss: 3.1246 - accuracy: 0.17 - ETA: 3:17 - loss: 3.1250 - accuracy: 0.17 - ETA: 3:16 - loss: 3.1261 - accuracy: 0.17 - ETA: 3:15 - loss: 3.1254 - accuracy: 0.17 - ETA: 3:14 - loss: 3.1243 - accuracy: 0.17 - ETA: 3:13 - loss: 3.1250 - accuracy: 0.17 - ETA: 3:12 - loss: 3.1262 - accuracy: 0.17 - ETA: 3:11 - loss: 3.1263 - accuracy: 0.17 - ETA: 3:10 - loss: 3.1242 - accuracy: 0.17 - ETA: 3:09 - loss: 3.1234 - accuracy: 0.17 - ETA: 3:08 - loss: 3.1225 - accuracy: 0.17 - ETA: 3:07 - loss: 3.1215 - accuracy: 0.17 - ETA: 3:06 - loss: 3.1212 - accuracy: 0.17 - ETA: 3:05 - loss: 3.1199 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1193 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1192 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1199 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1215 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1221 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1217 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1223 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1216 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1209 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1204 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1203 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1202 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1205 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1210 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1201 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1196 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1197 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1199 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1194 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1186 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1205 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1223 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1231 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1233 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1239 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1234 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1231 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1221 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1217 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1211 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1211 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1218 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1210 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1221 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1218 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1216 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1222 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1219 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1214 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1202 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1203 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1202 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1201 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1203 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1203 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1191 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1193 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1188 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1192 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1190 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1191 - accuracy: 0.1780"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.1188 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1175 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1166 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1170 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1170 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1164 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1149 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1145 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1133 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1139 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1130 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1128 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1123 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1129 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1135 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1136 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1118 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1108 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1108 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1110 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1106 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1097 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1087 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1091 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1080 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1077 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1075 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1065 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1059 - accuracy: 0.18 - ETA: 1:49 - loss: 3.1057 - accuracy: 0.18 - ETA: 1:48 - loss: 3.1051 - accuracy: 0.18 - ETA: 1:47 - loss: 3.1050 - accuracy: 0.18 - ETA: 1:46 - loss: 3.1046 - accuracy: 0.18 - ETA: 1:45 - loss: 3.1038 - accuracy: 0.18 - ETA: 1:44 - loss: 3.1038 - accuracy: 0.18 - ETA: 1:43 - loss: 3.1040 - accuracy: 0.18 - ETA: 1:42 - loss: 3.1039 - accuracy: 0.18 - ETA: 1:41 - loss: 3.1035 - accuracy: 0.18 - ETA: 1:40 - loss: 3.1030 - accuracy: 0.18 - ETA: 1:39 - loss: 3.1027 - accuracy: 0.18 - ETA: 1:38 - loss: 3.1024 - accuracy: 0.18 - ETA: 1:37 - loss: 3.1016 - accuracy: 0.18 - ETA: 1:36 - loss: 3.1018 - accuracy: 0.18 - ETA: 1:35 - loss: 3.1017 - accuracy: 0.18 - ETA: 1:34 - loss: 3.1007 - accuracy: 0.18 - ETA: 1:33 - loss: 3.1005 - accuracy: 0.18 - ETA: 1:32 - loss: 3.1001 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0997 - accuracy: 0.18 - ETA: 1:31 - loss: 3.1001 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0998 - accuracy: 0.18 - ETA: 1:29 - loss: 3.1007 - accuracy: 0.18 - ETA: 1:28 - loss: 3.1013 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1032 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1029 - accuracy: 0.18 - ETA: 1:25 - loss: 3.1032 - accuracy: 0.17 - ETA: 1:24 - loss: 3.1035 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1032 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1026 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1026 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1018 - accuracy: 0.18 - ETA: 1:19 - loss: 3.1015 - accuracy: 0.18 - ETA: 1:18 - loss: 3.1006 - accuracy: 0.18 - ETA: 1:17 - loss: 3.1001 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0997 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0996 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0994 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0991 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0991 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0990 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0989 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0992 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0975 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0977 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0978 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0979 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0977 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0975 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0972 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0966 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0960 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0962 - accuracy: 0.18 - ETA: 59s - loss: 3.0960 - accuracy: 0.1809 - ETA: 58s - loss: 3.0952 - accuracy: 0.181 - ETA: 57s - loss: 3.0953 - accuracy: 0.181 - ETA: 56s - loss: 3.0948 - accuracy: 0.181 - ETA: 56s - loss: 3.0951 - accuracy: 0.181 - ETA: 55s - loss: 3.0947 - accuracy: 0.181 - ETA: 54s - loss: 3.0947 - accuracy: 0.181 - ETA: 53s - loss: 3.0944 - accuracy: 0.181 - ETA: 52s - loss: 3.0951 - accuracy: 0.181 - ETA: 51s - loss: 3.0949 - accuracy: 0.181 - ETA: 50s - loss: 3.0951 - accuracy: 0.181 - ETA: 49s - loss: 3.0941 - accuracy: 0.181 - ETA: 48s - loss: 3.0929 - accuracy: 0.181 - ETA: 47s - loss: 3.0931 - accuracy: 0.181 - ETA: 46s - loss: 3.0930 - accuracy: 0.181 - ETA: 45s - loss: 3.0925 - accuracy: 0.181 - ETA: 44s - loss: 3.0920 - accuracy: 0.181 - ETA: 43s - loss: 3.0915 - accuracy: 0.181 - ETA: 42s - loss: 3.0915 - accuracy: 0.181 - ETA: 41s - loss: 3.0916 - accuracy: 0.181 - ETA: 40s - loss: 3.0921 - accuracy: 0.181 - ETA: 39s - loss: 3.0915 - accuracy: 0.181 - ETA: 38s - loss: 3.0913 - accuracy: 0.181 - ETA: 38s - loss: 3.0912 - accuracy: 0.181 - ETA: 37s - loss: 3.0913 - accuracy: 0.181 - ETA: 36s - loss: 3.0911 - accuracy: 0.181 - ETA: 35s - loss: 3.0909 - accuracy: 0.181 - ETA: 34s - loss: 3.0914 - accuracy: 0.181 - ETA: 33s - loss: 3.0915 - accuracy: 0.181 - ETA: 32s - loss: 3.0920 - accuracy: 0.181 - ETA: 31s - loss: 3.0915 - accuracy: 0.181 - ETA: 30s - loss: 3.0906 - accuracy: 0.181 - ETA: 29s - loss: 3.0903 - accuracy: 0.181 - ETA: 28s - loss: 3.0902 - accuracy: 0.181 - ETA: 27s - loss: 3.0898 - accuracy: 0.181 - ETA: 26s - loss: 3.0894 - accuracy: 0.181 - ETA: 25s - loss: 3.0897 - accuracy: 0.182 - ETA: 24s - loss: 3.0902 - accuracy: 0.181 - ETA: 23s - loss: 3.0900 - accuracy: 0.181 - ETA: 22s - loss: 3.0902 - accuracy: 0.181 - ETA: 21s - loss: 3.0906 - accuracy: 0.181 - ETA: 20s - loss: 3.0902 - accuracy: 0.181 - ETA: 19s - loss: 3.0898 - accuracy: 0.181 - ETA: 19s - loss: 3.0891 - accuracy: 0.182 - ETA: 18s - loss: 3.0888 - accuracy: 0.182 - ETA: 17s - loss: 3.0872 - accuracy: 0.182 - ETA: 16s - loss: 3.0872 - accuracy: 0.182 - ETA: 15s - loss: 3.0866 - accuracy: 0.182 - ETA: 14s - loss: 3.0866 - accuracy: 0.182 - ETA: 13s - loss: 3.0867 - accuracy: 0.182 - ETA: 12s - loss: 3.0863 - accuracy: 0.182 - ETA: 11s - loss: 3.0858 - accuracy: 0.182 - ETA: 10s - loss: 3.0855 - accuracy: 0.182 - ETA: 9s - loss: 3.0852 - accuracy: 0.182 - ETA: 8s - loss: 3.0846 - accuracy: 0.18 - ETA: 7s - loss: 3.0844 - accuracy: 0.18 - ETA: 6s - loss: 3.0845 - accuracy: 0.18 - ETA: 5s - loss: 3.0844 - accuracy: 0.18 - ETA: 4s - loss: 3.0840 - accuracy: 0.18 - ETA: 3s - loss: 3.0836 - accuracy: 0.18 - ETA: 2s - loss: 3.0832 - accuracy: 0.18 - ETA: 1s - loss: 3.0829 - accuracy: 0.18 - ETA: 1s - loss: 3.0822 - accuracy: 0.18 - ETA: 0s - loss: 3.0825 - accuracy: 0.18 - 337s 8ms/step - loss: 3.0825 - accuracy: 0.1835 - val_loss: 4.1740 - val_accuracy: 0.0370\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:59 - loss: 3.0484 - accuracy: 0.17 - ETA: 5:00 - loss: 3.0418 - accuracy: 0.17 - ETA: 4:58 - loss: 2.9886 - accuracy: 0.20 - ETA: 5:02 - loss: 2.9885 - accuracy: 0.19 - ETA: 5:02 - loss: 2.9576 - accuracy: 0.20 - ETA: 5:00 - loss: 3.0023 - accuracy: 0.19 - ETA: 5:00 - loss: 2.9845 - accuracy: 0.20 - ETA: 5:01 - loss: 2.9884 - accuracy: 0.19 - ETA: 4:59 - loss: 2.9907 - accuracy: 0.19 - ETA: 4:58 - loss: 2.9981 - accuracy: 0.19 - ETA: 4:58 - loss: 2.9831 - accuracy: 0.19 - ETA: 4:57 - loss: 2.9870 - accuracy: 0.19 - ETA: 4:56 - loss: 2.9991 - accuracy: 0.19 - ETA: 4:56 - loss: 2.9977 - accuracy: 0.19 - ETA: 4:55 - loss: 3.0041 - accuracy: 0.19 - ETA: 4:54 - loss: 2.9994 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0030 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0142 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0277 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0273 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0278 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0274 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0326 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0347 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0246 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0274 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0270 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0294 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0241 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0191 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0245 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0270 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0292 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0266 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0296 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0265 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0274 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0240 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0257 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0294 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0295 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0240 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0196 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0212 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0259 - accuracy: 0.18 - ETA: 4:29 - loss: 3.0293 - accuracy: 0.18 - ETA: 4:28 - loss: 3.0283 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0312 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0290 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0251 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0269 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0301 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0273 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0251 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0274 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0278 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0281 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0282 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0304 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0306 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0292 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0299 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0336 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0310 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0284 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0261 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0231 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0240 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0226 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0213 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0213 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0204 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0214 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0199 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0224 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0206 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0207 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0191 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0184 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0197 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0201 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0195 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0194 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0176 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0167 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0169 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0159 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0154 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0166 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0166 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0213 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0234 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0228 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0252 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0252 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0259 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0275 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0286 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0284 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0308 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0316 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0315 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0317 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0381 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0377 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0375 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0371 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0392 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0393 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0390 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0406 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0414 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0429 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0437 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0435 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0459 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0489 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0489 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0473 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0471 - accuracy: 0.18 - ETA: 3:19 - loss: 3.0503 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0519 - accuracy: 0.18 - ETA: 3:17 - loss: 3.0523 - accuracy: 0.18 - ETA: 3:16 - loss: 3.0544 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0565 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0574 - accuracy: 0.18 - ETA: 3:13 - loss: 3.0573 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0606 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0618 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0613 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0612 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0611 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0610 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0632 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0634 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0639 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0648 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0703 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0711 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0712 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0733 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0746 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0751 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0755 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0764 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0757 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0765 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0763 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0774 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0775 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0778 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0779 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0794 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0798 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0812 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0811 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0818 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0813 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0817 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0837 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0844 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0848 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0844 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0851 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0855 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0860 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0864 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0883 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0885 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0892 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0899 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0914 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0919 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0925 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0933 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0933 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0933 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0936 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0950 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0957 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0967 - accuracy: 0.17 - ETA: 2:21 - loss: 3.0958 - accuracy: 0.17 - ETA: 2:20 - loss: 3.0961 - accuracy: 0.17 - ETA: 2:19 - loss: 3.0966 - accuracy: 0.17 - ETA: 2:18 - loss: 3.0963 - accuracy: 0.17 - ETA: 2:17 - loss: 3.0965 - accuracy: 0.1796"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0979 - accuracy: 0.17 - ETA: 2:15 - loss: 3.0986 - accuracy: 0.17 - ETA: 2:14 - loss: 3.0992 - accuracy: 0.17 - ETA: 2:13 - loss: 3.0993 - accuracy: 0.17 - ETA: 2:13 - loss: 3.0984 - accuracy: 0.17 - ETA: 2:12 - loss: 3.0997 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1001 - accuracy: 0.17 - ETA: 2:10 - loss: 3.0998 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1001 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1000 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1010 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1016 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1018 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1020 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1016 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1029 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1029 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1053 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1050 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1055 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1060 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1062 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1066 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1074 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1089 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1082 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1082 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1087 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1097 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1111 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1105 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1094 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1100 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1098 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1090 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1096 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1095 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1096 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1097 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1085 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1090 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1080 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1079 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1078 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1075 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1067 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1073 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1077 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1072 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1078 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1087 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1086 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1089 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1091 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1097 - accuracy: 0.17 - ETA: 1:24 - loss: 3.1095 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1101 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1110 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1115 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1121 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1113 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1117 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1118 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1119 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1118 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1107 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1104 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1103 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1099 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1103 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1096 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1086 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1089 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1093 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1095 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1080 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1079 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1083 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1086 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1083 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1085 - accuracy: 0.17 - ETA: 59s - loss: 3.1082 - accuracy: 0.1790 - ETA: 58s - loss: 3.1089 - accuracy: 0.178 - ETA: 57s - loss: 3.1087 - accuracy: 0.178 - ETA: 57s - loss: 3.1088 - accuracy: 0.178 - ETA: 56s - loss: 3.1083 - accuracy: 0.178 - ETA: 55s - loss: 3.1075 - accuracy: 0.179 - ETA: 54s - loss: 3.1076 - accuracy: 0.179 - ETA: 53s - loss: 3.1071 - accuracy: 0.179 - ETA: 52s - loss: 3.1071 - accuracy: 0.179 - ETA: 51s - loss: 3.1067 - accuracy: 0.179 - ETA: 50s - loss: 3.1061 - accuracy: 0.179 - ETA: 49s - loss: 3.1059 - accuracy: 0.179 - ETA: 48s - loss: 3.1059 - accuracy: 0.179 - ETA: 47s - loss: 3.1057 - accuracy: 0.179 - ETA: 46s - loss: 3.1050 - accuracy: 0.179 - ETA: 45s - loss: 3.1049 - accuracy: 0.179 - ETA: 44s - loss: 3.1054 - accuracy: 0.179 - ETA: 43s - loss: 3.1052 - accuracy: 0.179 - ETA: 42s - loss: 3.1044 - accuracy: 0.179 - ETA: 41s - loss: 3.1050 - accuracy: 0.179 - ETA: 40s - loss: 3.1050 - accuracy: 0.179 - ETA: 39s - loss: 3.1051 - accuracy: 0.179 - ETA: 39s - loss: 3.1048 - accuracy: 0.179 - ETA: 38s - loss: 3.1050 - accuracy: 0.179 - ETA: 37s - loss: 3.1045 - accuracy: 0.179 - ETA: 36s - loss: 3.1031 - accuracy: 0.179 - ETA: 35s - loss: 3.1028 - accuracy: 0.179 - ETA: 34s - loss: 3.1023 - accuracy: 0.179 - ETA: 33s - loss: 3.1019 - accuracy: 0.179 - ETA: 32s - loss: 3.1019 - accuracy: 0.179 - ETA: 31s - loss: 3.1017 - accuracy: 0.179 - ETA: 30s - loss: 3.1010 - accuracy: 0.179 - ETA: 29s - loss: 3.1008 - accuracy: 0.179 - ETA: 28s - loss: 3.1005 - accuracy: 0.179 - ETA: 27s - loss: 3.1009 - accuracy: 0.179 - ETA: 26s - loss: 3.1008 - accuracy: 0.179 - ETA: 25s - loss: 3.1002 - accuracy: 0.180 - ETA: 24s - loss: 3.0996 - accuracy: 0.180 - ETA: 23s - loss: 3.0992 - accuracy: 0.180 - ETA: 22s - loss: 3.0987 - accuracy: 0.180 - ETA: 21s - loss: 3.0986 - accuracy: 0.180 - ETA: 20s - loss: 3.0983 - accuracy: 0.180 - ETA: 20s - loss: 3.0988 - accuracy: 0.180 - ETA: 19s - loss: 3.0982 - accuracy: 0.180 - ETA: 18s - loss: 3.0979 - accuracy: 0.180 - ETA: 17s - loss: 3.0970 - accuracy: 0.180 - ETA: 16s - loss: 3.0968 - accuracy: 0.180 - ETA: 15s - loss: 3.0967 - accuracy: 0.180 - ETA: 14s - loss: 3.0961 - accuracy: 0.180 - ETA: 13s - loss: 3.0955 - accuracy: 0.180 - ETA: 12s - loss: 3.0954 - accuracy: 0.180 - ETA: 11s - loss: 3.0949 - accuracy: 0.181 - ETA: 10s - loss: 3.0952 - accuracy: 0.181 - ETA: 9s - loss: 3.0952 - accuracy: 0.181 - ETA: 8s - loss: 3.0951 - accuracy: 0.18 - ETA: 7s - loss: 3.0951 - accuracy: 0.18 - ETA: 6s - loss: 3.0951 - accuracy: 0.18 - ETA: 5s - loss: 3.0947 - accuracy: 0.18 - ETA: 4s - loss: 3.0946 - accuracy: 0.18 - ETA: 3s - loss: 3.0943 - accuracy: 0.18 - ETA: 2s - loss: 3.0939 - accuracy: 0.18 - ETA: 1s - loss: 3.0937 - accuracy: 0.18 - ETA: 1s - loss: 3.0941 - accuracy: 0.18 - ETA: 0s - loss: 3.0943 - accuracy: 0.18 - 338s 8ms/step - loss: 3.0944 - accuracy: 0.1807 - val_loss: 4.1801 - val_accuracy: 0.0390\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:58 - loss: 3.0226 - accuracy: 0.20 - ETA: 5:13 - loss: 3.0645 - accuracy: 0.17 - ETA: 5:11 - loss: 2.9797 - accuracy: 0.19 - ETA: 5:07 - loss: 3.0095 - accuracy: 0.18 - ETA: 5:08 - loss: 3.0146 - accuracy: 0.20 - ETA: 5:07 - loss: 3.0109 - accuracy: 0.19 - ETA: 5:07 - loss: 2.9749 - accuracy: 0.19 - ETA: 5:06 - loss: 2.9926 - accuracy: 0.19 - ETA: 5:06 - loss: 2.9918 - accuracy: 0.19 - ETA: 5:02 - loss: 2.9910 - accuracy: 0.19 - ETA: 5:01 - loss: 3.0024 - accuracy: 0.19 - ETA: 4:59 - loss: 2.9871 - accuracy: 0.19 - ETA: 4:59 - loss: 2.9748 - accuracy: 0.20 - ETA: 4:57 - loss: 2.9827 - accuracy: 0.20 - ETA: 4:55 - loss: 2.9750 - accuracy: 0.20 - ETA: 4:54 - loss: 2.9708 - accuracy: 0.20 - ETA: 4:53 - loss: 2.9744 - accuracy: 0.20 - ETA: 4:52 - loss: 2.9800 - accuracy: 0.20 - ETA: 4:51 - loss: 2.9849 - accuracy: 0.20 - ETA: 4:50 - loss: 2.9831 - accuracy: 0.20 - ETA: 4:50 - loss: 2.9901 - accuracy: 0.20 - ETA: 4:49 - loss: 2.9952 - accuracy: 0.20 - ETA: 4:49 - loss: 2.9905 - accuracy: 0.20 - ETA: 4:48 - loss: 2.9882 - accuracy: 0.20 - ETA: 4:48 - loss: 2.9834 - accuracy: 0.20 - ETA: 4:47 - loss: 2.9767 - accuracy: 0.20 - ETA: 4:47 - loss: 2.9776 - accuracy: 0.20 - ETA: 4:47 - loss: 2.9773 - accuracy: 0.20 - ETA: 4:46 - loss: 2.9786 - accuracy: 0.20 - ETA: 4:45 - loss: 2.9866 - accuracy: 0.20 - ETA: 4:44 - loss: 2.9942 - accuracy: 0.20 - ETA: 4:43 - loss: 3.0018 - accuracy: 0.20 - ETA: 4:42 - loss: 3.0071 - accuracy: 0.20 - ETA: 4:41 - loss: 3.0072 - accuracy: 0.20 - ETA: 4:40 - loss: 3.0016 - accuracy: 0.20 - ETA: 4:39 - loss: 3.0030 - accuracy: 0.20 - ETA: 4:39 - loss: 3.0045 - accuracy: 0.20 - ETA: 4:37 - loss: 3.0117 - accuracy: 0.20 - ETA: 4:36 - loss: 3.0160 - accuracy: 0.20 - ETA: 4:35 - loss: 3.0154 - accuracy: 0.20 - ETA: 4:34 - loss: 3.0171 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0175 - accuracy: 0.20 - ETA: 4:32 - loss: 3.0173 - accuracy: 0.20 - ETA: 4:32 - loss: 3.0182 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0180 - accuracy: 0.20 - ETA: 4:30 - loss: 3.0193 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0190 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0172 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0173 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0147 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0157 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0143 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0168 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0175 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0194 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0220 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0248 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0257 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0274 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0269 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0295 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0280 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0296 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0285 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0300 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0328 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0324 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0309 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0326 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0295 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0300 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0289 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0282 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0286 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0286 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0282 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0267 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0272 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0264 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0259 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0275 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0277 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0278 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0268 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0346 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0342 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0352 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0332 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0332 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0345 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0370 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0374 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0377 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0392 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0398 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0402 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0393 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0402 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0416 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0438 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0423 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0429 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0430 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0423 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0422 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0436 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0429 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0436 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0437 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0429 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0432 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0443 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0435 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0422 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0411 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0420 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0424 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0410 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0414 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0403 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0397 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0405 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0399 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0405 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0410 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0398 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0396 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0404 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0400 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0395 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0403 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0406 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0405 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0403 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0403 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0404 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0410 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0405 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0411 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0401 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0413 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0414 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0399 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0403 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0404 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0405 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0393 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0377 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0389 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0412 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0422 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0412 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0407 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0404 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0394 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0389 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0379 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0382 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0383 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0384 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0386 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0376 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0370 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0376 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0378 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0376 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0375 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0379 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0379 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0378 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0375 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0361 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0350 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0352 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0355 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0364 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0358 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0375 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0367 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0376 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0383 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0382 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0392 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0407 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0412 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0420 - accuracy: 0.1912"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0434 - accuracy: 0.19 - ETA: 2:16 - loss: 3.0434 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0438 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0446 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0454 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0455 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0467 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0473 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0483 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0489 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0492 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0502 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0525 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0533 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0522 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0524 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0525 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0531 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0542 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0572 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0580 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0580 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0591 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0588 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0596 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0605 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0607 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0608 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0609 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0610 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0617 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0621 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0627 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0627 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0633 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0643 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0650 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0652 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0663 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0671 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0674 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0665 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0676 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0677 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0673 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0680 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0678 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0682 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0676 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0678 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0673 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0667 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0669 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0674 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0682 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0700 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0709 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0710 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0724 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0716 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0719 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0720 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0749 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0753 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0753 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0759 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0758 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0760 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0758 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0764 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0762 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0756 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0758 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0763 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0755 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0753 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0753 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0752 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0757 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0758 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0760 - accuracy: 0.18 - ETA: 59s - loss: 3.0764 - accuracy: 0.1862 - ETA: 58s - loss: 3.0771 - accuracy: 0.185 - ETA: 57s - loss: 3.0776 - accuracy: 0.185 - ETA: 56s - loss: 3.0774 - accuracy: 0.186 - ETA: 55s - loss: 3.0776 - accuracy: 0.186 - ETA: 54s - loss: 3.0784 - accuracy: 0.185 - ETA: 53s - loss: 3.0780 - accuracy: 0.185 - ETA: 52s - loss: 3.0786 - accuracy: 0.185 - ETA: 51s - loss: 3.0781 - accuracy: 0.185 - ETA: 50s - loss: 3.0785 - accuracy: 0.185 - ETA: 49s - loss: 3.0794 - accuracy: 0.185 - ETA: 48s - loss: 3.0791 - accuracy: 0.185 - ETA: 47s - loss: 3.0795 - accuracy: 0.185 - ETA: 46s - loss: 3.0797 - accuracy: 0.185 - ETA: 45s - loss: 3.0804 - accuracy: 0.185 - ETA: 44s - loss: 3.0801 - accuracy: 0.185 - ETA: 43s - loss: 3.0799 - accuracy: 0.185 - ETA: 42s - loss: 3.0806 - accuracy: 0.185 - ETA: 42s - loss: 3.0807 - accuracy: 0.185 - ETA: 41s - loss: 3.0797 - accuracy: 0.185 - ETA: 40s - loss: 3.0796 - accuracy: 0.185 - ETA: 39s - loss: 3.0804 - accuracy: 0.185 - ETA: 38s - loss: 3.0805 - accuracy: 0.185 - ETA: 37s - loss: 3.0809 - accuracy: 0.185 - ETA: 36s - loss: 3.0813 - accuracy: 0.185 - ETA: 35s - loss: 3.0813 - accuracy: 0.185 - ETA: 34s - loss: 3.0807 - accuracy: 0.185 - ETA: 33s - loss: 3.0812 - accuracy: 0.185 - ETA: 32s - loss: 3.0820 - accuracy: 0.185 - ETA: 31s - loss: 3.0822 - accuracy: 0.185 - ETA: 30s - loss: 3.0828 - accuracy: 0.185 - ETA: 29s - loss: 3.0837 - accuracy: 0.184 - ETA: 28s - loss: 3.0843 - accuracy: 0.184 - ETA: 27s - loss: 3.0843 - accuracy: 0.184 - ETA: 26s - loss: 3.0842 - accuracy: 0.184 - ETA: 25s - loss: 3.0850 - accuracy: 0.184 - ETA: 24s - loss: 3.0852 - accuracy: 0.184 - ETA: 23s - loss: 3.0851 - accuracy: 0.184 - ETA: 22s - loss: 3.0850 - accuracy: 0.184 - ETA: 22s - loss: 3.0850 - accuracy: 0.184 - ETA: 21s - loss: 3.0850 - accuracy: 0.184 - ETA: 20s - loss: 3.0850 - accuracy: 0.184 - ETA: 19s - loss: 3.0849 - accuracy: 0.184 - ETA: 18s - loss: 3.0851 - accuracy: 0.184 - ETA: 17s - loss: 3.0856 - accuracy: 0.184 - ETA: 16s - loss: 3.0851 - accuracy: 0.184 - ETA: 15s - loss: 3.0854 - accuracy: 0.184 - ETA: 14s - loss: 3.0853 - accuracy: 0.184 - ETA: 13s - loss: 3.0853 - accuracy: 0.184 - ETA: 12s - loss: 3.0850 - accuracy: 0.184 - ETA: 11s - loss: 3.0847 - accuracy: 0.184 - ETA: 10s - loss: 3.0854 - accuracy: 0.184 - ETA: 9s - loss: 3.0844 - accuracy: 0.184 - ETA: 8s - loss: 3.0842 - accuracy: 0.18 - ETA: 7s - loss: 3.0840 - accuracy: 0.18 - ETA: 6s - loss: 3.0842 - accuracy: 0.18 - ETA: 5s - loss: 3.0836 - accuracy: 0.18 - ETA: 4s - loss: 3.0837 - accuracy: 0.18 - ETA: 3s - loss: 3.0834 - accuracy: 0.18 - ETA: 2s - loss: 3.0836 - accuracy: 0.18 - ETA: 1s - loss: 3.0834 - accuracy: 0.18 - ETA: 1s - loss: 3.0836 - accuracy: 0.18 - ETA: 0s - loss: 3.0833 - accuracy: 0.18 - 338s 8ms/step - loss: 3.0833 - accuracy: 0.1844 - val_loss: 4.1175 - val_accuracy: 0.0392\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:12 - loss: 3.1756 - accuracy: 0.12 - ETA: 5:09 - loss: 3.0494 - accuracy: 0.15 - ETA: 5:07 - loss: 3.0607 - accuracy: 0.15 - ETA: 5:07 - loss: 3.0878 - accuracy: 0.15 - ETA: 5:05 - loss: 3.1274 - accuracy: 0.15 - ETA: 5:04 - loss: 3.1333 - accuracy: 0.16 - ETA: 5:02 - loss: 3.1138 - accuracy: 0.16 - ETA: 5:01 - loss: 3.1164 - accuracy: 0.17 - ETA: 4:59 - loss: 3.1204 - accuracy: 0.16 - ETA: 4:59 - loss: 3.1144 - accuracy: 0.17 - ETA: 4:58 - loss: 3.1025 - accuracy: 0.17 - ETA: 4:57 - loss: 3.0896 - accuracy: 0.17 - ETA: 4:56 - loss: 3.1013 - accuracy: 0.17 - ETA: 4:58 - loss: 3.0975 - accuracy: 0.17 - ETA: 4:57 - loss: 3.0995 - accuracy: 0.17 - ETA: 4:57 - loss: 3.1047 - accuracy: 0.17 - ETA: 4:57 - loss: 3.0936 - accuracy: 0.17 - ETA: 4:57 - loss: 3.0928 - accuracy: 0.17 - ETA: 4:55 - loss: 3.0956 - accuracy: 0.17 - ETA: 4:54 - loss: 3.0943 - accuracy: 0.17 - ETA: 4:52 - loss: 3.1024 - accuracy: 0.17 - ETA: 4:51 - loss: 3.0975 - accuracy: 0.17 - ETA: 4:50 - loss: 3.0914 - accuracy: 0.17 - ETA: 4:50 - loss: 3.0967 - accuracy: 0.17 - ETA: 4:49 - loss: 3.0918 - accuracy: 0.17 - ETA: 4:48 - loss: 3.0898 - accuracy: 0.17 - ETA: 4:47 - loss: 3.0921 - accuracy: 0.17 - ETA: 4:45 - loss: 3.0892 - accuracy: 0.17 - ETA: 4:45 - loss: 3.0909 - accuracy: 0.17 - ETA: 4:44 - loss: 3.1012 - accuracy: 0.17 - ETA: 4:43 - loss: 3.1015 - accuracy: 0.17 - ETA: 4:42 - loss: 3.1044 - accuracy: 0.17 - ETA: 4:41 - loss: 3.1040 - accuracy: 0.17 - ETA: 4:40 - loss: 3.1055 - accuracy: 0.17 - ETA: 4:39 - loss: 3.1030 - accuracy: 0.17 - ETA: 4:38 - loss: 3.1027 - accuracy: 0.17 - ETA: 4:37 - loss: 3.0990 - accuracy: 0.17 - ETA: 4:36 - loss: 3.1007 - accuracy: 0.17 - ETA: 4:35 - loss: 3.1056 - accuracy: 0.17 - ETA: 4:34 - loss: 3.0984 - accuracy: 0.17 - ETA: 4:33 - loss: 3.0952 - accuracy: 0.17 - ETA: 4:33 - loss: 3.0949 - accuracy: 0.17 - ETA: 4:32 - loss: 3.0921 - accuracy: 0.17 - ETA: 4:31 - loss: 3.1062 - accuracy: 0.17 - ETA: 4:30 - loss: 3.1069 - accuracy: 0.17 - ETA: 4:29 - loss: 3.1143 - accuracy: 0.17 - ETA: 4:28 - loss: 3.1137 - accuracy: 0.17 - ETA: 4:27 - loss: 3.1147 - accuracy: 0.17 - ETA: 4:26 - loss: 3.1157 - accuracy: 0.17 - ETA: 4:25 - loss: 3.1173 - accuracy: 0.17 - ETA: 4:24 - loss: 3.1131 - accuracy: 0.17 - ETA: 4:23 - loss: 3.1116 - accuracy: 0.17 - ETA: 4:22 - loss: 3.1094 - accuracy: 0.17 - ETA: 4:21 - loss: 3.1101 - accuracy: 0.17 - ETA: 4:20 - loss: 3.1094 - accuracy: 0.17 - ETA: 4:19 - loss: 3.1056 - accuracy: 0.17 - ETA: 4:19 - loss: 3.1074 - accuracy: 0.17 - ETA: 4:18 - loss: 3.1080 - accuracy: 0.17 - ETA: 4:17 - loss: 3.1096 - accuracy: 0.17 - ETA: 4:16 - loss: 3.1068 - accuracy: 0.17 - ETA: 4:15 - loss: 3.1061 - accuracy: 0.17 - ETA: 4:14 - loss: 3.1043 - accuracy: 0.17 - ETA: 4:13 - loss: 3.1045 - accuracy: 0.17 - ETA: 4:12 - loss: 3.1044 - accuracy: 0.17 - ETA: 4:11 - loss: 3.1040 - accuracy: 0.17 - ETA: 4:10 - loss: 3.1028 - accuracy: 0.17 - ETA: 4:09 - loss: 3.0996 - accuracy: 0.17 - ETA: 4:08 - loss: 3.1011 - accuracy: 0.17 - ETA: 4:07 - loss: 3.1020 - accuracy: 0.17 - ETA: 4:06 - loss: 3.1006 - accuracy: 0.17 - ETA: 4:05 - loss: 3.1015 - accuracy: 0.17 - ETA: 4:04 - loss: 3.0998 - accuracy: 0.17 - ETA: 4:03 - loss: 3.0976 - accuracy: 0.17 - ETA: 4:02 - loss: 3.0930 - accuracy: 0.18 - ETA: 4:01 - loss: 3.0932 - accuracy: 0.18 - ETA: 4:00 - loss: 3.0908 - accuracy: 0.18 - ETA: 3:59 - loss: 3.0906 - accuracy: 0.18 - ETA: 3:59 - loss: 3.0878 - accuracy: 0.18 - ETA: 3:58 - loss: 3.0881 - accuracy: 0.18 - ETA: 3:57 - loss: 3.0881 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0890 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0902 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0864 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0842 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0838 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0839 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0813 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0814 - accuracy: 0.18 - ETA: 3:49 - loss: 3.0825 - accuracy: 0.18 - ETA: 3:48 - loss: 3.0820 - accuracy: 0.18 - ETA: 3:47 - loss: 3.0812 - accuracy: 0.18 - ETA: 3:46 - loss: 3.0814 - accuracy: 0.18 - ETA: 3:45 - loss: 3.0786 - accuracy: 0.18 - ETA: 3:44 - loss: 3.0806 - accuracy: 0.18 - ETA: 3:43 - loss: 3.0785 - accuracy: 0.18 - ETA: 3:42 - loss: 3.0789 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0748 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0761 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0745 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0774 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0795 - accuracy: 0.18 - ETA: 3:37 - loss: 3.0784 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0784 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0773 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0797 - accuracy: 0.18 - ETA: 3:33 - loss: 3.0797 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0799 - accuracy: 0.18 - ETA: 3:31 - loss: 3.0788 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0800 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0804 - accuracy: 0.18 - ETA: 3:28 - loss: 3.0799 - accuracy: 0.18 - ETA: 3:27 - loss: 3.0791 - accuracy: 0.18 - ETA: 3:26 - loss: 3.0794 - accuracy: 0.18 - ETA: 3:25 - loss: 3.0785 - accuracy: 0.18 - ETA: 3:24 - loss: 3.0773 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0749 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0757 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0765 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0750 - accuracy: 0.18 - ETA: 3:19 - loss: 3.0741 - accuracy: 0.18 - ETA: 3:19 - loss: 3.0734 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0719 - accuracy: 0.18 - ETA: 3:17 - loss: 3.0721 - accuracy: 0.18 - ETA: 3:16 - loss: 3.0724 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0716 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0712 - accuracy: 0.18 - ETA: 3:13 - loss: 3.0708 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0706 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0688 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0686 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0680 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0681 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0672 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0673 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0670 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0659 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0654 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0649 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0641 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0620 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0611 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0605 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0601 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0595 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0599 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0601 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0612 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0618 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0616 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0613 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0615 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0614 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0611 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0603 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0595 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0591 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0612 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0620 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0618 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0621 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0625 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0632 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0659 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0657 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0658 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0648 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0645 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0647 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0647 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0658 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0648 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0657 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0651 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0659 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0664 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0660 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0661 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0652 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0649 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0647 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0640 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0635 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0640 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0620 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0621 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0625 - accuracy: 0.1854"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0630 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0625 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0621 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0615 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0605 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0600 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0602 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0604 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0606 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0602 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0593 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0590 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0587 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0599 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0593 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0587 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0589 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0585 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0586 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0586 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0583 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0581 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0572 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0565 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0560 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0559 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0561 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0571 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0578 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0580 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0573 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0566 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0559 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0554 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0553 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0543 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0536 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0536 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0539 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0549 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0558 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0551 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0557 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0564 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0560 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0550 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0550 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0554 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0557 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0561 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0575 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0578 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0578 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0583 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0589 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0586 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0618 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0619 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0617 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0617 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0611 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0608 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0607 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0606 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0603 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0602 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0609 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0605 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0600 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0600 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0589 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0592 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0592 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0592 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0589 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0584 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0584 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0577 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0579 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0573 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0575 - accuracy: 0.18 - ETA: 59s - loss: 3.0578 - accuracy: 0.1875 - ETA: 58s - loss: 3.0574 - accuracy: 0.187 - ETA: 57s - loss: 3.0573 - accuracy: 0.187 - ETA: 57s - loss: 3.0576 - accuracy: 0.187 - ETA: 56s - loss: 3.0580 - accuracy: 0.187 - ETA: 55s - loss: 3.0578 - accuracy: 0.187 - ETA: 54s - loss: 3.0583 - accuracy: 0.187 - ETA: 53s - loss: 3.0582 - accuracy: 0.187 - ETA: 52s - loss: 3.0586 - accuracy: 0.187 - ETA: 51s - loss: 3.0592 - accuracy: 0.187 - ETA: 50s - loss: 3.0599 - accuracy: 0.186 - ETA: 49s - loss: 3.0589 - accuracy: 0.186 - ETA: 48s - loss: 3.0585 - accuracy: 0.187 - ETA: 47s - loss: 3.0588 - accuracy: 0.187 - ETA: 46s - loss: 3.0586 - accuracy: 0.187 - ETA: 45s - loss: 3.0584 - accuracy: 0.187 - ETA: 44s - loss: 3.0581 - accuracy: 0.187 - ETA: 43s - loss: 3.0578 - accuracy: 0.187 - ETA: 42s - loss: 3.0580 - accuracy: 0.187 - ETA: 41s - loss: 3.0579 - accuracy: 0.187 - ETA: 40s - loss: 3.0571 - accuracy: 0.187 - ETA: 39s - loss: 3.0565 - accuracy: 0.187 - ETA: 39s - loss: 3.0565 - accuracy: 0.187 - ETA: 38s - loss: 3.0564 - accuracy: 0.187 - ETA: 37s - loss: 3.0565 - accuracy: 0.187 - ETA: 36s - loss: 3.0565 - accuracy: 0.187 - ETA: 35s - loss: 3.0563 - accuracy: 0.187 - ETA: 34s - loss: 3.0567 - accuracy: 0.187 - ETA: 33s - loss: 3.0568 - accuracy: 0.187 - ETA: 32s - loss: 3.0564 - accuracy: 0.187 - ETA: 31s - loss: 3.0576 - accuracy: 0.187 - ETA: 30s - loss: 3.0579 - accuracy: 0.187 - ETA: 29s - loss: 3.0577 - accuracy: 0.187 - ETA: 28s - loss: 3.0587 - accuracy: 0.187 - ETA: 27s - loss: 3.0588 - accuracy: 0.186 - ETA: 26s - loss: 3.0591 - accuracy: 0.186 - ETA: 25s - loss: 3.0597 - accuracy: 0.186 - ETA: 24s - loss: 3.0596 - accuracy: 0.186 - ETA: 23s - loss: 3.0601 - accuracy: 0.186 - ETA: 22s - loss: 3.0599 - accuracy: 0.186 - ETA: 21s - loss: 3.0606 - accuracy: 0.186 - ETA: 20s - loss: 3.0605 - accuracy: 0.186 - ETA: 20s - loss: 3.0596 - accuracy: 0.186 - ETA: 19s - loss: 3.0597 - accuracy: 0.186 - ETA: 18s - loss: 3.0594 - accuracy: 0.186 - ETA: 17s - loss: 3.0583 - accuracy: 0.187 - ETA: 16s - loss: 3.0579 - accuracy: 0.187 - ETA: 15s - loss: 3.0580 - accuracy: 0.187 - ETA: 14s - loss: 3.0583 - accuracy: 0.187 - ETA: 13s - loss: 3.0578 - accuracy: 0.187 - ETA: 12s - loss: 3.0572 - accuracy: 0.187 - ETA: 11s - loss: 3.0573 - accuracy: 0.187 - ETA: 10s - loss: 3.0573 - accuracy: 0.187 - ETA: 9s - loss: 3.0570 - accuracy: 0.187 - ETA: 8s - loss: 3.0575 - accuracy: 0.18 - ETA: 7s - loss: 3.0581 - accuracy: 0.18 - ETA: 6s - loss: 3.0583 - accuracy: 0.18 - ETA: 5s - loss: 3.0582 - accuracy: 0.18 - ETA: 4s - loss: 3.0585 - accuracy: 0.18 - ETA: 3s - loss: 3.0589 - accuracy: 0.18 - ETA: 2s - loss: 3.0598 - accuracy: 0.18 - ETA: 1s - loss: 3.0599 - accuracy: 0.18 - ETA: 1s - loss: 3.0595 - accuracy: 0.18 - ETA: 0s - loss: 3.0597 - accuracy: 0.18 - 337s 8ms/step - loss: 3.0596 - accuracy: 0.1870 - val_loss: 4.0655 - val_accuracy: 0.0361\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:10 - loss: 3.0781 - accuracy: 0.17 - ETA: 5:00 - loss: 3.0416 - accuracy: 0.18 - ETA: 5:00 - loss: 3.0092 - accuracy: 0.19 - ETA: 4:59 - loss: 3.0370 - accuracy: 0.18 - ETA: 5:05 - loss: 3.0429 - accuracy: 0.18 - ETA: 5:08 - loss: 3.0353 - accuracy: 0.18 - ETA: 5:10 - loss: 3.0113 - accuracy: 0.19 - ETA: 5:11 - loss: 3.0123 - accuracy: 0.19 - ETA: 5:08 - loss: 3.0184 - accuracy: 0.19 - ETA: 5:09 - loss: 3.0294 - accuracy: 0.18 - ETA: 5:08 - loss: 3.0441 - accuracy: 0.18 - ETA: 5:07 - loss: 3.0614 - accuracy: 0.18 - ETA: 5:05 - loss: 3.0692 - accuracy: 0.18 - ETA: 5:03 - loss: 3.0686 - accuracy: 0.18 - ETA: 5:01 - loss: 3.0649 - accuracy: 0.18 - ETA: 5:00 - loss: 3.0704 - accuracy: 0.18 - ETA: 4:59 - loss: 3.0678 - accuracy: 0.18 - ETA: 4:57 - loss: 3.0605 - accuracy: 0.18 - ETA: 4:57 - loss: 3.0546 - accuracy: 0.18 - ETA: 4:56 - loss: 3.0498 - accuracy: 0.18 - ETA: 4:55 - loss: 3.0509 - accuracy: 0.18 - ETA: 4:55 - loss: 3.0429 - accuracy: 0.18 - ETA: 4:53 - loss: 3.0425 - accuracy: 0.18 - ETA: 4:52 - loss: 3.0327 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0373 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0380 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0490 - accuracy: 0.18 - ETA: 4:47 - loss: 3.0437 - accuracy: 0.18 - ETA: 4:45 - loss: 3.0466 - accuracy: 0.18 - ETA: 4:44 - loss: 3.0428 - accuracy: 0.18 - ETA: 4:43 - loss: 3.0481 - accuracy: 0.18 - ETA: 4:42 - loss: 3.0482 - accuracy: 0.18 - ETA: 4:41 - loss: 3.0453 - accuracy: 0.18 - ETA: 4:40 - loss: 3.0498 - accuracy: 0.18 - ETA: 4:39 - loss: 3.0519 - accuracy: 0.18 - ETA: 4:39 - loss: 3.0572 - accuracy: 0.18 - ETA: 4:39 - loss: 3.0604 - accuracy: 0.18 - ETA: 4:38 - loss: 3.0595 - accuracy: 0.18 - ETA: 4:37 - loss: 3.0581 - accuracy: 0.18 - ETA: 4:36 - loss: 3.0600 - accuracy: 0.18 - ETA: 4:35 - loss: 3.0532 - accuracy: 0.18 - ETA: 4:34 - loss: 3.0493 - accuracy: 0.18 - ETA: 4:34 - loss: 3.0504 - accuracy: 0.18 - ETA: 4:32 - loss: 3.0480 - accuracy: 0.18 - ETA: 4:32 - loss: 3.0551 - accuracy: 0.18 - ETA: 4:31 - loss: 3.0546 - accuracy: 0.18 - ETA: 4:30 - loss: 3.0532 - accuracy: 0.18 - ETA: 4:29 - loss: 3.0497 - accuracy: 0.18 - ETA: 4:28 - loss: 3.0545 - accuracy: 0.18 - ETA: 4:28 - loss: 3.0534 - accuracy: 0.18 - ETA: 4:27 - loss: 3.0552 - accuracy: 0.18 - ETA: 4:26 - loss: 3.0521 - accuracy: 0.18 - ETA: 4:25 - loss: 3.0559 - accuracy: 0.18 - ETA: 4:24 - loss: 3.0575 - accuracy: 0.18 - ETA: 4:23 - loss: 3.0582 - accuracy: 0.18 - ETA: 4:22 - loss: 3.0560 - accuracy: 0.18 - ETA: 4:21 - loss: 3.0574 - accuracy: 0.18 - ETA: 4:20 - loss: 3.0600 - accuracy: 0.18 - ETA: 4:19 - loss: 3.0627 - accuracy: 0.18 - ETA: 4:18 - loss: 3.0629 - accuracy: 0.18 - ETA: 4:17 - loss: 3.0607 - accuracy: 0.18 - ETA: 4:15 - loss: 3.0608 - accuracy: 0.18 - ETA: 4:14 - loss: 3.0564 - accuracy: 0.18 - ETA: 4:14 - loss: 3.0574 - accuracy: 0.18 - ETA: 4:13 - loss: 3.0578 - accuracy: 0.18 - ETA: 4:12 - loss: 3.0563 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0544 - accuracy: 0.18 - ETA: 4:10 - loss: 3.0533 - accuracy: 0.18 - ETA: 4:09 - loss: 3.0530 - accuracy: 0.18 - ETA: 4:08 - loss: 3.0529 - accuracy: 0.18 - ETA: 4:07 - loss: 3.0505 - accuracy: 0.18 - ETA: 4:06 - loss: 3.0504 - accuracy: 0.18 - ETA: 4:05 - loss: 3.0484 - accuracy: 0.18 - ETA: 4:04 - loss: 3.0502 - accuracy: 0.18 - ETA: 4:03 - loss: 3.0496 - accuracy: 0.18 - ETA: 4:03 - loss: 3.0502 - accuracy: 0.18 - ETA: 4:02 - loss: 3.0492 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0518 - accuracy: 0.18 - ETA: 4:00 - loss: 3.0514 - accuracy: 0.18 - ETA: 3:59 - loss: 3.0501 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0502 - accuracy: 0.18 - ETA: 3:57 - loss: 3.0512 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0489 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0493 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0505 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0518 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0524 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0530 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0502 - accuracy: 0.18 - ETA: 3:49 - loss: 3.0501 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0529 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0518 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0525 - accuracy: 0.18 - ETA: 3:45 - loss: 3.0531 - accuracy: 0.18 - ETA: 3:45 - loss: 3.0542 - accuracy: 0.18 - ETA: 3:44 - loss: 3.0544 - accuracy: 0.18 - ETA: 3:43 - loss: 3.0546 - accuracy: 0.18 - ETA: 3:42 - loss: 3.0559 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0569 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0566 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0541 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0536 - accuracy: 0.18 - ETA: 3:37 - loss: 3.0540 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0542 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0535 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0544 - accuracy: 0.18 - ETA: 3:33 - loss: 3.0544 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0546 - accuracy: 0.18 - ETA: 3:31 - loss: 3.0554 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0528 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0541 - accuracy: 0.18 - ETA: 3:28 - loss: 3.0539 - accuracy: 0.18 - ETA: 3:27 - loss: 3.0532 - accuracy: 0.18 - ETA: 3:26 - loss: 3.0517 - accuracy: 0.18 - ETA: 3:25 - loss: 3.0503 - accuracy: 0.18 - ETA: 3:24 - loss: 3.0512 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0507 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0515 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0513 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0510 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0516 - accuracy: 0.18 - ETA: 3:19 - loss: 3.0502 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0521 - accuracy: 0.18 - ETA: 3:17 - loss: 3.0518 - accuracy: 0.18 - ETA: 3:16 - loss: 3.0527 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0505 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0512 - accuracy: 0.18 - ETA: 3:13 - loss: 3.0522 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0526 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0530 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0538 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0533 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0536 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0541 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0521 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0519 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0498 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0499 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0503 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0490 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0494 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0493 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0491 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0495 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0503 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0504 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0502 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0479 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0476 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0472 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0481 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0483 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0476 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0458 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0484 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0475 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0464 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0459 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0453 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0453 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0450 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0443 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0443 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0449 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0447 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0433 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0443 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0455 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0456 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0448 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0441 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0438 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0457 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0489 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0480 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0479 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0483 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0479 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0484 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0474 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0470 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0463 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0467 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0477 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0475 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0466 - accuracy: 0.1910"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0461 - accuracy: 0.19 - ETA: 2:16 - loss: 3.0466 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0460 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0450 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0449 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0441 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0441 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0440 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0429 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0435 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0430 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0431 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0435 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0434 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0442 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0451 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0450 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0448 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0449 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0453 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0451 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0441 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0443 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0445 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0445 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0443 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0442 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0440 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0438 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0437 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0432 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0433 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0438 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0437 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0436 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0429 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0437 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0428 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0436 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0429 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0423 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0419 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0430 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0433 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0429 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0425 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0430 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0431 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0429 - accuracy: 0.19 - ETA: 1:30 - loss: 3.0431 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0432 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0434 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0430 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0432 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0429 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0431 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0429 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0435 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0436 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0435 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0437 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0429 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0432 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0426 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0422 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0423 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0416 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0417 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0406 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0404 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0401 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0404 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0407 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0402 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0401 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0396 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0397 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0397 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0398 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0399 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0395 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0389 - accuracy: 0.19 - ETA: 59s - loss: 3.0392 - accuracy: 0.1920 - ETA: 58s - loss: 3.0391 - accuracy: 0.192 - ETA: 57s - loss: 3.0393 - accuracy: 0.191 - ETA: 56s - loss: 3.0390 - accuracy: 0.192 - ETA: 55s - loss: 3.0389 - accuracy: 0.192 - ETA: 54s - loss: 3.0381 - accuracy: 0.192 - ETA: 53s - loss: 3.0373 - accuracy: 0.192 - ETA: 52s - loss: 3.0369 - accuracy: 0.192 - ETA: 51s - loss: 3.0367 - accuracy: 0.192 - ETA: 50s - loss: 3.0364 - accuracy: 0.192 - ETA: 49s - loss: 3.0366 - accuracy: 0.192 - ETA: 48s - loss: 3.0361 - accuracy: 0.192 - ETA: 47s - loss: 3.0358 - accuracy: 0.192 - ETA: 46s - loss: 3.0354 - accuracy: 0.192 - ETA: 45s - loss: 3.0356 - accuracy: 0.192 - ETA: 44s - loss: 3.0355 - accuracy: 0.192 - ETA: 43s - loss: 3.0360 - accuracy: 0.192 - ETA: 42s - loss: 3.0361 - accuracy: 0.192 - ETA: 42s - loss: 3.0357 - accuracy: 0.192 - ETA: 41s - loss: 3.0365 - accuracy: 0.192 - ETA: 40s - loss: 3.0361 - accuracy: 0.192 - ETA: 39s - loss: 3.0364 - accuracy: 0.192 - ETA: 38s - loss: 3.0364 - accuracy: 0.192 - ETA: 37s - loss: 3.0357 - accuracy: 0.192 - ETA: 36s - loss: 3.0367 - accuracy: 0.192 - ETA: 35s - loss: 3.0359 - accuracy: 0.192 - ETA: 34s - loss: 3.0350 - accuracy: 0.192 - ETA: 33s - loss: 3.0340 - accuracy: 0.192 - ETA: 32s - loss: 3.0338 - accuracy: 0.192 - ETA: 31s - loss: 3.0335 - accuracy: 0.192 - ETA: 30s - loss: 3.0342 - accuracy: 0.192 - ETA: 29s - loss: 3.0336 - accuracy: 0.192 - ETA: 28s - loss: 3.0334 - accuracy: 0.192 - ETA: 27s - loss: 3.0334 - accuracy: 0.192 - ETA: 26s - loss: 3.0335 - accuracy: 0.192 - ETA: 25s - loss: 3.0337 - accuracy: 0.192 - ETA: 24s - loss: 3.0338 - accuracy: 0.192 - ETA: 23s - loss: 3.0332 - accuracy: 0.192 - ETA: 22s - loss: 3.0341 - accuracy: 0.192 - ETA: 21s - loss: 3.0339 - accuracy: 0.192 - ETA: 21s - loss: 3.0342 - accuracy: 0.192 - ETA: 20s - loss: 3.0343 - accuracy: 0.192 - ETA: 19s - loss: 3.0349 - accuracy: 0.192 - ETA: 18s - loss: 3.0352 - accuracy: 0.192 - ETA: 17s - loss: 3.0353 - accuracy: 0.192 - ETA: 16s - loss: 3.0353 - accuracy: 0.192 - ETA: 15s - loss: 3.0355 - accuracy: 0.192 - ETA: 14s - loss: 3.0355 - accuracy: 0.192 - ETA: 13s - loss: 3.0359 - accuracy: 0.192 - ETA: 12s - loss: 3.0359 - accuracy: 0.191 - ETA: 11s - loss: 3.0366 - accuracy: 0.191 - ETA: 10s - loss: 3.0366 - accuracy: 0.191 - ETA: 9s - loss: 3.0364 - accuracy: 0.192 - ETA: 8s - loss: 3.0363 - accuracy: 0.19 - ETA: 7s - loss: 3.0361 - accuracy: 0.19 - ETA: 6s - loss: 3.0365 - accuracy: 0.19 - ETA: 5s - loss: 3.0380 - accuracy: 0.19 - ETA: 4s - loss: 3.0379 - accuracy: 0.19 - ETA: 3s - loss: 3.0379 - accuracy: 0.19 - ETA: 2s - loss: 3.0372 - accuracy: 0.19 - ETA: 1s - loss: 3.0373 - accuracy: 0.19 - ETA: 1s - loss: 3.0377 - accuracy: 0.19 - ETA: 0s - loss: 3.0380 - accuracy: 0.19 - 339s 8ms/step - loss: 3.0380 - accuracy: 0.1922 - val_loss: 4.1511 - val_accuracy: 0.0351\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:32 - loss: 3.1093 - accuracy: 0.16 - ETA: 5:24 - loss: 2.9932 - accuracy: 0.20 - ETA: 5:22 - loss: 3.0285 - accuracy: 0.19 - ETA: 5:24 - loss: 3.0217 - accuracy: 0.19 - ETA: 5:22 - loss: 3.0352 - accuracy: 0.19 - ETA: 5:19 - loss: 3.0276 - accuracy: 0.19 - ETA: 5:17 - loss: 3.0310 - accuracy: 0.19 - ETA: 5:17 - loss: 3.0380 - accuracy: 0.19 - ETA: 5:15 - loss: 3.0889 - accuracy: 0.19 - ETA: 5:13 - loss: 3.1077 - accuracy: 0.19 - ETA: 5:12 - loss: 3.1083 - accuracy: 0.19 - ETA: 5:10 - loss: 3.0989 - accuracy: 0.19 - ETA: 5:08 - loss: 3.0842 - accuracy: 0.19 - ETA: 5:08 - loss: 3.0664 - accuracy: 0.19 - ETA: 5:06 - loss: 3.0443 - accuracy: 0.20 - ETA: 5:04 - loss: 3.0461 - accuracy: 0.20 - ETA: 5:04 - loss: 3.0500 - accuracy: 0.19 - ETA: 5:03 - loss: 3.0378 - accuracy: 0.20 - ETA: 5:01 - loss: 3.0343 - accuracy: 0.20 - ETA: 5:00 - loss: 3.0341 - accuracy: 0.20 - ETA: 4:59 - loss: 3.0278 - accuracy: 0.20 - ETA: 4:58 - loss: 3.0243 - accuracy: 0.20 - ETA: 4:57 - loss: 3.0244 - accuracy: 0.20 - ETA: 4:56 - loss: 3.0220 - accuracy: 0.20 - ETA: 4:54 - loss: 3.0228 - accuracy: 0.20 - ETA: 4:53 - loss: 3.0284 - accuracy: 0.20 - ETA: 4:52 - loss: 3.0299 - accuracy: 0.20 - ETA: 4:51 - loss: 3.0267 - accuracy: 0.20 - ETA: 4:50 - loss: 3.0350 - accuracy: 0.20 - ETA: 4:49 - loss: 3.0350 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0397 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0411 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0412 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0395 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0368 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0368 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0375 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0413 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0420 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0393 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0436 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0392 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0352 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0313 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0315 - accuracy: 0.20 - ETA: 4:32 - loss: 3.0269 - accuracy: 0.20 - ETA: 4:32 - loss: 3.0319 - accuracy: 0.20 - ETA: 4:30 - loss: 3.0311 - accuracy: 0.20 - ETA: 4:29 - loss: 3.0300 - accuracy: 0.20 - ETA: 4:29 - loss: 3.0261 - accuracy: 0.20 - ETA: 4:27 - loss: 3.0240 - accuracy: 0.20 - ETA: 4:26 - loss: 3.0266 - accuracy: 0.20 - ETA: 4:25 - loss: 3.0364 - accuracy: 0.20 - ETA: 4:24 - loss: 3.0356 - accuracy: 0.20 - ETA: 4:24 - loss: 3.0329 - accuracy: 0.20 - ETA: 4:23 - loss: 3.0372 - accuracy: 0.20 - ETA: 4:21 - loss: 3.0369 - accuracy: 0.20 - ETA: 4:20 - loss: 3.0387 - accuracy: 0.20 - ETA: 4:20 - loss: 3.0381 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0394 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0386 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0384 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0396 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0402 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0425 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0436 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0456 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0460 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0467 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0466 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0481 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0482 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0484 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0450 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0452 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0489 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0493 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0507 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0502 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0506 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0524 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0551 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0563 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0571 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0564 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0537 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0540 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0526 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0538 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0552 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0562 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0556 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0548 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0544 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0549 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0523 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0506 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0502 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0533 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0529 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0532 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0536 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0541 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0546 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0558 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0551 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0555 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0571 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0578 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0565 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0551 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0542 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0540 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0544 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0546 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0545 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0526 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0525 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0530 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0545 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0549 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0552 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0546 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0554 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0562 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0574 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0589 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0595 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0599 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0589 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0582 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0585 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0599 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0603 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0595 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0605 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0604 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0601 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0616 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0617 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0628 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0624 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0641 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0642 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0658 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0674 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0682 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0684 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0688 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0695 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0704 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0710 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0698 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0693 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0701 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0711 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0718 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0732 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0729 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0851 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0864 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0865 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0852 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0851 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0843 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0847 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0831 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0832 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0835 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0833 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0828 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0829 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0825 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0809 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0806 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0813 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0804 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0814 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0818 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0814 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0824 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0820 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0807 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0809 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0805 - accuracy: 0.19 - ETA: 2:17 - loss: 3.0808 - accuracy: 0.1900"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0813 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0820 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0814 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0812 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0809 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0810 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0797 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0797 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0806 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0799 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0793 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0791 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0794 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0793 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0789 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0788 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0786 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0794 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0796 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0799 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0786 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0784 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0774 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0768 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0779 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0784 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0793 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0798 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0799 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0797 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0792 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0798 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0790 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0792 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0790 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0786 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0780 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0783 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0789 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0788 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0781 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0778 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0767 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0763 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0768 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0773 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0781 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0770 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0766 - accuracy: 0.19 - ETA: 1:30 - loss: 3.0771 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0781 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0783 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0785 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0785 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0785 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0785 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0787 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0784 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0774 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0776 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0770 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0768 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0770 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0768 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0760 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0757 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0759 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0771 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0773 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0766 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0758 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0753 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0758 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0756 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0752 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0750 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0740 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0730 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0730 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0727 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0729 - accuracy: 0.19 - ETA: 59s - loss: 3.0732 - accuracy: 0.1925 - ETA: 58s - loss: 3.0740 - accuracy: 0.192 - ETA: 57s - loss: 3.0734 - accuracy: 0.192 - ETA: 56s - loss: 3.0733 - accuracy: 0.192 - ETA: 56s - loss: 3.0729 - accuracy: 0.192 - ETA: 55s - loss: 3.0723 - accuracy: 0.192 - ETA: 54s - loss: 3.0720 - accuracy: 0.192 - ETA: 53s - loss: 3.0718 - accuracy: 0.192 - ETA: 52s - loss: 3.0724 - accuracy: 0.192 - ETA: 51s - loss: 3.0722 - accuracy: 0.192 - ETA: 50s - loss: 3.0725 - accuracy: 0.192 - ETA: 49s - loss: 3.0729 - accuracy: 0.192 - ETA: 48s - loss: 3.0722 - accuracy: 0.192 - ETA: 47s - loss: 3.0724 - accuracy: 0.192 - ETA: 46s - loss: 3.0724 - accuracy: 0.192 - ETA: 45s - loss: 3.0725 - accuracy: 0.192 - ETA: 44s - loss: 3.0728 - accuracy: 0.192 - ETA: 43s - loss: 3.0719 - accuracy: 0.192 - ETA: 42s - loss: 3.0721 - accuracy: 0.192 - ETA: 41s - loss: 3.0719 - accuracy: 0.192 - ETA: 40s - loss: 3.0724 - accuracy: 0.191 - ETA: 39s - loss: 3.0724 - accuracy: 0.191 - ETA: 38s - loss: 3.0724 - accuracy: 0.191 - ETA: 38s - loss: 3.0720 - accuracy: 0.191 - ETA: 37s - loss: 3.0715 - accuracy: 0.191 - ETA: 36s - loss: 3.0714 - accuracy: 0.191 - ETA: 35s - loss: 3.0718 - accuracy: 0.191 - ETA: 34s - loss: 3.0720 - accuracy: 0.191 - ETA: 33s - loss: 3.0714 - accuracy: 0.191 - ETA: 32s - loss: 3.0713 - accuracy: 0.191 - ETA: 31s - loss: 3.0717 - accuracy: 0.191 - ETA: 30s - loss: 3.0718 - accuracy: 0.191 - ETA: 29s - loss: 3.0733 - accuracy: 0.191 - ETA: 28s - loss: 3.0739 - accuracy: 0.191 - ETA: 27s - loss: 3.0738 - accuracy: 0.191 - ETA: 26s - loss: 3.0742 - accuracy: 0.191 - ETA: 25s - loss: 3.0741 - accuracy: 0.191 - ETA: 24s - loss: 3.0742 - accuracy: 0.191 - ETA: 23s - loss: 3.0745 - accuracy: 0.191 - ETA: 22s - loss: 3.0739 - accuracy: 0.191 - ETA: 21s - loss: 3.0742 - accuracy: 0.191 - ETA: 20s - loss: 3.0741 - accuracy: 0.191 - ETA: 20s - loss: 3.0734 - accuracy: 0.191 - ETA: 19s - loss: 3.0728 - accuracy: 0.191 - ETA: 18s - loss: 3.0728 - accuracy: 0.191 - ETA: 17s - loss: 3.0732 - accuracy: 0.191 - ETA: 16s - loss: 3.0728 - accuracy: 0.191 - ETA: 15s - loss: 3.0732 - accuracy: 0.191 - ETA: 14s - loss: 3.0726 - accuracy: 0.191 - ETA: 13s - loss: 3.0725 - accuracy: 0.191 - ETA: 12s - loss: 3.0723 - accuracy: 0.191 - ETA: 11s - loss: 3.0722 - accuracy: 0.191 - ETA: 10s - loss: 3.0724 - accuracy: 0.191 - ETA: 9s - loss: 3.0724 - accuracy: 0.191 - ETA: 8s - loss: 3.0719 - accuracy: 0.19 - ETA: 7s - loss: 3.0717 - accuracy: 0.19 - ETA: 6s - loss: 3.0711 - accuracy: 0.19 - ETA: 5s - loss: 3.0711 - accuracy: 0.19 - ETA: 4s - loss: 3.0710 - accuracy: 0.19 - ETA: 3s - loss: 3.0712 - accuracy: 0.19 - ETA: 2s - loss: 3.0716 - accuracy: 0.19 - ETA: 1s - loss: 3.0715 - accuracy: 0.19 - ETA: 1s - loss: 3.0713 - accuracy: 0.19 - ETA: 0s - loss: 3.0707 - accuracy: 0.19 - 338s 8ms/step - loss: 3.0706 - accuracy: 0.1915 - val_loss: 4.1690 - val_accuracy: 0.0358\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:04 - loss: 2.9140 - accuracy: 0.21 - ETA: 4:58 - loss: 2.9535 - accuracy: 0.20 - ETA: 4:57 - loss: 2.9151 - accuracy: 0.21 - ETA: 4:59 - loss: 2.9926 - accuracy: 0.20 - ETA: 5:02 - loss: 2.9528 - accuracy: 0.21 - ETA: 4:59 - loss: 2.9748 - accuracy: 0.20 - ETA: 4:59 - loss: 3.0338 - accuracy: 0.19 - ETA: 4:57 - loss: 3.0526 - accuracy: 0.19 - ETA: 4:57 - loss: 3.0453 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0631 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0478 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0552 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0689 - accuracy: 0.19 - ETA: 4:57 - loss: 3.0569 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0591 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0653 - accuracy: 0.19 - ETA: 4:54 - loss: 3.0631 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0770 - accuracy: 0.18 - ETA: 4:51 - loss: 3.0729 - accuracy: 0.18 - ETA: 4:51 - loss: 3.0781 - accuracy: 0.18 - ETA: 4:51 - loss: 3.0780 - accuracy: 0.18 - ETA: 4:50 - loss: 3.0719 - accuracy: 0.18 - ETA: 4:49 - loss: 3.0621 - accuracy: 0.18 - ETA: 4:48 - loss: 3.0586 - accuracy: 0.18 - ETA: 4:47 - loss: 3.0649 - accuracy: 0.18 - ETA: 4:46 - loss: 3.0657 - accuracy: 0.18 - ETA: 4:46 - loss: 3.0702 - accuracy: 0.18 - ETA: 4:45 - loss: 3.0713 - accuracy: 0.18 - ETA: 4:44 - loss: 3.0689 - accuracy: 0.18 - ETA: 4:43 - loss: 3.0596 - accuracy: 0.18 - ETA: 4:42 - loss: 3.0615 - accuracy: 0.18 - ETA: 4:41 - loss: 3.0567 - accuracy: 0.18 - ETA: 4:40 - loss: 3.0558 - accuracy: 0.18 - ETA: 4:39 - loss: 3.0476 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0473 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0523 - accuracy: 0.18 - ETA: 4:36 - loss: 3.0555 - accuracy: 0.18 - ETA: 4:35 - loss: 3.0567 - accuracy: 0.18 - ETA: 4:34 - loss: 3.0574 - accuracy: 0.18 - ETA: 4:33 - loss: 3.0604 - accuracy: 0.18 - ETA: 4:32 - loss: 3.0670 - accuracy: 0.18 - ETA: 4:32 - loss: 3.0654 - accuracy: 0.18 - ETA: 4:30 - loss: 3.0662 - accuracy: 0.18 - ETA: 4:29 - loss: 3.0639 - accuracy: 0.18 - ETA: 4:28 - loss: 3.0637 - accuracy: 0.18 - ETA: 4:27 - loss: 3.0630 - accuracy: 0.18 - ETA: 4:26 - loss: 3.0620 - accuracy: 0.18 - ETA: 4:25 - loss: 3.0636 - accuracy: 0.18 - ETA: 4:24 - loss: 3.0667 - accuracy: 0.18 - ETA: 4:23 - loss: 3.0682 - accuracy: 0.18 - ETA: 4:22 - loss: 3.0622 - accuracy: 0.18 - ETA: 4:21 - loss: 3.0637 - accuracy: 0.18 - ETA: 4:20 - loss: 3.0622 - accuracy: 0.18 - ETA: 4:19 - loss: 3.0627 - accuracy: 0.18 - ETA: 4:19 - loss: 3.0649 - accuracy: 0.18 - ETA: 4:18 - loss: 3.0629 - accuracy: 0.18 - ETA: 4:17 - loss: 3.0602 - accuracy: 0.18 - ETA: 4:17 - loss: 3.0613 - accuracy: 0.18 - ETA: 4:16 - loss: 3.0608 - accuracy: 0.18 - ETA: 4:15 - loss: 3.0624 - accuracy: 0.18 - ETA: 4:14 - loss: 3.0620 - accuracy: 0.18 - ETA: 4:13 - loss: 3.0643 - accuracy: 0.18 - ETA: 4:13 - loss: 3.0626 - accuracy: 0.18 - ETA: 4:12 - loss: 3.0614 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0633 - accuracy: 0.18 - ETA: 4:10 - loss: 3.0642 - accuracy: 0.18 - ETA: 4:09 - loss: 3.0641 - accuracy: 0.18 - ETA: 4:08 - loss: 3.0631 - accuracy: 0.18 - ETA: 4:07 - loss: 3.0640 - accuracy: 0.18 - ETA: 4:06 - loss: 3.0631 - accuracy: 0.18 - ETA: 4:05 - loss: 3.0634 - accuracy: 0.18 - ETA: 4:04 - loss: 3.0623 - accuracy: 0.18 - ETA: 4:04 - loss: 3.0607 - accuracy: 0.18 - ETA: 4:02 - loss: 3.0603 - accuracy: 0.18 - ETA: 4:01 - loss: 3.0606 - accuracy: 0.18 - ETA: 4:01 - loss: 3.0609 - accuracy: 0.18 - ETA: 4:00 - loss: 3.0629 - accuracy: 0.18 - ETA: 3:59 - loss: 3.0650 - accuracy: 0.18 - ETA: 3:58 - loss: 3.0629 - accuracy: 0.18 - ETA: 3:57 - loss: 3.0622 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0613 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0617 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0627 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0634 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0646 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0629 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0611 - accuracy: 0.18 - ETA: 3:49 - loss: 3.0619 - accuracy: 0.18 - ETA: 3:48 - loss: 3.0613 - accuracy: 0.18 - ETA: 3:47 - loss: 3.0620 - accuracy: 0.18 - ETA: 3:46 - loss: 3.0621 - accuracy: 0.18 - ETA: 3:45 - loss: 3.0616 - accuracy: 0.18 - ETA: 3:44 - loss: 3.0604 - accuracy: 0.18 - ETA: 3:43 - loss: 3.0595 - accuracy: 0.18 - ETA: 3:42 - loss: 3.0580 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0593 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0596 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0585 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0567 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0566 - accuracy: 0.18 - ETA: 3:37 - loss: 3.0582 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0584 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0583 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0584 - accuracy: 0.18 - ETA: 3:33 - loss: 3.0580 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0575 - accuracy: 0.18 - ETA: 3:31 - loss: 3.0582 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0587 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0595 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0578 - accuracy: 0.18 - ETA: 3:28 - loss: 3.0589 - accuracy: 0.18 - ETA: 3:27 - loss: 3.0588 - accuracy: 0.18 - ETA: 3:26 - loss: 3.0579 - accuracy: 0.18 - ETA: 3:25 - loss: 3.0565 - accuracy: 0.18 - ETA: 3:24 - loss: 3.0569 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0578 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0578 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0568 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0564 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0561 - accuracy: 0.18 - ETA: 3:19 - loss: 3.0577 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0561 - accuracy: 0.18 - ETA: 3:17 - loss: 3.0576 - accuracy: 0.18 - ETA: 3:16 - loss: 3.0569 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0577 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0576 - accuracy: 0.18 - ETA: 3:13 - loss: 3.0581 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0576 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0574 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0564 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0580 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0580 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0567 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0580 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0594 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0617 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0615 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0616 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0651 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0659 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0663 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0654 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0673 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0676 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0668 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0660 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0671 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0676 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0675 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0701 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0708 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0715 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0728 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0752 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0768 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0764 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0767 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0764 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0766 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0766 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0761 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0760 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0758 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0746 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0740 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0729 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0723 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0725 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0733 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0733 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0741 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0739 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0738 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0742 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0751 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0751 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0748 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0746 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0751 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0763 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0768 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0769 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0775 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0781 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0796 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0789 - accuracy: 0.1864"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0801 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0812 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0815 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0817 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0817 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0820 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0813 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0825 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0837 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0832 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0824 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0828 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0837 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0831 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0833 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0821 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0820 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0809 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0805 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0799 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0802 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0808 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0799 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0797 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0794 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0799 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0815 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0820 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0820 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0820 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0816 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0812 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0812 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0806 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0807 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0806 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0805 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0799 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0800 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0804 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0796 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0792 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0774 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0783 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0783 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0777 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0781 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0800 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0799 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0797 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0799 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0805 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0800 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0807 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0807 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0806 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0813 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0813 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0809 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0811 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0809 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0816 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0815 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0804 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0798 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0795 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0798 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0800 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0803 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0799 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0803 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0805 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0805 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0811 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0806 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0797 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0794 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0794 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0783 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0787 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0785 - accuracy: 0.18 - ETA: 59s - loss: 3.0786 - accuracy: 0.1876 - ETA: 58s - loss: 3.0788 - accuracy: 0.187 - ETA: 57s - loss: 3.0776 - accuracy: 0.187 - ETA: 56s - loss: 3.0778 - accuracy: 0.187 - ETA: 55s - loss: 3.0776 - accuracy: 0.187 - ETA: 54s - loss: 3.0779 - accuracy: 0.187 - ETA: 53s - loss: 3.0777 - accuracy: 0.187 - ETA: 53s - loss: 3.0779 - accuracy: 0.187 - ETA: 52s - loss: 3.0780 - accuracy: 0.187 - ETA: 51s - loss: 3.0775 - accuracy: 0.187 - ETA: 50s - loss: 3.0786 - accuracy: 0.187 - ETA: 49s - loss: 3.0779 - accuracy: 0.187 - ETA: 48s - loss: 3.0788 - accuracy: 0.187 - ETA: 47s - loss: 3.0788 - accuracy: 0.187 - ETA: 46s - loss: 3.0792 - accuracy: 0.187 - ETA: 45s - loss: 3.0790 - accuracy: 0.187 - ETA: 44s - loss: 3.0790 - accuracy: 0.187 - ETA: 43s - loss: 3.0794 - accuracy: 0.187 - ETA: 42s - loss: 3.0796 - accuracy: 0.187 - ETA: 41s - loss: 3.0796 - accuracy: 0.187 - ETA: 40s - loss: 3.0785 - accuracy: 0.187 - ETA: 39s - loss: 3.0785 - accuracy: 0.187 - ETA: 38s - loss: 3.0787 - accuracy: 0.187 - ETA: 37s - loss: 3.0786 - accuracy: 0.187 - ETA: 36s - loss: 3.0787 - accuracy: 0.187 - ETA: 35s - loss: 3.0792 - accuracy: 0.187 - ETA: 35s - loss: 3.0803 - accuracy: 0.187 - ETA: 34s - loss: 3.0806 - accuracy: 0.187 - ETA: 33s - loss: 3.0805 - accuracy: 0.187 - ETA: 32s - loss: 3.0808 - accuracy: 0.187 - ETA: 31s - loss: 3.0806 - accuracy: 0.187 - ETA: 30s - loss: 3.0804 - accuracy: 0.187 - ETA: 29s - loss: 3.0799 - accuracy: 0.187 - ETA: 28s - loss: 3.0788 - accuracy: 0.187 - ETA: 27s - loss: 3.0786 - accuracy: 0.187 - ETA: 26s - loss: 3.0788 - accuracy: 0.187 - ETA: 25s - loss: 3.0798 - accuracy: 0.187 - ETA: 24s - loss: 3.0804 - accuracy: 0.187 - ETA: 23s - loss: 3.0795 - accuracy: 0.187 - ETA: 22s - loss: 3.0795 - accuracy: 0.187 - ETA: 21s - loss: 3.0798 - accuracy: 0.187 - ETA: 20s - loss: 3.0800 - accuracy: 0.186 - ETA: 19s - loss: 3.0799 - accuracy: 0.187 - ETA: 18s - loss: 3.0790 - accuracy: 0.187 - ETA: 18s - loss: 3.0788 - accuracy: 0.187 - ETA: 17s - loss: 3.0786 - accuracy: 0.187 - ETA: 16s - loss: 3.0790 - accuracy: 0.187 - ETA: 15s - loss: 3.0783 - accuracy: 0.187 - ETA: 14s - loss: 3.0785 - accuracy: 0.187 - ETA: 13s - loss: 3.0782 - accuracy: 0.187 - ETA: 12s - loss: 3.0782 - accuracy: 0.187 - ETA: 11s - loss: 3.0782 - accuracy: 0.187 - ETA: 10s - loss: 3.0780 - accuracy: 0.187 - ETA: 9s - loss: 3.0782 - accuracy: 0.187 - ETA: 8s - loss: 3.0790 - accuracy: 0.18 - ETA: 7s - loss: 3.0789 - accuracy: 0.18 - ETA: 6s - loss: 3.0794 - accuracy: 0.18 - ETA: 5s - loss: 3.0795 - accuracy: 0.18 - ETA: 4s - loss: 3.0798 - accuracy: 0.18 - ETA: 3s - loss: 3.0801 - accuracy: 0.18 - ETA: 2s - loss: 3.0803 - accuracy: 0.18 - ETA: 1s - loss: 3.0804 - accuracy: 0.18 - ETA: 1s - loss: 3.0799 - accuracy: 0.18 - ETA: 0s - loss: 3.0807 - accuracy: 0.18 - 336s 8ms/step - loss: 3.0809 - accuracy: 0.1864 - val_loss: 4.0349 - val_accuracy: 0.0314\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:59 - loss: 3.1295 - accuracy: 0.16 - ETA: 4:57 - loss: 3.1360 - accuracy: 0.17 - ETA: 5:00 - loss: 3.1508 - accuracy: 0.18 - ETA: 5:02 - loss: 3.1706 - accuracy: 0.18 - ETA: 5:02 - loss: 3.1494 - accuracy: 0.18 - ETA: 4:59 - loss: 3.1578 - accuracy: 0.18 - ETA: 4:58 - loss: 3.1621 - accuracy: 0.17 - ETA: 5:01 - loss: 3.1813 - accuracy: 0.17 - ETA: 5:01 - loss: 3.1850 - accuracy: 0.17 - ETA: 5:03 - loss: 3.1934 - accuracy: 0.16 - ETA: 5:03 - loss: 3.1967 - accuracy: 0.16 - ETA: 5:03 - loss: 3.1999 - accuracy: 0.16 - ETA: 5:01 - loss: 3.1925 - accuracy: 0.16 - ETA: 5:00 - loss: 3.1978 - accuracy: 0.16 - ETA: 4:58 - loss: 3.2054 - accuracy: 0.16 - ETA: 4:57 - loss: 3.2045 - accuracy: 0.16 - ETA: 4:57 - loss: 3.2151 - accuracy: 0.15 - ETA: 4:55 - loss: 3.2125 - accuracy: 0.15 - ETA: 4:54 - loss: 3.2103 - accuracy: 0.15 - ETA: 4:53 - loss: 3.2184 - accuracy: 0.15 - ETA: 4:53 - loss: 3.2178 - accuracy: 0.15 - ETA: 4:52 - loss: 3.2150 - accuracy: 0.15 - ETA: 4:50 - loss: 3.2211 - accuracy: 0.16 - ETA: 4:49 - loss: 3.2195 - accuracy: 0.16 - ETA: 4:48 - loss: 3.2224 - accuracy: 0.16 - ETA: 4:47 - loss: 3.2238 - accuracy: 0.16 - ETA: 4:46 - loss: 3.2289 - accuracy: 0.16 - ETA: 4:45 - loss: 3.2375 - accuracy: 0.16 - ETA: 4:45 - loss: 3.2296 - accuracy: 0.16 - ETA: 4:43 - loss: 3.2267 - accuracy: 0.16 - ETA: 4:43 - loss: 3.2217 - accuracy: 0.16 - ETA: 4:41 - loss: 3.2183 - accuracy: 0.16 - ETA: 4:40 - loss: 3.2142 - accuracy: 0.16 - ETA: 4:39 - loss: 3.2080 - accuracy: 0.16 - ETA: 4:38 - loss: 3.2069 - accuracy: 0.16 - ETA: 4:37 - loss: 3.2094 - accuracy: 0.16 - ETA: 4:37 - loss: 3.2138 - accuracy: 0.16 - ETA: 4:36 - loss: 3.2088 - accuracy: 0.16 - ETA: 4:35 - loss: 3.2118 - accuracy: 0.16 - ETA: 4:34 - loss: 3.2085 - accuracy: 0.16 - ETA: 4:34 - loss: 3.2122 - accuracy: 0.16 - ETA: 4:32 - loss: 3.2129 - accuracy: 0.16 - ETA: 4:31 - loss: 3.2073 - accuracy: 0.16 - ETA: 4:31 - loss: 3.2059 - accuracy: 0.16 - ETA: 4:30 - loss: 3.1992 - accuracy: 0.16 - ETA: 4:29 - loss: 3.1947 - accuracy: 0.16 - ETA: 4:28 - loss: 3.1920 - accuracy: 0.16 - ETA: 4:27 - loss: 3.1924 - accuracy: 0.16 - ETA: 4:27 - loss: 3.2030 - accuracy: 0.16 - ETA: 4:26 - loss: 3.2035 - accuracy: 0.16 - ETA: 4:25 - loss: 3.2039 - accuracy: 0.16 - ETA: 4:24 - loss: 3.2061 - accuracy: 0.16 - ETA: 4:24 - loss: 3.2014 - accuracy: 0.16 - ETA: 4:22 - loss: 3.2008 - accuracy: 0.17 - ETA: 4:22 - loss: 3.2035 - accuracy: 0.17 - ETA: 4:21 - loss: 3.2035 - accuracy: 0.17 - ETA: 4:20 - loss: 3.2063 - accuracy: 0.17 - ETA: 4:19 - loss: 3.2077 - accuracy: 0.17 - ETA: 4:18 - loss: 3.2075 - accuracy: 0.17 - ETA: 4:17 - loss: 3.2056 - accuracy: 0.17 - ETA: 4:16 - loss: 3.2014 - accuracy: 0.17 - ETA: 4:15 - loss: 3.2011 - accuracy: 0.17 - ETA: 4:14 - loss: 3.1996 - accuracy: 0.17 - ETA: 4:13 - loss: 3.2032 - accuracy: 0.17 - ETA: 4:12 - loss: 3.2029 - accuracy: 0.17 - ETA: 4:11 - loss: 3.2049 - accuracy: 0.17 - ETA: 4:11 - loss: 3.2049 - accuracy: 0.17 - ETA: 4:10 - loss: 3.2014 - accuracy: 0.17 - ETA: 4:09 - loss: 3.2016 - accuracy: 0.17 - ETA: 4:07 - loss: 3.2002 - accuracy: 0.17 - ETA: 4:07 - loss: 3.2004 - accuracy: 0.17 - ETA: 4:06 - loss: 3.1999 - accuracy: 0.17 - ETA: 4:05 - loss: 3.1981 - accuracy: 0.17 - ETA: 4:04 - loss: 3.1985 - accuracy: 0.17 - ETA: 4:02 - loss: 3.1955 - accuracy: 0.17 - ETA: 4:02 - loss: 3.1950 - accuracy: 0.17 - ETA: 4:01 - loss: 3.1947 - accuracy: 0.17 - ETA: 4:00 - loss: 3.1957 - accuracy: 0.17 - ETA: 3:59 - loss: 3.1963 - accuracy: 0.17 - ETA: 3:58 - loss: 3.1995 - accuracy: 0.17 - ETA: 3:57 - loss: 3.1991 - accuracy: 0.17 - ETA: 3:56 - loss: 3.1962 - accuracy: 0.17 - ETA: 3:55 - loss: 3.1962 - accuracy: 0.17 - ETA: 3:54 - loss: 3.1967 - accuracy: 0.17 - ETA: 3:53 - loss: 3.1942 - accuracy: 0.17 - ETA: 3:52 - loss: 3.2005 - accuracy: 0.17 - ETA: 3:51 - loss: 3.1985 - accuracy: 0.17 - ETA: 3:50 - loss: 3.1973 - accuracy: 0.17 - ETA: 3:49 - loss: 3.1949 - accuracy: 0.17 - ETA: 3:48 - loss: 3.1948 - accuracy: 0.17 - ETA: 3:47 - loss: 3.1942 - accuracy: 0.17 - ETA: 3:46 - loss: 3.1934 - accuracy: 0.17 - ETA: 3:45 - loss: 3.1925 - accuracy: 0.17 - ETA: 3:44 - loss: 3.1930 - accuracy: 0.17 - ETA: 3:43 - loss: 3.1917 - accuracy: 0.17 - ETA: 3:42 - loss: 3.1915 - accuracy: 0.17 - ETA: 3:41 - loss: 3.1908 - accuracy: 0.17 - ETA: 3:40 - loss: 3.1909 - accuracy: 0.17 - ETA: 3:39 - loss: 3.1906 - accuracy: 0.17 - ETA: 3:38 - loss: 3.1913 - accuracy: 0.17 - ETA: 3:37 - loss: 3.1919 - accuracy: 0.17 - ETA: 3:36 - loss: 3.1926 - accuracy: 0.17 - ETA: 3:35 - loss: 3.1915 - accuracy: 0.17 - ETA: 3:35 - loss: 3.1924 - accuracy: 0.17 - ETA: 3:34 - loss: 3.1930 - accuracy: 0.17 - ETA: 3:33 - loss: 3.1930 - accuracy: 0.17 - ETA: 3:32 - loss: 3.1919 - accuracy: 0.17 - ETA: 3:31 - loss: 3.1902 - accuracy: 0.17 - ETA: 3:30 - loss: 3.1906 - accuracy: 0.17 - ETA: 3:29 - loss: 3.1890 - accuracy: 0.17 - ETA: 3:28 - loss: 3.1883 - accuracy: 0.17 - ETA: 3:27 - loss: 3.1876 - accuracy: 0.17 - ETA: 3:26 - loss: 3.1870 - accuracy: 0.17 - ETA: 3:25 - loss: 3.1856 - accuracy: 0.17 - ETA: 3:24 - loss: 3.1852 - accuracy: 0.17 - ETA: 3:23 - loss: 3.1853 - accuracy: 0.17 - ETA: 3:22 - loss: 3.1869 - accuracy: 0.17 - ETA: 3:21 - loss: 3.1863 - accuracy: 0.17 - ETA: 3:20 - loss: 3.1850 - accuracy: 0.17 - ETA: 3:20 - loss: 3.1846 - accuracy: 0.17 - ETA: 3:19 - loss: 3.1848 - accuracy: 0.17 - ETA: 3:18 - loss: 3.1846 - accuracy: 0.17 - ETA: 3:17 - loss: 3.1852 - accuracy: 0.17 - ETA: 3:16 - loss: 3.1850 - accuracy: 0.17 - ETA: 3:15 - loss: 3.1864 - accuracy: 0.17 - ETA: 3:14 - loss: 3.1850 - accuracy: 0.17 - ETA: 3:13 - loss: 3.1847 - accuracy: 0.17 - ETA: 3:12 - loss: 3.1830 - accuracy: 0.17 - ETA: 3:11 - loss: 3.1815 - accuracy: 0.17 - ETA: 3:10 - loss: 3.1807 - accuracy: 0.17 - ETA: 3:09 - loss: 3.1800 - accuracy: 0.17 - ETA: 3:08 - loss: 3.1797 - accuracy: 0.17 - ETA: 3:07 - loss: 3.1781 - accuracy: 0.17 - ETA: 3:07 - loss: 3.1778 - accuracy: 0.17 - ETA: 3:06 - loss: 3.1791 - accuracy: 0.17 - ETA: 3:05 - loss: 3.1790 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1787 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1776 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1762 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1764 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1763 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1763 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1759 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1749 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1735 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1739 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1748 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1752 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1757 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1744 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1739 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1742 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1720 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1706 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1697 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1682 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1677 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1668 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1669 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1675 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1668 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1655 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1636 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1632 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1624 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1620 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1608 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1613 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1619 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1614 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1610 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1594 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1598 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1595 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1593 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1595 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1597 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1589 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1586 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1587 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1584 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1567 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1550 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1547 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1537 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1538 - accuracy: 0.1770"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.1525 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1531 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1528 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1518 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1519 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1514 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1517 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1511 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1510 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1507 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1502 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1499 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1499 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1498 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1493 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1483 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1476 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1471 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1466 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1457 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1455 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1444 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1440 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1429 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1424 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1421 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1413 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1408 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1408 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1414 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1415 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1419 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1419 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1431 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1424 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1422 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1428 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1429 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1418 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1421 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1426 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1426 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1429 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1424 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1416 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1420 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1415 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1412 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1418 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1420 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1414 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1412 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1397 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1396 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1396 - accuracy: 0.17 - ETA: 1:24 - loss: 3.1401 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1401 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1398 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1394 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1388 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1390 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1386 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1379 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1383 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1383 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1383 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1375 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1373 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1365 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1358 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1353 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1349 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1358 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1347 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1349 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1341 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1347 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1342 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1344 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1343 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1355 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1353 - accuracy: 0.17 - ETA: 59s - loss: 3.1355 - accuracy: 0.1790 - ETA: 58s - loss: 3.1356 - accuracy: 0.178 - ETA: 57s - loss: 3.1357 - accuracy: 0.179 - ETA: 56s - loss: 3.1349 - accuracy: 0.179 - ETA: 55s - loss: 3.1345 - accuracy: 0.179 - ETA: 54s - loss: 3.1341 - accuracy: 0.179 - ETA: 53s - loss: 3.1337 - accuracy: 0.179 - ETA: 52s - loss: 3.1334 - accuracy: 0.179 - ETA: 51s - loss: 3.1331 - accuracy: 0.179 - ETA: 50s - loss: 3.1321 - accuracy: 0.179 - ETA: 49s - loss: 3.1317 - accuracy: 0.179 - ETA: 48s - loss: 3.1316 - accuracy: 0.179 - ETA: 47s - loss: 3.1315 - accuracy: 0.179 - ETA: 46s - loss: 3.1314 - accuracy: 0.179 - ETA: 45s - loss: 3.1313 - accuracy: 0.179 - ETA: 44s - loss: 3.1309 - accuracy: 0.179 - ETA: 43s - loss: 3.1301 - accuracy: 0.179 - ETA: 42s - loss: 3.1303 - accuracy: 0.179 - ETA: 41s - loss: 3.1293 - accuracy: 0.179 - ETA: 40s - loss: 3.1292 - accuracy: 0.179 - ETA: 40s - loss: 3.1290 - accuracy: 0.179 - ETA: 39s - loss: 3.1295 - accuracy: 0.179 - ETA: 38s - loss: 3.1313 - accuracy: 0.179 - ETA: 37s - loss: 3.1306 - accuracy: 0.179 - ETA: 36s - loss: 3.1304 - accuracy: 0.179 - ETA: 35s - loss: 3.1298 - accuracy: 0.179 - ETA: 34s - loss: 3.1295 - accuracy: 0.179 - ETA: 33s - loss: 3.1298 - accuracy: 0.179 - ETA: 32s - loss: 3.1291 - accuracy: 0.179 - ETA: 31s - loss: 3.1289 - accuracy: 0.179 - ETA: 30s - loss: 3.1280 - accuracy: 0.179 - ETA: 29s - loss: 3.1278 - accuracy: 0.179 - ETA: 28s - loss: 3.1271 - accuracy: 0.180 - ETA: 27s - loss: 3.1268 - accuracy: 0.180 - ETA: 26s - loss: 3.1264 - accuracy: 0.180 - ETA: 25s - loss: 3.1261 - accuracy: 0.180 - ETA: 24s - loss: 3.1260 - accuracy: 0.180 - ETA: 23s - loss: 3.1255 - accuracy: 0.180 - ETA: 22s - loss: 3.1252 - accuracy: 0.180 - ETA: 21s - loss: 3.1248 - accuracy: 0.180 - ETA: 21s - loss: 3.1245 - accuracy: 0.180 - ETA: 20s - loss: 3.1240 - accuracy: 0.180 - ETA: 19s - loss: 3.1237 - accuracy: 0.181 - ETA: 18s - loss: 3.1240 - accuracy: 0.180 - ETA: 17s - loss: 3.1238 - accuracy: 0.180 - ETA: 16s - loss: 3.1241 - accuracy: 0.180 - ETA: 15s - loss: 3.1240 - accuracy: 0.180 - ETA: 14s - loss: 3.1240 - accuracy: 0.181 - ETA: 13s - loss: 3.1235 - accuracy: 0.181 - ETA: 12s - loss: 3.1229 - accuracy: 0.181 - ETA: 11s - loss: 3.1232 - accuracy: 0.181 - ETA: 10s - loss: 3.1227 - accuracy: 0.181 - ETA: 9s - loss: 3.1228 - accuracy: 0.181 - ETA: 8s - loss: 3.1225 - accuracy: 0.18 - ETA: 7s - loss: 3.1219 - accuracy: 0.18 - ETA: 6s - loss: 3.1219 - accuracy: 0.18 - ETA: 5s - loss: 3.1247 - accuracy: 0.18 - ETA: 4s - loss: 3.1243 - accuracy: 0.18 - ETA: 3s - loss: 3.1232 - accuracy: 0.18 - ETA: 2s - loss: 3.1235 - accuracy: 0.18 - ETA: 1s - loss: 3.1226 - accuracy: 0.18 - ETA: 1s - loss: 3.1231 - accuracy: 0.18 - ETA: 0s - loss: 3.1224 - accuracy: 0.18 - 338s 8ms/step - loss: 3.1222 - accuracy: 0.1820 - val_loss: 4.0435 - val_accuracy: 0.0379\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:20 - loss: 2.9788 - accuracy: 0.21 - ETA: 5:11 - loss: 2.9131 - accuracy: 0.22 - ETA: 5:07 - loss: 2.9790 - accuracy: 0.21 - ETA: 5:07 - loss: 2.9657 - accuracy: 0.20 - ETA: 5:09 - loss: 3.0055 - accuracy: 0.19 - ETA: 5:08 - loss: 3.0189 - accuracy: 0.18 - ETA: 5:08 - loss: 3.0269 - accuracy: 0.18 - ETA: 5:06 - loss: 3.0591 - accuracy: 0.18 - ETA: 5:08 - loss: 3.0535 - accuracy: 0.18 - ETA: 5:06 - loss: 3.0583 - accuracy: 0.18 - ETA: 5:04 - loss: 3.0576 - accuracy: 0.17 - ETA: 5:03 - loss: 3.0625 - accuracy: 0.17 - ETA: 5:02 - loss: 3.0635 - accuracy: 0.17 - ETA: 5:01 - loss: 3.0911 - accuracy: 0.17 - ETA: 5:00 - loss: 3.0806 - accuracy: 0.17 - ETA: 4:58 - loss: 3.0802 - accuracy: 0.17 - ETA: 4:57 - loss: 3.0775 - accuracy: 0.17 - ETA: 4:56 - loss: 3.0746 - accuracy: 0.18 - ETA: 4:54 - loss: 3.0666 - accuracy: 0.18 - ETA: 4:53 - loss: 3.0634 - accuracy: 0.18 - ETA: 4:53 - loss: 3.0657 - accuracy: 0.17 - ETA: 4:52 - loss: 3.0511 - accuracy: 0.18 - ETA: 4:51 - loss: 3.0520 - accuracy: 0.18 - ETA: 4:50 - loss: 3.0426 - accuracy: 0.18 - ETA: 4:49 - loss: 3.0450 - accuracy: 0.18 - ETA: 4:48 - loss: 3.0562 - accuracy: 0.18 - ETA: 4:47 - loss: 3.0572 - accuracy: 0.18 - ETA: 4:46 - loss: 3.0534 - accuracy: 0.18 - ETA: 4:45 - loss: 3.0512 - accuracy: 0.18 - ETA: 4:44 - loss: 3.0533 - accuracy: 0.18 - ETA: 4:43 - loss: 3.0518 - accuracy: 0.18 - ETA: 4:42 - loss: 3.0544 - accuracy: 0.18 - ETA: 4:41 - loss: 3.0596 - accuracy: 0.18 - ETA: 4:40 - loss: 3.0582 - accuracy: 0.18 - ETA: 4:39 - loss: 3.0531 - accuracy: 0.18 - ETA: 4:38 - loss: 3.0518 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0521 - accuracy: 0.18 - ETA: 4:37 - loss: 3.0552 - accuracy: 0.18 - ETA: 4:37 - loss: 3.0565 - accuracy: 0.18 - ETA: 4:36 - loss: 3.0598 - accuracy: 0.18 - ETA: 4:35 - loss: 3.0658 - accuracy: 0.18 - ETA: 4:34 - loss: 3.0682 - accuracy: 0.18 - ETA: 4:33 - loss: 3.0660 - accuracy: 0.18 - ETA: 4:32 - loss: 3.0660 - accuracy: 0.18 - ETA: 4:31 - loss: 3.0674 - accuracy: 0.18 - ETA: 4:30 - loss: 3.0652 - accuracy: 0.18 - ETA: 4:29 - loss: 3.0653 - accuracy: 0.18 - ETA: 4:28 - loss: 3.0647 - accuracy: 0.18 - ETA: 4:27 - loss: 3.0625 - accuracy: 0.18 - ETA: 4:26 - loss: 3.0606 - accuracy: 0.18 - ETA: 4:25 - loss: 3.0635 - accuracy: 0.18 - ETA: 4:24 - loss: 3.0675 - accuracy: 0.18 - ETA: 4:23 - loss: 3.0685 - accuracy: 0.18 - ETA: 4:23 - loss: 3.0661 - accuracy: 0.18 - ETA: 4:22 - loss: 3.0686 - accuracy: 0.18 - ETA: 4:21 - loss: 3.0661 - accuracy: 0.18 - ETA: 4:20 - loss: 3.0659 - accuracy: 0.18 - ETA: 4:19 - loss: 3.0661 - accuracy: 0.18 - ETA: 4:18 - loss: 3.0648 - accuracy: 0.18 - ETA: 4:17 - loss: 3.0650 - accuracy: 0.18 - ETA: 4:16 - loss: 3.0641 - accuracy: 0.18 - ETA: 4:15 - loss: 3.0630 - accuracy: 0.18 - ETA: 4:14 - loss: 3.0622 - accuracy: 0.18 - ETA: 4:14 - loss: 3.0620 - accuracy: 0.18 - ETA: 4:13 - loss: 3.0617 - accuracy: 0.18 - ETA: 4:12 - loss: 3.0645 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0647 - accuracy: 0.18 - ETA: 4:10 - loss: 3.0655 - accuracy: 0.18 - ETA: 4:09 - loss: 3.0675 - accuracy: 0.18 - ETA: 4:08 - loss: 3.0715 - accuracy: 0.18 - ETA: 4:07 - loss: 3.0722 - accuracy: 0.18 - ETA: 4:06 - loss: 3.0749 - accuracy: 0.18 - ETA: 4:05 - loss: 3.0744 - accuracy: 0.18 - ETA: 4:04 - loss: 3.0715 - accuracy: 0.18 - ETA: 4:03 - loss: 3.0707 - accuracy: 0.18 - ETA: 4:02 - loss: 3.0717 - accuracy: 0.18 - ETA: 4:01 - loss: 3.0708 - accuracy: 0.18 - ETA: 4:00 - loss: 3.0716 - accuracy: 0.18 - ETA: 3:59 - loss: 3.0703 - accuracy: 0.18 - ETA: 3:58 - loss: 3.0671 - accuracy: 0.18 - ETA: 3:57 - loss: 3.0659 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0668 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0655 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0648 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0653 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0637 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0642 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0617 - accuracy: 0.18 - ETA: 3:49 - loss: 3.0608 - accuracy: 0.18 - ETA: 3:48 - loss: 3.0564 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0561 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0553 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0553 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0553 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0541 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0565 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0552 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0546 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0543 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0517 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0513 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0517 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0522 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0509 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0513 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0523 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0532 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0497 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0489 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0500 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0503 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0498 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0556 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0536 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0537 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0537 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0545 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0535 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0540 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0523 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0548 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0548 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0562 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0555 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0576 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0577 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0589 - accuracy: 0.18 - ETA: 3:13 - loss: 3.0569 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0561 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0569 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0549 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0550 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0556 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0543 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0528 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0526 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0521 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0531 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0529 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0513 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0515 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0509 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0518 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0505 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0505 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0502 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0506 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0509 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0500 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0509 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0500 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0485 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0484 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0471 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0485 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0486 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0500 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0503 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0498 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0499 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0503 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0507 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0515 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0516 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0522 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0514 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0528 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0531 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0520 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0523 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0519 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0508 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0513 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0507 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0519 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0511 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0517 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0530 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0538 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0540 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0534 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0522 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0519 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0509 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0514 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0504 - accuracy: 0.1897"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0496 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0485 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0477 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0471 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0469 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0464 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0463 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0450 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0452 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0466 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0476 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0477 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0479 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0473 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0466 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0466 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0462 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0459 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0455 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0455 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0444 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0444 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0434 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0445 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0451 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0437 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0435 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0443 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0442 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0431 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0432 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0440 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0446 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0440 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0442 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0441 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0447 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0454 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0450 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0446 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0448 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0445 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0441 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0438 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0436 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0439 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0435 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0432 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0423 - accuracy: 0.19 - ETA: 1:30 - loss: 3.0422 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0418 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0407 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0405 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0403 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0403 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0405 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0394 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0401 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0398 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0398 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0399 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0398 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0398 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0405 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0410 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0410 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0406 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0407 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0413 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0418 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0420 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0417 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0420 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0420 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0419 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0422 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0421 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0416 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0420 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0425 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0426 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0430 - accuracy: 0.19 - ETA: 59s - loss: 3.0427 - accuracy: 0.1932 - ETA: 58s - loss: 3.0425 - accuracy: 0.193 - ETA: 57s - loss: 3.0435 - accuracy: 0.192 - ETA: 56s - loss: 3.0431 - accuracy: 0.192 - ETA: 55s - loss: 3.0435 - accuracy: 0.192 - ETA: 54s - loss: 3.0438 - accuracy: 0.192 - ETA: 53s - loss: 3.0434 - accuracy: 0.192 - ETA: 52s - loss: 3.0435 - accuracy: 0.192 - ETA: 51s - loss: 3.0433 - accuracy: 0.192 - ETA: 50s - loss: 3.0430 - accuracy: 0.192 - ETA: 49s - loss: 3.0429 - accuracy: 0.192 - ETA: 48s - loss: 3.0426 - accuracy: 0.192 - ETA: 47s - loss: 3.0426 - accuracy: 0.192 - ETA: 46s - loss: 3.0419 - accuracy: 0.192 - ETA: 45s - loss: 3.0423 - accuracy: 0.192 - ETA: 44s - loss: 3.0427 - accuracy: 0.192 - ETA: 43s - loss: 3.0426 - accuracy: 0.192 - ETA: 42s - loss: 3.0426 - accuracy: 0.192 - ETA: 41s - loss: 3.0433 - accuracy: 0.192 - ETA: 41s - loss: 3.0438 - accuracy: 0.192 - ETA: 40s - loss: 3.0437 - accuracy: 0.192 - ETA: 39s - loss: 3.0437 - accuracy: 0.192 - ETA: 38s - loss: 3.0438 - accuracy: 0.192 - ETA: 37s - loss: 3.0440 - accuracy: 0.192 - ETA: 36s - loss: 3.0447 - accuracy: 0.191 - ETA: 35s - loss: 3.0447 - accuracy: 0.191 - ETA: 34s - loss: 3.0443 - accuracy: 0.191 - ETA: 33s - loss: 3.0436 - accuracy: 0.191 - ETA: 32s - loss: 3.0433 - accuracy: 0.191 - ETA: 31s - loss: 3.0428 - accuracy: 0.192 - ETA: 30s - loss: 3.0427 - accuracy: 0.192 - ETA: 29s - loss: 3.0425 - accuracy: 0.192 - ETA: 28s - loss: 3.0434 - accuracy: 0.191 - ETA: 27s - loss: 3.0431 - accuracy: 0.191 - ETA: 26s - loss: 3.0432 - accuracy: 0.192 - ETA: 25s - loss: 3.0437 - accuracy: 0.192 - ETA: 24s - loss: 3.0429 - accuracy: 0.192 - ETA: 23s - loss: 3.0443 - accuracy: 0.191 - ETA: 22s - loss: 3.0454 - accuracy: 0.191 - ETA: 21s - loss: 3.0457 - accuracy: 0.191 - ETA: 21s - loss: 3.0456 - accuracy: 0.191 - ETA: 20s - loss: 3.0460 - accuracy: 0.191 - ETA: 19s - loss: 3.0457 - accuracy: 0.191 - ETA: 18s - loss: 3.0452 - accuracy: 0.191 - ETA: 17s - loss: 3.0454 - accuracy: 0.191 - ETA: 16s - loss: 3.0453 - accuracy: 0.191 - ETA: 15s - loss: 3.0446 - accuracy: 0.191 - ETA: 14s - loss: 3.0444 - accuracy: 0.191 - ETA: 13s - loss: 3.0443 - accuracy: 0.191 - ETA: 12s - loss: 3.0444 - accuracy: 0.191 - ETA: 11s - loss: 3.0447 - accuracy: 0.191 - ETA: 10s - loss: 3.0451 - accuracy: 0.191 - ETA: 9s - loss: 3.0453 - accuracy: 0.191 - ETA: 8s - loss: 3.0448 - accuracy: 0.19 - ETA: 7s - loss: 3.0445 - accuracy: 0.19 - ETA: 6s - loss: 3.0442 - accuracy: 0.19 - ETA: 5s - loss: 3.0439 - accuracy: 0.19 - ETA: 4s - loss: 3.0441 - accuracy: 0.19 - ETA: 3s - loss: 3.0444 - accuracy: 0.19 - ETA: 2s - loss: 3.0443 - accuracy: 0.19 - ETA: 1s - loss: 3.0444 - accuracy: 0.19 - ETA: 1s - loss: 3.0444 - accuracy: 0.19 - ETA: 0s - loss: 3.0441 - accuracy: 0.19 - 338s 8ms/step - loss: 3.0442 - accuracy: 0.1916 - val_loss: 4.1714 - val_accuracy: 0.0364\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:38 - loss: 3.0147 - accuracy: 0.18 - ETA: 5:26 - loss: 3.0099 - accuracy: 0.18 - ETA: 5:19 - loss: 2.9561 - accuracy: 0.20 - ETA: 5:16 - loss: 3.0296 - accuracy: 0.20 - ETA: 5:15 - loss: 3.1452 - accuracy: 0.18 - ETA: 5:13 - loss: 3.1506 - accuracy: 0.18 - ETA: 5:11 - loss: 3.1785 - accuracy: 0.19 - ETA: 5:09 - loss: 3.1760 - accuracy: 0.19 - ETA: 5:08 - loss: 3.1746 - accuracy: 0.19 - ETA: 5:06 - loss: 3.1618 - accuracy: 0.19 - ETA: 5:05 - loss: 3.1501 - accuracy: 0.19 - ETA: 5:04 - loss: 3.1359 - accuracy: 0.19 - ETA: 5:02 - loss: 3.1202 - accuracy: 0.19 - ETA: 5:00 - loss: 3.1020 - accuracy: 0.20 - ETA: 4:58 - loss: 3.1034 - accuracy: 0.20 - ETA: 4:57 - loss: 3.0997 - accuracy: 0.20 - ETA: 4:56 - loss: 3.1039 - accuracy: 0.19 - ETA: 4:55 - loss: 3.1026 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0936 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0886 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0811 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0885 - accuracy: 0.18 - ETA: 4:50 - loss: 3.0809 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0787 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0761 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0767 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0728 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0662 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0603 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0616 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0600 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0658 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0618 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0673 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0620 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0632 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0664 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0642 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0647 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0676 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0667 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0647 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0678 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0639 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0598 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0611 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0651 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0652 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0646 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0657 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0632 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0606 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0621 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0619 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0620 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0623 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0608 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0613 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0636 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0655 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0644 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0615 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0626 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0629 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0632 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0646 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0653 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0667 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0649 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0689 - accuracy: 0.18 - ETA: 4:05 - loss: 3.0684 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0681 - accuracy: 0.18 - ETA: 4:03 - loss: 3.0664 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0672 - accuracy: 0.18 - ETA: 4:01 - loss: 3.0682 - accuracy: 0.18 - ETA: 4:00 - loss: 3.0665 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0655 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0661 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0699 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0696 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0693 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0705 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0728 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0736 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0731 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0709 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0727 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0729 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0717 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0733 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0744 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0762 - accuracy: 0.18 - ETA: 3:44 - loss: 3.0760 - accuracy: 0.18 - ETA: 3:43 - loss: 3.0773 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0769 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0777 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0790 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0802 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0792 - accuracy: 0.18 - ETA: 3:37 - loss: 3.0802 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0808 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0814 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0806 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0799 - accuracy: 0.18 - ETA: 3:33 - loss: 3.0799 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0790 - accuracy: 0.18 - ETA: 3:31 - loss: 3.0793 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0782 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0758 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0746 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0750 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0763 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0750 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0752 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0749 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0743 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0751 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0761 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0778 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0790 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0793 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0789 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0774 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0786 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0803 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0794 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0780 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0795 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0807 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0819 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0794 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0797 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0788 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0788 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0772 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0781 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0764 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0774 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0778 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0760 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0764 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0757 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0781 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0797 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0794 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0786 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0800 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0789 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0783 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0811 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0795 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0793 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0790 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0781 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0795 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0803 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0802 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0794 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0809 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0816 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0819 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0824 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0818 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0808 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0802 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0796 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0802 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0792 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0797 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0783 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0788 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0792 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0794 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0789 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0789 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0795 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0791 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0789 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0785 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0791 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0779 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0777 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0772 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0770 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0766 - accuracy: 0.19 - ETA: 2:17 - loss: 3.0765 - accuracy: 0.1905"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0763 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0765 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0757 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0758 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0761 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0776 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0760 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0771 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0762 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0764 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0760 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0767 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0768 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0766 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0774 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0772 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0768 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0760 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0750 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0740 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0729 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0722 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0725 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0735 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0738 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0736 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0733 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0736 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0739 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0729 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0731 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0727 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0730 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0737 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0740 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0742 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0744 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0734 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0723 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0725 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0732 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0732 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0731 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0733 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0728 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0721 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0720 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0719 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0712 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0712 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0711 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0712 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0716 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0712 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0709 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0712 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0713 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0718 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0712 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0715 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0712 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0712 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0701 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0705 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0703 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0694 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0688 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0687 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0683 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0680 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0676 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0683 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0680 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0677 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0672 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0667 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0667 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0661 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0666 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0669 - accuracy: 0.19 - ETA: 59s - loss: 3.0664 - accuracy: 0.1904 - ETA: 58s - loss: 3.0657 - accuracy: 0.190 - ETA: 57s - loss: 3.0657 - accuracy: 0.190 - ETA: 56s - loss: 3.0658 - accuracy: 0.190 - ETA: 55s - loss: 3.0655 - accuracy: 0.190 - ETA: 54s - loss: 3.0650 - accuracy: 0.190 - ETA: 54s - loss: 3.0659 - accuracy: 0.190 - ETA: 53s - loss: 3.0654 - accuracy: 0.190 - ETA: 52s - loss: 3.0651 - accuracy: 0.190 - ETA: 51s - loss: 3.0653 - accuracy: 0.190 - ETA: 50s - loss: 3.0654 - accuracy: 0.190 - ETA: 49s - loss: 3.0653 - accuracy: 0.190 - ETA: 48s - loss: 3.0650 - accuracy: 0.190 - ETA: 47s - loss: 3.0651 - accuracy: 0.190 - ETA: 46s - loss: 3.0656 - accuracy: 0.190 - ETA: 45s - loss: 3.0649 - accuracy: 0.190 - ETA: 44s - loss: 3.0642 - accuracy: 0.191 - ETA: 43s - loss: 3.0638 - accuracy: 0.191 - ETA: 42s - loss: 3.0639 - accuracy: 0.191 - ETA: 41s - loss: 3.0642 - accuracy: 0.190 - ETA: 40s - loss: 3.0637 - accuracy: 0.191 - ETA: 39s - loss: 3.0636 - accuracy: 0.191 - ETA: 38s - loss: 3.0634 - accuracy: 0.191 - ETA: 37s - loss: 3.0631 - accuracy: 0.191 - ETA: 36s - loss: 3.0636 - accuracy: 0.190 - ETA: 36s - loss: 3.0641 - accuracy: 0.190 - ETA: 35s - loss: 3.0646 - accuracy: 0.190 - ETA: 34s - loss: 3.0646 - accuracy: 0.190 - ETA: 33s - loss: 3.0633 - accuracy: 0.191 - ETA: 32s - loss: 3.0628 - accuracy: 0.191 - ETA: 31s - loss: 3.0630 - accuracy: 0.191 - ETA: 30s - loss: 3.0632 - accuracy: 0.191 - ETA: 29s - loss: 3.0632 - accuracy: 0.191 - ETA: 28s - loss: 3.0635 - accuracy: 0.191 - ETA: 27s - loss: 3.0638 - accuracy: 0.191 - ETA: 26s - loss: 3.0638 - accuracy: 0.191 - ETA: 25s - loss: 3.0644 - accuracy: 0.190 - ETA: 24s - loss: 3.0646 - accuracy: 0.190 - ETA: 23s - loss: 3.0645 - accuracy: 0.190 - ETA: 22s - loss: 3.0647 - accuracy: 0.190 - ETA: 21s - loss: 3.0645 - accuracy: 0.190 - ETA: 20s - loss: 3.0643 - accuracy: 0.191 - ETA: 19s - loss: 3.0643 - accuracy: 0.191 - ETA: 19s - loss: 3.0636 - accuracy: 0.191 - ETA: 18s - loss: 3.0630 - accuracy: 0.191 - ETA: 17s - loss: 3.0631 - accuracy: 0.191 - ETA: 16s - loss: 3.0636 - accuracy: 0.191 - ETA: 15s - loss: 3.0636 - accuracy: 0.191 - ETA: 14s - loss: 3.0632 - accuracy: 0.191 - ETA: 13s - loss: 3.0639 - accuracy: 0.191 - ETA: 12s - loss: 3.0641 - accuracy: 0.191 - ETA: 11s - loss: 3.0640 - accuracy: 0.191 - ETA: 10s - loss: 3.0635 - accuracy: 0.191 - ETA: 9s - loss: 3.0634 - accuracy: 0.191 - ETA: 8s - loss: 3.0636 - accuracy: 0.19 - ETA: 7s - loss: 3.0639 - accuracy: 0.19 - ETA: 6s - loss: 3.0641 - accuracy: 0.19 - ETA: 5s - loss: 3.0641 - accuracy: 0.19 - ETA: 4s - loss: 3.0638 - accuracy: 0.19 - ETA: 3s - loss: 3.0638 - accuracy: 0.19 - ETA: 2s - loss: 3.0640 - accuracy: 0.19 - ETA: 1s - loss: 3.0638 - accuracy: 0.19 - ETA: 1s - loss: 3.0642 - accuracy: 0.19 - ETA: 0s - loss: 3.0642 - accuracy: 0.19 - 337s 8ms/step - loss: 3.0642 - accuracy: 0.1915 - val_loss: 4.1398 - val_accuracy: 0.0423\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:36 - loss: 3.1494 - accuracy: 0.15 - ETA: 5:21 - loss: 3.1065 - accuracy: 0.18 - ETA: 5:11 - loss: 3.0983 - accuracy: 0.19 - ETA: 5:10 - loss: 3.0702 - accuracy: 0.20 - ETA: 5:12 - loss: 3.0790 - accuracy: 0.20 - ETA: 5:12 - loss: 3.0761 - accuracy: 0.19 - ETA: 5:12 - loss: 3.1168 - accuracy: 0.18 - ETA: 5:11 - loss: 3.1281 - accuracy: 0.18 - ETA: 5:13 - loss: 3.1231 - accuracy: 0.17 - ETA: 5:13 - loss: 3.1135 - accuracy: 0.18 - ETA: 5:12 - loss: 3.0836 - accuracy: 0.19 - ETA: 5:11 - loss: 3.0862 - accuracy: 0.19 - ETA: 5:08 - loss: 3.0701 - accuracy: 0.19 - ETA: 5:07 - loss: 3.0518 - accuracy: 0.19 - ETA: 5:05 - loss: 3.0594 - accuracy: 0.19 - ETA: 5:04 - loss: 3.0709 - accuracy: 0.18 - ETA: 5:01 - loss: 3.0657 - accuracy: 0.19 - ETA: 5:00 - loss: 3.0549 - accuracy: 0.19 - ETA: 4:59 - loss: 3.0556 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0497 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0464 - accuracy: 0.19 - ETA: 4:57 - loss: 3.0393 - accuracy: 0.19 - ETA: 4:57 - loss: 3.0376 - accuracy: 0.19 - ETA: 4:57 - loss: 3.0389 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0383 - accuracy: 0.19 - ETA: 4:55 - loss: 3.0321 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0375 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0387 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0422 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0435 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0416 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0438 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0435 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0453 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0438 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0403 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0364 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0296 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0321 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0354 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0339 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0329 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0309 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0322 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0350 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0329 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0340 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0327 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0348 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0358 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0328 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0319 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0294 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0302 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0274 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0252 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0222 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0218 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0204 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0220 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0207 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0212 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0201 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0196 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0194 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0196 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0202 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0180 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0172 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0178 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0195 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0227 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0226 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0226 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0249 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0230 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0243 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0215 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0226 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0230 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0271 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0266 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0282 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0308 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0319 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0330 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0338 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0331 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0340 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0365 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0370 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0384 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0375 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0354 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0352 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0345 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0356 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0376 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0383 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0399 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0416 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0412 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0432 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0422 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0429 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0415 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0394 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0399 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0409 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0404 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0423 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0441 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0445 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0435 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0448 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0430 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0442 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0452 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0453 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0455 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0446 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0456 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0443 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0451 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0463 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0446 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0556 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0555 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0557 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0561 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0553 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0554 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0544 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0544 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0532 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0536 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0538 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0543 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0539 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0536 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0535 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0546 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0562 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0553 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0550 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0540 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0537 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0533 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0531 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0533 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0540 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0532 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0518 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0526 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0512 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0504 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0508 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0495 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0493 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0487 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0489 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0476 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0475 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0474 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0480 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0478 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0476 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0481 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0485 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0499 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0519 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0553 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0547 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0545 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0538 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0540 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0533 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0532 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0544 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0550 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0544 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0565 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0559 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0563 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0549 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0551 - accuracy: 0.1943"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0551 - accuracy: 0.19 - ETA: 2:16 - loss: 3.0549 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0550 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0544 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0540 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0554 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0570 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0568 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0567 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0571 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0570 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0577 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0575 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0571 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0570 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0561 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0547 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0551 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0550 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0549 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0546 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0533 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0537 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0536 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0536 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0534 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0533 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0542 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0543 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0538 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0546 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0550 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0555 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0548 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0549 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0554 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0543 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0533 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0538 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0532 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0537 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0541 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0547 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0550 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0548 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0550 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0549 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0546 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0541 - accuracy: 0.19 - ETA: 1:30 - loss: 3.0535 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0526 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0520 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0516 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0519 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0517 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0511 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0504 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0505 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0515 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0522 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0533 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0530 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0527 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0528 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0526 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0525 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0530 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0524 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0525 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0537 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0540 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0533 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0535 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0537 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0534 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0538 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0541 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0539 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0534 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0533 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0530 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0533 - accuracy: 0.19 - ETA: 59s - loss: 3.0535 - accuracy: 0.1946 - ETA: 58s - loss: 3.0535 - accuracy: 0.194 - ETA: 57s - loss: 3.0539 - accuracy: 0.194 - ETA: 56s - loss: 3.0562 - accuracy: 0.194 - ETA: 55s - loss: 3.0556 - accuracy: 0.194 - ETA: 54s - loss: 3.0555 - accuracy: 0.194 - ETA: 53s - loss: 3.0556 - accuracy: 0.194 - ETA: 52s - loss: 3.0555 - accuracy: 0.194 - ETA: 51s - loss: 3.0548 - accuracy: 0.194 - ETA: 50s - loss: 3.0546 - accuracy: 0.194 - ETA: 49s - loss: 3.0547 - accuracy: 0.194 - ETA: 48s - loss: 3.0543 - accuracy: 0.194 - ETA: 47s - loss: 3.0543 - accuracy: 0.194 - ETA: 46s - loss: 3.0545 - accuracy: 0.195 - ETA: 45s - loss: 3.0549 - accuracy: 0.194 - ETA: 44s - loss: 3.0545 - accuracy: 0.195 - ETA: 43s - loss: 3.0542 - accuracy: 0.195 - ETA: 43s - loss: 3.0546 - accuracy: 0.195 - ETA: 42s - loss: 3.0537 - accuracy: 0.195 - ETA: 41s - loss: 3.0539 - accuracy: 0.195 - ETA: 40s - loss: 3.0531 - accuracy: 0.195 - ETA: 39s - loss: 3.0530 - accuracy: 0.195 - ETA: 38s - loss: 3.0526 - accuracy: 0.195 - ETA: 37s - loss: 3.0529 - accuracy: 0.195 - ETA: 36s - loss: 3.0527 - accuracy: 0.195 - ETA: 35s - loss: 3.0528 - accuracy: 0.195 - ETA: 34s - loss: 3.0530 - accuracy: 0.194 - ETA: 33s - loss: 3.0537 - accuracy: 0.194 - ETA: 32s - loss: 3.0533 - accuracy: 0.194 - ETA: 31s - loss: 3.0534 - accuracy: 0.194 - ETA: 30s - loss: 3.0542 - accuracy: 0.194 - ETA: 29s - loss: 3.0540 - accuracy: 0.194 - ETA: 28s - loss: 3.0530 - accuracy: 0.195 - ETA: 27s - loss: 3.0527 - accuracy: 0.195 - ETA: 26s - loss: 3.0525 - accuracy: 0.194 - ETA: 25s - loss: 3.0524 - accuracy: 0.194 - ETA: 24s - loss: 3.0528 - accuracy: 0.195 - ETA: 23s - loss: 3.0530 - accuracy: 0.195 - ETA: 22s - loss: 3.0529 - accuracy: 0.195 - ETA: 22s - loss: 3.0531 - accuracy: 0.194 - ETA: 21s - loss: 3.0530 - accuracy: 0.194 - ETA: 20s - loss: 3.0528 - accuracy: 0.194 - ETA: 19s - loss: 3.0528 - accuracy: 0.194 - ETA: 18s - loss: 3.0526 - accuracy: 0.194 - ETA: 17s - loss: 3.0527 - accuracy: 0.194 - ETA: 16s - loss: 3.0532 - accuracy: 0.194 - ETA: 15s - loss: 3.0536 - accuracy: 0.194 - ETA: 14s - loss: 3.0532 - accuracy: 0.194 - ETA: 13s - loss: 3.0535 - accuracy: 0.194 - ETA: 12s - loss: 3.0545 - accuracy: 0.194 - ETA: 11s - loss: 3.0543 - accuracy: 0.194 - ETA: 10s - loss: 3.0540 - accuracy: 0.194 - ETA: 9s - loss: 3.0533 - accuracy: 0.194 - ETA: 8s - loss: 3.0532 - accuracy: 0.19 - ETA: 7s - loss: 3.0530 - accuracy: 0.19 - ETA: 6s - loss: 3.0532 - accuracy: 0.19 - ETA: 5s - loss: 3.0530 - accuracy: 0.19 - ETA: 4s - loss: 3.0527 - accuracy: 0.19 - ETA: 3s - loss: 3.0528 - accuracy: 0.19 - ETA: 2s - loss: 3.0521 - accuracy: 0.19 - ETA: 1s - loss: 3.0522 - accuracy: 0.19 - ETA: 1s - loss: 3.0517 - accuracy: 0.19 - ETA: 0s - loss: 3.0519 - accuracy: 0.19 - 339s 8ms/step - loss: 3.0518 - accuracy: 0.1948 - val_loss: 4.1366 - val_accuracy: 0.0356\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:25 - loss: 2.8710 - accuracy: 0.21 - ETA: 5:21 - loss: 2.9960 - accuracy: 0.20 - ETA: 5:20 - loss: 2.9639 - accuracy: 0.21 - ETA: 5:15 - loss: 3.0013 - accuracy: 0.20 - ETA: 5:18 - loss: 2.9985 - accuracy: 0.20 - ETA: 5:13 - loss: 2.9934 - accuracy: 0.20 - ETA: 5:09 - loss: 2.9793 - accuracy: 0.20 - ETA: 5:03 - loss: 2.9742 - accuracy: 0.21 - ETA: 5:03 - loss: 2.9804 - accuracy: 0.20 - ETA: 5:04 - loss: 2.9874 - accuracy: 0.20 - ETA: 5:04 - loss: 3.0102 - accuracy: 0.20 - ETA: 5:05 - loss: 3.0148 - accuracy: 0.20 - ETA: 5:05 - loss: 3.0320 - accuracy: 0.19 - ETA: 5:05 - loss: 3.0308 - accuracy: 0.19 - ETA: 5:03 - loss: 3.0269 - accuracy: 0.20 - ETA: 5:02 - loss: 3.0242 - accuracy: 0.20 - ETA: 5:02 - loss: 3.0277 - accuracy: 0.20 - ETA: 5:01 - loss: 3.0327 - accuracy: 0.20 - ETA: 4:58 - loss: 3.0226 - accuracy: 0.20 - ETA: 4:57 - loss: 3.0267 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0230 - accuracy: 0.20 - ETA: 4:55 - loss: 3.0232 - accuracy: 0.20 - ETA: 4:54 - loss: 3.0091 - accuracy: 0.20 - ETA: 4:53 - loss: 3.0223 - accuracy: 0.20 - ETA: 4:52 - loss: 3.0313 - accuracy: 0.20 - ETA: 4:51 - loss: 3.0359 - accuracy: 0.20 - ETA: 4:49 - loss: 3.0270 - accuracy: 0.20 - ETA: 4:49 - loss: 3.0253 - accuracy: 0.20 - ETA: 4:48 - loss: 3.0236 - accuracy: 0.20 - ETA: 4:47 - loss: 3.0203 - accuracy: 0.20 - ETA: 4:46 - loss: 3.0158 - accuracy: 0.20 - ETA: 4:45 - loss: 3.0142 - accuracy: 0.20 - ETA: 4:44 - loss: 3.0120 - accuracy: 0.20 - ETA: 4:43 - loss: 3.0127 - accuracy: 0.20 - ETA: 4:42 - loss: 3.0243 - accuracy: 0.20 - ETA: 4:41 - loss: 3.0209 - accuracy: 0.20 - ETA: 4:40 - loss: 3.0215 - accuracy: 0.20 - ETA: 4:39 - loss: 3.0201 - accuracy: 0.20 - ETA: 4:38 - loss: 3.0255 - accuracy: 0.20 - ETA: 4:36 - loss: 3.0268 - accuracy: 0.20 - ETA: 4:36 - loss: 3.0281 - accuracy: 0.20 - ETA: 4:35 - loss: 3.0318 - accuracy: 0.20 - ETA: 4:34 - loss: 3.0311 - accuracy: 0.20 - ETA: 4:34 - loss: 3.0303 - accuracy: 0.20 - ETA: 4:33 - loss: 3.0308 - accuracy: 0.20 - ETA: 4:31 - loss: 3.0322 - accuracy: 0.20 - ETA: 4:30 - loss: 3.0316 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0310 - accuracy: 0.20 - ETA: 4:28 - loss: 3.0299 - accuracy: 0.20 - ETA: 4:27 - loss: 3.0232 - accuracy: 0.20 - ETA: 4:26 - loss: 3.0205 - accuracy: 0.20 - ETA: 4:25 - loss: 3.0159 - accuracy: 0.20 - ETA: 4:24 - loss: 3.0116 - accuracy: 0.20 - ETA: 4:23 - loss: 3.0159 - accuracy: 0.20 - ETA: 4:22 - loss: 3.0168 - accuracy: 0.20 - ETA: 4:21 - loss: 3.0189 - accuracy: 0.20 - ETA: 4:20 - loss: 3.0201 - accuracy: 0.20 - ETA: 4:19 - loss: 3.0198 - accuracy: 0.20 - ETA: 4:18 - loss: 3.0158 - accuracy: 0.20 - ETA: 4:17 - loss: 3.0139 - accuracy: 0.20 - ETA: 4:16 - loss: 3.0140 - accuracy: 0.20 - ETA: 4:15 - loss: 3.0166 - accuracy: 0.20 - ETA: 4:14 - loss: 3.0192 - accuracy: 0.20 - ETA: 4:13 - loss: 3.0191 - accuracy: 0.20 - ETA: 4:12 - loss: 3.0222 - accuracy: 0.20 - ETA: 4:11 - loss: 3.0230 - accuracy: 0.20 - ETA: 4:10 - loss: 3.0235 - accuracy: 0.20 - ETA: 4:09 - loss: 3.0225 - accuracy: 0.20 - ETA: 4:08 - loss: 3.0237 - accuracy: 0.20 - ETA: 4:07 - loss: 3.0246 - accuracy: 0.20 - ETA: 4:06 - loss: 3.0252 - accuracy: 0.20 - ETA: 4:06 - loss: 3.0286 - accuracy: 0.20 - ETA: 4:05 - loss: 3.0309 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0342 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0384 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0386 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0407 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0421 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0442 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0445 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0434 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0447 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0469 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0616 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0614 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0631 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0642 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0625 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0639 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0644 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0660 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0669 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0687 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0706 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0704 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0733 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0743 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0748 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0757 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0758 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0757 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0774 - accuracy: 0.18 - ETA: 3:37 - loss: 3.0793 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0789 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0778 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0787 - accuracy: 0.18 - ETA: 3:33 - loss: 3.0793 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0786 - accuracy: 0.18 - ETA: 3:31 - loss: 3.0779 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0779 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0782 - accuracy: 0.18 - ETA: 3:28 - loss: 3.0807 - accuracy: 0.18 - ETA: 3:27 - loss: 3.0806 - accuracy: 0.18 - ETA: 3:26 - loss: 3.0804 - accuracy: 0.18 - ETA: 3:25 - loss: 3.0809 - accuracy: 0.18 - ETA: 3:24 - loss: 3.0814 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0821 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0810 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0812 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0805 - accuracy: 0.18 - ETA: 3:19 - loss: 3.0818 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0810 - accuracy: 0.18 - ETA: 3:17 - loss: 3.0804 - accuracy: 0.18 - ETA: 3:16 - loss: 3.0810 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0797 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0773 - accuracy: 0.18 - ETA: 3:13 - loss: 3.0774 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0768 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0795 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0801 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0790 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0811 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0814 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0794 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0785 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0781 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0782 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0776 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0771 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0751 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0750 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0754 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0751 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0756 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0763 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0759 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0749 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0735 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0735 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0747 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0733 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0734 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0732 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0740 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0750 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0762 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0762 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0753 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0761 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0747 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0752 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0749 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0754 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0764 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0766 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0762 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0761 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0766 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0766 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0774 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0783 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0774 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0778 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0786 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0777 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0776 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0785 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0772 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0776 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0783 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0780 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0780 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0795 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0798 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0793 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0819 - accuracy: 0.1880"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0837 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0833 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0844 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0842 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0830 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0826 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0833 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0829 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0837 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0827 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0825 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0824 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0829 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0841 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0839 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0852 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0863 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0879 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0892 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0898 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0902 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0898 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0900 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0904 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0905 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0914 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0918 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0927 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0922 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0925 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0927 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0924 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0935 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0937 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0940 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0942 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0940 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0950 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0945 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0940 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0931 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0934 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0933 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0941 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0938 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0934 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0938 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0939 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0938 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0943 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0950 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0935 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0939 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0933 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0927 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0928 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0920 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0913 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0908 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0893 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0887 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0883 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0885 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0886 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0878 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0876 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0883 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0882 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0877 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0875 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0881 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0874 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0878 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0873 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0872 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0871 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0868 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0869 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0866 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0866 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0863 - accuracy: 0.18 - ETA: 59s - loss: 3.0865 - accuracy: 0.1867 - ETA: 58s - loss: 3.0864 - accuracy: 0.186 - ETA: 58s - loss: 3.0869 - accuracy: 0.186 - ETA: 57s - loss: 3.0872 - accuracy: 0.186 - ETA: 56s - loss: 3.0878 - accuracy: 0.186 - ETA: 55s - loss: 3.0879 - accuracy: 0.186 - ETA: 54s - loss: 3.0880 - accuracy: 0.186 - ETA: 53s - loss: 3.0883 - accuracy: 0.185 - ETA: 52s - loss: 3.0883 - accuracy: 0.186 - ETA: 51s - loss: 3.0883 - accuracy: 0.186 - ETA: 50s - loss: 3.0882 - accuracy: 0.186 - ETA: 49s - loss: 3.0878 - accuracy: 0.186 - ETA: 48s - loss: 3.0875 - accuracy: 0.186 - ETA: 47s - loss: 3.0874 - accuracy: 0.186 - ETA: 46s - loss: 3.0876 - accuracy: 0.186 - ETA: 45s - loss: 3.0880 - accuracy: 0.186 - ETA: 44s - loss: 3.0888 - accuracy: 0.185 - ETA: 43s - loss: 3.0884 - accuracy: 0.185 - ETA: 42s - loss: 3.0883 - accuracy: 0.185 - ETA: 41s - loss: 3.0879 - accuracy: 0.186 - ETA: 40s - loss: 3.0887 - accuracy: 0.185 - ETA: 39s - loss: 3.0889 - accuracy: 0.185 - ETA: 39s - loss: 3.0891 - accuracy: 0.185 - ETA: 38s - loss: 3.0892 - accuracy: 0.185 - ETA: 37s - loss: 3.0894 - accuracy: 0.185 - ETA: 36s - loss: 3.0891 - accuracy: 0.185 - ETA: 35s - loss: 3.0891 - accuracy: 0.185 - ETA: 34s - loss: 3.0885 - accuracy: 0.186 - ETA: 33s - loss: 3.0885 - accuracy: 0.186 - ETA: 32s - loss: 3.0884 - accuracy: 0.186 - ETA: 31s - loss: 3.0877 - accuracy: 0.186 - ETA: 30s - loss: 3.0873 - accuracy: 0.186 - ETA: 29s - loss: 3.0871 - accuracy: 0.186 - ETA: 28s - loss: 3.0870 - accuracy: 0.186 - ETA: 27s - loss: 3.0873 - accuracy: 0.185 - ETA: 26s - loss: 3.0875 - accuracy: 0.185 - ETA: 25s - loss: 3.0871 - accuracy: 0.185 - ETA: 24s - loss: 3.0861 - accuracy: 0.186 - ETA: 23s - loss: 3.0860 - accuracy: 0.186 - ETA: 22s - loss: 3.0860 - accuracy: 0.186 - ETA: 21s - loss: 3.0863 - accuracy: 0.185 - ETA: 20s - loss: 3.0863 - accuracy: 0.185 - ETA: 20s - loss: 3.0864 - accuracy: 0.185 - ETA: 19s - loss: 3.0866 - accuracy: 0.185 - ETA: 18s - loss: 3.0863 - accuracy: 0.185 - ETA: 17s - loss: 3.0862 - accuracy: 0.186 - ETA: 16s - loss: 3.0858 - accuracy: 0.186 - ETA: 15s - loss: 3.0854 - accuracy: 0.186 - ETA: 14s - loss: 3.0851 - accuracy: 0.186 - ETA: 13s - loss: 3.0849 - accuracy: 0.186 - ETA: 12s - loss: 3.0842 - accuracy: 0.186 - ETA: 11s - loss: 3.0840 - accuracy: 0.186 - ETA: 10s - loss: 3.0841 - accuracy: 0.186 - ETA: 9s - loss: 3.0840 - accuracy: 0.186 - ETA: 8s - loss: 3.0837 - accuracy: 0.18 - ETA: 7s - loss: 3.0835 - accuracy: 0.18 - ETA: 6s - loss: 3.0830 - accuracy: 0.18 - ETA: 5s - loss: 3.0828 - accuracy: 0.18 - ETA: 4s - loss: 3.0827 - accuracy: 0.18 - ETA: 3s - loss: 3.0822 - accuracy: 0.18 - ETA: 2s - loss: 3.0826 - accuracy: 0.18 - ETA: 1s - loss: 3.0821 - accuracy: 0.18 - ETA: 1s - loss: 3.0826 - accuracy: 0.18 - ETA: 0s - loss: 3.0824 - accuracy: 0.18 - 337s 8ms/step - loss: 3.0824 - accuracy: 0.1863 - val_loss: 4.2738 - val_accuracy: 0.0400\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:29 - loss: 2.9695 - accuracy: 0.21 - ETA: 5:33 - loss: 3.0974 - accuracy: 0.19 - ETA: 5:27 - loss: 3.0780 - accuracy: 0.19 - ETA: 5:27 - loss: 3.1284 - accuracy: 0.19 - ETA: 5:24 - loss: 3.1241 - accuracy: 0.20 - ETA: 5:20 - loss: 3.1502 - accuracy: 0.19 - ETA: 5:16 - loss: 3.1196 - accuracy: 0.19 - ETA: 5:13 - loss: 3.1223 - accuracy: 0.19 - ETA: 5:11 - loss: 3.1335 - accuracy: 0.18 - ETA: 5:11 - loss: 3.1079 - accuracy: 0.19 - ETA: 5:09 - loss: 3.1164 - accuracy: 0.19 - ETA: 5:09 - loss: 3.1248 - accuracy: 0.18 - ETA: 5:08 - loss: 3.1049 - accuracy: 0.19 - ETA: 5:06 - loss: 3.0978 - accuracy: 0.19 - ETA: 5:04 - loss: 3.0961 - accuracy: 0.19 - ETA: 5:02 - loss: 3.0998 - accuracy: 0.19 - ETA: 5:01 - loss: 3.0984 - accuracy: 0.19 - ETA: 5:00 - loss: 3.1000 - accuracy: 0.19 - ETA: 4:58 - loss: 3.1073 - accuracy: 0.19 - ETA: 4:57 - loss: 3.1094 - accuracy: 0.18 - ETA: 4:56 - loss: 3.1147 - accuracy: 0.18 - ETA: 4:55 - loss: 3.1069 - accuracy: 0.18 - ETA: 4:53 - loss: 3.1043 - accuracy: 0.18 - ETA: 4:53 - loss: 3.1040 - accuracy: 0.18 - ETA: 4:52 - loss: 3.1043 - accuracy: 0.18 - ETA: 4:50 - loss: 3.1059 - accuracy: 0.18 - ETA: 4:49 - loss: 3.1183 - accuracy: 0.18 - ETA: 4:49 - loss: 3.1221 - accuracy: 0.18 - ETA: 4:47 - loss: 3.1250 - accuracy: 0.17 - ETA: 4:46 - loss: 3.1299 - accuracy: 0.17 - ETA: 4:46 - loss: 3.1275 - accuracy: 0.17 - ETA: 4:45 - loss: 3.1255 - accuracy: 0.17 - ETA: 4:44 - loss: 3.1252 - accuracy: 0.17 - ETA: 4:44 - loss: 3.1252 - accuracy: 0.17 - ETA: 4:43 - loss: 3.1236 - accuracy: 0.17 - ETA: 4:42 - loss: 3.1257 - accuracy: 0.17 - ETA: 4:41 - loss: 3.1203 - accuracy: 0.17 - ETA: 4:39 - loss: 3.1209 - accuracy: 0.17 - ETA: 4:39 - loss: 3.1126 - accuracy: 0.17 - ETA: 4:38 - loss: 3.1144 - accuracy: 0.17 - ETA: 4:36 - loss: 3.1101 - accuracy: 0.17 - ETA: 4:35 - loss: 3.1149 - accuracy: 0.17 - ETA: 4:34 - loss: 3.1156 - accuracy: 0.17 - ETA: 4:33 - loss: 3.1163 - accuracy: 0.17 - ETA: 4:32 - loss: 3.1120 - accuracy: 0.18 - ETA: 4:31 - loss: 3.1111 - accuracy: 0.17 - ETA: 4:30 - loss: 3.1116 - accuracy: 0.17 - ETA: 4:29 - loss: 3.1107 - accuracy: 0.17 - ETA: 4:28 - loss: 3.1082 - accuracy: 0.17 - ETA: 4:27 - loss: 3.1066 - accuracy: 0.17 - ETA: 4:26 - loss: 3.1066 - accuracy: 0.17 - ETA: 4:25 - loss: 3.1025 - accuracy: 0.17 - ETA: 4:24 - loss: 3.1013 - accuracy: 0.18 - ETA: 4:23 - loss: 3.1037 - accuracy: 0.18 - ETA: 4:22 - loss: 3.1012 - accuracy: 0.18 - ETA: 4:21 - loss: 3.1006 - accuracy: 0.18 - ETA: 4:20 - loss: 3.1022 - accuracy: 0.18 - ETA: 4:19 - loss: 3.1015 - accuracy: 0.18 - ETA: 4:18 - loss: 3.0980 - accuracy: 0.18 - ETA: 4:17 - loss: 3.0948 - accuracy: 0.18 - ETA: 4:16 - loss: 3.0916 - accuracy: 0.18 - ETA: 4:15 - loss: 3.0933 - accuracy: 0.18 - ETA: 4:14 - loss: 3.0952 - accuracy: 0.18 - ETA: 4:13 - loss: 3.0942 - accuracy: 0.18 - ETA: 4:12 - loss: 3.0949 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0964 - accuracy: 0.18 - ETA: 4:10 - loss: 3.0963 - accuracy: 0.18 - ETA: 4:10 - loss: 3.0957 - accuracy: 0.18 - ETA: 4:09 - loss: 3.0976 - accuracy: 0.17 - ETA: 4:08 - loss: 3.0961 - accuracy: 0.18 - ETA: 4:07 - loss: 3.0963 - accuracy: 0.18 - ETA: 4:06 - loss: 3.0941 - accuracy: 0.18 - ETA: 4:06 - loss: 3.0965 - accuracy: 0.18 - ETA: 4:05 - loss: 3.0990 - accuracy: 0.18 - ETA: 4:04 - loss: 3.0964 - accuracy: 0.18 - ETA: 4:03 - loss: 3.0961 - accuracy: 0.18 - ETA: 4:02 - loss: 3.0968 - accuracy: 0.18 - ETA: 4:01 - loss: 3.0950 - accuracy: 0.18 - ETA: 4:00 - loss: 3.0942 - accuracy: 0.18 - ETA: 3:58 - loss: 3.0916 - accuracy: 0.18 - ETA: 3:58 - loss: 3.0913 - accuracy: 0.18 - ETA: 3:57 - loss: 3.0912 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0914 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0910 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0908 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0915 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0904 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0870 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0844 - accuracy: 0.18 - ETA: 3:49 - loss: 3.0850 - accuracy: 0.18 - ETA: 3:48 - loss: 3.0838 - accuracy: 0.18 - ETA: 3:47 - loss: 3.0876 - accuracy: 0.18 - ETA: 3:46 - loss: 3.0883 - accuracy: 0.18 - ETA: 3:45 - loss: 3.0870 - accuracy: 0.18 - ETA: 3:44 - loss: 3.0846 - accuracy: 0.18 - ETA: 3:43 - loss: 3.0839 - accuracy: 0.18 - ETA: 3:42 - loss: 3.0834 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0832 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0823 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0831 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0806 - accuracy: 0.18 - ETA: 3:37 - loss: 3.0803 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0802 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0788 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0790 - accuracy: 0.18 - ETA: 3:33 - loss: 3.0769 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0765 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0754 - accuracy: 0.18 - ETA: 3:31 - loss: 3.0744 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0747 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0742 - accuracy: 0.18 - ETA: 3:28 - loss: 3.0726 - accuracy: 0.18 - ETA: 3:27 - loss: 3.0712 - accuracy: 0.18 - ETA: 3:26 - loss: 3.0718 - accuracy: 0.18 - ETA: 3:25 - loss: 3.0703 - accuracy: 0.18 - ETA: 3:24 - loss: 3.0682 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0695 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0673 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0674 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0689 - accuracy: 0.18 - ETA: 3:19 - loss: 3.0690 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0683 - accuracy: 0.18 - ETA: 3:17 - loss: 3.0664 - accuracy: 0.18 - ETA: 3:16 - loss: 3.0662 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0653 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0643 - accuracy: 0.18 - ETA: 3:13 - loss: 3.0642 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0644 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0646 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0644 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0623 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0618 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0609 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0616 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0607 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0605 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0596 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0604 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0610 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0611 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0607 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0612 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0609 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0600 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0596 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0600 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0587 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0570 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0587 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0572 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0559 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0567 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0543 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0548 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0567 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0569 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0587 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0584 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0582 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0586 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0585 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0588 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0587 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0602 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0598 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0594 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0583 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0594 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0587 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0576 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0570 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0573 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0569 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0573 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0567 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0567 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0553 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0556 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0557 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0562 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0565 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0574 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0577 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0614 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0624 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0624 - accuracy: 0.1873"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0622 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0634 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0641 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0643 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0645 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0643 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0656 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0655 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0656 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0665 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0664 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0663 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0652 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0654 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0657 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0658 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0645 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0651 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0658 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0653 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0663 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0671 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0677 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0680 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0686 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0687 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0685 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0684 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0675 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0672 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0671 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0675 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0671 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0676 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0682 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0687 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0688 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0686 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0690 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0688 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0686 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0688 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0685 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0684 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0686 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0682 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0685 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0681 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0677 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0673 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0672 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0677 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0676 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0665 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0661 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0658 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0656 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0645 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0643 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0644 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0643 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0642 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0642 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0646 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0652 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0650 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0650 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0648 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0646 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0643 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0638 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0642 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0641 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0636 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0638 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0631 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0627 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0613 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0609 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0611 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0604 - accuracy: 0.18 - ETA: 59s - loss: 3.0609 - accuracy: 0.1885 - ETA: 59s - loss: 3.0609 - accuracy: 0.188 - ETA: 58s - loss: 3.0597 - accuracy: 0.188 - ETA: 57s - loss: 3.0599 - accuracy: 0.188 - ETA: 56s - loss: 3.0595 - accuracy: 0.188 - ETA: 55s - loss: 3.0590 - accuracy: 0.188 - ETA: 54s - loss: 3.0589 - accuracy: 0.188 - ETA: 53s - loss: 3.0596 - accuracy: 0.188 - ETA: 52s - loss: 3.0598 - accuracy: 0.188 - ETA: 51s - loss: 3.0598 - accuracy: 0.189 - ETA: 50s - loss: 3.0609 - accuracy: 0.189 - ETA: 49s - loss: 3.0628 - accuracy: 0.189 - ETA: 48s - loss: 3.0625 - accuracy: 0.189 - ETA: 47s - loss: 3.0626 - accuracy: 0.189 - ETA: 46s - loss: 3.0629 - accuracy: 0.189 - ETA: 45s - loss: 3.0634 - accuracy: 0.189 - ETA: 44s - loss: 3.0641 - accuracy: 0.189 - ETA: 43s - loss: 3.0636 - accuracy: 0.189 - ETA: 42s - loss: 3.0637 - accuracy: 0.188 - ETA: 41s - loss: 3.0639 - accuracy: 0.188 - ETA: 40s - loss: 3.0637 - accuracy: 0.188 - ETA: 40s - loss: 3.0635 - accuracy: 0.188 - ETA: 39s - loss: 3.0638 - accuracy: 0.189 - ETA: 38s - loss: 3.0635 - accuracy: 0.189 - ETA: 37s - loss: 3.0638 - accuracy: 0.188 - ETA: 36s - loss: 3.0641 - accuracy: 0.188 - ETA: 35s - loss: 3.0648 - accuracy: 0.188 - ETA: 34s - loss: 3.0652 - accuracy: 0.188 - ETA: 33s - loss: 3.0658 - accuracy: 0.188 - ETA: 32s - loss: 3.0673 - accuracy: 0.188 - ETA: 31s - loss: 3.0689 - accuracy: 0.188 - ETA: 30s - loss: 3.0690 - accuracy: 0.188 - ETA: 29s - loss: 3.0698 - accuracy: 0.188 - ETA: 28s - loss: 3.0699 - accuracy: 0.188 - ETA: 27s - loss: 3.0696 - accuracy: 0.188 - ETA: 26s - loss: 3.0697 - accuracy: 0.188 - ETA: 25s - loss: 3.0695 - accuracy: 0.188 - ETA: 24s - loss: 3.0692 - accuracy: 0.188 - ETA: 23s - loss: 3.0698 - accuracy: 0.188 - ETA: 22s - loss: 3.0698 - accuracy: 0.188 - ETA: 21s - loss: 3.0698 - accuracy: 0.188 - ETA: 21s - loss: 3.0703 - accuracy: 0.188 - ETA: 20s - loss: 3.0705 - accuracy: 0.188 - ETA: 19s - loss: 3.0705 - accuracy: 0.188 - ETA: 18s - loss: 3.0704 - accuracy: 0.188 - ETA: 17s - loss: 3.0710 - accuracy: 0.188 - ETA: 16s - loss: 3.0715 - accuracy: 0.188 - ETA: 15s - loss: 3.0721 - accuracy: 0.188 - ETA: 14s - loss: 3.0719 - accuracy: 0.188 - ETA: 13s - loss: 3.0725 - accuracy: 0.188 - ETA: 12s - loss: 3.0725 - accuracy: 0.188 - ETA: 11s - loss: 3.0725 - accuracy: 0.188 - ETA: 10s - loss: 3.0730 - accuracy: 0.188 - ETA: 9s - loss: 3.0737 - accuracy: 0.188 - ETA: 8s - loss: 3.0739 - accuracy: 0.18 - ETA: 7s - loss: 3.0742 - accuracy: 0.18 - ETA: 6s - loss: 3.0738 - accuracy: 0.18 - ETA: 5s - loss: 3.0734 - accuracy: 0.18 - ETA: 4s - loss: 3.0740 - accuracy: 0.18 - ETA: 3s - loss: 3.0749 - accuracy: 0.18 - ETA: 2s - loss: 3.0750 - accuracy: 0.18 - ETA: 1s - loss: 3.0752 - accuracy: 0.18 - ETA: 1s - loss: 3.0753 - accuracy: 0.18 - ETA: 0s - loss: 3.0754 - accuracy: 0.18 - 338s 8ms/step - loss: 3.0754 - accuracy: 0.1882 - val_loss: 4.1636 - val_accuracy: 0.0325\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:00 - loss: 3.1522 - accuracy: 0.17 - ETA: 4:58 - loss: 3.0419 - accuracy: 0.20 - ETA: 4:58 - loss: 3.1457 - accuracy: 0.18 - ETA: 4:59 - loss: 3.0962 - accuracy: 0.18 - ETA: 5:00 - loss: 3.1398 - accuracy: 0.19 - ETA: 5:00 - loss: 3.1309 - accuracy: 0.19 - ETA: 4:59 - loss: 3.1110 - accuracy: 0.18 - ETA: 4:58 - loss: 3.1152 - accuracy: 0.18 - ETA: 4:58 - loss: 3.1217 - accuracy: 0.18 - ETA: 4:57 - loss: 3.1204 - accuracy: 0.18 - ETA: 4:58 - loss: 3.1311 - accuracy: 0.17 - ETA: 4:58 - loss: 3.1254 - accuracy: 0.17 - ETA: 4:56 - loss: 3.1208 - accuracy: 0.17 - ETA: 4:55 - loss: 3.1297 - accuracy: 0.17 - ETA: 4:54 - loss: 3.1233 - accuracy: 0.17 - ETA: 4:53 - loss: 3.1082 - accuracy: 0.17 - ETA: 4:52 - loss: 3.1134 - accuracy: 0.17 - ETA: 4:51 - loss: 3.1183 - accuracy: 0.17 - ETA: 4:51 - loss: 3.1241 - accuracy: 0.17 - ETA: 4:50 - loss: 3.1232 - accuracy: 0.17 - ETA: 4:49 - loss: 3.1218 - accuracy: 0.17 - ETA: 4:48 - loss: 3.1123 - accuracy: 0.17 - ETA: 4:47 - loss: 3.1132 - accuracy: 0.17 - ETA: 4:46 - loss: 3.1241 - accuracy: 0.17 - ETA: 4:45 - loss: 3.1269 - accuracy: 0.17 - ETA: 4:44 - loss: 3.1280 - accuracy: 0.17 - ETA: 4:44 - loss: 3.1347 - accuracy: 0.17 - ETA: 4:44 - loss: 3.1350 - accuracy: 0.17 - ETA: 4:43 - loss: 3.1244 - accuracy: 0.17 - ETA: 4:43 - loss: 3.1269 - accuracy: 0.17 - ETA: 4:42 - loss: 3.1224 - accuracy: 0.17 - ETA: 4:42 - loss: 3.1223 - accuracy: 0.17 - ETA: 4:41 - loss: 3.1194 - accuracy: 0.17 - ETA: 4:40 - loss: 3.1208 - accuracy: 0.17 - ETA: 4:40 - loss: 3.1195 - accuracy: 0.17 - ETA: 4:39 - loss: 3.1191 - accuracy: 0.17 - ETA: 4:38 - loss: 3.1196 - accuracy: 0.17 - ETA: 4:37 - loss: 3.1189 - accuracy: 0.17 - ETA: 4:36 - loss: 3.1235 - accuracy: 0.17 - ETA: 4:35 - loss: 3.1272 - accuracy: 0.17 - ETA: 4:35 - loss: 3.1291 - accuracy: 0.17 - ETA: 4:34 - loss: 3.1299 - accuracy: 0.17 - ETA: 4:32 - loss: 3.1262 - accuracy: 0.17 - ETA: 4:31 - loss: 3.1257 - accuracy: 0.17 - ETA: 4:31 - loss: 3.1207 - accuracy: 0.17 - ETA: 4:30 - loss: 3.1226 - accuracy: 0.17 - ETA: 4:29 - loss: 3.1227 - accuracy: 0.17 - ETA: 4:29 - loss: 3.1234 - accuracy: 0.17 - ETA: 4:28 - loss: 3.1234 - accuracy: 0.17 - ETA: 4:27 - loss: 3.1245 - accuracy: 0.17 - ETA: 4:27 - loss: 3.1223 - accuracy: 0.17 - ETA: 4:26 - loss: 3.1215 - accuracy: 0.17 - ETA: 4:25 - loss: 3.1179 - accuracy: 0.17 - ETA: 4:24 - loss: 3.1180 - accuracy: 0.17 - ETA: 4:23 - loss: 3.1151 - accuracy: 0.17 - ETA: 4:22 - loss: 3.1119 - accuracy: 0.17 - ETA: 4:22 - loss: 3.1112 - accuracy: 0.17 - ETA: 4:21 - loss: 3.1049 - accuracy: 0.17 - ETA: 4:20 - loss: 3.1075 - accuracy: 0.17 - ETA: 4:19 - loss: 3.1056 - accuracy: 0.18 - ETA: 4:18 - loss: 3.1045 - accuracy: 0.17 - ETA: 4:18 - loss: 3.1039 - accuracy: 0.18 - ETA: 4:17 - loss: 3.1035 - accuracy: 0.18 - ETA: 4:16 - loss: 3.1053 - accuracy: 0.18 - ETA: 4:15 - loss: 3.1042 - accuracy: 0.18 - ETA: 4:15 - loss: 3.1059 - accuracy: 0.18 - ETA: 4:14 - loss: 3.1027 - accuracy: 0.18 - ETA: 4:13 - loss: 3.1002 - accuracy: 0.18 - ETA: 4:12 - loss: 3.0988 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0965 - accuracy: 0.18 - ETA: 4:09 - loss: 3.0952 - accuracy: 0.18 - ETA: 4:08 - loss: 3.0960 - accuracy: 0.18 - ETA: 4:08 - loss: 3.0940 - accuracy: 0.18 - ETA: 4:07 - loss: 3.0909 - accuracy: 0.18 - ETA: 4:06 - loss: 3.0892 - accuracy: 0.18 - ETA: 4:05 - loss: 3.0875 - accuracy: 0.18 - ETA: 4:04 - loss: 3.0849 - accuracy: 0.18 - ETA: 4:03 - loss: 3.0808 - accuracy: 0.18 - ETA: 4:02 - loss: 3.0804 - accuracy: 0.18 - ETA: 4:01 - loss: 3.0809 - accuracy: 0.18 - ETA: 4:00 - loss: 3.0810 - accuracy: 0.18 - ETA: 3:59 - loss: 3.0796 - accuracy: 0.18 - ETA: 3:58 - loss: 3.0808 - accuracy: 0.18 - ETA: 3:57 - loss: 3.0837 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0822 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0793 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0785 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0795 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0785 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0775 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0763 - accuracy: 0.18 - ETA: 3:49 - loss: 3.0766 - accuracy: 0.18 - ETA: 3:48 - loss: 3.0770 - accuracy: 0.18 - ETA: 3:47 - loss: 3.0732 - accuracy: 0.18 - ETA: 3:46 - loss: 3.0727 - accuracy: 0.18 - ETA: 3:45 - loss: 3.0704 - accuracy: 0.18 - ETA: 3:44 - loss: 3.0740 - accuracy: 0.18 - ETA: 3:43 - loss: 3.0734 - accuracy: 0.18 - ETA: 3:42 - loss: 3.0720 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0703 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0714 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0732 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0744 - accuracy: 0.18 - ETA: 3:37 - loss: 3.0734 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0713 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0725 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0725 - accuracy: 0.18 - ETA: 3:33 - loss: 3.0735 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0781 - accuracy: 0.18 - ETA: 3:31 - loss: 3.0802 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0803 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0809 - accuracy: 0.18 - ETA: 3:28 - loss: 3.0816 - accuracy: 0.18 - ETA: 3:27 - loss: 3.0810 - accuracy: 0.18 - ETA: 3:26 - loss: 3.0808 - accuracy: 0.18 - ETA: 3:25 - loss: 3.0813 - accuracy: 0.18 - ETA: 3:24 - loss: 3.0810 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0829 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0811 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0813 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0812 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0809 - accuracy: 0.18 - ETA: 3:19 - loss: 3.0800 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0813 - accuracy: 0.18 - ETA: 3:17 - loss: 3.0822 - accuracy: 0.18 - ETA: 3:16 - loss: 3.0825 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0831 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0822 - accuracy: 0.18 - ETA: 3:13 - loss: 3.0825 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0825 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0832 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0840 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0837 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0841 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0832 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0817 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0816 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0821 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0836 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0830 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0828 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0831 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0823 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0812 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0824 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0836 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0842 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0836 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0834 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0828 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0833 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0830 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0837 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0845 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0849 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0847 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0837 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0833 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0840 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0835 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0834 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0829 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0837 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0841 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0839 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0831 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0831 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0839 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0845 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0831 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0835 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0840 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0840 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0839 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0853 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0842 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0845 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0842 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0839 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0831 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0830 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0826 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0819 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0812 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0813 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0822 - accuracy: 0.1872"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0825 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0818 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0813 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0810 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0817 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0823 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0829 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0829 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0824 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0816 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0821 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0822 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0822 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0825 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0823 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0828 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0832 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0833 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0835 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0838 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0827 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0827 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0833 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0836 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0835 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0837 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0830 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0824 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0826 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0819 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0821 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0829 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0816 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0805 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0798 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0803 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0792 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0787 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0781 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0783 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0785 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0783 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0789 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0787 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0774 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0758 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0755 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0751 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0746 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0746 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0741 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0736 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0738 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0742 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0726 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0732 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0727 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0721 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0711 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0707 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0703 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0694 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0688 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0680 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0682 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0682 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0678 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0687 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0689 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0688 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0687 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0684 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0680 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0674 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0674 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0672 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0667 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0673 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0669 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0664 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0656 - accuracy: 0.18 - ETA: 59s - loss: 3.0653 - accuracy: 0.1897 - ETA: 58s - loss: 3.0656 - accuracy: 0.189 - ETA: 57s - loss: 3.0651 - accuracy: 0.189 - ETA: 56s - loss: 3.0656 - accuracy: 0.189 - ETA: 55s - loss: 3.0641 - accuracy: 0.190 - ETA: 54s - loss: 3.0641 - accuracy: 0.190 - ETA: 53s - loss: 3.0639 - accuracy: 0.190 - ETA: 52s - loss: 3.0635 - accuracy: 0.190 - ETA: 51s - loss: 3.0633 - accuracy: 0.190 - ETA: 50s - loss: 3.0621 - accuracy: 0.191 - ETA: 49s - loss: 3.0615 - accuracy: 0.191 - ETA: 48s - loss: 3.0610 - accuracy: 0.191 - ETA: 47s - loss: 3.0608 - accuracy: 0.191 - ETA: 46s - loss: 3.0610 - accuracy: 0.191 - ETA: 45s - loss: 3.0608 - accuracy: 0.191 - ETA: 45s - loss: 3.0616 - accuracy: 0.191 - ETA: 44s - loss: 3.0622 - accuracy: 0.190 - ETA: 43s - loss: 3.0621 - accuracy: 0.190 - ETA: 42s - loss: 3.0614 - accuracy: 0.190 - ETA: 41s - loss: 3.0618 - accuracy: 0.190 - ETA: 40s - loss: 3.0619 - accuracy: 0.190 - ETA: 39s - loss: 3.0615 - accuracy: 0.190 - ETA: 38s - loss: 3.0610 - accuracy: 0.190 - ETA: 37s - loss: 3.0605 - accuracy: 0.190 - ETA: 36s - loss: 3.0613 - accuracy: 0.190 - ETA: 35s - loss: 3.0607 - accuracy: 0.190 - ETA: 34s - loss: 3.0607 - accuracy: 0.190 - ETA: 33s - loss: 3.0606 - accuracy: 0.190 - ETA: 32s - loss: 3.0605 - accuracy: 0.190 - ETA: 31s - loss: 3.0612 - accuracy: 0.190 - ETA: 30s - loss: 3.0612 - accuracy: 0.190 - ETA: 29s - loss: 3.0610 - accuracy: 0.190 - ETA: 28s - loss: 3.0604 - accuracy: 0.190 - ETA: 27s - loss: 3.0599 - accuracy: 0.190 - ETA: 26s - loss: 3.0599 - accuracy: 0.190 - ETA: 25s - loss: 3.0600 - accuracy: 0.190 - ETA: 24s - loss: 3.0592 - accuracy: 0.190 - ETA: 23s - loss: 3.0593 - accuracy: 0.190 - ETA: 23s - loss: 3.0595 - accuracy: 0.190 - ETA: 22s - loss: 3.0596 - accuracy: 0.190 - ETA: 21s - loss: 3.0595 - accuracy: 0.190 - ETA: 20s - loss: 3.0585 - accuracy: 0.190 - ETA: 19s - loss: 3.0588 - accuracy: 0.190 - ETA: 18s - loss: 3.0589 - accuracy: 0.190 - ETA: 17s - loss: 3.0584 - accuracy: 0.190 - ETA: 16s - loss: 3.0580 - accuracy: 0.190 - ETA: 15s - loss: 3.0585 - accuracy: 0.190 - ETA: 14s - loss: 3.0582 - accuracy: 0.190 - ETA: 13s - loss: 3.0571 - accuracy: 0.191 - ETA: 12s - loss: 3.0565 - accuracy: 0.191 - ETA: 11s - loss: 3.0561 - accuracy: 0.191 - ETA: 10s - loss: 3.0559 - accuracy: 0.191 - ETA: 9s - loss: 3.0552 - accuracy: 0.191 - ETA: 8s - loss: 3.0551 - accuracy: 0.19 - ETA: 7s - loss: 3.0548 - accuracy: 0.19 - ETA: 6s - loss: 3.0541 - accuracy: 0.19 - ETA: 5s - loss: 3.0545 - accuracy: 0.19 - ETA: 4s - loss: 3.0549 - accuracy: 0.19 - ETA: 3s - loss: 3.0546 - accuracy: 0.19 - ETA: 2s - loss: 3.0539 - accuracy: 0.19 - ETA: 1s - loss: 3.0543 - accuracy: 0.19 - ETA: 1s - loss: 3.0549 - accuracy: 0.19 - ETA: 0s - loss: 3.0549 - accuracy: 0.19 - 339s 8ms/step - loss: 3.0549 - accuracy: 0.1916 - val_loss: 4.2228 - val_accuracy: 0.0379\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:08 - loss: 2.8113 - accuracy: 0.25 - ETA: 5:05 - loss: 2.9813 - accuracy: 0.21 - ETA: 5:05 - loss: 2.9878 - accuracy: 0.19 - ETA: 5:05 - loss: 3.0207 - accuracy: 0.18 - ETA: 5:04 - loss: 3.0001 - accuracy: 0.20 - ETA: 5:02 - loss: 3.0112 - accuracy: 0.19 - ETA: 5:02 - loss: 3.0274 - accuracy: 0.19 - ETA: 5:01 - loss: 3.0507 - accuracy: 0.18 - ETA: 5:00 - loss: 3.0397 - accuracy: 0.18 - ETA: 4:59 - loss: 3.0410 - accuracy: 0.18 - ETA: 5:00 - loss: 3.0454 - accuracy: 0.18 - ETA: 5:00 - loss: 3.0334 - accuracy: 0.18 - ETA: 5:01 - loss: 3.0471 - accuracy: 0.18 - ETA: 5:00 - loss: 3.0523 - accuracy: 0.18 - ETA: 4:59 - loss: 3.0544 - accuracy: 0.18 - ETA: 4:59 - loss: 3.0661 - accuracy: 0.18 - ETA: 4:58 - loss: 3.0688 - accuracy: 0.18 - ETA: 4:57 - loss: 3.0689 - accuracy: 0.18 - ETA: 4:56 - loss: 3.0643 - accuracy: 0.18 - ETA: 4:56 - loss: 3.0604 - accuracy: 0.18 - ETA: 4:54 - loss: 3.0523 - accuracy: 0.18 - ETA: 4:54 - loss: 3.0431 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0360 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0438 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0503 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0518 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0540 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0539 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0474 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0409 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0379 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0310 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0317 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0308 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0331 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0297 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0383 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0422 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0440 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0428 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0423 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0383 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0385 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0388 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0365 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0409 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0431 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0479 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0521 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0488 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0482 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0485 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0477 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0516 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0540 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0564 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0583 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0573 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0585 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0560 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0579 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0618 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0609 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0613 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0610 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0630 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0645 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0657 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0678 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0663 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0655 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0686 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0679 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0710 - accuracy: 0.18 - ETA: 4:02 - loss: 3.0765 - accuracy: 0.18 - ETA: 4:01 - loss: 3.0793 - accuracy: 0.18 - ETA: 4:00 - loss: 3.0845 - accuracy: 0.18 - ETA: 3:59 - loss: 3.0886 - accuracy: 0.18 - ETA: 3:58 - loss: 3.0900 - accuracy: 0.18 - ETA: 3:57 - loss: 3.0888 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0869 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0896 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0904 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0917 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0921 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0918 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0942 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0944 - accuracy: 0.18 - ETA: 3:49 - loss: 3.0937 - accuracy: 0.18 - ETA: 3:48 - loss: 3.0956 - accuracy: 0.18 - ETA: 3:47 - loss: 3.0982 - accuracy: 0.18 - ETA: 3:46 - loss: 3.1011 - accuracy: 0.18 - ETA: 3:45 - loss: 3.0999 - accuracy: 0.18 - ETA: 3:44 - loss: 3.1000 - accuracy: 0.18 - ETA: 3:43 - loss: 3.1029 - accuracy: 0.18 - ETA: 3:42 - loss: 3.1057 - accuracy: 0.18 - ETA: 3:41 - loss: 3.1108 - accuracy: 0.18 - ETA: 3:40 - loss: 3.1148 - accuracy: 0.18 - ETA: 3:39 - loss: 3.1187 - accuracy: 0.18 - ETA: 3:38 - loss: 3.1213 - accuracy: 0.18 - ETA: 3:38 - loss: 3.1216 - accuracy: 0.18 - ETA: 3:37 - loss: 3.1228 - accuracy: 0.18 - ETA: 3:36 - loss: 3.1240 - accuracy: 0.18 - ETA: 3:35 - loss: 3.1239 - accuracy: 0.18 - ETA: 3:34 - loss: 3.1255 - accuracy: 0.18 - ETA: 3:33 - loss: 3.1259 - accuracy: 0.18 - ETA: 3:32 - loss: 3.1253 - accuracy: 0.18 - ETA: 3:31 - loss: 3.1249 - accuracy: 0.18 - ETA: 3:30 - loss: 3.1257 - accuracy: 0.18 - ETA: 3:29 - loss: 3.1289 - accuracy: 0.17 - ETA: 3:28 - loss: 3.1290 - accuracy: 0.17 - ETA: 3:27 - loss: 3.1305 - accuracy: 0.17 - ETA: 3:26 - loss: 3.1320 - accuracy: 0.17 - ETA: 3:25 - loss: 3.1333 - accuracy: 0.17 - ETA: 3:24 - loss: 3.1332 - accuracy: 0.17 - ETA: 3:23 - loss: 3.1350 - accuracy: 0.17 - ETA: 3:22 - loss: 3.1354 - accuracy: 0.17 - ETA: 3:21 - loss: 3.1363 - accuracy: 0.17 - ETA: 3:20 - loss: 3.1386 - accuracy: 0.17 - ETA: 3:19 - loss: 3.1390 - accuracy: 0.17 - ETA: 3:18 - loss: 3.1405 - accuracy: 0.17 - ETA: 3:18 - loss: 3.1406 - accuracy: 0.17 - ETA: 3:17 - loss: 3.1410 - accuracy: 0.17 - ETA: 3:16 - loss: 3.1429 - accuracy: 0.17 - ETA: 3:15 - loss: 3.1427 - accuracy: 0.17 - ETA: 3:14 - loss: 3.1431 - accuracy: 0.17 - ETA: 3:13 - loss: 3.1429 - accuracy: 0.17 - ETA: 3:12 - loss: 3.1418 - accuracy: 0.17 - ETA: 3:11 - loss: 3.1421 - accuracy: 0.17 - ETA: 3:10 - loss: 3.1439 - accuracy: 0.17 - ETA: 3:09 - loss: 3.1432 - accuracy: 0.17 - ETA: 3:08 - loss: 3.1439 - accuracy: 0.17 - ETA: 3:08 - loss: 3.1440 - accuracy: 0.17 - ETA: 3:07 - loss: 3.1467 - accuracy: 0.17 - ETA: 3:06 - loss: 3.1483 - accuracy: 0.17 - ETA: 3:05 - loss: 3.1482 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1487 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1483 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1492 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1487 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1493 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1493 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1482 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1476 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1478 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1460 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1454 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1448 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1447 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1445 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1459 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1446 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1433 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1437 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1428 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1423 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1428 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1435 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1433 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1441 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1443 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1447 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1435 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1444 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1453 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1448 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1454 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1443 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1447 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1451 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1455 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1459 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1455 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1443 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1455 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1440 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1440 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1436 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1435 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1432 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1435 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1444 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1438 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1448 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1454 - accuracy: 0.17 - ETA: 2:17 - loss: 3.1448 - accuracy: 0.1758"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.1442 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1443 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1440 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1439 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1437 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1433 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1426 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1429 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1418 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1395 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1403 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1406 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1404 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1409 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1411 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1408 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1416 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1408 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1411 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1423 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1425 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1418 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1413 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1402 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1405 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1404 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1414 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1415 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1423 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1427 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1425 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1428 - accuracy: 0.17 - ETA: 1:46 - loss: 3.1422 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1421 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1416 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1416 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1415 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1417 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1414 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1418 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1424 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1419 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1420 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1418 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1420 - accuracy: 0.17 - ETA: 1:34 - loss: 3.1422 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1413 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1413 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1416 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1409 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1400 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1403 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1398 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1399 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1397 - accuracy: 0.17 - ETA: 1:24 - loss: 3.1393 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1386 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1386 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1382 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1390 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1381 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1380 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1380 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1375 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1370 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1372 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1372 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1372 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1381 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1380 - accuracy: 0.17 - ETA: 1:10 - loss: 3.1378 - accuracy: 0.17 - ETA: 1:09 - loss: 3.1370 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1370 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1374 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1366 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1367 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1364 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1361 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1362 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1355 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1347 - accuracy: 0.17 - ETA: 59s - loss: 3.1345 - accuracy: 0.1773 - ETA: 58s - loss: 3.1343 - accuracy: 0.177 - ETA: 58s - loss: 3.1348 - accuracy: 0.177 - ETA: 57s - loss: 3.1345 - accuracy: 0.177 - ETA: 56s - loss: 3.1362 - accuracy: 0.177 - ETA: 55s - loss: 3.1365 - accuracy: 0.177 - ETA: 54s - loss: 3.1362 - accuracy: 0.177 - ETA: 53s - loss: 3.1357 - accuracy: 0.177 - ETA: 52s - loss: 3.1352 - accuracy: 0.177 - ETA: 51s - loss: 3.1349 - accuracy: 0.177 - ETA: 50s - loss: 3.1341 - accuracy: 0.177 - ETA: 49s - loss: 3.1335 - accuracy: 0.177 - ETA: 48s - loss: 3.1331 - accuracy: 0.177 - ETA: 47s - loss: 3.1336 - accuracy: 0.177 - ETA: 46s - loss: 3.1330 - accuracy: 0.177 - ETA: 45s - loss: 3.1328 - accuracy: 0.178 - ETA: 44s - loss: 3.1326 - accuracy: 0.178 - ETA: 43s - loss: 3.1334 - accuracy: 0.178 - ETA: 42s - loss: 3.1339 - accuracy: 0.178 - ETA: 41s - loss: 3.1338 - accuracy: 0.177 - ETA: 40s - loss: 3.1342 - accuracy: 0.177 - ETA: 39s - loss: 3.1339 - accuracy: 0.177 - ETA: 38s - loss: 3.1330 - accuracy: 0.178 - ETA: 38s - loss: 3.1331 - accuracy: 0.178 - ETA: 37s - loss: 3.1325 - accuracy: 0.178 - ETA: 36s - loss: 3.1319 - accuracy: 0.178 - ETA: 35s - loss: 3.1321 - accuracy: 0.178 - ETA: 34s - loss: 3.1319 - accuracy: 0.178 - ETA: 33s - loss: 3.1315 - accuracy: 0.178 - ETA: 32s - loss: 3.1314 - accuracy: 0.178 - ETA: 31s - loss: 3.1307 - accuracy: 0.178 - ETA: 30s - loss: 3.1300 - accuracy: 0.178 - ETA: 29s - loss: 3.1299 - accuracy: 0.178 - ETA: 28s - loss: 3.1292 - accuracy: 0.178 - ETA: 27s - loss: 3.1296 - accuracy: 0.178 - ETA: 26s - loss: 3.1297 - accuracy: 0.178 - ETA: 25s - loss: 3.1293 - accuracy: 0.178 - ETA: 24s - loss: 3.1296 - accuracy: 0.177 - ETA: 23s - loss: 3.1298 - accuracy: 0.177 - ETA: 22s - loss: 3.1295 - accuracy: 0.177 - ETA: 21s - loss: 3.1287 - accuracy: 0.178 - ETA: 20s - loss: 3.1281 - accuracy: 0.178 - ETA: 19s - loss: 3.1273 - accuracy: 0.178 - ETA: 19s - loss: 3.1272 - accuracy: 0.178 - ETA: 18s - loss: 3.1269 - accuracy: 0.178 - ETA: 17s - loss: 3.1264 - accuracy: 0.178 - ETA: 16s - loss: 3.1257 - accuracy: 0.178 - ETA: 15s - loss: 3.1249 - accuracy: 0.178 - ETA: 14s - loss: 3.1254 - accuracy: 0.178 - ETA: 13s - loss: 3.1254 - accuracy: 0.178 - ETA: 12s - loss: 3.1243 - accuracy: 0.178 - ETA: 11s - loss: 3.1239 - accuracy: 0.178 - ETA: 10s - loss: 3.1240 - accuracy: 0.178 - ETA: 9s - loss: 3.1239 - accuracy: 0.178 - ETA: 8s - loss: 3.1233 - accuracy: 0.17 - ETA: 7s - loss: 3.1233 - accuracy: 0.17 - ETA: 6s - loss: 3.1228 - accuracy: 0.17 - ETA: 5s - loss: 3.1228 - accuracy: 0.17 - ETA: 4s - loss: 3.1230 - accuracy: 0.17 - ETA: 3s - loss: 3.1232 - accuracy: 0.17 - ETA: 2s - loss: 3.1230 - accuracy: 0.17 - ETA: 1s - loss: 3.1231 - accuracy: 0.17 - ETA: 1s - loss: 3.1228 - accuracy: 0.17 - ETA: 0s - loss: 3.1228 - accuracy: 0.17 - 338s 8ms/step - loss: 3.1227 - accuracy: 0.1791 - val_loss: 4.1611 - val_accuracy: 0.0359\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:10 - loss: 3.1459 - accuracy: 0.18 - ETA: 5:04 - loss: 3.1795 - accuracy: 0.15 - ETA: 5:10 - loss: 3.1273 - accuracy: 0.15 - ETA: 5:09 - loss: 3.0360 - accuracy: 0.17 - ETA: 5:08 - loss: 3.0415 - accuracy: 0.17 - ETA: 5:05 - loss: 3.0585 - accuracy: 0.16 - ETA: 5:05 - loss: 3.0417 - accuracy: 0.17 - ETA: 5:02 - loss: 3.0431 - accuracy: 0.17 - ETA: 5:01 - loss: 3.0428 - accuracy: 0.18 - ETA: 5:01 - loss: 3.0364 - accuracy: 0.18 - ETA: 4:58 - loss: 3.0260 - accuracy: 0.18 - ETA: 4:58 - loss: 3.0202 - accuracy: 0.18 - ETA: 4:57 - loss: 3.0326 - accuracy: 0.18 - ETA: 4:57 - loss: 3.0339 - accuracy: 0.18 - ETA: 4:56 - loss: 3.0475 - accuracy: 0.18 - ETA: 4:55 - loss: 3.0636 - accuracy: 0.18 - ETA: 4:54 - loss: 3.0787 - accuracy: 0.18 - ETA: 4:52 - loss: 3.0916 - accuracy: 0.18 - ETA: 4:51 - loss: 3.0873 - accuracy: 0.18 - ETA: 4:51 - loss: 3.0914 - accuracy: 0.18 - ETA: 4:50 - loss: 3.0943 - accuracy: 0.18 - ETA: 4:49 - loss: 3.0938 - accuracy: 0.18 - ETA: 4:48 - loss: 3.1120 - accuracy: 0.18 - ETA: 4:47 - loss: 3.1175 - accuracy: 0.18 - ETA: 4:47 - loss: 3.1186 - accuracy: 0.18 - ETA: 4:45 - loss: 3.1165 - accuracy: 0.18 - ETA: 4:45 - loss: 3.1209 - accuracy: 0.18 - ETA: 4:44 - loss: 3.1174 - accuracy: 0.18 - ETA: 4:43 - loss: 3.1140 - accuracy: 0.18 - ETA: 4:43 - loss: 3.1133 - accuracy: 0.18 - ETA: 4:41 - loss: 3.1163 - accuracy: 0.18 - ETA: 4:41 - loss: 3.1203 - accuracy: 0.18 - ETA: 4:40 - loss: 3.1202 - accuracy: 0.18 - ETA: 4:39 - loss: 3.1208 - accuracy: 0.18 - ETA: 4:38 - loss: 3.1191 - accuracy: 0.18 - ETA: 4:38 - loss: 3.1121 - accuracy: 0.18 - ETA: 4:37 - loss: 3.1157 - accuracy: 0.18 - ETA: 4:36 - loss: 3.1206 - accuracy: 0.17 - ETA: 4:35 - loss: 3.1195 - accuracy: 0.18 - ETA: 4:34 - loss: 3.1217 - accuracy: 0.18 - ETA: 4:33 - loss: 3.1259 - accuracy: 0.17 - ETA: 4:32 - loss: 3.1170 - accuracy: 0.18 - ETA: 4:32 - loss: 3.1185 - accuracy: 0.18 - ETA: 4:31 - loss: 3.1173 - accuracy: 0.18 - ETA: 4:30 - loss: 3.1183 - accuracy: 0.18 - ETA: 4:30 - loss: 3.1171 - accuracy: 0.18 - ETA: 4:29 - loss: 3.1151 - accuracy: 0.18 - ETA: 4:28 - loss: 3.1136 - accuracy: 0.18 - ETA: 4:27 - loss: 3.1168 - accuracy: 0.18 - ETA: 4:26 - loss: 3.1147 - accuracy: 0.18 - ETA: 4:25 - loss: 3.1159 - accuracy: 0.17 - ETA: 4:24 - loss: 3.1155 - accuracy: 0.18 - ETA: 4:23 - loss: 3.1127 - accuracy: 0.18 - ETA: 4:22 - loss: 3.1113 - accuracy: 0.18 - ETA: 4:21 - loss: 3.1153 - accuracy: 0.17 - ETA: 4:20 - loss: 3.1128 - accuracy: 0.18 - ETA: 4:19 - loss: 3.1135 - accuracy: 0.17 - ETA: 4:18 - loss: 3.1121 - accuracy: 0.17 - ETA: 4:17 - loss: 3.1137 - accuracy: 0.17 - ETA: 4:16 - loss: 3.1129 - accuracy: 0.17 - ETA: 4:15 - loss: 3.1165 - accuracy: 0.17 - ETA: 4:15 - loss: 3.1142 - accuracy: 0.17 - ETA: 4:14 - loss: 3.1133 - accuracy: 0.17 - ETA: 4:13 - loss: 3.1111 - accuracy: 0.17 - ETA: 4:12 - loss: 3.1107 - accuracy: 0.18 - ETA: 4:11 - loss: 3.1080 - accuracy: 0.18 - ETA: 4:10 - loss: 3.1024 - accuracy: 0.18 - ETA: 4:09 - loss: 3.1000 - accuracy: 0.18 - ETA: 4:08 - loss: 3.0969 - accuracy: 0.18 - ETA: 4:07 - loss: 3.0929 - accuracy: 0.18 - ETA: 4:06 - loss: 3.0922 - accuracy: 0.18 - ETA: 4:05 - loss: 3.0919 - accuracy: 0.18 - ETA: 4:04 - loss: 3.0951 - accuracy: 0.18 - ETA: 4:03 - loss: 3.0945 - accuracy: 0.18 - ETA: 4:02 - loss: 3.0915 - accuracy: 0.18 - ETA: 4:01 - loss: 3.0903 - accuracy: 0.18 - ETA: 4:00 - loss: 3.0925 - accuracy: 0.18 - ETA: 3:59 - loss: 3.0928 - accuracy: 0.18 - ETA: 3:58 - loss: 3.0924 - accuracy: 0.18 - ETA: 3:57 - loss: 3.0917 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0901 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0886 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0921 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0894 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0879 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0883 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0898 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0911 - accuracy: 0.18 - ETA: 3:49 - loss: 3.0897 - accuracy: 0.18 - ETA: 3:48 - loss: 3.0883 - accuracy: 0.18 - ETA: 3:47 - loss: 3.0876 - accuracy: 0.18 - ETA: 3:46 - loss: 3.0859 - accuracy: 0.18 - ETA: 3:45 - loss: 3.0863 - accuracy: 0.18 - ETA: 3:44 - loss: 3.0846 - accuracy: 0.18 - ETA: 3:43 - loss: 3.0848 - accuracy: 0.18 - ETA: 3:42 - loss: 3.0866 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0859 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0852 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0859 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0851 - accuracy: 0.18 - ETA: 3:37 - loss: 3.0843 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0833 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0840 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0854 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0844 - accuracy: 0.18 - ETA: 3:33 - loss: 3.0848 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0834 - accuracy: 0.18 - ETA: 3:31 - loss: 3.0836 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0824 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0842 - accuracy: 0.18 - ETA: 3:28 - loss: 3.0852 - accuracy: 0.18 - ETA: 3:27 - loss: 3.0853 - accuracy: 0.18 - ETA: 3:26 - loss: 3.0844 - accuracy: 0.18 - ETA: 3:25 - loss: 3.0838 - accuracy: 0.18 - ETA: 3:24 - loss: 3.0838 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0821 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0829 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0832 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0831 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0863 - accuracy: 0.18 - ETA: 3:19 - loss: 3.0863 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0865 - accuracy: 0.18 - ETA: 3:17 - loss: 3.0873 - accuracy: 0.18 - ETA: 3:16 - loss: 3.0869 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0858 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0848 - accuracy: 0.18 - ETA: 3:13 - loss: 3.0859 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0855 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0855 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0865 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0865 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0880 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0869 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0862 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0864 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0862 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0865 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0859 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0866 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0866 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0865 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0869 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0868 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0863 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0856 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0853 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0835 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0838 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0836 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0841 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0848 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0862 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0874 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0865 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0864 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0868 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0857 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0852 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0853 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0851 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0847 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0853 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0859 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0857 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0868 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0874 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0871 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0866 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0872 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0898 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0897 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0900 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0889 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0881 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0884 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0879 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0878 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0882 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0888 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0880 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0875 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0880 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0881 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0891 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0896 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0899 - accuracy: 0.1865"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0903 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0916 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0917 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0931 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0934 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0938 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0936 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0935 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0942 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0952 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0961 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0971 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0973 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0977 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0973 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0989 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0993 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0984 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0994 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0990 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0985 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0983 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0984 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0991 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0983 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0998 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0990 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0989 - accuracy: 0.18 - ETA: 1:49 - loss: 3.1005 - accuracy: 0.18 - ETA: 1:48 - loss: 3.1005 - accuracy: 0.18 - ETA: 1:48 - loss: 3.1011 - accuracy: 0.18 - ETA: 1:47 - loss: 3.1007 - accuracy: 0.18 - ETA: 1:46 - loss: 3.1003 - accuracy: 0.18 - ETA: 1:45 - loss: 3.1006 - accuracy: 0.18 - ETA: 1:44 - loss: 3.1010 - accuracy: 0.18 - ETA: 1:43 - loss: 3.1013 - accuracy: 0.18 - ETA: 1:42 - loss: 3.1014 - accuracy: 0.18 - ETA: 1:41 - loss: 3.1013 - accuracy: 0.18 - ETA: 1:40 - loss: 3.1016 - accuracy: 0.18 - ETA: 1:39 - loss: 3.1011 - accuracy: 0.18 - ETA: 1:38 - loss: 3.1006 - accuracy: 0.18 - ETA: 1:37 - loss: 3.1004 - accuracy: 0.18 - ETA: 1:36 - loss: 3.1004 - accuracy: 0.18 - ETA: 1:35 - loss: 3.1002 - accuracy: 0.18 - ETA: 1:34 - loss: 3.1001 - accuracy: 0.18 - ETA: 1:33 - loss: 3.1006 - accuracy: 0.18 - ETA: 1:32 - loss: 3.1005 - accuracy: 0.18 - ETA: 1:31 - loss: 3.1009 - accuracy: 0.18 - ETA: 1:30 - loss: 3.1010 - accuracy: 0.18 - ETA: 1:30 - loss: 3.1010 - accuracy: 0.18 - ETA: 1:29 - loss: 3.1021 - accuracy: 0.18 - ETA: 1:28 - loss: 3.1022 - accuracy: 0.18 - ETA: 1:27 - loss: 3.1016 - accuracy: 0.18 - ETA: 1:26 - loss: 3.1015 - accuracy: 0.18 - ETA: 1:25 - loss: 3.1024 - accuracy: 0.18 - ETA: 1:24 - loss: 3.1031 - accuracy: 0.18 - ETA: 1:23 - loss: 3.1029 - accuracy: 0.18 - ETA: 1:22 - loss: 3.1032 - accuracy: 0.18 - ETA: 1:21 - loss: 3.1032 - accuracy: 0.18 - ETA: 1:20 - loss: 3.1030 - accuracy: 0.18 - ETA: 1:19 - loss: 3.1032 - accuracy: 0.18 - ETA: 1:18 - loss: 3.1033 - accuracy: 0.18 - ETA: 1:17 - loss: 3.1044 - accuracy: 0.18 - ETA: 1:16 - loss: 3.1035 - accuracy: 0.18 - ETA: 1:15 - loss: 3.1033 - accuracy: 0.18 - ETA: 1:14 - loss: 3.1037 - accuracy: 0.18 - ETA: 1:14 - loss: 3.1031 - accuracy: 0.18 - ETA: 1:13 - loss: 3.1030 - accuracy: 0.18 - ETA: 1:12 - loss: 3.1036 - accuracy: 0.18 - ETA: 1:11 - loss: 3.1030 - accuracy: 0.18 - ETA: 1:10 - loss: 3.1029 - accuracy: 0.18 - ETA: 1:09 - loss: 3.1032 - accuracy: 0.18 - ETA: 1:08 - loss: 3.1023 - accuracy: 0.18 - ETA: 1:07 - loss: 3.1025 - accuracy: 0.18 - ETA: 1:06 - loss: 3.1024 - accuracy: 0.18 - ETA: 1:05 - loss: 3.1025 - accuracy: 0.18 - ETA: 1:04 - loss: 3.1021 - accuracy: 0.18 - ETA: 1:03 - loss: 3.1030 - accuracy: 0.18 - ETA: 1:02 - loss: 3.1036 - accuracy: 0.18 - ETA: 1:01 - loss: 3.1030 - accuracy: 0.18 - ETA: 1:00 - loss: 3.1028 - accuracy: 0.18 - ETA: 59s - loss: 3.1027 - accuracy: 0.1843 - ETA: 58s - loss: 3.1028 - accuracy: 0.184 - ETA: 57s - loss: 3.1024 - accuracy: 0.184 - ETA: 57s - loss: 3.1013 - accuracy: 0.184 - ETA: 56s - loss: 3.1008 - accuracy: 0.184 - ETA: 55s - loss: 3.1013 - accuracy: 0.184 - ETA: 54s - loss: 3.1011 - accuracy: 0.184 - ETA: 53s - loss: 3.1006 - accuracy: 0.184 - ETA: 52s - loss: 3.1006 - accuracy: 0.184 - ETA: 51s - loss: 3.1000 - accuracy: 0.184 - ETA: 50s - loss: 3.1002 - accuracy: 0.184 - ETA: 49s - loss: 3.1004 - accuracy: 0.184 - ETA: 48s - loss: 3.1007 - accuracy: 0.184 - ETA: 47s - loss: 3.1008 - accuracy: 0.184 - ETA: 46s - loss: 3.0999 - accuracy: 0.184 - ETA: 45s - loss: 3.1000 - accuracy: 0.184 - ETA: 44s - loss: 3.1000 - accuracy: 0.184 - ETA: 43s - loss: 3.0997 - accuracy: 0.184 - ETA: 42s - loss: 3.0998 - accuracy: 0.184 - ETA: 41s - loss: 3.1005 - accuracy: 0.184 - ETA: 40s - loss: 3.1007 - accuracy: 0.184 - ETA: 39s - loss: 3.1007 - accuracy: 0.184 - ETA: 38s - loss: 3.1008 - accuracy: 0.184 - ETA: 38s - loss: 3.1008 - accuracy: 0.184 - ETA: 37s - loss: 3.0996 - accuracy: 0.184 - ETA: 36s - loss: 3.0998 - accuracy: 0.184 - ETA: 35s - loss: 3.1006 - accuracy: 0.184 - ETA: 34s - loss: 3.1008 - accuracy: 0.184 - ETA: 33s - loss: 3.1004 - accuracy: 0.184 - ETA: 32s - loss: 3.1001 - accuracy: 0.184 - ETA: 31s - loss: 3.1005 - accuracy: 0.184 - ETA: 30s - loss: 3.1001 - accuracy: 0.184 - ETA: 29s - loss: 3.1003 - accuracy: 0.184 - ETA: 28s - loss: 3.1000 - accuracy: 0.184 - ETA: 27s - loss: 3.1001 - accuracy: 0.184 - ETA: 26s - loss: 3.1004 - accuracy: 0.184 - ETA: 25s - loss: 3.1009 - accuracy: 0.184 - ETA: 24s - loss: 3.1005 - accuracy: 0.184 - ETA: 23s - loss: 3.0999 - accuracy: 0.184 - ETA: 22s - loss: 3.0996 - accuracy: 0.184 - ETA: 21s - loss: 3.0991 - accuracy: 0.184 - ETA: 20s - loss: 3.0986 - accuracy: 0.184 - ETA: 20s - loss: 3.0981 - accuracy: 0.184 - ETA: 19s - loss: 3.0977 - accuracy: 0.184 - ETA: 18s - loss: 3.0973 - accuracy: 0.184 - ETA: 17s - loss: 3.0972 - accuracy: 0.184 - ETA: 16s - loss: 3.0964 - accuracy: 0.185 - ETA: 15s - loss: 3.0962 - accuracy: 0.184 - ETA: 14s - loss: 3.0959 - accuracy: 0.185 - ETA: 13s - loss: 3.0952 - accuracy: 0.185 - ETA: 12s - loss: 3.0949 - accuracy: 0.185 - ETA: 11s - loss: 3.0948 - accuracy: 0.185 - ETA: 10s - loss: 3.0948 - accuracy: 0.185 - ETA: 9s - loss: 3.0941 - accuracy: 0.185 - ETA: 8s - loss: 3.0935 - accuracy: 0.18 - ETA: 7s - loss: 3.0931 - accuracy: 0.18 - ETA: 6s - loss: 3.0928 - accuracy: 0.18 - ETA: 5s - loss: 3.0929 - accuracy: 0.18 - ETA: 4s - loss: 3.0922 - accuracy: 0.18 - ETA: 3s - loss: 3.0924 - accuracy: 0.18 - ETA: 2s - loss: 3.0920 - accuracy: 0.18 - ETA: 1s - loss: 3.0912 - accuracy: 0.18 - ETA: 1s - loss: 3.0907 - accuracy: 0.18 - ETA: 0s - loss: 3.0900 - accuracy: 0.18 - 338s 8ms/step - loss: 3.0900 - accuracy: 0.1856 - val_loss: 4.0871 - val_accuracy: 0.0340\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:11 - loss: 3.0806 - accuracy: 0.19 - ETA: 5:08 - loss: 3.0896 - accuracy: 0.16 - ETA: 5:08 - loss: 3.0344 - accuracy: 0.18 - ETA: 5:04 - loss: 3.0213 - accuracy: 0.18 - ETA: 5:05 - loss: 3.0217 - accuracy: 0.19 - ETA: 5:09 - loss: 3.0584 - accuracy: 0.19 - ETA: 5:07 - loss: 3.0321 - accuracy: 0.19 - ETA: 5:08 - loss: 3.0357 - accuracy: 0.19 - ETA: 5:05 - loss: 3.0307 - accuracy: 0.19 - ETA: 5:05 - loss: 3.0283 - accuracy: 0.19 - ETA: 5:04 - loss: 3.0336 - accuracy: 0.19 - ETA: 5:02 - loss: 3.0395 - accuracy: 0.19 - ETA: 5:01 - loss: 3.0280 - accuracy: 0.20 - ETA: 4:59 - loss: 3.0163 - accuracy: 0.20 - ETA: 4:59 - loss: 2.9936 - accuracy: 0.20 - ETA: 4:59 - loss: 2.9773 - accuracy: 0.21 - ETA: 4:58 - loss: 2.9771 - accuracy: 0.21 - ETA: 4:57 - loss: 2.9778 - accuracy: 0.20 - ETA: 4:55 - loss: 2.9708 - accuracy: 0.20 - ETA: 4:54 - loss: 2.9847 - accuracy: 0.20 - ETA: 4:55 - loss: 2.9871 - accuracy: 0.20 - ETA: 4:53 - loss: 2.9969 - accuracy: 0.20 - ETA: 4:52 - loss: 2.9844 - accuracy: 0.20 - ETA: 4:50 - loss: 2.9969 - accuracy: 0.20 - ETA: 4:49 - loss: 2.9861 - accuracy: 0.20 - ETA: 4:49 - loss: 2.9776 - accuracy: 0.20 - ETA: 4:48 - loss: 2.9772 - accuracy: 0.20 - ETA: 4:47 - loss: 2.9873 - accuracy: 0.20 - ETA: 4:47 - loss: 2.9786 - accuracy: 0.20 - ETA: 4:45 - loss: 2.9793 - accuracy: 0.20 - ETA: 4:44 - loss: 2.9878 - accuracy: 0.20 - ETA: 4:43 - loss: 2.9888 - accuracy: 0.20 - ETA: 4:43 - loss: 2.9928 - accuracy: 0.20 - ETA: 4:42 - loss: 2.9926 - accuracy: 0.20 - ETA: 4:41 - loss: 2.9964 - accuracy: 0.20 - ETA: 4:41 - loss: 2.9963 - accuracy: 0.20 - ETA: 4:40 - loss: 3.0017 - accuracy: 0.20 - ETA: 4:40 - loss: 3.0066 - accuracy: 0.20 - ETA: 4:38 - loss: 3.0105 - accuracy: 0.20 - ETA: 4:38 - loss: 3.0125 - accuracy: 0.20 - ETA: 4:36 - loss: 3.0099 - accuracy: 0.20 - ETA: 4:36 - loss: 3.0038 - accuracy: 0.20 - ETA: 4:35 - loss: 3.0075 - accuracy: 0.20 - ETA: 4:34 - loss: 3.0088 - accuracy: 0.20 - ETA: 4:33 - loss: 3.0087 - accuracy: 0.20 - ETA: 4:32 - loss: 3.0037 - accuracy: 0.20 - ETA: 4:31 - loss: 3.0086 - accuracy: 0.20 - ETA: 4:30 - loss: 3.0119 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0123 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0093 - accuracy: 0.20 - ETA: 4:27 - loss: 3.0097 - accuracy: 0.20 - ETA: 4:26 - loss: 3.0088 - accuracy: 0.20 - ETA: 4:25 - loss: 3.0073 - accuracy: 0.20 - ETA: 4:24 - loss: 3.0090 - accuracy: 0.20 - ETA: 4:23 - loss: 3.0093 - accuracy: 0.20 - ETA: 4:22 - loss: 3.0133 - accuracy: 0.20 - ETA: 4:21 - loss: 3.0126 - accuracy: 0.20 - ETA: 4:20 - loss: 3.0138 - accuracy: 0.20 - ETA: 4:19 - loss: 3.0158 - accuracy: 0.20 - ETA: 4:18 - loss: 3.0132 - accuracy: 0.20 - ETA: 4:17 - loss: 3.0183 - accuracy: 0.20 - ETA: 4:16 - loss: 3.0160 - accuracy: 0.20 - ETA: 4:15 - loss: 3.0139 - accuracy: 0.20 - ETA: 4:14 - loss: 3.0127 - accuracy: 0.20 - ETA: 4:13 - loss: 3.0125 - accuracy: 0.20 - ETA: 4:12 - loss: 3.0110 - accuracy: 0.20 - ETA: 4:11 - loss: 3.0129 - accuracy: 0.20 - ETA: 4:10 - loss: 3.0163 - accuracy: 0.20 - ETA: 4:09 - loss: 3.0171 - accuracy: 0.20 - ETA: 4:08 - loss: 3.0162 - accuracy: 0.20 - ETA: 4:07 - loss: 3.0192 - accuracy: 0.20 - ETA: 4:06 - loss: 3.0197 - accuracy: 0.20 - ETA: 4:05 - loss: 3.0168 - accuracy: 0.20 - ETA: 4:04 - loss: 3.0176 - accuracy: 0.20 - ETA: 4:03 - loss: 3.0169 - accuracy: 0.20 - ETA: 4:02 - loss: 3.0159 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0140 - accuracy: 0.20 - ETA: 4:00 - loss: 3.0121 - accuracy: 0.20 - ETA: 3:59 - loss: 3.0085 - accuracy: 0.20 - ETA: 3:58 - loss: 3.0099 - accuracy: 0.20 - ETA: 3:57 - loss: 3.0063 - accuracy: 0.20 - ETA: 3:56 - loss: 3.0065 - accuracy: 0.20 - ETA: 3:55 - loss: 3.0033 - accuracy: 0.20 - ETA: 3:54 - loss: 3.0024 - accuracy: 0.20 - ETA: 3:54 - loss: 3.0027 - accuracy: 0.20 - ETA: 3:53 - loss: 3.0049 - accuracy: 0.20 - ETA: 3:52 - loss: 3.0043 - accuracy: 0.20 - ETA: 3:51 - loss: 3.0054 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0033 - accuracy: 0.20 - ETA: 3:49 - loss: 3.0024 - accuracy: 0.20 - ETA: 3:48 - loss: 3.0015 - accuracy: 0.20 - ETA: 3:47 - loss: 3.0036 - accuracy: 0.20 - ETA: 3:46 - loss: 3.0042 - accuracy: 0.20 - ETA: 3:45 - loss: 3.0026 - accuracy: 0.20 - ETA: 3:44 - loss: 3.0053 - accuracy: 0.20 - ETA: 3:43 - loss: 3.0059 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0078 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0099 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0125 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0145 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0154 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0155 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0170 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0186 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0174 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0181 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0165 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0150 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0145 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0164 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0159 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0176 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0169 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0157 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0170 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0166 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0173 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0159 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0156 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0175 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0184 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0190 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0218 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0253 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0257 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0282 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0281 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0271 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0267 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0274 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0294 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0303 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0337 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0334 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0339 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0332 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0333 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0340 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0345 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0347 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0378 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0385 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0389 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0388 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0376 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0378 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0387 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0390 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0388 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0394 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0392 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0401 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0412 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0404 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0397 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0385 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0404 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0409 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0405 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0413 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0426 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0424 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0415 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0423 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0413 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0418 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0413 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0417 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0413 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0412 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0420 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0418 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0416 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0411 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0408 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0404 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0402 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0408 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0397 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0404 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0414 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0420 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0416 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0416 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0418 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0416 - accuracy: 0.1912"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0416 - accuracy: 0.19 - ETA: 2:16 - loss: 3.0428 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0427 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0428 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0422 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0418 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0411 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0427 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0437 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0451 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0445 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0455 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0462 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0471 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0467 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0470 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0478 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0478 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0477 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0486 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0484 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0487 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0485 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0491 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0492 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0497 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0508 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0500 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0505 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0503 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0498 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0500 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0506 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0505 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0500 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0516 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0514 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0511 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0510 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0509 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0506 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0525 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0522 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0530 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0534 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0549 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0550 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0557 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0557 - accuracy: 0.19 - ETA: 1:30 - loss: 3.0549 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0547 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0534 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0535 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0542 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0541 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0548 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0549 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0549 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0543 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0550 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0548 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0549 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0553 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0547 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0551 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0563 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0555 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0551 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0552 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0550 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0544 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0550 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0545 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0545 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0547 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0553 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0557 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0558 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0557 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0557 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0554 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0566 - accuracy: 0.18 - ETA: 59s - loss: 3.0567 - accuracy: 0.1897 - ETA: 58s - loss: 3.0562 - accuracy: 0.189 - ETA: 57s - loss: 3.0565 - accuracy: 0.189 - ETA: 56s - loss: 3.0567 - accuracy: 0.189 - ETA: 55s - loss: 3.0566 - accuracy: 0.189 - ETA: 54s - loss: 3.0571 - accuracy: 0.189 - ETA: 53s - loss: 3.0563 - accuracy: 0.189 - ETA: 52s - loss: 3.0558 - accuracy: 0.189 - ETA: 51s - loss: 3.0569 - accuracy: 0.189 - ETA: 50s - loss: 3.0574 - accuracy: 0.189 - ETA: 49s - loss: 3.0571 - accuracy: 0.189 - ETA: 48s - loss: 3.0580 - accuracy: 0.189 - ETA: 47s - loss: 3.0579 - accuracy: 0.189 - ETA: 46s - loss: 3.0582 - accuracy: 0.189 - ETA: 45s - loss: 3.0585 - accuracy: 0.189 - ETA: 44s - loss: 3.0579 - accuracy: 0.189 - ETA: 43s - loss: 3.0582 - accuracy: 0.189 - ETA: 42s - loss: 3.0588 - accuracy: 0.189 - ETA: 41s - loss: 3.0594 - accuracy: 0.189 - ETA: 40s - loss: 3.0600 - accuracy: 0.189 - ETA: 40s - loss: 3.0608 - accuracy: 0.189 - ETA: 39s - loss: 3.0613 - accuracy: 0.189 - ETA: 38s - loss: 3.0613 - accuracy: 0.189 - ETA: 37s - loss: 3.0606 - accuracy: 0.189 - ETA: 36s - loss: 3.0607 - accuracy: 0.189 - ETA: 35s - loss: 3.0601 - accuracy: 0.189 - ETA: 34s - loss: 3.0606 - accuracy: 0.189 - ETA: 33s - loss: 3.0604 - accuracy: 0.189 - ETA: 32s - loss: 3.0595 - accuracy: 0.189 - ETA: 31s - loss: 3.0594 - accuracy: 0.189 - ETA: 30s - loss: 3.0598 - accuracy: 0.189 - ETA: 29s - loss: 3.0602 - accuracy: 0.189 - ETA: 28s - loss: 3.0605 - accuracy: 0.189 - ETA: 27s - loss: 3.0609 - accuracy: 0.189 - ETA: 26s - loss: 3.0604 - accuracy: 0.189 - ETA: 25s - loss: 3.0608 - accuracy: 0.189 - ETA: 24s - loss: 3.0611 - accuracy: 0.189 - ETA: 23s - loss: 3.0605 - accuracy: 0.189 - ETA: 22s - loss: 3.0604 - accuracy: 0.189 - ETA: 21s - loss: 3.0608 - accuracy: 0.189 - ETA: 20s - loss: 3.0604 - accuracy: 0.189 - ETA: 20s - loss: 3.0604 - accuracy: 0.189 - ETA: 19s - loss: 3.0609 - accuracy: 0.189 - ETA: 18s - loss: 3.0612 - accuracy: 0.189 - ETA: 17s - loss: 3.0609 - accuracy: 0.189 - ETA: 16s - loss: 3.0615 - accuracy: 0.188 - ETA: 15s - loss: 3.0615 - accuracy: 0.188 - ETA: 14s - loss: 3.0611 - accuracy: 0.188 - ETA: 13s - loss: 3.0614 - accuracy: 0.188 - ETA: 12s - loss: 3.0612 - accuracy: 0.188 - ETA: 11s - loss: 3.0613 - accuracy: 0.188 - ETA: 10s - loss: 3.0608 - accuracy: 0.188 - ETA: 9s - loss: 3.0608 - accuracy: 0.188 - ETA: 8s - loss: 3.0604 - accuracy: 0.18 - ETA: 7s - loss: 3.0605 - accuracy: 0.18 - ETA: 6s - loss: 3.0601 - accuracy: 0.18 - ETA: 5s - loss: 3.0604 - accuracy: 0.18 - ETA: 4s - loss: 3.0600 - accuracy: 0.18 - ETA: 3s - loss: 3.0595 - accuracy: 0.18 - ETA: 2s - loss: 3.0597 - accuracy: 0.18 - ETA: 1s - loss: 3.0594 - accuracy: 0.18 - ETA: 1s - loss: 3.0600 - accuracy: 0.18 - ETA: 0s - loss: 3.0601 - accuracy: 0.18 - 338s 8ms/step - loss: 3.0602 - accuracy: 0.1887 - val_loss: 4.1958 - val_accuracy: 0.0358\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:02 - loss: 2.9225 - accuracy: 0.25 - ETA: 4:57 - loss: 2.9796 - accuracy: 0.23 - ETA: 5:02 - loss: 3.0387 - accuracy: 0.21 - ETA: 5:05 - loss: 3.0494 - accuracy: 0.21 - ETA: 5:04 - loss: 3.0315 - accuracy: 0.21 - ETA: 5:03 - loss: 2.9726 - accuracy: 0.22 - ETA: 5:04 - loss: 2.9828 - accuracy: 0.21 - ETA: 5:04 - loss: 2.9922 - accuracy: 0.21 - ETA: 5:03 - loss: 2.9979 - accuracy: 0.20 - ETA: 5:04 - loss: 2.9878 - accuracy: 0.20 - ETA: 5:04 - loss: 3.0060 - accuracy: 0.20 - ETA: 5:02 - loss: 3.0195 - accuracy: 0.20 - ETA: 5:01 - loss: 3.0356 - accuracy: 0.19 - ETA: 5:01 - loss: 3.0337 - accuracy: 0.19 - ETA: 4:59 - loss: 3.0324 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0406 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0355 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0366 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0386 - accuracy: 0.19 - ETA: 4:55 - loss: 3.0343 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0436 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0372 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0374 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0445 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0444 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0481 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0581 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0637 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0643 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0696 - accuracy: 0.18 - ETA: 4:46 - loss: 3.0734 - accuracy: 0.18 - ETA: 4:45 - loss: 3.0767 - accuracy: 0.18 - ETA: 4:44 - loss: 3.0690 - accuracy: 0.18 - ETA: 4:43 - loss: 3.0679 - accuracy: 0.18 - ETA: 4:42 - loss: 3.0744 - accuracy: 0.18 - ETA: 4:40 - loss: 3.0707 - accuracy: 0.18 - ETA: 4:39 - loss: 3.0738 - accuracy: 0.18 - ETA: 4:38 - loss: 3.0753 - accuracy: 0.18 - ETA: 4:37 - loss: 3.0747 - accuracy: 0.18 - ETA: 4:36 - loss: 3.0839 - accuracy: 0.18 - ETA: 4:35 - loss: 3.0803 - accuracy: 0.18 - ETA: 4:34 - loss: 3.0888 - accuracy: 0.18 - ETA: 4:33 - loss: 3.0883 - accuracy: 0.18 - ETA: 4:32 - loss: 3.0850 - accuracy: 0.18 - ETA: 4:32 - loss: 3.0882 - accuracy: 0.18 - ETA: 4:31 - loss: 3.0922 - accuracy: 0.18 - ETA: 4:29 - loss: 3.1020 - accuracy: 0.18 - ETA: 4:28 - loss: 3.0998 - accuracy: 0.18 - ETA: 4:27 - loss: 3.0984 - accuracy: 0.18 - ETA: 4:27 - loss: 3.0980 - accuracy: 0.18 - ETA: 4:26 - loss: 3.0964 - accuracy: 0.18 - ETA: 4:25 - loss: 3.0999 - accuracy: 0.18 - ETA: 4:24 - loss: 3.1020 - accuracy: 0.18 - ETA: 4:23 - loss: 3.0987 - accuracy: 0.18 - ETA: 4:22 - loss: 3.0986 - accuracy: 0.18 - ETA: 4:21 - loss: 3.1008 - accuracy: 0.18 - ETA: 4:20 - loss: 3.1031 - accuracy: 0.18 - ETA: 4:19 - loss: 3.1024 - accuracy: 0.18 - ETA: 4:18 - loss: 3.1014 - accuracy: 0.18 - ETA: 4:17 - loss: 3.1013 - accuracy: 0.18 - ETA: 4:16 - loss: 3.1020 - accuracy: 0.18 - ETA: 4:15 - loss: 3.1018 - accuracy: 0.18 - ETA: 4:14 - loss: 3.1023 - accuracy: 0.18 - ETA: 4:13 - loss: 3.1011 - accuracy: 0.18 - ETA: 4:12 - loss: 3.1025 - accuracy: 0.18 - ETA: 4:11 - loss: 3.1031 - accuracy: 0.18 - ETA: 4:10 - loss: 3.0995 - accuracy: 0.18 - ETA: 4:09 - loss: 3.0991 - accuracy: 0.18 - ETA: 4:08 - loss: 3.0994 - accuracy: 0.18 - ETA: 4:07 - loss: 3.0998 - accuracy: 0.18 - ETA: 4:06 - loss: 3.1002 - accuracy: 0.18 - ETA: 4:05 - loss: 3.1023 - accuracy: 0.18 - ETA: 4:04 - loss: 3.1017 - accuracy: 0.18 - ETA: 4:03 - loss: 3.1027 - accuracy: 0.18 - ETA: 4:02 - loss: 3.1014 - accuracy: 0.18 - ETA: 4:01 - loss: 3.1010 - accuracy: 0.18 - ETA: 4:00 - loss: 3.0993 - accuracy: 0.18 - ETA: 3:59 - loss: 3.0977 - accuracy: 0.18 - ETA: 3:58 - loss: 3.0962 - accuracy: 0.18 - ETA: 3:57 - loss: 3.0985 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0975 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0979 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0976 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0964 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0944 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0932 - accuracy: 0.18 - ETA: 3:48 - loss: 3.0903 - accuracy: 0.18 - ETA: 3:47 - loss: 3.0890 - accuracy: 0.18 - ETA: 3:46 - loss: 3.0882 - accuracy: 0.18 - ETA: 3:44 - loss: 3.0883 - accuracy: 0.18 - ETA: 3:43 - loss: 3.0873 - accuracy: 0.18 - ETA: 3:42 - loss: 3.0878 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0877 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0881 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0902 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0872 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0860 - accuracy: 0.18 - ETA: 3:37 - loss: 3.0870 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0868 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0842 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0839 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0830 - accuracy: 0.18 - ETA: 3:33 - loss: 3.0824 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0809 - accuracy: 0.18 - ETA: 3:31 - loss: 3.0829 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0818 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0814 - accuracy: 0.18 - ETA: 3:28 - loss: 3.0807 - accuracy: 0.18 - ETA: 3:27 - loss: 3.0804 - accuracy: 0.18 - ETA: 3:26 - loss: 3.0804 - accuracy: 0.18 - ETA: 3:26 - loss: 3.0793 - accuracy: 0.18 - ETA: 3:25 - loss: 3.0808 - accuracy: 0.18 - ETA: 3:24 - loss: 3.0817 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0814 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0818 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0815 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0825 - accuracy: 0.18 - ETA: 3:19 - loss: 3.0811 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0795 - accuracy: 0.18 - ETA: 3:17 - loss: 3.0810 - accuracy: 0.18 - ETA: 3:16 - loss: 3.0790 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0801 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0801 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0798 - accuracy: 0.18 - ETA: 3:13 - loss: 3.0805 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0799 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0803 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0801 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0794 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0792 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0787 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0789 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0801 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0809 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0811 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0827 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0837 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0834 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0825 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0826 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0823 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0836 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0823 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0817 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0818 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0811 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0805 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0804 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0806 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0810 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0811 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0805 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0794 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0804 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0818 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0811 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0806 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0809 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0806 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0799 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0796 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0794 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0792 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0792 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0777 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0767 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0759 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0757 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0755 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0759 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0761 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0757 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0748 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0749 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0745 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0742 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0750 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0746 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0737 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0739 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0760 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0775 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0802 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0809 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0806 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0805 - accuracy: 0.1862"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:15 - loss: 3.0805 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0798 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0809 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0816 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0814 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0813 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0822 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0817 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0809 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0806 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0805 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0808 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0807 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0799 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0790 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0780 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0774 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0772 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0770 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0780 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0777 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0775 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0774 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0780 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0777 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0777 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0778 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0779 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0782 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0775 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0781 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0781 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0786 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0794 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0796 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0801 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0806 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0813 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0812 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0810 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0813 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0807 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0800 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0802 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0797 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0800 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0809 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0807 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0800 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0809 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0814 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0820 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0815 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0818 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0812 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0812 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0877 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0892 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0894 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0898 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0900 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0905 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0905 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0900 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0895 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0890 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0882 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0882 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0881 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0891 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0897 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0904 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0903 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0903 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0901 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0900 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0893 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0890 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0886 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0882 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0879 - accuracy: 0.18 - ETA: 59s - loss: 3.0870 - accuracy: 0.1850 - ETA: 58s - loss: 3.0868 - accuracy: 0.185 - ETA: 57s - loss: 3.0870 - accuracy: 0.185 - ETA: 56s - loss: 3.0871 - accuracy: 0.185 - ETA: 55s - loss: 3.0872 - accuracy: 0.185 - ETA: 54s - loss: 3.0870 - accuracy: 0.185 - ETA: 53s - loss: 3.0869 - accuracy: 0.185 - ETA: 52s - loss: 3.0868 - accuracy: 0.185 - ETA: 51s - loss: 3.0866 - accuracy: 0.185 - ETA: 51s - loss: 3.0861 - accuracy: 0.185 - ETA: 50s - loss: 3.0859 - accuracy: 0.185 - ETA: 49s - loss: 3.0857 - accuracy: 0.185 - ETA: 48s - loss: 3.0861 - accuracy: 0.185 - ETA: 47s - loss: 3.0858 - accuracy: 0.185 - ETA: 46s - loss: 3.0857 - accuracy: 0.185 - ETA: 45s - loss: 3.0853 - accuracy: 0.185 - ETA: 44s - loss: 3.0848 - accuracy: 0.185 - ETA: 43s - loss: 3.0848 - accuracy: 0.186 - ETA: 42s - loss: 3.0847 - accuracy: 0.186 - ETA: 41s - loss: 3.0849 - accuracy: 0.185 - ETA: 40s - loss: 3.0847 - accuracy: 0.185 - ETA: 39s - loss: 3.0849 - accuracy: 0.185 - ETA: 38s - loss: 3.0851 - accuracy: 0.185 - ETA: 37s - loss: 3.0852 - accuracy: 0.185 - ETA: 36s - loss: 3.0847 - accuracy: 0.185 - ETA: 35s - loss: 3.0849 - accuracy: 0.185 - ETA: 34s - loss: 3.0849 - accuracy: 0.185 - ETA: 34s - loss: 3.0845 - accuracy: 0.185 - ETA: 33s - loss: 3.0842 - accuracy: 0.185 - ETA: 32s - loss: 3.0840 - accuracy: 0.185 - ETA: 31s - loss: 3.0834 - accuracy: 0.185 - ETA: 30s - loss: 3.0838 - accuracy: 0.185 - ETA: 29s - loss: 3.0841 - accuracy: 0.185 - ETA: 28s - loss: 3.0839 - accuracy: 0.185 - ETA: 27s - loss: 3.0835 - accuracy: 0.185 - ETA: 26s - loss: 3.0830 - accuracy: 0.185 - ETA: 25s - loss: 3.0830 - accuracy: 0.185 - ETA: 24s - loss: 3.0831 - accuracy: 0.185 - ETA: 23s - loss: 3.0831 - accuracy: 0.185 - ETA: 22s - loss: 3.0829 - accuracy: 0.185 - ETA: 21s - loss: 3.0829 - accuracy: 0.185 - ETA: 20s - loss: 3.0829 - accuracy: 0.186 - ETA: 19s - loss: 3.0831 - accuracy: 0.185 - ETA: 18s - loss: 3.0842 - accuracy: 0.185 - ETA: 18s - loss: 3.0842 - accuracy: 0.185 - ETA: 17s - loss: 3.0840 - accuracy: 0.186 - ETA: 16s - loss: 3.0832 - accuracy: 0.186 - ETA: 15s - loss: 3.0820 - accuracy: 0.186 - ETA: 14s - loss: 3.0814 - accuracy: 0.186 - ETA: 13s - loss: 3.0808 - accuracy: 0.186 - ETA: 12s - loss: 3.0810 - accuracy: 0.186 - ETA: 11s - loss: 3.0811 - accuracy: 0.186 - ETA: 10s - loss: 3.0810 - accuracy: 0.186 - ETA: 9s - loss: 3.0812 - accuracy: 0.186 - ETA: 8s - loss: 3.0821 - accuracy: 0.18 - ETA: 7s - loss: 3.0814 - accuracy: 0.18 - ETA: 6s - loss: 3.0813 - accuracy: 0.18 - ETA: 5s - loss: 3.0821 - accuracy: 0.18 - ETA: 4s - loss: 3.0821 - accuracy: 0.18 - ETA: 3s - loss: 3.0820 - accuracy: 0.18 - ETA: 2s - loss: 3.0821 - accuracy: 0.18 - ETA: 1s - loss: 3.0822 - accuracy: 0.18 - ETA: 1s - loss: 3.0825 - accuracy: 0.18 - ETA: 0s - loss: 3.0831 - accuracy: 0.18 - 336s 8ms/step - loss: 3.0828 - accuracy: 0.1866 - val_loss: 4.3830 - val_accuracy: 0.0390\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:09 - loss: 2.9961 - accuracy: 0.16 - ETA: 5:06 - loss: 3.0499 - accuracy: 0.16 - ETA: 5:07 - loss: 3.0749 - accuracy: 0.16 - ETA: 5:07 - loss: 3.1725 - accuracy: 0.18 - ETA: 5:06 - loss: 3.1902 - accuracy: 0.17 - ETA: 5:05 - loss: 3.1711 - accuracy: 0.16 - ETA: 5:05 - loss: 3.1721 - accuracy: 0.17 - ETA: 5:07 - loss: 3.1622 - accuracy: 0.17 - ETA: 5:06 - loss: 3.1827 - accuracy: 0.17 - ETA: 5:04 - loss: 3.2219 - accuracy: 0.17 - ETA: 5:03 - loss: 3.2015 - accuracy: 0.18 - ETA: 5:02 - loss: 3.2140 - accuracy: 0.17 - ETA: 4:59 - loss: 3.2273 - accuracy: 0.17 - ETA: 4:58 - loss: 3.3803 - accuracy: 0.17 - ETA: 4:57 - loss: 3.3697 - accuracy: 0.17 - ETA: 4:55 - loss: 3.3570 - accuracy: 0.17 - ETA: 4:55 - loss: 3.3369 - accuracy: 0.18 - ETA: 4:55 - loss: 3.3203 - accuracy: 0.18 - ETA: 4:55 - loss: 3.3066 - accuracy: 0.18 - ETA: 4:56 - loss: 3.2944 - accuracy: 0.18 - ETA: 4:55 - loss: 3.2895 - accuracy: 0.17 - ETA: 4:55 - loss: 3.2754 - accuracy: 0.18 - ETA: 4:54 - loss: 3.2759 - accuracy: 0.17 - ETA: 4:53 - loss: 3.2693 - accuracy: 0.18 - ETA: 4:52 - loss: 3.2633 - accuracy: 0.18 - ETA: 4:50 - loss: 3.2491 - accuracy: 0.18 - ETA: 4:49 - loss: 3.2428 - accuracy: 0.18 - ETA: 4:48 - loss: 3.2432 - accuracy: 0.18 - ETA: 4:47 - loss: 3.2377 - accuracy: 0.18 - ETA: 4:46 - loss: 3.2350 - accuracy: 0.17 - ETA: 4:45 - loss: 3.2331 - accuracy: 0.18 - ETA: 4:44 - loss: 3.2373 - accuracy: 0.17 - ETA: 4:43 - loss: 3.2362 - accuracy: 0.17 - ETA: 4:42 - loss: 3.2311 - accuracy: 0.17 - ETA: 4:41 - loss: 3.2288 - accuracy: 0.17 - ETA: 4:39 - loss: 3.2229 - accuracy: 0.17 - ETA: 4:38 - loss: 3.2176 - accuracy: 0.17 - ETA: 4:37 - loss: 3.2147 - accuracy: 0.17 - ETA: 4:36 - loss: 3.2152 - accuracy: 0.17 - ETA: 4:35 - loss: 3.2136 - accuracy: 0.17 - ETA: 4:34 - loss: 3.2055 - accuracy: 0.17 - ETA: 4:33 - loss: 3.2009 - accuracy: 0.18 - ETA: 4:32 - loss: 3.1939 - accuracy: 0.18 - ETA: 4:31 - loss: 3.1913 - accuracy: 0.18 - ETA: 4:30 - loss: 3.1929 - accuracy: 0.18 - ETA: 4:29 - loss: 3.1924 - accuracy: 0.18 - ETA: 4:28 - loss: 3.1923 - accuracy: 0.18 - ETA: 4:27 - loss: 3.1899 - accuracy: 0.18 - ETA: 4:26 - loss: 3.1835 - accuracy: 0.18 - ETA: 4:25 - loss: 3.1796 - accuracy: 0.18 - ETA: 4:24 - loss: 3.1705 - accuracy: 0.18 - ETA: 4:23 - loss: 3.1708 - accuracy: 0.18 - ETA: 4:22 - loss: 3.1683 - accuracy: 0.18 - ETA: 4:21 - loss: 3.1690 - accuracy: 0.18 - ETA: 4:20 - loss: 3.1685 - accuracy: 0.18 - ETA: 4:19 - loss: 3.1699 - accuracy: 0.18 - ETA: 4:18 - loss: 3.1699 - accuracy: 0.18 - ETA: 4:16 - loss: 3.1691 - accuracy: 0.18 - ETA: 4:15 - loss: 3.1672 - accuracy: 0.18 - ETA: 4:15 - loss: 3.1654 - accuracy: 0.18 - ETA: 4:14 - loss: 3.1656 - accuracy: 0.18 - ETA: 4:13 - loss: 3.1644 - accuracy: 0.18 - ETA: 4:12 - loss: 3.1612 - accuracy: 0.18 - ETA: 4:11 - loss: 3.1602 - accuracy: 0.18 - ETA: 4:10 - loss: 3.1560 - accuracy: 0.18 - ETA: 4:09 - loss: 3.1511 - accuracy: 0.18 - ETA: 4:08 - loss: 3.1494 - accuracy: 0.18 - ETA: 4:07 - loss: 3.1459 - accuracy: 0.18 - ETA: 4:06 - loss: 3.1459 - accuracy: 0.18 - ETA: 4:05 - loss: 3.1471 - accuracy: 0.18 - ETA: 4:05 - loss: 3.1463 - accuracy: 0.18 - ETA: 4:04 - loss: 3.1461 - accuracy: 0.18 - ETA: 4:03 - loss: 3.1455 - accuracy: 0.18 - ETA: 4:02 - loss: 3.1447 - accuracy: 0.18 - ETA: 4:01 - loss: 3.1427 - accuracy: 0.18 - ETA: 4:00 - loss: 3.1411 - accuracy: 0.18 - ETA: 3:59 - loss: 3.1375 - accuracy: 0.18 - ETA: 3:58 - loss: 3.1391 - accuracy: 0.18 - ETA: 3:57 - loss: 3.1383 - accuracy: 0.18 - ETA: 3:56 - loss: 3.1359 - accuracy: 0.18 - ETA: 3:56 - loss: 3.1335 - accuracy: 0.18 - ETA: 3:55 - loss: 3.1323 - accuracy: 0.18 - ETA: 3:54 - loss: 3.1302 - accuracy: 0.18 - ETA: 3:53 - loss: 3.1284 - accuracy: 0.18 - ETA: 3:52 - loss: 3.1286 - accuracy: 0.18 - ETA: 3:51 - loss: 3.1275 - accuracy: 0.18 - ETA: 3:50 - loss: 3.1272 - accuracy: 0.18 - ETA: 3:50 - loss: 3.1273 - accuracy: 0.18 - ETA: 3:49 - loss: 3.1253 - accuracy: 0.18 - ETA: 3:48 - loss: 3.1216 - accuracy: 0.18 - ETA: 3:47 - loss: 3.1208 - accuracy: 0.19 - ETA: 3:46 - loss: 3.1170 - accuracy: 0.19 - ETA: 3:45 - loss: 3.1167 - accuracy: 0.19 - ETA: 3:44 - loss: 3.1163 - accuracy: 0.19 - ETA: 3:43 - loss: 3.1158 - accuracy: 0.19 - ETA: 3:43 - loss: 3.1137 - accuracy: 0.19 - ETA: 3:42 - loss: 3.1126 - accuracy: 0.19 - ETA: 3:41 - loss: 3.1109 - accuracy: 0.19 - ETA: 3:40 - loss: 3.1098 - accuracy: 0.19 - ETA: 3:39 - loss: 3.1090 - accuracy: 0.19 - ETA: 3:38 - loss: 3.1068 - accuracy: 0.19 - ETA: 3:37 - loss: 3.1050 - accuracy: 0.19 - ETA: 3:36 - loss: 3.1031 - accuracy: 0.19 - ETA: 3:35 - loss: 3.1003 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0984 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0981 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0962 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0950 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0952 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0960 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0964 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0949 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0945 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0919 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0919 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0913 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0919 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0936 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0924 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0908 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0915 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0918 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0907 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0901 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0888 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0888 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0870 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0850 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0842 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0842 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0848 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0843 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0862 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0863 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0856 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0855 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0842 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0833 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0823 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0816 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0813 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0816 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0825 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0821 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0825 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0827 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0831 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0822 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0818 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0814 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0813 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0808 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0808 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0813 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0821 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0833 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0826 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0828 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0820 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0829 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0825 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0822 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0813 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0821 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0823 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0826 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0826 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0824 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0816 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0824 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0830 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0827 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0822 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0816 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0816 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0801 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0797 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0806 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0808 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0810 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0807 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0815 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0812 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0817 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0811 - accuracy: 0.19 - ETA: 2:17 - loss: 3.0805 - accuracy: 0.1934"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0814 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0818 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0815 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0821 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0834 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0833 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0824 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0819 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0808 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0813 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0815 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0812 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0819 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0824 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0819 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0835 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0839 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0837 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0826 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0837 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0843 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0844 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0838 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0839 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0840 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0836 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0839 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0846 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0844 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0844 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0845 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0853 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0857 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0853 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0849 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0842 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0834 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0839 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0833 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0842 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0840 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0831 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0835 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0842 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0840 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0837 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0840 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0838 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0835 - accuracy: 0.19 - ETA: 1:30 - loss: 3.0839 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0834 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0834 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0839 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0839 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0844 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0842 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0850 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0845 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0845 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0837 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0839 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0849 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0841 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0833 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0831 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0824 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0823 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0826 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0818 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0815 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0819 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0813 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0819 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0817 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0815 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0813 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0804 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0804 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0808 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0807 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0812 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0810 - accuracy: 0.19 - ETA: 59s - loss: 3.0812 - accuracy: 0.1925 - ETA: 58s - loss: 3.0806 - accuracy: 0.192 - ETA: 57s - loss: 3.0802 - accuracy: 0.192 - ETA: 56s - loss: 3.0801 - accuracy: 0.192 - ETA: 55s - loss: 3.0799 - accuracy: 0.192 - ETA: 54s - loss: 3.0805 - accuracy: 0.192 - ETA: 53s - loss: 3.0798 - accuracy: 0.192 - ETA: 52s - loss: 3.0795 - accuracy: 0.192 - ETA: 51s - loss: 3.0785 - accuracy: 0.192 - ETA: 50s - loss: 3.0782 - accuracy: 0.193 - ETA: 49s - loss: 3.0789 - accuracy: 0.193 - ETA: 48s - loss: 3.0791 - accuracy: 0.193 - ETA: 47s - loss: 3.0790 - accuracy: 0.193 - ETA: 46s - loss: 3.0791 - accuracy: 0.193 - ETA: 45s - loss: 3.0791 - accuracy: 0.192 - ETA: 44s - loss: 3.0786 - accuracy: 0.192 - ETA: 43s - loss: 3.0790 - accuracy: 0.192 - ETA: 42s - loss: 3.0788 - accuracy: 0.192 - ETA: 42s - loss: 3.0785 - accuracy: 0.192 - ETA: 41s - loss: 3.0792 - accuracy: 0.192 - ETA: 40s - loss: 3.0790 - accuracy: 0.192 - ETA: 39s - loss: 3.0793 - accuracy: 0.192 - ETA: 38s - loss: 3.0796 - accuracy: 0.192 - ETA: 37s - loss: 3.0796 - accuracy: 0.192 - ETA: 36s - loss: 3.0797 - accuracy: 0.192 - ETA: 35s - loss: 3.0802 - accuracy: 0.192 - ETA: 34s - loss: 3.0812 - accuracy: 0.191 - ETA: 33s - loss: 3.0814 - accuracy: 0.191 - ETA: 32s - loss: 3.0808 - accuracy: 0.191 - ETA: 31s - loss: 3.0813 - accuracy: 0.191 - ETA: 30s - loss: 3.0816 - accuracy: 0.191 - ETA: 29s - loss: 3.0820 - accuracy: 0.191 - ETA: 28s - loss: 3.0817 - accuracy: 0.191 - ETA: 27s - loss: 3.0818 - accuracy: 0.191 - ETA: 26s - loss: 3.0819 - accuracy: 0.191 - ETA: 25s - loss: 3.0808 - accuracy: 0.191 - ETA: 24s - loss: 3.0810 - accuracy: 0.191 - ETA: 23s - loss: 3.0805 - accuracy: 0.191 - ETA: 22s - loss: 3.0802 - accuracy: 0.191 - ETA: 22s - loss: 3.0804 - accuracy: 0.191 - ETA: 21s - loss: 3.0810 - accuracy: 0.191 - ETA: 20s - loss: 3.0813 - accuracy: 0.191 - ETA: 19s - loss: 3.0815 - accuracy: 0.191 - ETA: 18s - loss: 3.0813 - accuracy: 0.191 - ETA: 17s - loss: 3.0814 - accuracy: 0.191 - ETA: 16s - loss: 3.0810 - accuracy: 0.191 - ETA: 15s - loss: 3.0813 - accuracy: 0.191 - ETA: 14s - loss: 3.0810 - accuracy: 0.191 - ETA: 13s - loss: 3.0803 - accuracy: 0.191 - ETA: 12s - loss: 3.0796 - accuracy: 0.191 - ETA: 11s - loss: 3.0798 - accuracy: 0.191 - ETA: 10s - loss: 3.0800 - accuracy: 0.191 - ETA: 9s - loss: 3.0791 - accuracy: 0.191 - ETA: 8s - loss: 3.0787 - accuracy: 0.19 - ETA: 7s - loss: 3.0788 - accuracy: 0.19 - ETA: 6s - loss: 3.0787 - accuracy: 0.19 - ETA: 5s - loss: 3.0778 - accuracy: 0.19 - ETA: 4s - loss: 3.0774 - accuracy: 0.19 - ETA: 3s - loss: 3.0772 - accuracy: 0.19 - ETA: 2s - loss: 3.0767 - accuracy: 0.19 - ETA: 1s - loss: 3.0761 - accuracy: 0.19 - ETA: 1s - loss: 3.0767 - accuracy: 0.19 - ETA: 0s - loss: 3.0763 - accuracy: 0.19 - 339s 8ms/step - loss: 3.0764 - accuracy: 0.1922 - val_loss: 4.1340 - val_accuracy: 0.0342\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:10 - loss: 2.9807 - accuracy: 0.18 - ETA: 5:10 - loss: 2.9552 - accuracy: 0.19 - ETA: 5:10 - loss: 3.0155 - accuracy: 0.17 - ETA: 5:10 - loss: 3.0369 - accuracy: 0.18 - ETA: 5:06 - loss: 3.0543 - accuracy: 0.18 - ETA: 5:06 - loss: 3.0544 - accuracy: 0.17 - ETA: 5:05 - loss: 3.0535 - accuracy: 0.17 - ETA: 5:04 - loss: 3.0616 - accuracy: 0.17 - ETA: 5:07 - loss: 3.0677 - accuracy: 0.18 - ETA: 5:08 - loss: 3.0625 - accuracy: 0.18 - ETA: 5:08 - loss: 3.0637 - accuracy: 0.18 - ETA: 5:08 - loss: 3.0556 - accuracy: 0.18 - ETA: 5:07 - loss: 3.0503 - accuracy: 0.18 - ETA: 5:06 - loss: 3.0632 - accuracy: 0.18 - ETA: 5:05 - loss: 3.0773 - accuracy: 0.17 - ETA: 5:04 - loss: 3.0739 - accuracy: 0.18 - ETA: 5:02 - loss: 3.0578 - accuracy: 0.18 - ETA: 5:01 - loss: 3.0550 - accuracy: 0.18 - ETA: 4:59 - loss: 3.0647 - accuracy: 0.18 - ETA: 4:59 - loss: 3.0607 - accuracy: 0.18 - ETA: 4:57 - loss: 3.0614 - accuracy: 0.18 - ETA: 4:56 - loss: 3.0557 - accuracy: 0.18 - ETA: 4:55 - loss: 3.0498 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0460 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0488 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0413 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0364 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0366 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0409 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0415 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0558 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0608 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0597 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0605 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0603 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0573 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0541 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0509 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0625 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0615 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0641 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0579 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0557 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0592 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0617 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0596 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0592 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0553 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0505 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0504 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0508 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0498 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0499 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0464 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0451 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0442 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0439 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0418 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0440 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0420 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0413 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0401 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0402 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0392 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0365 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0368 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0381 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0370 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0359 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0357 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0365 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0406 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0419 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0463 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0477 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0483 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0468 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0470 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0473 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0500 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0504 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0482 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0500 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0478 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0485 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0489 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0499 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0490 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0499 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0492 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0506 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0507 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0524 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0528 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0534 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0536 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0545 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0565 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0564 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0561 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0555 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0535 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0541 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0540 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0534 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0533 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0505 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0511 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0506 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0503 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0503 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0509 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0517 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0496 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0518 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0501 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0473 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0461 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0458 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0477 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0467 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0468 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0452 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0451 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0448 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0446 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0454 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0457 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0448 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0443 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0427 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0413 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0433 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0420 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0434 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0448 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0458 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0460 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0441 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0438 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0433 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0429 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0425 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0417 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0425 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0432 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0441 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0432 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0424 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0438 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0439 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0434 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0429 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0436 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0437 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0438 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0444 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0441 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0444 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0445 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0449 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0430 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0428 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0436 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0434 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0432 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0432 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0431 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0431 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0432 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0437 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0444 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0439 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0439 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0430 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0427 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0428 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0427 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0414 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0407 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0401 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0393 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0385 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0382 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0385 - accuracy: 0.19 - ETA: 2:17 - loss: 3.0382 - accuracy: 0.1949"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0372 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0379 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0382 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0382 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0376 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0375 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0382 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0384 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0382 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0383 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0382 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0391 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0385 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0391 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0395 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0392 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0398 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0396 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0393 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0389 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0395 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0403 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0425 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0470 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0486 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0485 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0486 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0481 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0487 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0494 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0492 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0487 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0479 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0480 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0489 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0511 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0516 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0519 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0524 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0526 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0517 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0520 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0524 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0522 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0526 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0523 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0522 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0511 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0500 - accuracy: 0.19 - ETA: 1:30 - loss: 3.0505 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0509 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0512 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0516 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0509 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0516 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0512 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0513 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0518 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0508 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0511 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0506 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0497 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0494 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0492 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0490 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0485 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0487 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0498 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0494 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0498 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0495 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0493 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0486 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0493 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0492 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0491 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0494 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0492 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0483 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0499 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0499 - accuracy: 0.19 - ETA: 59s - loss: 3.0502 - accuracy: 0.1946 - ETA: 58s - loss: 3.0506 - accuracy: 0.194 - ETA: 58s - loss: 3.0501 - accuracy: 0.194 - ETA: 57s - loss: 3.0497 - accuracy: 0.194 - ETA: 56s - loss: 3.0497 - accuracy: 0.194 - ETA: 55s - loss: 3.0494 - accuracy: 0.194 - ETA: 54s - loss: 3.0493 - accuracy: 0.194 - ETA: 53s - loss: 3.0491 - accuracy: 0.194 - ETA: 52s - loss: 3.0493 - accuracy: 0.194 - ETA: 51s - loss: 3.0504 - accuracy: 0.194 - ETA: 50s - loss: 3.0499 - accuracy: 0.194 - ETA: 49s - loss: 3.0495 - accuracy: 0.194 - ETA: 48s - loss: 3.0498 - accuracy: 0.194 - ETA: 47s - loss: 3.0505 - accuracy: 0.194 - ETA: 46s - loss: 3.0504 - accuracy: 0.194 - ETA: 45s - loss: 3.0498 - accuracy: 0.194 - ETA: 44s - loss: 3.0498 - accuracy: 0.194 - ETA: 43s - loss: 3.0492 - accuracy: 0.194 - ETA: 42s - loss: 3.0500 - accuracy: 0.194 - ETA: 41s - loss: 3.0500 - accuracy: 0.194 - ETA: 40s - loss: 3.0501 - accuracy: 0.194 - ETA: 39s - loss: 3.0493 - accuracy: 0.195 - ETA: 39s - loss: 3.0486 - accuracy: 0.195 - ETA: 38s - loss: 3.0485 - accuracy: 0.195 - ETA: 37s - loss: 3.0490 - accuracy: 0.195 - ETA: 36s - loss: 3.0497 - accuracy: 0.195 - ETA: 35s - loss: 3.0496 - accuracy: 0.195 - ETA: 34s - loss: 3.0495 - accuracy: 0.195 - ETA: 33s - loss: 3.0487 - accuracy: 0.195 - ETA: 32s - loss: 3.0479 - accuracy: 0.195 - ETA: 31s - loss: 3.0473 - accuracy: 0.195 - ETA: 30s - loss: 3.0470 - accuracy: 0.195 - ETA: 29s - loss: 3.0475 - accuracy: 0.195 - ETA: 28s - loss: 3.0475 - accuracy: 0.195 - ETA: 27s - loss: 3.0471 - accuracy: 0.195 - ETA: 26s - loss: 3.0466 - accuracy: 0.195 - ETA: 25s - loss: 3.0467 - accuracy: 0.195 - ETA: 24s - loss: 3.0456 - accuracy: 0.196 - ETA: 23s - loss: 3.0456 - accuracy: 0.196 - ETA: 22s - loss: 3.0463 - accuracy: 0.196 - ETA: 21s - loss: 3.0459 - accuracy: 0.196 - ETA: 20s - loss: 3.0454 - accuracy: 0.196 - ETA: 20s - loss: 3.0457 - accuracy: 0.196 - ETA: 19s - loss: 3.0451 - accuracy: 0.196 - ETA: 18s - loss: 3.0450 - accuracy: 0.196 - ETA: 17s - loss: 3.0452 - accuracy: 0.196 - ETA: 16s - loss: 3.0451 - accuracy: 0.196 - ETA: 15s - loss: 3.0451 - accuracy: 0.196 - ETA: 14s - loss: 3.0450 - accuracy: 0.196 - ETA: 13s - loss: 3.0451 - accuracy: 0.196 - ETA: 12s - loss: 3.0453 - accuracy: 0.196 - ETA: 11s - loss: 3.0456 - accuracy: 0.196 - ETA: 10s - loss: 3.0457 - accuracy: 0.196 - ETA: 9s - loss: 3.0461 - accuracy: 0.196 - ETA: 8s - loss: 3.0467 - accuracy: 0.19 - ETA: 7s - loss: 3.0464 - accuracy: 0.19 - ETA: 6s - loss: 3.0463 - accuracy: 0.19 - ETA: 5s - loss: 3.0474 - accuracy: 0.19 - ETA: 4s - loss: 3.0487 - accuracy: 0.19 - ETA: 3s - loss: 3.0498 - accuracy: 0.19 - ETA: 2s - loss: 3.0509 - accuracy: 0.19 - ETA: 1s - loss: 3.0511 - accuracy: 0.19 - ETA: 1s - loss: 3.0514 - accuracy: 0.19 - ETA: 0s - loss: 3.0516 - accuracy: 0.19 - 338s 8ms/step - loss: 3.0517 - accuracy: 0.1960 - val_loss: 4.0599 - val_accuracy: 0.0344\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:25 - loss: 2.8112 - accuracy: 0.25 - ETA: 5:21 - loss: 2.9894 - accuracy: 0.23 - ETA: 5:21 - loss: 3.0603 - accuracy: 0.21 - ETA: 5:22 - loss: 3.0832 - accuracy: 0.19 - ETA: 5:14 - loss: 3.0704 - accuracy: 0.19 - ETA: 5:11 - loss: 3.0879 - accuracy: 0.19 - ETA: 5:11 - loss: 3.0834 - accuracy: 0.18 - ETA: 5:11 - loss: 3.0902 - accuracy: 0.18 - ETA: 5:11 - loss: 3.0780 - accuracy: 0.18 - ETA: 5:10 - loss: 3.0723 - accuracy: 0.19 - ETA: 5:09 - loss: 3.0641 - accuracy: 0.19 - ETA: 5:06 - loss: 3.0611 - accuracy: 0.19 - ETA: 5:05 - loss: 3.0554 - accuracy: 0.19 - ETA: 5:03 - loss: 3.0598 - accuracy: 0.19 - ETA: 5:01 - loss: 3.0554 - accuracy: 0.20 - ETA: 5:00 - loss: 3.0589 - accuracy: 0.20 - ETA: 4:59 - loss: 3.0576 - accuracy: 0.19 - ETA: 4:57 - loss: 3.0514 - accuracy: 0.20 - ETA: 4:57 - loss: 3.0466 - accuracy: 0.20 - ETA: 4:55 - loss: 3.0416 - accuracy: 0.20 - ETA: 4:54 - loss: 3.0452 - accuracy: 0.20 - ETA: 4:53 - loss: 3.0435 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0424 - accuracy: 0.20 - ETA: 4:52 - loss: 3.0347 - accuracy: 0.20 - ETA: 4:50 - loss: 3.0327 - accuracy: 0.20 - ETA: 4:49 - loss: 3.0313 - accuracy: 0.20 - ETA: 4:49 - loss: 3.0325 - accuracy: 0.20 - ETA: 4:47 - loss: 3.0304 - accuracy: 0.20 - ETA: 4:46 - loss: 3.0259 - accuracy: 0.20 - ETA: 4:46 - loss: 3.0252 - accuracy: 0.20 - ETA: 4:45 - loss: 3.0251 - accuracy: 0.20 - ETA: 4:44 - loss: 3.0278 - accuracy: 0.20 - ETA: 4:43 - loss: 3.0276 - accuracy: 0.20 - ETA: 4:41 - loss: 3.0293 - accuracy: 0.20 - ETA: 4:40 - loss: 3.0255 - accuracy: 0.20 - ETA: 4:39 - loss: 3.0306 - accuracy: 0.20 - ETA: 4:38 - loss: 3.0247 - accuracy: 0.20 - ETA: 4:37 - loss: 3.0257 - accuracy: 0.20 - ETA: 4:36 - loss: 3.0269 - accuracy: 0.20 - ETA: 4:35 - loss: 3.0299 - accuracy: 0.20 - ETA: 4:34 - loss: 3.0324 - accuracy: 0.20 - ETA: 4:33 - loss: 3.0360 - accuracy: 0.20 - ETA: 4:32 - loss: 3.0353 - accuracy: 0.20 - ETA: 4:31 - loss: 3.0352 - accuracy: 0.20 - ETA: 4:30 - loss: 3.0387 - accuracy: 0.20 - ETA: 4:29 - loss: 3.0396 - accuracy: 0.20 - ETA: 4:28 - loss: 3.0382 - accuracy: 0.20 - ETA: 4:28 - loss: 3.0378 - accuracy: 0.20 - ETA: 4:27 - loss: 3.0355 - accuracy: 0.20 - ETA: 4:26 - loss: 3.0300 - accuracy: 0.20 - ETA: 4:24 - loss: 3.0260 - accuracy: 0.20 - ETA: 4:23 - loss: 3.0248 - accuracy: 0.20 - ETA: 4:22 - loss: 3.0221 - accuracy: 0.20 - ETA: 4:21 - loss: 3.0233 - accuracy: 0.20 - ETA: 4:21 - loss: 3.0236 - accuracy: 0.20 - ETA: 4:20 - loss: 3.0267 - accuracy: 0.20 - ETA: 4:19 - loss: 3.0251 - accuracy: 0.20 - ETA: 4:18 - loss: 3.0247 - accuracy: 0.20 - ETA: 4:17 - loss: 3.0240 - accuracy: 0.20 - ETA: 4:16 - loss: 3.0242 - accuracy: 0.20 - ETA: 4:15 - loss: 3.0221 - accuracy: 0.20 - ETA: 4:14 - loss: 3.0245 - accuracy: 0.20 - ETA: 4:13 - loss: 3.0207 - accuracy: 0.20 - ETA: 4:12 - loss: 3.0218 - accuracy: 0.20 - ETA: 4:11 - loss: 3.0182 - accuracy: 0.20 - ETA: 4:11 - loss: 3.0191 - accuracy: 0.20 - ETA: 4:10 - loss: 3.0235 - accuracy: 0.20 - ETA: 4:09 - loss: 3.0251 - accuracy: 0.20 - ETA: 4:08 - loss: 3.0244 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0244 - accuracy: 0.20 - ETA: 4:06 - loss: 3.0248 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0239 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0236 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0246 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0251 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0269 - accuracy: 0.20 - ETA: 4:01 - loss: 3.0305 - accuracy: 0.20 - ETA: 4:00 - loss: 3.0343 - accuracy: 0.20 - ETA: 3:59 - loss: 3.0388 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0430 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0448 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0461 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0500 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0527 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0568 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0586 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0584 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0579 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0570 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0563 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0583 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0590 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0605 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0631 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0603 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0599 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0627 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0646 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0651 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0657 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0652 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0669 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0676 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0669 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0680 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0673 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0685 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0675 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0668 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0664 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0660 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0677 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0697 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0704 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0684 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0670 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0653 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0661 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0653 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0662 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0684 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0680 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0675 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0687 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0695 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0674 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0670 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0668 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0670 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0665 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0658 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0669 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0669 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0676 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0684 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0694 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0704 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0693 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0703 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0711 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0718 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0721 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0720 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0708 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0714 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0730 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0734 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0748 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0758 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0756 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0752 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0757 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0762 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0756 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0747 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0737 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0747 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0739 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0730 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0728 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0720 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0730 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0737 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0731 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0738 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0739 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0738 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0737 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0748 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0741 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0736 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0735 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0737 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0739 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0744 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0751 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0757 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0755 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0750 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0740 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0743 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0745 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0737 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0738 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0744 - accuracy: 0.19 - ETA: 2:17 - loss: 3.0745 - accuracy: 0.1912"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0736 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0725 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0718 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0710 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0716 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0720 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0719 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0716 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0716 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0710 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0716 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0710 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0701 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0700 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0699 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0699 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0703 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0713 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0719 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0717 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0716 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0716 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0719 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0711 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0718 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0710 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0708 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0715 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0707 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0708 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0708 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0705 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0707 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0712 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0718 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0717 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0722 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0726 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0729 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0746 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0753 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0756 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0763 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0764 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0757 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0754 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0757 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0758 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0759 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0771 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0763 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0772 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0776 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0789 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0799 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0802 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0808 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0811 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0818 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0823 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0828 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0832 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0836 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0846 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0849 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0854 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0851 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0852 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0850 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0852 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0854 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0848 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0847 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0843 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0853 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0848 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0847 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0850 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0855 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0854 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0855 - accuracy: 0.18 - ETA: 59s - loss: 3.0854 - accuracy: 0.1872 - ETA: 58s - loss: 3.0858 - accuracy: 0.187 - ETA: 57s - loss: 3.0859 - accuracy: 0.187 - ETA: 56s - loss: 3.0853 - accuracy: 0.187 - ETA: 55s - loss: 3.0870 - accuracy: 0.187 - ETA: 54s - loss: 3.0872 - accuracy: 0.187 - ETA: 54s - loss: 3.0884 - accuracy: 0.187 - ETA: 53s - loss: 3.0894 - accuracy: 0.187 - ETA: 52s - loss: 3.0896 - accuracy: 0.186 - ETA: 51s - loss: 3.0893 - accuracy: 0.187 - ETA: 50s - loss: 3.0896 - accuracy: 0.187 - ETA: 49s - loss: 3.0900 - accuracy: 0.186 - ETA: 48s - loss: 3.0902 - accuracy: 0.187 - ETA: 47s - loss: 3.0905 - accuracy: 0.187 - ETA: 46s - loss: 3.0903 - accuracy: 0.187 - ETA: 45s - loss: 3.0904 - accuracy: 0.187 - ETA: 44s - loss: 3.0905 - accuracy: 0.186 - ETA: 43s - loss: 3.0908 - accuracy: 0.186 - ETA: 42s - loss: 3.0909 - accuracy: 0.186 - ETA: 41s - loss: 3.0912 - accuracy: 0.186 - ETA: 40s - loss: 3.0915 - accuracy: 0.186 - ETA: 39s - loss: 3.0921 - accuracy: 0.186 - ETA: 38s - loss: 3.0924 - accuracy: 0.186 - ETA: 37s - loss: 3.0917 - accuracy: 0.186 - ETA: 37s - loss: 3.0922 - accuracy: 0.186 - ETA: 36s - loss: 3.0920 - accuracy: 0.186 - ETA: 35s - loss: 3.0913 - accuracy: 0.186 - ETA: 34s - loss: 3.0911 - accuracy: 0.186 - ETA: 33s - loss: 3.0912 - accuracy: 0.186 - ETA: 32s - loss: 3.0912 - accuracy: 0.186 - ETA: 31s - loss: 3.0915 - accuracy: 0.186 - ETA: 30s - loss: 3.0917 - accuracy: 0.186 - ETA: 29s - loss: 3.0918 - accuracy: 0.186 - ETA: 28s - loss: 3.0918 - accuracy: 0.186 - ETA: 27s - loss: 3.0916 - accuracy: 0.186 - ETA: 26s - loss: 3.0913 - accuracy: 0.186 - ETA: 25s - loss: 3.0911 - accuracy: 0.186 - ETA: 24s - loss: 3.0907 - accuracy: 0.186 - ETA: 23s - loss: 3.0907 - accuracy: 0.186 - ETA: 22s - loss: 3.0905 - accuracy: 0.186 - ETA: 21s - loss: 3.0906 - accuracy: 0.186 - ETA: 20s - loss: 3.0907 - accuracy: 0.186 - ETA: 19s - loss: 3.0903 - accuracy: 0.186 - ETA: 19s - loss: 3.0910 - accuracy: 0.186 - ETA: 18s - loss: 3.0912 - accuracy: 0.186 - ETA: 17s - loss: 3.0915 - accuracy: 0.186 - ETA: 16s - loss: 3.0913 - accuracy: 0.186 - ETA: 15s - loss: 3.0911 - accuracy: 0.186 - ETA: 14s - loss: 3.0909 - accuracy: 0.186 - ETA: 13s - loss: 3.0903 - accuracy: 0.186 - ETA: 12s - loss: 3.0908 - accuracy: 0.186 - ETA: 11s - loss: 3.0909 - accuracy: 0.186 - ETA: 10s - loss: 3.0913 - accuracy: 0.186 - ETA: 9s - loss: 3.0906 - accuracy: 0.186 - ETA: 8s - loss: 3.0899 - accuracy: 0.18 - ETA: 7s - loss: 3.0895 - accuracy: 0.18 - ETA: 6s - loss: 3.0887 - accuracy: 0.18 - ETA: 5s - loss: 3.0887 - accuracy: 0.18 - ETA: 4s - loss: 3.0890 - accuracy: 0.18 - ETA: 3s - loss: 3.0888 - accuracy: 0.18 - ETA: 2s - loss: 3.0885 - accuracy: 0.18 - ETA: 1s - loss: 3.0881 - accuracy: 0.18 - ETA: 1s - loss: 3.0880 - accuracy: 0.18 - ETA: 0s - loss: 3.0878 - accuracy: 0.18 - 337s 8ms/step - loss: 3.0876 - accuracy: 0.1870 - val_loss: 4.3748 - val_accuracy: 0.0386\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:04 - loss: 3.0782 - accuracy: 0.18 - ETA: 5:04 - loss: 3.0875 - accuracy: 0.19 - ETA: 5:00 - loss: 2.9865 - accuracy: 0.21 - ETA: 5:04 - loss: 2.9667 - accuracy: 0.22 - ETA: 5:05 - loss: 2.9589 - accuracy: 0.22 - ETA: 5:02 - loss: 2.9645 - accuracy: 0.21 - ETA: 5:04 - loss: 2.9745 - accuracy: 0.21 - ETA: 5:02 - loss: 3.0116 - accuracy: 0.20 - ETA: 5:01 - loss: 3.0483 - accuracy: 0.19 - ETA: 5:01 - loss: 3.0177 - accuracy: 0.20 - ETA: 4:58 - loss: 3.0169 - accuracy: 0.20 - ETA: 4:57 - loss: 3.0297 - accuracy: 0.20 - ETA: 4:56 - loss: 3.0293 - accuracy: 0.20 - ETA: 4:57 - loss: 3.0214 - accuracy: 0.20 - ETA: 4:57 - loss: 3.0231 - accuracy: 0.20 - ETA: 4:57 - loss: 3.0209 - accuracy: 0.20 - ETA: 4:57 - loss: 3.0196 - accuracy: 0.20 - ETA: 4:55 - loss: 3.0344 - accuracy: 0.20 - ETA: 4:55 - loss: 3.0332 - accuracy: 0.20 - ETA: 4:54 - loss: 3.0187 - accuracy: 0.20 - ETA: 4:53 - loss: 3.0264 - accuracy: 0.20 - ETA: 4:52 - loss: 3.0233 - accuracy: 0.20 - ETA: 4:51 - loss: 3.0249 - accuracy: 0.20 - ETA: 4:51 - loss: 3.0255 - accuracy: 0.20 - ETA: 4:49 - loss: 3.0232 - accuracy: 0.20 - ETA: 4:48 - loss: 3.0179 - accuracy: 0.20 - ETA: 4:48 - loss: 3.0280 - accuracy: 0.20 - ETA: 4:47 - loss: 3.0227 - accuracy: 0.20 - ETA: 4:46 - loss: 3.0252 - accuracy: 0.20 - ETA: 4:44 - loss: 3.0258 - accuracy: 0.20 - ETA: 4:43 - loss: 3.0306 - accuracy: 0.20 - ETA: 4:42 - loss: 3.0296 - accuracy: 0.20 - ETA: 4:41 - loss: 3.0294 - accuracy: 0.20 - ETA: 4:39 - loss: 3.0316 - accuracy: 0.20 - ETA: 4:38 - loss: 3.0267 - accuracy: 0.20 - ETA: 4:37 - loss: 3.0227 - accuracy: 0.20 - ETA: 4:36 - loss: 3.0236 - accuracy: 0.20 - ETA: 4:35 - loss: 3.0270 - accuracy: 0.20 - ETA: 4:34 - loss: 3.0306 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0288 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0295 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0319 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0311 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0320 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0345 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0412 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0393 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0383 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0415 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0402 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0409 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0448 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0412 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0448 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0433 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0425 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0436 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0420 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0439 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0428 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0411 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0409 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0397 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0380 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0419 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0433 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0457 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0450 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0457 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0448 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0421 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0447 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0649 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0619 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0626 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0634 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0628 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0636 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0654 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0655 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0661 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0704 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0718 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0715 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0704 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0700 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0695 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0701 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0700 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0702 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0725 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0729 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0724 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0711 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0720 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0737 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0727 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0752 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0756 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0759 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0755 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0765 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0779 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0787 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0780 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0780 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0782 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0789 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0798 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0812 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0834 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0863 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0859 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0880 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0882 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0907 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0922 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0925 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0935 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0945 - accuracy: 0.18 - ETA: 3:19 - loss: 3.0957 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0949 - accuracy: 0.18 - ETA: 3:17 - loss: 3.0953 - accuracy: 0.18 - ETA: 3:16 - loss: 3.0934 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0926 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0927 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0940 - accuracy: 0.18 - ETA: 3:13 - loss: 3.0935 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0930 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0931 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0928 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0933 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0943 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0951 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0944 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0971 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0956 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0951 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0959 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0961 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0963 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0954 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0958 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0952 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0935 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0948 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0944 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0943 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0929 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0926 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0924 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0942 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0952 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0921 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0925 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0906 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0903 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0909 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0927 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0940 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0953 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0948 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0952 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0945 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0950 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0930 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0923 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0929 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0921 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0922 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0929 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0920 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0921 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0907 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0913 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0920 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0932 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0931 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0930 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0931 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0936 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0937 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0953 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0968 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0968 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0969 - accuracy: 0.1864"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0965 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0967 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0976 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0971 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0965 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0963 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0971 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0968 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0974 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0984 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0986 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0989 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0990 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0983 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0977 - accuracy: 0.18 - ETA: 2:02 - loss: 3.1019 - accuracy: 0.18 - ETA: 2:01 - loss: 3.1022 - accuracy: 0.18 - ETA: 2:01 - loss: 3.1031 - accuracy: 0.18 - ETA: 2:00 - loss: 3.1015 - accuracy: 0.18 - ETA: 1:59 - loss: 3.1033 - accuracy: 0.18 - ETA: 1:58 - loss: 3.1050 - accuracy: 0.18 - ETA: 1:57 - loss: 3.1057 - accuracy: 0.18 - ETA: 1:56 - loss: 3.1068 - accuracy: 0.18 - ETA: 1:55 - loss: 3.1093 - accuracy: 0.18 - ETA: 1:54 - loss: 3.1089 - accuracy: 0.18 - ETA: 1:53 - loss: 3.1099 - accuracy: 0.18 - ETA: 1:52 - loss: 3.1110 - accuracy: 0.18 - ETA: 1:51 - loss: 3.1110 - accuracy: 0.18 - ETA: 1:50 - loss: 3.1111 - accuracy: 0.18 - ETA: 1:49 - loss: 3.1121 - accuracy: 0.18 - ETA: 1:48 - loss: 3.1113 - accuracy: 0.18 - ETA: 1:47 - loss: 3.1122 - accuracy: 0.18 - ETA: 1:46 - loss: 3.1123 - accuracy: 0.18 - ETA: 1:45 - loss: 3.1124 - accuracy: 0.18 - ETA: 1:44 - loss: 3.1120 - accuracy: 0.18 - ETA: 1:43 - loss: 3.1116 - accuracy: 0.18 - ETA: 1:42 - loss: 3.1115 - accuracy: 0.18 - ETA: 1:41 - loss: 3.1116 - accuracy: 0.18 - ETA: 1:41 - loss: 3.1109 - accuracy: 0.18 - ETA: 1:40 - loss: 3.1105 - accuracy: 0.18 - ETA: 1:39 - loss: 3.1103 - accuracy: 0.18 - ETA: 1:38 - loss: 3.1099 - accuracy: 0.18 - ETA: 1:37 - loss: 3.1097 - accuracy: 0.18 - ETA: 1:36 - loss: 3.1101 - accuracy: 0.18 - ETA: 1:35 - loss: 3.1103 - accuracy: 0.18 - ETA: 1:34 - loss: 3.1098 - accuracy: 0.18 - ETA: 1:33 - loss: 3.1097 - accuracy: 0.18 - ETA: 1:32 - loss: 3.1095 - accuracy: 0.18 - ETA: 1:31 - loss: 3.1090 - accuracy: 0.18 - ETA: 1:30 - loss: 3.1093 - accuracy: 0.18 - ETA: 1:29 - loss: 3.1091 - accuracy: 0.18 - ETA: 1:28 - loss: 3.1093 - accuracy: 0.18 - ETA: 1:27 - loss: 3.1092 - accuracy: 0.18 - ETA: 1:26 - loss: 3.1090 - accuracy: 0.18 - ETA: 1:25 - loss: 3.1082 - accuracy: 0.18 - ETA: 1:24 - loss: 3.1091 - accuracy: 0.18 - ETA: 1:23 - loss: 3.1082 - accuracy: 0.18 - ETA: 1:22 - loss: 3.1080 - accuracy: 0.18 - ETA: 1:22 - loss: 3.1070 - accuracy: 0.18 - ETA: 1:21 - loss: 3.1067 - accuracy: 0.18 - ETA: 1:20 - loss: 3.1063 - accuracy: 0.18 - ETA: 1:19 - loss: 3.1059 - accuracy: 0.18 - ETA: 1:18 - loss: 3.1060 - accuracy: 0.18 - ETA: 1:17 - loss: 3.1054 - accuracy: 0.18 - ETA: 1:16 - loss: 3.1052 - accuracy: 0.18 - ETA: 1:15 - loss: 3.1060 - accuracy: 0.18 - ETA: 1:14 - loss: 3.1054 - accuracy: 0.18 - ETA: 1:13 - loss: 3.1052 - accuracy: 0.18 - ETA: 1:12 - loss: 3.1047 - accuracy: 0.18 - ETA: 1:11 - loss: 3.1048 - accuracy: 0.18 - ETA: 1:10 - loss: 3.1040 - accuracy: 0.18 - ETA: 1:09 - loss: 3.1043 - accuracy: 0.18 - ETA: 1:08 - loss: 3.1047 - accuracy: 0.18 - ETA: 1:07 - loss: 3.1049 - accuracy: 0.18 - ETA: 1:06 - loss: 3.1054 - accuracy: 0.18 - ETA: 1:05 - loss: 3.1046 - accuracy: 0.18 - ETA: 1:04 - loss: 3.1039 - accuracy: 0.18 - ETA: 1:03 - loss: 3.1035 - accuracy: 0.18 - ETA: 1:02 - loss: 3.1030 - accuracy: 0.18 - ETA: 1:01 - loss: 3.1029 - accuracy: 0.18 - ETA: 1:01 - loss: 3.1023 - accuracy: 0.18 - ETA: 1:00 - loss: 3.1019 - accuracy: 0.18 - ETA: 59s - loss: 3.1016 - accuracy: 0.1871 - ETA: 58s - loss: 3.1010 - accuracy: 0.187 - ETA: 57s - loss: 3.1009 - accuracy: 0.187 - ETA: 56s - loss: 3.1011 - accuracy: 0.187 - ETA: 55s - loss: 3.1014 - accuracy: 0.186 - ETA: 54s - loss: 3.1004 - accuracy: 0.187 - ETA: 53s - loss: 3.1005 - accuracy: 0.187 - ETA: 52s - loss: 3.1001 - accuracy: 0.187 - ETA: 51s - loss: 3.0996 - accuracy: 0.187 - ETA: 50s - loss: 3.0993 - accuracy: 0.187 - ETA: 49s - loss: 3.0995 - accuracy: 0.187 - ETA: 48s - loss: 3.0999 - accuracy: 0.187 - ETA: 47s - loss: 3.0993 - accuracy: 0.187 - ETA: 46s - loss: 3.0995 - accuracy: 0.187 - ETA: 45s - loss: 3.0995 - accuracy: 0.187 - ETA: 44s - loss: 3.0995 - accuracy: 0.187 - ETA: 43s - loss: 3.0991 - accuracy: 0.187 - ETA: 42s - loss: 3.0991 - accuracy: 0.187 - ETA: 41s - loss: 3.0992 - accuracy: 0.187 - ETA: 41s - loss: 3.0996 - accuracy: 0.187 - ETA: 40s - loss: 3.0995 - accuracy: 0.187 - ETA: 39s - loss: 3.1004 - accuracy: 0.186 - ETA: 38s - loss: 3.1002 - accuracy: 0.187 - ETA: 37s - loss: 3.1004 - accuracy: 0.187 - ETA: 36s - loss: 3.1009 - accuracy: 0.186 - ETA: 35s - loss: 3.1009 - accuracy: 0.186 - ETA: 34s - loss: 3.1009 - accuracy: 0.186 - ETA: 33s - loss: 3.1015 - accuracy: 0.186 - ETA: 32s - loss: 3.1015 - accuracy: 0.186 - ETA: 31s - loss: 3.1020 - accuracy: 0.186 - ETA: 30s - loss: 3.1017 - accuracy: 0.186 - ETA: 29s - loss: 3.1018 - accuracy: 0.186 - ETA: 28s - loss: 3.1014 - accuracy: 0.186 - ETA: 27s - loss: 3.1013 - accuracy: 0.186 - ETA: 26s - loss: 3.1017 - accuracy: 0.186 - ETA: 25s - loss: 3.1011 - accuracy: 0.186 - ETA: 24s - loss: 3.1012 - accuracy: 0.186 - ETA: 23s - loss: 3.1010 - accuracy: 0.186 - ETA: 22s - loss: 3.1007 - accuracy: 0.186 - ETA: 21s - loss: 3.1000 - accuracy: 0.186 - ETA: 21s - loss: 3.0998 - accuracy: 0.186 - ETA: 20s - loss: 3.0994 - accuracy: 0.186 - ETA: 19s - loss: 3.0992 - accuracy: 0.186 - ETA: 18s - loss: 3.0995 - accuracy: 0.186 - ETA: 17s - loss: 3.0993 - accuracy: 0.186 - ETA: 16s - loss: 3.0993 - accuracy: 0.186 - ETA: 15s - loss: 3.0995 - accuracy: 0.186 - ETA: 14s - loss: 3.0993 - accuracy: 0.186 - ETA: 13s - loss: 3.0994 - accuracy: 0.186 - ETA: 12s - loss: 3.0993 - accuracy: 0.186 - ETA: 11s - loss: 3.0987 - accuracy: 0.186 - ETA: 10s - loss: 3.0993 - accuracy: 0.186 - ETA: 9s - loss: 3.0993 - accuracy: 0.186 - ETA: 8s - loss: 3.0989 - accuracy: 0.18 - ETA: 7s - loss: 3.0985 - accuracy: 0.18 - ETA: 6s - loss: 3.0986 - accuracy: 0.18 - ETA: 5s - loss: 3.0989 - accuracy: 0.18 - ETA: 4s - loss: 3.0988 - accuracy: 0.18 - ETA: 3s - loss: 3.0989 - accuracy: 0.18 - ETA: 2s - loss: 3.0985 - accuracy: 0.18 - ETA: 1s - loss: 3.0987 - accuracy: 0.18 - ETA: 1s - loss: 3.0988 - accuracy: 0.18 - ETA: 0s - loss: 3.0990 - accuracy: 0.18 - 338s 8ms/step - loss: 3.0989 - accuracy: 0.1867 - val_loss: 4.2202 - val_accuracy: 0.0346\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:24 - loss: 3.0153 - accuracy: 0.22 - ETA: 5:13 - loss: 3.0480 - accuracy: 0.20 - ETA: 5:16 - loss: 3.0638 - accuracy: 0.19 - ETA: 5:15 - loss: 3.0187 - accuracy: 0.20 - ETA: 5:12 - loss: 3.0393 - accuracy: 0.19 - ETA: 5:09 - loss: 3.0737 - accuracy: 0.19 - ETA: 5:06 - loss: 3.0709 - accuracy: 0.18 - ETA: 5:03 - loss: 3.0693 - accuracy: 0.19 - ETA: 5:03 - loss: 3.0674 - accuracy: 0.19 - ETA: 5:01 - loss: 3.0777 - accuracy: 0.18 - ETA: 5:01 - loss: 3.0917 - accuracy: 0.18 - ETA: 5:00 - loss: 3.0934 - accuracy: 0.18 - ETA: 4:59 - loss: 3.1000 - accuracy: 0.18 - ETA: 4:59 - loss: 3.0879 - accuracy: 0.18 - ETA: 4:57 - loss: 3.0788 - accuracy: 0.18 - ETA: 4:57 - loss: 3.0826 - accuracy: 0.18 - ETA: 4:55 - loss: 3.0716 - accuracy: 0.18 - ETA: 4:54 - loss: 3.0746 - accuracy: 0.18 - ETA: 4:54 - loss: 3.0749 - accuracy: 0.18 - ETA: 4:53 - loss: 3.0767 - accuracy: 0.18 - ETA: 4:52 - loss: 3.0772 - accuracy: 0.18 - ETA: 4:51 - loss: 3.0697 - accuracy: 0.18 - ETA: 4:50 - loss: 3.0703 - accuracy: 0.18 - ETA: 4:49 - loss: 3.0764 - accuracy: 0.18 - ETA: 4:48 - loss: 3.0767 - accuracy: 0.18 - ETA: 4:47 - loss: 3.0751 - accuracy: 0.18 - ETA: 4:46 - loss: 3.0755 - accuracy: 0.18 - ETA: 4:45 - loss: 3.0682 - accuracy: 0.18 - ETA: 4:43 - loss: 3.0737 - accuracy: 0.18 - ETA: 4:42 - loss: 3.0728 - accuracy: 0.18 - ETA: 4:41 - loss: 3.0695 - accuracy: 0.18 - ETA: 4:40 - loss: 3.0724 - accuracy: 0.18 - ETA: 4:39 - loss: 3.0716 - accuracy: 0.18 - ETA: 4:38 - loss: 3.0781 - accuracy: 0.18 - ETA: 4:37 - loss: 3.0781 - accuracy: 0.18 - ETA: 4:36 - loss: 3.0743 - accuracy: 0.18 - ETA: 4:36 - loss: 3.0759 - accuracy: 0.18 - ETA: 4:35 - loss: 3.0698 - accuracy: 0.18 - ETA: 4:34 - loss: 3.0699 - accuracy: 0.18 - ETA: 4:33 - loss: 3.0664 - accuracy: 0.18 - ETA: 4:33 - loss: 3.0666 - accuracy: 0.18 - ETA: 4:31 - loss: 3.0657 - accuracy: 0.18 - ETA: 4:31 - loss: 3.0683 - accuracy: 0.18 - ETA: 4:29 - loss: 3.0715 - accuracy: 0.18 - ETA: 4:28 - loss: 3.0677 - accuracy: 0.18 - ETA: 4:28 - loss: 3.0711 - accuracy: 0.18 - ETA: 4:27 - loss: 3.0733 - accuracy: 0.18 - ETA: 4:26 - loss: 3.0715 - accuracy: 0.18 - ETA: 4:25 - loss: 3.0765 - accuracy: 0.18 - ETA: 4:24 - loss: 3.0750 - accuracy: 0.18 - ETA: 4:23 - loss: 3.0719 - accuracy: 0.18 - ETA: 4:22 - loss: 3.0697 - accuracy: 0.18 - ETA: 4:22 - loss: 3.0696 - accuracy: 0.18 - ETA: 4:21 - loss: 3.0767 - accuracy: 0.18 - ETA: 4:21 - loss: 3.0773 - accuracy: 0.18 - ETA: 4:20 - loss: 3.0801 - accuracy: 0.18 - ETA: 4:19 - loss: 3.0834 - accuracy: 0.18 - ETA: 4:18 - loss: 3.0797 - accuracy: 0.18 - ETA: 4:17 - loss: 3.0818 - accuracy: 0.18 - ETA: 4:16 - loss: 3.0808 - accuracy: 0.18 - ETA: 4:15 - loss: 3.0812 - accuracy: 0.18 - ETA: 4:14 - loss: 3.0781 - accuracy: 0.18 - ETA: 4:13 - loss: 3.0753 - accuracy: 0.18 - ETA: 4:12 - loss: 3.0779 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0796 - accuracy: 0.18 - ETA: 4:10 - loss: 3.0801 - accuracy: 0.18 - ETA: 4:09 - loss: 3.0813 - accuracy: 0.18 - ETA: 4:08 - loss: 3.0794 - accuracy: 0.18 - ETA: 4:07 - loss: 3.0772 - accuracy: 0.18 - ETA: 4:06 - loss: 3.0756 - accuracy: 0.18 - ETA: 4:05 - loss: 3.0745 - accuracy: 0.18 - ETA: 4:05 - loss: 3.0736 - accuracy: 0.18 - ETA: 4:04 - loss: 3.0713 - accuracy: 0.18 - ETA: 4:03 - loss: 3.0708 - accuracy: 0.18 - ETA: 4:02 - loss: 3.0688 - accuracy: 0.18 - ETA: 4:01 - loss: 3.0641 - accuracy: 0.18 - ETA: 4:00 - loss: 3.0643 - accuracy: 0.18 - ETA: 3:59 - loss: 3.0622 - accuracy: 0.18 - ETA: 3:58 - loss: 3.0627 - accuracy: 0.18 - ETA: 3:57 - loss: 3.0629 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0644 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0643 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0629 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0650 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0630 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0641 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0621 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0627 - accuracy: 0.18 - ETA: 3:49 - loss: 3.0656 - accuracy: 0.18 - ETA: 3:48 - loss: 3.0659 - accuracy: 0.18 - ETA: 3:47 - loss: 3.0650 - accuracy: 0.18 - ETA: 3:46 - loss: 3.0663 - accuracy: 0.18 - ETA: 3:45 - loss: 3.0657 - accuracy: 0.18 - ETA: 3:44 - loss: 3.0663 - accuracy: 0.18 - ETA: 3:43 - loss: 3.0640 - accuracy: 0.18 - ETA: 3:42 - loss: 3.0628 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0615 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0599 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0590 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0578 - accuracy: 0.18 - ETA: 3:37 - loss: 3.0568 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0568 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0547 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0549 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0537 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0521 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0509 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0516 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0504 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0509 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0500 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0480 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0495 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0486 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0488 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0498 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0510 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0507 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0508 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0504 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0495 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0504 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0515 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0529 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0547 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0550 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0577 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0595 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0586 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0574 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0560 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0553 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0548 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0553 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0551 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0550 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0547 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0564 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0565 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0558 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0569 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0570 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0570 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0564 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0580 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0586 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0573 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0575 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0581 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0586 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0567 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0566 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0560 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0556 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0550 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0541 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0537 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0544 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0540 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0526 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0533 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0524 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0523 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0525 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0524 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0523 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0517 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0516 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0515 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0517 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0523 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0523 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0527 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0526 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0533 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0522 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0521 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0522 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0531 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0521 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0519 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0513 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0518 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0512 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0514 - accuracy: 0.19 - ETA: 2:17 - loss: 3.0511 - accuracy: 0.1914"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0508 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0519 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0516 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0502 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0508 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0523 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0516 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0519 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0537 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0544 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0560 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0566 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0570 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0567 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0565 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0578 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0583 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0581 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0576 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0579 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0588 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0588 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0586 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0584 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0579 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0581 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0585 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0583 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0588 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0594 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0606 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0612 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0603 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0598 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0606 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0614 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0606 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0612 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0614 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0625 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0630 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0632 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0636 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0634 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0647 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0654 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0666 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0663 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0667 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0663 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0669 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0672 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0666 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0675 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0677 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0682 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0686 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0682 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0678 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0682 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0682 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0691 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0691 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0698 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0698 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0700 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0702 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0691 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0688 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0695 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0697 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0704 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0711 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0704 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0712 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0717 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0720 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0715 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0711 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0720 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0717 - accuracy: 0.18 - ETA: 59s - loss: 3.0719 - accuracy: 0.1873 - ETA: 58s - loss: 3.0719 - accuracy: 0.187 - ETA: 57s - loss: 3.0720 - accuracy: 0.187 - ETA: 56s - loss: 3.0719 - accuracy: 0.187 - ETA: 55s - loss: 3.0717 - accuracy: 0.187 - ETA: 55s - loss: 3.0712 - accuracy: 0.187 - ETA: 54s - loss: 3.0708 - accuracy: 0.187 - ETA: 53s - loss: 3.0700 - accuracy: 0.187 - ETA: 52s - loss: 3.0706 - accuracy: 0.187 - ETA: 51s - loss: 3.0708 - accuracy: 0.187 - ETA: 50s - loss: 3.0707 - accuracy: 0.187 - ETA: 49s - loss: 3.0703 - accuracy: 0.187 - ETA: 48s - loss: 3.0700 - accuracy: 0.187 - ETA: 47s - loss: 3.0696 - accuracy: 0.188 - ETA: 46s - loss: 3.0697 - accuracy: 0.188 - ETA: 45s - loss: 3.0702 - accuracy: 0.188 - ETA: 44s - loss: 3.0704 - accuracy: 0.188 - ETA: 43s - loss: 3.0713 - accuracy: 0.188 - ETA: 42s - loss: 3.0707 - accuracy: 0.188 - ETA: 41s - loss: 3.0705 - accuracy: 0.188 - ETA: 40s - loss: 3.0709 - accuracy: 0.188 - ETA: 39s - loss: 3.0706 - accuracy: 0.188 - ETA: 38s - loss: 3.0705 - accuracy: 0.188 - ETA: 37s - loss: 3.0697 - accuracy: 0.188 - ETA: 37s - loss: 3.0703 - accuracy: 0.188 - ETA: 36s - loss: 3.0705 - accuracy: 0.188 - ETA: 35s - loss: 3.0707 - accuracy: 0.188 - ETA: 34s - loss: 3.0709 - accuracy: 0.188 - ETA: 33s - loss: 3.0711 - accuracy: 0.187 - ETA: 32s - loss: 3.0719 - accuracy: 0.187 - ETA: 31s - loss: 3.0717 - accuracy: 0.187 - ETA: 30s - loss: 3.0724 - accuracy: 0.187 - ETA: 29s - loss: 3.0719 - accuracy: 0.187 - ETA: 28s - loss: 3.0716 - accuracy: 0.187 - ETA: 27s - loss: 3.0710 - accuracy: 0.187 - ETA: 26s - loss: 3.0719 - accuracy: 0.187 - ETA: 25s - loss: 3.0728 - accuracy: 0.187 - ETA: 24s - loss: 3.0734 - accuracy: 0.187 - ETA: 23s - loss: 3.0730 - accuracy: 0.187 - ETA: 22s - loss: 3.0734 - accuracy: 0.187 - ETA: 21s - loss: 3.0737 - accuracy: 0.187 - ETA: 20s - loss: 3.0737 - accuracy: 0.187 - ETA: 19s - loss: 3.0736 - accuracy: 0.187 - ETA: 19s - loss: 3.0732 - accuracy: 0.187 - ETA: 18s - loss: 3.0733 - accuracy: 0.187 - ETA: 17s - loss: 3.0740 - accuracy: 0.187 - ETA: 16s - loss: 3.0745 - accuracy: 0.186 - ETA: 15s - loss: 3.0743 - accuracy: 0.186 - ETA: 14s - loss: 3.0744 - accuracy: 0.187 - ETA: 13s - loss: 3.0743 - accuracy: 0.187 - ETA: 12s - loss: 3.0748 - accuracy: 0.187 - ETA: 11s - loss: 3.0753 - accuracy: 0.187 - ETA: 10s - loss: 3.0757 - accuracy: 0.187 - ETA: 9s - loss: 3.0766 - accuracy: 0.187 - ETA: 8s - loss: 3.0770 - accuracy: 0.18 - ETA: 7s - loss: 3.0786 - accuracy: 0.18 - ETA: 6s - loss: 3.0800 - accuracy: 0.18 - ETA: 5s - loss: 3.0820 - accuracy: 0.18 - ETA: 4s - loss: 3.0827 - accuracy: 0.18 - ETA: 3s - loss: 3.0839 - accuracy: 0.18 - ETA: 2s - loss: 3.0834 - accuracy: 0.18 - ETA: 1s - loss: 3.0838 - accuracy: 0.18 - ETA: 1s - loss: 3.0836 - accuracy: 0.18 - ETA: 0s - loss: 3.0831 - accuracy: 0.18 - 336s 8ms/step - loss: 3.0830 - accuracy: 0.1874 - val_loss: 4.1283 - val_accuracy: 0.0376\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:18 - loss: 3.2793 - accuracy: 0.16 - ETA: 5:14 - loss: 3.1353 - accuracy: 0.19 - ETA: 5:13 - loss: 3.1757 - accuracy: 0.20 - ETA: 5:14 - loss: 3.1338 - accuracy: 0.20 - ETA: 5:11 - loss: 3.1327 - accuracy: 0.20 - ETA: 5:07 - loss: 3.1631 - accuracy: 0.19 - ETA: 5:06 - loss: 3.1488 - accuracy: 0.19 - ETA: 5:04 - loss: 3.1408 - accuracy: 0.19 - ETA: 5:02 - loss: 3.1465 - accuracy: 0.19 - ETA: 5:01 - loss: 3.1362 - accuracy: 0.19 - ETA: 4:59 - loss: 3.1557 - accuracy: 0.18 - ETA: 4:58 - loss: 3.1604 - accuracy: 0.18 - ETA: 4:58 - loss: 3.1514 - accuracy: 0.18 - ETA: 4:57 - loss: 3.1443 - accuracy: 0.18 - ETA: 4:55 - loss: 3.1402 - accuracy: 0.18 - ETA: 4:54 - loss: 3.1414 - accuracy: 0.18 - ETA: 4:53 - loss: 3.1303 - accuracy: 0.18 - ETA: 4:53 - loss: 3.1180 - accuracy: 0.18 - ETA: 4:52 - loss: 3.1140 - accuracy: 0.19 - ETA: 4:51 - loss: 3.1041 - accuracy: 0.19 - ETA: 4:50 - loss: 3.1079 - accuracy: 0.19 - ETA: 4:49 - loss: 3.1079 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0995 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0972 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0993 - accuracy: 0.19 - ETA: 4:45 - loss: 3.1028 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0988 - accuracy: 0.19 - ETA: 4:43 - loss: 3.1018 - accuracy: 0.19 - ETA: 4:42 - loss: 3.1109 - accuracy: 0.19 - ETA: 4:41 - loss: 3.1135 - accuracy: 0.19 - ETA: 4:40 - loss: 3.1111 - accuracy: 0.19 - ETA: 4:39 - loss: 3.1068 - accuracy: 0.18 - ETA: 4:38 - loss: 3.1110 - accuracy: 0.18 - ETA: 4:37 - loss: 3.1107 - accuracy: 0.18 - ETA: 4:36 - loss: 3.1128 - accuracy: 0.18 - ETA: 4:35 - loss: 3.1111 - accuracy: 0.19 - ETA: 4:34 - loss: 3.1115 - accuracy: 0.18 - ETA: 4:33 - loss: 3.1153 - accuracy: 0.18 - ETA: 4:32 - loss: 3.1183 - accuracy: 0.18 - ETA: 4:31 - loss: 3.1184 - accuracy: 0.18 - ETA: 4:30 - loss: 3.1141 - accuracy: 0.18 - ETA: 4:30 - loss: 3.1082 - accuracy: 0.18 - ETA: 4:29 - loss: 3.1068 - accuracy: 0.18 - ETA: 4:28 - loss: 3.1096 - accuracy: 0.18 - ETA: 4:28 - loss: 3.0997 - accuracy: 0.19 - ETA: 4:27 - loss: 3.1021 - accuracy: 0.19 - ETA: 4:27 - loss: 3.1078 - accuracy: 0.18 - ETA: 4:26 - loss: 3.1104 - accuracy: 0.18 - ETA: 4:25 - loss: 3.1091 - accuracy: 0.18 - ETA: 4:24 - loss: 3.1053 - accuracy: 0.18 - ETA: 4:23 - loss: 3.1068 - accuracy: 0.18 - ETA: 4:22 - loss: 3.1034 - accuracy: 0.18 - ETA: 4:22 - loss: 3.1049 - accuracy: 0.18 - ETA: 4:21 - loss: 3.1011 - accuracy: 0.18 - ETA: 4:20 - loss: 3.1001 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0997 - accuracy: 0.18 - ETA: 4:17 - loss: 3.0984 - accuracy: 0.18 - ETA: 4:16 - loss: 3.1017 - accuracy: 0.18 - ETA: 4:16 - loss: 3.1015 - accuracy: 0.18 - ETA: 4:15 - loss: 3.0993 - accuracy: 0.18 - ETA: 4:14 - loss: 3.0982 - accuracy: 0.18 - ETA: 4:13 - loss: 3.1011 - accuracy: 0.18 - ETA: 4:13 - loss: 3.0989 - accuracy: 0.18 - ETA: 4:12 - loss: 3.1002 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0995 - accuracy: 0.18 - ETA: 4:10 - loss: 3.1020 - accuracy: 0.18 - ETA: 4:09 - loss: 3.1024 - accuracy: 0.18 - ETA: 4:08 - loss: 3.1055 - accuracy: 0.18 - ETA: 4:07 - loss: 3.1057 - accuracy: 0.18 - ETA: 4:06 - loss: 3.1061 - accuracy: 0.18 - ETA: 4:06 - loss: 3.1064 - accuracy: 0.18 - ETA: 4:05 - loss: 3.1068 - accuracy: 0.18 - ETA: 4:04 - loss: 3.1063 - accuracy: 0.18 - ETA: 4:03 - loss: 3.1061 - accuracy: 0.18 - ETA: 4:02 - loss: 3.1033 - accuracy: 0.18 - ETA: 4:01 - loss: 3.1010 - accuracy: 0.18 - ETA: 4:00 - loss: 3.1027 - accuracy: 0.18 - ETA: 3:59 - loss: 3.0996 - accuracy: 0.18 - ETA: 3:58 - loss: 3.1003 - accuracy: 0.18 - ETA: 3:57 - loss: 3.1019 - accuracy: 0.18 - ETA: 3:56 - loss: 3.1012 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0995 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0990 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0975 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0990 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0978 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0963 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0967 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0952 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0958 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0958 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0968 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0966 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0949 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0922 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0923 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0918 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0918 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0928 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0919 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0935 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0951 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0964 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0942 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0940 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0944 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0957 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0961 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0963 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0956 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0945 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0938 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0957 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0968 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0979 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0984 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0987 - accuracy: 0.19 - ETA: 3:22 - loss: 3.1000 - accuracy: 0.19 - ETA: 3:21 - loss: 3.1007 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0999 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0997 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0992 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0987 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0989 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0977 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0971 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0978 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0991 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0980 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0978 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0985 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0991 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0997 - accuracy: 0.18 - ETA: 3:06 - loss: 3.1010 - accuracy: 0.18 - ETA: 3:05 - loss: 3.1011 - accuracy: 0.18 - ETA: 3:05 - loss: 3.1014 - accuracy: 0.18 - ETA: 3:04 - loss: 3.1030 - accuracy: 0.18 - ETA: 3:03 - loss: 3.1032 - accuracy: 0.18 - ETA: 3:02 - loss: 3.1043 - accuracy: 0.18 - ETA: 3:01 - loss: 3.1044 - accuracy: 0.18 - ETA: 3:00 - loss: 3.1046 - accuracy: 0.18 - ETA: 2:59 - loss: 3.1032 - accuracy: 0.18 - ETA: 2:58 - loss: 3.1020 - accuracy: 0.18 - ETA: 2:57 - loss: 3.1037 - accuracy: 0.19 - ETA: 2:56 - loss: 3.1029 - accuracy: 0.19 - ETA: 2:55 - loss: 3.1034 - accuracy: 0.19 - ETA: 2:54 - loss: 3.1049 - accuracy: 0.18 - ETA: 2:53 - loss: 3.1045 - accuracy: 0.18 - ETA: 2:52 - loss: 3.1039 - accuracy: 0.18 - ETA: 2:51 - loss: 3.1039 - accuracy: 0.18 - ETA: 2:50 - loss: 3.1035 - accuracy: 0.18 - ETA: 2:49 - loss: 3.1027 - accuracy: 0.19 - ETA: 2:48 - loss: 3.1034 - accuracy: 0.18 - ETA: 2:47 - loss: 3.1044 - accuracy: 0.18 - ETA: 2:46 - loss: 3.1047 - accuracy: 0.18 - ETA: 2:46 - loss: 3.1045 - accuracy: 0.18 - ETA: 2:45 - loss: 3.1048 - accuracy: 0.18 - ETA: 2:44 - loss: 3.1041 - accuracy: 0.18 - ETA: 2:43 - loss: 3.1057 - accuracy: 0.18 - ETA: 2:42 - loss: 3.1063 - accuracy: 0.18 - ETA: 2:41 - loss: 3.1067 - accuracy: 0.18 - ETA: 2:40 - loss: 3.1074 - accuracy: 0.18 - ETA: 2:39 - loss: 3.1083 - accuracy: 0.18 - ETA: 2:38 - loss: 3.1080 - accuracy: 0.18 - ETA: 2:37 - loss: 3.1074 - accuracy: 0.18 - ETA: 2:36 - loss: 3.1075 - accuracy: 0.18 - ETA: 2:35 - loss: 3.1079 - accuracy: 0.18 - ETA: 2:34 - loss: 3.1089 - accuracy: 0.18 - ETA: 2:33 - loss: 3.1091 - accuracy: 0.18 - ETA: 2:32 - loss: 3.1107 - accuracy: 0.18 - ETA: 2:31 - loss: 3.1106 - accuracy: 0.18 - ETA: 2:30 - loss: 3.1106 - accuracy: 0.18 - ETA: 2:29 - loss: 3.1099 - accuracy: 0.18 - ETA: 2:28 - loss: 3.1091 - accuracy: 0.18 - ETA: 2:27 - loss: 3.1096 - accuracy: 0.18 - ETA: 2:26 - loss: 3.1099 - accuracy: 0.18 - ETA: 2:26 - loss: 3.1109 - accuracy: 0.18 - ETA: 2:25 - loss: 3.1112 - accuracy: 0.18 - ETA: 2:24 - loss: 3.1106 - accuracy: 0.18 - ETA: 2:23 - loss: 3.1113 - accuracy: 0.18 - ETA: 2:22 - loss: 3.1105 - accuracy: 0.18 - ETA: 2:21 - loss: 3.1103 - accuracy: 0.18 - ETA: 2:20 - loss: 3.1105 - accuracy: 0.18 - ETA: 2:19 - loss: 3.1101 - accuracy: 0.18 - ETA: 2:18 - loss: 3.1103 - accuracy: 0.18 - ETA: 2:17 - loss: 3.1108 - accuracy: 0.1875"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.1117 - accuracy: 0.18 - ETA: 2:15 - loss: 3.1103 - accuracy: 0.18 - ETA: 2:15 - loss: 3.1095 - accuracy: 0.18 - ETA: 2:14 - loss: 3.1095 - accuracy: 0.18 - ETA: 2:13 - loss: 3.1094 - accuracy: 0.18 - ETA: 2:12 - loss: 3.1085 - accuracy: 0.18 - ETA: 2:11 - loss: 3.1092 - accuracy: 0.18 - ETA: 2:10 - loss: 3.1087 - accuracy: 0.18 - ETA: 2:09 - loss: 3.1088 - accuracy: 0.18 - ETA: 2:08 - loss: 3.1076 - accuracy: 0.18 - ETA: 2:07 - loss: 3.1076 - accuracy: 0.18 - ETA: 2:06 - loss: 3.1071 - accuracy: 0.18 - ETA: 2:05 - loss: 3.1069 - accuracy: 0.18 - ETA: 2:04 - loss: 3.1077 - accuracy: 0.18 - ETA: 2:03 - loss: 3.1078 - accuracy: 0.18 - ETA: 2:02 - loss: 3.1068 - accuracy: 0.18 - ETA: 2:01 - loss: 3.1063 - accuracy: 0.18 - ETA: 2:00 - loss: 3.1063 - accuracy: 0.18 - ETA: 1:59 - loss: 3.1062 - accuracy: 0.18 - ETA: 1:58 - loss: 3.1068 - accuracy: 0.18 - ETA: 1:57 - loss: 3.1069 - accuracy: 0.18 - ETA: 1:56 - loss: 3.1081 - accuracy: 0.18 - ETA: 1:56 - loss: 3.1085 - accuracy: 0.18 - ETA: 1:55 - loss: 3.1080 - accuracy: 0.18 - ETA: 1:54 - loss: 3.1079 - accuracy: 0.18 - ETA: 1:53 - loss: 3.1076 - accuracy: 0.18 - ETA: 1:52 - loss: 3.1077 - accuracy: 0.18 - ETA: 1:51 - loss: 3.1090 - accuracy: 0.18 - ETA: 1:50 - loss: 3.1091 - accuracy: 0.18 - ETA: 1:49 - loss: 3.1093 - accuracy: 0.18 - ETA: 1:48 - loss: 3.1100 - accuracy: 0.18 - ETA: 1:47 - loss: 3.1106 - accuracy: 0.18 - ETA: 1:46 - loss: 3.1096 - accuracy: 0.18 - ETA: 1:45 - loss: 3.1104 - accuracy: 0.18 - ETA: 1:44 - loss: 3.1113 - accuracy: 0.18 - ETA: 1:43 - loss: 3.1111 - accuracy: 0.18 - ETA: 1:42 - loss: 3.1101 - accuracy: 0.18 - ETA: 1:41 - loss: 3.1102 - accuracy: 0.18 - ETA: 1:40 - loss: 3.1107 - accuracy: 0.18 - ETA: 1:39 - loss: 3.1107 - accuracy: 0.18 - ETA: 1:38 - loss: 3.1101 - accuracy: 0.18 - ETA: 1:38 - loss: 3.1103 - accuracy: 0.18 - ETA: 1:37 - loss: 3.1097 - accuracy: 0.18 - ETA: 1:36 - loss: 3.1094 - accuracy: 0.18 - ETA: 1:35 - loss: 3.1098 - accuracy: 0.18 - ETA: 1:34 - loss: 3.1101 - accuracy: 0.18 - ETA: 1:33 - loss: 3.1099 - accuracy: 0.18 - ETA: 1:32 - loss: 3.1103 - accuracy: 0.18 - ETA: 1:31 - loss: 3.1100 - accuracy: 0.18 - ETA: 1:30 - loss: 3.1102 - accuracy: 0.18 - ETA: 1:29 - loss: 3.1108 - accuracy: 0.18 - ETA: 1:28 - loss: 3.1106 - accuracy: 0.18 - ETA: 1:27 - loss: 3.1106 - accuracy: 0.18 - ETA: 1:26 - loss: 3.1103 - accuracy: 0.18 - ETA: 1:25 - loss: 3.1107 - accuracy: 0.18 - ETA: 1:24 - loss: 3.1111 - accuracy: 0.18 - ETA: 1:23 - loss: 3.1113 - accuracy: 0.18 - ETA: 1:22 - loss: 3.1107 - accuracy: 0.18 - ETA: 1:21 - loss: 3.1113 - accuracy: 0.18 - ETA: 1:20 - loss: 3.1110 - accuracy: 0.18 - ETA: 1:19 - loss: 3.1117 - accuracy: 0.18 - ETA: 1:18 - loss: 3.1116 - accuracy: 0.18 - ETA: 1:18 - loss: 3.1115 - accuracy: 0.18 - ETA: 1:17 - loss: 3.1126 - accuracy: 0.18 - ETA: 1:16 - loss: 3.1127 - accuracy: 0.18 - ETA: 1:15 - loss: 3.1131 - accuracy: 0.18 - ETA: 1:14 - loss: 3.1127 - accuracy: 0.18 - ETA: 1:13 - loss: 3.1133 - accuracy: 0.18 - ETA: 1:12 - loss: 3.1135 - accuracy: 0.18 - ETA: 1:11 - loss: 3.1138 - accuracy: 0.18 - ETA: 1:10 - loss: 3.1134 - accuracy: 0.18 - ETA: 1:09 - loss: 3.1133 - accuracy: 0.18 - ETA: 1:08 - loss: 3.1134 - accuracy: 0.18 - ETA: 1:07 - loss: 3.1133 - accuracy: 0.18 - ETA: 1:06 - loss: 3.1134 - accuracy: 0.18 - ETA: 1:05 - loss: 3.1126 - accuracy: 0.18 - ETA: 1:04 - loss: 3.1122 - accuracy: 0.18 - ETA: 1:03 - loss: 3.1132 - accuracy: 0.18 - ETA: 1:02 - loss: 3.1123 - accuracy: 0.18 - ETA: 1:01 - loss: 3.1124 - accuracy: 0.18 - ETA: 1:00 - loss: 3.1127 - accuracy: 0.18 - ETA: 59s - loss: 3.1128 - accuracy: 0.1853 - ETA: 59s - loss: 3.1133 - accuracy: 0.185 - ETA: 58s - loss: 3.1138 - accuracy: 0.185 - ETA: 57s - loss: 3.1145 - accuracy: 0.184 - ETA: 56s - loss: 3.1147 - accuracy: 0.184 - ETA: 55s - loss: 3.1145 - accuracy: 0.184 - ETA: 54s - loss: 3.1152 - accuracy: 0.184 - ETA: 53s - loss: 3.1151 - accuracy: 0.184 - ETA: 52s - loss: 3.1153 - accuracy: 0.184 - ETA: 51s - loss: 3.1151 - accuracy: 0.184 - ETA: 50s - loss: 3.1154 - accuracy: 0.184 - ETA: 49s - loss: 3.1156 - accuracy: 0.184 - ETA: 48s - loss: 3.1163 - accuracy: 0.184 - ETA: 47s - loss: 3.1163 - accuracy: 0.184 - ETA: 46s - loss: 3.1164 - accuracy: 0.184 - ETA: 45s - loss: 3.1190 - accuracy: 0.184 - ETA: 44s - loss: 3.1189 - accuracy: 0.184 - ETA: 43s - loss: 3.1192 - accuracy: 0.184 - ETA: 42s - loss: 3.1189 - accuracy: 0.184 - ETA: 41s - loss: 3.1191 - accuracy: 0.184 - ETA: 40s - loss: 3.1197 - accuracy: 0.184 - ETA: 39s - loss: 3.1204 - accuracy: 0.184 - ETA: 39s - loss: 3.1204 - accuracy: 0.184 - ETA: 38s - loss: 3.1264 - accuracy: 0.184 - ETA: 37s - loss: 3.1263 - accuracy: 0.184 - ETA: 36s - loss: 3.1259 - accuracy: 0.184 - ETA: 35s - loss: 3.1258 - accuracy: 0.184 - ETA: 34s - loss: 3.1262 - accuracy: 0.184 - ETA: 33s - loss: 3.1263 - accuracy: 0.184 - ETA: 32s - loss: 3.1260 - accuracy: 0.184 - ETA: 31s - loss: 3.1261 - accuracy: 0.184 - ETA: 30s - loss: 3.1257 - accuracy: 0.184 - ETA: 29s - loss: 3.1259 - accuracy: 0.184 - ETA: 28s - loss: 3.1257 - accuracy: 0.184 - ETA: 27s - loss: 3.1256 - accuracy: 0.184 - ETA: 26s - loss: 3.1255 - accuracy: 0.184 - ETA: 25s - loss: 3.1261 - accuracy: 0.184 - ETA: 24s - loss: 3.1265 - accuracy: 0.184 - ETA: 23s - loss: 3.1262 - accuracy: 0.184 - ETA: 22s - loss: 3.1260 - accuracy: 0.184 - ETA: 21s - loss: 3.1260 - accuracy: 0.184 - ETA: 20s - loss: 3.1261 - accuracy: 0.184 - ETA: 20s - loss: 3.1270 - accuracy: 0.184 - ETA: 19s - loss: 3.1270 - accuracy: 0.183 - ETA: 18s - loss: 3.1269 - accuracy: 0.183 - ETA: 17s - loss: 3.1271 - accuracy: 0.183 - ETA: 16s - loss: 3.1272 - accuracy: 0.183 - ETA: 15s - loss: 3.1271 - accuracy: 0.183 - ETA: 14s - loss: 3.1276 - accuracy: 0.183 - ETA: 13s - loss: 3.1268 - accuracy: 0.183 - ETA: 12s - loss: 3.1273 - accuracy: 0.183 - ETA: 11s - loss: 3.1274 - accuracy: 0.183 - ETA: 10s - loss: 3.1278 - accuracy: 0.183 - ETA: 9s - loss: 3.1274 - accuracy: 0.183 - ETA: 8s - loss: 3.1265 - accuracy: 0.18 - ETA: 7s - loss: 3.1266 - accuracy: 0.18 - ETA: 6s - loss: 3.1267 - accuracy: 0.18 - ETA: 5s - loss: 3.1270 - accuracy: 0.18 - ETA: 4s - loss: 3.1277 - accuracy: 0.18 - ETA: 3s - loss: 3.1275 - accuracy: 0.18 - ETA: 2s - loss: 3.1279 - accuracy: 0.18 - ETA: 1s - loss: 3.1276 - accuracy: 0.18 - ETA: 1s - loss: 3.1283 - accuracy: 0.18 - ETA: 0s - loss: 3.1280 - accuracy: 0.18 - 338s 8ms/step - loss: 3.1280 - accuracy: 0.1842 - val_loss: 4.2023 - val_accuracy: 0.0377\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:40 - loss: 3.0612 - accuracy: 0.20 - ETA: 5:31 - loss: 3.2592 - accuracy: 0.16 - ETA: 5:28 - loss: 3.2724 - accuracy: 0.15 - ETA: 5:27 - loss: 3.2426 - accuracy: 0.15 - ETA: 5:24 - loss: 3.2174 - accuracy: 0.16 - ETA: 5:21 - loss: 3.1995 - accuracy: 0.17 - ETA: 5:23 - loss: 3.1720 - accuracy: 0.17 - ETA: 5:21 - loss: 3.1638 - accuracy: 0.17 - ETA: 5:18 - loss: 3.1650 - accuracy: 0.17 - ETA: 5:15 - loss: 3.1488 - accuracy: 0.17 - ETA: 5:14 - loss: 3.1453 - accuracy: 0.18 - ETA: 5:13 - loss: 3.1509 - accuracy: 0.18 - ETA: 5:12 - loss: 3.1644 - accuracy: 0.18 - ETA: 5:10 - loss: 3.1710 - accuracy: 0.18 - ETA: 5:08 - loss: 3.1680 - accuracy: 0.18 - ETA: 5:06 - loss: 3.1874 - accuracy: 0.17 - ETA: 5:05 - loss: 3.1825 - accuracy: 0.17 - ETA: 5:03 - loss: 3.1901 - accuracy: 0.17 - ETA: 5:01 - loss: 3.1948 - accuracy: 0.17 - ETA: 4:59 - loss: 3.1956 - accuracy: 0.17 - ETA: 4:58 - loss: 3.1980 - accuracy: 0.17 - ETA: 4:56 - loss: 3.1972 - accuracy: 0.17 - ETA: 4:55 - loss: 3.1910 - accuracy: 0.17 - ETA: 4:53 - loss: 3.1967 - accuracy: 0.17 - ETA: 4:53 - loss: 3.1950 - accuracy: 0.17 - ETA: 4:51 - loss: 3.1959 - accuracy: 0.17 - ETA: 4:50 - loss: 3.1845 - accuracy: 0.17 - ETA: 4:49 - loss: 3.1791 - accuracy: 0.17 - ETA: 4:48 - loss: 3.1765 - accuracy: 0.17 - ETA: 4:47 - loss: 3.1753 - accuracy: 0.17 - ETA: 4:45 - loss: 3.1740 - accuracy: 0.17 - ETA: 4:44 - loss: 3.1716 - accuracy: 0.17 - ETA: 4:44 - loss: 3.1704 - accuracy: 0.17 - ETA: 4:43 - loss: 3.1720 - accuracy: 0.17 - ETA: 4:42 - loss: 3.1716 - accuracy: 0.17 - ETA: 4:41 - loss: 3.1716 - accuracy: 0.17 - ETA: 4:41 - loss: 3.1723 - accuracy: 0.17 - ETA: 4:40 - loss: 3.1732 - accuracy: 0.17 - ETA: 4:40 - loss: 3.1810 - accuracy: 0.17 - ETA: 4:39 - loss: 3.1783 - accuracy: 0.17 - ETA: 4:38 - loss: 3.1787 - accuracy: 0.17 - ETA: 4:37 - loss: 3.1780 - accuracy: 0.17 - ETA: 4:36 - loss: 3.1729 - accuracy: 0.17 - ETA: 4:35 - loss: 3.1751 - accuracy: 0.17 - ETA: 4:34 - loss: 3.1750 - accuracy: 0.17 - ETA: 4:32 - loss: 3.1733 - accuracy: 0.17 - ETA: 4:32 - loss: 3.1711 - accuracy: 0.17 - ETA: 4:31 - loss: 3.1738 - accuracy: 0.17 - ETA: 4:30 - loss: 3.1752 - accuracy: 0.17 - ETA: 4:29 - loss: 3.1794 - accuracy: 0.17 - ETA: 4:28 - loss: 3.1741 - accuracy: 0.17 - ETA: 4:26 - loss: 3.1850 - accuracy: 0.17 - ETA: 4:26 - loss: 3.1844 - accuracy: 0.17 - ETA: 4:25 - loss: 3.1862 - accuracy: 0.17 - ETA: 4:24 - loss: 3.1857 - accuracy: 0.17 - ETA: 4:22 - loss: 3.1833 - accuracy: 0.17 - ETA: 4:21 - loss: 3.1811 - accuracy: 0.17 - ETA: 4:21 - loss: 3.1776 - accuracy: 0.17 - ETA: 4:19 - loss: 3.1772 - accuracy: 0.17 - ETA: 4:18 - loss: 3.1755 - accuracy: 0.17 - ETA: 4:17 - loss: 3.1793 - accuracy: 0.17 - ETA: 4:16 - loss: 3.1752 - accuracy: 0.17 - ETA: 4:16 - loss: 3.1755 - accuracy: 0.17 - ETA: 4:15 - loss: 3.1764 - accuracy: 0.17 - ETA: 4:14 - loss: 3.1755 - accuracy: 0.17 - ETA: 4:13 - loss: 3.1760 - accuracy: 0.17 - ETA: 4:12 - loss: 3.1763 - accuracy: 0.17 - ETA: 4:11 - loss: 3.1760 - accuracy: 0.17 - ETA: 4:10 - loss: 3.1754 - accuracy: 0.17 - ETA: 4:09 - loss: 3.1752 - accuracy: 0.17 - ETA: 4:08 - loss: 3.1692 - accuracy: 0.17 - ETA: 4:07 - loss: 3.1685 - accuracy: 0.17 - ETA: 4:06 - loss: 3.1669 - accuracy: 0.17 - ETA: 4:05 - loss: 3.1645 - accuracy: 0.17 - ETA: 4:04 - loss: 3.1636 - accuracy: 0.17 - ETA: 4:03 - loss: 3.1654 - accuracy: 0.17 - ETA: 4:02 - loss: 3.1638 - accuracy: 0.17 - ETA: 4:01 - loss: 3.1610 - accuracy: 0.17 - ETA: 4:00 - loss: 3.1594 - accuracy: 0.17 - ETA: 3:59 - loss: 3.1590 - accuracy: 0.17 - ETA: 3:58 - loss: 3.1555 - accuracy: 0.17 - ETA: 3:57 - loss: 3.1547 - accuracy: 0.17 - ETA: 3:56 - loss: 3.1536 - accuracy: 0.17 - ETA: 3:55 - loss: 3.1540 - accuracy: 0.17 - ETA: 3:55 - loss: 3.1548 - accuracy: 0.17 - ETA: 3:54 - loss: 3.1568 - accuracy: 0.17 - ETA: 3:53 - loss: 3.1554 - accuracy: 0.17 - ETA: 3:52 - loss: 3.1533 - accuracy: 0.17 - ETA: 3:51 - loss: 3.1531 - accuracy: 0.17 - ETA: 3:50 - loss: 3.1523 - accuracy: 0.17 - ETA: 3:49 - loss: 3.1526 - accuracy: 0.17 - ETA: 3:48 - loss: 3.1579 - accuracy: 0.17 - ETA: 3:47 - loss: 3.1571 - accuracy: 0.17 - ETA: 3:46 - loss: 3.1578 - accuracy: 0.17 - ETA: 3:45 - loss: 3.1578 - accuracy: 0.17 - ETA: 3:44 - loss: 3.1642 - accuracy: 0.17 - ETA: 3:43 - loss: 3.1646 - accuracy: 0.17 - ETA: 3:42 - loss: 3.1650 - accuracy: 0.17 - ETA: 3:41 - loss: 3.1696 - accuracy: 0.17 - ETA: 3:40 - loss: 3.1706 - accuracy: 0.17 - ETA: 3:39 - loss: 3.1706 - accuracy: 0.17 - ETA: 3:38 - loss: 3.1730 - accuracy: 0.17 - ETA: 3:37 - loss: 3.1729 - accuracy: 0.17 - ETA: 3:36 - loss: 3.1777 - accuracy: 0.17 - ETA: 3:36 - loss: 3.1789 - accuracy: 0.17 - ETA: 3:35 - loss: 3.1799 - accuracy: 0.17 - ETA: 3:34 - loss: 3.1804 - accuracy: 0.17 - ETA: 3:33 - loss: 3.1812 - accuracy: 0.17 - ETA: 3:32 - loss: 3.1817 - accuracy: 0.17 - ETA: 3:31 - loss: 3.1815 - accuracy: 0.17 - ETA: 3:30 - loss: 3.1805 - accuracy: 0.17 - ETA: 3:29 - loss: 3.1805 - accuracy: 0.17 - ETA: 3:28 - loss: 3.1819 - accuracy: 0.17 - ETA: 3:27 - loss: 3.1839 - accuracy: 0.17 - ETA: 3:26 - loss: 3.1843 - accuracy: 0.17 - ETA: 3:25 - loss: 3.1845 - accuracy: 0.17 - ETA: 3:24 - loss: 3.1864 - accuracy: 0.17 - ETA: 3:23 - loss: 3.1858 - accuracy: 0.17 - ETA: 3:22 - loss: 3.1865 - accuracy: 0.17 - ETA: 3:21 - loss: 3.1870 - accuracy: 0.17 - ETA: 3:20 - loss: 3.1872 - accuracy: 0.17 - ETA: 3:19 - loss: 3.1884 - accuracy: 0.17 - ETA: 3:18 - loss: 3.1871 - accuracy: 0.17 - ETA: 3:17 - loss: 3.1858 - accuracy: 0.17 - ETA: 3:16 - loss: 3.1861 - accuracy: 0.17 - ETA: 3:15 - loss: 3.1842 - accuracy: 0.17 - ETA: 3:14 - loss: 3.1847 - accuracy: 0.17 - ETA: 3:13 - loss: 3.1857 - accuracy: 0.17 - ETA: 3:12 - loss: 3.1867 - accuracy: 0.17 - ETA: 3:11 - loss: 3.1856 - accuracy: 0.17 - ETA: 3:11 - loss: 3.1871 - accuracy: 0.17 - ETA: 3:10 - loss: 3.1858 - accuracy: 0.17 - ETA: 3:09 - loss: 3.1862 - accuracy: 0.17 - ETA: 3:08 - loss: 3.1863 - accuracy: 0.17 - ETA: 3:07 - loss: 3.1866 - accuracy: 0.17 - ETA: 3:06 - loss: 3.1870 - accuracy: 0.17 - ETA: 3:05 - loss: 3.1860 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1861 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1842 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1836 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1835 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1827 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1822 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1801 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1805 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1805 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1810 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1816 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1796 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1794 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1791 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1785 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1789 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1791 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1782 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1799 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1808 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1793 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1776 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1783 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1772 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1774 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1772 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1765 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1758 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1758 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1759 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1759 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1750 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1766 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1763 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1777 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1779 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1763 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1760 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1770 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1769 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1771 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1758 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1749 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1748 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1733 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1738 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1741 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1739 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1730 - accuracy: 0.1773"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.1721 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1714 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1710 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1693 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1685 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1687 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1683 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1681 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1676 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1677 - accuracy: 0.17 - ETA: 2:07 - loss: 3.1677 - accuracy: 0.17 - ETA: 2:06 - loss: 3.1670 - accuracy: 0.17 - ETA: 2:05 - loss: 3.1669 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1664 - accuracy: 0.17 - ETA: 2:04 - loss: 3.1685 - accuracy: 0.17 - ETA: 2:03 - loss: 3.1677 - accuracy: 0.17 - ETA: 2:02 - loss: 3.1671 - accuracy: 0.17 - ETA: 2:01 - loss: 3.1673 - accuracy: 0.17 - ETA: 2:00 - loss: 3.1673 - accuracy: 0.17 - ETA: 1:59 - loss: 3.1665 - accuracy: 0.17 - ETA: 1:58 - loss: 3.1650 - accuracy: 0.17 - ETA: 1:57 - loss: 3.1650 - accuracy: 0.17 - ETA: 1:56 - loss: 3.1646 - accuracy: 0.17 - ETA: 1:55 - loss: 3.1647 - accuracy: 0.17 - ETA: 1:54 - loss: 3.1639 - accuracy: 0.17 - ETA: 1:53 - loss: 3.1641 - accuracy: 0.17 - ETA: 1:52 - loss: 3.1632 - accuracy: 0.17 - ETA: 1:51 - loss: 3.1618 - accuracy: 0.17 - ETA: 1:50 - loss: 3.1617 - accuracy: 0.17 - ETA: 1:49 - loss: 3.1680 - accuracy: 0.17 - ETA: 1:48 - loss: 3.1664 - accuracy: 0.17 - ETA: 1:47 - loss: 3.1657 - accuracy: 0.18 - ETA: 1:46 - loss: 3.1670 - accuracy: 0.17 - ETA: 1:45 - loss: 3.1675 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1679 - accuracy: 0.17 - ETA: 1:44 - loss: 3.1679 - accuracy: 0.17 - ETA: 1:43 - loss: 3.1664 - accuracy: 0.17 - ETA: 1:42 - loss: 3.1653 - accuracy: 0.17 - ETA: 1:41 - loss: 3.1653 - accuracy: 0.17 - ETA: 1:40 - loss: 3.1654 - accuracy: 0.17 - ETA: 1:39 - loss: 3.1649 - accuracy: 0.17 - ETA: 1:38 - loss: 3.1646 - accuracy: 0.17 - ETA: 1:37 - loss: 3.1639 - accuracy: 0.17 - ETA: 1:36 - loss: 3.1641 - accuracy: 0.17 - ETA: 1:35 - loss: 3.1634 - accuracy: 0.18 - ETA: 1:34 - loss: 3.1636 - accuracy: 0.17 - ETA: 1:33 - loss: 3.1639 - accuracy: 0.17 - ETA: 1:32 - loss: 3.1638 - accuracy: 0.17 - ETA: 1:31 - loss: 3.1638 - accuracy: 0.17 - ETA: 1:30 - loss: 3.1628 - accuracy: 0.17 - ETA: 1:29 - loss: 3.1627 - accuracy: 0.17 - ETA: 1:28 - loss: 3.1623 - accuracy: 0.17 - ETA: 1:27 - loss: 3.1634 - accuracy: 0.17 - ETA: 1:26 - loss: 3.1626 - accuracy: 0.17 - ETA: 1:25 - loss: 3.1633 - accuracy: 0.17 - ETA: 1:24 - loss: 3.1621 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1628 - accuracy: 0.17 - ETA: 1:23 - loss: 3.1626 - accuracy: 0.17 - ETA: 1:22 - loss: 3.1626 - accuracy: 0.17 - ETA: 1:21 - loss: 3.1633 - accuracy: 0.17 - ETA: 1:20 - loss: 3.1629 - accuracy: 0.17 - ETA: 1:19 - loss: 3.1629 - accuracy: 0.17 - ETA: 1:18 - loss: 3.1629 - accuracy: 0.17 - ETA: 1:17 - loss: 3.1615 - accuracy: 0.17 - ETA: 1:16 - loss: 3.1616 - accuracy: 0.17 - ETA: 1:15 - loss: 3.1619 - accuracy: 0.17 - ETA: 1:14 - loss: 3.1615 - accuracy: 0.17 - ETA: 1:13 - loss: 3.1608 - accuracy: 0.17 - ETA: 1:12 - loss: 3.1609 - accuracy: 0.17 - ETA: 1:11 - loss: 3.1593 - accuracy: 0.18 - ETA: 1:10 - loss: 3.1588 - accuracy: 0.18 - ETA: 1:09 - loss: 3.1585 - accuracy: 0.17 - ETA: 1:08 - loss: 3.1580 - accuracy: 0.17 - ETA: 1:07 - loss: 3.1579 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1573 - accuracy: 0.17 - ETA: 1:06 - loss: 3.1582 - accuracy: 0.17 - ETA: 1:05 - loss: 3.1589 - accuracy: 0.17 - ETA: 1:04 - loss: 3.1575 - accuracy: 0.17 - ETA: 1:03 - loss: 3.1573 - accuracy: 0.17 - ETA: 1:02 - loss: 3.1579 - accuracy: 0.17 - ETA: 1:01 - loss: 3.1575 - accuracy: 0.17 - ETA: 1:00 - loss: 3.1570 - accuracy: 0.17 - ETA: 59s - loss: 3.1561 - accuracy: 0.1798 - ETA: 58s - loss: 3.1551 - accuracy: 0.180 - ETA: 57s - loss: 3.1550 - accuracy: 0.180 - ETA: 56s - loss: 3.1538 - accuracy: 0.180 - ETA: 55s - loss: 3.1588 - accuracy: 0.180 - ETA: 54s - loss: 3.1588 - accuracy: 0.180 - ETA: 53s - loss: 3.1575 - accuracy: 0.180 - ETA: 52s - loss: 3.1580 - accuracy: 0.180 - ETA: 51s - loss: 3.1571 - accuracy: 0.180 - ETA: 50s - loss: 3.1574 - accuracy: 0.180 - ETA: 49s - loss: 3.1574 - accuracy: 0.180 - ETA: 48s - loss: 3.1574 - accuracy: 0.180 - ETA: 47s - loss: 3.1568 - accuracy: 0.180 - ETA: 46s - loss: 3.1572 - accuracy: 0.180 - ETA: 45s - loss: 3.1572 - accuracy: 0.180 - ETA: 45s - loss: 3.1570 - accuracy: 0.180 - ETA: 44s - loss: 3.1574 - accuracy: 0.180 - ETA: 43s - loss: 3.1567 - accuracy: 0.180 - ETA: 42s - loss: 3.1567 - accuracy: 0.180 - ETA: 41s - loss: 3.1563 - accuracy: 0.180 - ETA: 40s - loss: 3.1558 - accuracy: 0.180 - ETA: 39s - loss: 3.1557 - accuracy: 0.180 - ETA: 38s - loss: 3.1552 - accuracy: 0.180 - ETA: 37s - loss: 3.1555 - accuracy: 0.180 - ETA: 36s - loss: 3.1555 - accuracy: 0.180 - ETA: 35s - loss: 3.1553 - accuracy: 0.180 - ETA: 34s - loss: 3.1540 - accuracy: 0.180 - ETA: 33s - loss: 3.1539 - accuracy: 0.180 - ETA: 32s - loss: 3.1544 - accuracy: 0.180 - ETA: 31s - loss: 3.1534 - accuracy: 0.180 - ETA: 30s - loss: 3.1536 - accuracy: 0.180 - ETA: 29s - loss: 3.1531 - accuracy: 0.180 - ETA: 28s - loss: 3.1530 - accuracy: 0.180 - ETA: 27s - loss: 3.1524 - accuracy: 0.180 - ETA: 26s - loss: 3.1527 - accuracy: 0.180 - ETA: 25s - loss: 3.1527 - accuracy: 0.180 - ETA: 24s - loss: 3.1530 - accuracy: 0.180 - ETA: 23s - loss: 3.1523 - accuracy: 0.180 - ETA: 23s - loss: 3.1524 - accuracy: 0.180 - ETA: 22s - loss: 3.1513 - accuracy: 0.180 - ETA: 21s - loss: 3.1508 - accuracy: 0.180 - ETA: 20s - loss: 3.1509 - accuracy: 0.180 - ETA: 19s - loss: 3.1511 - accuracy: 0.180 - ETA: 18s - loss: 3.1509 - accuracy: 0.180 - ETA: 17s - loss: 3.1504 - accuracy: 0.180 - ETA: 16s - loss: 3.1499 - accuracy: 0.180 - ETA: 15s - loss: 3.1502 - accuracy: 0.180 - ETA: 14s - loss: 3.1501 - accuracy: 0.180 - ETA: 13s - loss: 3.1493 - accuracy: 0.180 - ETA: 12s - loss: 3.1485 - accuracy: 0.180 - ETA: 11s - loss: 3.1484 - accuracy: 0.180 - ETA: 10s - loss: 3.1482 - accuracy: 0.180 - ETA: 9s - loss: 3.1474 - accuracy: 0.181 - ETA: 8s - loss: 3.1474 - accuracy: 0.18 - ETA: 7s - loss: 3.1457 - accuracy: 0.18 - ETA: 6s - loss: 3.1454 - accuracy: 0.18 - ETA: 5s - loss: 3.1441 - accuracy: 0.18 - ETA: 4s - loss: 3.1435 - accuracy: 0.18 - ETA: 3s - loss: 3.1430 - accuracy: 0.18 - ETA: 2s - loss: 3.1430 - accuracy: 0.18 - ETA: 1s - loss: 3.1429 - accuracy: 0.18 - ETA: 1s - loss: 3.1425 - accuracy: 0.18 - ETA: 0s - loss: 3.1420 - accuracy: 0.18 - 340s 8ms/step - loss: 3.1419 - accuracy: 0.1820 - val_loss: 4.2273 - val_accuracy: 0.0378\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:54 - loss: 3.0924 - accuracy: 0.15 - ETA: 4:59 - loss: 3.1130 - accuracy: 0.17 - ETA: 4:57 - loss: 2.9578 - accuracy: 0.22 - ETA: 4:57 - loss: 3.0295 - accuracy: 0.20 - ETA: 4:57 - loss: 3.0174 - accuracy: 0.20 - ETA: 4:55 - loss: 3.0445 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0305 - accuracy: 0.19 - ETA: 4:55 - loss: 3.0219 - accuracy: 0.19 - ETA: 4:55 - loss: 3.0199 - accuracy: 0.19 - ETA: 4:54 - loss: 2.9947 - accuracy: 0.20 - ETA: 4:54 - loss: 3.0099 - accuracy: 0.19 - ETA: 4:54 - loss: 3.0206 - accuracy: 0.19 - ETA: 4:54 - loss: 3.0428 - accuracy: 0.19 - ETA: 4:54 - loss: 3.0435 - accuracy: 0.19 - ETA: 4:54 - loss: 3.0440 - accuracy: 0.19 - ETA: 4:54 - loss: 3.0583 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0525 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0544 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0472 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0471 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0465 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0512 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0515 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0477 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0481 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0511 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0493 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0462 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0463 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0535 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0580 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0556 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0616 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0662 - accuracy: 0.18 - ETA: 4:39 - loss: 3.0641 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0617 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0585 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0552 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0568 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0566 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0557 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0554 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0557 - accuracy: 0.18 - ETA: 4:30 - loss: 3.0582 - accuracy: 0.18 - ETA: 4:29 - loss: 3.0573 - accuracy: 0.18 - ETA: 4:28 - loss: 3.0569 - accuracy: 0.18 - ETA: 4:27 - loss: 3.0591 - accuracy: 0.18 - ETA: 4:26 - loss: 3.0603 - accuracy: 0.18 - ETA: 4:25 - loss: 3.0619 - accuracy: 0.18 - ETA: 4:24 - loss: 3.0587 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0571 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0550 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0583 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0574 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0598 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0623 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0619 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0606 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0576 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0560 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0563 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0564 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0530 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0579 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0550 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0528 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0534 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0538 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0544 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0555 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0559 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0562 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0572 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0540 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0523 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0506 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0484 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0466 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0464 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0450 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0455 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0427 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0437 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0469 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0464 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0455 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0440 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0441 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0474 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0476 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0474 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0494 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0481 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0491 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0485 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0500 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0480 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0456 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0453 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0465 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0455 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0456 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0480 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0484 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0453 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0457 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0460 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0461 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0469 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0481 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0497 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0505 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0502 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0489 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0479 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0473 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0478 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0462 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0463 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0469 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0477 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0461 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0463 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0459 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0445 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0448 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0469 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0490 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0493 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0502 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0509 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0511 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0519 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0506 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0519 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0513 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0527 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0535 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0530 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0524 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0519 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0509 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0508 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0524 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0533 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0536 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0556 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0537 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0542 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0534 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0512 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0515 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0515 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0509 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0501 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0482 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0483 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0485 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0488 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0485 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0469 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0472 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0465 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0461 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0462 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0453 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0452 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0443 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0449 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0452 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0452 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0455 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0451 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0453 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0446 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0457 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0469 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0478 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0471 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0466 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0459 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0463 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0455 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0459 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0466 - accuracy: 0.19 - ETA: 2:17 - loss: 3.0459 - accuracy: 0.1944"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0460 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0461 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0471 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0473 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0470 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0468 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0471 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0470 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0488 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0485 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0500 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0492 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0500 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0503 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0502 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0506 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0509 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0507 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0505 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0507 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0507 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0495 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0500 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0508 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0499 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0499 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0502 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0507 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0513 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0515 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0520 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0522 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0533 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0538 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0539 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0544 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0549 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0558 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0553 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0553 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0556 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0559 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0560 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0559 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0556 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0552 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0551 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0548 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0547 - accuracy: 0.19 - ETA: 1:30 - loss: 3.0556 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0556 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0555 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0557 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0561 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0562 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0560 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0557 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0561 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0567 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0564 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0564 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0567 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0566 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0558 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0554 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0551 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0557 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0556 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0545 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0543 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0549 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0558 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0562 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0563 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0585 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0584 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0587 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0590 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0593 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0592 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0589 - accuracy: 0.19 - ETA: 59s - loss: 3.0585 - accuracy: 0.1939 - ETA: 58s - loss: 3.0586 - accuracy: 0.194 - ETA: 58s - loss: 3.0583 - accuracy: 0.194 - ETA: 57s - loss: 3.0579 - accuracy: 0.194 - ETA: 56s - loss: 3.0577 - accuracy: 0.194 - ETA: 55s - loss: 3.0572 - accuracy: 0.194 - ETA: 54s - loss: 3.0579 - accuracy: 0.193 - ETA: 53s - loss: 3.0575 - accuracy: 0.194 - ETA: 52s - loss: 3.0579 - accuracy: 0.194 - ETA: 51s - loss: 3.0577 - accuracy: 0.193 - ETA: 50s - loss: 3.0575 - accuracy: 0.193 - ETA: 49s - loss: 3.0581 - accuracy: 0.193 - ETA: 48s - loss: 3.0575 - accuracy: 0.193 - ETA: 47s - loss: 3.0580 - accuracy: 0.193 - ETA: 46s - loss: 3.0578 - accuracy: 0.193 - ETA: 45s - loss: 3.0574 - accuracy: 0.193 - ETA: 44s - loss: 3.0578 - accuracy: 0.193 - ETA: 43s - loss: 3.0579 - accuracy: 0.193 - ETA: 42s - loss: 3.0573 - accuracy: 0.194 - ETA: 41s - loss: 3.0569 - accuracy: 0.194 - ETA: 40s - loss: 3.0570 - accuracy: 0.194 - ETA: 39s - loss: 3.0572 - accuracy: 0.194 - ETA: 39s - loss: 3.0569 - accuracy: 0.194 - ETA: 38s - loss: 3.0565 - accuracy: 0.194 - ETA: 37s - loss: 3.0563 - accuracy: 0.194 - ETA: 36s - loss: 3.0562 - accuracy: 0.194 - ETA: 35s - loss: 3.0561 - accuracy: 0.194 - ETA: 34s - loss: 3.0565 - accuracy: 0.194 - ETA: 33s - loss: 3.0565 - accuracy: 0.194 - ETA: 32s - loss: 3.0563 - accuracy: 0.194 - ETA: 31s - loss: 3.0558 - accuracy: 0.194 - ETA: 30s - loss: 3.0562 - accuracy: 0.194 - ETA: 29s - loss: 3.0559 - accuracy: 0.194 - ETA: 28s - loss: 3.0562 - accuracy: 0.194 - ETA: 27s - loss: 3.0557 - accuracy: 0.194 - ETA: 26s - loss: 3.0555 - accuracy: 0.194 - ETA: 25s - loss: 3.0554 - accuracy: 0.194 - ETA: 24s - loss: 3.0557 - accuracy: 0.194 - ETA: 23s - loss: 3.0557 - accuracy: 0.193 - ETA: 22s - loss: 3.0551 - accuracy: 0.194 - ETA: 21s - loss: 3.0545 - accuracy: 0.193 - ETA: 21s - loss: 3.0539 - accuracy: 0.194 - ETA: 20s - loss: 3.0543 - accuracy: 0.193 - ETA: 19s - loss: 3.0549 - accuracy: 0.193 - ETA: 18s - loss: 3.0553 - accuracy: 0.193 - ETA: 17s - loss: 3.0551 - accuracy: 0.193 - ETA: 16s - loss: 3.0542 - accuracy: 0.193 - ETA: 15s - loss: 3.0544 - accuracy: 0.193 - ETA: 14s - loss: 3.0547 - accuracy: 0.193 - ETA: 13s - loss: 3.0547 - accuracy: 0.193 - ETA: 12s - loss: 3.0541 - accuracy: 0.193 - ETA: 11s - loss: 3.0541 - accuracy: 0.193 - ETA: 10s - loss: 3.0539 - accuracy: 0.193 - ETA: 9s - loss: 3.0543 - accuracy: 0.193 - ETA: 8s - loss: 3.0535 - accuracy: 0.19 - ETA: 7s - loss: 3.0538 - accuracy: 0.19 - ETA: 6s - loss: 3.0538 - accuracy: 0.19 - ETA: 5s - loss: 3.0543 - accuracy: 0.19 - ETA: 4s - loss: 3.0539 - accuracy: 0.19 - ETA: 3s - loss: 3.0538 - accuracy: 0.19 - ETA: 2s - loss: 3.0536 - accuracy: 0.19 - ETA: 1s - loss: 3.0534 - accuracy: 0.19 - ETA: 1s - loss: 3.0528 - accuracy: 0.19 - ETA: 0s - loss: 3.0528 - accuracy: 0.19 - 338s 8ms/step - loss: 3.0529 - accuracy: 0.1941 - val_loss: 4.3665 - val_accuracy: 0.0382\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:05 - loss: 2.9128 - accuracy: 0.23 - ETA: 5:06 - loss: 3.3580 - accuracy: 0.20 - ETA: 5:13 - loss: 3.2880 - accuracy: 0.19 - ETA: 5:12 - loss: 3.2252 - accuracy: 0.19 - ETA: 5:13 - loss: 3.1919 - accuracy: 0.19 - ETA: 5:10 - loss: 3.1820 - accuracy: 0.19 - ETA: 5:08 - loss: 3.1276 - accuracy: 0.19 - ETA: 5:06 - loss: 3.1015 - accuracy: 0.19 - ETA: 5:08 - loss: 3.0676 - accuracy: 0.20 - ETA: 5:06 - loss: 3.0691 - accuracy: 0.20 - ETA: 5:03 - loss: 3.0905 - accuracy: 0.20 - ETA: 5:03 - loss: 3.0947 - accuracy: 0.19 - ETA: 5:02 - loss: 3.0803 - accuracy: 0.19 - ETA: 5:02 - loss: 3.0728 - accuracy: 0.20 - ETA: 5:02 - loss: 3.0685 - accuracy: 0.20 - ETA: 5:02 - loss: 3.0665 - accuracy: 0.19 - ETA: 5:01 - loss: 3.0667 - accuracy: 0.19 - ETA: 5:00 - loss: 3.0681 - accuracy: 0.19 - ETA: 4:59 - loss: 3.0586 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0570 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0561 - accuracy: 0.19 - ETA: 4:55 - loss: 3.0526 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0531 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0487 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0541 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0514 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0409 - accuracy: 0.20 - ETA: 4:49 - loss: 3.0372 - accuracy: 0.20 - ETA: 4:48 - loss: 3.0368 - accuracy: 0.20 - ETA: 4:47 - loss: 3.0349 - accuracy: 0.20 - ETA: 4:46 - loss: 3.0365 - accuracy: 0.20 - ETA: 4:44 - loss: 3.0301 - accuracy: 0.20 - ETA: 4:43 - loss: 3.0257 - accuracy: 0.20 - ETA: 4:42 - loss: 3.0242 - accuracy: 0.20 - ETA: 4:41 - loss: 3.0180 - accuracy: 0.20 - ETA: 4:40 - loss: 3.0119 - accuracy: 0.20 - ETA: 4:39 - loss: 3.0152 - accuracy: 0.20 - ETA: 4:38 - loss: 3.0202 - accuracy: 0.20 - ETA: 4:37 - loss: 3.0141 - accuracy: 0.20 - ETA: 4:36 - loss: 3.0099 - accuracy: 0.20 - ETA: 4:35 - loss: 3.0092 - accuracy: 0.20 - ETA: 4:35 - loss: 3.0106 - accuracy: 0.20 - ETA: 4:34 - loss: 3.0219 - accuracy: 0.20 - ETA: 4:33 - loss: 3.0275 - accuracy: 0.20 - ETA: 4:32 - loss: 3.0253 - accuracy: 0.20 - ETA: 4:31 - loss: 3.0225 - accuracy: 0.20 - ETA: 4:30 - loss: 3.0159 - accuracy: 0.20 - ETA: 4:29 - loss: 3.0099 - accuracy: 0.20 - ETA: 4:28 - loss: 3.0092 - accuracy: 0.20 - ETA: 4:27 - loss: 3.0086 - accuracy: 0.20 - ETA: 4:26 - loss: 3.0089 - accuracy: 0.20 - ETA: 4:25 - loss: 3.0106 - accuracy: 0.20 - ETA: 4:24 - loss: 3.0108 - accuracy: 0.20 - ETA: 4:22 - loss: 3.0128 - accuracy: 0.20 - ETA: 4:21 - loss: 3.0140 - accuracy: 0.20 - ETA: 4:20 - loss: 3.0089 - accuracy: 0.20 - ETA: 4:19 - loss: 3.0084 - accuracy: 0.20 - ETA: 4:19 - loss: 3.0098 - accuracy: 0.20 - ETA: 4:18 - loss: 3.0100 - accuracy: 0.20 - ETA: 4:17 - loss: 3.0093 - accuracy: 0.20 - ETA: 4:16 - loss: 3.0110 - accuracy: 0.20 - ETA: 4:15 - loss: 3.0121 - accuracy: 0.20 - ETA: 4:14 - loss: 3.0120 - accuracy: 0.20 - ETA: 4:13 - loss: 3.0110 - accuracy: 0.20 - ETA: 4:12 - loss: 3.0100 - accuracy: 0.20 - ETA: 4:11 - loss: 3.0093 - accuracy: 0.20 - ETA: 4:10 - loss: 3.0101 - accuracy: 0.20 - ETA: 4:09 - loss: 3.0121 - accuracy: 0.20 - ETA: 4:08 - loss: 3.0125 - accuracy: 0.20 - ETA: 4:08 - loss: 3.0143 - accuracy: 0.20 - ETA: 4:06 - loss: 3.0153 - accuracy: 0.20 - ETA: 4:06 - loss: 3.0136 - accuracy: 0.20 - ETA: 4:05 - loss: 3.0131 - accuracy: 0.20 - ETA: 4:04 - loss: 3.0125 - accuracy: 0.20 - ETA: 4:03 - loss: 3.0083 - accuracy: 0.20 - ETA: 4:02 - loss: 3.0096 - accuracy: 0.20 - ETA: 4:01 - loss: 3.0106 - accuracy: 0.20 - ETA: 4:00 - loss: 3.0122 - accuracy: 0.20 - ETA: 3:59 - loss: 3.0122 - accuracy: 0.20 - ETA: 3:58 - loss: 3.0103 - accuracy: 0.20 - ETA: 3:57 - loss: 3.0102 - accuracy: 0.20 - ETA: 3:56 - loss: 3.0121 - accuracy: 0.20 - ETA: 3:55 - loss: 3.0106 - accuracy: 0.20 - ETA: 3:54 - loss: 3.0096 - accuracy: 0.20 - ETA: 3:53 - loss: 3.0108 - accuracy: 0.20 - ETA: 3:53 - loss: 3.0097 - accuracy: 0.20 - ETA: 3:52 - loss: 3.0078 - accuracy: 0.20 - ETA: 3:51 - loss: 3.0097 - accuracy: 0.20 - ETA: 3:51 - loss: 3.0071 - accuracy: 0.20 - ETA: 3:50 - loss: 3.0082 - accuracy: 0.20 - ETA: 3:49 - loss: 3.0099 - accuracy: 0.20 - ETA: 3:48 - loss: 3.0102 - accuracy: 0.20 - ETA: 3:47 - loss: 3.0103 - accuracy: 0.20 - ETA: 3:46 - loss: 3.0103 - accuracy: 0.20 - ETA: 3:45 - loss: 3.0133 - accuracy: 0.20 - ETA: 3:44 - loss: 3.0136 - accuracy: 0.20 - ETA: 3:43 - loss: 3.0145 - accuracy: 0.20 - ETA: 3:42 - loss: 3.0131 - accuracy: 0.20 - ETA: 3:41 - loss: 3.0127 - accuracy: 0.20 - ETA: 3:40 - loss: 3.0119 - accuracy: 0.20 - ETA: 3:39 - loss: 3.0124 - accuracy: 0.20 - ETA: 3:38 - loss: 3.0135 - accuracy: 0.20 - ETA: 3:37 - loss: 3.0139 - accuracy: 0.20 - ETA: 3:36 - loss: 3.0145 - accuracy: 0.20 - ETA: 3:35 - loss: 3.0151 - accuracy: 0.20 - ETA: 3:34 - loss: 3.0154 - accuracy: 0.20 - ETA: 3:33 - loss: 3.0151 - accuracy: 0.20 - ETA: 3:32 - loss: 3.0128 - accuracy: 0.20 - ETA: 3:31 - loss: 3.0134 - accuracy: 0.20 - ETA: 3:30 - loss: 3.0136 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0133 - accuracy: 0.20 - ETA: 3:29 - loss: 3.0147 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0143 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0134 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0139 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0137 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0121 - accuracy: 0.20 - ETA: 3:23 - loss: 3.0128 - accuracy: 0.20 - ETA: 3:21 - loss: 3.0133 - accuracy: 0.20 - ETA: 3:21 - loss: 3.0149 - accuracy: 0.20 - ETA: 3:20 - loss: 3.0152 - accuracy: 0.20 - ETA: 3:19 - loss: 3.0161 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0157 - accuracy: 0.20 - ETA: 3:17 - loss: 3.0160 - accuracy: 0.20 - ETA: 3:16 - loss: 3.0165 - accuracy: 0.20 - ETA: 3:15 - loss: 3.0158 - accuracy: 0.20 - ETA: 3:14 - loss: 3.0139 - accuracy: 0.20 - ETA: 3:13 - loss: 3.0127 - accuracy: 0.20 - ETA: 3:12 - loss: 3.0131 - accuracy: 0.20 - ETA: 3:11 - loss: 3.0126 - accuracy: 0.20 - ETA: 3:10 - loss: 3.0124 - accuracy: 0.20 - ETA: 3:09 - loss: 3.0107 - accuracy: 0.20 - ETA: 3:08 - loss: 3.0110 - accuracy: 0.20 - ETA: 3:07 - loss: 3.0107 - accuracy: 0.20 - ETA: 3:06 - loss: 3.0099 - accuracy: 0.20 - ETA: 3:05 - loss: 3.0097 - accuracy: 0.20 - ETA: 3:04 - loss: 3.0095 - accuracy: 0.20 - ETA: 3:03 - loss: 3.0103 - accuracy: 0.20 - ETA: 3:02 - loss: 3.0111 - accuracy: 0.20 - ETA: 3:01 - loss: 3.0111 - accuracy: 0.20 - ETA: 3:01 - loss: 3.0098 - accuracy: 0.20 - ETA: 3:00 - loss: 3.0106 - accuracy: 0.20 - ETA: 2:59 - loss: 3.0102 - accuracy: 0.20 - ETA: 2:58 - loss: 3.0109 - accuracy: 0.20 - ETA: 2:57 - loss: 3.0116 - accuracy: 0.20 - ETA: 2:56 - loss: 3.0129 - accuracy: 0.20 - ETA: 2:55 - loss: 3.0135 - accuracy: 0.20 - ETA: 2:54 - loss: 3.0127 - accuracy: 0.20 - ETA: 2:53 - loss: 3.0127 - accuracy: 0.20 - ETA: 2:52 - loss: 3.0134 - accuracy: 0.20 - ETA: 2:51 - loss: 3.0125 - accuracy: 0.20 - ETA: 2:50 - loss: 3.0117 - accuracy: 0.20 - ETA: 2:49 - loss: 3.0123 - accuracy: 0.20 - ETA: 2:48 - loss: 3.0104 - accuracy: 0.20 - ETA: 2:47 - loss: 3.0098 - accuracy: 0.20 - ETA: 2:47 - loss: 3.0090 - accuracy: 0.20 - ETA: 2:46 - loss: 3.0094 - accuracy: 0.20 - ETA: 2:45 - loss: 3.0093 - accuracy: 0.20 - ETA: 2:44 - loss: 3.0089 - accuracy: 0.20 - ETA: 2:43 - loss: 3.0089 - accuracy: 0.20 - ETA: 2:42 - loss: 3.0091 - accuracy: 0.20 - ETA: 2:41 - loss: 3.0079 - accuracy: 0.20 - ETA: 2:40 - loss: 3.0085 - accuracy: 0.20 - ETA: 2:39 - loss: 3.0078 - accuracy: 0.20 - ETA: 2:38 - loss: 3.0070 - accuracy: 0.20 - ETA: 2:37 - loss: 3.0076 - accuracy: 0.20 - ETA: 2:36 - loss: 3.0078 - accuracy: 0.20 - ETA: 2:35 - loss: 3.0074 - accuracy: 0.20 - ETA: 2:34 - loss: 3.0071 - accuracy: 0.20 - ETA: 2:33 - loss: 3.0071 - accuracy: 0.20 - ETA: 2:32 - loss: 3.0057 - accuracy: 0.20 - ETA: 2:31 - loss: 3.0057 - accuracy: 0.20 - ETA: 2:30 - loss: 3.0051 - accuracy: 0.20 - ETA: 2:30 - loss: 3.0047 - accuracy: 0.20 - ETA: 2:29 - loss: 3.0048 - accuracy: 0.20 - ETA: 2:28 - loss: 3.0032 - accuracy: 0.20 - ETA: 2:27 - loss: 3.0035 - accuracy: 0.20 - ETA: 2:26 - loss: 3.0043 - accuracy: 0.20 - ETA: 2:25 - loss: 3.0048 - accuracy: 0.20 - ETA: 2:24 - loss: 3.0045 - accuracy: 0.20 - ETA: 2:23 - loss: 3.0036 - accuracy: 0.20 - ETA: 2:22 - loss: 3.0036 - accuracy: 0.20 - ETA: 2:21 - loss: 3.0041 - accuracy: 0.20 - ETA: 2:20 - loss: 3.0046 - accuracy: 0.20 - ETA: 2:19 - loss: 3.0050 - accuracy: 0.20 - ETA: 2:18 - loss: 3.0058 - accuracy: 0.2019"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0062 - accuracy: 0.20 - ETA: 2:16 - loss: 3.0056 - accuracy: 0.20 - ETA: 2:15 - loss: 3.0062 - accuracy: 0.20 - ETA: 2:14 - loss: 3.0057 - accuracy: 0.20 - ETA: 2:13 - loss: 3.0057 - accuracy: 0.20 - ETA: 2:12 - loss: 3.0064 - accuracy: 0.20 - ETA: 2:11 - loss: 3.0061 - accuracy: 0.20 - ETA: 2:10 - loss: 3.0044 - accuracy: 0.20 - ETA: 2:09 - loss: 3.0063 - accuracy: 0.20 - ETA: 2:08 - loss: 3.0061 - accuracy: 0.20 - ETA: 2:07 - loss: 3.0060 - accuracy: 0.20 - ETA: 2:06 - loss: 3.0062 - accuracy: 0.20 - ETA: 2:06 - loss: 3.0058 - accuracy: 0.20 - ETA: 2:05 - loss: 3.0060 - accuracy: 0.20 - ETA: 2:04 - loss: 3.0060 - accuracy: 0.20 - ETA: 2:03 - loss: 3.0061 - accuracy: 0.20 - ETA: 2:02 - loss: 3.0050 - accuracy: 0.20 - ETA: 2:01 - loss: 3.0068 - accuracy: 0.20 - ETA: 2:00 - loss: 3.0060 - accuracy: 0.20 - ETA: 1:59 - loss: 3.0055 - accuracy: 0.20 - ETA: 1:58 - loss: 3.0048 - accuracy: 0.20 - ETA: 1:57 - loss: 3.0046 - accuracy: 0.20 - ETA: 1:56 - loss: 3.0036 - accuracy: 0.20 - ETA: 1:55 - loss: 3.0054 - accuracy: 0.20 - ETA: 1:54 - loss: 3.0060 - accuracy: 0.20 - ETA: 1:53 - loss: 3.0061 - accuracy: 0.20 - ETA: 1:52 - loss: 3.0057 - accuracy: 0.20 - ETA: 1:51 - loss: 3.0068 - accuracy: 0.20 - ETA: 1:50 - loss: 3.0061 - accuracy: 0.20 - ETA: 1:49 - loss: 3.0065 - accuracy: 0.20 - ETA: 1:48 - loss: 3.0061 - accuracy: 0.20 - ETA: 1:47 - loss: 3.0068 - accuracy: 0.20 - ETA: 1:46 - loss: 3.0075 - accuracy: 0.20 - ETA: 1:45 - loss: 3.0079 - accuracy: 0.20 - ETA: 1:44 - loss: 3.0082 - accuracy: 0.20 - ETA: 1:44 - loss: 3.0086 - accuracy: 0.20 - ETA: 1:43 - loss: 3.0078 - accuracy: 0.20 - ETA: 1:42 - loss: 3.0089 - accuracy: 0.20 - ETA: 1:41 - loss: 3.0082 - accuracy: 0.20 - ETA: 1:40 - loss: 3.0083 - accuracy: 0.20 - ETA: 1:39 - loss: 3.0080 - accuracy: 0.20 - ETA: 1:38 - loss: 3.0081 - accuracy: 0.20 - ETA: 1:37 - loss: 3.0085 - accuracy: 0.20 - ETA: 1:36 - loss: 3.0088 - accuracy: 0.20 - ETA: 1:35 - loss: 3.0088 - accuracy: 0.20 - ETA: 1:34 - loss: 3.0079 - accuracy: 0.20 - ETA: 1:33 - loss: 3.0081 - accuracy: 0.20 - ETA: 1:32 - loss: 3.0077 - accuracy: 0.20 - ETA: 1:31 - loss: 3.0088 - accuracy: 0.20 - ETA: 1:30 - loss: 3.0095 - accuracy: 0.20 - ETA: 1:29 - loss: 3.0092 - accuracy: 0.20 - ETA: 1:28 - loss: 3.0096 - accuracy: 0.20 - ETA: 1:27 - loss: 3.0095 - accuracy: 0.20 - ETA: 1:26 - loss: 3.0098 - accuracy: 0.20 - ETA: 1:25 - loss: 3.0120 - accuracy: 0.20 - ETA: 1:25 - loss: 3.0118 - accuracy: 0.20 - ETA: 1:24 - loss: 3.0115 - accuracy: 0.20 - ETA: 1:23 - loss: 3.0104 - accuracy: 0.20 - ETA: 1:22 - loss: 3.0097 - accuracy: 0.20 - ETA: 1:21 - loss: 3.0104 - accuracy: 0.20 - ETA: 1:20 - loss: 3.0108 - accuracy: 0.20 - ETA: 1:19 - loss: 3.0108 - accuracy: 0.20 - ETA: 1:18 - loss: 3.0110 - accuracy: 0.20 - ETA: 1:17 - loss: 3.0115 - accuracy: 0.20 - ETA: 1:16 - loss: 3.0120 - accuracy: 0.20 - ETA: 1:15 - loss: 3.0124 - accuracy: 0.20 - ETA: 1:14 - loss: 3.0136 - accuracy: 0.20 - ETA: 1:13 - loss: 3.0141 - accuracy: 0.20 - ETA: 1:12 - loss: 3.0140 - accuracy: 0.20 - ETA: 1:11 - loss: 3.0151 - accuracy: 0.20 - ETA: 1:10 - loss: 3.0152 - accuracy: 0.20 - ETA: 1:09 - loss: 3.0159 - accuracy: 0.20 - ETA: 1:08 - loss: 3.0158 - accuracy: 0.20 - ETA: 1:08 - loss: 3.0185 - accuracy: 0.20 - ETA: 1:07 - loss: 3.0189 - accuracy: 0.20 - ETA: 1:06 - loss: 3.0196 - accuracy: 0.20 - ETA: 1:05 - loss: 3.0203 - accuracy: 0.20 - ETA: 1:04 - loss: 3.0203 - accuracy: 0.20 - ETA: 1:03 - loss: 3.0206 - accuracy: 0.20 - ETA: 1:02 - loss: 3.0203 - accuracy: 0.20 - ETA: 1:01 - loss: 3.0201 - accuracy: 0.20 - ETA: 1:00 - loss: 3.0203 - accuracy: 0.20 - ETA: 59s - loss: 3.0205 - accuracy: 0.2006 - ETA: 58s - loss: 3.0207 - accuracy: 0.200 - ETA: 57s - loss: 3.0201 - accuracy: 0.200 - ETA: 56s - loss: 3.0201 - accuracy: 0.200 - ETA: 55s - loss: 3.0198 - accuracy: 0.200 - ETA: 54s - loss: 3.0204 - accuracy: 0.200 - ETA: 53s - loss: 3.0199 - accuracy: 0.200 - ETA: 52s - loss: 3.0200 - accuracy: 0.200 - ETA: 51s - loss: 3.0203 - accuracy: 0.200 - ETA: 50s - loss: 3.0209 - accuracy: 0.200 - ETA: 49s - loss: 3.0213 - accuracy: 0.200 - ETA: 48s - loss: 3.0209 - accuracy: 0.200 - ETA: 47s - loss: 3.0211 - accuracy: 0.200 - ETA: 46s - loss: 3.0213 - accuracy: 0.200 - ETA: 45s - loss: 3.0225 - accuracy: 0.200 - ETA: 45s - loss: 3.0221 - accuracy: 0.200 - ETA: 44s - loss: 3.0220 - accuracy: 0.200 - ETA: 43s - loss: 3.0218 - accuracy: 0.200 - ETA: 42s - loss: 3.0210 - accuracy: 0.200 - ETA: 41s - loss: 3.0214 - accuracy: 0.200 - ETA: 40s - loss: 3.0219 - accuracy: 0.200 - ETA: 39s - loss: 3.0217 - accuracy: 0.200 - ETA: 38s - loss: 3.0225 - accuracy: 0.199 - ETA: 37s - loss: 3.0222 - accuracy: 0.200 - ETA: 36s - loss: 3.0219 - accuracy: 0.200 - ETA: 35s - loss: 3.0225 - accuracy: 0.200 - ETA: 34s - loss: 3.0229 - accuracy: 0.199 - ETA: 33s - loss: 3.0232 - accuracy: 0.199 - ETA: 32s - loss: 3.0227 - accuracy: 0.199 - ETA: 31s - loss: 3.0220 - accuracy: 0.200 - ETA: 30s - loss: 3.0216 - accuracy: 0.200 - ETA: 29s - loss: 3.0211 - accuracy: 0.200 - ETA: 28s - loss: 3.0216 - accuracy: 0.200 - ETA: 27s - loss: 3.0215 - accuracy: 0.199 - ETA: 26s - loss: 3.0210 - accuracy: 0.200 - ETA: 25s - loss: 3.0210 - accuracy: 0.199 - ETA: 24s - loss: 3.0201 - accuracy: 0.200 - ETA: 23s - loss: 3.0201 - accuracy: 0.200 - ETA: 23s - loss: 3.0202 - accuracy: 0.200 - ETA: 22s - loss: 3.0198 - accuracy: 0.200 - ETA: 21s - loss: 3.0198 - accuracy: 0.200 - ETA: 20s - loss: 3.0205 - accuracy: 0.200 - ETA: 19s - loss: 3.0201 - accuracy: 0.200 - ETA: 18s - loss: 3.0199 - accuracy: 0.200 - ETA: 17s - loss: 3.0203 - accuracy: 0.200 - ETA: 16s - loss: 3.0208 - accuracy: 0.200 - ETA: 15s - loss: 3.0207 - accuracy: 0.200 - ETA: 14s - loss: 3.0203 - accuracy: 0.200 - ETA: 13s - loss: 3.0202 - accuracy: 0.200 - ETA: 12s - loss: 3.0206 - accuracy: 0.200 - ETA: 11s - loss: 3.0203 - accuracy: 0.200 - ETA: 10s - loss: 3.0198 - accuracy: 0.200 - ETA: 9s - loss: 3.0189 - accuracy: 0.200 - ETA: 8s - loss: 3.0189 - accuracy: 0.20 - ETA: 7s - loss: 3.0184 - accuracy: 0.20 - ETA: 6s - loss: 3.0183 - accuracy: 0.20 - ETA: 5s - loss: 3.0180 - accuracy: 0.20 - ETA: 4s - loss: 3.0179 - accuracy: 0.20 - ETA: 3s - loss: 3.0186 - accuracy: 0.19 - ETA: 2s - loss: 3.0183 - accuracy: 0.19 - ETA: 1s - loss: 3.0188 - accuracy: 0.19 - ETA: 1s - loss: 3.0192 - accuracy: 0.19 - ETA: 0s - loss: 3.0196 - accuracy: 0.19 - 340s 8ms/step - loss: 3.0197 - accuracy: 0.1995 - val_loss: 4.1486 - val_accuracy: 0.0377\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:16 - loss: 2.8033 - accuracy: 0.23 - ETA: 5:09 - loss: 2.9116 - accuracy: 0.20 - ETA: 5:14 - loss: 2.9874 - accuracy: 0.19 - ETA: 5:17 - loss: 2.9877 - accuracy: 0.20 - ETA: 5:18 - loss: 3.0002 - accuracy: 0.20 - ETA: 5:16 - loss: 2.9931 - accuracy: 0.20 - ETA: 5:15 - loss: 3.0186 - accuracy: 0.19 - ETA: 5:16 - loss: 3.0116 - accuracy: 0.20 - ETA: 5:15 - loss: 3.0172 - accuracy: 0.19 - ETA: 5:13 - loss: 3.0069 - accuracy: 0.19 - ETA: 5:11 - loss: 3.0249 - accuracy: 0.19 - ETA: 5:09 - loss: 3.0160 - accuracy: 0.19 - ETA: 5:08 - loss: 3.0145 - accuracy: 0.19 - ETA: 5:06 - loss: 3.0157 - accuracy: 0.19 - ETA: 5:05 - loss: 3.0262 - accuracy: 0.19 - ETA: 5:03 - loss: 3.0187 - accuracy: 0.19 - ETA: 5:01 - loss: 3.0274 - accuracy: 0.19 - ETA: 5:00 - loss: 3.0207 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0313 - accuracy: 0.19 - ETA: 4:57 - loss: 3.0333 - accuracy: 0.18 - ETA: 4:56 - loss: 3.0371 - accuracy: 0.18 - ETA: 4:54 - loss: 3.0371 - accuracy: 0.18 - ETA: 4:52 - loss: 3.0353 - accuracy: 0.18 - ETA: 4:51 - loss: 3.0387 - accuracy: 0.18 - ETA: 4:50 - loss: 3.0440 - accuracy: 0.18 - ETA: 4:49 - loss: 3.0476 - accuracy: 0.18 - ETA: 4:48 - loss: 3.0479 - accuracy: 0.18 - ETA: 4:47 - loss: 3.0538 - accuracy: 0.18 - ETA: 4:46 - loss: 3.0381 - accuracy: 0.18 - ETA: 4:45 - loss: 3.0308 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0363 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0353 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0296 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0272 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0371 - accuracy: 0.18 - ETA: 4:39 - loss: 3.0391 - accuracy: 0.18 - ETA: 4:38 - loss: 3.0439 - accuracy: 0.18 - ETA: 4:37 - loss: 3.0447 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0443 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0431 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0378 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0392 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0417 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0390 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0355 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0338 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0289 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0302 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0323 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0269 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0228 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0227 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0209 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0252 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0238 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0219 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0291 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0298 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0308 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0279 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0298 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0376 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0366 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0384 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0375 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0374 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0400 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0427 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0434 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0444 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0430 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0449 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0453 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0453 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0431 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0431 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0433 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0445 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0432 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0437 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0437 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0458 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0463 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0471 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0467 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0468 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0483 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0506 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0482 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0482 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0481 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0476 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0477 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0467 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0489 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0485 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0483 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0478 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0483 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0492 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0482 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0493 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0496 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0486 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0478 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0498 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0496 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0492 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0495 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0497 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0497 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0495 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0506 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0500 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0495 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0494 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0507 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0502 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0511 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0513 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0516 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0541 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0558 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0564 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0564 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0570 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0579 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0590 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0593 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0612 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0598 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0604 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0601 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0608 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0616 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0625 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0608 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0607 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0612 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0602 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0599 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0596 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0587 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0593 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0584 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0596 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0593 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0596 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0598 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0587 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0590 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0601 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0594 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0583 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0592 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0605 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0601 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0595 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0587 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0592 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0570 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0572 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0572 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0569 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0560 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0571 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0577 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0581 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0580 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0588 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0590 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0593 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0603 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0607 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0618 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0622 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0629 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0642 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0649 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0632 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0638 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0639 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0653 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0653 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0657 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0662 - accuracy: 0.1869"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0658 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0661 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0672 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0668 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0667 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0671 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0672 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0676 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0674 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0682 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0679 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0678 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0667 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0668 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0672 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0674 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0685 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0694 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0705 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0705 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0704 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0702 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0704 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0696 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0688 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0696 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0702 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0703 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0702 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0700 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0701 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0701 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0702 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0696 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0700 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0713 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0710 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0711 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0711 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0710 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0718 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0720 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0721 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0735 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0748 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0743 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0742 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0751 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0752 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0744 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0748 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0752 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0762 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0755 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0752 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0756 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0753 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0751 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0751 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0760 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0752 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0751 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0755 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0748 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0743 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0741 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0737 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0738 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0737 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0737 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0729 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0721 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0721 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0717 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0715 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0722 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0726 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0732 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0733 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0738 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0738 - accuracy: 0.18 - ETA: 59s - loss: 3.0741 - accuracy: 0.1841 - ETA: 58s - loss: 3.0742 - accuracy: 0.184 - ETA: 57s - loss: 3.0738 - accuracy: 0.184 - ETA: 56s - loss: 3.0737 - accuracy: 0.184 - ETA: 56s - loss: 3.0735 - accuracy: 0.184 - ETA: 55s - loss: 3.0738 - accuracy: 0.184 - ETA: 54s - loss: 3.0736 - accuracy: 0.184 - ETA: 53s - loss: 3.0749 - accuracy: 0.183 - ETA: 52s - loss: 3.0756 - accuracy: 0.183 - ETA: 51s - loss: 3.0760 - accuracy: 0.183 - ETA: 50s - loss: 3.0760 - accuracy: 0.183 - ETA: 49s - loss: 3.0766 - accuracy: 0.183 - ETA: 48s - loss: 3.0762 - accuracy: 0.183 - ETA: 47s - loss: 3.0762 - accuracy: 0.183 - ETA: 46s - loss: 3.0769 - accuracy: 0.183 - ETA: 45s - loss: 3.0769 - accuracy: 0.183 - ETA: 44s - loss: 3.0761 - accuracy: 0.183 - ETA: 43s - loss: 3.0764 - accuracy: 0.183 - ETA: 42s - loss: 3.0768 - accuracy: 0.183 - ETA: 41s - loss: 3.0773 - accuracy: 0.183 - ETA: 40s - loss: 3.0775 - accuracy: 0.183 - ETA: 39s - loss: 3.0784 - accuracy: 0.182 - ETA: 39s - loss: 3.0788 - accuracy: 0.182 - ETA: 38s - loss: 3.0789 - accuracy: 0.182 - ETA: 37s - loss: 3.0789 - accuracy: 0.182 - ETA: 36s - loss: 3.0792 - accuracy: 0.182 - ETA: 35s - loss: 3.0789 - accuracy: 0.182 - ETA: 34s - loss: 3.0802 - accuracy: 0.182 - ETA: 33s - loss: 3.0806 - accuracy: 0.182 - ETA: 32s - loss: 3.0804 - accuracy: 0.182 - ETA: 31s - loss: 3.0806 - accuracy: 0.182 - ETA: 30s - loss: 3.0799 - accuracy: 0.182 - ETA: 29s - loss: 3.0810 - accuracy: 0.182 - ETA: 28s - loss: 3.0804 - accuracy: 0.182 - ETA: 27s - loss: 3.0807 - accuracy: 0.182 - ETA: 26s - loss: 3.0815 - accuracy: 0.182 - ETA: 25s - loss: 3.0817 - accuracy: 0.182 - ETA: 24s - loss: 3.0811 - accuracy: 0.182 - ETA: 23s - loss: 3.0817 - accuracy: 0.182 - ETA: 22s - loss: 3.0824 - accuracy: 0.182 - ETA: 21s - loss: 3.0823 - accuracy: 0.182 - ETA: 20s - loss: 3.0823 - accuracy: 0.182 - ETA: 20s - loss: 3.0823 - accuracy: 0.182 - ETA: 19s - loss: 3.0821 - accuracy: 0.182 - ETA: 18s - loss: 3.0820 - accuracy: 0.182 - ETA: 17s - loss: 3.0823 - accuracy: 0.182 - ETA: 16s - loss: 3.0830 - accuracy: 0.182 - ETA: 15s - loss: 3.0847 - accuracy: 0.182 - ETA: 14s - loss: 3.0849 - accuracy: 0.182 - ETA: 13s - loss: 3.0850 - accuracy: 0.182 - ETA: 12s - loss: 3.0858 - accuracy: 0.182 - ETA: 11s - loss: 3.0857 - accuracy: 0.182 - ETA: 10s - loss: 3.0855 - accuracy: 0.182 - ETA: 9s - loss: 3.0855 - accuracy: 0.182 - ETA: 8s - loss: 3.0852 - accuracy: 0.18 - ETA: 7s - loss: 3.0857 - accuracy: 0.18 - ETA: 6s - loss: 3.0853 - accuracy: 0.18 - ETA: 5s - loss: 3.0856 - accuracy: 0.18 - ETA: 4s - loss: 3.0853 - accuracy: 0.18 - ETA: 3s - loss: 3.0858 - accuracy: 0.18 - ETA: 2s - loss: 3.0862 - accuracy: 0.18 - ETA: 1s - loss: 3.0863 - accuracy: 0.18 - ETA: 1s - loss: 3.0867 - accuracy: 0.18 - ETA: 0s - loss: 3.0864 - accuracy: 0.18 - 336s 8ms/step - loss: 3.0865 - accuracy: 0.1823 - val_loss: 4.3645 - val_accuracy: 0.0356\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:13 - loss: 3.0017 - accuracy: 0.19 - ETA: 5:16 - loss: 3.1019 - accuracy: 0.19 - ETA: 5:16 - loss: 3.1336 - accuracy: 0.18 - ETA: 5:14 - loss: 3.1209 - accuracy: 0.19 - ETA: 5:08 - loss: 3.1366 - accuracy: 0.19 - ETA: 5:05 - loss: 3.1851 - accuracy: 0.18 - ETA: 5:04 - loss: 3.1556 - accuracy: 0.19 - ETA: 5:04 - loss: 3.1608 - accuracy: 0.18 - ETA: 5:03 - loss: 3.1434 - accuracy: 0.18 - ETA: 5:01 - loss: 3.1541 - accuracy: 0.17 - ETA: 5:01 - loss: 3.1438 - accuracy: 0.17 - ETA: 5:00 - loss: 3.1305 - accuracy: 0.18 - ETA: 4:58 - loss: 3.1331 - accuracy: 0.18 - ETA: 4:57 - loss: 3.1384 - accuracy: 0.17 - ETA: 4:57 - loss: 3.1344 - accuracy: 0.17 - ETA: 4:56 - loss: 3.1288 - accuracy: 0.18 - ETA: 4:55 - loss: 3.1415 - accuracy: 0.17 - ETA: 4:54 - loss: 3.1538 - accuracy: 0.17 - ETA: 4:52 - loss: 3.1481 - accuracy: 0.17 - ETA: 4:51 - loss: 3.1470 - accuracy: 0.17 - ETA: 4:50 - loss: 3.1411 - accuracy: 0.17 - ETA: 4:50 - loss: 3.1407 - accuracy: 0.17 - ETA: 4:49 - loss: 3.1259 - accuracy: 0.17 - ETA: 4:48 - loss: 3.1199 - accuracy: 0.18 - ETA: 4:47 - loss: 3.1127 - accuracy: 0.18 - ETA: 4:47 - loss: 3.1211 - accuracy: 0.18 - ETA: 4:46 - loss: 3.1194 - accuracy: 0.18 - ETA: 4:46 - loss: 3.1236 - accuracy: 0.18 - ETA: 4:46 - loss: 3.1258 - accuracy: 0.18 - ETA: 4:46 - loss: 3.1251 - accuracy: 0.17 - ETA: 4:45 - loss: 3.1178 - accuracy: 0.18 - ETA: 4:44 - loss: 3.1143 - accuracy: 0.18 - ETA: 4:43 - loss: 3.1121 - accuracy: 0.18 - ETA: 4:42 - loss: 3.1120 - accuracy: 0.18 - ETA: 4:41 - loss: 3.1090 - accuracy: 0.18 - ETA: 4:40 - loss: 3.1083 - accuracy: 0.18 - ETA: 4:38 - loss: 3.1108 - accuracy: 0.18 - ETA: 4:37 - loss: 3.1150 - accuracy: 0.18 - ETA: 4:37 - loss: 3.1157 - accuracy: 0.18 - ETA: 4:36 - loss: 3.1114 - accuracy: 0.18 - ETA: 4:35 - loss: 3.1096 - accuracy: 0.18 - ETA: 4:34 - loss: 3.1108 - accuracy: 0.18 - ETA: 4:33 - loss: 3.1130 - accuracy: 0.18 - ETA: 4:32 - loss: 3.1165 - accuracy: 0.18 - ETA: 4:31 - loss: 3.1165 - accuracy: 0.18 - ETA: 4:30 - loss: 3.1199 - accuracy: 0.18 - ETA: 4:29 - loss: 3.1225 - accuracy: 0.18 - ETA: 4:28 - loss: 3.1298 - accuracy: 0.17 - ETA: 4:27 - loss: 3.1260 - accuracy: 0.17 - ETA: 4:26 - loss: 3.1307 - accuracy: 0.17 - ETA: 4:25 - loss: 3.1328 - accuracy: 0.17 - ETA: 4:24 - loss: 3.1379 - accuracy: 0.17 - ETA: 4:23 - loss: 3.1383 - accuracy: 0.17 - ETA: 4:22 - loss: 3.1422 - accuracy: 0.17 - ETA: 4:21 - loss: 3.1430 - accuracy: 0.17 - ETA: 4:20 - loss: 3.1472 - accuracy: 0.17 - ETA: 4:19 - loss: 3.1450 - accuracy: 0.17 - ETA: 4:18 - loss: 3.1500 - accuracy: 0.17 - ETA: 4:18 - loss: 3.1595 - accuracy: 0.17 - ETA: 4:17 - loss: 3.1637 - accuracy: 0.17 - ETA: 4:16 - loss: 3.1694 - accuracy: 0.17 - ETA: 4:15 - loss: 3.1688 - accuracy: 0.17 - ETA: 4:14 - loss: 3.1750 - accuracy: 0.17 - ETA: 4:13 - loss: 3.1768 - accuracy: 0.17 - ETA: 4:12 - loss: 3.1813 - accuracy: 0.17 - ETA: 4:11 - loss: 3.1770 - accuracy: 0.17 - ETA: 4:10 - loss: 3.1804 - accuracy: 0.17 - ETA: 4:09 - loss: 3.1802 - accuracy: 0.17 - ETA: 4:09 - loss: 3.1806 - accuracy: 0.17 - ETA: 4:08 - loss: 3.1799 - accuracy: 0.17 - ETA: 4:07 - loss: 3.1791 - accuracy: 0.17 - ETA: 4:06 - loss: 3.1806 - accuracy: 0.17 - ETA: 4:05 - loss: 3.1812 - accuracy: 0.17 - ETA: 4:04 - loss: 3.1796 - accuracy: 0.17 - ETA: 4:03 - loss: 3.1783 - accuracy: 0.17 - ETA: 4:02 - loss: 3.1793 - accuracy: 0.17 - ETA: 4:01 - loss: 3.1797 - accuracy: 0.17 - ETA: 4:00 - loss: 3.1781 - accuracy: 0.17 - ETA: 3:59 - loss: 3.1777 - accuracy: 0.17 - ETA: 3:58 - loss: 3.1777 - accuracy: 0.17 - ETA: 3:58 - loss: 3.1791 - accuracy: 0.17 - ETA: 3:57 - loss: 3.1791 - accuracy: 0.17 - ETA: 3:56 - loss: 3.1794 - accuracy: 0.17 - ETA: 3:55 - loss: 3.1786 - accuracy: 0.17 - ETA: 3:54 - loss: 3.1802 - accuracy: 0.17 - ETA: 3:53 - loss: 3.1779 - accuracy: 0.17 - ETA: 3:52 - loss: 3.1771 - accuracy: 0.17 - ETA: 3:51 - loss: 3.1741 - accuracy: 0.17 - ETA: 3:50 - loss: 3.1716 - accuracy: 0.17 - ETA: 3:49 - loss: 3.1703 - accuracy: 0.17 - ETA: 3:48 - loss: 3.1723 - accuracy: 0.17 - ETA: 3:47 - loss: 3.1738 - accuracy: 0.17 - ETA: 3:46 - loss: 3.1728 - accuracy: 0.17 - ETA: 3:45 - loss: 3.1721 - accuracy: 0.17 - ETA: 3:44 - loss: 3.1717 - accuracy: 0.17 - ETA: 3:43 - loss: 3.1700 - accuracy: 0.17 - ETA: 3:42 - loss: 3.1706 - accuracy: 0.17 - ETA: 3:41 - loss: 3.1716 - accuracy: 0.17 - ETA: 3:40 - loss: 3.1716 - accuracy: 0.17 - ETA: 3:39 - loss: 3.1713 - accuracy: 0.17 - ETA: 3:38 - loss: 3.1713 - accuracy: 0.17 - ETA: 3:37 - loss: 3.1700 - accuracy: 0.17 - ETA: 3:36 - loss: 3.1690 - accuracy: 0.17 - ETA: 3:35 - loss: 3.1693 - accuracy: 0.17 - ETA: 3:34 - loss: 3.1687 - accuracy: 0.17 - ETA: 3:33 - loss: 3.1709 - accuracy: 0.17 - ETA: 3:32 - loss: 3.1712 - accuracy: 0.17 - ETA: 3:31 - loss: 3.1701 - accuracy: 0.17 - ETA: 3:31 - loss: 3.1689 - accuracy: 0.17 - ETA: 3:30 - loss: 3.1665 - accuracy: 0.17 - ETA: 3:29 - loss: 3.1651 - accuracy: 0.17 - ETA: 3:28 - loss: 3.1665 - accuracy: 0.17 - ETA: 3:27 - loss: 3.1651 - accuracy: 0.17 - ETA: 3:26 - loss: 3.1637 - accuracy: 0.17 - ETA: 3:25 - loss: 3.1646 - accuracy: 0.17 - ETA: 3:24 - loss: 3.1630 - accuracy: 0.17 - ETA: 3:23 - loss: 3.1621 - accuracy: 0.17 - ETA: 3:22 - loss: 3.1629 - accuracy: 0.17 - ETA: 3:21 - loss: 3.1624 - accuracy: 0.17 - ETA: 3:20 - loss: 3.1609 - accuracy: 0.17 - ETA: 3:19 - loss: 3.1618 - accuracy: 0.17 - ETA: 3:18 - loss: 3.1621 - accuracy: 0.17 - ETA: 3:17 - loss: 3.1617 - accuracy: 0.17 - ETA: 3:16 - loss: 3.1616 - accuracy: 0.17 - ETA: 3:15 - loss: 3.1613 - accuracy: 0.17 - ETA: 3:14 - loss: 3.1611 - accuracy: 0.17 - ETA: 3:13 - loss: 3.1609 - accuracy: 0.17 - ETA: 3:13 - loss: 3.1599 - accuracy: 0.17 - ETA: 3:12 - loss: 3.1588 - accuracy: 0.17 - ETA: 3:11 - loss: 3.1590 - accuracy: 0.17 - ETA: 3:10 - loss: 3.1576 - accuracy: 0.17 - ETA: 3:09 - loss: 3.1569 - accuracy: 0.17 - ETA: 3:08 - loss: 3.1557 - accuracy: 0.17 - ETA: 3:07 - loss: 3.1563 - accuracy: 0.17 - ETA: 3:06 - loss: 3.1565 - accuracy: 0.17 - ETA: 3:05 - loss: 3.1565 - accuracy: 0.17 - ETA: 3:04 - loss: 3.1569 - accuracy: 0.17 - ETA: 3:03 - loss: 3.1569 - accuracy: 0.17 - ETA: 3:02 - loss: 3.1568 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1564 - accuracy: 0.17 - ETA: 3:01 - loss: 3.1553 - accuracy: 0.17 - ETA: 3:00 - loss: 3.1551 - accuracy: 0.17 - ETA: 2:59 - loss: 3.1552 - accuracy: 0.17 - ETA: 2:58 - loss: 3.1552 - accuracy: 0.17 - ETA: 2:57 - loss: 3.1539 - accuracy: 0.17 - ETA: 2:56 - loss: 3.1547 - accuracy: 0.17 - ETA: 2:55 - loss: 3.1548 - accuracy: 0.17 - ETA: 2:54 - loss: 3.1531 - accuracy: 0.17 - ETA: 2:53 - loss: 3.1541 - accuracy: 0.17 - ETA: 2:52 - loss: 3.1516 - accuracy: 0.17 - ETA: 2:51 - loss: 3.1520 - accuracy: 0.17 - ETA: 2:50 - loss: 3.1500 - accuracy: 0.17 - ETA: 2:49 - loss: 3.1500 - accuracy: 0.17 - ETA: 2:48 - loss: 3.1486 - accuracy: 0.17 - ETA: 2:47 - loss: 3.1483 - accuracy: 0.17 - ETA: 2:46 - loss: 3.1478 - accuracy: 0.17 - ETA: 2:45 - loss: 3.1485 - accuracy: 0.17 - ETA: 2:44 - loss: 3.1486 - accuracy: 0.17 - ETA: 2:43 - loss: 3.1474 - accuracy: 0.17 - ETA: 2:42 - loss: 3.1478 - accuracy: 0.17 - ETA: 2:41 - loss: 3.1480 - accuracy: 0.17 - ETA: 2:40 - loss: 3.1464 - accuracy: 0.17 - ETA: 2:39 - loss: 3.1457 - accuracy: 0.17 - ETA: 2:38 - loss: 3.1448 - accuracy: 0.17 - ETA: 2:37 - loss: 3.1438 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1423 - accuracy: 0.17 - ETA: 2:36 - loss: 3.1427 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1416 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1416 - accuracy: 0.17 - ETA: 2:33 - loss: 3.1405 - accuracy: 0.17 - ETA: 2:32 - loss: 3.1401 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1403 - accuracy: 0.17 - ETA: 2:30 - loss: 3.1400 - accuracy: 0.17 - ETA: 2:29 - loss: 3.1388 - accuracy: 0.17 - ETA: 2:28 - loss: 3.1376 - accuracy: 0.17 - ETA: 2:27 - loss: 3.1372 - accuracy: 0.17 - ETA: 2:26 - loss: 3.1360 - accuracy: 0.17 - ETA: 2:25 - loss: 3.1345 - accuracy: 0.17 - ETA: 2:24 - loss: 3.1334 - accuracy: 0.17 - ETA: 2:23 - loss: 3.1327 - accuracy: 0.17 - ETA: 2:22 - loss: 3.1316 - accuracy: 0.17 - ETA: 2:21 - loss: 3.1321 - accuracy: 0.17 - ETA: 2:20 - loss: 3.1312 - accuracy: 0.17 - ETA: 2:19 - loss: 3.1314 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1320 - accuracy: 0.17 - ETA: 2:18 - loss: 3.1309 - accuracy: 0.1782"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.1318 - accuracy: 0.17 - ETA: 2:16 - loss: 3.1316 - accuracy: 0.17 - ETA: 2:15 - loss: 3.1307 - accuracy: 0.17 - ETA: 2:14 - loss: 3.1302 - accuracy: 0.17 - ETA: 2:13 - loss: 3.1292 - accuracy: 0.17 - ETA: 2:12 - loss: 3.1287 - accuracy: 0.17 - ETA: 2:11 - loss: 3.1315 - accuracy: 0.17 - ETA: 2:10 - loss: 3.1305 - accuracy: 0.17 - ETA: 2:09 - loss: 3.1299 - accuracy: 0.17 - ETA: 2:08 - loss: 3.1293 - accuracy: 0.18 - ETA: 2:07 - loss: 3.1283 - accuracy: 0.18 - ETA: 2:06 - loss: 3.1272 - accuracy: 0.18 - ETA: 2:05 - loss: 3.1278 - accuracy: 0.18 - ETA: 2:04 - loss: 3.1280 - accuracy: 0.18 - ETA: 2:03 - loss: 3.1273 - accuracy: 0.18 - ETA: 2:02 - loss: 3.1265 - accuracy: 0.18 - ETA: 2:01 - loss: 3.1256 - accuracy: 0.18 - ETA: 2:00 - loss: 3.1234 - accuracy: 0.18 - ETA: 2:00 - loss: 3.1224 - accuracy: 0.18 - ETA: 1:59 - loss: 3.1225 - accuracy: 0.18 - ETA: 1:58 - loss: 3.1233 - accuracy: 0.18 - ETA: 1:57 - loss: 3.1239 - accuracy: 0.18 - ETA: 1:56 - loss: 3.1230 - accuracy: 0.18 - ETA: 1:55 - loss: 3.1225 - accuracy: 0.18 - ETA: 1:54 - loss: 3.1220 - accuracy: 0.18 - ETA: 1:53 - loss: 3.1221 - accuracy: 0.18 - ETA: 1:52 - loss: 3.1206 - accuracy: 0.18 - ETA: 1:51 - loss: 3.1206 - accuracy: 0.18 - ETA: 1:50 - loss: 3.1205 - accuracy: 0.18 - ETA: 1:49 - loss: 3.1204 - accuracy: 0.18 - ETA: 1:48 - loss: 3.1205 - accuracy: 0.18 - ETA: 1:47 - loss: 3.1204 - accuracy: 0.18 - ETA: 1:46 - loss: 3.1201 - accuracy: 0.18 - ETA: 1:45 - loss: 3.1205 - accuracy: 0.18 - ETA: 1:45 - loss: 3.1197 - accuracy: 0.18 - ETA: 1:44 - loss: 3.1202 - accuracy: 0.18 - ETA: 1:43 - loss: 3.1198 - accuracy: 0.18 - ETA: 1:42 - loss: 3.1190 - accuracy: 0.18 - ETA: 1:41 - loss: 3.1181 - accuracy: 0.18 - ETA: 1:40 - loss: 3.1179 - accuracy: 0.18 - ETA: 1:39 - loss: 3.1221 - accuracy: 0.18 - ETA: 1:38 - loss: 3.1220 - accuracy: 0.18 - ETA: 1:37 - loss: 3.1231 - accuracy: 0.18 - ETA: 1:36 - loss: 3.1232 - accuracy: 0.18 - ETA: 1:35 - loss: 3.1244 - accuracy: 0.18 - ETA: 1:34 - loss: 3.1253 - accuracy: 0.18 - ETA: 1:33 - loss: 3.1258 - accuracy: 0.18 - ETA: 1:32 - loss: 3.1263 - accuracy: 0.18 - ETA: 1:31 - loss: 3.1274 - accuracy: 0.18 - ETA: 1:30 - loss: 3.1269 - accuracy: 0.18 - ETA: 1:29 - loss: 3.1274 - accuracy: 0.18 - ETA: 1:28 - loss: 3.1266 - accuracy: 0.18 - ETA: 1:27 - loss: 3.1269 - accuracy: 0.18 - ETA: 1:26 - loss: 3.1267 - accuracy: 0.18 - ETA: 1:25 - loss: 3.1261 - accuracy: 0.18 - ETA: 1:24 - loss: 3.1258 - accuracy: 0.18 - ETA: 1:23 - loss: 3.1253 - accuracy: 0.18 - ETA: 1:22 - loss: 3.1244 - accuracy: 0.18 - ETA: 1:22 - loss: 3.1239 - accuracy: 0.18 - ETA: 1:21 - loss: 3.1245 - accuracy: 0.18 - ETA: 1:20 - loss: 3.1236 - accuracy: 0.18 - ETA: 1:19 - loss: 3.1235 - accuracy: 0.18 - ETA: 1:18 - loss: 3.1245 - accuracy: 0.18 - ETA: 1:17 - loss: 3.1240 - accuracy: 0.18 - ETA: 1:16 - loss: 3.1245 - accuracy: 0.18 - ETA: 1:15 - loss: 3.1248 - accuracy: 0.18 - ETA: 1:14 - loss: 3.1249 - accuracy: 0.18 - ETA: 1:13 - loss: 3.1236 - accuracy: 0.18 - ETA: 1:12 - loss: 3.1228 - accuracy: 0.18 - ETA: 1:11 - loss: 3.1227 - accuracy: 0.18 - ETA: 1:10 - loss: 3.1221 - accuracy: 0.18 - ETA: 1:09 - loss: 3.1219 - accuracy: 0.18 - ETA: 1:08 - loss: 3.1239 - accuracy: 0.18 - ETA: 1:07 - loss: 3.1241 - accuracy: 0.18 - ETA: 1:06 - loss: 3.1236 - accuracy: 0.18 - ETA: 1:05 - loss: 3.1236 - accuracy: 0.18 - ETA: 1:04 - loss: 3.1235 - accuracy: 0.18 - ETA: 1:03 - loss: 3.1230 - accuracy: 0.18 - ETA: 1:02 - loss: 3.1237 - accuracy: 0.18 - ETA: 1:01 - loss: 3.1230 - accuracy: 0.18 - ETA: 1:01 - loss: 3.1226 - accuracy: 0.18 - ETA: 1:00 - loss: 3.1232 - accuracy: 0.18 - ETA: 59s - loss: 3.1228 - accuracy: 0.1830 - ETA: 58s - loss: 3.1227 - accuracy: 0.183 - ETA: 57s - loss: 3.1217 - accuracy: 0.183 - ETA: 56s - loss: 3.1215 - accuracy: 0.183 - ETA: 55s - loss: 3.1206 - accuracy: 0.183 - ETA: 54s - loss: 3.1203 - accuracy: 0.183 - ETA: 53s - loss: 3.1206 - accuracy: 0.183 - ETA: 52s - loss: 3.1204 - accuracy: 0.183 - ETA: 51s - loss: 3.1196 - accuracy: 0.183 - ETA: 50s - loss: 3.1197 - accuracy: 0.183 - ETA: 49s - loss: 3.1199 - accuracy: 0.183 - ETA: 48s - loss: 3.1200 - accuracy: 0.183 - ETA: 47s - loss: 3.1199 - accuracy: 0.183 - ETA: 46s - loss: 3.1207 - accuracy: 0.183 - ETA: 45s - loss: 3.1207 - accuracy: 0.183 - ETA: 44s - loss: 3.1211 - accuracy: 0.183 - ETA: 43s - loss: 3.1199 - accuracy: 0.183 - ETA: 42s - loss: 3.1203 - accuracy: 0.183 - ETA: 41s - loss: 3.1204 - accuracy: 0.183 - ETA: 41s - loss: 3.1196 - accuracy: 0.183 - ETA: 40s - loss: 3.1193 - accuracy: 0.183 - ETA: 39s - loss: 3.1195 - accuracy: 0.183 - ETA: 38s - loss: 3.1190 - accuracy: 0.183 - ETA: 37s - loss: 3.1185 - accuracy: 0.183 - ETA: 36s - loss: 3.1178 - accuracy: 0.183 - ETA: 35s - loss: 3.1184 - accuracy: 0.183 - ETA: 34s - loss: 3.1188 - accuracy: 0.182 - ETA: 33s - loss: 3.1194 - accuracy: 0.182 - ETA: 32s - loss: 3.1194 - accuracy: 0.182 - ETA: 31s - loss: 3.1192 - accuracy: 0.182 - ETA: 30s - loss: 3.1196 - accuracy: 0.182 - ETA: 29s - loss: 3.1186 - accuracy: 0.183 - ETA: 28s - loss: 3.1185 - accuracy: 0.183 - ETA: 27s - loss: 3.1185 - accuracy: 0.183 - ETA: 26s - loss: 3.1182 - accuracy: 0.183 - ETA: 25s - loss: 3.1187 - accuracy: 0.182 - ETA: 24s - loss: 3.1190 - accuracy: 0.182 - ETA: 23s - loss: 3.1193 - accuracy: 0.182 - ETA: 22s - loss: 3.1197 - accuracy: 0.182 - ETA: 21s - loss: 3.1200 - accuracy: 0.182 - ETA: 21s - loss: 3.1197 - accuracy: 0.182 - ETA: 20s - loss: 3.1191 - accuracy: 0.182 - ETA: 19s - loss: 3.1192 - accuracy: 0.182 - ETA: 18s - loss: 3.1196 - accuracy: 0.182 - ETA: 17s - loss: 3.1187 - accuracy: 0.182 - ETA: 16s - loss: 3.1188 - accuracy: 0.182 - ETA: 15s - loss: 3.1193 - accuracy: 0.182 - ETA: 14s - loss: 3.1191 - accuracy: 0.182 - ETA: 13s - loss: 3.1187 - accuracy: 0.182 - ETA: 12s - loss: 3.1190 - accuracy: 0.182 - ETA: 11s - loss: 3.1187 - accuracy: 0.182 - ETA: 10s - loss: 3.1194 - accuracy: 0.182 - ETA: 9s - loss: 3.1197 - accuracy: 0.182 - ETA: 8s - loss: 3.1202 - accuracy: 0.18 - ETA: 7s - loss: 3.1201 - accuracy: 0.18 - ETA: 6s - loss: 3.1202 - accuracy: 0.18 - ETA: 5s - loss: 3.1208 - accuracy: 0.18 - ETA: 4s - loss: 3.1201 - accuracy: 0.18 - ETA: 3s - loss: 3.1197 - accuracy: 0.18 - ETA: 2s - loss: 3.1195 - accuracy: 0.18 - ETA: 1s - loss: 3.1204 - accuracy: 0.18 - ETA: 1s - loss: 3.1203 - accuracy: 0.18 - ETA: 0s - loss: 3.1211 - accuracy: 0.18 - 339s 8ms/step - loss: 3.1212 - accuracy: 0.1820 - val_loss: 4.1015 - val_accuracy: 0.0390\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:48 - loss: 2.9892 - accuracy: 0.21 - ETA: 5:22 - loss: 3.1729 - accuracy: 0.16 - ETA: 5:16 - loss: 3.0973 - accuracy: 0.19 - ETA: 5:15 - loss: 3.1012 - accuracy: 0.18 - ETA: 5:12 - loss: 3.1196 - accuracy: 0.17 - ETA: 5:11 - loss: 3.1463 - accuracy: 0.17 - ETA: 5:11 - loss: 3.1563 - accuracy: 0.17 - ETA: 5:08 - loss: 3.1508 - accuracy: 0.17 - ETA: 5:07 - loss: 3.1323 - accuracy: 0.17 - ETA: 5:06 - loss: 3.1391 - accuracy: 0.17 - ETA: 5:04 - loss: 3.1393 - accuracy: 0.18 - ETA: 5:02 - loss: 3.1461 - accuracy: 0.18 - ETA: 5:00 - loss: 3.1378 - accuracy: 0.18 - ETA: 4:59 - loss: 3.1404 - accuracy: 0.18 - ETA: 4:58 - loss: 3.1292 - accuracy: 0.18 - ETA: 4:57 - loss: 3.1255 - accuracy: 0.18 - ETA: 4:56 - loss: 3.1147 - accuracy: 0.18 - ETA: 4:55 - loss: 3.1134 - accuracy: 0.18 - ETA: 4:54 - loss: 3.1036 - accuracy: 0.18 - ETA: 4:53 - loss: 3.0956 - accuracy: 0.18 - ETA: 4:53 - loss: 3.0935 - accuracy: 0.18 - ETA: 4:52 - loss: 3.0904 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0892 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0845 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0841 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0825 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0828 - accuracy: 0.18 - ETA: 4:47 - loss: 3.0838 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0861 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0882 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0901 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0897 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0973 - accuracy: 0.18 - ETA: 4:42 - loss: 3.0956 - accuracy: 0.18 - ETA: 4:41 - loss: 3.0988 - accuracy: 0.18 - ETA: 4:40 - loss: 3.0986 - accuracy: 0.18 - ETA: 4:39 - loss: 3.0947 - accuracy: 0.18 - ETA: 4:38 - loss: 3.0901 - accuracy: 0.18 - ETA: 4:37 - loss: 3.0895 - accuracy: 0.18 - ETA: 4:36 - loss: 3.0875 - accuracy: 0.18 - ETA: 4:35 - loss: 3.0861 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0871 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0825 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0836 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0806 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0796 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0789 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0816 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0795 - accuracy: 0.18 - ETA: 4:27 - loss: 3.0782 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0740 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0731 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0704 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0710 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0717 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0721 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0674 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0700 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0741 - accuracy: 0.18 - ETA: 4:18 - loss: 3.0746 - accuracy: 0.18 - ETA: 4:17 - loss: 3.0777 - accuracy: 0.18 - ETA: 4:16 - loss: 3.0847 - accuracy: 0.18 - ETA: 4:16 - loss: 3.0862 - accuracy: 0.18 - ETA: 4:15 - loss: 3.0839 - accuracy: 0.18 - ETA: 4:14 - loss: 3.0847 - accuracy: 0.18 - ETA: 4:13 - loss: 3.0842 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0830 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0809 - accuracy: 0.18 - ETA: 4:09 - loss: 3.0781 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0748 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0759 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0727 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0690 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0672 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0690 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0683 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0661 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0674 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0666 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0663 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0640 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0608 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0590 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0582 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0573 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0563 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0564 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0545 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0565 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0572 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0590 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0582 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0565 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0549 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0554 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0561 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0543 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0550 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0553 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0546 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0556 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0560 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0547 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0538 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0534 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0511 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0503 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0497 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0483 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0479 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0483 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0483 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0487 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0487 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0488 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0471 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0465 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0461 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0455 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0485 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0496 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0484 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0475 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0458 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0469 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0469 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0458 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0450 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0472 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0466 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0461 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0452 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0447 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0454 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0458 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0454 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0461 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0471 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0469 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0480 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0474 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0476 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0470 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0466 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0474 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0473 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0479 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0477 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0468 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0453 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0451 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0454 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0461 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0460 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0454 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0455 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0440 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0447 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0433 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0466 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0469 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0482 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0487 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0481 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0489 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0487 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0476 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0465 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0467 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0467 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0487 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0490 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0473 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0463 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0465 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0462 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0451 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0452 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0442 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0431 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0422 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0425 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0414 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0415 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0400 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0387 - accuracy: 0.1995"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0385 - accuracy: 0.19 - ETA: 2:16 - loss: 3.0386 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0378 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0374 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0377 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0362 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0356 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0351 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0347 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0338 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0329 - accuracy: 0.20 - ETA: 2:07 - loss: 3.0328 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0330 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0335 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0328 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0327 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0316 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0309 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0307 - accuracy: 0.20 - ETA: 1:59 - loss: 3.0299 - accuracy: 0.20 - ETA: 1:58 - loss: 3.0309 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0313 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0321 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0327 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0335 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0333 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0345 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0338 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0341 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0348 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0346 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0341 - accuracy: 0.20 - ETA: 1:47 - loss: 3.0350 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0351 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0344 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0350 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0348 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0350 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0359 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0361 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0355 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0349 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0351 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0352 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0354 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0346 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0337 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0330 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0324 - accuracy: 0.20 - ETA: 1:30 - loss: 3.0336 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0340 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0334 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0332 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0330 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0329 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0333 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0325 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0317 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0314 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0310 - accuracy: 0.20 - ETA: 1:20 - loss: 3.0308 - accuracy: 0.20 - ETA: 1:19 - loss: 3.0313 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0305 - accuracy: 0.20 - ETA: 1:17 - loss: 3.0300 - accuracy: 0.20 - ETA: 1:16 - loss: 3.0307 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0309 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0304 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0303 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0302 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0294 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0294 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0292 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0293 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0291 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0289 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0286 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0294 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0287 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0291 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0296 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0294 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0293 - accuracy: 0.19 - ETA: 59s - loss: 3.0287 - accuracy: 0.1998 - ETA: 58s - loss: 3.0281 - accuracy: 0.200 - ETA: 57s - loss: 3.0280 - accuracy: 0.200 - ETA: 56s - loss: 3.0282 - accuracy: 0.200 - ETA: 55s - loss: 3.0290 - accuracy: 0.200 - ETA: 54s - loss: 3.0295 - accuracy: 0.199 - ETA: 53s - loss: 3.0297 - accuracy: 0.199 - ETA: 52s - loss: 3.0298 - accuracy: 0.200 - ETA: 51s - loss: 3.0300 - accuracy: 0.199 - ETA: 50s - loss: 3.0306 - accuracy: 0.199 - ETA: 49s - loss: 3.0308 - accuracy: 0.199 - ETA: 48s - loss: 3.0306 - accuracy: 0.199 - ETA: 47s - loss: 3.0306 - accuracy: 0.199 - ETA: 46s - loss: 3.0311 - accuracy: 0.199 - ETA: 45s - loss: 3.0305 - accuracy: 0.199 - ETA: 44s - loss: 3.0302 - accuracy: 0.199 - ETA: 44s - loss: 3.0303 - accuracy: 0.199 - ETA: 43s - loss: 3.0304 - accuracy: 0.199 - ETA: 42s - loss: 3.0302 - accuracy: 0.199 - ETA: 41s - loss: 3.0298 - accuracy: 0.199 - ETA: 40s - loss: 3.0298 - accuracy: 0.199 - ETA: 39s - loss: 3.0303 - accuracy: 0.199 - ETA: 38s - loss: 3.0295 - accuracy: 0.199 - ETA: 37s - loss: 3.0296 - accuracy: 0.199 - ETA: 36s - loss: 3.0300 - accuracy: 0.199 - ETA: 35s - loss: 3.0304 - accuracy: 0.199 - ETA: 34s - loss: 3.0302 - accuracy: 0.199 - ETA: 33s - loss: 3.0302 - accuracy: 0.199 - ETA: 32s - loss: 3.0298 - accuracy: 0.199 - ETA: 31s - loss: 3.0295 - accuracy: 0.199 - ETA: 30s - loss: 3.0298 - accuracy: 0.199 - ETA: 29s - loss: 3.0288 - accuracy: 0.199 - ETA: 28s - loss: 3.0288 - accuracy: 0.199 - ETA: 27s - loss: 3.0293 - accuracy: 0.199 - ETA: 26s - loss: 3.0296 - accuracy: 0.199 - ETA: 25s - loss: 3.0295 - accuracy: 0.199 - ETA: 24s - loss: 3.0298 - accuracy: 0.199 - ETA: 23s - loss: 3.0294 - accuracy: 0.199 - ETA: 22s - loss: 3.0301 - accuracy: 0.199 - ETA: 22s - loss: 3.0299 - accuracy: 0.199 - ETA: 21s - loss: 3.0302 - accuracy: 0.199 - ETA: 20s - loss: 3.0310 - accuracy: 0.199 - ETA: 19s - loss: 3.0312 - accuracy: 0.199 - ETA: 18s - loss: 3.0310 - accuracy: 0.199 - ETA: 17s - loss: 3.0308 - accuracy: 0.198 - ETA: 16s - loss: 3.0308 - accuracy: 0.198 - ETA: 15s - loss: 3.0303 - accuracy: 0.198 - ETA: 14s - loss: 3.0300 - accuracy: 0.199 - ETA: 13s - loss: 3.0301 - accuracy: 0.199 - ETA: 12s - loss: 3.0299 - accuracy: 0.199 - ETA: 11s - loss: 3.0298 - accuracy: 0.199 - ETA: 10s - loss: 3.0292 - accuracy: 0.199 - ETA: 9s - loss: 3.0292 - accuracy: 0.199 - ETA: 8s - loss: 3.0290 - accuracy: 0.19 - ETA: 7s - loss: 3.0289 - accuracy: 0.19 - ETA: 6s - loss: 3.0289 - accuracy: 0.19 - ETA: 5s - loss: 3.0292 - accuracy: 0.19 - ETA: 4s - loss: 3.0293 - accuracy: 0.19 - ETA: 3s - loss: 3.0297 - accuracy: 0.19 - ETA: 2s - loss: 3.0293 - accuracy: 0.19 - ETA: 1s - loss: 3.0293 - accuracy: 0.19 - ETA: 1s - loss: 3.0290 - accuracy: 0.19 - ETA: 0s - loss: 3.0287 - accuracy: 0.19 - 339s 8ms/step - loss: 3.0289 - accuracy: 0.1988 - val_loss: 4.2488 - val_accuracy: 0.0386\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:28 - loss: 3.0187 - accuracy: 0.19 - ETA: 5:27 - loss: 2.9599 - accuracy: 0.22 - ETA: 5:18 - loss: 2.9196 - accuracy: 0.23 - ETA: 5:13 - loss: 2.9234 - accuracy: 0.24 - ETA: 5:11 - loss: 2.8762 - accuracy: 0.24 - ETA: 5:10 - loss: 2.9231 - accuracy: 0.22 - ETA: 5:08 - loss: 2.9039 - accuracy: 0.23 - ETA: 5:06 - loss: 2.9046 - accuracy: 0.23 - ETA: 5:04 - loss: 2.9117 - accuracy: 0.23 - ETA: 5:04 - loss: 2.9104 - accuracy: 0.23 - ETA: 5:04 - loss: 2.9111 - accuracy: 0.22 - ETA: 5:02 - loss: 2.9063 - accuracy: 0.22 - ETA: 5:02 - loss: 2.9077 - accuracy: 0.22 - ETA: 5:00 - loss: 2.8991 - accuracy: 0.22 - ETA: 5:00 - loss: 2.9107 - accuracy: 0.22 - ETA: 4:58 - loss: 2.9160 - accuracy: 0.22 - ETA: 4:57 - loss: 2.9171 - accuracy: 0.22 - ETA: 4:57 - loss: 2.9361 - accuracy: 0.21 - ETA: 4:55 - loss: 2.9336 - accuracy: 0.21 - ETA: 4:54 - loss: 2.9479 - accuracy: 0.21 - ETA: 4:53 - loss: 2.9447 - accuracy: 0.21 - ETA: 4:52 - loss: 2.9460 - accuracy: 0.21 - ETA: 4:51 - loss: 2.9450 - accuracy: 0.21 - ETA: 4:50 - loss: 2.9546 - accuracy: 0.20 - ETA: 4:48 - loss: 2.9580 - accuracy: 0.20 - ETA: 4:47 - loss: 2.9608 - accuracy: 0.20 - ETA: 4:47 - loss: 2.9622 - accuracy: 0.20 - ETA: 4:46 - loss: 2.9604 - accuracy: 0.20 - ETA: 4:45 - loss: 2.9681 - accuracy: 0.20 - ETA: 4:44 - loss: 2.9711 - accuracy: 0.20 - ETA: 4:43 - loss: 2.9784 - accuracy: 0.20 - ETA: 4:41 - loss: 2.9819 - accuracy: 0.20 - ETA: 4:40 - loss: 2.9884 - accuracy: 0.20 - ETA: 4:39 - loss: 2.9850 - accuracy: 0.20 - ETA: 4:37 - loss: 2.9846 - accuracy: 0.20 - ETA: 4:37 - loss: 2.9848 - accuracy: 0.20 - ETA: 4:36 - loss: 2.9809 - accuracy: 0.20 - ETA: 4:35 - loss: 2.9815 - accuracy: 0.20 - ETA: 4:34 - loss: 2.9811 - accuracy: 0.20 - ETA: 4:33 - loss: 2.9807 - accuracy: 0.20 - ETA: 4:32 - loss: 2.9734 - accuracy: 0.20 - ETA: 4:31 - loss: 2.9696 - accuracy: 0.20 - ETA: 4:30 - loss: 2.9718 - accuracy: 0.20 - ETA: 4:30 - loss: 2.9696 - accuracy: 0.20 - ETA: 4:29 - loss: 2.9685 - accuracy: 0.20 - ETA: 4:29 - loss: 2.9667 - accuracy: 0.20 - ETA: 4:28 - loss: 2.9627 - accuracy: 0.20 - ETA: 4:27 - loss: 2.9618 - accuracy: 0.20 - ETA: 4:27 - loss: 2.9629 - accuracy: 0.20 - ETA: 4:26 - loss: 2.9644 - accuracy: 0.20 - ETA: 4:25 - loss: 2.9698 - accuracy: 0.20 - ETA: 4:24 - loss: 2.9720 - accuracy: 0.20 - ETA: 4:23 - loss: 2.9746 - accuracy: 0.20 - ETA: 4:22 - loss: 2.9772 - accuracy: 0.20 - ETA: 4:21 - loss: 2.9771 - accuracy: 0.20 - ETA: 4:20 - loss: 2.9786 - accuracy: 0.20 - ETA: 4:19 - loss: 2.9794 - accuracy: 0.20 - ETA: 4:18 - loss: 2.9811 - accuracy: 0.20 - ETA: 4:17 - loss: 2.9809 - accuracy: 0.20 - ETA: 4:16 - loss: 2.9815 - accuracy: 0.20 - ETA: 4:15 - loss: 2.9845 - accuracy: 0.20 - ETA: 4:14 - loss: 2.9860 - accuracy: 0.20 - ETA: 4:13 - loss: 2.9860 - accuracy: 0.20 - ETA: 4:12 - loss: 2.9836 - accuracy: 0.20 - ETA: 4:11 - loss: 2.9797 - accuracy: 0.20 - ETA: 4:10 - loss: 2.9849 - accuracy: 0.20 - ETA: 4:09 - loss: 2.9841 - accuracy: 0.20 - ETA: 4:08 - loss: 2.9836 - accuracy: 0.20 - ETA: 4:07 - loss: 2.9828 - accuracy: 0.20 - ETA: 4:07 - loss: 2.9863 - accuracy: 0.20 - ETA: 4:06 - loss: 2.9867 - accuracy: 0.20 - ETA: 4:05 - loss: 2.9830 - accuracy: 0.20 - ETA: 4:04 - loss: 2.9822 - accuracy: 0.20 - ETA: 4:03 - loss: 2.9808 - accuracy: 0.20 - ETA: 4:02 - loss: 2.9814 - accuracy: 0.20 - ETA: 4:01 - loss: 2.9775 - accuracy: 0.20 - ETA: 4:00 - loss: 2.9756 - accuracy: 0.20 - ETA: 3:59 - loss: 2.9763 - accuracy: 0.20 - ETA: 3:58 - loss: 2.9745 - accuracy: 0.20 - ETA: 3:57 - loss: 2.9723 - accuracy: 0.20 - ETA: 3:56 - loss: 2.9692 - accuracy: 0.20 - ETA: 3:55 - loss: 2.9689 - accuracy: 0.20 - ETA: 3:54 - loss: 2.9681 - accuracy: 0.20 - ETA: 3:53 - loss: 2.9679 - accuracy: 0.20 - ETA: 3:52 - loss: 2.9691 - accuracy: 0.20 - ETA: 3:51 - loss: 2.9684 - accuracy: 0.20 - ETA: 3:50 - loss: 2.9681 - accuracy: 0.20 - ETA: 3:49 - loss: 2.9660 - accuracy: 0.20 - ETA: 3:48 - loss: 2.9697 - accuracy: 0.20 - ETA: 3:47 - loss: 2.9702 - accuracy: 0.20 - ETA: 3:46 - loss: 2.9710 - accuracy: 0.20 - ETA: 3:46 - loss: 2.9716 - accuracy: 0.20 - ETA: 3:45 - loss: 2.9734 - accuracy: 0.20 - ETA: 3:44 - loss: 2.9731 - accuracy: 0.20 - ETA: 3:43 - loss: 2.9750 - accuracy: 0.20 - ETA: 3:42 - loss: 2.9733 - accuracy: 0.20 - ETA: 3:41 - loss: 2.9744 - accuracy: 0.20 - ETA: 3:40 - loss: 2.9771 - accuracy: 0.20 - ETA: 3:39 - loss: 2.9771 - accuracy: 0.20 - ETA: 3:38 - loss: 2.9772 - accuracy: 0.20 - ETA: 3:37 - loss: 2.9764 - accuracy: 0.20 - ETA: 3:36 - loss: 2.9770 - accuracy: 0.20 - ETA: 3:35 - loss: 2.9784 - accuracy: 0.20 - ETA: 3:34 - loss: 2.9805 - accuracy: 0.20 - ETA: 3:33 - loss: 2.9795 - accuracy: 0.20 - ETA: 3:32 - loss: 2.9784 - accuracy: 0.20 - ETA: 3:31 - loss: 2.9779 - accuracy: 0.20 - ETA: 3:30 - loss: 2.9764 - accuracy: 0.20 - ETA: 3:29 - loss: 2.9779 - accuracy: 0.20 - ETA: 3:28 - loss: 2.9764 - accuracy: 0.20 - ETA: 3:27 - loss: 2.9773 - accuracy: 0.20 - ETA: 3:27 - loss: 2.9781 - accuracy: 0.20 - ETA: 3:26 - loss: 2.9785 - accuracy: 0.20 - ETA: 3:25 - loss: 2.9798 - accuracy: 0.20 - ETA: 3:24 - loss: 2.9803 - accuracy: 0.20 - ETA: 3:23 - loss: 2.9798 - accuracy: 0.20 - ETA: 3:22 - loss: 2.9805 - accuracy: 0.20 - ETA: 3:21 - loss: 2.9810 - accuracy: 0.20 - ETA: 3:20 - loss: 2.9828 - accuracy: 0.20 - ETA: 3:19 - loss: 2.9833 - accuracy: 0.20 - ETA: 3:18 - loss: 2.9848 - accuracy: 0.20 - ETA: 3:18 - loss: 2.9834 - accuracy: 0.20 - ETA: 3:17 - loss: 2.9839 - accuracy: 0.20 - ETA: 3:16 - loss: 2.9843 - accuracy: 0.20 - ETA: 3:15 - loss: 2.9844 - accuracy: 0.20 - ETA: 3:14 - loss: 2.9839 - accuracy: 0.20 - ETA: 3:13 - loss: 2.9848 - accuracy: 0.20 - ETA: 3:12 - loss: 2.9853 - accuracy: 0.20 - ETA: 3:11 - loss: 2.9850 - accuracy: 0.20 - ETA: 3:10 - loss: 2.9849 - accuracy: 0.20 - ETA: 3:09 - loss: 2.9869 - accuracy: 0.20 - ETA: 3:08 - loss: 2.9863 - accuracy: 0.20 - ETA: 3:07 - loss: 2.9866 - accuracy: 0.20 - ETA: 3:06 - loss: 2.9870 - accuracy: 0.20 - ETA: 3:05 - loss: 2.9851 - accuracy: 0.20 - ETA: 3:04 - loss: 2.9856 - accuracy: 0.20 - ETA: 3:03 - loss: 2.9854 - accuracy: 0.20 - ETA: 3:02 - loss: 2.9844 - accuracy: 0.20 - ETA: 3:01 - loss: 2.9841 - accuracy: 0.20 - ETA: 3:00 - loss: 2.9849 - accuracy: 0.20 - ETA: 2:59 - loss: 2.9844 - accuracy: 0.20 - ETA: 2:58 - loss: 2.9844 - accuracy: 0.20 - ETA: 2:57 - loss: 2.9846 - accuracy: 0.20 - ETA: 2:57 - loss: 2.9839 - accuracy: 0.20 - ETA: 2:56 - loss: 2.9850 - accuracy: 0.20 - ETA: 2:55 - loss: 2.9857 - accuracy: 0.20 - ETA: 2:54 - loss: 2.9862 - accuracy: 0.20 - ETA: 2:53 - loss: 2.9870 - accuracy: 0.20 - ETA: 2:52 - loss: 2.9854 - accuracy: 0.20 - ETA: 2:51 - loss: 2.9868 - accuracy: 0.20 - ETA: 2:50 - loss: 2.9872 - accuracy: 0.20 - ETA: 2:49 - loss: 2.9868 - accuracy: 0.20 - ETA: 2:48 - loss: 2.9879 - accuracy: 0.20 - ETA: 2:47 - loss: 2.9896 - accuracy: 0.20 - ETA: 2:46 - loss: 2.9882 - accuracy: 0.20 - ETA: 2:45 - loss: 2.9898 - accuracy: 0.20 - ETA: 2:44 - loss: 2.9906 - accuracy: 0.20 - ETA: 2:43 - loss: 2.9901 - accuracy: 0.20 - ETA: 2:43 - loss: 2.9915 - accuracy: 0.20 - ETA: 2:42 - loss: 2.9918 - accuracy: 0.20 - ETA: 2:41 - loss: 2.9917 - accuracy: 0.20 - ETA: 2:40 - loss: 2.9923 - accuracy: 0.20 - ETA: 2:39 - loss: 2.9933 - accuracy: 0.20 - ETA: 2:38 - loss: 2.9945 - accuracy: 0.20 - ETA: 2:37 - loss: 2.9942 - accuracy: 0.20 - ETA: 2:36 - loss: 2.9939 - accuracy: 0.20 - ETA: 2:35 - loss: 2.9946 - accuracy: 0.20 - ETA: 2:34 - loss: 2.9953 - accuracy: 0.20 - ETA: 2:33 - loss: 2.9957 - accuracy: 0.20 - ETA: 2:32 - loss: 2.9952 - accuracy: 0.20 - ETA: 2:31 - loss: 2.9967 - accuracy: 0.20 - ETA: 2:30 - loss: 2.9974 - accuracy: 0.20 - ETA: 2:29 - loss: 2.9973 - accuracy: 0.20 - ETA: 2:28 - loss: 2.9979 - accuracy: 0.20 - ETA: 2:27 - loss: 2.9981 - accuracy: 0.20 - ETA: 2:26 - loss: 2.9985 - accuracy: 0.20 - ETA: 2:25 - loss: 2.9988 - accuracy: 0.20 - ETA: 2:24 - loss: 2.9992 - accuracy: 0.20 - ETA: 2:23 - loss: 2.9997 - accuracy: 0.20 - ETA: 2:23 - loss: 2.9999 - accuracy: 0.20 - ETA: 2:22 - loss: 3.0000 - accuracy: 0.20 - ETA: 2:21 - loss: 3.0008 - accuracy: 0.20 - ETA: 2:20 - loss: 3.0013 - accuracy: 0.20 - ETA: 2:19 - loss: 3.0014 - accuracy: 0.20 - ETA: 2:18 - loss: 3.0020 - accuracy: 0.20 - ETA: 2:17 - loss: 3.0022 - accuracy: 0.2031"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0027 - accuracy: 0.20 - ETA: 2:15 - loss: 3.0028 - accuracy: 0.20 - ETA: 2:14 - loss: 3.0037 - accuracy: 0.20 - ETA: 2:13 - loss: 3.0038 - accuracy: 0.20 - ETA: 2:12 - loss: 3.0046 - accuracy: 0.20 - ETA: 2:11 - loss: 3.0043 - accuracy: 0.20 - ETA: 2:11 - loss: 3.0054 - accuracy: 0.20 - ETA: 2:10 - loss: 3.0055 - accuracy: 0.20 - ETA: 2:09 - loss: 3.0055 - accuracy: 0.20 - ETA: 2:08 - loss: 3.0050 - accuracy: 0.20 - ETA: 2:07 - loss: 3.0054 - accuracy: 0.20 - ETA: 2:06 - loss: 3.0061 - accuracy: 0.20 - ETA: 2:05 - loss: 3.0056 - accuracy: 0.20 - ETA: 2:04 - loss: 3.0049 - accuracy: 0.20 - ETA: 2:03 - loss: 3.0041 - accuracy: 0.20 - ETA: 2:02 - loss: 3.0048 - accuracy: 0.20 - ETA: 2:01 - loss: 3.0049 - accuracy: 0.20 - ETA: 2:00 - loss: 3.0041 - accuracy: 0.20 - ETA: 1:59 - loss: 3.0046 - accuracy: 0.20 - ETA: 1:58 - loss: 3.0049 - accuracy: 0.20 - ETA: 1:57 - loss: 3.0057 - accuracy: 0.20 - ETA: 1:56 - loss: 3.0062 - accuracy: 0.20 - ETA: 1:55 - loss: 3.0053 - accuracy: 0.20 - ETA: 1:54 - loss: 3.0060 - accuracy: 0.20 - ETA: 1:53 - loss: 3.0058 - accuracy: 0.20 - ETA: 1:52 - loss: 3.0050 - accuracy: 0.20 - ETA: 1:51 - loss: 3.0054 - accuracy: 0.20 - ETA: 1:51 - loss: 3.0054 - accuracy: 0.20 - ETA: 1:50 - loss: 3.0058 - accuracy: 0.20 - ETA: 1:49 - loss: 3.0054 - accuracy: 0.20 - ETA: 1:48 - loss: 3.0052 - accuracy: 0.20 - ETA: 1:47 - loss: 3.0050 - accuracy: 0.20 - ETA: 1:46 - loss: 3.0055 - accuracy: 0.20 - ETA: 1:45 - loss: 3.0055 - accuracy: 0.20 - ETA: 1:44 - loss: 3.0054 - accuracy: 0.20 - ETA: 1:43 - loss: 3.0055 - accuracy: 0.20 - ETA: 1:42 - loss: 3.0060 - accuracy: 0.20 - ETA: 1:41 - loss: 3.0062 - accuracy: 0.20 - ETA: 1:40 - loss: 3.0055 - accuracy: 0.20 - ETA: 1:39 - loss: 3.0046 - accuracy: 0.20 - ETA: 1:38 - loss: 3.0040 - accuracy: 0.20 - ETA: 1:37 - loss: 3.0048 - accuracy: 0.20 - ETA: 1:36 - loss: 3.0041 - accuracy: 0.20 - ETA: 1:35 - loss: 3.0038 - accuracy: 0.20 - ETA: 1:34 - loss: 3.0046 - accuracy: 0.20 - ETA: 1:33 - loss: 3.0055 - accuracy: 0.20 - ETA: 1:33 - loss: 3.0056 - accuracy: 0.20 - ETA: 1:32 - loss: 3.0060 - accuracy: 0.20 - ETA: 1:31 - loss: 3.0055 - accuracy: 0.20 - ETA: 1:30 - loss: 3.0060 - accuracy: 0.20 - ETA: 1:29 - loss: 3.0055 - accuracy: 0.20 - ETA: 1:28 - loss: 3.0054 - accuracy: 0.20 - ETA: 1:27 - loss: 3.0055 - accuracy: 0.20 - ETA: 1:26 - loss: 3.0057 - accuracy: 0.20 - ETA: 1:25 - loss: 3.0068 - accuracy: 0.20 - ETA: 1:24 - loss: 3.0065 - accuracy: 0.20 - ETA: 1:23 - loss: 3.0059 - accuracy: 0.20 - ETA: 1:22 - loss: 3.0064 - accuracy: 0.20 - ETA: 1:21 - loss: 3.0067 - accuracy: 0.20 - ETA: 1:20 - loss: 3.0071 - accuracy: 0.20 - ETA: 1:19 - loss: 3.0069 - accuracy: 0.20 - ETA: 1:18 - loss: 3.0078 - accuracy: 0.20 - ETA: 1:17 - loss: 3.0077 - accuracy: 0.20 - ETA: 1:16 - loss: 3.0079 - accuracy: 0.20 - ETA: 1:15 - loss: 3.0085 - accuracy: 0.20 - ETA: 1:15 - loss: 3.0087 - accuracy: 0.20 - ETA: 1:14 - loss: 3.0095 - accuracy: 0.20 - ETA: 1:13 - loss: 3.0103 - accuracy: 0.20 - ETA: 1:12 - loss: 3.0117 - accuracy: 0.20 - ETA: 1:11 - loss: 3.0111 - accuracy: 0.20 - ETA: 1:10 - loss: 3.0118 - accuracy: 0.20 - ETA: 1:09 - loss: 3.0116 - accuracy: 0.20 - ETA: 1:08 - loss: 3.0112 - accuracy: 0.20 - ETA: 1:07 - loss: 3.0108 - accuracy: 0.20 - ETA: 1:06 - loss: 3.0109 - accuracy: 0.20 - ETA: 1:05 - loss: 3.0112 - accuracy: 0.20 - ETA: 1:04 - loss: 3.0121 - accuracy: 0.20 - ETA: 1:03 - loss: 3.0119 - accuracy: 0.20 - ETA: 1:02 - loss: 3.0121 - accuracy: 0.20 - ETA: 1:01 - loss: 3.0119 - accuracy: 0.20 - ETA: 1:00 - loss: 3.0117 - accuracy: 0.20 - ETA: 59s - loss: 3.0121 - accuracy: 0.2012 - ETA: 58s - loss: 3.0118 - accuracy: 0.201 - ETA: 58s - loss: 3.0166 - accuracy: 0.201 - ETA: 57s - loss: 3.0165 - accuracy: 0.201 - ETA: 56s - loss: 3.0163 - accuracy: 0.201 - ETA: 55s - loss: 3.0167 - accuracy: 0.201 - ETA: 54s - loss: 3.0171 - accuracy: 0.201 - ETA: 53s - loss: 3.0169 - accuracy: 0.201 - ETA: 52s - loss: 3.0171 - accuracy: 0.201 - ETA: 51s - loss: 3.0178 - accuracy: 0.201 - ETA: 50s - loss: 3.0186 - accuracy: 0.200 - ETA: 49s - loss: 3.0187 - accuracy: 0.200 - ETA: 48s - loss: 3.0185 - accuracy: 0.200 - ETA: 47s - loss: 3.0190 - accuracy: 0.200 - ETA: 46s - loss: 3.0198 - accuracy: 0.200 - ETA: 45s - loss: 3.0206 - accuracy: 0.200 - ETA: 44s - loss: 3.0205 - accuracy: 0.200 - ETA: 43s - loss: 3.0208 - accuracy: 0.200 - ETA: 42s - loss: 3.0209 - accuracy: 0.200 - ETA: 41s - loss: 3.0204 - accuracy: 0.200 - ETA: 40s - loss: 3.0213 - accuracy: 0.200 - ETA: 39s - loss: 3.0222 - accuracy: 0.199 - ETA: 39s - loss: 3.0229 - accuracy: 0.199 - ETA: 38s - loss: 3.0236 - accuracy: 0.199 - ETA: 37s - loss: 3.0235 - accuracy: 0.199 - ETA: 36s - loss: 3.0250 - accuracy: 0.199 - ETA: 35s - loss: 3.0254 - accuracy: 0.199 - ETA: 34s - loss: 3.0251 - accuracy: 0.199 - ETA: 33s - loss: 3.0247 - accuracy: 0.199 - ETA: 32s - loss: 3.0242 - accuracy: 0.199 - ETA: 31s - loss: 3.0241 - accuracy: 0.199 - ETA: 30s - loss: 3.0243 - accuracy: 0.199 - ETA: 29s - loss: 3.0250 - accuracy: 0.199 - ETA: 28s - loss: 3.0250 - accuracy: 0.199 - ETA: 27s - loss: 3.0245 - accuracy: 0.199 - ETA: 26s - loss: 3.0238 - accuracy: 0.199 - ETA: 25s - loss: 3.0234 - accuracy: 0.199 - ETA: 24s - loss: 3.0237 - accuracy: 0.199 - ETA: 23s - loss: 3.0236 - accuracy: 0.199 - ETA: 22s - loss: 3.0242 - accuracy: 0.199 - ETA: 21s - loss: 3.0244 - accuracy: 0.199 - ETA: 20s - loss: 3.0242 - accuracy: 0.199 - ETA: 20s - loss: 3.0239 - accuracy: 0.199 - ETA: 19s - loss: 3.0237 - accuracy: 0.199 - ETA: 18s - loss: 3.0233 - accuracy: 0.199 - ETA: 17s - loss: 3.0239 - accuracy: 0.199 - ETA: 16s - loss: 3.0237 - accuracy: 0.199 - ETA: 15s - loss: 3.0241 - accuracy: 0.199 - ETA: 14s - loss: 3.0230 - accuracy: 0.199 - ETA: 13s - loss: 3.0222 - accuracy: 0.199 - ETA: 12s - loss: 3.0214 - accuracy: 0.199 - ETA: 11s - loss: 3.0215 - accuracy: 0.199 - ETA: 10s - loss: 3.0212 - accuracy: 0.199 - ETA: 9s - loss: 3.0209 - accuracy: 0.199 - ETA: 8s - loss: 3.0211 - accuracy: 0.19 - ETA: 7s - loss: 3.0211 - accuracy: 0.19 - ETA: 6s - loss: 3.0213 - accuracy: 0.19 - ETA: 5s - loss: 3.0213 - accuracy: 0.19 - ETA: 4s - loss: 3.0206 - accuracy: 0.20 - ETA: 3s - loss: 3.0206 - accuracy: 0.20 - ETA: 2s - loss: 3.0206 - accuracy: 0.20 - ETA: 1s - loss: 3.0205 - accuracy: 0.20 - ETA: 1s - loss: 3.0200 - accuracy: 0.20 - ETA: 0s - loss: 3.0195 - accuracy: 0.20 - 338s 8ms/step - loss: 3.0196 - accuracy: 0.2006 - val_loss: 4.3408 - val_accuracy: 0.0385\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:08 - loss: 2.9112 - accuracy: 0.22 - ETA: 5:05 - loss: 2.9596 - accuracy: 0.23 - ETA: 5:07 - loss: 3.0243 - accuracy: 0.20 - ETA: 5:02 - loss: 3.0233 - accuracy: 0.20 - ETA: 5:03 - loss: 3.0246 - accuracy: 0.19 - ETA: 5:02 - loss: 3.0112 - accuracy: 0.20 - ETA: 5:02 - loss: 2.9989 - accuracy: 0.20 - ETA: 5:03 - loss: 3.0002 - accuracy: 0.20 - ETA: 5:01 - loss: 2.9901 - accuracy: 0.20 - ETA: 5:01 - loss: 2.9872 - accuracy: 0.20 - ETA: 5:00 - loss: 3.0260 - accuracy: 0.20 - ETA: 4:59 - loss: 3.0174 - accuracy: 0.20 - ETA: 4:58 - loss: 3.0065 - accuracy: 0.20 - ETA: 4:58 - loss: 2.9989 - accuracy: 0.20 - ETA: 4:57 - loss: 3.0008 - accuracy: 0.20 - ETA: 4:56 - loss: 3.0170 - accuracy: 0.20 - ETA: 4:55 - loss: 3.0247 - accuracy: 0.20 - ETA: 4:54 - loss: 3.0343 - accuracy: 0.20 - ETA: 4:53 - loss: 3.0310 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0378 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0371 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0288 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0329 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0369 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0381 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0419 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0438 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0441 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0466 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0478 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0394 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0348 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0287 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0308 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0290 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0277 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0323 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0289 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0294 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0286 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0286 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0428 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0463 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0448 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0434 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0471 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0445 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0423 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0447 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0460 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0496 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0454 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0447 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0436 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0457 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0486 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0501 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0484 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0496 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0524 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0549 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0532 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0493 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0501 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0544 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0532 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0538 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0568 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0554 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0575 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0601 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0602 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0608 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0616 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0619 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0636 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0640 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0612 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0603 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0641 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0626 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0622 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0581 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0579 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0589 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0601 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0591 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0600 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0572 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0588 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0602 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0609 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0609 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0591 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0577 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0562 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0594 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0589 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0569 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0548 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0553 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0554 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0551 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0538 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0528 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0527 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0535 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0560 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0539 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0528 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0534 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0535 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0510 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0526 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0508 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0505 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0481 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0483 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0460 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0452 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0443 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0434 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0427 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0425 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0411 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0412 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0405 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0413 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0398 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0408 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0399 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0395 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0391 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0374 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0386 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0390 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0400 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0402 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0385 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0402 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0384 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0387 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0394 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0396 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0400 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0413 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0411 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0422 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0435 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0452 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0457 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0445 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0438 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0434 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0435 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0454 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0463 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0458 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0452 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0466 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0473 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0471 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0474 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0460 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0462 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0456 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0451 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0449 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0442 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0448 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0436 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0436 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0429 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0429 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0436 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0435 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0427 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0424 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0431 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0412 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0415 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0404 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0408 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0400 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0397 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0393 - accuracy: 0.1983"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0393 - accuracy: 0.19 - ETA: 2:16 - loss: 3.0394 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0397 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0413 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0423 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0410 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0414 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0420 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0402 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0399 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0398 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0391 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0381 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0386 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0378 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0374 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0367 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0365 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0356 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0346 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0339 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0338 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0336 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0339 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0319 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0325 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0321 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0329 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0326 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0323 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0321 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0328 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0326 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0326 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0327 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0321 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0316 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0316 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0324 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0321 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0310 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0307 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0302 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0289 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0283 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0278 - accuracy: 0.20 - ETA: 1:33 - loss: 3.0279 - accuracy: 0.20 - ETA: 1:32 - loss: 3.0274 - accuracy: 0.20 - ETA: 1:31 - loss: 3.0275 - accuracy: 0.20 - ETA: 1:30 - loss: 3.0275 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0272 - accuracy: 0.20 - ETA: 1:28 - loss: 3.0286 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0272 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0271 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0266 - accuracy: 0.20 - ETA: 1:24 - loss: 3.0266 - accuracy: 0.20 - ETA: 1:23 - loss: 3.0269 - accuracy: 0.20 - ETA: 1:22 - loss: 3.0276 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0280 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0286 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0285 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0280 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0280 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0287 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0281 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0279 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0275 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0271 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0281 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0287 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0286 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0295 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0292 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0299 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0302 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0303 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0297 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0289 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0287 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0289 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0294 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0280 - accuracy: 0.19 - ETA: 59s - loss: 3.0285 - accuracy: 0.1992 - ETA: 58s - loss: 3.0293 - accuracy: 0.199 - ETA: 57s - loss: 3.0294 - accuracy: 0.199 - ETA: 56s - loss: 3.0294 - accuracy: 0.199 - ETA: 55s - loss: 3.0288 - accuracy: 0.199 - ETA: 54s - loss: 3.0289 - accuracy: 0.199 - ETA: 53s - loss: 3.0284 - accuracy: 0.199 - ETA: 52s - loss: 3.0292 - accuracy: 0.199 - ETA: 51s - loss: 3.0293 - accuracy: 0.199 - ETA: 50s - loss: 3.0300 - accuracy: 0.199 - ETA: 49s - loss: 3.0300 - accuracy: 0.199 - ETA: 48s - loss: 3.0301 - accuracy: 0.199 - ETA: 47s - loss: 3.0306 - accuracy: 0.199 - ETA: 46s - loss: 3.0299 - accuracy: 0.199 - ETA: 45s - loss: 3.0299 - accuracy: 0.199 - ETA: 44s - loss: 3.0301 - accuracy: 0.199 - ETA: 43s - loss: 3.0297 - accuracy: 0.199 - ETA: 42s - loss: 3.0296 - accuracy: 0.199 - ETA: 41s - loss: 3.0297 - accuracy: 0.199 - ETA: 41s - loss: 3.0292 - accuracy: 0.200 - ETA: 40s - loss: 3.0290 - accuracy: 0.199 - ETA: 39s - loss: 3.0293 - accuracy: 0.199 - ETA: 38s - loss: 3.0296 - accuracy: 0.199 - ETA: 37s - loss: 3.0302 - accuracy: 0.199 - ETA: 36s - loss: 3.0298 - accuracy: 0.200 - ETA: 35s - loss: 3.0301 - accuracy: 0.200 - ETA: 34s - loss: 3.0300 - accuracy: 0.199 - ETA: 33s - loss: 3.0300 - accuracy: 0.199 - ETA: 32s - loss: 3.0302 - accuracy: 0.199 - ETA: 31s - loss: 3.0301 - accuracy: 0.199 - ETA: 30s - loss: 3.0303 - accuracy: 0.199 - ETA: 29s - loss: 3.0298 - accuracy: 0.200 - ETA: 28s - loss: 3.0302 - accuracy: 0.200 - ETA: 27s - loss: 3.0306 - accuracy: 0.199 - ETA: 26s - loss: 3.0307 - accuracy: 0.199 - ETA: 25s - loss: 3.0313 - accuracy: 0.199 - ETA: 24s - loss: 3.0322 - accuracy: 0.199 - ETA: 23s - loss: 3.0326 - accuracy: 0.199 - ETA: 22s - loss: 3.0323 - accuracy: 0.199 - ETA: 21s - loss: 3.0319 - accuracy: 0.199 - ETA: 21s - loss: 3.0321 - accuracy: 0.199 - ETA: 20s - loss: 3.0320 - accuracy: 0.199 - ETA: 19s - loss: 3.0318 - accuracy: 0.199 - ETA: 18s - loss: 3.0326 - accuracy: 0.199 - ETA: 17s - loss: 3.0318 - accuracy: 0.199 - ETA: 16s - loss: 3.0318 - accuracy: 0.199 - ETA: 15s - loss: 3.0320 - accuracy: 0.199 - ETA: 14s - loss: 3.0319 - accuracy: 0.199 - ETA: 13s - loss: 3.0315 - accuracy: 0.199 - ETA: 12s - loss: 3.0313 - accuracy: 0.199 - ETA: 11s - loss: 3.0318 - accuracy: 0.199 - ETA: 10s - loss: 3.0320 - accuracy: 0.199 - ETA: 9s - loss: 3.0321 - accuracy: 0.199 - ETA: 8s - loss: 3.0324 - accuracy: 0.19 - ETA: 7s - loss: 3.0324 - accuracy: 0.19 - ETA: 6s - loss: 3.0330 - accuracy: 0.19 - ETA: 5s - loss: 3.0331 - accuracy: 0.19 - ETA: 4s - loss: 3.0328 - accuracy: 0.19 - ETA: 3s - loss: 3.0333 - accuracy: 0.19 - ETA: 2s - loss: 3.0339 - accuracy: 0.19 - ETA: 1s - loss: 3.0342 - accuracy: 0.19 - ETA: 1s - loss: 3.0340 - accuracy: 0.19 - ETA: 0s - loss: 3.0333 - accuracy: 0.19 - 338s 8ms/step - loss: 3.0332 - accuracy: 0.1985 - val_loss: 4.5089 - val_accuracy: 0.0382\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:19 - loss: 3.0476 - accuracy: 0.22 - ETA: 5:09 - loss: 3.0995 - accuracy: 0.21 - ETA: 5:08 - loss: 3.0544 - accuracy: 0.21 - ETA: 5:07 - loss: 3.0446 - accuracy: 0.22 - ETA: 5:06 - loss: 3.0071 - accuracy: 0.22 - ETA: 5:04 - loss: 3.0521 - accuracy: 0.21 - ETA: 5:04 - loss: 3.0484 - accuracy: 0.21 - ETA: 5:06 - loss: 3.0439 - accuracy: 0.20 - ETA: 5:04 - loss: 3.0540 - accuracy: 0.20 - ETA: 5:02 - loss: 3.0670 - accuracy: 0.19 - ETA: 5:01 - loss: 3.0921 - accuracy: 0.19 - ETA: 5:00 - loss: 3.1018 - accuracy: 0.18 - ETA: 4:59 - loss: 3.1015 - accuracy: 0.18 - ETA: 4:58 - loss: 3.0912 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0811 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0602 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0485 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0440 - accuracy: 0.20 - ETA: 4:55 - loss: 3.0426 - accuracy: 0.20 - ETA: 4:54 - loss: 3.0447 - accuracy: 0.20 - ETA: 4:54 - loss: 3.0517 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0464 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0442 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0405 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0403 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0378 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0354 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0354 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0336 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0302 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0348 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0391 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0337 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0340 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0292 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0280 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0307 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0303 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0293 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0295 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0301 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0293 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0219 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0257 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0263 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0239 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0223 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0267 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0233 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0232 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0232 - accuracy: 0.20 - ETA: 4:29 - loss: 3.0181 - accuracy: 0.20 - ETA: 4:28 - loss: 3.0174 - accuracy: 0.20 - ETA: 4:27 - loss: 3.0219 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0179 - accuracy: 0.20 - ETA: 4:25 - loss: 3.0168 - accuracy: 0.20 - ETA: 4:24 - loss: 3.0150 - accuracy: 0.20 - ETA: 4:23 - loss: 3.0121 - accuracy: 0.20 - ETA: 4:22 - loss: 3.0131 - accuracy: 0.20 - ETA: 4:21 - loss: 3.0129 - accuracy: 0.20 - ETA: 4:20 - loss: 3.0146 - accuracy: 0.20 - ETA: 4:19 - loss: 3.0169 - accuracy: 0.20 - ETA: 4:18 - loss: 3.0144 - accuracy: 0.20 - ETA: 4:17 - loss: 3.0155 - accuracy: 0.20 - ETA: 4:16 - loss: 3.0127 - accuracy: 0.20 - ETA: 4:15 - loss: 3.0133 - accuracy: 0.20 - ETA: 4:14 - loss: 3.0105 - accuracy: 0.20 - ETA: 4:13 - loss: 3.0108 - accuracy: 0.20 - ETA: 4:12 - loss: 3.0089 - accuracy: 0.20 - ETA: 4:11 - loss: 3.0093 - accuracy: 0.20 - ETA: 4:10 - loss: 3.0079 - accuracy: 0.20 - ETA: 4:09 - loss: 3.0085 - accuracy: 0.20 - ETA: 4:08 - loss: 3.0095 - accuracy: 0.20 - ETA: 4:07 - loss: 3.0087 - accuracy: 0.20 - ETA: 4:06 - loss: 3.0090 - accuracy: 0.20 - ETA: 4:05 - loss: 3.0079 - accuracy: 0.20 - ETA: 4:04 - loss: 3.0061 - accuracy: 0.20 - ETA: 4:03 - loss: 3.0066 - accuracy: 0.20 - ETA: 4:02 - loss: 3.0073 - accuracy: 0.20 - ETA: 4:01 - loss: 3.0068 - accuracy: 0.20 - ETA: 4:00 - loss: 3.0056 - accuracy: 0.20 - ETA: 3:59 - loss: 3.0049 - accuracy: 0.20 - ETA: 3:58 - loss: 3.0029 - accuracy: 0.20 - ETA: 3:57 - loss: 3.0010 - accuracy: 0.20 - ETA: 3:56 - loss: 2.9988 - accuracy: 0.20 - ETA: 3:55 - loss: 2.9975 - accuracy: 0.20 - ETA: 3:54 - loss: 2.9959 - accuracy: 0.20 - ETA: 3:53 - loss: 2.9951 - accuracy: 0.20 - ETA: 3:52 - loss: 2.9951 - accuracy: 0.20 - ETA: 3:51 - loss: 2.9931 - accuracy: 0.20 - ETA: 3:50 - loss: 2.9930 - accuracy: 0.20 - ETA: 3:49 - loss: 2.9963 - accuracy: 0.20 - ETA: 3:49 - loss: 2.9973 - accuracy: 0.20 - ETA: 3:48 - loss: 2.9961 - accuracy: 0.20 - ETA: 3:47 - loss: 2.9967 - accuracy: 0.20 - ETA: 3:46 - loss: 2.9969 - accuracy: 0.20 - ETA: 3:45 - loss: 2.9929 - accuracy: 0.20 - ETA: 3:44 - loss: 2.9932 - accuracy: 0.20 - ETA: 3:43 - loss: 2.9928 - accuracy: 0.20 - ETA: 3:42 - loss: 2.9931 - accuracy: 0.20 - ETA: 3:41 - loss: 2.9940 - accuracy: 0.20 - ETA: 3:40 - loss: 2.9954 - accuracy: 0.20 - ETA: 3:39 - loss: 2.9963 - accuracy: 0.20 - ETA: 3:38 - loss: 2.9957 - accuracy: 0.20 - ETA: 3:37 - loss: 2.9963 - accuracy: 0.20 - ETA: 3:36 - loss: 2.9954 - accuracy: 0.20 - ETA: 3:35 - loss: 2.9972 - accuracy: 0.20 - ETA: 3:34 - loss: 2.9972 - accuracy: 0.20 - ETA: 3:33 - loss: 2.9992 - accuracy: 0.20 - ETA: 3:32 - loss: 2.9976 - accuracy: 0.20 - ETA: 3:31 - loss: 2.9975 - accuracy: 0.20 - ETA: 3:30 - loss: 3.0003 - accuracy: 0.20 - ETA: 3:29 - loss: 2.9999 - accuracy: 0.20 - ETA: 3:28 - loss: 3.0003 - accuracy: 0.20 - ETA: 3:27 - loss: 2.9994 - accuracy: 0.20 - ETA: 3:26 - loss: 2.9991 - accuracy: 0.20 - ETA: 3:25 - loss: 2.9989 - accuracy: 0.20 - ETA: 3:24 - loss: 2.9990 - accuracy: 0.20 - ETA: 3:23 - loss: 3.0001 - accuracy: 0.20 - ETA: 3:22 - loss: 2.9995 - accuracy: 0.20 - ETA: 3:21 - loss: 3.0009 - accuracy: 0.20 - ETA: 3:20 - loss: 3.0020 - accuracy: 0.20 - ETA: 3:19 - loss: 3.0035 - accuracy: 0.20 - ETA: 3:18 - loss: 3.0036 - accuracy: 0.20 - ETA: 3:17 - loss: 3.0037 - accuracy: 0.20 - ETA: 3:16 - loss: 3.0042 - accuracy: 0.20 - ETA: 3:15 - loss: 3.0070 - accuracy: 0.20 - ETA: 3:14 - loss: 3.0070 - accuracy: 0.20 - ETA: 3:13 - loss: 3.0056 - accuracy: 0.20 - ETA: 3:12 - loss: 3.0079 - accuracy: 0.20 - ETA: 3:11 - loss: 3.0092 - accuracy: 0.20 - ETA: 3:10 - loss: 3.0083 - accuracy: 0.20 - ETA: 3:09 - loss: 3.0054 - accuracy: 0.20 - ETA: 3:08 - loss: 3.0045 - accuracy: 0.20 - ETA: 3:07 - loss: 3.0059 - accuracy: 0.20 - ETA: 3:06 - loss: 3.0067 - accuracy: 0.20 - ETA: 3:05 - loss: 3.0072 - accuracy: 0.20 - ETA: 3:04 - loss: 3.0118 - accuracy: 0.20 - ETA: 3:03 - loss: 3.0127 - accuracy: 0.20 - ETA: 3:02 - loss: 3.0132 - accuracy: 0.20 - ETA: 3:01 - loss: 3.0128 - accuracy: 0.20 - ETA: 3:00 - loss: 3.0153 - accuracy: 0.20 - ETA: 2:59 - loss: 3.0150 - accuracy: 0.20 - ETA: 2:59 - loss: 3.0160 - accuracy: 0.20 - ETA: 2:58 - loss: 3.0156 - accuracy: 0.20 - ETA: 2:57 - loss: 3.0164 - accuracy: 0.20 - ETA: 2:56 - loss: 3.0170 - accuracy: 0.20 - ETA: 2:55 - loss: 3.0184 - accuracy: 0.20 - ETA: 2:54 - loss: 3.0176 - accuracy: 0.20 - ETA: 2:53 - loss: 3.0162 - accuracy: 0.20 - ETA: 2:52 - loss: 3.0168 - accuracy: 0.20 - ETA: 2:51 - loss: 3.0192 - accuracy: 0.20 - ETA: 2:50 - loss: 3.0193 - accuracy: 0.20 - ETA: 2:49 - loss: 3.0191 - accuracy: 0.20 - ETA: 2:48 - loss: 3.0187 - accuracy: 0.20 - ETA: 2:47 - loss: 3.0175 - accuracy: 0.20 - ETA: 2:46 - loss: 3.0178 - accuracy: 0.20 - ETA: 2:45 - loss: 3.0189 - accuracy: 0.20 - ETA: 2:44 - loss: 3.0191 - accuracy: 0.20 - ETA: 2:43 - loss: 3.0194 - accuracy: 0.20 - ETA: 2:42 - loss: 3.0198 - accuracy: 0.20 - ETA: 2:41 - loss: 3.0181 - accuracy: 0.20 - ETA: 2:40 - loss: 3.0170 - accuracy: 0.20 - ETA: 2:39 - loss: 3.0173 - accuracy: 0.20 - ETA: 2:38 - loss: 3.0178 - accuracy: 0.20 - ETA: 2:37 - loss: 3.0173 - accuracy: 0.20 - ETA: 2:36 - loss: 3.0173 - accuracy: 0.20 - ETA: 2:35 - loss: 3.0188 - accuracy: 0.20 - ETA: 2:35 - loss: 3.0188 - accuracy: 0.20 - ETA: 2:34 - loss: 3.0200 - accuracy: 0.20 - ETA: 2:33 - loss: 3.0197 - accuracy: 0.20 - ETA: 2:32 - loss: 3.0206 - accuracy: 0.20 - ETA: 2:31 - loss: 3.0210 - accuracy: 0.20 - ETA: 2:30 - loss: 3.0205 - accuracy: 0.20 - ETA: 2:29 - loss: 3.0215 - accuracy: 0.20 - ETA: 2:28 - loss: 3.0213 - accuracy: 0.20 - ETA: 2:27 - loss: 3.0205 - accuracy: 0.20 - ETA: 2:26 - loss: 3.0216 - accuracy: 0.20 - ETA: 2:25 - loss: 3.0216 - accuracy: 0.20 - ETA: 2:24 - loss: 3.0231 - accuracy: 0.20 - ETA: 2:23 - loss: 3.0230 - accuracy: 0.20 - ETA: 2:22 - loss: 3.0239 - accuracy: 0.20 - ETA: 2:21 - loss: 3.0243 - accuracy: 0.20 - ETA: 2:20 - loss: 3.0248 - accuracy: 0.20 - ETA: 2:19 - loss: 3.0255 - accuracy: 0.20 - ETA: 2:18 - loss: 3.0253 - accuracy: 0.2007"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0234 - accuracy: 0.20 - ETA: 2:16 - loss: 3.0225 - accuracy: 0.20 - ETA: 2:15 - loss: 3.0225 - accuracy: 0.20 - ETA: 2:14 - loss: 3.0237 - accuracy: 0.20 - ETA: 2:14 - loss: 3.0227 - accuracy: 0.20 - ETA: 2:13 - loss: 3.0223 - accuracy: 0.20 - ETA: 2:12 - loss: 3.0228 - accuracy: 0.20 - ETA: 2:11 - loss: 3.0227 - accuracy: 0.20 - ETA: 2:10 - loss: 3.0235 - accuracy: 0.20 - ETA: 2:09 - loss: 3.0237 - accuracy: 0.20 - ETA: 2:08 - loss: 3.0239 - accuracy: 0.20 - ETA: 2:07 - loss: 3.0235 - accuracy: 0.20 - ETA: 2:06 - loss: 3.0228 - accuracy: 0.20 - ETA: 2:05 - loss: 3.0227 - accuracy: 0.20 - ETA: 2:04 - loss: 3.0233 - accuracy: 0.20 - ETA: 2:03 - loss: 3.0233 - accuracy: 0.20 - ETA: 2:02 - loss: 3.0232 - accuracy: 0.20 - ETA: 2:01 - loss: 3.0227 - accuracy: 0.20 - ETA: 2:00 - loss: 3.0230 - accuracy: 0.20 - ETA: 1:59 - loss: 3.0248 - accuracy: 0.20 - ETA: 1:58 - loss: 3.0248 - accuracy: 0.20 - ETA: 1:57 - loss: 3.0255 - accuracy: 0.20 - ETA: 1:56 - loss: 3.0256 - accuracy: 0.20 - ETA: 1:55 - loss: 3.0252 - accuracy: 0.20 - ETA: 1:54 - loss: 3.0248 - accuracy: 0.20 - ETA: 1:53 - loss: 3.0249 - accuracy: 0.20 - ETA: 1:52 - loss: 3.0288 - accuracy: 0.20 - ETA: 1:51 - loss: 3.0286 - accuracy: 0.20 - ETA: 1:50 - loss: 3.0293 - accuracy: 0.20 - ETA: 1:49 - loss: 3.0282 - accuracy: 0.20 - ETA: 1:48 - loss: 3.0281 - accuracy: 0.20 - ETA: 1:47 - loss: 3.0289 - accuracy: 0.20 - ETA: 1:47 - loss: 3.0293 - accuracy: 0.20 - ETA: 1:46 - loss: 3.0291 - accuracy: 0.20 - ETA: 1:45 - loss: 3.0284 - accuracy: 0.20 - ETA: 1:44 - loss: 3.0280 - accuracy: 0.20 - ETA: 1:43 - loss: 3.0272 - accuracy: 0.20 - ETA: 1:42 - loss: 3.0274 - accuracy: 0.20 - ETA: 1:41 - loss: 3.0284 - accuracy: 0.20 - ETA: 1:40 - loss: 3.0286 - accuracy: 0.20 - ETA: 1:39 - loss: 3.0270 - accuracy: 0.20 - ETA: 1:38 - loss: 3.0274 - accuracy: 0.20 - ETA: 1:37 - loss: 3.0269 - accuracy: 0.20 - ETA: 1:36 - loss: 3.0268 - accuracy: 0.20 - ETA: 1:35 - loss: 3.0267 - accuracy: 0.20 - ETA: 1:34 - loss: 3.0261 - accuracy: 0.20 - ETA: 1:33 - loss: 3.0256 - accuracy: 0.20 - ETA: 1:32 - loss: 3.0255 - accuracy: 0.20 - ETA: 1:31 - loss: 3.0254 - accuracy: 0.20 - ETA: 1:30 - loss: 3.0255 - accuracy: 0.20 - ETA: 1:29 - loss: 3.0252 - accuracy: 0.20 - ETA: 1:29 - loss: 3.0246 - accuracy: 0.20 - ETA: 1:28 - loss: 3.0244 - accuracy: 0.20 - ETA: 1:27 - loss: 3.0248 - accuracy: 0.20 - ETA: 1:26 - loss: 3.0243 - accuracy: 0.20 - ETA: 1:25 - loss: 3.0237 - accuracy: 0.20 - ETA: 1:24 - loss: 3.0228 - accuracy: 0.20 - ETA: 1:23 - loss: 3.0235 - accuracy: 0.20 - ETA: 1:22 - loss: 3.0232 - accuracy: 0.20 - ETA: 1:21 - loss: 3.0243 - accuracy: 0.20 - ETA: 1:20 - loss: 3.0239 - accuracy: 0.20 - ETA: 1:19 - loss: 3.0242 - accuracy: 0.20 - ETA: 1:18 - loss: 3.0243 - accuracy: 0.20 - ETA: 1:17 - loss: 3.0235 - accuracy: 0.20 - ETA: 1:16 - loss: 3.0239 - accuracy: 0.20 - ETA: 1:15 - loss: 3.0235 - accuracy: 0.20 - ETA: 1:14 - loss: 3.0228 - accuracy: 0.20 - ETA: 1:13 - loss: 3.0227 - accuracy: 0.20 - ETA: 1:12 - loss: 3.0228 - accuracy: 0.20 - ETA: 1:11 - loss: 3.0231 - accuracy: 0.20 - ETA: 1:10 - loss: 3.0241 - accuracy: 0.20 - ETA: 1:09 - loss: 3.0242 - accuracy: 0.20 - ETA: 1:08 - loss: 3.0238 - accuracy: 0.20 - ETA: 1:07 - loss: 3.0242 - accuracy: 0.20 - ETA: 1:06 - loss: 3.0246 - accuracy: 0.20 - ETA: 1:05 - loss: 3.0242 - accuracy: 0.20 - ETA: 1:05 - loss: 3.0241 - accuracy: 0.20 - ETA: 1:04 - loss: 3.0242 - accuracy: 0.20 - ETA: 1:03 - loss: 3.0240 - accuracy: 0.20 - ETA: 1:02 - loss: 3.0236 - accuracy: 0.20 - ETA: 1:01 - loss: 3.0237 - accuracy: 0.20 - ETA: 1:00 - loss: 3.0237 - accuracy: 0.20 - ETA: 59s - loss: 3.0241 - accuracy: 0.2000 - ETA: 58s - loss: 3.0237 - accuracy: 0.200 - ETA: 57s - loss: 3.0237 - accuracy: 0.200 - ETA: 56s - loss: 3.0231 - accuracy: 0.200 - ETA: 55s - loss: 3.0228 - accuracy: 0.200 - ETA: 54s - loss: 3.0227 - accuracy: 0.200 - ETA: 53s - loss: 3.0226 - accuracy: 0.200 - ETA: 52s - loss: 3.0232 - accuracy: 0.200 - ETA: 51s - loss: 3.0232 - accuracy: 0.200 - ETA: 50s - loss: 3.0228 - accuracy: 0.200 - ETA: 49s - loss: 3.0229 - accuracy: 0.200 - ETA: 48s - loss: 3.0230 - accuracy: 0.200 - ETA: 47s - loss: 3.0227 - accuracy: 0.200 - ETA: 46s - loss: 3.0228 - accuracy: 0.200 - ETA: 45s - loss: 3.0223 - accuracy: 0.200 - ETA: 44s - loss: 3.0222 - accuracy: 0.200 - ETA: 43s - loss: 3.0218 - accuracy: 0.200 - ETA: 43s - loss: 3.0213 - accuracy: 0.200 - ETA: 42s - loss: 3.0213 - accuracy: 0.200 - ETA: 41s - loss: 3.0211 - accuracy: 0.200 - ETA: 40s - loss: 3.0207 - accuracy: 0.200 - ETA: 39s - loss: 3.0205 - accuracy: 0.201 - ETA: 38s - loss: 3.0200 - accuracy: 0.201 - ETA: 37s - loss: 3.0198 - accuracy: 0.201 - ETA: 36s - loss: 3.0201 - accuracy: 0.201 - ETA: 35s - loss: 3.0206 - accuracy: 0.201 - ETA: 34s - loss: 3.0206 - accuracy: 0.201 - ETA: 33s - loss: 3.0196 - accuracy: 0.201 - ETA: 32s - loss: 3.0196 - accuracy: 0.201 - ETA: 31s - loss: 3.0195 - accuracy: 0.201 - ETA: 30s - loss: 3.0193 - accuracy: 0.201 - ETA: 29s - loss: 3.0189 - accuracy: 0.201 - ETA: 28s - loss: 3.0184 - accuracy: 0.201 - ETA: 27s - loss: 3.0178 - accuracy: 0.201 - ETA: 26s - loss: 3.0175 - accuracy: 0.201 - ETA: 25s - loss: 3.0176 - accuracy: 0.201 - ETA: 24s - loss: 3.0179 - accuracy: 0.201 - ETA: 23s - loss: 3.0177 - accuracy: 0.201 - ETA: 22s - loss: 3.0175 - accuracy: 0.201 - ETA: 22s - loss: 3.0175 - accuracy: 0.201 - ETA: 21s - loss: 3.0174 - accuracy: 0.201 - ETA: 20s - loss: 3.0171 - accuracy: 0.201 - ETA: 19s - loss: 3.0176 - accuracy: 0.201 - ETA: 18s - loss: 3.0177 - accuracy: 0.201 - ETA: 17s - loss: 3.0179 - accuracy: 0.201 - ETA: 16s - loss: 3.0174 - accuracy: 0.201 - ETA: 15s - loss: 3.0171 - accuracy: 0.201 - ETA: 14s - loss: 3.0173 - accuracy: 0.201 - ETA: 13s - loss: 3.0174 - accuracy: 0.201 - ETA: 12s - loss: 3.0177 - accuracy: 0.201 - ETA: 11s - loss: 3.0167 - accuracy: 0.201 - ETA: 10s - loss: 3.0163 - accuracy: 0.201 - ETA: 9s - loss: 3.0165 - accuracy: 0.201 - ETA: 8s - loss: 3.0167 - accuracy: 0.20 - ETA: 7s - loss: 3.0169 - accuracy: 0.20 - ETA: 6s - loss: 3.0169 - accuracy: 0.20 - ETA: 5s - loss: 3.0165 - accuracy: 0.20 - ETA: 4s - loss: 3.0168 - accuracy: 0.20 - ETA: 3s - loss: 3.0164 - accuracy: 0.20 - ETA: 2s - loss: 3.0162 - accuracy: 0.20 - ETA: 1s - loss: 3.0160 - accuracy: 0.20 - ETA: 1s - loss: 3.0162 - accuracy: 0.20 - ETA: 0s - loss: 3.0153 - accuracy: 0.20 - 339s 8ms/step - loss: 3.0154 - accuracy: 0.2021 - val_loss: 4.3553 - val_accuracy: 0.0369\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:27 - loss: 2.9507 - accuracy: 0.16 - ETA: 5:14 - loss: 2.8802 - accuracy: 0.20 - ETA: 5:09 - loss: 2.8410 - accuracy: 0.21 - ETA: 5:06 - loss: 2.8954 - accuracy: 0.20 - ETA: 5:08 - loss: 2.8552 - accuracy: 0.20 - ETA: 5:05 - loss: 2.8933 - accuracy: 0.21 - ETA: 5:05 - loss: 2.8893 - accuracy: 0.22 - ETA: 5:05 - loss: 2.8878 - accuracy: 0.21 - ETA: 5:07 - loss: 2.8897 - accuracy: 0.22 - ETA: 5:05 - loss: 2.8847 - accuracy: 0.22 - ETA: 5:04 - loss: 2.8944 - accuracy: 0.21 - ETA: 5:03 - loss: 2.9115 - accuracy: 0.21 - ETA: 5:01 - loss: 2.9204 - accuracy: 0.21 - ETA: 5:00 - loss: 2.9279 - accuracy: 0.21 - ETA: 4:58 - loss: 2.9292 - accuracy: 0.21 - ETA: 4:58 - loss: 2.9268 - accuracy: 0.21 - ETA: 4:57 - loss: 2.9322 - accuracy: 0.21 - ETA: 4:57 - loss: 2.9432 - accuracy: 0.20 - ETA: 4:57 - loss: 2.9506 - accuracy: 0.20 - ETA: 4:58 - loss: 2.9565 - accuracy: 0.20 - ETA: 4:57 - loss: 2.9759 - accuracy: 0.20 - ETA: 4:56 - loss: 2.9717 - accuracy: 0.20 - ETA: 4:55 - loss: 2.9637 - accuracy: 0.20 - ETA: 4:53 - loss: 2.9637 - accuracy: 0.20 - ETA: 4:53 - loss: 2.9667 - accuracy: 0.20 - ETA: 4:52 - loss: 2.9688 - accuracy: 0.20 - ETA: 4:51 - loss: 2.9695 - accuracy: 0.20 - ETA: 4:50 - loss: 2.9703 - accuracy: 0.20 - ETA: 4:49 - loss: 2.9734 - accuracy: 0.20 - ETA: 4:48 - loss: 2.9756 - accuracy: 0.20 - ETA: 4:47 - loss: 2.9759 - accuracy: 0.20 - ETA: 4:45 - loss: 2.9731 - accuracy: 0.20 - ETA: 4:44 - loss: 2.9725 - accuracy: 0.20 - ETA: 4:43 - loss: 2.9715 - accuracy: 0.20 - ETA: 4:42 - loss: 2.9725 - accuracy: 0.20 - ETA: 4:41 - loss: 2.9778 - accuracy: 0.20 - ETA: 4:40 - loss: 2.9792 - accuracy: 0.20 - ETA: 4:39 - loss: 2.9817 - accuracy: 0.20 - ETA: 4:38 - loss: 2.9821 - accuracy: 0.20 - ETA: 4:37 - loss: 2.9799 - accuracy: 0.20 - ETA: 4:36 - loss: 2.9779 - accuracy: 0.20 - ETA: 4:35 - loss: 2.9747 - accuracy: 0.20 - ETA: 4:34 - loss: 2.9786 - accuracy: 0.20 - ETA: 4:33 - loss: 2.9790 - accuracy: 0.20 - ETA: 4:32 - loss: 2.9798 - accuracy: 0.20 - ETA: 4:31 - loss: 2.9775 - accuracy: 0.20 - ETA: 4:30 - loss: 2.9808 - accuracy: 0.20 - ETA: 4:29 - loss: 2.9832 - accuracy: 0.20 - ETA: 4:28 - loss: 2.9830 - accuracy: 0.20 - ETA: 4:27 - loss: 2.9858 - accuracy: 0.19 - ETA: 4:26 - loss: 2.9871 - accuracy: 0.19 - ETA: 4:25 - loss: 2.9904 - accuracy: 0.19 - ETA: 4:24 - loss: 2.9916 - accuracy: 0.19 - ETA: 4:24 - loss: 2.9931 - accuracy: 0.19 - ETA: 4:22 - loss: 2.9935 - accuracy: 0.19 - ETA: 4:21 - loss: 2.9937 - accuracy: 0.19 - ETA: 4:20 - loss: 2.9924 - accuracy: 0.19 - ETA: 4:19 - loss: 2.9927 - accuracy: 0.19 - ETA: 4:19 - loss: 2.9961 - accuracy: 0.19 - ETA: 4:17 - loss: 2.9965 - accuracy: 0.19 - ETA: 4:16 - loss: 2.9971 - accuracy: 0.19 - ETA: 4:15 - loss: 2.9952 - accuracy: 0.19 - ETA: 4:15 - loss: 2.9952 - accuracy: 0.19 - ETA: 4:13 - loss: 2.9943 - accuracy: 0.19 - ETA: 4:12 - loss: 2.9935 - accuracy: 0.19 - ETA: 4:11 - loss: 2.9920 - accuracy: 0.19 - ETA: 4:10 - loss: 2.9928 - accuracy: 0.19 - ETA: 4:09 - loss: 2.9930 - accuracy: 0.20 - ETA: 4:08 - loss: 2.9902 - accuracy: 0.20 - ETA: 4:07 - loss: 2.9917 - accuracy: 0.20 - ETA: 4:06 - loss: 2.9952 - accuracy: 0.20 - ETA: 4:05 - loss: 2.9942 - accuracy: 0.20 - ETA: 4:04 - loss: 2.9958 - accuracy: 0.20 - ETA: 4:03 - loss: 2.9961 - accuracy: 0.20 - ETA: 4:02 - loss: 2.9936 - accuracy: 0.20 - ETA: 4:01 - loss: 2.9942 - accuracy: 0.20 - ETA: 4:01 - loss: 2.9956 - accuracy: 0.20 - ETA: 4:00 - loss: 2.9989 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0006 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0004 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0011 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0035 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0053 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0032 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0054 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0058 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0061 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0106 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0116 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0123 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0148 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0161 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0183 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0177 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0181 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0196 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0195 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0206 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0227 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0254 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0257 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0276 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0280 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0300 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0315 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0337 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0359 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0376 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0385 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0388 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0401 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0405 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0401 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0414 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0414 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0414 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0418 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0431 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0428 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0417 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0429 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0433 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0453 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0442 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0450 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0461 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0474 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0484 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0472 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0464 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0473 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0479 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0502 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0501 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0499 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0504 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0502 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0492 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0502 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0509 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0510 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0527 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0533 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0532 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0536 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0525 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0525 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0531 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0542 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0526 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0544 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0551 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0563 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0561 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0561 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0556 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0541 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0551 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0530 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0531 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0523 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0502 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0505 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0507 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0514 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0510 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0508 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0506 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0505 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0512 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0508 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0499 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0491 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0472 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0470 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0468 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0468 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0460 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0449 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0448 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0456 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0466 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0456 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0447 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0451 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0460 - accuracy: 0.1966"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0469 - accuracy: 0.19 - ETA: 2:16 - loss: 3.0473 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0471 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0480 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0482 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0479 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0480 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0476 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0475 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0485 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0490 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0483 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0487 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0479 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0476 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0470 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0469 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0473 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0462 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0457 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0456 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0447 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0447 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0441 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0437 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0436 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0443 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0446 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0445 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0446 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0453 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0452 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0450 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0450 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0439 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0448 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0441 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0432 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0432 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0431 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0427 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0434 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0427 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0430 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0427 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0420 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0426 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0433 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0436 - accuracy: 0.19 - ETA: 1:30 - loss: 3.0436 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0432 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0430 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0427 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0442 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0449 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0443 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0439 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0447 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0454 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0454 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0454 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0458 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0464 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0459 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0456 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0460 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0465 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0471 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0474 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0474 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0468 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0471 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0465 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0472 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0479 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0486 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0481 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0479 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0475 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0473 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0471 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0474 - accuracy: 0.19 - ETA: 59s - loss: 3.0478 - accuracy: 0.1948 - ETA: 58s - loss: 3.0486 - accuracy: 0.194 - ETA: 57s - loss: 3.0489 - accuracy: 0.194 - ETA: 56s - loss: 3.0488 - accuracy: 0.194 - ETA: 55s - loss: 3.0478 - accuracy: 0.194 - ETA: 54s - loss: 3.0472 - accuracy: 0.195 - ETA: 53s - loss: 3.0476 - accuracy: 0.195 - ETA: 52s - loss: 3.0468 - accuracy: 0.195 - ETA: 51s - loss: 3.0463 - accuracy: 0.195 - ETA: 50s - loss: 3.0471 - accuracy: 0.195 - ETA: 49s - loss: 3.0477 - accuracy: 0.195 - ETA: 48s - loss: 3.0477 - accuracy: 0.195 - ETA: 47s - loss: 3.0484 - accuracy: 0.194 - ETA: 46s - loss: 3.0487 - accuracy: 0.194 - ETA: 45s - loss: 3.0497 - accuracy: 0.194 - ETA: 44s - loss: 3.0494 - accuracy: 0.194 - ETA: 44s - loss: 3.0492 - accuracy: 0.194 - ETA: 43s - loss: 3.0491 - accuracy: 0.194 - ETA: 42s - loss: 3.0488 - accuracy: 0.194 - ETA: 41s - loss: 3.0485 - accuracy: 0.195 - ETA: 40s - loss: 3.0490 - accuracy: 0.194 - ETA: 39s - loss: 3.0494 - accuracy: 0.194 - ETA: 38s - loss: 3.0500 - accuracy: 0.194 - ETA: 37s - loss: 3.0501 - accuracy: 0.194 - ETA: 36s - loss: 3.0502 - accuracy: 0.194 - ETA: 35s - loss: 3.0504 - accuracy: 0.194 - ETA: 34s - loss: 3.0503 - accuracy: 0.194 - ETA: 33s - loss: 3.0494 - accuracy: 0.194 - ETA: 32s - loss: 3.0498 - accuracy: 0.194 - ETA: 31s - loss: 3.0500 - accuracy: 0.194 - ETA: 30s - loss: 3.0502 - accuracy: 0.194 - ETA: 29s - loss: 3.0502 - accuracy: 0.194 - ETA: 28s - loss: 3.0498 - accuracy: 0.194 - ETA: 27s - loss: 3.0498 - accuracy: 0.194 - ETA: 26s - loss: 3.0501 - accuracy: 0.194 - ETA: 25s - loss: 3.0509 - accuracy: 0.194 - ETA: 24s - loss: 3.0507 - accuracy: 0.194 - ETA: 24s - loss: 3.0506 - accuracy: 0.194 - ETA: 23s - loss: 3.0513 - accuracy: 0.194 - ETA: 22s - loss: 3.0516 - accuracy: 0.194 - ETA: 21s - loss: 3.0522 - accuracy: 0.194 - ETA: 20s - loss: 3.0522 - accuracy: 0.193 - ETA: 19s - loss: 3.0551 - accuracy: 0.193 - ETA: 18s - loss: 3.0550 - accuracy: 0.193 - ETA: 17s - loss: 3.0556 - accuracy: 0.193 - ETA: 16s - loss: 3.0559 - accuracy: 0.193 - ETA: 15s - loss: 3.0557 - accuracy: 0.193 - ETA: 14s - loss: 3.0561 - accuracy: 0.193 - ETA: 13s - loss: 3.0559 - accuracy: 0.193 - ETA: 12s - loss: 3.0553 - accuracy: 0.193 - ETA: 11s - loss: 3.0548 - accuracy: 0.193 - ETA: 10s - loss: 3.0571 - accuracy: 0.193 - ETA: 9s - loss: 3.0572 - accuracy: 0.193 - ETA: 8s - loss: 3.0572 - accuracy: 0.19 - ETA: 7s - loss: 3.0577 - accuracy: 0.19 - ETA: 6s - loss: 3.0574 - accuracy: 0.19 - ETA: 5s - loss: 3.0563 - accuracy: 0.19 - ETA: 4s - loss: 3.0558 - accuracy: 0.19 - ETA: 3s - loss: 3.0560 - accuracy: 0.19 - ETA: 2s - loss: 3.0559 - accuracy: 0.19 - ETA: 1s - loss: 3.0550 - accuracy: 0.19 - ETA: 1s - loss: 3.0546 - accuracy: 0.19 - ETA: 0s - loss: 3.0550 - accuracy: 0.19 - 338s 8ms/step - loss: 3.0549 - accuracy: 0.1939 - val_loss: 4.3866 - val_accuracy: 0.0357\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:05 - loss: 3.2170 - accuracy: 0.16 - ETA: 5:05 - loss: 3.1520 - accuracy: 0.16 - ETA: 5:04 - loss: 3.0830 - accuracy: 0.17 - ETA: 5:01 - loss: 3.0698 - accuracy: 0.16 - ETA: 4:58 - loss: 3.0570 - accuracy: 0.17 - ETA: 4:58 - loss: 3.0503 - accuracy: 0.17 - ETA: 5:01 - loss: 3.0526 - accuracy: 0.17 - ETA: 5:03 - loss: 3.0365 - accuracy: 0.17 - ETA: 5:03 - loss: 3.0475 - accuracy: 0.17 - ETA: 5:03 - loss: 3.0459 - accuracy: 0.17 - ETA: 5:03 - loss: 3.0536 - accuracy: 0.17 - ETA: 5:01 - loss: 3.0544 - accuracy: 0.17 - ETA: 5:00 - loss: 3.0604 - accuracy: 0.17 - ETA: 4:58 - loss: 3.0606 - accuracy: 0.17 - ETA: 4:57 - loss: 3.0667 - accuracy: 0.17 - ETA: 4:55 - loss: 3.0645 - accuracy: 0.17 - ETA: 4:55 - loss: 3.0719 - accuracy: 0.17 - ETA: 4:53 - loss: 3.0722 - accuracy: 0.17 - ETA: 4:52 - loss: 3.0642 - accuracy: 0.17 - ETA: 4:51 - loss: 3.0714 - accuracy: 0.17 - ETA: 4:50 - loss: 3.0647 - accuracy: 0.17 - ETA: 4:49 - loss: 3.0710 - accuracy: 0.17 - ETA: 4:48 - loss: 3.0719 - accuracy: 0.17 - ETA: 4:46 - loss: 3.0829 - accuracy: 0.17 - ETA: 4:45 - loss: 3.0777 - accuracy: 0.17 - ETA: 4:44 - loss: 3.0736 - accuracy: 0.17 - ETA: 4:43 - loss: 3.0699 - accuracy: 0.17 - ETA: 4:43 - loss: 3.0642 - accuracy: 0.17 - ETA: 4:42 - loss: 3.0677 - accuracy: 0.17 - ETA: 4:41 - loss: 3.0696 - accuracy: 0.17 - ETA: 4:40 - loss: 3.0725 - accuracy: 0.17 - ETA: 4:39 - loss: 3.0703 - accuracy: 0.17 - ETA: 4:38 - loss: 3.0700 - accuracy: 0.17 - ETA: 4:37 - loss: 3.0700 - accuracy: 0.17 - ETA: 4:36 - loss: 3.0739 - accuracy: 0.17 - ETA: 4:35 - loss: 3.0759 - accuracy: 0.17 - ETA: 4:34 - loss: 3.0777 - accuracy: 0.17 - ETA: 4:33 - loss: 3.0776 - accuracy: 0.18 - ETA: 4:32 - loss: 3.0742 - accuracy: 0.18 - ETA: 4:30 - loss: 3.0799 - accuracy: 0.18 - ETA: 4:29 - loss: 3.0774 - accuracy: 0.18 - ETA: 4:28 - loss: 3.0796 - accuracy: 0.18 - ETA: 4:27 - loss: 3.0787 - accuracy: 0.18 - ETA: 4:27 - loss: 3.0771 - accuracy: 0.18 - ETA: 4:26 - loss: 3.0770 - accuracy: 0.18 - ETA: 4:25 - loss: 3.0666 - accuracy: 0.18 - ETA: 4:25 - loss: 3.0672 - accuracy: 0.18 - ETA: 4:24 - loss: 3.0697 - accuracy: 0.18 - ETA: 4:23 - loss: 3.0680 - accuracy: 0.18 - ETA: 4:22 - loss: 3.0705 - accuracy: 0.18 - ETA: 4:22 - loss: 3.0721 - accuracy: 0.18 - ETA: 4:21 - loss: 3.0757 - accuracy: 0.18 - ETA: 4:19 - loss: 3.0736 - accuracy: 0.18 - ETA: 4:18 - loss: 3.0759 - accuracy: 0.18 - ETA: 4:17 - loss: 3.0752 - accuracy: 0.18 - ETA: 4:16 - loss: 3.0767 - accuracy: 0.18 - ETA: 4:15 - loss: 3.0757 - accuracy: 0.18 - ETA: 4:14 - loss: 3.0771 - accuracy: 0.18 - ETA: 4:13 - loss: 3.0772 - accuracy: 0.18 - ETA: 4:12 - loss: 3.0794 - accuracy: 0.18 - ETA: 4:12 - loss: 3.0788 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0780 - accuracy: 0.18 - ETA: 4:10 - loss: 3.0829 - accuracy: 0.18 - ETA: 4:09 - loss: 3.0827 - accuracy: 0.18 - ETA: 4:08 - loss: 3.0820 - accuracy: 0.18 - ETA: 4:07 - loss: 3.0818 - accuracy: 0.18 - ETA: 4:06 - loss: 3.0818 - accuracy: 0.18 - ETA: 4:05 - loss: 3.0817 - accuracy: 0.18 - ETA: 4:04 - loss: 3.0824 - accuracy: 0.18 - ETA: 4:03 - loss: 3.0827 - accuracy: 0.18 - ETA: 4:02 - loss: 3.0853 - accuracy: 0.18 - ETA: 4:01 - loss: 3.0860 - accuracy: 0.18 - ETA: 4:00 - loss: 3.0857 - accuracy: 0.18 - ETA: 3:59 - loss: 3.0851 - accuracy: 0.18 - ETA: 3:58 - loss: 3.0846 - accuracy: 0.18 - ETA: 3:57 - loss: 3.0844 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0844 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0820 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0829 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0825 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0838 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0838 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0850 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0866 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0873 - accuracy: 0.18 - ETA: 3:49 - loss: 3.0864 - accuracy: 0.18 - ETA: 3:48 - loss: 3.0888 - accuracy: 0.18 - ETA: 3:47 - loss: 3.0879 - accuracy: 0.18 - ETA: 3:46 - loss: 3.0873 - accuracy: 0.18 - ETA: 3:45 - loss: 3.0864 - accuracy: 0.18 - ETA: 3:44 - loss: 3.0861 - accuracy: 0.18 - ETA: 3:43 - loss: 3.0839 - accuracy: 0.18 - ETA: 3:42 - loss: 3.0845 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0839 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0851 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0846 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0836 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0828 - accuracy: 0.18 - ETA: 3:37 - loss: 3.0839 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0862 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0863 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0860 - accuracy: 0.18 - ETA: 3:33 - loss: 3.0854 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0835 - accuracy: 0.18 - ETA: 3:31 - loss: 3.0833 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0843 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0857 - accuracy: 0.18 - ETA: 3:28 - loss: 3.0828 - accuracy: 0.18 - ETA: 3:27 - loss: 3.0812 - accuracy: 0.18 - ETA: 3:26 - loss: 3.0814 - accuracy: 0.18 - ETA: 3:25 - loss: 3.0816 - accuracy: 0.18 - ETA: 3:24 - loss: 3.0823 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0821 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0827 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0833 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0810 - accuracy: 0.18 - ETA: 3:19 - loss: 3.0805 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0809 - accuracy: 0.18 - ETA: 3:17 - loss: 3.0790 - accuracy: 0.18 - ETA: 3:16 - loss: 3.0802 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0773 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0774 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0780 - accuracy: 0.18 - ETA: 3:13 - loss: 3.0767 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0752 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0753 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0739 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0728 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0745 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0738 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0726 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0722 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0719 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0715 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0712 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0698 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0694 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0691 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0690 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0687 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0677 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0689 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0698 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0700 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0696 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0691 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0696 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0692 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0694 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0694 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0687 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0681 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0683 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0685 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0679 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0665 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0659 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0654 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0661 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0655 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0660 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0678 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0686 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0701 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0726 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0734 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0746 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0750 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0756 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0769 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0784 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0791 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0794 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0783 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0775 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0789 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0796 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0800 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0799 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0813 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0810 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0811 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0810 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0807 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0795 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0799 - accuracy: 0.1892"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:14 - loss: 3.0809 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0800 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0803 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0808 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0802 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0803 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0813 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0805 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0803 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0800 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0803 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0812 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0814 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0819 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0810 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0805 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0797 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0799 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0802 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0799 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0793 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0802 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0807 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0821 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0819 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0827 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0831 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0834 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0835 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0838 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0838 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0836 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0832 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0848 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0847 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0841 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0834 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0829 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0841 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0846 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0841 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0838 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0838 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0833 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0838 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0836 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0839 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0826 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0824 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0823 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0837 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0851 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0854 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0850 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0844 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0831 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0824 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0818 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0813 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0807 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0801 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0802 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0804 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0794 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0790 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0795 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0791 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0793 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0789 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0785 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0790 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0798 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0796 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0805 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0804 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0807 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0810 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0798 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0800 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0792 - accuracy: 0.18 - ETA: 59s - loss: 3.0788 - accuracy: 0.1895 - ETA: 58s - loss: 3.0784 - accuracy: 0.189 - ETA: 58s - loss: 3.0776 - accuracy: 0.189 - ETA: 57s - loss: 3.0766 - accuracy: 0.189 - ETA: 56s - loss: 3.0764 - accuracy: 0.189 - ETA: 55s - loss: 3.0760 - accuracy: 0.189 - ETA: 54s - loss: 3.0754 - accuracy: 0.189 - ETA: 53s - loss: 3.0747 - accuracy: 0.190 - ETA: 52s - loss: 3.0745 - accuracy: 0.190 - ETA: 51s - loss: 3.0742 - accuracy: 0.190 - ETA: 50s - loss: 3.0752 - accuracy: 0.190 - ETA: 49s - loss: 3.0747 - accuracy: 0.190 - ETA: 48s - loss: 3.0746 - accuracy: 0.190 - ETA: 47s - loss: 3.0738 - accuracy: 0.190 - ETA: 46s - loss: 3.0741 - accuracy: 0.190 - ETA: 45s - loss: 3.0740 - accuracy: 0.190 - ETA: 44s - loss: 3.0747 - accuracy: 0.190 - ETA: 44s - loss: 3.0751 - accuracy: 0.190 - ETA: 43s - loss: 3.0751 - accuracy: 0.190 - ETA: 42s - loss: 3.0751 - accuracy: 0.190 - ETA: 41s - loss: 3.0741 - accuracy: 0.191 - ETA: 40s - loss: 3.0747 - accuracy: 0.191 - ETA: 39s - loss: 3.0745 - accuracy: 0.191 - ETA: 38s - loss: 3.0741 - accuracy: 0.191 - ETA: 37s - loss: 3.0737 - accuracy: 0.191 - ETA: 36s - loss: 3.0732 - accuracy: 0.191 - ETA: 35s - loss: 3.0729 - accuracy: 0.191 - ETA: 34s - loss: 3.0737 - accuracy: 0.191 - ETA: 33s - loss: 3.0733 - accuracy: 0.191 - ETA: 32s - loss: 3.0740 - accuracy: 0.191 - ETA: 31s - loss: 3.0738 - accuracy: 0.191 - ETA: 31s - loss: 3.0739 - accuracy: 0.191 - ETA: 30s - loss: 3.0738 - accuracy: 0.191 - ETA: 29s - loss: 3.0731 - accuracy: 0.191 - ETA: 28s - loss: 3.0730 - accuracy: 0.191 - ETA: 27s - loss: 3.0726 - accuracy: 0.191 - ETA: 26s - loss: 3.0720 - accuracy: 0.191 - ETA: 25s - loss: 3.0717 - accuracy: 0.191 - ETA: 24s - loss: 3.0716 - accuracy: 0.191 - ETA: 23s - loss: 3.0716 - accuracy: 0.191 - ETA: 22s - loss: 3.0719 - accuracy: 0.191 - ETA: 21s - loss: 3.0719 - accuracy: 0.192 - ETA: 20s - loss: 3.0724 - accuracy: 0.191 - ETA: 19s - loss: 3.0721 - accuracy: 0.191 - ETA: 18s - loss: 3.0718 - accuracy: 0.192 - ETA: 17s - loss: 3.0719 - accuracy: 0.192 - ETA: 17s - loss: 3.0719 - accuracy: 0.191 - ETA: 16s - loss: 3.0715 - accuracy: 0.192 - ETA: 15s - loss: 3.0715 - accuracy: 0.192 - ETA: 14s - loss: 3.0716 - accuracy: 0.192 - ETA: 13s - loss: 3.0716 - accuracy: 0.192 - ETA: 12s - loss: 3.0719 - accuracy: 0.192 - ETA: 11s - loss: 3.0718 - accuracy: 0.192 - ETA: 10s - loss: 3.0721 - accuracy: 0.192 - ETA: 9s - loss: 3.0729 - accuracy: 0.192 - ETA: 8s - loss: 3.0725 - accuracy: 0.19 - ETA: 7s - loss: 3.0731 - accuracy: 0.19 - ETA: 6s - loss: 3.0734 - accuracy: 0.19 - ETA: 5s - loss: 3.0738 - accuracy: 0.19 - ETA: 4s - loss: 3.0738 - accuracy: 0.19 - ETA: 3s - loss: 3.0739 - accuracy: 0.19 - ETA: 2s - loss: 3.0736 - accuracy: 0.19 - ETA: 1s - loss: 3.0728 - accuracy: 0.19 - ETA: 1s - loss: 3.0728 - accuracy: 0.19 - ETA: 0s - loss: 3.0731 - accuracy: 0.19 - 334s 8ms/step - loss: 3.0730 - accuracy: 0.1918 - val_loss: 4.4052 - val_accuracy: 0.0376\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:27 - loss: 3.0163 - accuracy: 0.20 - ETA: 5:29 - loss: 2.9319 - accuracy: 0.20 - ETA: 5:26 - loss: 2.9729 - accuracy: 0.20 - ETA: 5:23 - loss: 2.9370 - accuracy: 0.20 - ETA: 5:20 - loss: 2.9842 - accuracy: 0.20 - ETA: 5:18 - loss: 2.9759 - accuracy: 0.20 - ETA: 5:14 - loss: 2.9597 - accuracy: 0.21 - ETA: 5:13 - loss: 2.9668 - accuracy: 0.21 - ETA: 5:12 - loss: 2.9887 - accuracy: 0.20 - ETA: 5:09 - loss: 3.0093 - accuracy: 0.20 - ETA: 5:08 - loss: 3.0125 - accuracy: 0.20 - ETA: 5:07 - loss: 2.9922 - accuracy: 0.20 - ETA: 5:06 - loss: 2.9813 - accuracy: 0.20 - ETA: 5:04 - loss: 2.9903 - accuracy: 0.20 - ETA: 5:03 - loss: 2.9911 - accuracy: 0.20 - ETA: 5:02 - loss: 2.9949 - accuracy: 0.20 - ETA: 5:00 - loss: 3.0011 - accuracy: 0.20 - ETA: 4:59 - loss: 3.0132 - accuracy: 0.20 - ETA: 4:58 - loss: 3.0151 - accuracy: 0.20 - ETA: 4:57 - loss: 3.0197 - accuracy: 0.20 - ETA: 4:55 - loss: 3.0118 - accuracy: 0.20 - ETA: 4:54 - loss: 3.0091 - accuracy: 0.20 - ETA: 4:53 - loss: 3.0068 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0086 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0077 - accuracy: 0.20 - ETA: 4:51 - loss: 3.0173 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0257 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0177 - accuracy: 0.20 - ETA: 4:47 - loss: 3.0187 - accuracy: 0.20 - ETA: 4:46 - loss: 3.0214 - accuracy: 0.20 - ETA: 4:45 - loss: 3.0225 - accuracy: 0.20 - ETA: 4:44 - loss: 3.0215 - accuracy: 0.20 - ETA: 4:43 - loss: 3.0205 - accuracy: 0.20 - ETA: 4:42 - loss: 3.0221 - accuracy: 0.20 - ETA: 4:41 - loss: 3.0237 - accuracy: 0.20 - ETA: 4:40 - loss: 3.0250 - accuracy: 0.20 - ETA: 4:38 - loss: 3.0263 - accuracy: 0.20 - ETA: 4:37 - loss: 3.0273 - accuracy: 0.20 - ETA: 4:36 - loss: 3.0284 - accuracy: 0.20 - ETA: 4:35 - loss: 3.0273 - accuracy: 0.20 - ETA: 4:35 - loss: 3.0268 - accuracy: 0.20 - ETA: 4:33 - loss: 3.0310 - accuracy: 0.20 - ETA: 4:33 - loss: 3.0325 - accuracy: 0.20 - ETA: 4:32 - loss: 3.0309 - accuracy: 0.20 - ETA: 4:31 - loss: 3.0339 - accuracy: 0.20 - ETA: 4:30 - loss: 3.0265 - accuracy: 0.20 - ETA: 4:30 - loss: 3.0250 - accuracy: 0.20 - ETA: 4:29 - loss: 3.0250 - accuracy: 0.20 - ETA: 4:28 - loss: 3.0298 - accuracy: 0.20 - ETA: 4:27 - loss: 3.0324 - accuracy: 0.20 - ETA: 4:26 - loss: 3.0360 - accuracy: 0.20 - ETA: 4:25 - loss: 3.0313 - accuracy: 0.20 - ETA: 4:23 - loss: 3.0317 - accuracy: 0.20 - ETA: 4:22 - loss: 3.0319 - accuracy: 0.20 - ETA: 4:22 - loss: 3.0315 - accuracy: 0.20 - ETA: 4:21 - loss: 3.0288 - accuracy: 0.20 - ETA: 4:20 - loss: 3.0278 - accuracy: 0.20 - ETA: 4:19 - loss: 3.0285 - accuracy: 0.20 - ETA: 4:18 - loss: 3.0285 - accuracy: 0.20 - ETA: 4:17 - loss: 3.0320 - accuracy: 0.20 - ETA: 4:16 - loss: 3.0307 - accuracy: 0.20 - ETA: 4:15 - loss: 3.0276 - accuracy: 0.20 - ETA: 4:14 - loss: 3.0284 - accuracy: 0.20 - ETA: 4:13 - loss: 3.0285 - accuracy: 0.20 - ETA: 4:12 - loss: 3.0267 - accuracy: 0.20 - ETA: 4:11 - loss: 3.0290 - accuracy: 0.20 - ETA: 4:10 - loss: 3.0274 - accuracy: 0.20 - ETA: 4:09 - loss: 3.0264 - accuracy: 0.20 - ETA: 4:09 - loss: 3.0274 - accuracy: 0.20 - ETA: 4:08 - loss: 3.0258 - accuracy: 0.20 - ETA: 4:07 - loss: 3.0246 - accuracy: 0.20 - ETA: 4:06 - loss: 3.0230 - accuracy: 0.20 - ETA: 4:05 - loss: 3.0247 - accuracy: 0.20 - ETA: 4:04 - loss: 3.0230 - accuracy: 0.20 - ETA: 4:04 - loss: 3.0250 - accuracy: 0.20 - ETA: 4:03 - loss: 3.0249 - accuracy: 0.20 - ETA: 4:02 - loss: 3.0229 - accuracy: 0.20 - ETA: 4:00 - loss: 3.0248 - accuracy: 0.20 - ETA: 4:00 - loss: 3.0271 - accuracy: 0.20 - ETA: 3:59 - loss: 3.0293 - accuracy: 0.20 - ETA: 3:58 - loss: 3.0297 - accuracy: 0.20 - ETA: 3:57 - loss: 3.0327 - accuracy: 0.20 - ETA: 3:56 - loss: 3.0484 - accuracy: 0.20 - ETA: 3:55 - loss: 3.0484 - accuracy: 0.20 - ETA: 3:54 - loss: 3.0527 - accuracy: 0.20 - ETA: 3:53 - loss: 3.0522 - accuracy: 0.20 - ETA: 3:52 - loss: 3.0528 - accuracy: 0.20 - ETA: 3:51 - loss: 3.0519 - accuracy: 0.20 - ETA: 3:50 - loss: 3.0495 - accuracy: 0.20 - ETA: 3:49 - loss: 3.0480 - accuracy: 0.20 - ETA: 3:48 - loss: 3.0458 - accuracy: 0.20 - ETA: 3:47 - loss: 3.0477 - accuracy: 0.20 - ETA: 3:46 - loss: 3.0481 - accuracy: 0.20 - ETA: 3:45 - loss: 3.0496 - accuracy: 0.20 - ETA: 3:44 - loss: 3.0498 - accuracy: 0.20 - ETA: 3:43 - loss: 3.0515 - accuracy: 0.20 - ETA: 3:42 - loss: 3.0503 - accuracy: 0.20 - ETA: 3:41 - loss: 3.0484 - accuracy: 0.20 - ETA: 3:40 - loss: 3.0484 - accuracy: 0.20 - ETA: 3:39 - loss: 3.0493 - accuracy: 0.20 - ETA: 3:38 - loss: 3.0497 - accuracy: 0.20 - ETA: 3:37 - loss: 3.0501 - accuracy: 0.20 - ETA: 3:36 - loss: 3.0490 - accuracy: 0.20 - ETA: 3:35 - loss: 3.0486 - accuracy: 0.20 - ETA: 3:34 - loss: 3.0476 - accuracy: 0.20 - ETA: 3:33 - loss: 3.0473 - accuracy: 0.20 - ETA: 3:32 - loss: 3.0458 - accuracy: 0.20 - ETA: 3:31 - loss: 3.0448 - accuracy: 0.20 - ETA: 3:30 - loss: 3.0438 - accuracy: 0.20 - ETA: 3:29 - loss: 3.0454 - accuracy: 0.20 - ETA: 3:28 - loss: 3.0455 - accuracy: 0.20 - ETA: 3:27 - loss: 3.0450 - accuracy: 0.20 - ETA: 3:27 - loss: 3.0457 - accuracy: 0.20 - ETA: 3:26 - loss: 3.0460 - accuracy: 0.20 - ETA: 3:25 - loss: 3.0465 - accuracy: 0.20 - ETA: 3:24 - loss: 3.0453 - accuracy: 0.20 - ETA: 3:23 - loss: 3.0448 - accuracy: 0.20 - ETA: 3:22 - loss: 3.0437 - accuracy: 0.20 - ETA: 3:21 - loss: 3.0444 - accuracy: 0.20 - ETA: 3:20 - loss: 3.0461 - accuracy: 0.20 - ETA: 3:19 - loss: 3.0438 - accuracy: 0.20 - ETA: 3:18 - loss: 3.0429 - accuracy: 0.20 - ETA: 3:17 - loss: 3.0426 - accuracy: 0.20 - ETA: 3:16 - loss: 3.0408 - accuracy: 0.20 - ETA: 3:15 - loss: 3.0389 - accuracy: 0.20 - ETA: 3:14 - loss: 3.0384 - accuracy: 0.20 - ETA: 3:13 - loss: 3.0372 - accuracy: 0.20 - ETA: 3:12 - loss: 3.0365 - accuracy: 0.20 - ETA: 3:11 - loss: 3.0360 - accuracy: 0.20 - ETA: 3:10 - loss: 3.0346 - accuracy: 0.20 - ETA: 3:09 - loss: 3.0346 - accuracy: 0.20 - ETA: 3:09 - loss: 3.0341 - accuracy: 0.20 - ETA: 3:08 - loss: 3.0334 - accuracy: 0.20 - ETA: 3:07 - loss: 3.0351 - accuracy: 0.20 - ETA: 3:06 - loss: 3.0350 - accuracy: 0.20 - ETA: 3:05 - loss: 3.0351 - accuracy: 0.20 - ETA: 3:04 - loss: 3.0352 - accuracy: 0.20 - ETA: 3:03 - loss: 3.0361 - accuracy: 0.20 - ETA: 3:02 - loss: 3.0358 - accuracy: 0.20 - ETA: 3:01 - loss: 3.0340 - accuracy: 0.20 - ETA: 3:00 - loss: 3.0334 - accuracy: 0.20 - ETA: 2:59 - loss: 3.0330 - accuracy: 0.20 - ETA: 2:58 - loss: 3.0336 - accuracy: 0.20 - ETA: 2:57 - loss: 3.0321 - accuracy: 0.20 - ETA: 2:57 - loss: 3.0324 - accuracy: 0.20 - ETA: 2:56 - loss: 3.0326 - accuracy: 0.20 - ETA: 2:55 - loss: 3.0350 - accuracy: 0.20 - ETA: 2:54 - loss: 3.0347 - accuracy: 0.20 - ETA: 2:53 - loss: 3.0353 - accuracy: 0.20 - ETA: 2:52 - loss: 3.0350 - accuracy: 0.20 - ETA: 2:51 - loss: 3.0358 - accuracy: 0.20 - ETA: 2:50 - loss: 3.0373 - accuracy: 0.20 - ETA: 2:49 - loss: 3.0357 - accuracy: 0.20 - ETA: 2:48 - loss: 3.0361 - accuracy: 0.20 - ETA: 2:47 - loss: 3.0367 - accuracy: 0.20 - ETA: 2:46 - loss: 3.0369 - accuracy: 0.20 - ETA: 2:45 - loss: 3.0351 - accuracy: 0.20 - ETA: 2:44 - loss: 3.0355 - accuracy: 0.20 - ETA: 2:43 - loss: 3.0354 - accuracy: 0.20 - ETA: 2:42 - loss: 3.0353 - accuracy: 0.20 - ETA: 2:41 - loss: 3.0407 - accuracy: 0.20 - ETA: 2:40 - loss: 3.0404 - accuracy: 0.20 - ETA: 2:39 - loss: 3.0394 - accuracy: 0.20 - ETA: 2:38 - loss: 3.0417 - accuracy: 0.20 - ETA: 2:37 - loss: 3.0413 - accuracy: 0.20 - ETA: 2:36 - loss: 3.0410 - accuracy: 0.20 - ETA: 2:35 - loss: 3.0404 - accuracy: 0.20 - ETA: 2:34 - loss: 3.0398 - accuracy: 0.20 - ETA: 2:33 - loss: 3.0400 - accuracy: 0.20 - ETA: 2:32 - loss: 3.0402 - accuracy: 0.20 - ETA: 2:32 - loss: 3.0392 - accuracy: 0.20 - ETA: 2:31 - loss: 3.0398 - accuracy: 0.20 - ETA: 2:30 - loss: 3.0405 - accuracy: 0.20 - ETA: 2:29 - loss: 3.0408 - accuracy: 0.20 - ETA: 2:28 - loss: 3.0416 - accuracy: 0.20 - ETA: 2:27 - loss: 3.0413 - accuracy: 0.20 - ETA: 2:26 - loss: 3.0405 - accuracy: 0.20 - ETA: 2:25 - loss: 3.0409 - accuracy: 0.20 - ETA: 2:24 - loss: 3.0404 - accuracy: 0.20 - ETA: 2:23 - loss: 3.0404 - accuracy: 0.20 - ETA: 2:22 - loss: 3.0410 - accuracy: 0.20 - ETA: 2:21 - loss: 3.0411 - accuracy: 0.20 - ETA: 2:20 - loss: 3.0397 - accuracy: 0.20 - ETA: 2:19 - loss: 3.0401 - accuracy: 0.20 - ETA: 2:18 - loss: 3.0399 - accuracy: 0.20 - ETA: 2:17 - loss: 3.0399 - accuracy: 0.2018"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0405 - accuracy: 0.20 - ETA: 2:15 - loss: 3.0401 - accuracy: 0.20 - ETA: 2:14 - loss: 3.0391 - accuracy: 0.20 - ETA: 2:13 - loss: 3.0382 - accuracy: 0.20 - ETA: 2:12 - loss: 3.0373 - accuracy: 0.20 - ETA: 2:11 - loss: 3.0363 - accuracy: 0.20 - ETA: 2:10 - loss: 3.0363 - accuracy: 0.20 - ETA: 2:09 - loss: 3.0349 - accuracy: 0.20 - ETA: 2:09 - loss: 3.0344 - accuracy: 0.20 - ETA: 2:08 - loss: 3.0350 - accuracy: 0.20 - ETA: 2:07 - loss: 3.0362 - accuracy: 0.20 - ETA: 2:06 - loss: 3.0359 - accuracy: 0.20 - ETA: 2:05 - loss: 3.0355 - accuracy: 0.20 - ETA: 2:04 - loss: 3.0354 - accuracy: 0.20 - ETA: 2:03 - loss: 3.0351 - accuracy: 0.20 - ETA: 2:02 - loss: 3.0356 - accuracy: 0.20 - ETA: 2:01 - loss: 3.0352 - accuracy: 0.20 - ETA: 2:00 - loss: 3.0346 - accuracy: 0.20 - ETA: 1:59 - loss: 3.0350 - accuracy: 0.20 - ETA: 1:58 - loss: 3.0347 - accuracy: 0.20 - ETA: 1:57 - loss: 3.0344 - accuracy: 0.20 - ETA: 1:56 - loss: 3.0350 - accuracy: 0.20 - ETA: 1:55 - loss: 3.0343 - accuracy: 0.20 - ETA: 1:54 - loss: 3.0339 - accuracy: 0.20 - ETA: 1:53 - loss: 3.0330 - accuracy: 0.20 - ETA: 1:52 - loss: 3.0319 - accuracy: 0.20 - ETA: 1:52 - loss: 3.0318 - accuracy: 0.20 - ETA: 1:51 - loss: 3.0314 - accuracy: 0.20 - ETA: 1:50 - loss: 3.0322 - accuracy: 0.20 - ETA: 1:49 - loss: 3.0330 - accuracy: 0.20 - ETA: 1:48 - loss: 3.0330 - accuracy: 0.20 - ETA: 1:47 - loss: 3.0336 - accuracy: 0.20 - ETA: 1:46 - loss: 3.0343 - accuracy: 0.20 - ETA: 1:45 - loss: 3.0347 - accuracy: 0.20 - ETA: 1:44 - loss: 3.0351 - accuracy: 0.20 - ETA: 1:43 - loss: 3.0355 - accuracy: 0.20 - ETA: 1:42 - loss: 3.0363 - accuracy: 0.20 - ETA: 1:41 - loss: 3.0382 - accuracy: 0.20 - ETA: 1:40 - loss: 3.0392 - accuracy: 0.20 - ETA: 1:39 - loss: 3.0394 - accuracy: 0.20 - ETA: 1:38 - loss: 3.0403 - accuracy: 0.20 - ETA: 1:37 - loss: 3.0411 - accuracy: 0.20 - ETA: 1:36 - loss: 3.0420 - accuracy: 0.20 - ETA: 1:35 - loss: 3.0426 - accuracy: 0.20 - ETA: 1:35 - loss: 3.0429 - accuracy: 0.20 - ETA: 1:34 - loss: 3.0430 - accuracy: 0.20 - ETA: 1:33 - loss: 3.0434 - accuracy: 0.20 - ETA: 1:32 - loss: 3.0434 - accuracy: 0.20 - ETA: 1:31 - loss: 3.0434 - accuracy: 0.20 - ETA: 1:30 - loss: 3.0442 - accuracy: 0.20 - ETA: 1:29 - loss: 3.0445 - accuracy: 0.20 - ETA: 1:28 - loss: 3.0440 - accuracy: 0.20 - ETA: 1:27 - loss: 3.0446 - accuracy: 0.20 - ETA: 1:26 - loss: 3.0438 - accuracy: 0.20 - ETA: 1:25 - loss: 3.0446 - accuracy: 0.20 - ETA: 1:24 - loss: 3.0449 - accuracy: 0.20 - ETA: 1:23 - loss: 3.0452 - accuracy: 0.20 - ETA: 1:22 - loss: 3.0444 - accuracy: 0.20 - ETA: 1:21 - loss: 3.0447 - accuracy: 0.20 - ETA: 1:20 - loss: 3.0450 - accuracy: 0.20 - ETA: 1:19 - loss: 3.0449 - accuracy: 0.20 - ETA: 1:18 - loss: 3.0451 - accuracy: 0.20 - ETA: 1:17 - loss: 3.0452 - accuracy: 0.20 - ETA: 1:16 - loss: 3.0449 - accuracy: 0.20 - ETA: 1:16 - loss: 3.0456 - accuracy: 0.20 - ETA: 1:15 - loss: 3.0463 - accuracy: 0.20 - ETA: 1:14 - loss: 3.0462 - accuracy: 0.20 - ETA: 1:13 - loss: 3.0460 - accuracy: 0.20 - ETA: 1:12 - loss: 3.0463 - accuracy: 0.20 - ETA: 1:11 - loss: 3.0464 - accuracy: 0.20 - ETA: 1:10 - loss: 3.0461 - accuracy: 0.20 - ETA: 1:09 - loss: 3.0463 - accuracy: 0.20 - ETA: 1:08 - loss: 3.0470 - accuracy: 0.20 - ETA: 1:07 - loss: 3.0469 - accuracy: 0.20 - ETA: 1:06 - loss: 3.0467 - accuracy: 0.20 - ETA: 1:05 - loss: 3.0467 - accuracy: 0.20 - ETA: 1:04 - loss: 3.0466 - accuracy: 0.20 - ETA: 1:03 - loss: 3.0466 - accuracy: 0.20 - ETA: 1:02 - loss: 3.0467 - accuracy: 0.20 - ETA: 1:01 - loss: 3.0472 - accuracy: 0.20 - ETA: 1:00 - loss: 3.0466 - accuracy: 0.20 - ETA: 59s - loss: 3.0458 - accuracy: 0.2012 - ETA: 58s - loss: 3.0455 - accuracy: 0.201 - ETA: 57s - loss: 3.0458 - accuracy: 0.201 - ETA: 56s - loss: 3.0453 - accuracy: 0.201 - ETA: 56s - loss: 3.0458 - accuracy: 0.201 - ETA: 55s - loss: 3.0458 - accuracy: 0.201 - ETA: 54s - loss: 3.0454 - accuracy: 0.201 - ETA: 53s - loss: 3.0452 - accuracy: 0.201 - ETA: 52s - loss: 3.0444 - accuracy: 0.201 - ETA: 51s - loss: 3.0434 - accuracy: 0.201 - ETA: 50s - loss: 3.0438 - accuracy: 0.201 - ETA: 49s - loss: 3.0429 - accuracy: 0.201 - ETA: 48s - loss: 3.0418 - accuracy: 0.201 - ETA: 47s - loss: 3.0416 - accuracy: 0.201 - ETA: 46s - loss: 3.0410 - accuracy: 0.202 - ETA: 45s - loss: 3.0400 - accuracy: 0.202 - ETA: 44s - loss: 3.0404 - accuracy: 0.202 - ETA: 43s - loss: 3.0392 - accuracy: 0.202 - ETA: 42s - loss: 3.0397 - accuracy: 0.202 - ETA: 41s - loss: 3.0402 - accuracy: 0.201 - ETA: 40s - loss: 3.0402 - accuracy: 0.201 - ETA: 39s - loss: 3.0404 - accuracy: 0.201 - ETA: 38s - loss: 3.0398 - accuracy: 0.202 - ETA: 38s - loss: 3.0398 - accuracy: 0.202 - ETA: 37s - loss: 3.0397 - accuracy: 0.202 - ETA: 36s - loss: 3.0397 - accuracy: 0.201 - ETA: 35s - loss: 3.0395 - accuracy: 0.201 - ETA: 34s - loss: 3.0398 - accuracy: 0.201 - ETA: 33s - loss: 3.0395 - accuracy: 0.201 - ETA: 32s - loss: 3.0392 - accuracy: 0.201 - ETA: 31s - loss: 3.0386 - accuracy: 0.201 - ETA: 30s - loss: 3.0386 - accuracy: 0.201 - ETA: 29s - loss: 3.0389 - accuracy: 0.201 - ETA: 28s - loss: 3.0388 - accuracy: 0.201 - ETA: 27s - loss: 3.0392 - accuracy: 0.201 - ETA: 26s - loss: 3.0398 - accuracy: 0.201 - ETA: 25s - loss: 3.0397 - accuracy: 0.201 - ETA: 24s - loss: 3.0396 - accuracy: 0.201 - ETA: 23s - loss: 3.0390 - accuracy: 0.201 - ETA: 22s - loss: 3.0386 - accuracy: 0.201 - ETA: 21s - loss: 3.0384 - accuracy: 0.201 - ETA: 20s - loss: 3.0381 - accuracy: 0.201 - ETA: 20s - loss: 3.0378 - accuracy: 0.201 - ETA: 19s - loss: 3.0380 - accuracy: 0.201 - ETA: 18s - loss: 3.0383 - accuracy: 0.201 - ETA: 17s - loss: 3.0384 - accuracy: 0.201 - ETA: 16s - loss: 3.0383 - accuracy: 0.201 - ETA: 15s - loss: 3.0380 - accuracy: 0.201 - ETA: 14s - loss: 3.0378 - accuracy: 0.201 - ETA: 13s - loss: 3.0377 - accuracy: 0.201 - ETA: 12s - loss: 3.0374 - accuracy: 0.201 - ETA: 11s - loss: 3.0374 - accuracy: 0.201 - ETA: 10s - loss: 3.0371 - accuracy: 0.201 - ETA: 9s - loss: 3.0366 - accuracy: 0.201 - ETA: 8s - loss: 3.0362 - accuracy: 0.20 - ETA: 7s - loss: 3.0356 - accuracy: 0.20 - ETA: 6s - loss: 3.0352 - accuracy: 0.20 - ETA: 5s - loss: 3.0352 - accuracy: 0.20 - ETA: 4s - loss: 3.0351 - accuracy: 0.20 - ETA: 3s - loss: 3.0352 - accuracy: 0.20 - ETA: 2s - loss: 3.0352 - accuracy: 0.20 - ETA: 1s - loss: 3.0360 - accuracy: 0.20 - ETA: 1s - loss: 3.0362 - accuracy: 0.20 - ETA: 0s - loss: 3.0363 - accuracy: 0.20 - 337s 8ms/step - loss: 3.0363 - accuracy: 0.2018 - val_loss: 4.3023 - val_accuracy: 0.0408\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:12 - loss: 3.1336 - accuracy: 0.17 - ETA: 5:07 - loss: 3.0968 - accuracy: 0.19 - ETA: 5:04 - loss: 3.0576 - accuracy: 0.21 - ETA: 5:04 - loss: 2.9512 - accuracy: 0.22 - ETA: 5:04 - loss: 2.9593 - accuracy: 0.21 - ETA: 5:01 - loss: 2.9573 - accuracy: 0.22 - ETA: 5:02 - loss: 2.9763 - accuracy: 0.21 - ETA: 5:01 - loss: 2.9490 - accuracy: 0.21 - ETA: 4:58 - loss: 2.9758 - accuracy: 0.21 - ETA: 4:57 - loss: 2.9736 - accuracy: 0.21 - ETA: 4:57 - loss: 2.9829 - accuracy: 0.20 - ETA: 4:57 - loss: 2.9759 - accuracy: 0.21 - ETA: 4:57 - loss: 2.9719 - accuracy: 0.21 - ETA: 4:56 - loss: 2.9912 - accuracy: 0.21 - ETA: 4:55 - loss: 2.9925 - accuracy: 0.20 - ETA: 4:54 - loss: 3.0002 - accuracy: 0.20 - ETA: 4:54 - loss: 3.0018 - accuracy: 0.20 - ETA: 4:54 - loss: 3.0098 - accuracy: 0.20 - ETA: 4:53 - loss: 3.0265 - accuracy: 0.20 - ETA: 4:52 - loss: 3.0602 - accuracy: 0.20 - ETA: 4:51 - loss: 3.0728 - accuracy: 0.20 - ETA: 4:50 - loss: 3.0719 - accuracy: 0.20 - ETA: 4:50 - loss: 3.0719 - accuracy: 0.20 - ETA: 4:49 - loss: 3.0727 - accuracy: 0.20 - ETA: 4:48 - loss: 3.0843 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0804 - accuracy: 0.20 - ETA: 4:47 - loss: 3.0731 - accuracy: 0.20 - ETA: 4:46 - loss: 3.0751 - accuracy: 0.20 - ETA: 4:45 - loss: 3.0837 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0829 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0795 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0807 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0807 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0790 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0775 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0764 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0786 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0811 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0844 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0856 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0862 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0889 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0836 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0789 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0808 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0815 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0789 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0753 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0782 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0747 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0730 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0712 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0696 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0679 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0663 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0672 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0627 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0666 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0675 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0650 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0639 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0653 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0671 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0655 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0662 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0685 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0663 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0681 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0661 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0630 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0627 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0629 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0620 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0562 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0570 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0563 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0549 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0548 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0549 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0549 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0544 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0544 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0540 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0544 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0557 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0572 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0553 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0565 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0593 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0579 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0577 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0578 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0579 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0597 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0609 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0615 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0610 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0612 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0603 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0611 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0616 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0593 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0572 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0586 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0577 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0569 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0563 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0559 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0566 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0549 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0522 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0524 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0513 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0509 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0484 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0477 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0486 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0470 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0475 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0474 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0463 - accuracy: 0.20 - ETA: 3:18 - loss: 3.0467 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0456 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0437 - accuracy: 0.20 - ETA: 3:15 - loss: 3.0446 - accuracy: 0.20 - ETA: 3:14 - loss: 3.0444 - accuracy: 0.20 - ETA: 3:13 - loss: 3.0442 - accuracy: 0.20 - ETA: 3:12 - loss: 3.0437 - accuracy: 0.20 - ETA: 3:12 - loss: 3.0441 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0452 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0432 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0436 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0445 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0451 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0464 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0477 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0488 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0482 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0475 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0483 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0497 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0504 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0496 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0502 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0507 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0490 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0497 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0499 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0497 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0495 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0490 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0493 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0481 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0477 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0468 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0460 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0455 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0451 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0451 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0444 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0427 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0431 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0431 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0420 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0424 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0420 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0426 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0414 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0427 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0483 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0469 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0460 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0461 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0458 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0461 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0464 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0446 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0443 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0460 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0450 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0459 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0459 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0457 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0450 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0457 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0459 - accuracy: 0.1984"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0444 - accuracy: 0.19 - ETA: 2:16 - loss: 3.0446 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0454 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0460 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0447 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0439 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0436 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0432 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0435 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0440 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0433 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0431 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0429 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0428 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0431 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0434 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0436 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0445 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0433 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0423 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0429 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0434 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0434 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0433 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0425 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0423 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0425 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0421 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0409 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0415 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0406 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0399 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0383 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0377 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0366 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0363 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0358 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0349 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0346 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0337 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0339 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0334 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0322 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0324 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0319 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0320 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0315 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0318 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0311 - accuracy: 0.19 - ETA: 1:30 - loss: 3.0302 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0306 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0300 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0297 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0297 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0284 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0291 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0291 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0287 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0286 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0287 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0284 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0285 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0283 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0289 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0293 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0299 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0299 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0296 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0303 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0299 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0292 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0292 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0297 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0300 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0301 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0295 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0295 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0292 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0291 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0291 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0285 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0283 - accuracy: 0.19 - ETA: 59s - loss: 3.0279 - accuracy: 0.1994 - ETA: 58s - loss: 3.0283 - accuracy: 0.199 - ETA: 57s - loss: 3.0281 - accuracy: 0.199 - ETA: 56s - loss: 3.0282 - accuracy: 0.199 - ETA: 55s - loss: 3.0283 - accuracy: 0.199 - ETA: 54s - loss: 3.0283 - accuracy: 0.199 - ETA: 53s - loss: 3.0287 - accuracy: 0.199 - ETA: 52s - loss: 3.0282 - accuracy: 0.199 - ETA: 51s - loss: 3.0284 - accuracy: 0.199 - ETA: 50s - loss: 3.0281 - accuracy: 0.199 - ETA: 49s - loss: 3.0279 - accuracy: 0.199 - ETA: 48s - loss: 3.0279 - accuracy: 0.199 - ETA: 47s - loss: 3.0271 - accuracy: 0.199 - ETA: 46s - loss: 3.0279 - accuracy: 0.199 - ETA: 45s - loss: 3.0292 - accuracy: 0.199 - ETA: 44s - loss: 3.0295 - accuracy: 0.199 - ETA: 43s - loss: 3.0292 - accuracy: 0.199 - ETA: 43s - loss: 3.0290 - accuracy: 0.199 - ETA: 42s - loss: 3.0285 - accuracy: 0.199 - ETA: 41s - loss: 3.0281 - accuracy: 0.200 - ETA: 40s - loss: 3.0282 - accuracy: 0.200 - ETA: 39s - loss: 3.0288 - accuracy: 0.199 - ETA: 38s - loss: 3.0289 - accuracy: 0.200 - ETA: 37s - loss: 3.0294 - accuracy: 0.199 - ETA: 36s - loss: 3.0300 - accuracy: 0.199 - ETA: 35s - loss: 3.0307 - accuracy: 0.199 - ETA: 34s - loss: 3.0301 - accuracy: 0.199 - ETA: 33s - loss: 3.0309 - accuracy: 0.199 - ETA: 32s - loss: 3.0315 - accuracy: 0.199 - ETA: 31s - loss: 3.0320 - accuracy: 0.199 - ETA: 30s - loss: 3.0321 - accuracy: 0.199 - ETA: 29s - loss: 3.0334 - accuracy: 0.199 - ETA: 28s - loss: 3.0339 - accuracy: 0.199 - ETA: 27s - loss: 3.0344 - accuracy: 0.198 - ETA: 26s - loss: 3.0342 - accuracy: 0.198 - ETA: 25s - loss: 3.0345 - accuracy: 0.198 - ETA: 24s - loss: 3.0342 - accuracy: 0.198 - ETA: 23s - loss: 3.0348 - accuracy: 0.198 - ETA: 22s - loss: 3.0349 - accuracy: 0.198 - ETA: 22s - loss: 3.0355 - accuracy: 0.198 - ETA: 21s - loss: 3.0353 - accuracy: 0.198 - ETA: 20s - loss: 3.0353 - accuracy: 0.198 - ETA: 19s - loss: 3.0353 - accuracy: 0.198 - ETA: 18s - loss: 3.0358 - accuracy: 0.198 - ETA: 17s - loss: 3.0358 - accuracy: 0.198 - ETA: 16s - loss: 3.0365 - accuracy: 0.197 - ETA: 15s - loss: 3.0375 - accuracy: 0.197 - ETA: 14s - loss: 3.0379 - accuracy: 0.197 - ETA: 13s - loss: 3.0379 - accuracy: 0.197 - ETA: 12s - loss: 3.0387 - accuracy: 0.197 - ETA: 11s - loss: 3.0387 - accuracy: 0.197 - ETA: 10s - loss: 3.0392 - accuracy: 0.197 - ETA: 9s - loss: 3.0392 - accuracy: 0.197 - ETA: 8s - loss: 3.0398 - accuracy: 0.19 - ETA: 7s - loss: 3.0396 - accuracy: 0.19 - ETA: 6s - loss: 3.0398 - accuracy: 0.19 - ETA: 5s - loss: 3.0406 - accuracy: 0.19 - ETA: 4s - loss: 3.0406 - accuracy: 0.19 - ETA: 3s - loss: 3.0411 - accuracy: 0.19 - ETA: 2s - loss: 3.0411 - accuracy: 0.19 - ETA: 1s - loss: 3.0412 - accuracy: 0.19 - ETA: 1s - loss: 3.0415 - accuracy: 0.19 - ETA: 0s - loss: 3.0420 - accuracy: 0.19 - 339s 8ms/step - loss: 3.0422 - accuracy: 0.1967 - val_loss: 4.2420 - val_accuracy: 0.0284\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:12 - loss: 3.3351 - accuracy: 0.14 - ETA: 5:15 - loss: 3.2608 - accuracy: 0.15 - ETA: 5:08 - loss: 3.2301 - accuracy: 0.15 - ETA: 5:05 - loss: 3.1760 - accuracy: 0.15 - ETA: 5:04 - loss: 3.1625 - accuracy: 0.16 - ETA: 5:03 - loss: 3.1687 - accuracy: 0.15 - ETA: 5:03 - loss: 3.1686 - accuracy: 0.16 - ETA: 5:03 - loss: 3.1609 - accuracy: 0.16 - ETA: 5:01 - loss: 3.1685 - accuracy: 0.16 - ETA: 4:59 - loss: 3.1749 - accuracy: 0.16 - ETA: 4:59 - loss: 3.1644 - accuracy: 0.16 - ETA: 4:58 - loss: 3.1754 - accuracy: 0.16 - ETA: 4:58 - loss: 3.1725 - accuracy: 0.15 - ETA: 4:57 - loss: 3.1715 - accuracy: 0.16 - ETA: 4:55 - loss: 3.1691 - accuracy: 0.16 - ETA: 4:54 - loss: 3.1685 - accuracy: 0.16 - ETA: 4:53 - loss: 3.1654 - accuracy: 0.16 - ETA: 4:54 - loss: 3.1591 - accuracy: 0.16 - ETA: 4:52 - loss: 3.1581 - accuracy: 0.16 - ETA: 4:51 - loss: 3.1574 - accuracy: 0.16 - ETA: 4:50 - loss: 3.1491 - accuracy: 0.16 - ETA: 4:49 - loss: 3.1356 - accuracy: 0.16 - ETA: 4:48 - loss: 3.1367 - accuracy: 0.16 - ETA: 4:47 - loss: 3.1279 - accuracy: 0.16 - ETA: 4:47 - loss: 3.1227 - accuracy: 0.17 - ETA: 4:46 - loss: 3.1241 - accuracy: 0.17 - ETA: 4:45 - loss: 3.1250 - accuracy: 0.17 - ETA: 4:43 - loss: 3.1240 - accuracy: 0.17 - ETA: 4:43 - loss: 3.1219 - accuracy: 0.17 - ETA: 4:42 - loss: 3.1182 - accuracy: 0.17 - ETA: 4:41 - loss: 3.1168 - accuracy: 0.17 - ETA: 4:40 - loss: 3.1221 - accuracy: 0.17 - ETA: 4:39 - loss: 3.1302 - accuracy: 0.17 - ETA: 4:39 - loss: 3.1297 - accuracy: 0.17 - ETA: 4:38 - loss: 3.1241 - accuracy: 0.17 - ETA: 4:37 - loss: 3.1186 - accuracy: 0.17 - ETA: 4:36 - loss: 3.1209 - accuracy: 0.17 - ETA: 4:35 - loss: 3.1225 - accuracy: 0.17 - ETA: 4:34 - loss: 3.1256 - accuracy: 0.17 - ETA: 4:33 - loss: 3.1203 - accuracy: 0.17 - ETA: 4:32 - loss: 3.1208 - accuracy: 0.17 - ETA: 4:31 - loss: 3.1203 - accuracy: 0.17 - ETA: 4:30 - loss: 3.1192 - accuracy: 0.17 - ETA: 4:29 - loss: 3.1206 - accuracy: 0.17 - ETA: 4:28 - loss: 3.1216 - accuracy: 0.17 - ETA: 4:27 - loss: 3.1182 - accuracy: 0.17 - ETA: 4:26 - loss: 3.1185 - accuracy: 0.17 - ETA: 4:25 - loss: 3.1200 - accuracy: 0.17 - ETA: 4:24 - loss: 3.1215 - accuracy: 0.17 - ETA: 4:23 - loss: 3.1224 - accuracy: 0.17 - ETA: 4:22 - loss: 3.1199 - accuracy: 0.17 - ETA: 4:22 - loss: 3.1220 - accuracy: 0.17 - ETA: 4:21 - loss: 3.1227 - accuracy: 0.17 - ETA: 4:20 - loss: 3.1196 - accuracy: 0.17 - ETA: 4:20 - loss: 3.1182 - accuracy: 0.17 - ETA: 4:19 - loss: 3.1176 - accuracy: 0.17 - ETA: 4:18 - loss: 3.1152 - accuracy: 0.17 - ETA: 4:17 - loss: 3.1159 - accuracy: 0.17 - ETA: 4:16 - loss: 3.1158 - accuracy: 0.17 - ETA: 4:15 - loss: 3.1157 - accuracy: 0.17 - ETA: 4:14 - loss: 3.1128 - accuracy: 0.17 - ETA: 4:13 - loss: 3.1107 - accuracy: 0.17 - ETA: 4:12 - loss: 3.1100 - accuracy: 0.17 - ETA: 4:11 - loss: 3.1115 - accuracy: 0.17 - ETA: 4:10 - loss: 3.1136 - accuracy: 0.17 - ETA: 4:09 - loss: 3.1102 - accuracy: 0.17 - ETA: 4:09 - loss: 3.1091 - accuracy: 0.17 - ETA: 4:07 - loss: 3.1099 - accuracy: 0.18 - ETA: 4:06 - loss: 3.1075 - accuracy: 0.18 - ETA: 4:05 - loss: 3.1064 - accuracy: 0.18 - ETA: 4:05 - loss: 3.1059 - accuracy: 0.18 - ETA: 4:04 - loss: 3.1056 - accuracy: 0.18 - ETA: 4:03 - loss: 3.1057 - accuracy: 0.18 - ETA: 4:02 - loss: 3.1065 - accuracy: 0.18 - ETA: 4:01 - loss: 3.1062 - accuracy: 0.18 - ETA: 4:00 - loss: 3.1032 - accuracy: 0.18 - ETA: 3:59 - loss: 3.1021 - accuracy: 0.18 - ETA: 3:58 - loss: 3.1016 - accuracy: 0.18 - ETA: 3:57 - loss: 3.1023 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0999 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0998 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0991 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0975 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0969 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0984 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0958 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0947 - accuracy: 0.18 - ETA: 3:49 - loss: 3.0954 - accuracy: 0.18 - ETA: 3:48 - loss: 3.0988 - accuracy: 0.18 - ETA: 3:47 - loss: 3.0985 - accuracy: 0.18 - ETA: 3:46 - loss: 3.0968 - accuracy: 0.18 - ETA: 3:45 - loss: 3.0934 - accuracy: 0.18 - ETA: 3:44 - loss: 3.0915 - accuracy: 0.18 - ETA: 3:43 - loss: 3.0897 - accuracy: 0.18 - ETA: 3:42 - loss: 3.0873 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0854 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0846 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0845 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0855 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0860 - accuracy: 0.18 - ETA: 3:37 - loss: 3.0842 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0855 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0856 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0838 - accuracy: 0.18 - ETA: 3:33 - loss: 3.0824 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0827 - accuracy: 0.18 - ETA: 3:31 - loss: 3.0840 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0830 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0840 - accuracy: 0.18 - ETA: 3:28 - loss: 3.0834 - accuracy: 0.18 - ETA: 3:27 - loss: 3.0843 - accuracy: 0.18 - ETA: 3:26 - loss: 3.0847 - accuracy: 0.18 - ETA: 3:25 - loss: 3.0838 - accuracy: 0.18 - ETA: 3:24 - loss: 3.0825 - accuracy: 0.18 - ETA: 3:24 - loss: 3.0810 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0830 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0809 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0824 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0816 - accuracy: 0.18 - ETA: 3:19 - loss: 3.0798 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0808 - accuracy: 0.18 - ETA: 3:17 - loss: 3.0810 - accuracy: 0.18 - ETA: 3:16 - loss: 3.0789 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0787 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0793 - accuracy: 0.18 - ETA: 3:13 - loss: 3.0804 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0806 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0812 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0818 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0822 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0804 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0804 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0796 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0808 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0806 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0804 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0811 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0791 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0804 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0805 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0805 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0807 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0807 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0800 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0797 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0807 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0817 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0823 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0838 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0840 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0852 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0856 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0846 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0859 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0861 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0836 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0847 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0855 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0849 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0861 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0866 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0871 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0873 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0871 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0878 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0879 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0885 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0902 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0908 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0908 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0915 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0906 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0895 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0890 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0878 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0888 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0894 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0883 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0893 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0888 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0888 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0884 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0893 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0887 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0878 - accuracy: 0.18 - ETA: 2:17 - loss: 3.0883 - accuracy: 0.1859"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0886 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0884 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0883 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0878 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0877 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0876 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0873 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0873 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0865 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0862 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0865 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0865 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0858 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0847 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0839 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0830 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0837 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0837 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0838 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0836 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0837 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0834 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0831 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0823 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0824 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0819 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0834 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0832 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0834 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0829 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0827 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0827 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0819 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0812 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0809 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0802 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0808 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0807 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0799 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0812 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0813 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0810 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0813 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0810 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0805 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0798 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0803 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0798 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0787 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0788 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0786 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0782 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0789 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0784 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0782 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0784 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0782 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0774 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0778 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0775 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0777 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0776 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0778 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0778 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0775 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0772 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0771 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0760 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0759 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0762 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0765 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0778 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0774 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0774 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0780 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0782 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0785 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0783 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0780 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0779 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0779 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0783 - accuracy: 0.18 - ETA: 59s - loss: 3.0780 - accuracy: 0.1896 - ETA: 58s - loss: 3.0785 - accuracy: 0.189 - ETA: 57s - loss: 3.0782 - accuracy: 0.189 - ETA: 56s - loss: 3.0787 - accuracy: 0.189 - ETA: 55s - loss: 3.0787 - accuracy: 0.189 - ETA: 54s - loss: 3.0783 - accuracy: 0.189 - ETA: 53s - loss: 3.0782 - accuracy: 0.189 - ETA: 52s - loss: 3.0781 - accuracy: 0.189 - ETA: 51s - loss: 3.0780 - accuracy: 0.189 - ETA: 50s - loss: 3.0783 - accuracy: 0.189 - ETA: 49s - loss: 3.0793 - accuracy: 0.189 - ETA: 48s - loss: 3.0793 - accuracy: 0.189 - ETA: 47s - loss: 3.0797 - accuracy: 0.189 - ETA: 46s - loss: 3.0795 - accuracy: 0.189 - ETA: 45s - loss: 3.0797 - accuracy: 0.189 - ETA: 44s - loss: 3.0797 - accuracy: 0.189 - ETA: 43s - loss: 3.0800 - accuracy: 0.189 - ETA: 42s - loss: 3.0811 - accuracy: 0.189 - ETA: 41s - loss: 3.0818 - accuracy: 0.188 - ETA: 40s - loss: 3.0820 - accuracy: 0.189 - ETA: 40s - loss: 3.0816 - accuracy: 0.189 - ETA: 39s - loss: 3.0818 - accuracy: 0.188 - ETA: 38s - loss: 3.0815 - accuracy: 0.189 - ETA: 37s - loss: 3.0819 - accuracy: 0.188 - ETA: 36s - loss: 3.0819 - accuracy: 0.188 - ETA: 35s - loss: 3.0811 - accuracy: 0.188 - ETA: 34s - loss: 3.0824 - accuracy: 0.188 - ETA: 33s - loss: 3.0825 - accuracy: 0.188 - ETA: 32s - loss: 3.0824 - accuracy: 0.188 - ETA: 31s - loss: 3.0825 - accuracy: 0.188 - ETA: 30s - loss: 3.0825 - accuracy: 0.188 - ETA: 29s - loss: 3.0831 - accuracy: 0.188 - ETA: 28s - loss: 3.0831 - accuracy: 0.188 - ETA: 27s - loss: 3.0830 - accuracy: 0.188 - ETA: 26s - loss: 3.0830 - accuracy: 0.188 - ETA: 25s - loss: 3.0829 - accuracy: 0.188 - ETA: 24s - loss: 3.0823 - accuracy: 0.188 - ETA: 23s - loss: 3.0821 - accuracy: 0.188 - ETA: 22s - loss: 3.0815 - accuracy: 0.188 - ETA: 21s - loss: 3.0811 - accuracy: 0.188 - ETA: 21s - loss: 3.0807 - accuracy: 0.188 - ETA: 20s - loss: 3.0809 - accuracy: 0.188 - ETA: 19s - loss: 3.0813 - accuracy: 0.188 - ETA: 18s - loss: 3.0813 - accuracy: 0.188 - ETA: 17s - loss: 3.0806 - accuracy: 0.188 - ETA: 16s - loss: 3.0809 - accuracy: 0.188 - ETA: 15s - loss: 3.0816 - accuracy: 0.188 - ETA: 14s - loss: 3.0817 - accuracy: 0.188 - ETA: 13s - loss: 3.0812 - accuracy: 0.188 - ETA: 12s - loss: 3.0815 - accuracy: 0.188 - ETA: 11s - loss: 3.0812 - accuracy: 0.188 - ETA: 10s - loss: 3.0815 - accuracy: 0.188 - ETA: 9s - loss: 3.0814 - accuracy: 0.188 - ETA: 8s - loss: 3.0813 - accuracy: 0.18 - ETA: 7s - loss: 3.0816 - accuracy: 0.18 - ETA: 6s - loss: 3.0822 - accuracy: 0.18 - ETA: 5s - loss: 3.0830 - accuracy: 0.18 - ETA: 4s - loss: 3.0831 - accuracy: 0.18 - ETA: 3s - loss: 3.0836 - accuracy: 0.18 - ETA: 2s - loss: 3.0835 - accuracy: 0.18 - ETA: 1s - loss: 3.0830 - accuracy: 0.18 - ETA: 1s - loss: 3.0830 - accuracy: 0.18 - ETA: 0s - loss: 3.0830 - accuracy: 0.18 - 338s 8ms/step - loss: 3.0829 - accuracy: 0.1876 - val_loss: 4.4273 - val_accuracy: 0.0351\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:18 - loss: 3.0432 - accuracy: 0.24 - ETA: 5:11 - loss: 3.0900 - accuracy: 0.20 - ETA: 5:14 - loss: 3.0336 - accuracy: 0.20 - ETA: 5:15 - loss: 3.0518 - accuracy: 0.19 - ETA: 5:13 - loss: 3.0851 - accuracy: 0.17 - ETA: 5:10 - loss: 3.1027 - accuracy: 0.17 - ETA: 5:08 - loss: 3.0754 - accuracy: 0.18 - ETA: 5:08 - loss: 3.0665 - accuracy: 0.18 - ETA: 5:07 - loss: 3.0787 - accuracy: 0.19 - ETA: 5:05 - loss: 3.0893 - accuracy: 0.18 - ETA: 5:03 - loss: 3.0699 - accuracy: 0.19 - ETA: 5:02 - loss: 3.0803 - accuracy: 0.19 - ETA: 5:02 - loss: 3.0738 - accuracy: 0.18 - ETA: 5:01 - loss: 3.0572 - accuracy: 0.19 - ETA: 5:00 - loss: 3.0551 - accuracy: 0.19 - ETA: 4:59 - loss: 3.0615 - accuracy: 0.19 - ETA: 4:59 - loss: 3.0683 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0629 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0617 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0613 - accuracy: 0.19 - ETA: 4:54 - loss: 3.0724 - accuracy: 0.18 - ETA: 4:54 - loss: 3.0664 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0702 - accuracy: 0.18 - ETA: 4:52 - loss: 3.0753 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0716 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0739 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0675 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0658 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0689 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0662 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0663 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0683 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0656 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0636 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0644 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0679 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0681 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0646 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0590 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0588 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0577 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0587 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0564 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0546 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0496 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0512 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0511 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0493 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0509 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0488 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0446 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0447 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0461 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0477 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0495 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0510 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0505 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0649 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0670 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0690 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0692 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0665 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0681 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0689 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0689 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0675 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0681 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0684 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0665 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0674 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0664 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0656 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0634 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0593 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0593 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0597 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0607 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0626 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0616 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0636 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0614 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0568 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0575 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0594 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0549 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0550 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0543 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0522 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0522 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0526 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0519 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0540 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0564 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0568 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0569 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0570 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0560 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0541 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0537 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0541 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0559 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0562 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0556 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0531 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0539 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0493 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0469 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0464 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0458 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0444 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0436 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0441 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0430 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0449 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0440 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0439 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0477 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0484 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0490 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0503 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0498 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0509 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0481 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0478 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0469 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0480 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0491 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0478 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0487 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0487 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0473 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0468 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0456 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0454 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0447 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0432 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0434 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0424 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0434 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0429 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0430 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0417 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0451 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0465 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0465 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0460 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0455 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0464 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0466 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0468 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0463 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0456 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0452 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0453 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0431 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0431 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0440 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0436 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0428 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0431 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0417 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0401 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0383 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0381 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0366 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0379 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0378 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0376 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0375 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0360 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0347 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0335 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0338 - accuracy: 0.20 - ETA: 2:29 - loss: 3.0336 - accuracy: 0.20 - ETA: 2:28 - loss: 3.0343 - accuracy: 0.20 - ETA: 2:27 - loss: 3.0334 - accuracy: 0.20 - ETA: 2:26 - loss: 3.0327 - accuracy: 0.20 - ETA: 2:25 - loss: 3.0327 - accuracy: 0.20 - ETA: 2:24 - loss: 3.0326 - accuracy: 0.20 - ETA: 2:23 - loss: 3.0332 - accuracy: 0.20 - ETA: 2:22 - loss: 3.0316 - accuracy: 0.20 - ETA: 2:21 - loss: 3.0322 - accuracy: 0.20 - ETA: 2:20 - loss: 3.0316 - accuracy: 0.20 - ETA: 2:19 - loss: 3.0319 - accuracy: 0.20 - ETA: 2:19 - loss: 3.0317 - accuracy: 0.20 - ETA: 2:18 - loss: 3.0316 - accuracy: 0.2013"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0324 - accuracy: 0.20 - ETA: 2:16 - loss: 3.0325 - accuracy: 0.20 - ETA: 2:15 - loss: 3.0343 - accuracy: 0.20 - ETA: 2:14 - loss: 3.0333 - accuracy: 0.20 - ETA: 2:13 - loss: 3.0328 - accuracy: 0.20 - ETA: 2:12 - loss: 3.0342 - accuracy: 0.20 - ETA: 2:11 - loss: 3.0348 - accuracy: 0.20 - ETA: 2:10 - loss: 3.0342 - accuracy: 0.20 - ETA: 2:09 - loss: 3.0340 - accuracy: 0.20 - ETA: 2:08 - loss: 3.0344 - accuracy: 0.20 - ETA: 2:07 - loss: 3.0346 - accuracy: 0.20 - ETA: 2:06 - loss: 3.0342 - accuracy: 0.20 - ETA: 2:05 - loss: 3.0341 - accuracy: 0.20 - ETA: 2:03 - loss: 3.0348 - accuracy: 0.20 - ETA: 2:02 - loss: 3.0345 - accuracy: 0.20 - ETA: 2:01 - loss: 3.0356 - accuracy: 0.20 - ETA: 2:00 - loss: 3.0357 - accuracy: 0.20 - ETA: 1:59 - loss: 3.0358 - accuracy: 0.20 - ETA: 1:59 - loss: 3.0359 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0359 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0358 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0364 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0365 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0372 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0378 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0380 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0392 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0395 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0389 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0403 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0415 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0406 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0408 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0409 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0401 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0397 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0392 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0407 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0406 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0410 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0420 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0417 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0422 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0425 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0425 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0422 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0428 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0430 - accuracy: 0.19 - ETA: 1:30 - loss: 3.0436 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0447 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0466 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0463 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0466 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0469 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0468 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0475 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0477 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0481 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0480 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0480 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0479 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0481 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0475 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0477 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0470 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0472 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0475 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0486 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0493 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0495 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0500 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0505 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0497 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0493 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0502 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0510 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0514 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0513 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0515 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0505 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0504 - accuracy: 0.19 - ETA: 59s - loss: 3.0504 - accuracy: 0.1962 - ETA: 58s - loss: 3.0505 - accuracy: 0.196 - ETA: 57s - loss: 3.0500 - accuracy: 0.196 - ETA: 56s - loss: 3.0495 - accuracy: 0.196 - ETA: 56s - loss: 3.0492 - accuracy: 0.196 - ETA: 55s - loss: 3.0495 - accuracy: 0.196 - ETA: 54s - loss: 3.0495 - accuracy: 0.196 - ETA: 53s - loss: 3.0533 - accuracy: 0.196 - ETA: 52s - loss: 3.0530 - accuracy: 0.196 - ETA: 51s - loss: 3.0533 - accuracy: 0.196 - ETA: 50s - loss: 3.0527 - accuracy: 0.196 - ETA: 49s - loss: 3.0527 - accuracy: 0.196 - ETA: 48s - loss: 3.0524 - accuracy: 0.196 - ETA: 47s - loss: 3.0518 - accuracy: 0.196 - ETA: 46s - loss: 3.0520 - accuracy: 0.196 - ETA: 45s - loss: 3.0515 - accuracy: 0.196 - ETA: 44s - loss: 3.0545 - accuracy: 0.196 - ETA: 43s - loss: 3.0543 - accuracy: 0.196 - ETA: 42s - loss: 3.0542 - accuracy: 0.196 - ETA: 41s - loss: 3.0546 - accuracy: 0.195 - ETA: 40s - loss: 3.0540 - accuracy: 0.196 - ETA: 39s - loss: 3.0542 - accuracy: 0.196 - ETA: 38s - loss: 3.0544 - accuracy: 0.196 - ETA: 38s - loss: 3.0549 - accuracy: 0.195 - ETA: 37s - loss: 3.0552 - accuracy: 0.195 - ETA: 36s - loss: 3.0548 - accuracy: 0.195 - ETA: 35s - loss: 3.0553 - accuracy: 0.195 - ETA: 34s - loss: 3.0556 - accuracy: 0.195 - ETA: 33s - loss: 3.0556 - accuracy: 0.195 - ETA: 32s - loss: 3.0553 - accuracy: 0.195 - ETA: 31s - loss: 3.0552 - accuracy: 0.195 - ETA: 30s - loss: 3.0554 - accuracy: 0.195 - ETA: 29s - loss: 3.0549 - accuracy: 0.196 - ETA: 28s - loss: 3.0558 - accuracy: 0.195 - ETA: 27s - loss: 3.0555 - accuracy: 0.196 - ETA: 26s - loss: 3.0548 - accuracy: 0.196 - ETA: 25s - loss: 3.0545 - accuracy: 0.196 - ETA: 24s - loss: 3.0551 - accuracy: 0.196 - ETA: 23s - loss: 3.0559 - accuracy: 0.196 - ETA: 22s - loss: 3.0553 - accuracy: 0.196 - ETA: 21s - loss: 3.0551 - accuracy: 0.196 - ETA: 20s - loss: 3.0552 - accuracy: 0.196 - ETA: 20s - loss: 3.0558 - accuracy: 0.196 - ETA: 19s - loss: 3.0557 - accuracy: 0.196 - ETA: 18s - loss: 3.0561 - accuracy: 0.195 - ETA: 17s - loss: 3.0563 - accuracy: 0.195 - ETA: 16s - loss: 3.0559 - accuracy: 0.195 - ETA: 15s - loss: 3.0561 - accuracy: 0.195 - ETA: 14s - loss: 3.0571 - accuracy: 0.195 - ETA: 13s - loss: 3.0574 - accuracy: 0.195 - ETA: 12s - loss: 3.0577 - accuracy: 0.195 - ETA: 11s - loss: 3.0578 - accuracy: 0.195 - ETA: 10s - loss: 3.0576 - accuracy: 0.195 - ETA: 9s - loss: 3.0574 - accuracy: 0.195 - ETA: 8s - loss: 3.0569 - accuracy: 0.19 - ETA: 7s - loss: 3.0570 - accuracy: 0.19 - ETA: 6s - loss: 3.0569 - accuracy: 0.19 - ETA: 5s - loss: 3.0574 - accuracy: 0.19 - ETA: 4s - loss: 3.0575 - accuracy: 0.19 - ETA: 3s - loss: 3.0577 - accuracy: 0.19 - ETA: 2s - loss: 3.0575 - accuracy: 0.19 - ETA: 1s - loss: 3.0578 - accuracy: 0.19 - ETA: 1s - loss: 3.0578 - accuracy: 0.19 - ETA: 0s - loss: 3.0578 - accuracy: 0.19 - 338s 8ms/step - loss: 3.0579 - accuracy: 0.1948 - val_loss: 4.3697 - val_accuracy: 0.0342\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:13 - loss: 2.8563 - accuracy: 0.25 - ETA: 5:17 - loss: 2.9969 - accuracy: 0.21 - ETA: 5:12 - loss: 3.0025 - accuracy: 0.20 - ETA: 5:11 - loss: 3.0167 - accuracy: 0.20 - ETA: 5:09 - loss: 3.0254 - accuracy: 0.20 - ETA: 5:08 - loss: 3.0197 - accuracy: 0.20 - ETA: 5:06 - loss: 2.9924 - accuracy: 0.20 - ETA: 5:04 - loss: 2.9787 - accuracy: 0.21 - ETA: 5:03 - loss: 3.0108 - accuracy: 0.20 - ETA: 5:01 - loss: 3.0080 - accuracy: 0.20 - ETA: 5:01 - loss: 3.0052 - accuracy: 0.20 - ETA: 5:01 - loss: 3.0148 - accuracy: 0.20 - ETA: 5:00 - loss: 3.0192 - accuracy: 0.20 - ETA: 4:59 - loss: 3.0137 - accuracy: 0.20 - ETA: 4:59 - loss: 3.0249 - accuracy: 0.20 - ETA: 4:59 - loss: 3.0293 - accuracy: 0.20 - ETA: 4:58 - loss: 3.0347 - accuracy: 0.20 - ETA: 4:56 - loss: 3.0442 - accuracy: 0.19 - ETA: 4:55 - loss: 3.0256 - accuracy: 0.20 - ETA: 4:54 - loss: 3.0319 - accuracy: 0.20 - ETA: 4:53 - loss: 3.0330 - accuracy: 0.20 - ETA: 4:52 - loss: 3.0392 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0415 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0452 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0402 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0441 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0393 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0357 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0325 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0302 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0355 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0363 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0365 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0382 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0413 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0388 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0440 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0346 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0338 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0387 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0420 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0418 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0384 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0391 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0434 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0402 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0377 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0405 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0387 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0337 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0346 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0379 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0382 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0409 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0387 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0333 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0345 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0298 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0287 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0286 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0291 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0307 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0294 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0301 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0286 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0314 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0300 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0304 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0282 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0264 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0268 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0245 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0230 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0221 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0241 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0284 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0292 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0289 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0300 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0272 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0295 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0287 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0261 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0275 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0270 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0250 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0240 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0269 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0280 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0287 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0292 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0289 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0254 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0250 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0262 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0271 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0270 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0261 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0262 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0250 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0238 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0223 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0202 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0216 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0208 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0223 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0206 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0181 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0177 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0177 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0186 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0190 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0189 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0195 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0194 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0180 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0173 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0178 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0169 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0171 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0186 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0164 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0159 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0148 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0151 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0159 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0154 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0156 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0154 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0165 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0165 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0169 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0189 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0227 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0255 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0249 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0252 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0256 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0245 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0246 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0247 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0247 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0256 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0263 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0261 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0230 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0229 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0235 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0226 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0202 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0184 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0186 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0180 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0175 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0190 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0213 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0215 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0228 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0256 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0286 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0300 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0306 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0304 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0309 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0308 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0312 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0315 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0306 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0311 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0307 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0303 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0306 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0303 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0308 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0312 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0314 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0323 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0334 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0323 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0323 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0318 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0320 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0324 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0336 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0342 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0351 - accuracy: 0.1974"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0366 - accuracy: 0.19 - ETA: 2:16 - loss: 3.0369 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0360 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0350 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0422 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0424 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0436 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0435 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0427 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0419 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0404 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0407 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0406 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0407 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0412 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0405 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0415 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0415 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0412 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0415 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0406 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0404 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0404 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0395 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0405 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0394 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0397 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0397 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0385 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0380 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0389 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0395 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0394 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0402 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0405 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0399 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0431 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0427 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0422 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0407 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0410 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0404 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0398 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0400 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0399 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0398 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0394 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0395 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0396 - accuracy: 0.19 - ETA: 1:30 - loss: 3.0401 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0397 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0401 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0402 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0409 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0406 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0401 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0406 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0408 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0412 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0413 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0411 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0417 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0418 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0425 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0425 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0426 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0429 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0434 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0430 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0436 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0431 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0440 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0446 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0456 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0458 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0460 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0457 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0455 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0458 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0456 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0459 - accuracy: 0.19 - ETA: 59s - loss: 3.0454 - accuracy: 0.1960 - ETA: 58s - loss: 3.0460 - accuracy: 0.195 - ETA: 57s - loss: 3.0457 - accuracy: 0.195 - ETA: 56s - loss: 3.0458 - accuracy: 0.195 - ETA: 55s - loss: 3.0459 - accuracy: 0.195 - ETA: 54s - loss: 3.0461 - accuracy: 0.195 - ETA: 54s - loss: 3.0457 - accuracy: 0.195 - ETA: 53s - loss: 3.0467 - accuracy: 0.195 - ETA: 52s - loss: 3.0466 - accuracy: 0.195 - ETA: 51s - loss: 3.0470 - accuracy: 0.195 - ETA: 50s - loss: 3.0472 - accuracy: 0.195 - ETA: 49s - loss: 3.0478 - accuracy: 0.195 - ETA: 48s - loss: 3.0474 - accuracy: 0.195 - ETA: 47s - loss: 3.0472 - accuracy: 0.195 - ETA: 46s - loss: 3.0472 - accuracy: 0.195 - ETA: 45s - loss: 3.0472 - accuracy: 0.195 - ETA: 44s - loss: 3.0472 - accuracy: 0.195 - ETA: 43s - loss: 3.0472 - accuracy: 0.195 - ETA: 42s - loss: 3.0476 - accuracy: 0.195 - ETA: 41s - loss: 3.0475 - accuracy: 0.195 - ETA: 40s - loss: 3.0485 - accuracy: 0.195 - ETA: 39s - loss: 3.0486 - accuracy: 0.195 - ETA: 38s - loss: 3.0488 - accuracy: 0.195 - ETA: 37s - loss: 3.0492 - accuracy: 0.195 - ETA: 36s - loss: 3.0488 - accuracy: 0.195 - ETA: 35s - loss: 3.0479 - accuracy: 0.195 - ETA: 35s - loss: 3.0475 - accuracy: 0.195 - ETA: 34s - loss: 3.0473 - accuracy: 0.195 - ETA: 33s - loss: 3.0480 - accuracy: 0.195 - ETA: 32s - loss: 3.0483 - accuracy: 0.195 - ETA: 31s - loss: 3.0482 - accuracy: 0.195 - ETA: 30s - loss: 3.0475 - accuracy: 0.195 - ETA: 29s - loss: 3.0477 - accuracy: 0.195 - ETA: 28s - loss: 3.0476 - accuracy: 0.195 - ETA: 27s - loss: 3.0475 - accuracy: 0.196 - ETA: 26s - loss: 3.0472 - accuracy: 0.196 - ETA: 25s - loss: 3.0474 - accuracy: 0.195 - ETA: 24s - loss: 3.0474 - accuracy: 0.196 - ETA: 23s - loss: 3.0470 - accuracy: 0.196 - ETA: 22s - loss: 3.0475 - accuracy: 0.195 - ETA: 21s - loss: 3.0480 - accuracy: 0.195 - ETA: 20s - loss: 3.0486 - accuracy: 0.195 - ETA: 19s - loss: 3.0493 - accuracy: 0.195 - ETA: 18s - loss: 3.0508 - accuracy: 0.195 - ETA: 18s - loss: 3.0513 - accuracy: 0.195 - ETA: 17s - loss: 3.0518 - accuracy: 0.195 - ETA: 16s - loss: 3.0511 - accuracy: 0.195 - ETA: 15s - loss: 3.0511 - accuracy: 0.195 - ETA: 14s - loss: 3.0511 - accuracy: 0.195 - ETA: 13s - loss: 3.0531 - accuracy: 0.195 - ETA: 12s - loss: 3.0527 - accuracy: 0.195 - ETA: 11s - loss: 3.0519 - accuracy: 0.196 - ETA: 10s - loss: 3.0524 - accuracy: 0.195 - ETA: 9s - loss: 3.0526 - accuracy: 0.195 - ETA: 8s - loss: 3.0522 - accuracy: 0.19 - ETA: 7s - loss: 3.0519 - accuracy: 0.19 - ETA: 6s - loss: 3.0520 - accuracy: 0.19 - ETA: 5s - loss: 3.0519 - accuracy: 0.19 - ETA: 4s - loss: 3.0517 - accuracy: 0.19 - ETA: 3s - loss: 3.0517 - accuracy: 0.19 - ETA: 2s - loss: 3.0520 - accuracy: 0.19 - ETA: 1s - loss: 3.0514 - accuracy: 0.19 - ETA: 1s - loss: 3.0623 - accuracy: 0.19 - ETA: 0s - loss: 3.0623 - accuracy: 0.19 - 335s 8ms/step - loss: 3.0624 - accuracy: 0.1958 - val_loss: 4.1669 - val_accuracy: 0.0406\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:03 - loss: 3.4442 - accuracy: 0.12 - ETA: 5:07 - loss: 3.1998 - accuracy: 0.14 - ETA: 5:06 - loss: 3.1513 - accuracy: 0.15 - ETA: 5:07 - loss: 3.1633 - accuracy: 0.15 - ETA: 5:09 - loss: 3.1148 - accuracy: 0.16 - ETA: 5:07 - loss: 3.0890 - accuracy: 0.17 - ETA: 5:07 - loss: 3.0831 - accuracy: 0.17 - ETA: 5:05 - loss: 3.1131 - accuracy: 0.16 - ETA: 5:04 - loss: 3.1125 - accuracy: 0.17 - ETA: 5:04 - loss: 3.0946 - accuracy: 0.17 - ETA: 5:05 - loss: 3.0693 - accuracy: 0.18 - ETA: 5:02 - loss: 3.0768 - accuracy: 0.18 - ETA: 5:01 - loss: 3.0615 - accuracy: 0.18 - ETA: 4:59 - loss: 3.0439 - accuracy: 0.19 - ETA: 4:59 - loss: 3.0587 - accuracy: 0.19 - ETA: 4:57 - loss: 3.0565 - accuracy: 0.18 - ETA: 4:56 - loss: 3.0575 - accuracy: 0.19 - ETA: 4:54 - loss: 3.0595 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0723 - accuracy: 0.18 - ETA: 4:53 - loss: 3.0815 - accuracy: 0.18 - ETA: 4:52 - loss: 3.0799 - accuracy: 0.18 - ETA: 4:52 - loss: 3.0648 - accuracy: 0.18 - ETA: 4:51 - loss: 3.0584 - accuracy: 0.18 - ETA: 4:50 - loss: 3.0566 - accuracy: 0.18 - ETA: 4:50 - loss: 3.0607 - accuracy: 0.18 - ETA: 4:48 - loss: 3.0573 - accuracy: 0.18 - ETA: 4:49 - loss: 3.0536 - accuracy: 0.18 - ETA: 4:48 - loss: 3.0549 - accuracy: 0.18 - ETA: 4:47 - loss: 3.0554 - accuracy: 0.18 - ETA: 4:47 - loss: 3.0542 - accuracy: 0.18 - ETA: 4:47 - loss: 3.0501 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0485 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0455 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0469 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0458 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0472 - accuracy: 0.18 - ETA: 4:40 - loss: 3.0491 - accuracy: 0.18 - ETA: 4:39 - loss: 3.0412 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0355 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0348 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0333 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0316 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0313 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0303 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0290 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0293 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0333 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0348 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0331 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0331 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0274 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0285 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0295 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0298 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0362 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0355 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0387 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0383 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0412 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0443 - accuracy: 0.18 - ETA: 4:17 - loss: 3.0479 - accuracy: 0.18 - ETA: 4:16 - loss: 3.0498 - accuracy: 0.18 - ETA: 4:15 - loss: 3.0492 - accuracy: 0.18 - ETA: 4:14 - loss: 3.0483 - accuracy: 0.18 - ETA: 4:13 - loss: 3.0477 - accuracy: 0.18 - ETA: 4:12 - loss: 3.0486 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0513 - accuracy: 0.18 - ETA: 4:10 - loss: 3.0514 - accuracy: 0.18 - ETA: 4:09 - loss: 3.0525 - accuracy: 0.18 - ETA: 4:08 - loss: 3.0519 - accuracy: 0.18 - ETA: 4:07 - loss: 3.0541 - accuracy: 0.18 - ETA: 4:07 - loss: 3.0611 - accuracy: 0.18 - ETA: 4:06 - loss: 3.0627 - accuracy: 0.18 - ETA: 4:05 - loss: 3.0658 - accuracy: 0.18 - ETA: 4:04 - loss: 3.0639 - accuracy: 0.18 - ETA: 4:03 - loss: 3.0641 - accuracy: 0.18 - ETA: 4:02 - loss: 3.0632 - accuracy: 0.18 - ETA: 4:01 - loss: 3.0618 - accuracy: 0.18 - ETA: 4:00 - loss: 3.0621 - accuracy: 0.18 - ETA: 3:59 - loss: 3.0618 - accuracy: 0.18 - ETA: 3:58 - loss: 3.0633 - accuracy: 0.18 - ETA: 3:57 - loss: 3.0657 - accuracy: 0.18 - ETA: 3:56 - loss: 3.0675 - accuracy: 0.18 - ETA: 3:55 - loss: 3.0700 - accuracy: 0.18 - ETA: 3:54 - loss: 3.0720 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0706 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0711 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0728 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0727 - accuracy: 0.18 - ETA: 3:49 - loss: 3.0704 - accuracy: 0.18 - ETA: 3:49 - loss: 3.0690 - accuracy: 0.18 - ETA: 3:48 - loss: 3.0679 - accuracy: 0.18 - ETA: 3:47 - loss: 3.0675 - accuracy: 0.18 - ETA: 3:46 - loss: 3.0676 - accuracy: 0.18 - ETA: 3:45 - loss: 3.0680 - accuracy: 0.18 - ETA: 3:44 - loss: 3.0679 - accuracy: 0.18 - ETA: 3:43 - loss: 3.0678 - accuracy: 0.18 - ETA: 3:43 - loss: 3.0676 - accuracy: 0.18 - ETA: 3:42 - loss: 3.0685 - accuracy: 0.18 - ETA: 3:41 - loss: 3.0693 - accuracy: 0.18 - ETA: 3:40 - loss: 3.0691 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0703 - accuracy: 0.18 - ETA: 3:39 - loss: 3.0728 - accuracy: 0.18 - ETA: 3:38 - loss: 3.0740 - accuracy: 0.18 - ETA: 3:37 - loss: 3.0749 - accuracy: 0.18 - ETA: 3:36 - loss: 3.0737 - accuracy: 0.18 - ETA: 3:35 - loss: 3.0743 - accuracy: 0.18 - ETA: 3:34 - loss: 3.0730 - accuracy: 0.18 - ETA: 3:33 - loss: 3.0714 - accuracy: 0.18 - ETA: 3:32 - loss: 3.0709 - accuracy: 0.18 - ETA: 3:31 - loss: 3.0701 - accuracy: 0.18 - ETA: 3:30 - loss: 3.0693 - accuracy: 0.18 - ETA: 3:29 - loss: 3.0687 - accuracy: 0.18 - ETA: 3:28 - loss: 3.0717 - accuracy: 0.18 - ETA: 3:27 - loss: 3.0725 - accuracy: 0.18 - ETA: 3:26 - loss: 3.0722 - accuracy: 0.18 - ETA: 3:25 - loss: 3.0725 - accuracy: 0.18 - ETA: 3:24 - loss: 3.0738 - accuracy: 0.18 - ETA: 3:23 - loss: 3.0750 - accuracy: 0.18 - ETA: 3:22 - loss: 3.0754 - accuracy: 0.18 - ETA: 3:21 - loss: 3.0759 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0762 - accuracy: 0.18 - ETA: 3:20 - loss: 3.0765 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0777 - accuracy: 0.18 - ETA: 3:18 - loss: 3.0781 - accuracy: 0.18 - ETA: 3:17 - loss: 3.0773 - accuracy: 0.18 - ETA: 3:16 - loss: 3.0768 - accuracy: 0.18 - ETA: 3:15 - loss: 3.0758 - accuracy: 0.18 - ETA: 3:14 - loss: 3.0763 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0767 - accuracy: 0.18 - ETA: 3:12 - loss: 3.0770 - accuracy: 0.18 - ETA: 3:11 - loss: 3.0768 - accuracy: 0.18 - ETA: 3:10 - loss: 3.0766 - accuracy: 0.18 - ETA: 3:09 - loss: 3.0767 - accuracy: 0.18 - ETA: 3:08 - loss: 3.0748 - accuracy: 0.18 - ETA: 3:07 - loss: 3.0752 - accuracy: 0.18 - ETA: 3:06 - loss: 3.0755 - accuracy: 0.18 - ETA: 3:05 - loss: 3.0765 - accuracy: 0.18 - ETA: 3:04 - loss: 3.0758 - accuracy: 0.18 - ETA: 3:03 - loss: 3.0743 - accuracy: 0.18 - ETA: 3:02 - loss: 3.0742 - accuracy: 0.18 - ETA: 3:01 - loss: 3.0732 - accuracy: 0.18 - ETA: 3:00 - loss: 3.0729 - accuracy: 0.18 - ETA: 2:59 - loss: 3.0721 - accuracy: 0.18 - ETA: 2:58 - loss: 3.0730 - accuracy: 0.18 - ETA: 2:57 - loss: 3.0738 - accuracy: 0.18 - ETA: 2:56 - loss: 3.0743 - accuracy: 0.18 - ETA: 2:55 - loss: 3.0768 - accuracy: 0.18 - ETA: 2:54 - loss: 3.0748 - accuracy: 0.18 - ETA: 2:53 - loss: 3.0748 - accuracy: 0.18 - ETA: 2:52 - loss: 3.0740 - accuracy: 0.18 - ETA: 2:51 - loss: 3.0745 - accuracy: 0.18 - ETA: 2:50 - loss: 3.0761 - accuracy: 0.18 - ETA: 2:49 - loss: 3.0760 - accuracy: 0.18 - ETA: 2:48 - loss: 3.0755 - accuracy: 0.18 - ETA: 2:47 - loss: 3.0757 - accuracy: 0.18 - ETA: 2:46 - loss: 3.0758 - accuracy: 0.18 - ETA: 2:45 - loss: 3.0763 - accuracy: 0.18 - ETA: 2:44 - loss: 3.0755 - accuracy: 0.18 - ETA: 2:43 - loss: 3.0757 - accuracy: 0.18 - ETA: 2:42 - loss: 3.0760 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0774 - accuracy: 0.18 - ETA: 2:41 - loss: 3.0775 - accuracy: 0.18 - ETA: 2:40 - loss: 3.0772 - accuracy: 0.18 - ETA: 2:39 - loss: 3.0774 - accuracy: 0.18 - ETA: 2:38 - loss: 3.0774 - accuracy: 0.18 - ETA: 2:37 - loss: 3.0785 - accuracy: 0.18 - ETA: 2:36 - loss: 3.0777 - accuracy: 0.18 - ETA: 2:35 - loss: 3.0798 - accuracy: 0.18 - ETA: 2:34 - loss: 3.0788 - accuracy: 0.18 - ETA: 2:33 - loss: 3.0777 - accuracy: 0.18 - ETA: 2:32 - loss: 3.0791 - accuracy: 0.18 - ETA: 2:31 - loss: 3.0798 - accuracy: 0.18 - ETA: 2:30 - loss: 3.0803 - accuracy: 0.18 - ETA: 2:29 - loss: 3.0799 - accuracy: 0.18 - ETA: 2:28 - loss: 3.0796 - accuracy: 0.18 - ETA: 2:27 - loss: 3.0785 - accuracy: 0.18 - ETA: 2:26 - loss: 3.0788 - accuracy: 0.18 - ETA: 2:25 - loss: 3.0781 - accuracy: 0.18 - ETA: 2:24 - loss: 3.0774 - accuracy: 0.18 - ETA: 2:23 - loss: 3.0765 - accuracy: 0.18 - ETA: 2:22 - loss: 3.0764 - accuracy: 0.18 - ETA: 2:21 - loss: 3.0765 - accuracy: 0.18 - ETA: 2:20 - loss: 3.0766 - accuracy: 0.18 - ETA: 2:19 - loss: 3.0761 - accuracy: 0.18 - ETA: 2:18 - loss: 3.0760 - accuracy: 0.1864"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.0754 - accuracy: 0.18 - ETA: 2:16 - loss: 3.0751 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0745 - accuracy: 0.18 - ETA: 2:15 - loss: 3.0742 - accuracy: 0.18 - ETA: 2:14 - loss: 3.0741 - accuracy: 0.18 - ETA: 2:13 - loss: 3.0736 - accuracy: 0.18 - ETA: 2:12 - loss: 3.0734 - accuracy: 0.18 - ETA: 2:11 - loss: 3.0723 - accuracy: 0.18 - ETA: 2:10 - loss: 3.0721 - accuracy: 0.18 - ETA: 2:09 - loss: 3.0725 - accuracy: 0.18 - ETA: 2:08 - loss: 3.0722 - accuracy: 0.18 - ETA: 2:07 - loss: 3.0716 - accuracy: 0.18 - ETA: 2:06 - loss: 3.0711 - accuracy: 0.18 - ETA: 2:05 - loss: 3.0708 - accuracy: 0.18 - ETA: 2:04 - loss: 3.0720 - accuracy: 0.18 - ETA: 2:03 - loss: 3.0721 - accuracy: 0.18 - ETA: 2:02 - loss: 3.0735 - accuracy: 0.18 - ETA: 2:01 - loss: 3.0730 - accuracy: 0.18 - ETA: 2:00 - loss: 3.0737 - accuracy: 0.18 - ETA: 1:59 - loss: 3.0734 - accuracy: 0.18 - ETA: 1:58 - loss: 3.0737 - accuracy: 0.18 - ETA: 1:57 - loss: 3.0737 - accuracy: 0.18 - ETA: 1:56 - loss: 3.0739 - accuracy: 0.18 - ETA: 1:55 - loss: 3.0727 - accuracy: 0.18 - ETA: 1:54 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:53 - loss: 3.0737 - accuracy: 0.18 - ETA: 1:52 - loss: 3.0775 - accuracy: 0.18 - ETA: 1:51 - loss: 3.0773 - accuracy: 0.18 - ETA: 1:50 - loss: 3.0771 - accuracy: 0.18 - ETA: 1:49 - loss: 3.0762 - accuracy: 0.18 - ETA: 1:48 - loss: 3.0767 - accuracy: 0.18 - ETA: 1:47 - loss: 3.0764 - accuracy: 0.18 - ETA: 1:46 - loss: 3.0758 - accuracy: 0.18 - ETA: 1:45 - loss: 3.0763 - accuracy: 0.18 - ETA: 1:44 - loss: 3.0775 - accuracy: 0.18 - ETA: 1:43 - loss: 3.0770 - accuracy: 0.18 - ETA: 1:42 - loss: 3.0765 - accuracy: 0.18 - ETA: 1:41 - loss: 3.0762 - accuracy: 0.18 - ETA: 1:40 - loss: 3.0760 - accuracy: 0.18 - ETA: 1:39 - loss: 3.0758 - accuracy: 0.18 - ETA: 1:38 - loss: 3.0757 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0744 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0742 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0741 - accuracy: 0.18 - ETA: 1:34 - loss: 3.0741 - accuracy: 0.18 - ETA: 1:33 - loss: 3.0739 - accuracy: 0.18 - ETA: 1:32 - loss: 3.0738 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:31 - loss: 3.0723 - accuracy: 0.18 - ETA: 1:30 - loss: 3.0724 - accuracy: 0.18 - ETA: 1:29 - loss: 3.0725 - accuracy: 0.18 - ETA: 1:28 - loss: 3.0730 - accuracy: 0.18 - ETA: 1:27 - loss: 3.0727 - accuracy: 0.18 - ETA: 1:26 - loss: 3.0729 - accuracy: 0.18 - ETA: 1:25 - loss: 3.0735 - accuracy: 0.18 - ETA: 1:24 - loss: 3.0734 - accuracy: 0.18 - ETA: 1:23 - loss: 3.0723 - accuracy: 0.18 - ETA: 1:22 - loss: 3.0731 - accuracy: 0.18 - ETA: 1:21 - loss: 3.0723 - accuracy: 0.18 - ETA: 1:20 - loss: 3.0729 - accuracy: 0.18 - ETA: 1:19 - loss: 3.0723 - accuracy: 0.18 - ETA: 1:18 - loss: 3.0724 - accuracy: 0.18 - ETA: 1:17 - loss: 3.0721 - accuracy: 0.18 - ETA: 1:16 - loss: 3.0721 - accuracy: 0.18 - ETA: 1:15 - loss: 3.0728 - accuracy: 0.18 - ETA: 1:14 - loss: 3.0736 - accuracy: 0.18 - ETA: 1:13 - loss: 3.0742 - accuracy: 0.18 - ETA: 1:12 - loss: 3.0742 - accuracy: 0.18 - ETA: 1:11 - loss: 3.0735 - accuracy: 0.18 - ETA: 1:10 - loss: 3.0742 - accuracy: 0.18 - ETA: 1:09 - loss: 3.0748 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0751 - accuracy: 0.18 - ETA: 1:08 - loss: 3.0749 - accuracy: 0.18 - ETA: 1:07 - loss: 3.0751 - accuracy: 0.18 - ETA: 1:06 - loss: 3.0744 - accuracy: 0.18 - ETA: 1:05 - loss: 3.0740 - accuracy: 0.18 - ETA: 1:04 - loss: 3.0740 - accuracy: 0.18 - ETA: 1:03 - loss: 3.0740 - accuracy: 0.18 - ETA: 1:02 - loss: 3.0733 - accuracy: 0.18 - ETA: 1:01 - loss: 3.0737 - accuracy: 0.18 - ETA: 1:00 - loss: 3.0732 - accuracy: 0.18 - ETA: 59s - loss: 3.0732 - accuracy: 0.1901 - ETA: 58s - loss: 3.0733 - accuracy: 0.190 - ETA: 57s - loss: 3.0735 - accuracy: 0.189 - ETA: 56s - loss: 3.0740 - accuracy: 0.189 - ETA: 55s - loss: 3.0735 - accuracy: 0.189 - ETA: 54s - loss: 3.0729 - accuracy: 0.189 - ETA: 53s - loss: 3.0738 - accuracy: 0.189 - ETA: 52s - loss: 3.0738 - accuracy: 0.189 - ETA: 51s - loss: 3.0742 - accuracy: 0.189 - ETA: 50s - loss: 3.0742 - accuracy: 0.189 - ETA: 49s - loss: 3.0736 - accuracy: 0.189 - ETA: 48s - loss: 3.0736 - accuracy: 0.190 - ETA: 47s - loss: 3.0741 - accuracy: 0.189 - ETA: 46s - loss: 3.0742 - accuracy: 0.189 - ETA: 45s - loss: 3.0732 - accuracy: 0.190 - ETA: 44s - loss: 3.0733 - accuracy: 0.190 - ETA: 44s - loss: 3.0730 - accuracy: 0.190 - ETA: 43s - loss: 3.0735 - accuracy: 0.190 - ETA: 42s - loss: 3.0732 - accuracy: 0.190 - ETA: 41s - loss: 3.0737 - accuracy: 0.189 - ETA: 40s - loss: 3.0730 - accuracy: 0.190 - ETA: 39s - loss: 3.0733 - accuracy: 0.190 - ETA: 38s - loss: 3.0742 - accuracy: 0.190 - ETA: 37s - loss: 3.0746 - accuracy: 0.190 - ETA: 36s - loss: 3.0748 - accuracy: 0.190 - ETA: 35s - loss: 3.0741 - accuracy: 0.190 - ETA: 34s - loss: 3.0740 - accuracy: 0.190 - ETA: 33s - loss: 3.0746 - accuracy: 0.190 - ETA: 32s - loss: 3.0753 - accuracy: 0.190 - ETA: 31s - loss: 3.0749 - accuracy: 0.190 - ETA: 30s - loss: 3.0751 - accuracy: 0.190 - ETA: 29s - loss: 3.0750 - accuracy: 0.190 - ETA: 28s - loss: 3.0748 - accuracy: 0.190 - ETA: 27s - loss: 3.0746 - accuracy: 0.190 - ETA: 26s - loss: 3.0741 - accuracy: 0.190 - ETA: 25s - loss: 3.0737 - accuracy: 0.190 - ETA: 24s - loss: 3.0739 - accuracy: 0.190 - ETA: 23s - loss: 3.0741 - accuracy: 0.190 - ETA: 23s - loss: 3.0743 - accuracy: 0.189 - ETA: 22s - loss: 3.0741 - accuracy: 0.190 - ETA: 21s - loss: 3.0738 - accuracy: 0.190 - ETA: 20s - loss: 3.0743 - accuracy: 0.190 - ETA: 19s - loss: 3.0746 - accuracy: 0.189 - ETA: 18s - loss: 3.0746 - accuracy: 0.189 - ETA: 17s - loss: 3.0743 - accuracy: 0.190 - ETA: 16s - loss: 3.0732 - accuracy: 0.190 - ETA: 15s - loss: 3.0729 - accuracy: 0.190 - ETA: 14s - loss: 3.0728 - accuracy: 0.190 - ETA: 13s - loss: 3.0727 - accuracy: 0.190 - ETA: 12s - loss: 3.0728 - accuracy: 0.190 - ETA: 11s - loss: 3.0723 - accuracy: 0.190 - ETA: 10s - loss: 3.0724 - accuracy: 0.190 - ETA: 9s - loss: 3.0729 - accuracy: 0.190 - ETA: 8s - loss: 3.0728 - accuracy: 0.19 - ETA: 7s - loss: 3.0735 - accuracy: 0.19 - ETA: 6s - loss: 3.0732 - accuracy: 0.19 - ETA: 5s - loss: 3.0731 - accuracy: 0.19 - ETA: 4s - loss: 3.0720 - accuracy: 0.19 - ETA: 3s - loss: 3.0725 - accuracy: 0.19 - ETA: 2s - loss: 3.0717 - accuracy: 0.19 - ETA: 1s - loss: 3.0717 - accuracy: 0.19 - ETA: 1s - loss: 3.0717 - accuracy: 0.19 - ETA: 0s - loss: 3.0728 - accuracy: 0.19 - 339s 8ms/step - loss: 3.0729 - accuracy: 0.1908 - val_loss: 4.3396 - val_accuracy: 0.0376\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:13 - loss: 2.9313 - accuracy: 0.21 - ETA: 5:09 - loss: 2.9859 - accuracy: 0.21 - ETA: 5:11 - loss: 2.9449 - accuracy: 0.22 - ETA: 5:10 - loss: 2.9738 - accuracy: 0.21 - ETA: 5:10 - loss: 3.0195 - accuracy: 0.19 - ETA: 5:10 - loss: 3.0271 - accuracy: 0.20 - ETA: 5:08 - loss: 3.0264 - accuracy: 0.20 - ETA: 5:07 - loss: 3.0306 - accuracy: 0.20 - ETA: 5:04 - loss: 3.0555 - accuracy: 0.19 - ETA: 5:02 - loss: 3.0868 - accuracy: 0.18 - ETA: 5:01 - loss: 3.0873 - accuracy: 0.18 - ETA: 5:00 - loss: 3.0870 - accuracy: 0.18 - ETA: 4:59 - loss: 3.0838 - accuracy: 0.18 - ETA: 4:58 - loss: 3.0735 - accuracy: 0.18 - ETA: 4:56 - loss: 3.0583 - accuracy: 0.18 - ETA: 4:57 - loss: 3.0499 - accuracy: 0.18 - ETA: 4:58 - loss: 3.0437 - accuracy: 0.18 - ETA: 5:00 - loss: 3.0438 - accuracy: 0.18 - ETA: 5:00 - loss: 3.0456 - accuracy: 0.18 - ETA: 5:00 - loss: 3.0485 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0481 - accuracy: 0.18 - ETA: 4:58 - loss: 3.0490 - accuracy: 0.18 - ETA: 4:56 - loss: 3.0509 - accuracy: 0.18 - ETA: 4:56 - loss: 3.0567 - accuracy: 0.18 - ETA: 4:54 - loss: 3.0538 - accuracy: 0.18 - ETA: 4:53 - loss: 3.0515 - accuracy: 0.18 - ETA: 4:52 - loss: 3.0572 - accuracy: 0.18 - ETA: 4:50 - loss: 3.0578 - accuracy: 0.18 - ETA: 4:49 - loss: 3.0611 - accuracy: 0.18 - ETA: 4:48 - loss: 3.0635 - accuracy: 0.18 - ETA: 4:47 - loss: 3.0611 - accuracy: 0.18 - ETA: 4:46 - loss: 3.0561 - accuracy: 0.18 - ETA: 4:45 - loss: 3.0587 - accuracy: 0.18 - ETA: 4:44 - loss: 3.0600 - accuracy: 0.18 - ETA: 4:43 - loss: 3.0596 - accuracy: 0.18 - ETA: 4:42 - loss: 3.0581 - accuracy: 0.18 - ETA: 4:41 - loss: 3.0681 - accuracy: 0.18 - ETA: 4:40 - loss: 3.0780 - accuracy: 0.18 - ETA: 4:39 - loss: 3.0896 - accuracy: 0.18 - ETA: 4:38 - loss: 3.0914 - accuracy: 0.18 - ETA: 4:37 - loss: 3.0970 - accuracy: 0.18 - ETA: 4:36 - loss: 3.0976 - accuracy: 0.18 - ETA: 4:35 - loss: 3.0944 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0972 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0943 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0933 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0936 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0934 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0910 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0891 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0885 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0886 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0900 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0893 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0919 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0926 - accuracy: 0.18 - ETA: 4:21 - loss: 3.0942 - accuracy: 0.18 - ETA: 4:20 - loss: 3.0940 - accuracy: 0.18 - ETA: 4:19 - loss: 3.0978 - accuracy: 0.18 - ETA: 4:18 - loss: 3.0966 - accuracy: 0.18 - ETA: 4:17 - loss: 3.0976 - accuracy: 0.18 - ETA: 4:16 - loss: 3.0990 - accuracy: 0.18 - ETA: 4:15 - loss: 3.0995 - accuracy: 0.18 - ETA: 4:14 - loss: 3.1002 - accuracy: 0.18 - ETA: 4:13 - loss: 3.1001 - accuracy: 0.18 - ETA: 4:12 - loss: 3.0978 - accuracy: 0.18 - ETA: 4:11 - loss: 3.0989 - accuracy: 0.18 - ETA: 4:10 - loss: 3.0951 - accuracy: 0.18 - ETA: 4:09 - loss: 3.0927 - accuracy: 0.18 - ETA: 4:08 - loss: 3.0927 - accuracy: 0.18 - ETA: 4:07 - loss: 3.0916 - accuracy: 0.18 - ETA: 4:06 - loss: 3.0909 - accuracy: 0.18 - ETA: 4:05 - loss: 3.0903 - accuracy: 0.18 - ETA: 4:04 - loss: 3.0859 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0866 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0835 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0826 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0815 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0817 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0799 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0836 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0858 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0871 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0954 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0952 - accuracy: 0.18 - ETA: 3:53 - loss: 3.0982 - accuracy: 0.18 - ETA: 3:52 - loss: 3.0974 - accuracy: 0.18 - ETA: 3:51 - loss: 3.0986 - accuracy: 0.18 - ETA: 3:50 - loss: 3.0991 - accuracy: 0.18 - ETA: 3:49 - loss: 3.0999 - accuracy: 0.18 - ETA: 3:48 - loss: 3.1009 - accuracy: 0.18 - ETA: 3:47 - loss: 3.1025 - accuracy: 0.18 - ETA: 3:46 - loss: 3.1045 - accuracy: 0.18 - ETA: 3:45 - loss: 3.1048 - accuracy: 0.18 - ETA: 3:44 - loss: 3.1057 - accuracy: 0.18 - ETA: 3:44 - loss: 3.1081 - accuracy: 0.18 - ETA: 3:43 - loss: 3.1052 - accuracy: 0.18 - ETA: 3:42 - loss: 3.1078 - accuracy: 0.18 - ETA: 3:41 - loss: 3.1074 - accuracy: 0.18 - ETA: 3:40 - loss: 3.1087 - accuracy: 0.18 - ETA: 3:39 - loss: 3.1085 - accuracy: 0.18 - ETA: 3:38 - loss: 3.1095 - accuracy: 0.18 - ETA: 3:37 - loss: 3.1095 - accuracy: 0.18 - ETA: 3:36 - loss: 3.1090 - accuracy: 0.18 - ETA: 3:35 - loss: 3.1073 - accuracy: 0.18 - ETA: 3:34 - loss: 3.1076 - accuracy: 0.18 - ETA: 3:33 - loss: 3.1074 - accuracy: 0.18 - ETA: 3:32 - loss: 3.1073 - accuracy: 0.18 - ETA: 3:31 - loss: 3.1076 - accuracy: 0.18 - ETA: 3:30 - loss: 3.1080 - accuracy: 0.18 - ETA: 3:29 - loss: 3.1082 - accuracy: 0.18 - ETA: 3:28 - loss: 3.1106 - accuracy: 0.18 - ETA: 3:27 - loss: 3.1107 - accuracy: 0.18 - ETA: 3:27 - loss: 3.1117 - accuracy: 0.18 - ETA: 3:26 - loss: 3.1128 - accuracy: 0.18 - ETA: 3:25 - loss: 3.1127 - accuracy: 0.18 - ETA: 3:24 - loss: 3.1112 - accuracy: 0.18 - ETA: 3:23 - loss: 3.1112 - accuracy: 0.18 - ETA: 3:22 - loss: 3.1132 - accuracy: 0.18 - ETA: 3:21 - loss: 3.1123 - accuracy: 0.18 - ETA: 3:20 - loss: 3.1126 - accuracy: 0.18 - ETA: 3:19 - loss: 3.1116 - accuracy: 0.18 - ETA: 3:18 - loss: 3.1113 - accuracy: 0.18 - ETA: 3:17 - loss: 3.1087 - accuracy: 0.18 - ETA: 3:16 - loss: 3.1086 - accuracy: 0.18 - ETA: 3:15 - loss: 3.1077 - accuracy: 0.18 - ETA: 3:14 - loss: 3.1088 - accuracy: 0.18 - ETA: 3:13 - loss: 3.1092 - accuracy: 0.18 - ETA: 3:12 - loss: 3.1081 - accuracy: 0.18 - ETA: 3:11 - loss: 3.1061 - accuracy: 0.18 - ETA: 3:10 - loss: 3.1062 - accuracy: 0.18 - ETA: 3:09 - loss: 3.1057 - accuracy: 0.18 - ETA: 3:08 - loss: 3.1059 - accuracy: 0.18 - ETA: 3:07 - loss: 3.1074 - accuracy: 0.18 - ETA: 3:06 - loss: 3.1087 - accuracy: 0.18 - ETA: 3:05 - loss: 3.1081 - accuracy: 0.18 - ETA: 3:04 - loss: 3.1079 - accuracy: 0.18 - ETA: 3:03 - loss: 3.1095 - accuracy: 0.18 - ETA: 3:02 - loss: 3.1096 - accuracy: 0.18 - ETA: 3:01 - loss: 3.1088 - accuracy: 0.18 - ETA: 3:00 - loss: 3.1079 - accuracy: 0.18 - ETA: 2:59 - loss: 3.1077 - accuracy: 0.18 - ETA: 2:59 - loss: 3.1064 - accuracy: 0.18 - ETA: 2:58 - loss: 3.1051 - accuracy: 0.18 - ETA: 2:57 - loss: 3.1065 - accuracy: 0.18 - ETA: 2:56 - loss: 3.1050 - accuracy: 0.18 - ETA: 2:55 - loss: 3.1059 - accuracy: 0.18 - ETA: 2:54 - loss: 3.1050 - accuracy: 0.18 - ETA: 2:53 - loss: 3.1058 - accuracy: 0.18 - ETA: 2:52 - loss: 3.1059 - accuracy: 0.18 - ETA: 2:51 - loss: 3.1066 - accuracy: 0.18 - ETA: 2:50 - loss: 3.1073 - accuracy: 0.18 - ETA: 2:49 - loss: 3.1085 - accuracy: 0.18 - ETA: 2:48 - loss: 3.1098 - accuracy: 0.18 - ETA: 2:47 - loss: 3.1075 - accuracy: 0.18 - ETA: 2:46 - loss: 3.1071 - accuracy: 0.18 - ETA: 2:45 - loss: 3.1064 - accuracy: 0.18 - ETA: 2:45 - loss: 3.1056 - accuracy: 0.18 - ETA: 2:44 - loss: 3.1057 - accuracy: 0.18 - ETA: 2:43 - loss: 3.1055 - accuracy: 0.18 - ETA: 2:42 - loss: 3.1054 - accuracy: 0.18 - ETA: 2:41 - loss: 3.1062 - accuracy: 0.18 - ETA: 2:40 - loss: 3.1061 - accuracy: 0.18 - ETA: 2:39 - loss: 3.1062 - accuracy: 0.18 - ETA: 2:38 - loss: 3.1062 - accuracy: 0.18 - ETA: 2:37 - loss: 3.1063 - accuracy: 0.18 - ETA: 2:36 - loss: 3.1064 - accuracy: 0.18 - ETA: 2:35 - loss: 3.1065 - accuracy: 0.18 - ETA: 2:34 - loss: 3.1056 - accuracy: 0.18 - ETA: 2:33 - loss: 3.1051 - accuracy: 0.18 - ETA: 2:32 - loss: 3.1052 - accuracy: 0.18 - ETA: 2:31 - loss: 3.1052 - accuracy: 0.18 - ETA: 2:30 - loss: 3.1056 - accuracy: 0.18 - ETA: 2:29 - loss: 3.1057 - accuracy: 0.18 - ETA: 2:28 - loss: 3.1068 - accuracy: 0.18 - ETA: 2:27 - loss: 3.1067 - accuracy: 0.18 - ETA: 2:26 - loss: 3.1072 - accuracy: 0.18 - ETA: 2:25 - loss: 3.1074 - accuracy: 0.18 - ETA: 2:25 - loss: 3.1072 - accuracy: 0.18 - ETA: 2:24 - loss: 3.1077 - accuracy: 0.18 - ETA: 2:23 - loss: 3.1086 - accuracy: 0.18 - ETA: 2:22 - loss: 3.1076 - accuracy: 0.18 - ETA: 2:21 - loss: 3.1066 - accuracy: 0.18 - ETA: 2:20 - loss: 3.1062 - accuracy: 0.18 - ETA: 2:19 - loss: 3.1062 - accuracy: 0.18 - ETA: 2:18 - loss: 3.1071 - accuracy: 0.1826"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:17 - loss: 3.1076 - accuracy: 0.18 - ETA: 2:16 - loss: 3.1079 - accuracy: 0.18 - ETA: 2:15 - loss: 3.1075 - accuracy: 0.18 - ETA: 2:14 - loss: 3.1084 - accuracy: 0.18 - ETA: 2:13 - loss: 3.1089 - accuracy: 0.18 - ETA: 2:12 - loss: 3.1101 - accuracy: 0.18 - ETA: 2:11 - loss: 3.1110 - accuracy: 0.18 - ETA: 2:10 - loss: 3.1115 - accuracy: 0.18 - ETA: 2:09 - loss: 3.1132 - accuracy: 0.18 - ETA: 2:08 - loss: 3.1131 - accuracy: 0.18 - ETA: 2:07 - loss: 3.1133 - accuracy: 0.18 - ETA: 2:06 - loss: 3.1136 - accuracy: 0.18 - ETA: 2:05 - loss: 3.1134 - accuracy: 0.18 - ETA: 2:04 - loss: 3.1136 - accuracy: 0.18 - ETA: 2:03 - loss: 3.1137 - accuracy: 0.18 - ETA: 2:02 - loss: 3.1147 - accuracy: 0.18 - ETA: 2:01 - loss: 3.1155 - accuracy: 0.18 - ETA: 2:00 - loss: 3.1153 - accuracy: 0.18 - ETA: 1:59 - loss: 3.1146 - accuracy: 0.18 - ETA: 1:58 - loss: 3.1137 - accuracy: 0.18 - ETA: 1:57 - loss: 3.1135 - accuracy: 0.18 - ETA: 1:57 - loss: 3.1135 - accuracy: 0.18 - ETA: 1:56 - loss: 3.1143 - accuracy: 0.18 - ETA: 1:55 - loss: 3.1149 - accuracy: 0.18 - ETA: 1:54 - loss: 3.1138 - accuracy: 0.18 - ETA: 1:53 - loss: 3.1148 - accuracy: 0.18 - ETA: 1:52 - loss: 3.1143 - accuracy: 0.18 - ETA: 1:51 - loss: 3.1137 - accuracy: 0.18 - ETA: 1:50 - loss: 3.1151 - accuracy: 0.18 - ETA: 1:49 - loss: 3.1170 - accuracy: 0.18 - ETA: 1:48 - loss: 3.1177 - accuracy: 0.18 - ETA: 1:47 - loss: 3.1188 - accuracy: 0.18 - ETA: 1:46 - loss: 3.1189 - accuracy: 0.18 - ETA: 1:45 - loss: 3.1176 - accuracy: 0.18 - ETA: 1:44 - loss: 3.1186 - accuracy: 0.18 - ETA: 1:43 - loss: 3.1186 - accuracy: 0.18 - ETA: 1:42 - loss: 3.1184 - accuracy: 0.18 - ETA: 1:41 - loss: 3.1180 - accuracy: 0.18 - ETA: 1:40 - loss: 3.1178 - accuracy: 0.18 - ETA: 1:40 - loss: 3.1167 - accuracy: 0.18 - ETA: 1:39 - loss: 3.1161 - accuracy: 0.18 - ETA: 1:38 - loss: 3.1153 - accuracy: 0.18 - ETA: 1:37 - loss: 3.1155 - accuracy: 0.18 - ETA: 1:36 - loss: 3.1155 - accuracy: 0.18 - ETA: 1:35 - loss: 3.1159 - accuracy: 0.18 - ETA: 1:34 - loss: 3.1159 - accuracy: 0.18 - ETA: 1:33 - loss: 3.1166 - accuracy: 0.18 - ETA: 1:32 - loss: 3.1169 - accuracy: 0.18 - ETA: 1:31 - loss: 3.1163 - accuracy: 0.18 - ETA: 1:30 - loss: 3.1166 - accuracy: 0.18 - ETA: 1:29 - loss: 3.1166 - accuracy: 0.18 - ETA: 1:28 - loss: 3.1163 - accuracy: 0.18 - ETA: 1:27 - loss: 3.1162 - accuracy: 0.18 - ETA: 1:26 - loss: 3.1163 - accuracy: 0.18 - ETA: 1:25 - loss: 3.1173 - accuracy: 0.18 - ETA: 1:24 - loss: 3.1165 - accuracy: 0.18 - ETA: 1:23 - loss: 3.1169 - accuracy: 0.18 - ETA: 1:22 - loss: 3.1179 - accuracy: 0.18 - ETA: 1:21 - loss: 3.1196 - accuracy: 0.18 - ETA: 1:20 - loss: 3.1216 - accuracy: 0.18 - ETA: 1:19 - loss: 3.1223 - accuracy: 0.18 - ETA: 1:18 - loss: 3.1231 - accuracy: 0.18 - ETA: 1:18 - loss: 3.1233 - accuracy: 0.18 - ETA: 1:17 - loss: 3.1234 - accuracy: 0.18 - ETA: 1:16 - loss: 3.1235 - accuracy: 0.18 - ETA: 1:15 - loss: 3.1230 - accuracy: 0.18 - ETA: 1:14 - loss: 3.1232 - accuracy: 0.18 - ETA: 1:13 - loss: 3.1229 - accuracy: 0.18 - ETA: 1:12 - loss: 3.1228 - accuracy: 0.18 - ETA: 1:11 - loss: 3.1238 - accuracy: 0.18 - ETA: 1:10 - loss: 3.1237 - accuracy: 0.18 - ETA: 1:09 - loss: 3.1230 - accuracy: 0.18 - ETA: 1:08 - loss: 3.1232 - accuracy: 0.18 - ETA: 1:07 - loss: 3.1228 - accuracy: 0.18 - ETA: 1:06 - loss: 3.1235 - accuracy: 0.18 - ETA: 1:05 - loss: 3.1228 - accuracy: 0.18 - ETA: 1:04 - loss: 3.1226 - accuracy: 0.18 - ETA: 1:03 - loss: 3.1227 - accuracy: 0.18 - ETA: 1:02 - loss: 3.1226 - accuracy: 0.18 - ETA: 1:01 - loss: 3.1232 - accuracy: 0.18 - ETA: 1:00 - loss: 3.1230 - accuracy: 0.18 - ETA: 59s - loss: 3.1229 - accuracy: 0.1831 - ETA: 59s - loss: 3.1225 - accuracy: 0.183 - ETA: 58s - loss: 3.1222 - accuracy: 0.183 - ETA: 57s - loss: 3.1223 - accuracy: 0.183 - ETA: 56s - loss: 3.1224 - accuracy: 0.183 - ETA: 55s - loss: 3.1223 - accuracy: 0.183 - ETA: 54s - loss: 3.1221 - accuracy: 0.183 - ETA: 53s - loss: 3.1215 - accuracy: 0.183 - ETA: 52s - loss: 3.1210 - accuracy: 0.183 - ETA: 51s - loss: 3.1212 - accuracy: 0.183 - ETA: 50s - loss: 3.1217 - accuracy: 0.183 - ETA: 49s - loss: 3.1220 - accuracy: 0.183 - ETA: 48s - loss: 3.1221 - accuracy: 0.182 - ETA: 47s - loss: 3.1215 - accuracy: 0.183 - ETA: 46s - loss: 3.1218 - accuracy: 0.182 - ETA: 45s - loss: 3.1209 - accuracy: 0.182 - ETA: 44s - loss: 3.1211 - accuracy: 0.182 - ETA: 43s - loss: 3.1200 - accuracy: 0.182 - ETA: 42s - loss: 3.1202 - accuracy: 0.182 - ETA: 41s - loss: 3.1200 - accuracy: 0.182 - ETA: 40s - loss: 3.1200 - accuracy: 0.182 - ETA: 39s - loss: 3.1200 - accuracy: 0.182 - ETA: 39s - loss: 3.1200 - accuracy: 0.182 - ETA: 38s - loss: 3.1197 - accuracy: 0.182 - ETA: 37s - loss: 3.1191 - accuracy: 0.182 - ETA: 36s - loss: 3.1184 - accuracy: 0.182 - ETA: 35s - loss: 3.1181 - accuracy: 0.183 - ETA: 34s - loss: 3.1182 - accuracy: 0.183 - ETA: 33s - loss: 3.1180 - accuracy: 0.183 - ETA: 32s - loss: 3.1176 - accuracy: 0.183 - ETA: 31s - loss: 3.1170 - accuracy: 0.183 - ETA: 30s - loss: 3.1165 - accuracy: 0.183 - ETA: 29s - loss: 3.1167 - accuracy: 0.183 - ETA: 28s - loss: 3.1159 - accuracy: 0.183 - ETA: 27s - loss: 3.1159 - accuracy: 0.183 - ETA: 26s - loss: 3.1153 - accuracy: 0.183 - ETA: 25s - loss: 3.1147 - accuracy: 0.183 - ETA: 24s - loss: 3.1149 - accuracy: 0.183 - ETA: 23s - loss: 3.1145 - accuracy: 0.183 - ETA: 22s - loss: 3.1141 - accuracy: 0.183 - ETA: 21s - loss: 3.1136 - accuracy: 0.183 - ETA: 20s - loss: 3.1136 - accuracy: 0.183 - ETA: 20s - loss: 3.1135 - accuracy: 0.184 - ETA: 19s - loss: 3.1132 - accuracy: 0.184 - ETA: 18s - loss: 3.1135 - accuracy: 0.184 - ETA: 17s - loss: 3.1136 - accuracy: 0.184 - ETA: 16s - loss: 3.1131 - accuracy: 0.184 - ETA: 15s - loss: 3.1132 - accuracy: 0.184 - ETA: 14s - loss: 3.1128 - accuracy: 0.184 - ETA: 13s - loss: 3.1128 - accuracy: 0.184 - ETA: 12s - loss: 3.1128 - accuracy: 0.184 - ETA: 11s - loss: 3.1129 - accuracy: 0.184 - ETA: 10s - loss: 3.1138 - accuracy: 0.184 - ETA: 9s - loss: 3.1139 - accuracy: 0.184 - ETA: 8s - loss: 3.1140 - accuracy: 0.18 - ETA: 7s - loss: 3.1148 - accuracy: 0.18 - ETA: 6s - loss: 3.1149 - accuracy: 0.18 - ETA: 5s - loss: 3.1153 - accuracy: 0.18 - ETA: 4s - loss: 3.1153 - accuracy: 0.18 - ETA: 3s - loss: 3.1156 - accuracy: 0.18 - ETA: 2s - loss: 3.1153 - accuracy: 0.18 - ETA: 1s - loss: 3.1152 - accuracy: 0.18 - ETA: 1s - loss: 3.1147 - accuracy: 0.18 - ETA: 0s - loss: 3.1143 - accuracy: 0.18 - 336s 8ms/step - loss: 3.1144 - accuracy: 0.1846 - val_loss: 4.2797 - val_accuracy: 0.0356\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:05 - loss: 2.9241 - accuracy: 0.26 - ETA: 5:10 - loss: 3.1324 - accuracy: 0.19 - ETA: 5:12 - loss: 3.1178 - accuracy: 0.20 - ETA: 5:07 - loss: 3.1195 - accuracy: 0.19 - ETA: 5:05 - loss: 3.1118 - accuracy: 0.20 - ETA: 5:03 - loss: 3.1153 - accuracy: 0.19 - ETA: 5:00 - loss: 3.1007 - accuracy: 0.18 - ETA: 4:59 - loss: 3.0910 - accuracy: 0.18 - ETA: 5:01 - loss: 3.1113 - accuracy: 0.18 - ETA: 5:03 - loss: 3.0968 - accuracy: 0.18 - ETA: 5:02 - loss: 3.0932 - accuracy: 0.18 - ETA: 5:01 - loss: 3.0701 - accuracy: 0.19 - ETA: 5:00 - loss: 3.0614 - accuracy: 0.19 - ETA: 4:59 - loss: 3.0573 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0632 - accuracy: 0.19 - ETA: 4:59 - loss: 3.0693 - accuracy: 0.18 - ETA: 4:57 - loss: 3.0582 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0653 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0731 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0771 - accuracy: 0.19 - ETA: 4:55 - loss: 3.0843 - accuracy: 0.19 - ETA: 4:54 - loss: 3.0870 - accuracy: 0.18 - ETA: 4:52 - loss: 3.0817 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0812 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0799 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0749 - accuracy: 0.19 - ETA: 4:47 - loss: 3.0707 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0703 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0708 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0715 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0709 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0698 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0678 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0735 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0721 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0685 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0675 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0672 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0665 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0623 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0611 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0633 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0562 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0512 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0484 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0476 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0472 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0475 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0488 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0488 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0497 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0486 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0491 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0517 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0523 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0529 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0484 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0444 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0423 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0420 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0448 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0435 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0439 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0410 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0402 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0393 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0374 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0383 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0407 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0385 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0357 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0372 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0376 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0386 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0361 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0404 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0413 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0423 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0455 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0448 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0467 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0449 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0453 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0449 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0453 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0442 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0438 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0436 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0441 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0444 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0437 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0465 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0461 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0443 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0452 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0458 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0454 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0448 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0439 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0437 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0443 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0438 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0421 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0405 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0403 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0407 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0403 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0390 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0399 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0400 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0404 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0402 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0410 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0409 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0416 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0419 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0418 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0429 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0427 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0418 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0430 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0439 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0449 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0462 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0467 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0485 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0489 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0482 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0471 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0474 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0499 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0493 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0485 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0475 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0463 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0458 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0459 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0466 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0463 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0463 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0457 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0462 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0444 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0436 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0465 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0467 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0462 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0455 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0456 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0463 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0455 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0453 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0453 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0454 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0463 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0478 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0506 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0532 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0562 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0574 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0577 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0580 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0581 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0590 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0630 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0633 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0634 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0639 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0630 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0632 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0633 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0639 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0646 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0640 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0643 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0647 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0658 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0668 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0675 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0687 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0693 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0699 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0692 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0692 - accuracy: 0.19 - ETA: 2:17 - loss: 3.0699 - accuracy: 0.19 - ETA: 2:16 - loss: 3.0710 - accuracy: 0.1924"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:15 - loss: 3.0715 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0718 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0718 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0726 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0738 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0747 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0743 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0744 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0752 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0741 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0750 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0758 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0763 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0761 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0768 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0775 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0775 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0771 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0770 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0776 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0778 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0768 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0783 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0784 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0796 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0806 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0807 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0804 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0810 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0807 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0827 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0841 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0851 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0849 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0846 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0846 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0845 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0856 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0886 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0921 - accuracy: 0.18 - ETA: 1:37 - loss: 3.0947 - accuracy: 0.18 - ETA: 1:36 - loss: 3.0960 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0976 - accuracy: 0.18 - ETA: 1:35 - loss: 3.0992 - accuracy: 0.18 - ETA: 1:34 - loss: 3.1003 - accuracy: 0.18 - ETA: 1:33 - loss: 3.1017 - accuracy: 0.18 - ETA: 1:32 - loss: 3.1032 - accuracy: 0.18 - ETA: 1:31 - loss: 3.1040 - accuracy: 0.18 - ETA: 1:30 - loss: 3.1037 - accuracy: 0.18 - ETA: 1:29 - loss: 3.1033 - accuracy: 0.18 - ETA: 1:28 - loss: 3.1034 - accuracy: 0.18 - ETA: 1:27 - loss: 3.1036 - accuracy: 0.18 - ETA: 1:26 - loss: 3.1035 - accuracy: 0.18 - ETA: 1:25 - loss: 3.1043 - accuracy: 0.18 - ETA: 1:24 - loss: 3.1037 - accuracy: 0.18 - ETA: 1:23 - loss: 3.1044 - accuracy: 0.18 - ETA: 1:22 - loss: 3.1041 - accuracy: 0.18 - ETA: 1:21 - loss: 3.1044 - accuracy: 0.18 - ETA: 1:21 - loss: 3.1041 - accuracy: 0.18 - ETA: 1:20 - loss: 3.1043 - accuracy: 0.18 - ETA: 1:19 - loss: 3.1046 - accuracy: 0.18 - ETA: 1:18 - loss: 3.1042 - accuracy: 0.18 - ETA: 1:17 - loss: 3.1048 - accuracy: 0.18 - ETA: 1:16 - loss: 3.1054 - accuracy: 0.18 - ETA: 1:15 - loss: 3.1049 - accuracy: 0.18 - ETA: 1:14 - loss: 3.1045 - accuracy: 0.18 - ETA: 1:13 - loss: 3.1054 - accuracy: 0.18 - ETA: 1:12 - loss: 3.1058 - accuracy: 0.18 - ETA: 1:11 - loss: 3.1065 - accuracy: 0.18 - ETA: 1:10 - loss: 3.1071 - accuracy: 0.18 - ETA: 1:09 - loss: 3.1070 - accuracy: 0.18 - ETA: 1:08 - loss: 3.1070 - accuracy: 0.18 - ETA: 1:07 - loss: 3.1070 - accuracy: 0.18 - ETA: 1:07 - loss: 3.1063 - accuracy: 0.18 - ETA: 1:06 - loss: 3.1062 - accuracy: 0.18 - ETA: 1:05 - loss: 3.1068 - accuracy: 0.18 - ETA: 1:04 - loss: 3.1071 - accuracy: 0.18 - ETA: 1:03 - loss: 3.1073 - accuracy: 0.18 - ETA: 1:02 - loss: 3.1074 - accuracy: 0.18 - ETA: 1:01 - loss: 3.1081 - accuracy: 0.18 - ETA: 1:00 - loss: 3.1086 - accuracy: 0.18 - ETA: 59s - loss: 3.1089 - accuracy: 0.1864 - ETA: 58s - loss: 3.1101 - accuracy: 0.186 - ETA: 57s - loss: 3.1099 - accuracy: 0.185 - ETA: 56s - loss: 3.1102 - accuracy: 0.185 - ETA: 55s - loss: 3.1103 - accuracy: 0.185 - ETA: 54s - loss: 3.1105 - accuracy: 0.185 - ETA: 53s - loss: 3.1096 - accuracy: 0.185 - ETA: 52s - loss: 3.1101 - accuracy: 0.185 - ETA: 51s - loss: 3.1099 - accuracy: 0.185 - ETA: 51s - loss: 3.1107 - accuracy: 0.185 - ETA: 50s - loss: 3.1115 - accuracy: 0.185 - ETA: 49s - loss: 3.1118 - accuracy: 0.185 - ETA: 48s - loss: 3.1124 - accuracy: 0.185 - ETA: 47s - loss: 3.1119 - accuracy: 0.185 - ETA: 46s - loss: 3.1117 - accuracy: 0.185 - ETA: 45s - loss: 3.1120 - accuracy: 0.185 - ETA: 44s - loss: 3.1114 - accuracy: 0.185 - ETA: 43s - loss: 3.1129 - accuracy: 0.185 - ETA: 42s - loss: 3.1128 - accuracy: 0.185 - ETA: 41s - loss: 3.1129 - accuracy: 0.185 - ETA: 40s - loss: 3.1125 - accuracy: 0.185 - ETA: 39s - loss: 3.1125 - accuracy: 0.185 - ETA: 38s - loss: 3.1127 - accuracy: 0.185 - ETA: 37s - loss: 3.1129 - accuracy: 0.185 - ETA: 36s - loss: 3.1124 - accuracy: 0.185 - ETA: 35s - loss: 3.1128 - accuracy: 0.185 - ETA: 35s - loss: 3.1128 - accuracy: 0.185 - ETA: 34s - loss: 3.1124 - accuracy: 0.185 - ETA: 33s - loss: 3.1122 - accuracy: 0.185 - ETA: 32s - loss: 3.1114 - accuracy: 0.185 - ETA: 31s - loss: 3.1111 - accuracy: 0.185 - ETA: 30s - loss: 3.1109 - accuracy: 0.185 - ETA: 29s - loss: 3.1114 - accuracy: 0.185 - ETA: 28s - loss: 3.1110 - accuracy: 0.185 - ETA: 27s - loss: 3.1110 - accuracy: 0.185 - ETA: 26s - loss: 3.1113 - accuracy: 0.185 - ETA: 25s - loss: 3.1112 - accuracy: 0.185 - ETA: 24s - loss: 3.1112 - accuracy: 0.185 - ETA: 23s - loss: 3.1107 - accuracy: 0.185 - ETA: 22s - loss: 3.1105 - accuracy: 0.185 - ETA: 21s - loss: 3.1106 - accuracy: 0.185 - ETA: 20s - loss: 3.1106 - accuracy: 0.185 - ETA: 19s - loss: 3.1104 - accuracy: 0.185 - ETA: 18s - loss: 3.1103 - accuracy: 0.185 - ETA: 18s - loss: 3.1106 - accuracy: 0.185 - ETA: 17s - loss: 3.1108 - accuracy: 0.185 - ETA: 16s - loss: 3.1110 - accuracy: 0.185 - ETA: 15s - loss: 3.1110 - accuracy: 0.185 - ETA: 14s - loss: 3.1117 - accuracy: 0.185 - ETA: 13s - loss: 3.1113 - accuracy: 0.185 - ETA: 12s - loss: 3.1111 - accuracy: 0.185 - ETA: 11s - loss: 3.1108 - accuracy: 0.185 - ETA: 10s - loss: 3.1103 - accuracy: 0.185 - ETA: 9s - loss: 3.1099 - accuracy: 0.185 - ETA: 8s - loss: 3.1095 - accuracy: 0.18 - ETA: 7s - loss: 3.1098 - accuracy: 0.18 - ETA: 6s - loss: 3.1100 - accuracy: 0.18 - ETA: 5s - loss: 3.1098 - accuracy: 0.18 - ETA: 4s - loss: 3.1103 - accuracy: 0.18 - ETA: 3s - loss: 3.1092 - accuracy: 0.18 - ETA: 2s - loss: 3.1087 - accuracy: 0.18 - ETA: 1s - loss: 3.1087 - accuracy: 0.18 - ETA: 1s - loss: 3.1086 - accuracy: 0.18 - ETA: 0s - loss: 3.1081 - accuracy: 0.18 - 336s 8ms/step - loss: 3.1079 - accuracy: 0.1858 - val_loss: 4.4713 - val_accuracy: 0.0373\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:43 - loss: 3.1676 - accuracy: 0.11 - ETA: 5:50 - loss: 3.1697 - accuracy: 0.15 - ETA: 5:43 - loss: 3.1511 - accuracy: 0.16 - ETA: 5:40 - loss: 3.1665 - accuracy: 0.16 - ETA: 5:38 - loss: 3.1495 - accuracy: 0.17 - ETA: 5:36 - loss: 3.1108 - accuracy: 0.18 - ETA: 5:31 - loss: 3.0908 - accuracy: 0.18 - ETA: 5:29 - loss: 3.0697 - accuracy: 0.19 - ETA: 5:28 - loss: 3.2213 - accuracy: 0.18 - ETA: 5:26 - loss: 3.1918 - accuracy: 0.18 - ETA: 5:24 - loss: 3.1904 - accuracy: 0.19 - ETA: 5:22 - loss: 3.1864 - accuracy: 0.18 - ETA: 5:19 - loss: 3.1689 - accuracy: 0.18 - ETA: 5:16 - loss: 3.1627 - accuracy: 0.18 - ETA: 5:14 - loss: 3.1443 - accuracy: 0.19 - ETA: 5:12 - loss: 3.1418 - accuracy: 0.19 - ETA: 5:10 - loss: 3.1373 - accuracy: 0.18 - ETA: 5:09 - loss: 3.1326 - accuracy: 0.18 - ETA: 5:08 - loss: 3.1342 - accuracy: 0.18 - ETA: 5:06 - loss: 3.1267 - accuracy: 0.18 - ETA: 5:04 - loss: 3.1210 - accuracy: 0.18 - ETA: 5:02 - loss: 3.1119 - accuracy: 0.19 - ETA: 5:01 - loss: 3.1089 - accuracy: 0.19 - ETA: 5:01 - loss: 3.1128 - accuracy: 0.18 - ETA: 4:59 - loss: 3.1157 - accuracy: 0.18 - ETA: 4:57 - loss: 3.1058 - accuracy: 0.19 - ETA: 4:56 - loss: 3.1001 - accuracy: 0.19 - ETA: 4:54 - loss: 3.1018 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0971 - accuracy: 0.19 - ETA: 4:52 - loss: 3.1003 - accuracy: 0.19 - ETA: 4:50 - loss: 3.1011 - accuracy: 0.19 - ETA: 4:50 - loss: 3.1019 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0978 - accuracy: 0.19 - ETA: 4:47 - loss: 3.1008 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0972 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0988 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0945 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0972 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0926 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0940 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0970 - accuracy: 0.18 - ETA: 4:38 - loss: 3.0949 - accuracy: 0.18 - ETA: 4:37 - loss: 3.0958 - accuracy: 0.18 - ETA: 4:35 - loss: 3.0989 - accuracy: 0.18 - ETA: 4:34 - loss: 3.0991 - accuracy: 0.18 - ETA: 4:33 - loss: 3.0996 - accuracy: 0.18 - ETA: 4:32 - loss: 3.1016 - accuracy: 0.18 - ETA: 4:31 - loss: 3.0997 - accuracy: 0.18 - ETA: 4:30 - loss: 3.0968 - accuracy: 0.18 - ETA: 4:29 - loss: 3.0957 - accuracy: 0.18 - ETA: 4:28 - loss: 3.0983 - accuracy: 0.18 - ETA: 4:27 - loss: 3.0983 - accuracy: 0.18 - ETA: 4:26 - loss: 3.0909 - accuracy: 0.18 - ETA: 4:25 - loss: 3.0893 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0886 - accuracy: 0.18 - ETA: 4:23 - loss: 3.0872 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0886 - accuracy: 0.18 - ETA: 4:21 - loss: 3.0876 - accuracy: 0.18 - ETA: 4:20 - loss: 3.0858 - accuracy: 0.18 - ETA: 4:19 - loss: 3.0853 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0848 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0858 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0808 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0793 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0807 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0778 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0780 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0775 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0777 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0764 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0775 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0791 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0783 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0799 - accuracy: 0.19 - ETA: 4:04 - loss: 3.0771 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0789 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0760 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0726 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0723 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0707 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0691 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0681 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0667 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0721 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0740 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0739 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0750 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0744 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0717 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0717 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0853 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0839 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0824 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0818 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0831 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0806 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0802 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0785 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0787 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0793 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0804 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0808 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0811 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0823 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0817 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0801 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0797 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0801 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0780 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0784 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0776 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0789 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0778 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0786 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0795 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0803 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0785 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0778 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0778 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0783 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0812 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0797 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0801 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0801 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0805 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0803 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0812 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0797 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0804 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0823 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0811 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0807 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0799 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0800 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0795 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0796 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0787 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0789 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0805 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0804 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0813 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0827 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0826 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0823 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0814 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0809 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0801 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0807 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0804 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0815 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0815 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0816 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0809 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0811 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0808 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0801 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0792 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0799 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0805 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0801 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0815 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0806 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0808 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0815 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0809 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0803 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0800 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0796 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0799 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0798 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0785 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0766 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0761 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0779 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0767 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0764 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0767 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0764 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0755 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0744 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0747 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0751 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0742 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0741 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0746 - accuracy: 0.19 - ETA: 2:17 - loss: 3.0758 - accuracy: 0.1935"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:16 - loss: 3.0746 - accuracy: 0.19 - ETA: 2:15 - loss: 3.0753 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0753 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0751 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0758 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0757 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0761 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0757 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0756 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0752 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0738 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0739 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0735 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0742 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0739 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0741 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0746 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0742 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0749 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0752 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0759 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0761 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0759 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0758 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0745 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0744 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0736 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0739 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0740 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0736 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0729 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0735 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0737 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0740 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0742 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0748 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0749 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0758 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0749 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0749 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0751 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0749 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0744 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0751 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0741 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0735 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0735 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0723 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0721 - accuracy: 0.19 - ETA: 1:30 - loss: 3.0715 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0709 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0715 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0716 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0711 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0712 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0724 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0733 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0729 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0735 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0745 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0743 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0734 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0732 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0731 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0727 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0725 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0723 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0728 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0728 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0725 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0720 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0721 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0721 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0719 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0725 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0730 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0724 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0719 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0716 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0708 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0712 - accuracy: 0.19 - ETA: 59s - loss: 3.0711 - accuracy: 0.1956 - ETA: 58s - loss: 3.0712 - accuracy: 0.195 - ETA: 57s - loss: 3.0705 - accuracy: 0.195 - ETA: 56s - loss: 3.0703 - accuracy: 0.195 - ETA: 55s - loss: 3.0701 - accuracy: 0.195 - ETA: 54s - loss: 3.0693 - accuracy: 0.195 - ETA: 54s - loss: 3.0693 - accuracy: 0.195 - ETA: 53s - loss: 3.0692 - accuracy: 0.195 - ETA: 52s - loss: 3.0688 - accuracy: 0.195 - ETA: 51s - loss: 3.0690 - accuracy: 0.195 - ETA: 50s - loss: 3.0689 - accuracy: 0.196 - ETA: 49s - loss: 3.0683 - accuracy: 0.196 - ETA: 48s - loss: 3.0683 - accuracy: 0.196 - ETA: 47s - loss: 3.0682 - accuracy: 0.196 - ETA: 46s - loss: 3.0683 - accuracy: 0.196 - ETA: 45s - loss: 3.0690 - accuracy: 0.196 - ETA: 44s - loss: 3.0683 - accuracy: 0.196 - ETA: 43s - loss: 3.0674 - accuracy: 0.196 - ETA: 42s - loss: 3.0667 - accuracy: 0.196 - ETA: 41s - loss: 3.0668 - accuracy: 0.196 - ETA: 40s - loss: 3.0665 - accuracy: 0.196 - ETA: 39s - loss: 3.0666 - accuracy: 0.196 - ETA: 38s - loss: 3.0667 - accuracy: 0.196 - ETA: 37s - loss: 3.0669 - accuracy: 0.196 - ETA: 37s - loss: 3.0667 - accuracy: 0.196 - ETA: 36s - loss: 3.0668 - accuracy: 0.196 - ETA: 35s - loss: 3.0667 - accuracy: 0.196 - ETA: 34s - loss: 3.0664 - accuracy: 0.196 - ETA: 33s - loss: 3.0659 - accuracy: 0.196 - ETA: 32s - loss: 3.0646 - accuracy: 0.196 - ETA: 31s - loss: 3.0641 - accuracy: 0.196 - ETA: 30s - loss: 3.0636 - accuracy: 0.196 - ETA: 29s - loss: 3.0627 - accuracy: 0.197 - ETA: 28s - loss: 3.0623 - accuracy: 0.197 - ETA: 27s - loss: 3.0615 - accuracy: 0.197 - ETA: 26s - loss: 3.0609 - accuracy: 0.197 - ETA: 25s - loss: 3.0608 - accuracy: 0.197 - ETA: 24s - loss: 3.0605 - accuracy: 0.197 - ETA: 23s - loss: 3.0616 - accuracy: 0.197 - ETA: 22s - loss: 3.0609 - accuracy: 0.197 - ETA: 21s - loss: 3.0609 - accuracy: 0.197 - ETA: 20s - loss: 3.0608 - accuracy: 0.197 - ETA: 19s - loss: 3.0607 - accuracy: 0.197 - ETA: 19s - loss: 3.0614 - accuracy: 0.197 - ETA: 18s - loss: 3.0608 - accuracy: 0.197 - ETA: 17s - loss: 3.0609 - accuracy: 0.197 - ETA: 16s - loss: 3.0605 - accuracy: 0.197 - ETA: 15s - loss: 3.0606 - accuracy: 0.197 - ETA: 14s - loss: 3.0605 - accuracy: 0.197 - ETA: 13s - loss: 3.0608 - accuracy: 0.197 - ETA: 12s - loss: 3.0611 - accuracy: 0.197 - ETA: 11s - loss: 3.0613 - accuracy: 0.197 - ETA: 10s - loss: 3.0611 - accuracy: 0.197 - ETA: 9s - loss: 3.0603 - accuracy: 0.197 - ETA: 8s - loss: 3.0602 - accuracy: 0.19 - ETA: 7s - loss: 3.0604 - accuracy: 0.19 - ETA: 6s - loss: 3.0609 - accuracy: 0.19 - ETA: 5s - loss: 3.0613 - accuracy: 0.19 - ETA: 4s - loss: 3.0616 - accuracy: 0.19 - ETA: 3s - loss: 3.0617 - accuracy: 0.19 - ETA: 2s - loss: 3.0615 - accuracy: 0.19 - ETA: 1s - loss: 3.0609 - accuracy: 0.19 - ETA: 1s - loss: 3.0610 - accuracy: 0.19 - ETA: 0s - loss: 3.0607 - accuracy: 0.19 - 337s 8ms/step - loss: 3.0606 - accuracy: 0.1976 - val_loss: 4.4099 - val_accuracy: 0.0349\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:22 - loss: 2.7795 - accuracy: 0.21 - ETA: 5:12 - loss: 3.0417 - accuracy: 0.18 - ETA: 5:09 - loss: 3.0347 - accuracy: 0.19 - ETA: 5:06 - loss: 3.0395 - accuracy: 0.19 - ETA: 5:03 - loss: 3.0399 - accuracy: 0.19 - ETA: 5:02 - loss: 3.0165 - accuracy: 0.19 - ETA: 5:01 - loss: 3.0259 - accuracy: 0.19 - ETA: 5:02 - loss: 3.0117 - accuracy: 0.19 - ETA: 5:00 - loss: 3.0218 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0161 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0227 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0296 - accuracy: 0.19 - ETA: 4:58 - loss: 3.0322 - accuracy: 0.19 - ETA: 4:57 - loss: 3.0184 - accuracy: 0.19 - ETA: 4:56 - loss: 2.9913 - accuracy: 0.20 - ETA: 4:55 - loss: 3.0046 - accuracy: 0.20 - ETA: 4:54 - loss: 3.0074 - accuracy: 0.19 - ETA: 4:53 - loss: 3.0086 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0094 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0157 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0142 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0112 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0161 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0009 - accuracy: 0.20 - ETA: 4:48 - loss: 3.0116 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0146 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0189 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0235 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0158 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0097 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0087 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0140 - accuracy: 0.19 - ETA: 4:39 - loss: 3.0073 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0131 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0127 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0192 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0237 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0246 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0248 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0238 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0208 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0171 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0198 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0249 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0293 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0303 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0362 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0372 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0345 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0453 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0434 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0507 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0505 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0525 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0535 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0517 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0510 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0515 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0479 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0482 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0458 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0450 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0434 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0464 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0453 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0468 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0462 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0484 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0476 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0489 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0480 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0489 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0511 - accuracy: 0.19 - ETA: 4:01 - loss: 3.0475 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0479 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0470 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0491 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0492 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0545 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0544 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0543 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0553 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0566 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0542 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0543 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0548 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0546 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0540 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0549 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0558 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0544 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0536 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0527 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0554 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0570 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0561 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0545 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0527 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0523 - accuracy: 0.19 - ETA: 3:36 - loss: 3.0537 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0549 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0537 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0541 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0553 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0545 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0551 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0551 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0541 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0535 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0530 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0537 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0543 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0527 - accuracy: 0.19 - ETA: 3:23 - loss: 3.0527 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0531 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0520 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0533 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0524 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0519 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0526 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0523 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0540 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0554 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0547 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0553 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0535 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0537 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0532 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0516 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0525 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0514 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0497 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0481 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0482 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0475 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0477 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0474 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0451 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0448 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0446 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0450 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0445 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0424 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0403 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0393 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0389 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0375 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0359 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0353 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0343 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0350 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0348 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0354 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0368 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0371 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0367 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0367 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0365 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0363 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0368 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0375 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0378 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0378 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0377 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0389 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0401 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0410 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0409 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0401 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0398 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0396 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0408 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0404 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0413 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0422 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0420 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0429 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0416 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0406 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0416 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0408 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0408 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0411 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0418 - accuracy: 0.19 - ETA: 2:17 - loss: 3.0405 - accuracy: 0.19 - ETA: 2:16 - loss: 3.0400 - accuracy: 0.1956"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:15 - loss: 3.0410 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0406 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0402 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0408 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0397 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0389 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0398 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0392 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0397 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0396 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0398 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0393 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0395 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0391 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0391 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0379 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0380 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0382 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0377 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0377 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0372 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0382 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0395 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0392 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0391 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0392 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0398 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0398 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0405 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0406 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0405 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0413 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0407 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0409 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0413 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0411 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0416 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0410 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0419 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0421 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0429 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0430 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0431 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0437 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0445 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0465 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0472 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0484 - accuracy: 0.19 - ETA: 1:30 - loss: 3.0496 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0493 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0496 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0496 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0491 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0485 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0491 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0488 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0486 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0491 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0498 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0494 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0499 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0498 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0500 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0504 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0505 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0506 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0508 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0513 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0517 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0518 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0515 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0514 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0513 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0507 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0511 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0509 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0509 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0501 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0502 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0505 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0507 - accuracy: 0.19 - ETA: 59s - loss: 3.0509 - accuracy: 0.1953 - ETA: 58s - loss: 3.0511 - accuracy: 0.195 - ETA: 57s - loss: 3.0511 - accuracy: 0.195 - ETA: 56s - loss: 3.0507 - accuracy: 0.195 - ETA: 55s - loss: 3.0512 - accuracy: 0.195 - ETA: 54s - loss: 3.0517 - accuracy: 0.195 - ETA: 53s - loss: 3.0516 - accuracy: 0.195 - ETA: 52s - loss: 3.0521 - accuracy: 0.195 - ETA: 51s - loss: 3.0524 - accuracy: 0.195 - ETA: 50s - loss: 3.0528 - accuracy: 0.195 - ETA: 50s - loss: 3.0530 - accuracy: 0.194 - ETA: 49s - loss: 3.0526 - accuracy: 0.194 - ETA: 48s - loss: 3.0529 - accuracy: 0.194 - ETA: 47s - loss: 3.0529 - accuracy: 0.194 - ETA: 46s - loss: 3.0527 - accuracy: 0.194 - ETA: 45s - loss: 3.0522 - accuracy: 0.194 - ETA: 44s - loss: 3.0518 - accuracy: 0.195 - ETA: 43s - loss: 3.0519 - accuracy: 0.195 - ETA: 42s - loss: 3.0521 - accuracy: 0.195 - ETA: 41s - loss: 3.0520 - accuracy: 0.195 - ETA: 40s - loss: 3.0526 - accuracy: 0.195 - ETA: 39s - loss: 3.0524 - accuracy: 0.195 - ETA: 38s - loss: 3.0522 - accuracy: 0.195 - ETA: 37s - loss: 3.0521 - accuracy: 0.195 - ETA: 36s - loss: 3.0523 - accuracy: 0.195 - ETA: 35s - loss: 3.0517 - accuracy: 0.195 - ETA: 34s - loss: 3.0523 - accuracy: 0.195 - ETA: 33s - loss: 3.0522 - accuracy: 0.195 - ETA: 33s - loss: 3.0530 - accuracy: 0.195 - ETA: 32s - loss: 3.0525 - accuracy: 0.195 - ETA: 31s - loss: 3.0531 - accuracy: 0.195 - ETA: 30s - loss: 3.0525 - accuracy: 0.195 - ETA: 29s - loss: 3.0530 - accuracy: 0.195 - ETA: 28s - loss: 3.0523 - accuracy: 0.195 - ETA: 27s - loss: 3.0522 - accuracy: 0.195 - ETA: 26s - loss: 3.0519 - accuracy: 0.195 - ETA: 25s - loss: 3.0515 - accuracy: 0.195 - ETA: 24s - loss: 3.0516 - accuracy: 0.195 - ETA: 23s - loss: 3.0511 - accuracy: 0.195 - ETA: 22s - loss: 3.0509 - accuracy: 0.195 - ETA: 21s - loss: 3.0503 - accuracy: 0.196 - ETA: 20s - loss: 3.0512 - accuracy: 0.195 - ETA: 19s - loss: 3.0515 - accuracy: 0.195 - ETA: 18s - loss: 3.0508 - accuracy: 0.195 - ETA: 17s - loss: 3.0510 - accuracy: 0.195 - ETA: 17s - loss: 3.0510 - accuracy: 0.195 - ETA: 16s - loss: 3.0511 - accuracy: 0.195 - ETA: 15s - loss: 3.0510 - accuracy: 0.195 - ETA: 14s - loss: 3.0506 - accuracy: 0.195 - ETA: 13s - loss: 3.0510 - accuracy: 0.195 - ETA: 12s - loss: 3.0507 - accuracy: 0.195 - ETA: 11s - loss: 3.0511 - accuracy: 0.195 - ETA: 10s - loss: 3.0513 - accuracy: 0.195 - ETA: 9s - loss: 3.0515 - accuracy: 0.195 - ETA: 8s - loss: 3.0513 - accuracy: 0.19 - ETA: 7s - loss: 3.0516 - accuracy: 0.19 - ETA: 6s - loss: 3.0514 - accuracy: 0.19 - ETA: 5s - loss: 3.0513 - accuracy: 0.19 - ETA: 4s - loss: 3.0509 - accuracy: 0.19 - ETA: 3s - loss: 3.0506 - accuracy: 0.19 - ETA: 2s - loss: 3.0504 - accuracy: 0.19 - ETA: 1s - loss: 3.0504 - accuracy: 0.19 - ETA: 1s - loss: 3.0504 - accuracy: 0.19 - ETA: 0s - loss: 3.0497 - accuracy: 0.19 - 335s 8ms/step - loss: 3.0501 - accuracy: 0.1955 - val_loss: 4.6633 - val_accuracy: 0.0392\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 5:18 - loss: 3.0033 - accuracy: 0.22 - ETA: 5:08 - loss: 3.0559 - accuracy: 0.19 - ETA: 5:06 - loss: 3.0929 - accuracy: 0.18 - ETA: 5:03 - loss: 3.0734 - accuracy: 0.19 - ETA: 5:04 - loss: 3.1015 - accuracy: 0.18 - ETA: 5:02 - loss: 3.1120 - accuracy: 0.18 - ETA: 5:00 - loss: 3.1173 - accuracy: 0.18 - ETA: 4:59 - loss: 3.0925 - accuracy: 0.19 - ETA: 4:57 - loss: 3.0938 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0693 - accuracy: 0.19 - ETA: 4:56 - loss: 3.0587 - accuracy: 0.19 - ETA: 4:55 - loss: 3.0664 - accuracy: 0.19 - ETA: 4:55 - loss: 3.0607 - accuracy: 0.19 - ETA: 4:54 - loss: 3.0674 - accuracy: 0.19 - ETA: 4:52 - loss: 3.0658 - accuracy: 0.19 - ETA: 4:51 - loss: 3.0603 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0619 - accuracy: 0.19 - ETA: 4:50 - loss: 3.0584 - accuracy: 0.19 - ETA: 4:49 - loss: 3.0614 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0624 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0663 - accuracy: 0.19 - ETA: 4:48 - loss: 3.0616 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0681 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0668 - accuracy: 0.19 - ETA: 4:46 - loss: 3.0741 - accuracy: 0.19 - ETA: 4:45 - loss: 3.0697 - accuracy: 0.19 - ETA: 4:44 - loss: 3.0667 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0679 - accuracy: 0.19 - ETA: 4:43 - loss: 3.0704 - accuracy: 0.19 - ETA: 4:42 - loss: 3.0672 - accuracy: 0.19 - ETA: 4:41 - loss: 3.0722 - accuracy: 0.19 - ETA: 4:40 - loss: 3.0730 - accuracy: 0.19 - ETA: 4:38 - loss: 3.0798 - accuracy: 0.19 - ETA: 4:37 - loss: 3.0761 - accuracy: 0.19 - ETA: 4:36 - loss: 3.0740 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0726 - accuracy: 0.19 - ETA: 4:35 - loss: 3.0707 - accuracy: 0.19 - ETA: 4:34 - loss: 3.0681 - accuracy: 0.19 - ETA: 4:33 - loss: 3.0635 - accuracy: 0.19 - ETA: 4:32 - loss: 3.0625 - accuracy: 0.19 - ETA: 4:31 - loss: 3.0657 - accuracy: 0.19 - ETA: 4:30 - loss: 3.0682 - accuracy: 0.19 - ETA: 4:29 - loss: 3.0642 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0621 - accuracy: 0.19 - ETA: 4:28 - loss: 3.0580 - accuracy: 0.19 - ETA: 4:27 - loss: 3.0556 - accuracy: 0.19 - ETA: 4:26 - loss: 3.0564 - accuracy: 0.19 - ETA: 4:25 - loss: 3.0574 - accuracy: 0.19 - ETA: 4:24 - loss: 3.0562 - accuracy: 0.19 - ETA: 4:23 - loss: 3.0578 - accuracy: 0.19 - ETA: 4:22 - loss: 3.0552 - accuracy: 0.19 - ETA: 4:21 - loss: 3.0553 - accuracy: 0.19 - ETA: 4:20 - loss: 3.0546 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0539 - accuracy: 0.19 - ETA: 4:19 - loss: 3.0525 - accuracy: 0.19 - ETA: 4:18 - loss: 3.0529 - accuracy: 0.19 - ETA: 4:17 - loss: 3.0553 - accuracy: 0.19 - ETA: 4:16 - loss: 3.0520 - accuracy: 0.19 - ETA: 4:15 - loss: 3.0505 - accuracy: 0.19 - ETA: 4:14 - loss: 3.0516 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0544 - accuracy: 0.19 - ETA: 4:13 - loss: 3.0563 - accuracy: 0.19 - ETA: 4:12 - loss: 3.0551 - accuracy: 0.19 - ETA: 4:11 - loss: 3.0517 - accuracy: 0.19 - ETA: 4:10 - loss: 3.0554 - accuracy: 0.19 - ETA: 4:09 - loss: 3.0550 - accuracy: 0.19 - ETA: 4:08 - loss: 3.0552 - accuracy: 0.19 - ETA: 4:07 - loss: 3.0559 - accuracy: 0.19 - ETA: 4:06 - loss: 3.0552 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0528 - accuracy: 0.19 - ETA: 4:05 - loss: 3.0528 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0519 - accuracy: 0.19 - ETA: 4:03 - loss: 3.0530 - accuracy: 0.19 - ETA: 4:02 - loss: 3.0522 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0511 - accuracy: 0.19 - ETA: 4:00 - loss: 3.0513 - accuracy: 0.19 - ETA: 3:59 - loss: 3.0506 - accuracy: 0.19 - ETA: 3:58 - loss: 3.0504 - accuracy: 0.19 - ETA: 3:57 - loss: 3.0513 - accuracy: 0.19 - ETA: 3:56 - loss: 3.0521 - accuracy: 0.19 - ETA: 3:55 - loss: 3.0507 - accuracy: 0.19 - ETA: 3:54 - loss: 3.0497 - accuracy: 0.19 - ETA: 3:53 - loss: 3.0495 - accuracy: 0.19 - ETA: 3:52 - loss: 3.0482 - accuracy: 0.19 - ETA: 3:51 - loss: 3.0474 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0448 - accuracy: 0.19 - ETA: 3:50 - loss: 3.0431 - accuracy: 0.19 - ETA: 3:49 - loss: 3.0429 - accuracy: 0.19 - ETA: 3:48 - loss: 3.0415 - accuracy: 0.19 - ETA: 3:47 - loss: 3.0417 - accuracy: 0.19 - ETA: 3:46 - loss: 3.0427 - accuracy: 0.19 - ETA: 3:45 - loss: 3.0406 - accuracy: 0.19 - ETA: 3:44 - loss: 3.0403 - accuracy: 0.19 - ETA: 3:43 - loss: 3.0394 - accuracy: 0.19 - ETA: 3:42 - loss: 3.0402 - accuracy: 0.19 - ETA: 3:41 - loss: 3.0421 - accuracy: 0.19 - ETA: 3:40 - loss: 3.0407 - accuracy: 0.19 - ETA: 3:39 - loss: 3.0398 - accuracy: 0.19 - ETA: 3:38 - loss: 3.0389 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0396 - accuracy: 0.19 - ETA: 3:37 - loss: 3.0423 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0415 - accuracy: 0.19 - ETA: 3:35 - loss: 3.0408 - accuracy: 0.19 - ETA: 3:34 - loss: 3.0441 - accuracy: 0.19 - ETA: 3:33 - loss: 3.0424 - accuracy: 0.19 - ETA: 3:32 - loss: 3.0428 - accuracy: 0.19 - ETA: 3:31 - loss: 3.0441 - accuracy: 0.19 - ETA: 3:30 - loss: 3.0440 - accuracy: 0.19 - ETA: 3:29 - loss: 3.0444 - accuracy: 0.19 - ETA: 3:28 - loss: 3.0461 - accuracy: 0.19 - ETA: 3:27 - loss: 3.0471 - accuracy: 0.19 - ETA: 3:26 - loss: 3.0462 - accuracy: 0.19 - ETA: 3:25 - loss: 3.0506 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0524 - accuracy: 0.19 - ETA: 3:24 - loss: 3.0520 - accuracy: 0.19 - ETA: 3:22 - loss: 3.0549 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0535 - accuracy: 0.19 - ETA: 3:21 - loss: 3.0506 - accuracy: 0.19 - ETA: 3:20 - loss: 3.0512 - accuracy: 0.19 - ETA: 3:19 - loss: 3.0511 - accuracy: 0.19 - ETA: 3:18 - loss: 3.0509 - accuracy: 0.19 - ETA: 3:17 - loss: 3.0488 - accuracy: 0.19 - ETA: 3:16 - loss: 3.0490 - accuracy: 0.19 - ETA: 3:15 - loss: 3.0479 - accuracy: 0.19 - ETA: 3:14 - loss: 3.0478 - accuracy: 0.19 - ETA: 3:13 - loss: 3.0504 - accuracy: 0.19 - ETA: 3:12 - loss: 3.0499 - accuracy: 0.19 - ETA: 3:11 - loss: 3.0499 - accuracy: 0.19 - ETA: 3:10 - loss: 3.0504 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0509 - accuracy: 0.19 - ETA: 3:09 - loss: 3.0492 - accuracy: 0.19 - ETA: 3:08 - loss: 3.0485 - accuracy: 0.19 - ETA: 3:07 - loss: 3.0485 - accuracy: 0.19 - ETA: 3:06 - loss: 3.0484 - accuracy: 0.19 - ETA: 3:05 - loss: 3.0482 - accuracy: 0.19 - ETA: 3:04 - loss: 3.0479 - accuracy: 0.19 - ETA: 3:03 - loss: 3.0474 - accuracy: 0.19 - ETA: 3:02 - loss: 3.0461 - accuracy: 0.19 - ETA: 3:01 - loss: 3.0462 - accuracy: 0.19 - ETA: 3:00 - loss: 3.0466 - accuracy: 0.19 - ETA: 2:59 - loss: 3.0472 - accuracy: 0.19 - ETA: 2:58 - loss: 3.0477 - accuracy: 0.19 - ETA: 2:57 - loss: 3.0479 - accuracy: 0.19 - ETA: 2:56 - loss: 3.0466 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0468 - accuracy: 0.19 - ETA: 2:55 - loss: 3.0459 - accuracy: 0.19 - ETA: 2:54 - loss: 3.0477 - accuracy: 0.19 - ETA: 2:53 - loss: 3.0487 - accuracy: 0.19 - ETA: 2:52 - loss: 3.0477 - accuracy: 0.19 - ETA: 2:51 - loss: 3.0473 - accuracy: 0.19 - ETA: 2:50 - loss: 3.0467 - accuracy: 0.19 - ETA: 2:49 - loss: 3.0476 - accuracy: 0.19 - ETA: 2:48 - loss: 3.0489 - accuracy: 0.19 - ETA: 2:47 - loss: 3.0487 - accuracy: 0.19 - ETA: 2:46 - loss: 3.0497 - accuracy: 0.19 - ETA: 2:45 - loss: 3.0491 - accuracy: 0.19 - ETA: 2:44 - loss: 3.0483 - accuracy: 0.19 - ETA: 2:43 - loss: 3.0481 - accuracy: 0.19 - ETA: 2:42 - loss: 3.0478 - accuracy: 0.19 - ETA: 2:41 - loss: 3.0485 - accuracy: 0.19 - ETA: 2:40 - loss: 3.0479 - accuracy: 0.19 - ETA: 2:39 - loss: 3.0463 - accuracy: 0.19 - ETA: 2:38 - loss: 3.0450 - accuracy: 0.19 - ETA: 2:37 - loss: 3.0453 - accuracy: 0.19 - ETA: 2:36 - loss: 3.0447 - accuracy: 0.19 - ETA: 2:35 - loss: 3.0438 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0428 - accuracy: 0.19 - ETA: 2:34 - loss: 3.0423 - accuracy: 0.19 - ETA: 2:33 - loss: 3.0413 - accuracy: 0.19 - ETA: 2:32 - loss: 3.0417 - accuracy: 0.19 - ETA: 2:31 - loss: 3.0417 - accuracy: 0.19 - ETA: 2:30 - loss: 3.0436 - accuracy: 0.19 - ETA: 2:29 - loss: 3.0438 - accuracy: 0.19 - ETA: 2:28 - loss: 3.0440 - accuracy: 0.19 - ETA: 2:27 - loss: 3.0446 - accuracy: 0.19 - ETA: 2:26 - loss: 3.0446 - accuracy: 0.19 - ETA: 2:25 - loss: 3.0442 - accuracy: 0.19 - ETA: 2:24 - loss: 3.0446 - accuracy: 0.19 - ETA: 2:23 - loss: 3.0444 - accuracy: 0.19 - ETA: 2:22 - loss: 3.0449 - accuracy: 0.19 - ETA: 2:21 - loss: 3.0435 - accuracy: 0.19 - ETA: 2:20 - loss: 3.0430 - accuracy: 0.19 - ETA: 2:19 - loss: 3.0437 - accuracy: 0.19 - ETA: 2:18 - loss: 3.0423 - accuracy: 0.19 - ETA: 2:17 - loss: 3.0416 - accuracy: 0.19 - ETA: 2:16 - loss: 3.0414 - accuracy: 0.1958"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 2:15 - loss: 3.0417 - accuracy: 0.19 - ETA: 2:14 - loss: 3.0420 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0413 - accuracy: 0.19 - ETA: 2:13 - loss: 3.0405 - accuracy: 0.19 - ETA: 2:12 - loss: 3.0409 - accuracy: 0.19 - ETA: 2:11 - loss: 3.0415 - accuracy: 0.19 - ETA: 2:10 - loss: 3.0418 - accuracy: 0.19 - ETA: 2:09 - loss: 3.0424 - accuracy: 0.19 - ETA: 2:08 - loss: 3.0427 - accuracy: 0.19 - ETA: 2:07 - loss: 3.0425 - accuracy: 0.19 - ETA: 2:06 - loss: 3.0419 - accuracy: 0.19 - ETA: 2:05 - loss: 3.0416 - accuracy: 0.19 - ETA: 2:04 - loss: 3.0407 - accuracy: 0.19 - ETA: 2:03 - loss: 3.0403 - accuracy: 0.19 - ETA: 2:02 - loss: 3.0400 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0402 - accuracy: 0.19 - ETA: 2:01 - loss: 3.0411 - accuracy: 0.19 - ETA: 2:00 - loss: 3.0405 - accuracy: 0.19 - ETA: 1:59 - loss: 3.0557 - accuracy: 0.19 - ETA: 1:58 - loss: 3.0549 - accuracy: 0.19 - ETA: 1:57 - loss: 3.0548 - accuracy: 0.19 - ETA: 1:56 - loss: 3.0548 - accuracy: 0.19 - ETA: 1:55 - loss: 3.0537 - accuracy: 0.19 - ETA: 1:54 - loss: 3.0541 - accuracy: 0.19 - ETA: 1:53 - loss: 3.0532 - accuracy: 0.19 - ETA: 1:52 - loss: 3.0537 - accuracy: 0.19 - ETA: 1:51 - loss: 3.0536 - accuracy: 0.19 - ETA: 1:50 - loss: 3.0534 - accuracy: 0.19 - ETA: 1:49 - loss: 3.0552 - accuracy: 0.19 - ETA: 1:48 - loss: 3.0548 - accuracy: 0.19 - ETA: 1:47 - loss: 3.0547 - accuracy: 0.19 - ETA: 1:46 - loss: 3.0544 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0547 - accuracy: 0.19 - ETA: 1:45 - loss: 3.0543 - accuracy: 0.19 - ETA: 1:44 - loss: 3.0547 - accuracy: 0.19 - ETA: 1:43 - loss: 3.0544 - accuracy: 0.19 - ETA: 1:42 - loss: 3.0543 - accuracy: 0.19 - ETA: 1:41 - loss: 3.0542 - accuracy: 0.19 - ETA: 1:40 - loss: 3.0537 - accuracy: 0.19 - ETA: 1:39 - loss: 3.0546 - accuracy: 0.19 - ETA: 1:38 - loss: 3.0550 - accuracy: 0.19 - ETA: 1:37 - loss: 3.0555 - accuracy: 0.19 - ETA: 1:36 - loss: 3.0558 - accuracy: 0.19 - ETA: 1:35 - loss: 3.0554 - accuracy: 0.19 - ETA: 1:34 - loss: 3.0555 - accuracy: 0.19 - ETA: 1:33 - loss: 3.0559 - accuracy: 0.19 - ETA: 1:32 - loss: 3.0563 - accuracy: 0.19 - ETA: 1:31 - loss: 3.0567 - accuracy: 0.19 - ETA: 1:30 - loss: 3.0559 - accuracy: 0.19 - ETA: 1:29 - loss: 3.0560 - accuracy: 0.19 - ETA: 1:28 - loss: 3.0559 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0567 - accuracy: 0.19 - ETA: 1:27 - loss: 3.0568 - accuracy: 0.19 - ETA: 1:26 - loss: 3.0562 - accuracy: 0.19 - ETA: 1:25 - loss: 3.0561 - accuracy: 0.19 - ETA: 1:24 - loss: 3.0554 - accuracy: 0.19 - ETA: 1:23 - loss: 3.0550 - accuracy: 0.19 - ETA: 1:22 - loss: 3.0548 - accuracy: 0.19 - ETA: 1:21 - loss: 3.0551 - accuracy: 0.19 - ETA: 1:20 - loss: 3.0545 - accuracy: 0.19 - ETA: 1:19 - loss: 3.0550 - accuracy: 0.19 - ETA: 1:18 - loss: 3.0543 - accuracy: 0.19 - ETA: 1:17 - loss: 3.0540 - accuracy: 0.19 - ETA: 1:16 - loss: 3.0542 - accuracy: 0.19 - ETA: 1:15 - loss: 3.0542 - accuracy: 0.19 - ETA: 1:14 - loss: 3.0541 - accuracy: 0.19 - ETA: 1:13 - loss: 3.0554 - accuracy: 0.19 - ETA: 1:12 - loss: 3.0554 - accuracy: 0.19 - ETA: 1:11 - loss: 3.0569 - accuracy: 0.19 - ETA: 1:10 - loss: 3.0577 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0577 - accuracy: 0.19 - ETA: 1:09 - loss: 3.0591 - accuracy: 0.19 - ETA: 1:08 - loss: 3.0583 - accuracy: 0.19 - ETA: 1:07 - loss: 3.0582 - accuracy: 0.19 - ETA: 1:06 - loss: 3.0586 - accuracy: 0.19 - ETA: 1:05 - loss: 3.0588 - accuracy: 0.19 - ETA: 1:04 - loss: 3.0588 - accuracy: 0.19 - ETA: 1:03 - loss: 3.0591 - accuracy: 0.19 - ETA: 1:02 - loss: 3.0589 - accuracy: 0.19 - ETA: 1:01 - loss: 3.0593 - accuracy: 0.19 - ETA: 1:00 - loss: 3.0595 - accuracy: 0.19 - ETA: 59s - loss: 3.0595 - accuracy: 0.1959 - ETA: 58s - loss: 3.0596 - accuracy: 0.195 - ETA: 57s - loss: 3.0597 - accuracy: 0.195 - ETA: 56s - loss: 3.0599 - accuracy: 0.195 - ETA: 56s - loss: 3.0603 - accuracy: 0.195 - ETA: 55s - loss: 3.0609 - accuracy: 0.195 - ETA: 54s - loss: 3.0617 - accuracy: 0.195 - ETA: 53s - loss: 3.0623 - accuracy: 0.195 - ETA: 52s - loss: 3.0622 - accuracy: 0.195 - ETA: 51s - loss: 3.0618 - accuracy: 0.195 - ETA: 50s - loss: 3.0621 - accuracy: 0.195 - ETA: 49s - loss: 3.0625 - accuracy: 0.195 - ETA: 48s - loss: 3.0626 - accuracy: 0.195 - ETA: 47s - loss: 3.0628 - accuracy: 0.195 - ETA: 46s - loss: 3.0629 - accuracy: 0.195 - ETA: 45s - loss: 3.0635 - accuracy: 0.194 - ETA: 44s - loss: 3.0633 - accuracy: 0.195 - ETA: 43s - loss: 3.0629 - accuracy: 0.194 - ETA: 42s - loss: 3.0628 - accuracy: 0.194 - ETA: 41s - loss: 3.0625 - accuracy: 0.195 - ETA: 40s - loss: 3.0631 - accuracy: 0.194 - ETA: 39s - loss: 3.0633 - accuracy: 0.194 - ETA: 38s - loss: 3.0639 - accuracy: 0.194 - ETA: 37s - loss: 3.0647 - accuracy: 0.194 - ETA: 37s - loss: 3.0639 - accuracy: 0.194 - ETA: 36s - loss: 3.0648 - accuracy: 0.194 - ETA: 35s - loss: 3.0641 - accuracy: 0.194 - ETA: 34s - loss: 3.0639 - accuracy: 0.194 - ETA: 33s - loss: 3.0641 - accuracy: 0.194 - ETA: 32s - loss: 3.0637 - accuracy: 0.194 - ETA: 31s - loss: 3.0631 - accuracy: 0.194 - ETA: 30s - loss: 3.0643 - accuracy: 0.194 - ETA: 29s - loss: 3.0644 - accuracy: 0.194 - ETA: 28s - loss: 3.0643 - accuracy: 0.194 - ETA: 27s - loss: 3.0637 - accuracy: 0.194 - ETA: 26s - loss: 3.0635 - accuracy: 0.194 - ETA: 25s - loss: 3.0631 - accuracy: 0.194 - ETA: 24s - loss: 3.0636 - accuracy: 0.194 - ETA: 23s - loss: 3.0641 - accuracy: 0.194 - ETA: 22s - loss: 3.0641 - accuracy: 0.194 - ETA: 21s - loss: 3.0639 - accuracy: 0.194 - ETA: 20s - loss: 3.0636 - accuracy: 0.194 - ETA: 19s - loss: 3.0638 - accuracy: 0.194 - ETA: 19s - loss: 3.0629 - accuracy: 0.194 - ETA: 18s - loss: 3.0622 - accuracy: 0.194 - ETA: 17s - loss: 3.0623 - accuracy: 0.194 - ETA: 16s - loss: 3.0618 - accuracy: 0.194 - ETA: 15s - loss: 3.0622 - accuracy: 0.194 - ETA: 14s - loss: 3.0621 - accuracy: 0.194 - ETA: 13s - loss: 3.0618 - accuracy: 0.195 - ETA: 12s - loss: 3.0617 - accuracy: 0.194 - ETA: 11s - loss: 3.0614 - accuracy: 0.194 - ETA: 10s - loss: 3.0606 - accuracy: 0.195 - ETA: 9s - loss: 3.0598 - accuracy: 0.195 - ETA: 8s - loss: 3.0594 - accuracy: 0.19 - ETA: 7s - loss: 3.0593 - accuracy: 0.19 - ETA: 6s - loss: 3.0588 - accuracy: 0.19 - ETA: 5s - loss: 3.0587 - accuracy: 0.19 - ETA: 4s - loss: 3.0596 - accuracy: 0.19 - ETA: 3s - loss: 3.0601 - accuracy: 0.19 - ETA: 2s - loss: 3.0606 - accuracy: 0.19 - ETA: 1s - loss: 3.0603 - accuracy: 0.19 - ETA: 1s - loss: 3.0607 - accuracy: 0.19 - ETA: 0s - loss: 3.0602 - accuracy: 0.19 - 337s 8ms/step - loss: 3.0602 - accuracy: 0.1950 - val_loss: 4.3863 - val_accuracy: 0.0359\n",
      "7:39:03.339323\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "t5=datetime.datetime.now()\n",
    "print(t5)\n",
    "history=model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[mcp_save], batch_size=128)\n",
    "t6=datetime.datetime.now()\n",
    "print(t6-t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [3.9444654032301933, 3.9571416935947257, 3.9297480406604626, 3.913023831992478, 3.8945634974467, 3.87308010317772, 3.8503235593026814, 3.8734710119020583, 3.8726590988948293, 3.873644348221666, 3.8730728326162858, 3.854356324416757, 3.9060834473294994, 3.942205542801971, 3.9570386917580533, 3.930690352678109, 3.9903090807520285, 3.9351997340821034, 3.8875048475808263, 3.8881341178458104, 3.8707512646767954, 3.950949550798965, 4.068667607132473, 3.9355634940181092, 4.118806947216072, 4.0294390359033745, 3.904686902580404, 3.9617607585140404, 4.0019016569749235, 3.981201885641129, 4.067701466473804, 4.085525484419839, 4.117630083696634, 4.136453222337398, 4.230709481913705, 4.0277330021986595, 4.096606518644562, 4.118862955675577, 4.1670728695605215, 4.09030844955895, 4.129055843019599, 4.175965557230557, 4.131489007456471, 3.984609329956412, 4.016006185400964, 4.0061013140922705, 4.10418181899839, 4.061487499535199, 3.9780218082030605, 4.050549359588641, 4.03529002874206, 4.082606920772935, 4.2430973736573305, 4.047769892260417, 4.174036175395147, 4.18009651816326, 4.117475000955863, 4.065523580327761, 4.151055343690331, 4.168954211582477, 4.034935970516567, 4.043537234631666, 4.171408158384317, 4.139800858175323, 4.136623948402843, 4.273767610578311, 4.163639700847445, 4.222762444067256, 4.161090280445519, 4.087091122190342, 4.195833630958877, 4.382967004285243, 4.13398395255792, 4.059869945638359, 4.374813957890367, 4.2201992528053704, 4.128291043776972, 4.202268171998353, 4.227253031056265, 4.366535341795176, 4.148610073829746, 4.364500498725625, 4.101456783365745, 4.248799375874014, 4.340849316473591, 4.5089078153194, 4.3552617497516195, 4.386643315964933, 4.405183861448753, 4.302323490059621, 4.242020448581557, 4.427325240322288, 4.36967058142762, 4.166915105253976, 4.339562020978372, 4.27965168523393, 4.47129060148746, 4.40991567809731, 4.663344767558687, 4.386313695007328], 'val_accuracy': [0.014767692424356937, 0.025673065334558487, 0.019652390852570534, 0.024764284491539, 0.030671361833810806, 0.029421787708997726, 0.029308190569281578, 0.026354651898145676, 0.026581846177577972, 0.03362490236759186, 0.030557764694094658, 0.04487106576561928, 0.0374872200191021, 0.03896398842334747, 0.04305350407958031, 0.039418380707502365, 0.03907758742570877, 0.039531975984573364, 0.03975917398929596, 0.044530272483825684, 0.04180392995476723, 0.04668862745165825, 0.04782460629940033, 0.038168806582689285, 0.041349541395902634, 0.04543905332684517, 0.038736794143915176, 0.032716117799282074, 0.03907758742570877, 0.04169033467769623, 0.039418380707502365, 0.04157673567533493, 0.036464840173721313, 0.042031124234199524, 0.03850959986448288, 0.036919232457876205, 0.03680563345551491, 0.03907758742570877, 0.03975917398929596, 0.03430648520588875, 0.03828240558505058, 0.038850393146276474, 0.03248892351984978, 0.03464727848768234, 0.040099967271089554, 0.03680563345551491, 0.0372600257396698, 0.0374872200191021, 0.03544246405363083, 0.03476087749004364, 0.03623764589428902, 0.035101670771837234, 0.034420084208250046, 0.035669658333063126, 0.0370328314602375, 0.03896398842334747, 0.03919118642807007, 0.03612404689192772, 0.035101670771837234, 0.03578325733542442, 0.031352948397397995, 0.03794161230325699, 0.036351244896650314, 0.04225832223892212, 0.03555605933070183, 0.039986368268728256, 0.03248892351984978, 0.03794161230325699, 0.03589685261249542, 0.033965691924095154, 0.03578325733542442, 0.03896398842334747, 0.03419288992881775, 0.034420084208250046, 0.03862319514155388, 0.03464727848768234, 0.037600819021463394, 0.03771441429853439, 0.03782801330089569, 0.038168806582689285, 0.03771441429853439, 0.03555605933070183, 0.03896398842334747, 0.03862319514155388, 0.03850959986448288, 0.038168806582689285, 0.036919232457876205, 0.035669658333063126, 0.037600819021463394, 0.040781550109386444, 0.028399409726262093, 0.035101670771837234, 0.03419288992881775, 0.04055435582995415, 0.037600819021463394, 0.03555605933070183, 0.0372600257396698, 0.03487447276711464, 0.03919118642807007, 0.03589685261249542], 'loss': [4.141625439689183, 3.784812330507648, 3.697845288336578, 3.622617605369489, 3.5689137252687178, 3.521235304537251, 3.4950466363155135, 3.4622924270127595, 3.421846256209295, 3.3930049265385733, 3.360030979200782, 3.334394073130765, 3.3140639898050015, 3.30703076667494, 3.2761397397986802, 3.253075429405181, 3.22094008880832, 3.250515526398384, 3.21855804936962, 3.1852220610846738, 3.1811242122254977, 3.159644262793718, 3.17336438014872, 3.151880631817479, 3.1439168975196727, 3.154151284704236, 3.148801808944818, 3.1463561556282893, 3.107750340402365, 3.1222385228330007, 3.1210782537375263, 3.1468393873642926, 3.1383380122731377, 3.100268913376432, 3.099325779987724, 3.1038903765750643, 3.1021294534062926, 3.1121623506518565, 3.127143758223986, 3.0896002109630567, 3.0920681353811217, 3.0660715762629, 3.120292676862828, 3.1004224405500684, 3.0824407805772673, 3.0783458035710884, 3.0925817773962003, 3.0701247070052715, 3.0876062416502617, 3.0666133070223855, 3.0829214386100827, 3.164064507907736, 3.0538713384965224, 3.0856947068692357, 3.0824528852963526, 3.0944037445540404, 3.0833499994209994, 3.0596077921166733, 3.0379597933342235, 3.070573993444589, 3.0809206299392704, 3.12217864261555, 3.0442151831131308, 3.0641835735497835, 3.051773365821354, 3.082368927176584, 3.075416432977977, 3.0548953720045784, 3.1227273217874147, 3.089969602941572, 3.0601824907680655, 3.082830617233054, 3.0764154928121794, 3.0516837790472677, 3.087585404312027, 3.098948536061415, 3.0830156173131753, 3.1280291999538026, 3.141878639094989, 3.0529082877779192, 3.019672997603528, 3.0864700765617394, 3.1211558686673677, 3.0289357001191073, 3.0196055055334106, 3.033239464024171, 3.015415780253642, 3.054883321373846, 3.073001357955353, 3.0363273058680704, 3.0421526368576086, 3.0829445944787017, 3.057856135935673, 3.0623640254364237, 3.0728952337601942, 3.1144156231853857, 3.1079236352655797, 3.060597550205683, 3.0500911615609256, 3.06022012776643], 'accuracy': [0.043607533, 0.061446976, 0.076030016, 0.09377507, 0.101302564, 0.110033505, 0.113455094, 0.11432819, 0.122917555, 0.12636274, 0.13209684, 0.13516447, 0.13794893, 0.13875124, 0.14608996, 0.15083298, 0.15677947, 0.15408939, 0.1573222, 0.16119213, 0.16194724, 0.1663835, 0.16477889, 0.16791731, 0.1689084, 0.16824767, 0.16952193, 0.16897918, 0.17671905, 0.17457171, 0.17678985, 0.16954552, 0.17152768, 0.1780641, 0.17761575, 0.17686063, 0.17907877, 0.17435934, 0.1730379, 0.18158007, 0.18028222, 0.18450612, 0.17395818, 0.17917316, 0.18370381, 0.182052, 0.18309028, 0.18471849, 0.18181604, 0.18552078, 0.18150927, 0.17077257, 0.18651187, 0.18318467, 0.18349144, 0.18070698, 0.18443532, 0.18696022, 0.19215159, 0.19146727, 0.18639389, 0.18202841, 0.19160886, 0.19153807, 0.19477087, 0.1862759, 0.18823446, 0.19163245, 0.17914955, 0.18563877, 0.18873, 0.18655907, 0.19224598, 0.19604512, 0.18700741, 0.18667705, 0.18743216, 0.18422295, 0.18195762, 0.19406296, 0.1995139, 0.18226439, 0.18202841, 0.1987588, 0.20055218, 0.19852282, 0.20208599, 0.19387418, 0.19179763, 0.20175563, 0.19670583, 0.18755014, 0.19477087, 0.19578555, 0.19083014, 0.18455331, 0.18575676, 0.19762613, 0.19554958, 0.19498324]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot history: MAE\n",
    "plt.plot(history.history['loss'], label='MAE (testing data)')\n",
    "plt.plot(history.history['val_loss'], label='MAE (validation data)')\n",
    "plt.title('MAE')\n",
    "plt.ylabel('MAE value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = np.arange(0, len(history.history['loss']))\n",
    "\n",
    "# You can chose the style of your preference\n",
    "# print(plt.style.available) to see the available options\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "# Plot train loss, train acc, val loss and val acc against epochs passed\n",
    "plt.figure()\n",
    "plt.plot(N, history.history['loss'], label = \"train_loss\")\n",
    "plt.plot(N, history.history['accuracy'], label = \"train_acc\")\n",
    "plt.plot(N, history.history['val_loss'], label = \"val_loss\")\n",
    "plt.plot(N, history.history['val_accuracy'], label = \"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "# Make sure there exists a folder called output in the current directory\n",
    "# or replace 'output' with whatever direcory you want to put in the plots\n",
    "plt.show()\n",
    "plt.savefig('../Output/EpochNASNetMobile_OF.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
