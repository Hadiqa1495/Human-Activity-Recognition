{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\HH TRADERS\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.applications.resnet_v2 import ResNet101V2\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "import cv2\n",
    "import math\n",
    "import os\n",
    "from glob import glob\n",
    "from scipy import stats as s\n",
    "from sklearn.externals import joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_ResNet101V2 = ResNet101V2(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model architecture\n",
    "model_ResNet101V2 = Sequential()\n",
    "model_ResNet101V2.add(Dense(1024, activation='relu', input_shape=(100352,)))\n",
    "model_ResNet101V2.add(Dropout(0.5))\n",
    "model_ResNet101V2.add(Dense(512, activation='relu'))\n",
    "model_ResNet101V2.add(Dropout(0.5))\n",
    "model_ResNet101V2.add(Dense(256, activation='relu'))\n",
    "model_ResNet101V2.add(Dropout(0.5))\n",
    "model_ResNet101V2.add(Dense(128, activation='relu'))\n",
    "model_ResNet101V2.add(Dropout(0.5))\n",
    "model_ResNet101V2.add(Dense(51, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the trained weights\n",
    "model_ResNet101V2.load_weights(\"../Models/weightResNet101V2.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model_ResNet101V2.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_InceptionV3 = InceptionV3(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model architecture\n",
    "model_InceptionV3 = Sequential()\n",
    "model_InceptionV3.add(Dense(1024, activation='relu', input_shape=(51200,)))\n",
    "model_InceptionV3.add(Dropout(0.5))\n",
    "model_InceptionV3.add(Dense(512, activation='relu'))\n",
    "model_InceptionV3.add(Dropout(0.5))\n",
    "model_InceptionV3.add(Dense(256, activation='relu'))\n",
    "model_InceptionV3.add(Dropout(0.5))\n",
    "model_InceptionV3.add(Dense(128, activation='relu'))\n",
    "model_InceptionV3.add(Dropout(0.5))\n",
    "model_InceptionV3.add(Dense(51, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the trained weights\n",
    "model_InceptionV3.load_weights(\"../Models/weightInceptionV3.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model_InceptionV3.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brush_hair', 'cartwheel', 'catch', 'chew', 'clap', 'climb', 'climb_stairs', 'dive', 'draw_sword', 'dribble', 'drink', 'eat', 'fall_floor', 'fencing', 'flic_flac', 'golf', 'handstand', 'hit', 'hug', 'jump', 'kick', 'kick_ball', 'kiss', 'laugh', 'pick', 'pour', 'pullup', 'punch', 'push', 'pushup', 'ride_bike', 'ride_horse', 'run', 'shake_hands', 'shoot_ball', 'shoot_bow', 'shoot_gun', 'sit', 'situp', 'smile', 'smoke', 'somersault', 'stand', 'swing_baseball', 'sword', 'sword_exercise', 'talk', 'throw', 'turn', 'walk', 'wave']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brush_hair/April_09_brush_hair_u_nm_np1_ba_goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brush_hair/April_09_brush_hair_u_nm_np1_ba_goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brush_hair/April_09_brush_hair_u_nm_np1_ba_goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brush_hair/Blonde_being_brushed_brush_hair_f_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brush_hair/Blonde_being_brushed_brush_hair_u_c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_name\n",
       "0  brush_hair/April_09_brush_hair_u_nm_np1_ba_goo...\n",
       "1  brush_hair/April_09_brush_hair_u_nm_np1_ba_goo...\n",
       "2  brush_hair/April_09_brush_hair_u_nm_np1_ba_goo...\n",
       "3  brush_hair/Blonde_being_brushed_brush_hair_f_n...\n",
       "4  brush_hair/Blonde_being_brushed_brush_hair_u_c..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location='../data/test'\n",
    "classes=[]\n",
    "videos=[]\n",
    "test=pd.DataFrame()\n",
    "for foldername in sorted(os.listdir(location)):\n",
    "    classes.append(foldername)\n",
    "    for files in sorted(os.listdir(location+'/'+foldername)):\n",
    "        videos.append(foldername+'/'+files)\n",
    "test['video_name']=videos\n",
    "print(classes)\n",
    "test_videos=test['video_name']\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the tags\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "y = train['class']\n",
    "y = pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1530/1530 [44:17<00:00,  1.32s/it] \n"
     ]
    }
   ],
   "source": [
    "# creating two lists to store predicted and actual tags\n",
    "predict = []\n",
    "actual = []\n",
    "\n",
    "# for loop to extract frames from each test video\n",
    "for i in tqdm(range(test_videos.shape[0])):\n",
    "    count = 0\n",
    "    videoFile = test_videos[i]\n",
    "    # print('../data/test/'+videoFile)\n",
    "    cap = cv2.VideoCapture('../data/test/'+videoFile.split(' ')[0].split('/')[0]+'/'+videoFile.split(' ')[0].split('/')[1])   # capturing the video from the given path\n",
    "    frameRate = cap.get(5) #frame rate\n",
    "    x=1\n",
    "    # removing all other files from the temp folder\n",
    "    files = glob('../data/temp/*')\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    while(cap.isOpened()):\n",
    "        frameId = cap.get(1) #current frame number\n",
    "        ret, frame = cap.read()\n",
    "        if (ret != True):\n",
    "            break\n",
    "        if (frameId % math.floor(frameRate) == 0):\n",
    "            # storing the frames of this particular video in temp folder\n",
    "#            print('../data/temp/' + \"_frame%d.jpg\" % count)\n",
    "            filename ='../data/temp/' + \"_frame%d.jpg\" % count;count+=1\n",
    "            cv2.imwrite(filename, frame)\n",
    "    cap.release()\n",
    "    \n",
    "    # reading all the frames from temp folder\n",
    "    images = glob(\"../data/temp/*.jpg\")\n",
    "    \n",
    "    test_images = []\n",
    "    for i in range(len(images)):\n",
    "        img = image.load_img(images[i], target_size=(224,224,3))\n",
    "        img = image.img_to_array(img)\n",
    "        img = img/255\n",
    "        test_images.append(img)\n",
    "        \n",
    "    # converting all the frames for a test video into numpy array\n",
    "    test_images = np.array(test_images)\n",
    "    # print(test_images.shape)\n",
    "    prediction=[]\n",
    "     \n",
    "    # extracting features using pre-trained model\n",
    "    prediction_images = base_model_ResNet101V2.predict(test_images)\n",
    "    # converting features in one dimensional array\n",
    "    prediction_images = prediction_images.reshape(prediction_images.shape[0], 7*7*2048)\n",
    "    # predicting tags for each array\n",
    "    prediction_ResNet101V2 = model_ResNet101V2.predict_classes(prediction_images)\n",
    "    # appending the mode of predictions in predict list to assign the tag to the video\n",
    "    # print('prediction')\n",
    "    # print(prediction_VGG16)\n",
    "        \n",
    "    # extracting features using pre-trained model\n",
    "    prediction_images = base_model_InceptionV3.predict(test_images)\n",
    "    # converting features in one dimensional array\n",
    "    prediction_images = prediction_images.reshape(prediction_images.shape[0], 5*5*2048)\n",
    "    # predicting tags for each array\n",
    "    prediction_InceptionV3 = model_InceptionV3.predict_classes(prediction_images)\n",
    "    # print('prediction')\n",
    "    # print(prediction_InceptionV3)\n",
    "    # appending the mode of predictions in predict list to assign the tag to the video\n",
    "    prediction=np.concatenate((prediction_ResNet101V2,prediction_InceptionV3))\n",
    "    \n",
    "    predict.append(y.columns.values[s.mode(prediction)[0][0]])\n",
    "    # print(predict)\n",
    "    # appending the actual tag of the video\n",
    "    # print(videoFile.split('/')[0])\n",
    "    actual.append(videoFile.split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.84313725490196"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the accuracy of the predicted tags\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(predict, actual)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HH TRADERS\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "    brush_hair       0.28      0.50      0.36        30\n",
      "     cartwheel       0.00      0.00      0.00        30\n",
      "         catch       0.44      0.47      0.45        30\n",
      "          chew       0.22      0.60      0.32        30\n",
      "          clap       0.12      0.10      0.11        30\n",
      "         climb       0.37      0.77      0.49        30\n",
      "  climb_stairs       0.48      0.70      0.57        30\n",
      "          dive       0.29      0.33      0.31        30\n",
      "    draw_sword       0.43      0.67      0.52        30\n",
      "       dribble       0.56      0.93      0.70        30\n",
      "         drink       0.08      0.30      0.13        30\n",
      "           eat       0.24      0.50      0.32        30\n",
      "    fall_floor       0.00      0.00      0.00        30\n",
      "       fencing       0.50      0.27      0.35        30\n",
      "     flic_flac       0.44      0.13      0.21        30\n",
      "          golf       0.33      0.90      0.48        30\n",
      "     handstand       0.23      0.43      0.30        30\n",
      "           hit       0.00      0.00      0.00        30\n",
      "           hug       0.48      0.77      0.59        30\n",
      "          jump       0.20      0.03      0.06        30\n",
      "          kick       0.00      0.00      0.00        30\n",
      "     kick_ball       0.20      0.03      0.06        30\n",
      "          kiss       0.39      0.73      0.51        30\n",
      "         laugh       0.19      0.17      0.18        30\n",
      "          pick       0.00      0.00      0.00        30\n",
      "          pour       0.65      0.93      0.77        30\n",
      "        pullup       0.88      0.77      0.82        30\n",
      "         punch       0.63      0.40      0.49        30\n",
      "          push       0.71      0.57      0.63        30\n",
      "        pushup       0.74      0.47      0.57        30\n",
      "     ride_bike       0.53      0.83      0.65        30\n",
      "    ride_horse       0.48      0.97      0.64        30\n",
      "           run       0.33      0.03      0.06        30\n",
      "   shake_hands       0.34      0.50      0.41        30\n",
      "    shoot_ball       0.71      0.40      0.51        30\n",
      "     shoot_bow       0.82      0.77      0.79        30\n",
      "     shoot_gun       0.78      0.47      0.58        30\n",
      "           sit       0.00      0.00      0.00        30\n",
      "         situp       0.81      0.73      0.77        30\n",
      "         smile       0.17      0.03      0.06        30\n",
      "         smoke       0.40      0.33      0.36        30\n",
      "    somersault       0.32      0.63      0.43        30\n",
      "         stand       0.00      0.00      0.00        30\n",
      "swing_baseball       0.45      0.17      0.24        30\n",
      "         sword       0.24      0.13      0.17        30\n",
      "sword_exercise       0.29      0.07      0.11        30\n",
      "          talk       0.47      0.47      0.47        30\n",
      "         throw       0.00      0.00      0.00        30\n",
      "          turn       0.00      0.00      0.00        30\n",
      "          walk       0.30      0.27      0.28        30\n",
      "          wave       0.07      0.03      0.05        30\n",
      "\n",
      "      accuracy                           0.38      1530\n",
      "     macro avg       0.34      0.38      0.33      1530\n",
      "  weighted avg       0.34      0.38      0.33      1530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(actual,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0 14 ...  0  0  0]\n",
      " ...\n",
      " [ 3  0  0 ...  0  3  2]\n",
      " [ 2  0  0 ...  0  8  1]\n",
      " [ 1  0  0 ...  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "cm=metrics.confusion_matrix(actual,predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1, 'Accuracy score: 37.84313725490196')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square=True, cmap=\"Blues_r\");\n",
    "plt.ylabel(\"Actual label\");\n",
    "plt.xlabel(\"Predicted label\")\n",
    "all_sample_title=\"Accuracy score: {0}\".format(metrics.accuracy_score(predict, actual)*100)\n",
    "plt.title(all_sample_title,size=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
