{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib \n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42373</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_flow6.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42374</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_flow7.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42375</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_flow7.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42376</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_flow8.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42377</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_flow8.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         image class\n",
       "42373  winKen_wave_u_cm_np1_ri_bad_1_flow6.jpg  wave\n",
       "42374  winKen_wave_u_cm_np1_ri_bad_1_flow7.jpg  wave\n",
       "42375  winKen_wave_u_cm_np1_ri_bad_1_flow7.jpg  wave\n",
       "42376  winKen_wave_u_cm_np1_ri_bad_1_flow8.jpg  wave\n",
       "42377  winKen_wave_u_cm_np1_ri_bad_1_flow8.jpg  wave"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train_OF.csv')\n",
    "train.sort_values(by=['class', 'image'])\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42378/42378 [05:01<00:00, 140.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating an empty list\n",
    "train_image = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    # loading the image and keeping the target size as (224,224,3)\n",
    "    img = image.load_img('../data/train_frame_OF/'+train['image'][i], target_size=(224,224,3))\n",
    "    # converting it to array\n",
    "    img = image.img_to_array(img)\n",
    "    # normalizing the pixel value\n",
    "    img = img/255\n",
    "    # appending the image to the train_image list\n",
    "    train_image.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42378, 224, 224, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the list to numpy array\n",
    "X_train = np.array(train_image,np.float16)\n",
    "\n",
    "# shape of the array\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8798</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8799</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8800</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8801</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8802</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  image class\n",
       "8798  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "8799  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "8800  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "8801  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "8802  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = pd.read_csv('../data/val_OF.csv')\n",
    "val.sort_values(by=['class', 'image'])\n",
    "val.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8803/8803 [01:26<00:00, 101.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating an empty list\n",
    "val_image = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(val.shape[0])):\n",
    "    # loading the image and keeping the target size as (224,224,3)\n",
    "    img = image.load_img('../data/val_frame_OF/'+val['image'][i], target_size=(224,224,3))\n",
    "    # converting it to array\n",
    "    img = image.img_to_array(img)\n",
    "    # normalizing the pixel value\n",
    "    img = img/255\n",
    "    # appending the image to the train_image list\n",
    "    val_image.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8803, 224, 224, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the list to numpy array\n",
    "X_test = np.array(val_image,np.float16)\n",
    "\n",
    "# shape of the array\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42378,)\n",
      "(8803,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "image    8803\n",
       "class      51\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separating the target\n",
    "y_train = train['class']\n",
    "y_test = val['class']\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "train.nunique()\n",
    "val.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42378, 51)\n",
      "(8803, 51)\n"
     ]
    }
   ],
   "source": [
    "# creating dummies of target variable for train and validation set\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the base model of pre-trained VGG16 model\n",
    "base_model = InceptionResNetV2(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, None, 1536)\n"
     ]
    }
   ],
   "source": [
    "print(base_model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inception_resnet_v2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 3 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 3 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None, 3 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 3 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 3 96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, None, 3 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 6 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 6 192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, None, 6 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 6 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 8 5120        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 8 240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, None, 8 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 1 138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 1 576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, None, 1 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 1 0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 6 192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, None, 6 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 4 9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 9 55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 4 144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 9 288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, None, 4 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, None, 9 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, None, None, 1 0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 9 18432       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 6 76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 9 82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 6 12288       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 9 288         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 6 192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 9 288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, 6 192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, None, 9 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, None, 6 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, None, 9 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, None, 6 0           batch_normalization_12[0][0]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "mixed_5b (Concatenate)          (None, None, None, 3 0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 3 10240       mixed_5b[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, None, 3 96          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, None, 3 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 3 10240       mixed_5b[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 4 13824       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, None, 3 96          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, None, 4 144         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, None, 3 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, None, 4 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 3 10240       mixed_5b[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 3 9216        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 6 27648       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, None, 3 96          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, None, 3 96          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, None, 6 192         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, None, 3 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, None, 3 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, None, None, 6 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_mixed (Concatenate)   (None, None, None, 1 0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_conv (Conv2D)         (None, None, None, 3 41280       block35_1_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_1 (Lambda)              (None, None, None, 3 0           mixed_5b[0][0]                   \n",
      "                                                                 block35_1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_ac (Activation)       (None, None, None, 3 0           block35_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, None, None, 3 10240       block35_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, None, 3 96          conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, None, 3 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, None, None, 3 10240       block35_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, None, None, 4 13824       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, None, 3 96          conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, None, 4 144         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, None, None, 3 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, None, 4 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 3 10240       block35_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, None, None, 3 9216        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, None, None, 6 27648       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, None, 3 96          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, None, 3 96          conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, None, 6 192         conv2d_24[0][0]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, None, None, 3 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, None, 3 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, None, None, 6 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_mixed (Concatenate)   (None, None, None, 1 0           activation_19[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_conv (Conv2D)         (None, None, None, 3 41280       block35_2_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_2 (Lambda)              (None, None, None, 3 0           block35_1_ac[0][0]               \n",
      "                                                                 block35_2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_ac (Activation)       (None, None, None, 3 0           block35_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, None, None, 3 10240       block35_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, None, None, 3 96          conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, None, None, 3 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, None, None, 3 10240       block35_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, None, None, 4 13824       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, None, 3 96          conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, None, None, 4 144         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, None, None, 3 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, None, None, 4 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, None, None, 3 10240       block35_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, None, None, 3 9216        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, None, None, 6 27648       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, None, 3 96          conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, None, None, 3 96          conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, None, None, 6 192         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, None, None, 3 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, None, None, 3 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, None, None, 6 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_mixed (Concatenate)   (None, None, None, 1 0           activation_25[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_conv (Conv2D)         (None, None, None, 3 41280       block35_3_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_3 (Lambda)              (None, None, None, 3 0           block35_2_ac[0][0]               \n",
      "                                                                 block35_3_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_ac (Activation)       (None, None, None, 3 0           block35_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, None, None, 3 10240       block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, None, 3 96          conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, None, None, 3 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, None, None, 3 10240       block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, None, None, 4 13824       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, None, 3 96          conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, None, 4 144         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, None, None, 3 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, None, None, 4 0           batch_normalization_35[0][0]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, None, None, 3 10240       block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, None, None, 3 9216        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, None, None, 6 27648       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, None, 3 96          conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, None, 3 96          conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, None, 6 192         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, None, None, 3 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, None, None, 3 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, None, None, 6 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_mixed (Concatenate)   (None, None, None, 1 0           activation_31[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_conv (Conv2D)         (None, None, None, 3 41280       block35_4_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_4 (Lambda)              (None, None, None, 3 0           block35_3_ac[0][0]               \n",
      "                                                                 block35_4_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_ac (Activation)       (None, None, None, 3 0           block35_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, None, None, 3 10240       block35_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, None, None, 3 96          conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, None, None, 3 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, None, None, 3 10240       block35_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, None, None, 4 13824       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, None, None, 3 96          conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, None, None, 4 144         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, None, None, 3 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, None, None, 4 0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, None, None, 3 10240       block35_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, None, None, 3 9216        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, None, None, 6 27648       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, None, None, 3 96          conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, None, None, 3 96          conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, None, None, 6 192         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, None, None, 3 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, None, None, 3 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, None, None, 6 0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_mixed (Concatenate)   (None, None, None, 1 0           activation_37[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_conv (Conv2D)         (None, None, None, 3 41280       block35_5_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_5 (Lambda)              (None, None, None, 3 0           block35_4_ac[0][0]               \n",
      "                                                                 block35_5_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_ac (Activation)       (None, None, None, 3 0           block35_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, None, None, 3 10240       block35_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, None, None, 3 96          conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, None, None, 3 0           batch_normalization_46[0][0]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, None, None, 3 10240       block35_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, None, None, 4 13824       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, None, None, 3 96          conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, None, None, 4 144         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, None, None, 3 0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, None, None, 4 0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, None, None, 3 10240       block35_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, None, None, 3 9216        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, None, None, 6 27648       activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, None, None, 3 96          conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, None, None, 3 96          conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, None, None, 6 192         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, None, None, 3 0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, None, None, 3 0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, None, None, 6 0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_6_mixed (Concatenate)   (None, None, None, 1 0           activation_43[0][0]              \n",
      "                                                                 activation_45[0][0]              \n",
      "                                                                 activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_6_conv (Conv2D)         (None, None, None, 3 41280       block35_6_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_6 (Lambda)              (None, None, None, 3 0           block35_5_ac[0][0]               \n",
      "                                                                 block35_6_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_6_ac (Activation)       (None, None, None, 3 0           block35_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, None, None, 3 10240       block35_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, None, None, 3 96          conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, None, None, 3 0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, None, None, 3 10240       block35_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, None, None, 4 13824       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, None, None, 3 96          conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, None, None, 4 144         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, None, None, 3 0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, None, None, 4 0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, None, None, 3 10240       block35_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, None, None, 3 9216        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, None, None, 6 27648       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, None, None, 3 96          conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, None, None, 3 96          conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, None, None, 6 192         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, None, None, 3 0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, None, None, 3 0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, None, None, 6 0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_7_mixed (Concatenate)   (None, None, None, 1 0           activation_49[0][0]              \n",
      "                                                                 activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block35_7_conv (Conv2D)         (None, None, None, 3 41280       block35_7_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_7 (Lambda)              (None, None, None, 3 0           block35_6_ac[0][0]               \n",
      "                                                                 block35_7_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_7_ac (Activation)       (None, None, None, 3 0           block35_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, None, None, 3 10240       block35_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, None, None, 3 96          conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, None, None, 3 0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, None, None, 3 10240       block35_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, None, None, 4 13824       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, None, None, 3 96          conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, None, None, 4 144         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, None, None, 3 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, None, None, 4 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, None, None, 3 10240       block35_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, None, None, 3 9216        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, None, None, 6 27648       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, None, None, 3 96          conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, None, None, 3 96          conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, None, None, 6 192         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, None, None, 3 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, None, None, 3 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, None, None, 6 0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_8_mixed (Concatenate)   (None, None, None, 1 0           activation_55[0][0]              \n",
      "                                                                 activation_57[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_8_conv (Conv2D)         (None, None, None, 3 41280       block35_8_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_8 (Lambda)              (None, None, None, 3 0           block35_7_ac[0][0]               \n",
      "                                                                 block35_8_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_8_ac (Activation)       (None, None, None, 3 0           block35_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, None, None, 3 10240       block35_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, None, None, 3 96          conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, None, None, 3 0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, None, None, 3 10240       block35_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, None, None, 4 13824       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, None, None, 3 96          conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, None, None, 4 144         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, None, None, 3 0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, None, None, 4 0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, None, None, 3 10240       block35_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, None, None, 3 9216        activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, None, None, 6 27648       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, None, None, 3 96          conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, None, None, 3 96          conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_66 (BatchNo (None, None, None, 6 192         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, None, None, 3 0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, None, None, 3 0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, None, None, 6 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_9_mixed (Concatenate)   (None, None, None, 1 0           activation_61[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "                                                                 activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_9_conv (Conv2D)         (None, None, None, 3 41280       block35_9_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_9 (Lambda)              (None, None, None, 3 0           block35_8_ac[0][0]               \n",
      "                                                                 block35_9_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_9_ac (Activation)       (None, None, None, 3 0           block35_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, None, None, 3 10240       block35_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, None, None, 3 96          conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, None, None, 3 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, None, None, 3 10240       block35_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, None, None, 4 13824       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, None, None, 3 96          conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, None, None, 4 144         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, None, None, 3 0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, None, None, 4 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, None, None, 3 10240       block35_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, None, None, 3 9216        activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, None, None, 6 27648       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, None, None, 3 96          conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, None, None, 3 96          conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, None, None, 6 192         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, None, None, 3 0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, None, None, 3 0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, None, None, 6 0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_10_mixed (Concatenate)  (None, None, None, 1 0           activation_67[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_10_conv (Conv2D)        (None, None, None, 3 41280       block35_10_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block35_10 (Lambda)             (None, None, None, 3 0           block35_9_ac[0][0]               \n",
      "                                                                 block35_10_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_10_ac (Activation)      (None, None, None, 3 0           block35_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, None, None, 2 81920       block35_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, None, None, 2 768         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, None, None, 2 0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, None, None, 2 589824      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, None, None, 2 768         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, None, None, 2 0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, None, None, 3 1105920     block35_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, None, None, 3 884736      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_73 (BatchNo (None, None, None, 3 1152        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, None, None, 3 1152        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, None, None, 3 0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, None, None, 3 0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, None, None, 3 0           block35_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mixed_6a (Concatenate)          (None, None, None, 1 0           activation_73[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, None, None, 1 139264      mixed_6a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, None, None, 1 384         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, None, None, 1 0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, None, None, 1 143360      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, None, None, 1 480         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, None, None, 1 0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, None, None, 1 208896      mixed_6a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, None, None, 1 215040      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, None, None, 1 576         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, None, None, 1 576         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, None, None, 1 0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, None, None, 1 0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_1_mixed (Concatenate)   (None, None, None, 3 0           activation_77[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_1_conv (Conv2D)         (None, None, None, 1 418880      block17_1_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_1 (Lambda)              (None, None, None, 1 0           mixed_6a[0][0]                   \n",
      "                                                                 block17_1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_1_ac (Activation)       (None, None, None, 1 0           block17_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, None, None, 1 139264      block17_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, None, None, 1 384         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, None, None, 1 0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, None, None, 1 143360      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, None, None, 1 480         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, None, None, 1 0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, None, None, 1 208896      block17_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, None, None, 1 215040      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, None, None, 1 576         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, None, None, 1 576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, None, None, 1 0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, None, None, 1 0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_2_mixed (Concatenate)   (None, None, None, 3 0           activation_81[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_2_conv (Conv2D)         (None, None, None, 1 418880      block17_2_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_2 (Lambda)              (None, None, None, 1 0           block17_1_ac[0][0]               \n",
      "                                                                 block17_2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_2_ac (Activation)       (None, None, None, 1 0           block17_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_86 (Conv2D)              (None, None, None, 1 139264      block17_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, None, None, 1 384         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, None, None, 1 0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, None, None, 1 143360      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, None, None, 1 480         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, None, None, 1 0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, None, None, 1 208896      block17_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, None, None, 1 215040      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, None, None, 1 576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, None, None, 1 576         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, None, None, 1 0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, None, None, 1 0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_3_mixed (Concatenate)   (None, None, None, 3 0           activation_85[0][0]              \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_3_conv (Conv2D)         (None, None, None, 1 418880      block17_3_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_3 (Lambda)              (None, None, None, 1 0           block17_2_ac[0][0]               \n",
      "                                                                 block17_3_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_3_ac (Activation)       (None, None, None, 1 0           block17_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, None, None, 1 139264      block17_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, None, None, 1 384         conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, None, None, 1 0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, None, None, 1 143360      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, None, None, 1 480         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, None, None, 1 0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, None, None, 1 208896      block17_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, None, None, 1 215040      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, None, None, 1 576         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, None, None, 1 576         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, None, None, 1 0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, None, None, 1 0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_4_mixed (Concatenate)   (None, None, None, 3 0           activation_89[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_4_conv (Conv2D)         (None, None, None, 1 418880      block17_4_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_4 (Lambda)              (None, None, None, 1 0           block17_3_ac[0][0]               \n",
      "                                                                 block17_4_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_4_ac (Activation)       (None, None, None, 1 0           block17_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, None, None, 1 139264      block17_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, None, None, 1 384         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, None, None, 1 0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, None, None, 1 143360      activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, None, None, 1 480         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, None, None, 1 0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, None, None, 1 208896      block17_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_96 (Conv2D)              (None, None, None, 1 215040      activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, None, None, 1 576         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, None, None, 1 576         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, None, None, 1 0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, None, None, 1 0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_5_mixed (Concatenate)   (None, None, None, 3 0           activation_93[0][0]              \n",
      "                                                                 activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_5_conv (Conv2D)         (None, None, None, 1 418880      block17_5_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_5 (Lambda)              (None, None, None, 1 0           block17_4_ac[0][0]               \n",
      "                                                                 block17_5_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_5_ac (Activation)       (None, None, None, 1 0           block17_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, None, None, 1 139264      block17_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, None, None, 1 384         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, None, None, 1 0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, None, None, 1 143360      activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, None, None, 1 480         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, None, None, 1 0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, None, None, 1 208896      block17_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, None, None, 1 215040      activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, None, None, 1 576         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, None, None, 1 576         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, None, None, 1 0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, None, None, 1 0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_6_mixed (Concatenate)   (None, None, None, 3 0           activation_97[0][0]              \n",
      "                                                                 activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_6_conv (Conv2D)         (None, None, None, 1 418880      block17_6_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_6 (Lambda)              (None, None, None, 1 0           block17_5_ac[0][0]               \n",
      "                                                                 block17_6_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_6_ac (Activation)       (None, None, None, 1 0           block17_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, None, None, 1 139264      block17_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, None, None, 1 384         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, None, None, 1 0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, None, None, 1 143360      activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, None, None, 1 480         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, None, None, 1 0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, None, None, 1 208896      block17_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, None, None, 1 215040      activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, None, None, 1 576         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, None, None, 1 576         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, None, None, 1 0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, None, None, 1 0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_7_mixed (Concatenate)   (None, None, None, 3 0           activation_101[0][0]             \n",
      "                                                                 activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_7_conv (Conv2D)         (None, None, None, 1 418880      block17_7_mixed[0][0]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "block17_7 (Lambda)              (None, None, None, 1 0           block17_6_ac[0][0]               \n",
      "                                                                 block17_7_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_7_ac (Activation)       (None, None, None, 1 0           block17_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, None, None, 1 139264      block17_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, None, None, 1 384         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, None, None, 1 0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, None, None, 1 143360      activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, None, None, 1 480         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, None, None, 1 0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, None, None, 1 208896      block17_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, None, None, 1 215040      activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, None, None, 1 576         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, None, None, 1 576         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, None, None, 1 0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, None, None, 1 0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_8_mixed (Concatenate)   (None, None, None, 3 0           activation_105[0][0]             \n",
      "                                                                 activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_8_conv (Conv2D)         (None, None, None, 1 418880      block17_8_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_8 (Lambda)              (None, None, None, 1 0           block17_7_ac[0][0]               \n",
      "                                                                 block17_8_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_8_ac (Activation)       (None, None, None, 1 0           block17_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, None, None, 1 139264      block17_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, None, None, 1 384         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, None, None, 1 0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, None, None, 1 143360      activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, None, None, 1 480         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, None, None, 1 0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, None, None, 1 208896      block17_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, None, None, 1 215040      activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, None, None, 1 576         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, None, None, 1 576         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, None, None, 1 0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, None, None, 1 0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_9_mixed (Concatenate)   (None, None, None, 3 0           activation_109[0][0]             \n",
      "                                                                 activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_9_conv (Conv2D)         (None, None, None, 1 418880      block17_9_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_9 (Lambda)              (None, None, None, 1 0           block17_8_ac[0][0]               \n",
      "                                                                 block17_9_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_9_ac (Activation)       (None, None, None, 1 0           block17_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, None, None, 1 139264      block17_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, None, None, 1 384         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, None, None, 1 0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, None, None, 1 143360      activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_115 (BatchN (None, None, None, 1 480         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, None, None, 1 0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, None, None, 1 208896      block17_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, None, None, 1 215040      activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, None, None, 1 576         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, None, None, 1 576         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, None, None, 1 0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, None, None, 1 0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_10_mixed (Concatenate)  (None, None, None, 3 0           activation_113[0][0]             \n",
      "                                                                 activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_10_conv (Conv2D)        (None, None, None, 1 418880      block17_10_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_10 (Lambda)             (None, None, None, 1 0           block17_9_ac[0][0]               \n",
      "                                                                 block17_10_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_10_ac (Activation)      (None, None, None, 1 0           block17_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, None, None, 1 139264      block17_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, None, None, 1 384         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, None, None, 1 0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, None, None, 1 143360      activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, None, None, 1 480         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, None, None, 1 0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, None, None, 1 208896      block17_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, None, None, 1 215040      activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, None, None, 1 576         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, None, None, 1 576         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, None, None, 1 0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, None, None, 1 0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_11_mixed (Concatenate)  (None, None, None, 3 0           activation_117[0][0]             \n",
      "                                                                 activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_11_conv (Conv2D)        (None, None, None, 1 418880      block17_11_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_11 (Lambda)             (None, None, None, 1 0           block17_10_ac[0][0]              \n",
      "                                                                 block17_11_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_11_ac (Activation)      (None, None, None, 1 0           block17_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, None, None, 1 139264      block17_11_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, None, None, 1 384         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, None, None, 1 0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, None, None, 1 143360      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, None, None, 1 480         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, None, None, 1 0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, None, None, 1 208896      block17_11_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, None, None, 1 215040      activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, None, None, 1 576         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, None, None, 1 576         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, None, None, 1 0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_124 (Activation)     (None, None, None, 1 0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_12_mixed (Concatenate)  (None, None, None, 3 0           activation_121[0][0]             \n",
      "                                                                 activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_12_conv (Conv2D)        (None, None, None, 1 418880      block17_12_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_12 (Lambda)             (None, None, None, 1 0           block17_11_ac[0][0]              \n",
      "                                                                 block17_12_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_12_ac (Activation)      (None, None, None, 1 0           block17_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, None, None, 1 139264      block17_12_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, None, None, 1 384         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, None, None, 1 0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, None, None, 1 143360      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, None, None, 1 480         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, None, None, 1 0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, None, None, 1 208896      block17_12_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, None, None, 1 215040      activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, None, None, 1 576         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, None, None, 1 576         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, None, None, 1 0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, None, None, 1 0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_13_mixed (Concatenate)  (None, None, None, 3 0           activation_125[0][0]             \n",
      "                                                                 activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_13_conv (Conv2D)        (None, None, None, 1 418880      block17_13_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_13 (Lambda)             (None, None, None, 1 0           block17_12_ac[0][0]              \n",
      "                                                                 block17_13_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_13_ac (Activation)      (None, None, None, 1 0           block17_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, None, None, 1 139264      block17_13_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, None, None, 1 384         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, None, None, 1 0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, None, None, 1 143360      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, None, None, 1 480         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, None, None, 1 0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, None, None, 1 208896      block17_13_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, None, None, 1 215040      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, None, None, 1 576         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, None, None, 1 576         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, None, None, 1 0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, None, None, 1 0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_14_mixed (Concatenate)  (None, None, None, 3 0           activation_129[0][0]             \n",
      "                                                                 activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_14_conv (Conv2D)        (None, None, None, 1 418880      block17_14_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_14 (Lambda)             (None, None, None, 1 0           block17_13_ac[0][0]              \n",
      "                                                                 block17_14_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_14_ac (Activation)      (None, None, None, 1 0           block17_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, None, None, 1 139264      block17_14_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_134 (BatchN (None, None, None, 1 384         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, None, None, 1 0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, None, None, 1 143360      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, None, None, 1 480         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, None, None, 1 0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, None, None, 1 208896      block17_14_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, None, None, 1 215040      activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, None, None, 1 576         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, None, None, 1 576         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, None, None, 1 0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, None, None, 1 0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_15_mixed (Concatenate)  (None, None, None, 3 0           activation_133[0][0]             \n",
      "                                                                 activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_15_conv (Conv2D)        (None, None, None, 1 418880      block17_15_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_15 (Lambda)             (None, None, None, 1 0           block17_14_ac[0][0]              \n",
      "                                                                 block17_15_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_15_ac (Activation)      (None, None, None, 1 0           block17_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, None, None, 1 139264      block17_15_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, None, None, 1 384         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, None, None, 1 0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, None, None, 1 143360      activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, None, None, 1 480         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, None, None, 1 0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, None, None, 1 208896      block17_15_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, None, None, 1 215040      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, None, None, 1 576         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, None, None, 1 576         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, None, None, 1 0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, None, None, 1 0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_16_mixed (Concatenate)  (None, None, None, 3 0           activation_137[0][0]             \n",
      "                                                                 activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_16_conv (Conv2D)        (None, None, None, 1 418880      block17_16_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_16 (Lambda)             (None, None, None, 1 0           block17_15_ac[0][0]              \n",
      "                                                                 block17_16_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_16_ac (Activation)      (None, None, None, 1 0           block17_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, None, None, 1 139264      block17_16_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, None, None, 1 384         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, None, None, 1 0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, None, None, 1 143360      activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, None, None, 1 480         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, None, None, 1 0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, None, None, 1 208896      block17_16_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, None, None, 1 215040      activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_141 (BatchN (None, None, None, 1 576         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, None, None, 1 576         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, None, None, 1 0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, None, None, 1 0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_17_mixed (Concatenate)  (None, None, None, 3 0           activation_141[0][0]             \n",
      "                                                                 activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_17_conv (Conv2D)        (None, None, None, 1 418880      block17_17_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_17 (Lambda)             (None, None, None, 1 0           block17_16_ac[0][0]              \n",
      "                                                                 block17_17_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_17_ac (Activation)      (None, None, None, 1 0           block17_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, None, None, 1 139264      block17_17_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, None, None, 1 384         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, None, None, 1 0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, None, None, 1 143360      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, None, None, 1 480         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, None, None, 1 0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, None, None, 1 208896      block17_17_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, None, None, 1 215040      activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, None, None, 1 576         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, None, None, 1 576         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, None, None, 1 0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, None, None, 1 0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_18_mixed (Concatenate)  (None, None, None, 3 0           activation_145[0][0]             \n",
      "                                                                 activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_18_conv (Conv2D)        (None, None, None, 1 418880      block17_18_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_18 (Lambda)             (None, None, None, 1 0           block17_17_ac[0][0]              \n",
      "                                                                 block17_18_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_18_ac (Activation)      (None, None, None, 1 0           block17_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, None, None, 1 139264      block17_18_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, None, None, 1 384         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, None, None, 1 0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, None, None, 1 143360      activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, None, None, 1 480         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, None, None, 1 0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, None, None, 1 208896      block17_18_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, None, None, 1 215040      activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, None, None, 1 576         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, None, None, 1 576         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, None, None, 1 0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, None, None, 1 0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_19_mixed (Concatenate)  (None, None, None, 3 0           activation_149[0][0]             \n",
      "                                                                 activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_19_conv (Conv2D)        (None, None, None, 1 418880      block17_19_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_19 (Lambda)             (None, None, None, 1 0           block17_18_ac[0][0]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 block17_19_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_19_ac (Activation)      (None, None, None, 1 0           block17_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, None, None, 1 139264      block17_19_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, None, None, 1 384         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, None, None, 1 0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, None, None, 1 143360      activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, None, None, 1 480         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, None, None, 1 0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, None, None, 1 208896      block17_19_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, None, None, 1 215040      activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, None, None, 1 576         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, None, None, 1 576         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, None, None, 1 0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, None, None, 1 0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_20_mixed (Concatenate)  (None, None, None, 3 0           activation_153[0][0]             \n",
      "                                                                 activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_20_conv (Conv2D)        (None, None, None, 1 418880      block17_20_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_20 (Lambda)             (None, None, None, 1 0           block17_19_ac[0][0]              \n",
      "                                                                 block17_20_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_20_ac (Activation)      (None, None, None, 1 0           block17_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, None, None, 2 278528      block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, None, None, 2 768         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, None, None, 2 0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, None, None, 2 278528      block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, None, None, 2 278528      block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, None, None, 2 663552      activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, None, None, 2 768         conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, None, None, 2 768         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, None, None, 2 864         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, None, None, 2 0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, None, None, 2 0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, None, None, 2 0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, None, None, 3 884736      activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, None, None, 2 663552      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, None, None, 3 829440      activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, None, None, 3 1152        conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, None, None, 2 864         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, None, None, 3 960         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, None, None, 3 0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, None, None, 2 0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, None, None, 3 0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, None, None, 1 0           block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed_7a (Concatenate)          (None, None, None, 2 0           activation_158[0][0]             \n",
      "                                                                 activation_160[0][0]             \n",
      "                                                                 activation_163[0][0]             \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, None, None, 1 399360      mixed_7a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, None, None, 1 576         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, None, None, 1 0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, None, None, 2 129024      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, None, None, 2 672         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, None, None, 2 0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, None, None, 1 399360      mixed_7a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, None, None, 2 172032      activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, None, None, 1 576         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, None, None, 2 768         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, None, None, 1 0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, None, None, 2 0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_1_mixed (Concatenate)    (None, None, None, 4 0           activation_164[0][0]             \n",
      "                                                                 activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_1_conv (Conv2D)          (None, None, None, 2 933920      block8_1_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_1 (Lambda)               (None, None, None, 2 0           mixed_7a[0][0]                   \n",
      "                                                                 block8_1_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_1_ac (Activation)        (None, None, None, 2 0           block8_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, None, None, 1 399360      block8_1_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, None, None, 1 576         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, None, None, 1 0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, None, None, 2 129024      activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, None, None, 2 672         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, None, None, 2 0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, None, None, 1 399360      block8_1_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, None, None, 2 172032      activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, None, None, 1 576         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, None, None, 2 768         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, None, None, 1 0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, None, None, 2 0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_2_mixed (Concatenate)    (None, None, None, 4 0           activation_168[0][0]             \n",
      "                                                                 activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_2_conv (Conv2D)          (None, None, None, 2 933920      block8_2_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_2 (Lambda)               (None, None, None, 2 0           block8_1_ac[0][0]                \n",
      "                                                                 block8_2_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_2_ac (Activation)        (None, None, None, 2 0           block8_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, None, None, 1 399360      block8_2_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, None, None, 1 576         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, None, None, 1 0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, None, None, 2 129024      activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, None, None, 2 672         conv2d_174[0][0]                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, None, None, 2 0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, None, None, 1 399360      block8_2_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, None, None, 2 172032      activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, None, None, 1 576         conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, None, None, 2 768         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, None, None, 1 0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, None, None, 2 0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_3_mixed (Concatenate)    (None, None, None, 4 0           activation_172[0][0]             \n",
      "                                                                 activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_3_conv (Conv2D)          (None, None, None, 2 933920      block8_3_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_3 (Lambda)               (None, None, None, 2 0           block8_2_ac[0][0]                \n",
      "                                                                 block8_3_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_3_ac (Activation)        (None, None, None, 2 0           block8_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, None, None, 1 399360      block8_3_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, None, None, 1 576         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, None, None, 1 0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, None, None, 2 129024      activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, None, None, 2 672         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, None, None, 2 0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, None, None, 1 399360      block8_3_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, None, None, 2 172032      activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, None, None, 1 576         conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, None, None, 2 768         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, None, None, 1 0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, None, None, 2 0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_4_mixed (Concatenate)    (None, None, None, 4 0           activation_176[0][0]             \n",
      "                                                                 activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_4_conv (Conv2D)          (None, None, None, 2 933920      block8_4_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_4 (Lambda)               (None, None, None, 2 0           block8_3_ac[0][0]                \n",
      "                                                                 block8_4_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_4_ac (Activation)        (None, None, None, 2 0           block8_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, None, None, 1 399360      block8_4_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, None, None, 1 576         conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, None, None, 1 0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, None, None, 2 129024      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, None, None, 2 672         conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, None, None, 2 0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, None, None, 1 399360      block8_4_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, None, None, 2 172032      activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, None, None, 1 576         conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, None, None, 2 768         conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, None, None, 1 0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, None, None, 2 0           batch_normalization_183[0][0]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "block8_5_mixed (Concatenate)    (None, None, None, 4 0           activation_180[0][0]             \n",
      "                                                                 activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_5_conv (Conv2D)          (None, None, None, 2 933920      block8_5_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_5 (Lambda)               (None, None, None, 2 0           block8_4_ac[0][0]                \n",
      "                                                                 block8_5_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_5_ac (Activation)        (None, None, None, 2 0           block8_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, None, None, 1 399360      block8_5_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, None, None, 1 576         conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, None, None, 1 0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, None, None, 2 129024      activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, None, None, 2 672         conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, None, None, 2 0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, None, None, 1 399360      block8_5_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, None, None, 2 172032      activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, None, None, 1 576         conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, None, None, 2 768         conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, None, None, 1 0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, None, None, 2 0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_6_mixed (Concatenate)    (None, None, None, 4 0           activation_184[0][0]             \n",
      "                                                                 activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_6_conv (Conv2D)          (None, None, None, 2 933920      block8_6_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_6 (Lambda)               (None, None, None, 2 0           block8_5_ac[0][0]                \n",
      "                                                                 block8_6_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_6_ac (Activation)        (None, None, None, 2 0           block8_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, None, None, 1 399360      block8_6_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, None, None, 1 576         conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, None, None, 1 0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, None, None, 2 129024      activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, None, None, 2 672         conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, None, None, 2 0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, None, None, 1 399360      block8_6_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, None, None, 2 172032      activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, None, None, 1 576         conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, None, None, 2 768         conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, None, None, 1 0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, None, None, 2 0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_7_mixed (Concatenate)    (None, None, None, 4 0           activation_188[0][0]             \n",
      "                                                                 activation_191[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_7_conv (Conv2D)          (None, None, None, 2 933920      block8_7_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_7 (Lambda)               (None, None, None, 2 0           block8_6_ac[0][0]                \n",
      "                                                                 block8_7_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_7_ac (Activation)        (None, None, None, 2 0           block8_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, None, None, 1 399360      block8_7_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchN (None, None, None, 1 576         conv2d_193[0][0]                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, None, None, 1 0           batch_normalization_193[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, None, None, 2 129024      activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, None, None, 2 672         conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, None, None, 2 0           batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, None, None, 1 399360      block8_7_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, None, None, 2 172032      activation_194[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, None, None, 1 576         conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, None, None, 2 768         conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, None, None, 1 0           batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, None, None, 2 0           batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_8_mixed (Concatenate)    (None, None, None, 4 0           activation_192[0][0]             \n",
      "                                                                 activation_195[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_8_conv (Conv2D)          (None, None, None, 2 933920      block8_8_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_8 (Lambda)               (None, None, None, 2 0           block8_7_ac[0][0]                \n",
      "                                                                 block8_8_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_8_ac (Activation)        (None, None, None, 2 0           block8_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, None, None, 1 399360      block8_8_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, None, None, 1 576         conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, None, None, 1 0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, None, None, 2 129024      activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, None, None, 2 672         conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, None, None, 2 0           batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, None, None, 1 399360      block8_8_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, None, None, 2 172032      activation_198[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, None, None, 1 576         conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, None, None, 2 768         conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, None, None, 1 0           batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, None, None, 2 0           batch_normalization_199[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_9_mixed (Concatenate)    (None, None, None, 4 0           activation_196[0][0]             \n",
      "                                                                 activation_199[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_9_conv (Conv2D)          (None, None, None, 2 933920      block8_9_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_9 (Lambda)               (None, None, None, 2 0           block8_8_ac[0][0]                \n",
      "                                                                 block8_9_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_9_ac (Activation)        (None, None, None, 2 0           block8_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, None, None, 1 399360      block8_9_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, None, None, 1 576         conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, None, None, 1 0           batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, None, None, 2 129024      activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchN (None, None, None, 2 672         conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, None, None, 2 0           batch_normalization_202[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, None, None, 1 399360      block8_9_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, None, None, 2 172032      activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, None, None, 1 576         conv2d_200[0][0]                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchN (None, None, None, 2 768         conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, None, None, 1 0           batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, None, None, 2 0           batch_normalization_203[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_10_mixed (Concatenate)   (None, None, None, 4 0           activation_200[0][0]             \n",
      "                                                                 activation_203[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_10_conv (Conv2D)         (None, None, None, 2 933920      block8_10_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_10 (Lambda)              (None, None, None, 2 0           block8_9_ac[0][0]                \n",
      "                                                                 block8_10_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_7b (Conv2D)                (None, None, None, 1 3194880     block8_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_7b_bn (BatchNormalization) (None, None, None, 1 4608        conv_7b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_7b_ac (Activation)         (None, None, None, 1 0           conv_7b_bn[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 54,336,736\n",
      "Trainable params: 54,276,192\n",
      "Non-trainable params: 60,544\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'inception_resnet_v2',\n",
       " 'layers': [{'name': 'input_1',\n",
       "   'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, None, None, 3),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'name': 'input_1'},\n",
       "   'inbound_nodes': []},\n",
       "  {'name': 'conv2d_1',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['input_1', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_1',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_1', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_2',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_1', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_2', 0, 0, {}]]]},\n",
       "  {'name': 'activation_2',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_2', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_3',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_2', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_3',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_3', 0, 0, {}]]]},\n",
       "  {'name': 'activation_3',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_3', 0, 0, {}]]]},\n",
       "  {'name': 'max_pooling2d_1',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'max_pooling2d_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_3', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_4',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 80,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['max_pooling2d_1', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_4',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_4', 0, 0, {}]]]},\n",
       "  {'name': 'activation_4',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_4', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_5',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_4', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_5',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_5',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_5', 0, 0, {}]]]},\n",
       "  {'name': 'max_pooling2d_2',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'max_pooling2d_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_5', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_9',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['max_pooling2d_2', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_9',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_9', 0, 0, {}]]]},\n",
       "  {'name': 'activation_9',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_9', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_7',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 48,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['max_pooling2d_2', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_10',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 96,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_9', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_7',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_7', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_10',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_10', 0, 0, {}]]]},\n",
       "  {'name': 'activation_7',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_7', 0, 0, {}]]]},\n",
       "  {'name': 'activation_10',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_10', 0, 0, {}]]]},\n",
       "  {'name': 'average_pooling2d_1',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'average_pooling2d_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['max_pooling2d_2', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_6',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 96,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['max_pooling2d_2', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_8',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_7', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_11',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 96,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_10', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_12',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['average_pooling2d_1', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_6',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_6', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_8', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_11',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_11', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_12',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_12', 0, 0, {}]]]},\n",
       "  {'name': 'activation_6',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_6', 0, 0, {}]]]},\n",
       "  {'name': 'activation_8',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_11',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_11', 0, 0, {}]]]},\n",
       "  {'name': 'activation_12',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_12', 0, 0, {}]]]},\n",
       "  {'name': 'mixed_5b',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed_5b',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_6', 0, 0, {}],\n",
       "     ['activation_8', 0, 0, {}],\n",
       "     ['activation_11', 0, 0, {}],\n",
       "     ['activation_12', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_16',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_16',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed_5b', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_16',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_16',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_16', 0, 0, {}]]]},\n",
       "  {'name': 'activation_16',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_16',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_16', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_14',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_14',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed_5b', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_17',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_17',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 48,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_16', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_14',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_14',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_14', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_17',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_17',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_17', 0, 0, {}]]]},\n",
       "  {'name': 'activation_14',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_14',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_14', 0, 0, {}]]]},\n",
       "  {'name': 'activation_17',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_17',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_17', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_13',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_13',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed_5b', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_15',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_15',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_14', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_18',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_18',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_17', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_13',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_13',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_13', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_15',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_15',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_15', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_18',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_18',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_18', 0, 0, {}]]]},\n",
       "  {'name': 'activation_13',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_13',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_13', 0, 0, {}]]]},\n",
       "  {'name': 'activation_15',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_15',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_15', 0, 0, {}]]]},\n",
       "  {'name': 'activation_18',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_18',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_18', 0, 0, {}]]]},\n",
       "  {'name': 'block35_1_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block35_1_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_13', 0, 0, {}],\n",
       "     ['activation_15', 0, 0, {}],\n",
       "     ['activation_18', 0, 0, {}]]]},\n",
       "  {'name': 'block35_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block35_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 320,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_1_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block35_1',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block35_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 320),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.17}},\n",
       "   'inbound_nodes': [[['mixed_5b', 0, 0, {}], ['block35_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block35_1_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block35_1_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block35_1', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_22',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_22',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_1_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_22',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_22',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_22', 0, 0, {}]]]},\n",
       "  {'name': 'activation_22',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_22',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_22', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_20',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_20',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_1_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_23',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_23',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 48,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_22', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_20',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_20',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_20', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_23',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_23',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_23', 0, 0, {}]]]},\n",
       "  {'name': 'activation_20',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_20',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_20', 0, 0, {}]]]},\n",
       "  {'name': 'activation_23',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_23',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_23', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_19',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_19',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_1_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_21',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_21',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_20', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_24',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_24',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_23', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_19',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_19',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_19', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_21',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_21',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_21', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_24',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_24',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_24', 0, 0, {}]]]},\n",
       "  {'name': 'activation_19',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_19',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_19', 0, 0, {}]]]},\n",
       "  {'name': 'activation_21',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_21',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_21', 0, 0, {}]]]},\n",
       "  {'name': 'activation_24',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_24',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_24', 0, 0, {}]]]},\n",
       "  {'name': 'block35_2_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block35_2_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_19', 0, 0, {}],\n",
       "     ['activation_21', 0, 0, {}],\n",
       "     ['activation_24', 0, 0, {}]]]},\n",
       "  {'name': 'block35_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block35_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 320,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_2_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block35_2',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block35_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 320),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.17}},\n",
       "   'inbound_nodes': [[['block35_1_ac', 0, 0, {}],\n",
       "     ['block35_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block35_2_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block35_2_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block35_2', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_28',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_28',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_2_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_28',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_28',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_28', 0, 0, {}]]]},\n",
       "  {'name': 'activation_28',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_28',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_28', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_26',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_26',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_2_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_29',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_29',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 48,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_28', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_26',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_26',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_26', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_29',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_29',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_29', 0, 0, {}]]]},\n",
       "  {'name': 'activation_26',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_26',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_26', 0, 0, {}]]]},\n",
       "  {'name': 'activation_29',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_29',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_29', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_25',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_25',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_2_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_27',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_27',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_26', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_30',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_30',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_29', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_25',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_25',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_25', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_27',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_27',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_27', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_30',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_30',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_30', 0, 0, {}]]]},\n",
       "  {'name': 'activation_25',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_25',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_25', 0, 0, {}]]]},\n",
       "  {'name': 'activation_27',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_27',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_27', 0, 0, {}]]]},\n",
       "  {'name': 'activation_30',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_30',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_30', 0, 0, {}]]]},\n",
       "  {'name': 'block35_3_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block35_3_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_25', 0, 0, {}],\n",
       "     ['activation_27', 0, 0, {}],\n",
       "     ['activation_30', 0, 0, {}]]]},\n",
       "  {'name': 'block35_3_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block35_3_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 320,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_3_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block35_3',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block35_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 320),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.17}},\n",
       "   'inbound_nodes': [[['block35_2_ac', 0, 0, {}],\n",
       "     ['block35_3_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block35_3_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block35_3_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block35_3', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_34',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_34',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_3_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_34',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_34',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_34', 0, 0, {}]]]},\n",
       "  {'name': 'activation_34',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_34',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_34', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_32',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_32',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_3_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_35',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_35',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 48,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_34', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_32',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_32',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_32', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_35',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_35',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_35', 0, 0, {}]]]},\n",
       "  {'name': 'activation_32',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_32',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_32', 0, 0, {}]]]},\n",
       "  {'name': 'activation_35',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_35',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_35', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_31',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_31',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_3_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_33',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_33',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_32', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_36',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_36',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_35', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_31',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_31',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_31', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_33',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_33',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_33', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_36',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_36',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_36', 0, 0, {}]]]},\n",
       "  {'name': 'activation_31',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_31',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_31', 0, 0, {}]]]},\n",
       "  {'name': 'activation_33',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_33',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_33', 0, 0, {}]]]},\n",
       "  {'name': 'activation_36',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_36',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_36', 0, 0, {}]]]},\n",
       "  {'name': 'block35_4_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block35_4_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_31', 0, 0, {}],\n",
       "     ['activation_33', 0, 0, {}],\n",
       "     ['activation_36', 0, 0, {}]]]},\n",
       "  {'name': 'block35_4_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block35_4_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 320,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_4_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block35_4',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block35_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 320),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.17}},\n",
       "   'inbound_nodes': [[['block35_3_ac', 0, 0, {}],\n",
       "     ['block35_4_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block35_4_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block35_4_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block35_4', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_40',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_40',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_4_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_40',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_40',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_40', 0, 0, {}]]]},\n",
       "  {'name': 'activation_40',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_40',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_40', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_38',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_38',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_4_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_41',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_41',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 48,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_40', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_38',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_38',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_38', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_41',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_41',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_41', 0, 0, {}]]]},\n",
       "  {'name': 'activation_38',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_38',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_38', 0, 0, {}]]]},\n",
       "  {'name': 'activation_41',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_41',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_41', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_37',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_37',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_4_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_39',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_39',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_38', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_42',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_42',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_41', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_37',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_37',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_37', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_39',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_39',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_39', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_42',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_42',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_42', 0, 0, {}]]]},\n",
       "  {'name': 'activation_37',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_37',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_37', 0, 0, {}]]]},\n",
       "  {'name': 'activation_39',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_39',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_39', 0, 0, {}]]]},\n",
       "  {'name': 'activation_42',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_42',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_42', 0, 0, {}]]]},\n",
       "  {'name': 'block35_5_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block35_5_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_37', 0, 0, {}],\n",
       "     ['activation_39', 0, 0, {}],\n",
       "     ['activation_42', 0, 0, {}]]]},\n",
       "  {'name': 'block35_5_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block35_5_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 320,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_5_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block35_5',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block35_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 320),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.17}},\n",
       "   'inbound_nodes': [[['block35_4_ac', 0, 0, {}],\n",
       "     ['block35_5_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block35_5_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block35_5_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block35_5', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_46',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_46',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_5_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_46',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_46',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_46', 0, 0, {}]]]},\n",
       "  {'name': 'activation_46',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_46',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_46', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_44',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_44',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_5_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_47',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_47',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 48,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_46', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_44',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_44',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_44', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_47',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_47',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_47', 0, 0, {}]]]},\n",
       "  {'name': 'activation_44',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_44',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_44', 0, 0, {}]]]},\n",
       "  {'name': 'activation_47',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_47',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_47', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_43',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_43',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_5_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_45',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_45',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_44', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_48',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_48',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_47', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_43',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_43',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_43', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_45',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_45',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_45', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_48',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_48',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_48', 0, 0, {}]]]},\n",
       "  {'name': 'activation_43',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_43',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_43', 0, 0, {}]]]},\n",
       "  {'name': 'activation_45',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_45',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_45', 0, 0, {}]]]},\n",
       "  {'name': 'activation_48',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_48',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_48', 0, 0, {}]]]},\n",
       "  {'name': 'block35_6_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block35_6_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_43', 0, 0, {}],\n",
       "     ['activation_45', 0, 0, {}],\n",
       "     ['activation_48', 0, 0, {}]]]},\n",
       "  {'name': 'block35_6_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block35_6_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 320,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_6_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block35_6',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block35_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 320),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.17}},\n",
       "   'inbound_nodes': [[['block35_5_ac', 0, 0, {}],\n",
       "     ['block35_6_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block35_6_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block35_6_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block35_6', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_52',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_52',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_6_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_52',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_52',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_52', 0, 0, {}]]]},\n",
       "  {'name': 'activation_52',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_52',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_52', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_50',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_50',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_6_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_53',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_53',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 48,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_52', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_50',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_50',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_50', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_53',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_53',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_53', 0, 0, {}]]]},\n",
       "  {'name': 'activation_50',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_50',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_50', 0, 0, {}]]]},\n",
       "  {'name': 'activation_53',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_53',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_53', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_49',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_49',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_6_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_51',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_51',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_50', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_54',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_54',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_53', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_49',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_49',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_49', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_51',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_51',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_51', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_54',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_54',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_54', 0, 0, {}]]]},\n",
       "  {'name': 'activation_49',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_49',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_49', 0, 0, {}]]]},\n",
       "  {'name': 'activation_51',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_51',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_51', 0, 0, {}]]]},\n",
       "  {'name': 'activation_54',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_54',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_54', 0, 0, {}]]]},\n",
       "  {'name': 'block35_7_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block35_7_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_49', 0, 0, {}],\n",
       "     ['activation_51', 0, 0, {}],\n",
       "     ['activation_54', 0, 0, {}]]]},\n",
       "  {'name': 'block35_7_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block35_7_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 320,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_7_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block35_7',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block35_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 320),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.17}},\n",
       "   'inbound_nodes': [[['block35_6_ac', 0, 0, {}],\n",
       "     ['block35_7_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block35_7_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block35_7_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block35_7', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_58',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_58',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_7_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_58',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_58',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_58', 0, 0, {}]]]},\n",
       "  {'name': 'activation_58',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_58',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_58', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_56',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_56',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_7_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_59',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_59',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 48,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_58', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_56',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_56',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_56', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_59',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_59',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_59', 0, 0, {}]]]},\n",
       "  {'name': 'activation_56',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_56',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_56', 0, 0, {}]]]},\n",
       "  {'name': 'activation_59',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_59',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_59', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_55',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_55',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_7_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_57',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_57',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_56', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_60',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_60',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_59', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_55',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_55',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_55', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_57',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_57',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_57', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_60',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_60',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_60', 0, 0, {}]]]},\n",
       "  {'name': 'activation_55',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_55',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_55', 0, 0, {}]]]},\n",
       "  {'name': 'activation_57',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_57',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_57', 0, 0, {}]]]},\n",
       "  {'name': 'activation_60',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_60',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_60', 0, 0, {}]]]},\n",
       "  {'name': 'block35_8_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block35_8_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_55', 0, 0, {}],\n",
       "     ['activation_57', 0, 0, {}],\n",
       "     ['activation_60', 0, 0, {}]]]},\n",
       "  {'name': 'block35_8_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block35_8_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 320,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_8_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block35_8',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block35_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 320),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.17}},\n",
       "   'inbound_nodes': [[['block35_7_ac', 0, 0, {}],\n",
       "     ['block35_8_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block35_8_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block35_8_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block35_8', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_64',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_64',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_8_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_64',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_64',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_64', 0, 0, {}]]]},\n",
       "  {'name': 'activation_64',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_64',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_64', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_62',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_62',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_8_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_65',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_65',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 48,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_64', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_62',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_62',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_62', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_65',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_65',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_65', 0, 0, {}]]]},\n",
       "  {'name': 'activation_62',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_62',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_62', 0, 0, {}]]]},\n",
       "  {'name': 'activation_65',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_65',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_65', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_61',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_61',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_8_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_63',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_63',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_62', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_66',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_66',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_65', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_61',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_61',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_61', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_63',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_63',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_63', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_66',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_66',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_66', 0, 0, {}]]]},\n",
       "  {'name': 'activation_61',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_61',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_61', 0, 0, {}]]]},\n",
       "  {'name': 'activation_63',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_63',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_63', 0, 0, {}]]]},\n",
       "  {'name': 'activation_66',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_66',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_66', 0, 0, {}]]]},\n",
       "  {'name': 'block35_9_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block35_9_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_61', 0, 0, {}],\n",
       "     ['activation_63', 0, 0, {}],\n",
       "     ['activation_66', 0, 0, {}]]]},\n",
       "  {'name': 'block35_9_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block35_9_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 320,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_9_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block35_9',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block35_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 320),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.17}},\n",
       "   'inbound_nodes': [[['block35_8_ac', 0, 0, {}],\n",
       "     ['block35_9_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block35_9_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block35_9_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block35_9', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_70',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_70',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_9_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_70',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_70',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_70', 0, 0, {}]]]},\n",
       "  {'name': 'activation_70',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_70',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_70', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_68',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_68',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_9_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_71',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_71',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 48,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_70', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_68',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_68',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_68', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_71',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_71',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_71', 0, 0, {}]]]},\n",
       "  {'name': 'activation_68',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_68',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_68', 0, 0, {}]]]},\n",
       "  {'name': 'activation_71',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_71',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_71', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_67',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_67',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_9_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_69',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_69',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_68', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_72',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_72',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_71', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_67',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_67',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_67', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_69',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_69',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_69', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_72',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_72',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_72', 0, 0, {}]]]},\n",
       "  {'name': 'activation_67',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_67',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_67', 0, 0, {}]]]},\n",
       "  {'name': 'activation_69',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_69',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_69', 0, 0, {}]]]},\n",
       "  {'name': 'activation_72',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_72',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_72', 0, 0, {}]]]},\n",
       "  {'name': 'block35_10_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block35_10_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_67', 0, 0, {}],\n",
       "     ['activation_69', 0, 0, {}],\n",
       "     ['activation_72', 0, 0, {}]]]},\n",
       "  {'name': 'block35_10_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block35_10_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 320,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_10_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block35_10',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block35_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 320),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.17}},\n",
       "   'inbound_nodes': [[['block35_9_ac', 0, 0, {}],\n",
       "     ['block35_10_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block35_10_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block35_10_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block35_10', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_74',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_74',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_10_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_74',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_74',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_74', 0, 0, {}]]]},\n",
       "  {'name': 'activation_74',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_74',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_74', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_75',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_75',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_74', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_75',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_75',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_75', 0, 0, {}]]]},\n",
       "  {'name': 'activation_75',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_75',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_75', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_73',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_73',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block35_10_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_76',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_76',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_75', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_73',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_73',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_73', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_76',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_76',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_76', 0, 0, {}]]]},\n",
       "  {'name': 'activation_73',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_73',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_73', 0, 0, {}]]]},\n",
       "  {'name': 'activation_76',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_76',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_76', 0, 0, {}]]]},\n",
       "  {'name': 'max_pooling2d_3',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'max_pooling2d_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['block35_10_ac', 0, 0, {}]]]},\n",
       "  {'name': 'mixed_6a',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed_6a',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_73', 0, 0, {}],\n",
       "     ['activation_76', 0, 0, {}],\n",
       "     ['max_pooling2d_3', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_78',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_78',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed_6a', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_78',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_78',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_78', 0, 0, {}]]]},\n",
       "  {'name': 'activation_78',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_78',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_78', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_79',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_79',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_78', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_79',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_79',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_79', 0, 0, {}]]]},\n",
       "  {'name': 'activation_79',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_79',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_79', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_77',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_77',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed_6a', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_80',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_80',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_79', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_77',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_77',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_77', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_80',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_80',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_80', 0, 0, {}]]]},\n",
       "  {'name': 'activation_77',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_77',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_77', 0, 0, {}]]]},\n",
       "  {'name': 'activation_80',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_80',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_80', 0, 0, {}]]]},\n",
       "  {'name': 'block17_1_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_1_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_77', 0, 0, {}],\n",
       "     ['activation_80', 0, 0, {}]]]},\n",
       "  {'name': 'block17_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_1_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_1',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['mixed_6a', 0, 0, {}], ['block17_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_1_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_1_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_1', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_82',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_82',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_1_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_82',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_82',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_82', 0, 0, {}]]]},\n",
       "  {'name': 'activation_82',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_82',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_82', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_83',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_83',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_82', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_83',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_83',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_83', 0, 0, {}]]]},\n",
       "  {'name': 'activation_83',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_83',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_83', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_81',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_81',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_1_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_84',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_84',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_83', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_81',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_81',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_81', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_84',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_84',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_84', 0, 0, {}]]]},\n",
       "  {'name': 'activation_81',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_81',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_81', 0, 0, {}]]]},\n",
       "  {'name': 'activation_84',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_84',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_84', 0, 0, {}]]]},\n",
       "  {'name': 'block17_2_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_2_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_81', 0, 0, {}],\n",
       "     ['activation_84', 0, 0, {}]]]},\n",
       "  {'name': 'block17_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_2_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_2',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_1_ac', 0, 0, {}],\n",
       "     ['block17_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_2_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_2_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_2', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_86',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_86',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_2_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_86',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_86',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_86', 0, 0, {}]]]},\n",
       "  {'name': 'activation_86',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_86',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_86', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_87',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_87',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_86', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_87',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_87',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_87', 0, 0, {}]]]},\n",
       "  {'name': 'activation_87',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_87',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_87', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_85',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_85',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_2_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_88',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_88',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_87', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_85',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_85',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_85', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_88',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_88',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_88', 0, 0, {}]]]},\n",
       "  {'name': 'activation_85',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_85',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_85', 0, 0, {}]]]},\n",
       "  {'name': 'activation_88',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_88',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_88', 0, 0, {}]]]},\n",
       "  {'name': 'block17_3_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_3_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_85', 0, 0, {}],\n",
       "     ['activation_88', 0, 0, {}]]]},\n",
       "  {'name': 'block17_3_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_3_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_3_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_3',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_2_ac', 0, 0, {}],\n",
       "     ['block17_3_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_3_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_3_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_3', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_90',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_90',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_3_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_90',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_90',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_90', 0, 0, {}]]]},\n",
       "  {'name': 'activation_90',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_90',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_90', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_91',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_91',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_90', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_91',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_91',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_91', 0, 0, {}]]]},\n",
       "  {'name': 'activation_91',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_91',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_91', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_89',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_89',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_3_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_92',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_92',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_91', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_89',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_89',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_89', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_92',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_92',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_92', 0, 0, {}]]]},\n",
       "  {'name': 'activation_89',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_89',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_89', 0, 0, {}]]]},\n",
       "  {'name': 'activation_92',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_92',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_92', 0, 0, {}]]]},\n",
       "  {'name': 'block17_4_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_4_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_89', 0, 0, {}],\n",
       "     ['activation_92', 0, 0, {}]]]},\n",
       "  {'name': 'block17_4_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_4_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_4_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_4',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_3_ac', 0, 0, {}],\n",
       "     ['block17_4_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_4_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_4_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_4', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_94',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_94',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_4_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_94',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_94',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_94', 0, 0, {}]]]},\n",
       "  {'name': 'activation_94',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_94',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_94', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_95',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_95',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_94', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_95',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_95',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_95', 0, 0, {}]]]},\n",
       "  {'name': 'activation_95',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_95',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_95', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_93',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_93',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_4_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_96',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_96',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_95', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_93',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_93',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_93', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_96',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_96',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_96', 0, 0, {}]]]},\n",
       "  {'name': 'activation_93',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_93',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_93', 0, 0, {}]]]},\n",
       "  {'name': 'activation_96',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_96',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_96', 0, 0, {}]]]},\n",
       "  {'name': 'block17_5_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_5_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_93', 0, 0, {}],\n",
       "     ['activation_96', 0, 0, {}]]]},\n",
       "  {'name': 'block17_5_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_5_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_5_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_5',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_4_ac', 0, 0, {}],\n",
       "     ['block17_5_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_5_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_5_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_5', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_98',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_98',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_5_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_98',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_98',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_98', 0, 0, {}]]]},\n",
       "  {'name': 'activation_98',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_98',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_98', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_99',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_99',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_98', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_99',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_99',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_99', 0, 0, {}]]]},\n",
       "  {'name': 'activation_99',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_99',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_99', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_97',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_97',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_5_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_100',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_100',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_99', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_97',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_97',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_97', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_100',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_100',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_100', 0, 0, {}]]]},\n",
       "  {'name': 'activation_97',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_97',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_97', 0, 0, {}]]]},\n",
       "  {'name': 'activation_100',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_100',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_100', 0, 0, {}]]]},\n",
       "  {'name': 'block17_6_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_6_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_97', 0, 0, {}],\n",
       "     ['activation_100', 0, 0, {}]]]},\n",
       "  {'name': 'block17_6_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_6_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_6_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_6',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_5_ac', 0, 0, {}],\n",
       "     ['block17_6_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_6_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_6_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_6', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_102',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_102',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_6_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_102',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_102',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_102', 0, 0, {}]]]},\n",
       "  {'name': 'activation_102',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_102',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_102', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_103',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_103',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_102', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_103',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_103',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_103', 0, 0, {}]]]},\n",
       "  {'name': 'activation_103',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_103',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_103', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_101',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_101',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_6_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_104',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_104',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_103', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_101',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_101',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_101', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_104',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_104',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_104', 0, 0, {}]]]},\n",
       "  {'name': 'activation_101',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_101',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_101', 0, 0, {}]]]},\n",
       "  {'name': 'activation_104',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_104',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_104', 0, 0, {}]]]},\n",
       "  {'name': 'block17_7_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_7_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_101', 0, 0, {}],\n",
       "     ['activation_104', 0, 0, {}]]]},\n",
       "  {'name': 'block17_7_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_7_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_7_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_7',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_6_ac', 0, 0, {}],\n",
       "     ['block17_7_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_7_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_7_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_7', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_106',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_106',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_7_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_106',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_106',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_106', 0, 0, {}]]]},\n",
       "  {'name': 'activation_106',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_106',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_106', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_107',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_107',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_106', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_107',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_107',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_107', 0, 0, {}]]]},\n",
       "  {'name': 'activation_107',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_107',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_107', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_105',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_105',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_7_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_108',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_108',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_107', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_105',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_105',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_105', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_108',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_108',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_108', 0, 0, {}]]]},\n",
       "  {'name': 'activation_105',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_105',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_105', 0, 0, {}]]]},\n",
       "  {'name': 'activation_108',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_108',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_108', 0, 0, {}]]]},\n",
       "  {'name': 'block17_8_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_8_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_105', 0, 0, {}],\n",
       "     ['activation_108', 0, 0, {}]]]},\n",
       "  {'name': 'block17_8_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_8_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_8_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_8',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_7_ac', 0, 0, {}],\n",
       "     ['block17_8_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_8_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_8_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_8', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_110',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_110',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_8_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_110',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_110',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_110', 0, 0, {}]]]},\n",
       "  {'name': 'activation_110',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_110',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_110', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_111',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_111',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_110', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_111',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_111',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_111', 0, 0, {}]]]},\n",
       "  {'name': 'activation_111',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_111',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_111', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_109',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_109',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_8_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_112',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_112',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_111', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_109',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_109',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_109', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_112',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_112',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_112', 0, 0, {}]]]},\n",
       "  {'name': 'activation_109',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_109',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_109', 0, 0, {}]]]},\n",
       "  {'name': 'activation_112',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_112',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_112', 0, 0, {}]]]},\n",
       "  {'name': 'block17_9_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_9_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_109', 0, 0, {}],\n",
       "     ['activation_112', 0, 0, {}]]]},\n",
       "  {'name': 'block17_9_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_9_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_9_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_9',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_8_ac', 0, 0, {}],\n",
       "     ['block17_9_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_9_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_9_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_9', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_114',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_114',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_9_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_114',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_114',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_114', 0, 0, {}]]]},\n",
       "  {'name': 'activation_114',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_114',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_114', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_115',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_115',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_114', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_115',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_115',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_115', 0, 0, {}]]]},\n",
       "  {'name': 'activation_115',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_115',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_115', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_113',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_113',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_9_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_116',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_116',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_115', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_113',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_113',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_113', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_116',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_116',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_116', 0, 0, {}]]]},\n",
       "  {'name': 'activation_113',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_113',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_113', 0, 0, {}]]]},\n",
       "  {'name': 'activation_116',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_116',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_116', 0, 0, {}]]]},\n",
       "  {'name': 'block17_10_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_10_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_113', 0, 0, {}],\n",
       "     ['activation_116', 0, 0, {}]]]},\n",
       "  {'name': 'block17_10_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_10_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_10_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_10',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_9_ac', 0, 0, {}],\n",
       "     ['block17_10_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_10_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_10_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_10', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_118',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_118',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_10_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_118',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_118',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_118', 0, 0, {}]]]},\n",
       "  {'name': 'activation_118',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_118',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_118', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_119',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_119',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_118', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_119',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_119',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_119', 0, 0, {}]]]},\n",
       "  {'name': 'activation_119',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_119',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_119', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_117',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_117',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_10_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_120',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_120',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_119', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_117',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_117',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_117', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_120',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_120',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_120', 0, 0, {}]]]},\n",
       "  {'name': 'activation_117',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_117',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_117', 0, 0, {}]]]},\n",
       "  {'name': 'activation_120',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_120',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_120', 0, 0, {}]]]},\n",
       "  {'name': 'block17_11_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_11_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_117', 0, 0, {}],\n",
       "     ['activation_120', 0, 0, {}]]]},\n",
       "  {'name': 'block17_11_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_11_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_11_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_11',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_10_ac', 0, 0, {}],\n",
       "     ['block17_11_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_11_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_11_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_11', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_122',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_122',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_11_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_122',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_122',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_122', 0, 0, {}]]]},\n",
       "  {'name': 'activation_122',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_122',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_122', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_123',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_123',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_122', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_123',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_123',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_123', 0, 0, {}]]]},\n",
       "  {'name': 'activation_123',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_123',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_123', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_121',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_121',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_11_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_124',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_124',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_123', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_121',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_121',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_121', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_124',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_124',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_124', 0, 0, {}]]]},\n",
       "  {'name': 'activation_121',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_121',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_121', 0, 0, {}]]]},\n",
       "  {'name': 'activation_124',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_124',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_124', 0, 0, {}]]]},\n",
       "  {'name': 'block17_12_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_12_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_121', 0, 0, {}],\n",
       "     ['activation_124', 0, 0, {}]]]},\n",
       "  {'name': 'block17_12_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_12_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_12_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_12',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_11_ac', 0, 0, {}],\n",
       "     ['block17_12_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_12_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_12_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_12', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_126',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_126',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_12_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_126',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_126',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_126', 0, 0, {}]]]},\n",
       "  {'name': 'activation_126',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_126',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_126', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_127',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_127',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_126', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_127',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_127',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_127', 0, 0, {}]]]},\n",
       "  {'name': 'activation_127',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_127',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_127', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_125',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_125',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_12_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_128',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_128',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_127', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_125',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_125',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_125', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_128',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_128',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_128', 0, 0, {}]]]},\n",
       "  {'name': 'activation_125',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_125',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_125', 0, 0, {}]]]},\n",
       "  {'name': 'activation_128',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_128',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_128', 0, 0, {}]]]},\n",
       "  {'name': 'block17_13_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_13_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_125', 0, 0, {}],\n",
       "     ['activation_128', 0, 0, {}]]]},\n",
       "  {'name': 'block17_13_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_13_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_13_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_13',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_13',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_12_ac', 0, 0, {}],\n",
       "     ['block17_13_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_13_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_13_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_13', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_130',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_130',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_13_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_130',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_130',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_130', 0, 0, {}]]]},\n",
       "  {'name': 'activation_130',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_130',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_130', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_131',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_131',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_130', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_131',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_131',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_131', 0, 0, {}]]]},\n",
       "  {'name': 'activation_131',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_131',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_131', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_129',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_129',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_13_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_132',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_132',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_131', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_129',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_129',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_129', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_132',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_132',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_132', 0, 0, {}]]]},\n",
       "  {'name': 'activation_129',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_129',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_129', 0, 0, {}]]]},\n",
       "  {'name': 'activation_132',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_132',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_132', 0, 0, {}]]]},\n",
       "  {'name': 'block17_14_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_14_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_129', 0, 0, {}],\n",
       "     ['activation_132', 0, 0, {}]]]},\n",
       "  {'name': 'block17_14_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_14_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_14_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_14',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_14',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_13_ac', 0, 0, {}],\n",
       "     ['block17_14_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_14_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_14_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_14', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_134',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_134',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_14_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_134',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_134',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_134', 0, 0, {}]]]},\n",
       "  {'name': 'activation_134',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_134',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_134', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_135',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_135',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_134', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_135',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_135',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_135', 0, 0, {}]]]},\n",
       "  {'name': 'activation_135',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_135',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_135', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_133',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_133',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_14_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_136',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_136',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_135', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_133',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_133',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_133', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_136',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_136',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_136', 0, 0, {}]]]},\n",
       "  {'name': 'activation_133',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_133',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_133', 0, 0, {}]]]},\n",
       "  {'name': 'activation_136',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_136',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_136', 0, 0, {}]]]},\n",
       "  {'name': 'block17_15_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_15_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_133', 0, 0, {}],\n",
       "     ['activation_136', 0, 0, {}]]]},\n",
       "  {'name': 'block17_15_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_15_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_15_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_15',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_15',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_14_ac', 0, 0, {}],\n",
       "     ['block17_15_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_15_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_15_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_15', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_138',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_138',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_15_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_138',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_138',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_138', 0, 0, {}]]]},\n",
       "  {'name': 'activation_138',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_138',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_138', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_139',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_139',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_138', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_139',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_139',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_139', 0, 0, {}]]]},\n",
       "  {'name': 'activation_139',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_139',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_139', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_137',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_137',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_15_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_140',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_140',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_139', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_137',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_137',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_137', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_140',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_140',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_140', 0, 0, {}]]]},\n",
       "  {'name': 'activation_137',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_137',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_137', 0, 0, {}]]]},\n",
       "  {'name': 'activation_140',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_140',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_140', 0, 0, {}]]]},\n",
       "  {'name': 'block17_16_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_16_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_137', 0, 0, {}],\n",
       "     ['activation_140', 0, 0, {}]]]},\n",
       "  {'name': 'block17_16_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_16_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_16_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_16',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_16',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_15_ac', 0, 0, {}],\n",
       "     ['block17_16_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_16_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_16_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_16', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_142',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_142',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_16_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_142',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_142',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_142', 0, 0, {}]]]},\n",
       "  {'name': 'activation_142',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_142',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_142', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_143',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_143',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_142', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_143',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_143',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_143', 0, 0, {}]]]},\n",
       "  {'name': 'activation_143',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_143',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_143', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_141',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_141',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_16_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_144',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_144',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_143', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_141',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_141',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_141', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_144',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_144',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_144', 0, 0, {}]]]},\n",
       "  {'name': 'activation_141',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_141',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_141', 0, 0, {}]]]},\n",
       "  {'name': 'activation_144',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_144',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_144', 0, 0, {}]]]},\n",
       "  {'name': 'block17_17_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_17_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_141', 0, 0, {}],\n",
       "     ['activation_144', 0, 0, {}]]]},\n",
       "  {'name': 'block17_17_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_17_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_17_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_17',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_17',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_16_ac', 0, 0, {}],\n",
       "     ['block17_17_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_17_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_17_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_17', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_146',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_146',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_17_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_146',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_146',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_146', 0, 0, {}]]]},\n",
       "  {'name': 'activation_146',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_146',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_146', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_147',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_147',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_146', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_147',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_147',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_147', 0, 0, {}]]]},\n",
       "  {'name': 'activation_147',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_147',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_147', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_145',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_145',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_17_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_148',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_148',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_147', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_145',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_145',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_145', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_148',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_148',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_148', 0, 0, {}]]]},\n",
       "  {'name': 'activation_145',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_145',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_145', 0, 0, {}]]]},\n",
       "  {'name': 'activation_148',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_148',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_148', 0, 0, {}]]]},\n",
       "  {'name': 'block17_18_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_18_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_145', 0, 0, {}],\n",
       "     ['activation_148', 0, 0, {}]]]},\n",
       "  {'name': 'block17_18_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_18_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_18_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_18',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_18',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_17_ac', 0, 0, {}],\n",
       "     ['block17_18_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_18_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_18_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_18', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_150',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_150',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_18_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_150',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_150',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_150', 0, 0, {}]]]},\n",
       "  {'name': 'activation_150',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_150',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_150', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_151',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_151',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_150', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_151',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_151',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_151', 0, 0, {}]]]},\n",
       "  {'name': 'activation_151',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_151',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_151', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_149',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_149',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_18_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_152',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_152',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_151', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_149',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_149',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_149', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_152',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_152',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_152', 0, 0, {}]]]},\n",
       "  {'name': 'activation_149',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_149',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_149', 0, 0, {}]]]},\n",
       "  {'name': 'activation_152',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_152',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_152', 0, 0, {}]]]},\n",
       "  {'name': 'block17_19_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_19_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_149', 0, 0, {}],\n",
       "     ['activation_152', 0, 0, {}]]]},\n",
       "  {'name': 'block17_19_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_19_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_19_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_19',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_19',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_18_ac', 0, 0, {}],\n",
       "     ['block17_19_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_19_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_19_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_19', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_154',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_154',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_19_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_154',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_154',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_154', 0, 0, {}]]]},\n",
       "  {'name': 'activation_154',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_154',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_154', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_155',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_155',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_154', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_155',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_155',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_155', 0, 0, {}]]]},\n",
       "  {'name': 'activation_155',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_155',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_155', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_153',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_153',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_19_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_156',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_156',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_155', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_153',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_153',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_153', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_156',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_156',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_156', 0, 0, {}]]]},\n",
       "  {'name': 'activation_153',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_153',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_153', 0, 0, {}]]]},\n",
       "  {'name': 'activation_156',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_156',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_156', 0, 0, {}]]]},\n",
       "  {'name': 'block17_20_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block17_20_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_153', 0, 0, {}],\n",
       "     ['activation_156', 0, 0, {}]]]},\n",
       "  {'name': 'block17_20_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block17_20_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1088,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_20_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block17_20',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block17_20',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 1088),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.1}},\n",
       "   'inbound_nodes': [[['block17_19_ac', 0, 0, {}],\n",
       "     ['block17_20_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block17_20_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block17_20_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block17_20', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_161',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_161',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_20_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_161',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_161',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_161', 0, 0, {}]]]},\n",
       "  {'name': 'activation_161',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_161',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_161', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_157',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_157',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_20_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_159',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_159',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block17_20_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_162',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_162',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 288,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_161', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_157',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_157',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_157', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_159',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_159',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_159', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_162',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_162',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_162', 0, 0, {}]]]},\n",
       "  {'name': 'activation_157',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_157',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_157', 0, 0, {}]]]},\n",
       "  {'name': 'activation_159',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_159',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_159', 0, 0, {}]]]},\n",
       "  {'name': 'activation_162',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_162',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_162', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_158',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_158',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_157', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_160',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_160',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 288,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_159', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_163',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_163',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 320,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_162', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_158',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_158',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_158', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_160',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_160',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_160', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_163',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_163',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_163', 0, 0, {}]]]},\n",
       "  {'name': 'activation_158',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_158',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_158', 0, 0, {}]]]},\n",
       "  {'name': 'activation_160',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_160',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_160', 0, 0, {}]]]},\n",
       "  {'name': 'activation_163',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_163',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_163', 0, 0, {}]]]},\n",
       "  {'name': 'max_pooling2d_4',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'max_pooling2d_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['block17_20_ac', 0, 0, {}]]]},\n",
       "  {'name': 'mixed_7a',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed_7a',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_158', 0, 0, {}],\n",
       "     ['activation_160', 0, 0, {}],\n",
       "     ['activation_163', 0, 0, {}],\n",
       "     ['max_pooling2d_4', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_165',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_165',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed_7a', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_165',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_165',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_165', 0, 0, {}]]]},\n",
       "  {'name': 'activation_165',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_165',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_165', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_166',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_166',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 224,\n",
       "    'kernel_size': (1, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_165', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_166',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_166',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_166', 0, 0, {}]]]},\n",
       "  {'name': 'activation_166',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_166',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_166', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_164',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_164',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed_7a', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_167',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_167',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (3, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_166', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_164',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_164',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_164', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_167',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_167',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_167', 0, 0, {}]]]},\n",
       "  {'name': 'activation_164',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_164',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_164', 0, 0, {}]]]},\n",
       "  {'name': 'activation_167',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_167',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_167', 0, 0, {}]]]},\n",
       "  {'name': 'block8_1_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block8_1_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_164', 0, 0, {}],\n",
       "     ['activation_167', 0, 0, {}]]]},\n",
       "  {'name': 'block8_1_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block8_1_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 2080,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_1_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block8_1',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block8_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 2080),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.2}},\n",
       "   'inbound_nodes': [[['mixed_7a', 0, 0, {}], ['block8_1_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block8_1_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block8_1_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block8_1', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_169',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_169',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_1_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_169',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_169',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_169', 0, 0, {}]]]},\n",
       "  {'name': 'activation_169',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_169',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_169', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_170',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_170',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 224,\n",
       "    'kernel_size': (1, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_169', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_170',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_170',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_170', 0, 0, {}]]]},\n",
       "  {'name': 'activation_170',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_170',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_170', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_168',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_168',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_1_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_171',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_171',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (3, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_170', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_168',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_168',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_168', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_171',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_171',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_171', 0, 0, {}]]]},\n",
       "  {'name': 'activation_168',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_168',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_168', 0, 0, {}]]]},\n",
       "  {'name': 'activation_171',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_171',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_171', 0, 0, {}]]]},\n",
       "  {'name': 'block8_2_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block8_2_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_168', 0, 0, {}],\n",
       "     ['activation_171', 0, 0, {}]]]},\n",
       "  {'name': 'block8_2_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block8_2_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 2080,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_2_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block8_2',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block8_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 2080),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.2}},\n",
       "   'inbound_nodes': [[['block8_1_ac', 0, 0, {}],\n",
       "     ['block8_2_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block8_2_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block8_2_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block8_2', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_173',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_173',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_2_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_173',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_173',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_173', 0, 0, {}]]]},\n",
       "  {'name': 'activation_173',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_173',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_173', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_174',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_174',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 224,\n",
       "    'kernel_size': (1, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_173', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_174',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_174',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_174', 0, 0, {}]]]},\n",
       "  {'name': 'activation_174',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_174',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_174', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_172',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_172',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_2_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_175',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_175',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (3, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_174', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_172',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_172',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_172', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_175',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_175',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_175', 0, 0, {}]]]},\n",
       "  {'name': 'activation_172',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_172',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_172', 0, 0, {}]]]},\n",
       "  {'name': 'activation_175',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_175',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_175', 0, 0, {}]]]},\n",
       "  {'name': 'block8_3_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block8_3_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_172', 0, 0, {}],\n",
       "     ['activation_175', 0, 0, {}]]]},\n",
       "  {'name': 'block8_3_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block8_3_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 2080,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_3_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block8_3',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block8_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 2080),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.2}},\n",
       "   'inbound_nodes': [[['block8_2_ac', 0, 0, {}],\n",
       "     ['block8_3_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block8_3_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block8_3_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block8_3', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_177',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_177',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_3_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_177',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_177',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_177', 0, 0, {}]]]},\n",
       "  {'name': 'activation_177',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_177',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_177', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_178',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_178',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 224,\n",
       "    'kernel_size': (1, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_177', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_178',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_178',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_178', 0, 0, {}]]]},\n",
       "  {'name': 'activation_178',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_178',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_178', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_176',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_176',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_3_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_179',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_179',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (3, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_178', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_176',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_176',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_176', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_179',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_179',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_179', 0, 0, {}]]]},\n",
       "  {'name': 'activation_176',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_176',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_176', 0, 0, {}]]]},\n",
       "  {'name': 'activation_179',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_179',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_179', 0, 0, {}]]]},\n",
       "  {'name': 'block8_4_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block8_4_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_176', 0, 0, {}],\n",
       "     ['activation_179', 0, 0, {}]]]},\n",
       "  {'name': 'block8_4_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block8_4_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 2080,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_4_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block8_4',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block8_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 2080),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.2}},\n",
       "   'inbound_nodes': [[['block8_3_ac', 0, 0, {}],\n",
       "     ['block8_4_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block8_4_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block8_4_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block8_4', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_181',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_181',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_4_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_181',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_181',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_181', 0, 0, {}]]]},\n",
       "  {'name': 'activation_181',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_181',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_181', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_182',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_182',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 224,\n",
       "    'kernel_size': (1, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_181', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_182',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_182',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_182', 0, 0, {}]]]},\n",
       "  {'name': 'activation_182',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_182',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_182', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_180',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_180',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_4_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_183',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_183',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (3, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_182', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_180',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_180',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_180', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_183',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_183',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_183', 0, 0, {}]]]},\n",
       "  {'name': 'activation_180',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_180',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_180', 0, 0, {}]]]},\n",
       "  {'name': 'activation_183',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_183',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_183', 0, 0, {}]]]},\n",
       "  {'name': 'block8_5_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block8_5_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_180', 0, 0, {}],\n",
       "     ['activation_183', 0, 0, {}]]]},\n",
       "  {'name': 'block8_5_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block8_5_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 2080,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_5_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block8_5',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block8_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 2080),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.2}},\n",
       "   'inbound_nodes': [[['block8_4_ac', 0, 0, {}],\n",
       "     ['block8_5_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block8_5_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block8_5_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block8_5', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_185',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_185',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_5_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_185',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_185',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_185', 0, 0, {}]]]},\n",
       "  {'name': 'activation_185',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_185',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_185', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_186',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_186',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 224,\n",
       "    'kernel_size': (1, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_185', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_186',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_186',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_186', 0, 0, {}]]]},\n",
       "  {'name': 'activation_186',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_186',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_186', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_184',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_184',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_5_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_187',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_187',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (3, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_186', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_184',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_184',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_184', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_187',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_187',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_187', 0, 0, {}]]]},\n",
       "  {'name': 'activation_184',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_184',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_184', 0, 0, {}]]]},\n",
       "  {'name': 'activation_187',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_187',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_187', 0, 0, {}]]]},\n",
       "  {'name': 'block8_6_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block8_6_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_184', 0, 0, {}],\n",
       "     ['activation_187', 0, 0, {}]]]},\n",
       "  {'name': 'block8_6_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block8_6_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 2080,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_6_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block8_6',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block8_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 2080),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.2}},\n",
       "   'inbound_nodes': [[['block8_5_ac', 0, 0, {}],\n",
       "     ['block8_6_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block8_6_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block8_6_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block8_6', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_189',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_189',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_6_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_189',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_189',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_189', 0, 0, {}]]]},\n",
       "  {'name': 'activation_189',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_189',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_189', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_190',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_190',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 224,\n",
       "    'kernel_size': (1, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_189', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_190',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_190',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_190', 0, 0, {}]]]},\n",
       "  {'name': 'activation_190',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_190',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_190', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_188',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_188',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_6_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_191',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_191',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (3, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_190', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_188',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_188',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_188', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_191',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_191',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_191', 0, 0, {}]]]},\n",
       "  {'name': 'activation_188',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_188',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_188', 0, 0, {}]]]},\n",
       "  {'name': 'activation_191',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_191',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_191', 0, 0, {}]]]},\n",
       "  {'name': 'block8_7_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block8_7_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_188', 0, 0, {}],\n",
       "     ['activation_191', 0, 0, {}]]]},\n",
       "  {'name': 'block8_7_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block8_7_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 2080,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_7_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block8_7',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block8_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 2080),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.2}},\n",
       "   'inbound_nodes': [[['block8_6_ac', 0, 0, {}],\n",
       "     ['block8_7_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block8_7_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block8_7_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block8_7', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_193',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_193',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_7_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_193',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_193',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_193', 0, 0, {}]]]},\n",
       "  {'name': 'activation_193',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_193',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_193', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_194',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_194',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 224,\n",
       "    'kernel_size': (1, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_193', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_194',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_194',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_194', 0, 0, {}]]]},\n",
       "  {'name': 'activation_194',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_194',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_194', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_192',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_192',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_7_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_195',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_195',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (3, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_194', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_192',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_192',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_192', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_195',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_195',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_195', 0, 0, {}]]]},\n",
       "  {'name': 'activation_192',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_192',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_192', 0, 0, {}]]]},\n",
       "  {'name': 'activation_195',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_195',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_195', 0, 0, {}]]]},\n",
       "  {'name': 'block8_8_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block8_8_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_192', 0, 0, {}],\n",
       "     ['activation_195', 0, 0, {}]]]},\n",
       "  {'name': 'block8_8_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block8_8_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 2080,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_8_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block8_8',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block8_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 2080),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.2}},\n",
       "   'inbound_nodes': [[['block8_7_ac', 0, 0, {}],\n",
       "     ['block8_8_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block8_8_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block8_8_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block8_8', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_197',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_197',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_8_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_197',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_197',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_197', 0, 0, {}]]]},\n",
       "  {'name': 'activation_197',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_197',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_197', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_198',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_198',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 224,\n",
       "    'kernel_size': (1, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_197', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_198',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_198',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_198', 0, 0, {}]]]},\n",
       "  {'name': 'activation_198',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_198',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_198', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_196',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_196',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_8_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_199',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_199',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (3, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_198', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_196',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_196',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_196', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_199',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_199',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_199', 0, 0, {}]]]},\n",
       "  {'name': 'activation_196',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_196',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_196', 0, 0, {}]]]},\n",
       "  {'name': 'activation_199',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_199',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_199', 0, 0, {}]]]},\n",
       "  {'name': 'block8_9_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block8_9_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_196', 0, 0, {}],\n",
       "     ['activation_199', 0, 0, {}]]]},\n",
       "  {'name': 'block8_9_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block8_9_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 2080,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_9_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block8_9',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block8_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 2080),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 0.2}},\n",
       "   'inbound_nodes': [[['block8_8_ac', 0, 0, {}],\n",
       "     ['block8_9_conv', 0, 0, {}]]]},\n",
       "  {'name': 'block8_9_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'block8_9_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['block8_9', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_201',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_201',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_9_ac', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_201',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_201',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_201', 0, 0, {}]]]},\n",
       "  {'name': 'activation_201',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_201',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_201', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_202',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_202',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 224,\n",
       "    'kernel_size': (1, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_201', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_202',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_202',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_202', 0, 0, {}]]]},\n",
       "  {'name': 'activation_202',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_202',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_202', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_200',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_200',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_9_ac', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_203',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_203',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 256,\n",
       "    'kernel_size': (3, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_202', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_200',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_200',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_200', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_203',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_203',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_203', 0, 0, {}]]]},\n",
       "  {'name': 'activation_200',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_200',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_200', 0, 0, {}]]]},\n",
       "  {'name': 'activation_203',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_203',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_203', 0, 0, {}]]]},\n",
       "  {'name': 'block8_10_mixed',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'block8_10_mixed',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_200', 0, 0, {}],\n",
       "     ['activation_203', 0, 0, {}]]]},\n",
       "  {'name': 'block8_10_conv',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'block8_10_conv',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 2080,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_10_mixed', 0, 0, {}]]]},\n",
       "  {'name': 'block8_10',\n",
       "   'class_name': 'Lambda',\n",
       "   'config': {'name': 'block8_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQCGQB8ARQAFwBTACkDTukAAAAA6QEA\\nAACpACkC2gZpbnB1dHPaBXNjYWxlcgMAAAByAwAAAPpZQzpcVXNlcnNcSEggVFJBREVSU1xBbmFj\\nb25kYTNcbGliXHNpdGUtcGFja2FnZXNca2VyYXNfYXBwbGljYXRpb25zXGluY2VwdGlvbl9yZXNu\\nZXRfdjIucHnaCDxsYW1iZGE+qAAAAPMAAAAA\\n',\n",
       "     None,\n",
       "     None),\n",
       "    'function_type': 'lambda',\n",
       "    'output_shape': (None, None, 2080),\n",
       "    'output_shape_type': 'raw',\n",
       "    'arguments': {'scale': 1.0}},\n",
       "   'inbound_nodes': [[['block8_9_ac', 0, 0, {}],\n",
       "     ['block8_10_conv', 0, 0, {}]]]},\n",
       "  {'name': 'conv_7b',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv_7b',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 1536,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['block8_10', 0, 0, {}]]]},\n",
       "  {'name': 'conv_7b_bn',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'conv_7b_bn',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv_7b', 0, 0, {}]]]},\n",
       "  {'name': 'conv_7b_ac',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'conv_7b_ac',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['conv_7b_bn', 0, 0, {}]]]}],\n",
       " 'input_layers': [['input_1', 0, 0]],\n",
       " 'output_layers': [['conv_7b_ac', 0, 0]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-02 01:41:43.841025\n",
      "(42378, 5, 5, 1536)\n",
      "6:33:12.384639\n"
     ]
    }
   ],
   "source": [
    "t1=datetime.datetime.now()\n",
    "print(t1)\n",
    "# extracting features for training frames\n",
    "X_train = base_model.predict(X_train)\n",
    "print(X_train.shape)\n",
    "t2=datetime.datetime.now()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the training as well as validation frames in single dimension\n",
    "X_train = X_train.reshape(42378, 5*5*1536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open('../Pickle/InceptionResNetV2_X_train_OF.pickle',\"wb\")\n",
    "pickle.dump(X_train, pickle_out, protocol=4)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42378, 38400)\n"
     ]
    }
   ],
   "source": [
    "pickle_in = open('../Pickle/InceptionResNetV2_X_train_OF.pickle',\"rb\")\n",
    "X_train = pickle.load(pickle_in)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8803, 5, 5, 1536)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting features for validation frames\n",
    "X_test = base_model.predict(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(8803, 5*5*1536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Pickle/InceptionResNetV2_X_test.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model as a pickle in a file \n",
    "joblib.dump(X_test, '../Pickle/InceptionResNetV2_X_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file \n",
    "X_test = joblib.load('../Pickle/InceptionResNetV2_X_test.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42378, 38400)\n",
      "(8803, 38400)\n",
      "(42378, 51)\n",
      "(8803, 51)\n"
     ]
    }
   ],
   "source": [
    "# shape of images\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(38400,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(51, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 51)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              39322624  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 51)                6579      \n",
      "=================================================================\n",
      "Total params: 40,018,227\n",
      "Trainable params: 40,018,227\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential_1',\n",
       " 'layers': [{'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_1',\n",
       "    'trainable': True,\n",
       "    'batch_input_shape': (None, 38400),\n",
       "    'dtype': 'float32',\n",
       "    'units': 1024,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 512,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 256,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 128,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 51,\n",
       "    'activation': 'softmax',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}}]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_save = ModelCheckpoint('../Models/weightInceptionResNetV2_OF.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42378 samples, validate on 8803 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23552/42378 [===============>..............] - ETA: 11:16 - loss: 5.5890 - accuracy: 0.039 - ETA: 6:52 - loss: 13.4196 - accuracy: 0.035 - ETA: 5:23 - loss: 14.5697 - accuracy: 0.028 - ETA: 4:37 - loss: 14.5639 - accuracy: 0.031 - ETA: 4:09 - loss: 14.2969 - accuracy: 0.028 - ETA: 3:51 - loss: 13.9451 - accuracy: 0.024 - ETA: 3:37 - loss: 13.5136 - accuracy: 0.023 - ETA: 3:27 - loss: 13.0587 - accuracy: 0.023 - ETA: 3:20 - loss: 12.5830 - accuracy: 0.021 - ETA: 3:14 - loss: 12.0829 - accuracy: 0.019 - ETA: 3:08 - loss: 11.6361 - accuracy: 0.020 - ETA: 3:03 - loss: 11.2228 - accuracy: 0.020 - ETA: 3:00 - loss: 10.8696 - accuracy: 0.020 - ETA: 2:56 - loss: 10.5383 - accuracy: 0.020 - ETA: 2:53 - loss: 10.1827 - accuracy: 0.019 - ETA: 2:50 - loss: 9.8701 - accuracy: 0.020 - ETA: 2:48 - loss: 9.5672 - accuracy: 0.02 - ETA: 2:46 - loss: 9.2981 - accuracy: 0.02 - ETA: 2:44 - loss: 9.0545 - accuracy: 0.02 - ETA: 2:42 - loss: 8.8212 - accuracy: 0.02 - ETA: 2:40 - loss: 8.6105 - accuracy: 0.02 - ETA: 2:39 - loss: 8.4235 - accuracy: 0.02 - ETA: 2:37 - loss: 8.2420 - accuracy: 0.02 - ETA: 2:36 - loss: 8.0740 - accuracy: 0.02 - ETA: 2:34 - loss: 7.9214 - accuracy: 0.02 - ETA: 2:33 - loss: 7.7786 - accuracy: 0.02 - ETA: 2:32 - loss: 7.6438 - accuracy: 0.02 - ETA: 2:31 - loss: 7.5245 - accuracy: 0.02 - ETA: 2:29 - loss: 7.4057 - accuracy: 0.02 - ETA: 2:28 - loss: 7.2958 - accuracy: 0.02 - ETA: 2:28 - loss: 7.1912 - accuracy: 0.02 - ETA: 2:27 - loss: 7.0924 - accuracy: 0.02 - ETA: 2:26 - loss: 7.0001 - accuracy: 0.02 - ETA: 2:26 - loss: 6.9147 - accuracy: 0.02 - ETA: 2:25 - loss: 6.8333 - accuracy: 0.02 - ETA: 2:24 - loss: 6.7540 - accuracy: 0.02 - ETA: 2:24 - loss: 6.6810 - accuracy: 0.02 - ETA: 2:23 - loss: 6.6126 - accuracy: 0.02 - ETA: 2:22 - loss: 6.5443 - accuracy: 0.02 - ETA: 2:21 - loss: 6.4812 - accuracy: 0.02 - ETA: 2:20 - loss: 6.4196 - accuracy: 0.02 - ETA: 2:20 - loss: 6.3618 - accuracy: 0.02 - ETA: 2:19 - loss: 6.3057 - accuracy: 0.02 - ETA: 2:18 - loss: 6.2520 - accuracy: 0.02 - ETA: 2:17 - loss: 6.1997 - accuracy: 0.02 - ETA: 2:17 - loss: 6.1512 - accuracy: 0.02 - ETA: 2:16 - loss: 6.1046 - accuracy: 0.02 - ETA: 2:16 - loss: 6.0604 - accuracy: 0.02 - ETA: 2:15 - loss: 6.0180 - accuracy: 0.02 - ETA: 2:14 - loss: 5.9762 - accuracy: 0.02 - ETA: 2:14 - loss: 5.9359 - accuracy: 0.02 - ETA: 2:13 - loss: 5.8993 - accuracy: 0.02 - ETA: 2:13 - loss: 5.8618 - accuracy: 0.02 - ETA: 2:12 - loss: 5.8257 - accuracy: 0.02 - ETA: 2:12 - loss: 5.7905 - accuracy: 0.02 - ETA: 2:11 - loss: 5.7570 - accuracy: 0.02 - ETA: 2:10 - loss: 5.7260 - accuracy: 0.02 - ETA: 2:10 - loss: 5.6945 - accuracy: 0.02 - ETA: 2:09 - loss: 5.6658 - accuracy: 0.02 - ETA: 2:09 - loss: 5.6379 - accuracy: 0.02 - ETA: 2:08 - loss: 5.6102 - accuracy: 0.02 - ETA: 2:08 - loss: 5.5833 - accuracy: 0.02 - ETA: 2:07 - loss: 5.5566 - accuracy: 0.02 - ETA: 2:07 - loss: 5.5308 - accuracy: 0.02 - ETA: 2:06 - loss: 5.5066 - accuracy: 0.02 - ETA: 2:06 - loss: 5.4822 - accuracy: 0.02 - ETA: 2:06 - loss: 5.4592 - accuracy: 0.02 - ETA: 2:05 - loss: 5.4366 - accuracy: 0.02 - ETA: 2:05 - loss: 5.4143 - accuracy: 0.02 - ETA: 2:04 - loss: 5.3932 - accuracy: 0.02 - ETA: 2:04 - loss: 5.3734 - accuracy: 0.02 - ETA: 2:03 - loss: 5.3536 - accuracy: 0.02 - ETA: 2:03 - loss: 5.3337 - accuracy: 0.02 - ETA: 2:02 - loss: 5.3148 - accuracy: 0.02 - ETA: 2:02 - loss: 5.2960 - accuracy: 0.02 - ETA: 2:01 - loss: 5.2785 - accuracy: 0.02 - ETA: 2:01 - loss: 5.2619 - accuracy: 0.02 - ETA: 2:00 - loss: 5.2446 - accuracy: 0.02 - ETA: 2:00 - loss: 5.2282 - accuracy: 0.02 - ETA: 1:59 - loss: 5.2121 - accuracy: 0.02 - ETA: 1:59 - loss: 5.1967 - accuracy: 0.02 - ETA: 1:58 - loss: 5.1817 - accuracy: 0.02 - ETA: 1:58 - loss: 5.1662 - accuracy: 0.02 - ETA: 1:57 - loss: 5.1512 - accuracy: 0.02 - ETA: 1:57 - loss: 5.1369 - accuracy: 0.02 - ETA: 1:56 - loss: 5.1234 - accuracy: 0.02 - ETA: 1:56 - loss: 5.1100 - accuracy: 0.02 - ETA: 1:55 - loss: 5.0960 - accuracy: 0.02 - ETA: 1:55 - loss: 5.0828 - accuracy: 0.02 - ETA: 1:54 - loss: 5.0697 - accuracy: 0.02 - ETA: 1:53 - loss: 5.0568 - accuracy: 0.02 - ETA: 1:53 - loss: 5.0443 - accuracy: 0.02 - ETA: 1:52 - loss: 5.0318 - accuracy: 0.03 - ETA: 1:52 - loss: 5.0200 - accuracy: 0.03 - ETA: 1:51 - loss: 5.0084 - accuracy: 0.03 - ETA: 1:51 - loss: 4.9972 - accuracy: 0.03 - ETA: 1:50 - loss: 4.9863 - accuracy: 0.03 - ETA: 1:50 - loss: 4.9752 - accuracy: 0.03 - ETA: 1:49 - loss: 4.9648 - accuracy: 0.03 - ETA: 1:49 - loss: 4.9541 - accuracy: 0.03 - ETA: 1:48 - loss: 4.9438 - accuracy: 0.03 - ETA: 1:48 - loss: 4.9345 - accuracy: 0.03 - ETA: 1:47 - loss: 4.9246 - accuracy: 0.03 - ETA: 1:47 - loss: 4.9155 - accuracy: 0.03 - ETA: 1:46 - loss: 4.9062 - accuracy: 0.03 - ETA: 1:46 - loss: 4.8968 - accuracy: 0.03 - ETA: 1:45 - loss: 4.8875 - accuracy: 0.03 - ETA: 1:45 - loss: 4.8787 - accuracy: 0.03 - ETA: 1:44 - loss: 4.8698 - accuracy: 0.03 - ETA: 1:44 - loss: 4.8608 - accuracy: 0.03 - ETA: 1:43 - loss: 4.8525 - accuracy: 0.03 - ETA: 1:43 - loss: 4.8445 - accuracy: 0.03 - ETA: 1:42 - loss: 4.8363 - accuracy: 0.03 - ETA: 1:42 - loss: 4.8281 - accuracy: 0.03 - ETA: 1:41 - loss: 4.8201 - accuracy: 0.03 - ETA: 1:41 - loss: 4.8121 - accuracy: 0.03 - ETA: 1:40 - loss: 4.8045 - accuracy: 0.03 - ETA: 1:40 - loss: 4.7966 - accuracy: 0.03 - ETA: 1:39 - loss: 4.7891 - accuracy: 0.03 - ETA: 1:39 - loss: 4.7819 - accuracy: 0.03 - ETA: 1:38 - loss: 4.7749 - accuracy: 0.03 - ETA: 1:38 - loss: 4.7678 - accuracy: 0.03 - ETA: 1:37 - loss: 4.7610 - accuracy: 0.03 - ETA: 1:37 - loss: 4.7541 - accuracy: 0.03 - ETA: 1:36 - loss: 4.7472 - accuracy: 0.03 - ETA: 1:36 - loss: 4.7404 - accuracy: 0.03 - ETA: 1:35 - loss: 4.7340 - accuracy: 0.03 - ETA: 1:35 - loss: 4.7275 - accuracy: 0.03 - ETA: 1:34 - loss: 4.7215 - accuracy: 0.03 - ETA: 1:34 - loss: 4.7151 - accuracy: 0.03 - ETA: 1:33 - loss: 4.7087 - accuracy: 0.03 - ETA: 1:33 - loss: 4.7026 - accuracy: 0.03 - ETA: 1:32 - loss: 4.6969 - accuracy: 0.03 - ETA: 1:32 - loss: 4.6913 - accuracy: 0.03 - ETA: 1:31 - loss: 4.6858 - accuracy: 0.03 - ETA: 1:31 - loss: 4.6801 - accuracy: 0.03 - ETA: 1:30 - loss: 4.6745 - accuracy: 0.03 - ETA: 1:30 - loss: 4.6691 - accuracy: 0.03 - ETA: 1:29 - loss: 4.6634 - accuracy: 0.03 - ETA: 1:29 - loss: 4.6578 - accuracy: 0.03 - ETA: 1:28 - loss: 4.6522 - accuracy: 0.03 - ETA: 1:28 - loss: 4.6467 - accuracy: 0.03 - ETA: 1:27 - loss: 4.6417 - accuracy: 0.03 - ETA: 1:27 - loss: 4.6369 - accuracy: 0.03 - ETA: 1:26 - loss: 4.6322 - accuracy: 0.03 - ETA: 1:26 - loss: 4.6274 - accuracy: 0.03 - ETA: 1:25 - loss: 4.6224 - accuracy: 0.03 - ETA: 1:25 - loss: 4.6175 - accuracy: 0.03 - ETA: 1:24 - loss: 4.6133 - accuracy: 0.03 - ETA: 1:24 - loss: 4.6086 - accuracy: 0.03 - ETA: 1:24 - loss: 4.6038 - accuracy: 0.03 - ETA: 1:23 - loss: 4.5990 - accuracy: 0.03 - ETA: 1:23 - loss: 4.5946 - accuracy: 0.03 - ETA: 1:22 - loss: 4.5898 - accuracy: 0.03 - ETA: 1:22 - loss: 4.5852 - accuracy: 0.03 - ETA: 1:21 - loss: 4.5807 - accuracy: 0.03 - ETA: 1:21 - loss: 4.5760 - accuracy: 0.03 - ETA: 1:20 - loss: 4.5716 - accuracy: 0.03 - ETA: 1:20 - loss: 4.5674 - accuracy: 0.03 - ETA: 1:19 - loss: 4.5633 - accuracy: 0.03 - ETA: 1:19 - loss: 4.5588 - accuracy: 0.03 - ETA: 1:18 - loss: 4.5548 - accuracy: 0.03 - ETA: 1:18 - loss: 4.5510 - accuracy: 0.03 - ETA: 1:17 - loss: 4.5470 - accuracy: 0.03 - ETA: 1:17 - loss: 4.5428 - accuracy: 0.03 - ETA: 1:16 - loss: 4.5389 - accuracy: 0.03 - ETA: 1:16 - loss: 4.5349 - accuracy: 0.03 - ETA: 1:15 - loss: 4.5312 - accuracy: 0.03 - ETA: 1:15 - loss: 4.5276 - accuracy: 0.03 - ETA: 1:14 - loss: 4.5237 - accuracy: 0.03 - ETA: 1:14 - loss: 4.5201 - accuracy: 0.03 - ETA: 1:13 - loss: 4.5165 - accuracy: 0.03 - ETA: 1:13 - loss: 4.5127 - accuracy: 0.03 - ETA: 1:13 - loss: 4.5090 - accuracy: 0.03 - ETA: 1:12 - loss: 4.5058 - accuracy: 0.03 - ETA: 1:12 - loss: 4.5023 - accuracy: 0.03 - ETA: 1:11 - loss: 4.4988 - accuracy: 0.03 - ETA: 1:11 - loss: 4.4954 - accuracy: 0.03 - ETA: 1:10 - loss: 4.4918 - accuracy: 0.03 - ETA: 1:10 - loss: 4.4887 - accuracy: 0.03 - ETA: 1:09 - loss: 4.4854 - accuracy: 0.03 - ETA: 1:09 - loss: 4.4819 - accuracy: 0.03 - ETA: 1:08 - loss: 4.4784 - accuracy: 0.03 - ETA: 1:08 - loss: 4.4753 - accuracy: 0.0353\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:08 - loss: 4.4720 - accuracy: 0.03 - ETA: 1:07 - loss: 4.4689 - accuracy: 0.03 - ETA: 1:07 - loss: 4.4662 - accuracy: 0.03 - ETA: 1:06 - loss: 4.4633 - accuracy: 0.03 - ETA: 1:06 - loss: 4.4605 - accuracy: 0.03 - ETA: 1:05 - loss: 4.4579 - accuracy: 0.03 - ETA: 1:05 - loss: 4.4550 - accuracy: 0.03 - ETA: 1:04 - loss: 4.4516 - accuracy: 0.03 - ETA: 1:04 - loss: 4.4487 - accuracy: 0.03 - ETA: 1:03 - loss: 4.4457 - accuracy: 0.03 - ETA: 1:03 - loss: 4.4428 - accuracy: 0.03 - ETA: 1:02 - loss: 4.4400 - accuracy: 0.03 - ETA: 1:02 - loss: 4.4371 - accuracy: 0.03 - ETA: 1:01 - loss: 4.4343 - accuracy: 0.03 - ETA: 1:01 - loss: 4.4315 - accuracy: 0.03 - ETA: 1:00 - loss: 4.4286 - accuracy: 0.03 - ETA: 1:00 - loss: 4.4257 - accuracy: 0.03 - ETA: 59s - loss: 4.4228 - accuracy: 0.0361 - ETA: 59s - loss: 4.4202 - accuracy: 0.036 - ETA: 59s - loss: 4.4176 - accuracy: 0.036 - ETA: 58s - loss: 4.4147 - accuracy: 0.036 - ETA: 58s - loss: 4.4120 - accuracy: 0.036 - ETA: 57s - loss: 4.4094 - accuracy: 0.036 - ETA: 57s - loss: 4.4067 - accuracy: 0.036 - ETA: 56s - loss: 4.4043 - accuracy: 0.036 - ETA: 56s - loss: 4.4019 - accuracy: 0.036 - ETA: 55s - loss: 4.3995 - accuracy: 0.036 - ETA: 55s - loss: 4.3969 - accuracy: 0.036 - ETA: 54s - loss: 4.3943 - accuracy: 0.036 - ETA: 54s - loss: 4.3916 - accuracy: 0.036 - ETA: 53s - loss: 4.3890 - accuracy: 0.036 - ETA: 53s - loss: 4.3867 - accuracy: 0.036 - ETA: 52s - loss: 4.3842 - accuracy: 0.036 - ETA: 52s - loss: 4.3820 - accuracy: 0.036 - ETA: 52s - loss: 4.3798 - accuracy: 0.036 - ETA: 51s - loss: 4.3777 - accuracy: 0.036 - ETA: 51s - loss: 4.3756 - accuracy: 0.036 - ETA: 50s - loss: 4.3733 - accuracy: 0.036 - ETA: 50s - loss: 4.3712 - accuracy: 0.036 - ETA: 49s - loss: 4.3690 - accuracy: 0.036 - ETA: 49s - loss: 4.3668 - accuracy: 0.036 - ETA: 48s - loss: 4.3644 - accuracy: 0.037 - ETA: 48s - loss: 4.3623 - accuracy: 0.037 - ETA: 47s - loss: 4.3600 - accuracy: 0.037 - ETA: 47s - loss: 4.3579 - accuracy: 0.037 - ETA: 46s - loss: 4.3558 - accuracy: 0.037 - ETA: 46s - loss: 4.3538 - accuracy: 0.037 - ETA: 45s - loss: 4.3516 - accuracy: 0.037 - ETA: 45s - loss: 4.3491 - accuracy: 0.037 - ETA: 44s - loss: 4.3468 - accuracy: 0.037 - ETA: 44s - loss: 4.3448 - accuracy: 0.037 - ETA: 44s - loss: 4.3429 - accuracy: 0.037 - ETA: 43s - loss: 4.3403 - accuracy: 0.037 - ETA: 43s - loss: 4.3383 - accuracy: 0.037 - ETA: 42s - loss: 4.3364 - accuracy: 0.037 - ETA: 42s - loss: 4.3342 - accuracy: 0.038 - ETA: 41s - loss: 4.3322 - accuracy: 0.038 - ETA: 41s - loss: 4.3305 - accuracy: 0.038 - ETA: 40s - loss: 4.3285 - accuracy: 0.038 - ETA: 40s - loss: 4.3266 - accuracy: 0.038 - ETA: 39s - loss: 4.3245 - accuracy: 0.038 - ETA: 39s - loss: 4.3224 - accuracy: 0.038 - ETA: 38s - loss: 4.3204 - accuracy: 0.038 - ETA: 38s - loss: 4.3187 - accuracy: 0.038 - ETA: 38s - loss: 4.3168 - accuracy: 0.038 - ETA: 37s - loss: 4.3150 - accuracy: 0.038 - ETA: 37s - loss: 4.3132 - accuracy: 0.038 - ETA: 36s - loss: 4.3113 - accuracy: 0.039 - ETA: 36s - loss: 4.3095 - accuracy: 0.039 - ETA: 35s - loss: 4.3077 - accuracy: 0.039 - ETA: 35s - loss: 4.3062 - accuracy: 0.039 - ETA: 34s - loss: 4.3043 - accuracy: 0.039 - ETA: 34s - loss: 4.3026 - accuracy: 0.039 - ETA: 33s - loss: 4.3009 - accuracy: 0.038 - ETA: 33s - loss: 4.2992 - accuracy: 0.039 - ETA: 32s - loss: 4.2974 - accuracy: 0.039 - ETA: 32s - loss: 4.2958 - accuracy: 0.039 - ETA: 31s - loss: 4.2941 - accuracy: 0.039 - ETA: 31s - loss: 4.2922 - accuracy: 0.039 - ETA: 31s - loss: 4.2904 - accuracy: 0.039 - ETA: 30s - loss: 4.2886 - accuracy: 0.039 - ETA: 30s - loss: 4.2870 - accuracy: 0.039 - ETA: 29s - loss: 4.2853 - accuracy: 0.039 - ETA: 29s - loss: 4.2834 - accuracy: 0.039 - ETA: 28s - loss: 4.2821 - accuracy: 0.039 - ETA: 28s - loss: 4.2807 - accuracy: 0.039 - ETA: 27s - loss: 4.2792 - accuracy: 0.039 - ETA: 27s - loss: 4.2774 - accuracy: 0.039 - ETA: 26s - loss: 4.2758 - accuracy: 0.039 - ETA: 26s - loss: 4.2740 - accuracy: 0.039 - ETA: 25s - loss: 4.2725 - accuracy: 0.039 - ETA: 25s - loss: 4.2711 - accuracy: 0.039 - ETA: 24s - loss: 4.2695 - accuracy: 0.039 - ETA: 24s - loss: 4.2680 - accuracy: 0.040 - ETA: 24s - loss: 4.2664 - accuracy: 0.040 - ETA: 23s - loss: 4.2647 - accuracy: 0.040 - ETA: 23s - loss: 4.2631 - accuracy: 0.040 - ETA: 22s - loss: 4.2616 - accuracy: 0.040 - ETA: 22s - loss: 4.2601 - accuracy: 0.040 - ETA: 21s - loss: 4.2588 - accuracy: 0.040 - ETA: 21s - loss: 4.2574 - accuracy: 0.040 - ETA: 20s - loss: 4.2560 - accuracy: 0.040 - ETA: 20s - loss: 4.2546 - accuracy: 0.040 - ETA: 19s - loss: 4.2531 - accuracy: 0.040 - ETA: 19s - loss: 4.2518 - accuracy: 0.040 - ETA: 18s - loss: 4.2504 - accuracy: 0.040 - ETA: 18s - loss: 4.2491 - accuracy: 0.040 - ETA: 18s - loss: 4.2478 - accuracy: 0.040 - ETA: 17s - loss: 4.2463 - accuracy: 0.040 - ETA: 17s - loss: 4.2448 - accuracy: 0.040 - ETA: 16s - loss: 4.2436 - accuracy: 0.040 - ETA: 16s - loss: 4.2423 - accuracy: 0.040 - ETA: 15s - loss: 4.2407 - accuracy: 0.040 - ETA: 15s - loss: 4.2394 - accuracy: 0.040 - ETA: 14s - loss: 4.2383 - accuracy: 0.040 - ETA: 14s - loss: 4.2369 - accuracy: 0.040 - ETA: 13s - loss: 4.2357 - accuracy: 0.040 - ETA: 13s - loss: 4.2341 - accuracy: 0.041 - ETA: 12s - loss: 4.2330 - accuracy: 0.041 - ETA: 12s - loss: 4.2317 - accuracy: 0.041 - ETA: 12s - loss: 4.2303 - accuracy: 0.041 - ETA: 11s - loss: 4.2290 - accuracy: 0.041 - ETA: 11s - loss: 4.2279 - accuracy: 0.041 - ETA: 10s - loss: 4.2266 - accuracy: 0.041 - ETA: 10s - loss: 4.2250 - accuracy: 0.041 - ETA: 9s - loss: 4.2239 - accuracy: 0.041 - ETA: 9s - loss: 4.2226 - accuracy: 0.04 - ETA: 8s - loss: 4.2215 - accuracy: 0.04 - ETA: 8s - loss: 4.2201 - accuracy: 0.04 - ETA: 7s - loss: 4.2185 - accuracy: 0.04 - ETA: 7s - loss: 4.2175 - accuracy: 0.04 - ETA: 6s - loss: 4.2161 - accuracy: 0.04 - ETA: 6s - loss: 4.2150 - accuracy: 0.04 - ETA: 6s - loss: 4.2136 - accuracy: 0.04 - ETA: 5s - loss: 4.2124 - accuracy: 0.04 - ETA: 5s - loss: 4.2111 - accuracy: 0.04 - ETA: 4s - loss: 4.2098 - accuracy: 0.04 - ETA: 4s - loss: 4.2084 - accuracy: 0.04 - ETA: 3s - loss: 4.2068 - accuracy: 0.04 - ETA: 3s - loss: 4.2058 - accuracy: 0.04 - ETA: 2s - loss: 4.2047 - accuracy: 0.04 - ETA: 2s - loss: 4.2032 - accuracy: 0.04 - ETA: 1s - loss: 4.2019 - accuracy: 0.04 - ETA: 1s - loss: 4.2011 - accuracy: 0.04 - ETA: 0s - loss: 4.2001 - accuracy: 0.04 - ETA: 0s - loss: 4.1990 - accuracy: 0.04 - ETA: 0s - loss: 4.1979 - accuracy: 0.04 - 163s 4ms/step - loss: 4.1978 - accuracy: 0.0419 - val_loss: 3.9301 - val_accuracy: 0.0162\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:33 - loss: 3.7913 - accuracy: 0.06 - ETA: 2:31 - loss: 3.7979 - accuracy: 0.05 - ETA: 2:28 - loss: 3.7619 - accuracy: 0.07 - ETA: 2:26 - loss: 3.7935 - accuracy: 0.06 - ETA: 2:25 - loss: 3.7743 - accuracy: 0.07 - ETA: 2:24 - loss: 3.7770 - accuracy: 0.06 - ETA: 2:23 - loss: 3.7859 - accuracy: 0.06 - ETA: 2:22 - loss: 3.7928 - accuracy: 0.06 - ETA: 2:21 - loss: 3.7806 - accuracy: 0.07 - ETA: 2:20 - loss: 3.7843 - accuracy: 0.06 - ETA: 2:20 - loss: 3.7905 - accuracy: 0.07 - ETA: 2:20 - loss: 3.7908 - accuracy: 0.07 - ETA: 2:19 - loss: 3.7936 - accuracy: 0.06 - ETA: 2:19 - loss: 3.7991 - accuracy: 0.06 - ETA: 2:18 - loss: 3.7987 - accuracy: 0.06 - ETA: 2:18 - loss: 3.8023 - accuracy: 0.06 - ETA: 2:18 - loss: 3.8068 - accuracy: 0.06 - ETA: 2:17 - loss: 3.8113 - accuracy: 0.06 - ETA: 2:17 - loss: 3.8115 - accuracy: 0.06 - ETA: 2:17 - loss: 3.8139 - accuracy: 0.06 - ETA: 2:17 - loss: 3.8097 - accuracy: 0.06 - ETA: 2:16 - loss: 3.8094 - accuracy: 0.06 - ETA: 2:16 - loss: 3.8096 - accuracy: 0.06 - ETA: 2:16 - loss: 3.8101 - accuracy: 0.05 - ETA: 2:16 - loss: 3.8092 - accuracy: 0.05 - ETA: 2:16 - loss: 3.8067 - accuracy: 0.05 - ETA: 2:16 - loss: 3.8085 - accuracy: 0.05 - ETA: 2:16 - loss: 3.8089 - accuracy: 0.05 - ETA: 2:16 - loss: 3.8069 - accuracy: 0.05 - ETA: 2:15 - loss: 3.8061 - accuracy: 0.05 - ETA: 2:15 - loss: 3.8091 - accuracy: 0.05 - ETA: 2:14 - loss: 3.8082 - accuracy: 0.06 - ETA: 2:14 - loss: 3.8080 - accuracy: 0.06 - ETA: 2:13 - loss: 3.8049 - accuracy: 0.06 - ETA: 2:13 - loss: 3.8044 - accuracy: 0.06 - ETA: 2:13 - loss: 3.8012 - accuracy: 0.06 - ETA: 2:12 - loss: 3.8019 - accuracy: 0.06 - ETA: 2:12 - loss: 3.8037 - accuracy: 0.06 - ETA: 2:11 - loss: 3.8041 - accuracy: 0.06 - ETA: 2:11 - loss: 3.8035 - accuracy: 0.06 - ETA: 2:10 - loss: 3.8017 - accuracy: 0.06 - ETA: 2:10 - loss: 3.8020 - accuracy: 0.06 - ETA: 2:09 - loss: 3.8025 - accuracy: 0.06 - ETA: 2:09 - loss: 3.8029 - accuracy: 0.06 - ETA: 2:09 - loss: 3.8036 - accuracy: 0.05 - ETA: 2:08 - loss: 3.8039 - accuracy: 0.05 - ETA: 2:08 - loss: 3.8024 - accuracy: 0.05 - ETA: 2:07 - loss: 3.8029 - accuracy: 0.05 - ETA: 2:07 - loss: 3.8003 - accuracy: 0.05 - ETA: 2:06 - loss: 3.7988 - accuracy: 0.05 - ETA: 2:06 - loss: 3.7989 - accuracy: 0.05 - ETA: 2:06 - loss: 3.7987 - accuracy: 0.05 - ETA: 2:05 - loss: 3.7966 - accuracy: 0.05 - ETA: 2:05 - loss: 3.7966 - accuracy: 0.05 - ETA: 2:04 - loss: 3.7971 - accuracy: 0.05 - ETA: 2:04 - loss: 3.7967 - accuracy: 0.05 - ETA: 2:03 - loss: 3.7984 - accuracy: 0.05 - ETA: 2:03 - loss: 3.7960 - accuracy: 0.05 - ETA: 2:02 - loss: 3.7959 - accuracy: 0.05 - ETA: 2:02 - loss: 3.7970 - accuracy: 0.05 - ETA: 2:02 - loss: 3.7984 - accuracy: 0.05 - ETA: 2:02 - loss: 3.7982 - accuracy: 0.05 - ETA: 2:01 - loss: 3.7979 - accuracy: 0.05 - ETA: 2:01 - loss: 3.7978 - accuracy: 0.05 - ETA: 2:00 - loss: 3.7985 - accuracy: 0.05 - ETA: 2:00 - loss: 3.7983 - accuracy: 0.05 - ETA: 1:59 - loss: 3.7990 - accuracy: 0.05 - ETA: 1:59 - loss: 3.7987 - accuracy: 0.05 - ETA: 1:59 - loss: 3.7981 - accuracy: 0.05 - ETA: 1:58 - loss: 3.7966 - accuracy: 0.05 - ETA: 1:58 - loss: 3.7952 - accuracy: 0.05 - ETA: 1:57 - loss: 3.7972 - accuracy: 0.05 - ETA: 1:57 - loss: 3.7976 - accuracy: 0.05 - ETA: 1:56 - loss: 3.7980 - accuracy: 0.05 - ETA: 1:56 - loss: 3.7982 - accuracy: 0.05 - ETA: 1:55 - loss: 3.7976 - accuracy: 0.05 - ETA: 1:55 - loss: 3.7971 - accuracy: 0.05 - ETA: 1:54 - loss: 3.7976 - accuracy: 0.05 - ETA: 1:54 - loss: 3.7979 - accuracy: 0.05 - ETA: 1:53 - loss: 3.7973 - accuracy: 0.05 - ETA: 1:53 - loss: 3.7962 - accuracy: 0.05 - ETA: 1:52 - loss: 3.7966 - accuracy: 0.05 - ETA: 1:52 - loss: 3.7970 - accuracy: 0.05 - ETA: 1:51 - loss: 3.7971 - accuracy: 0.05 - ETA: 1:51 - loss: 3.7961 - accuracy: 0.05 - ETA: 1:51 - loss: 3.7963 - accuracy: 0.05 - ETA: 1:50 - loss: 3.7962 - accuracy: 0.05 - ETA: 1:50 - loss: 3.7962 - accuracy: 0.05 - ETA: 1:49 - loss: 3.7960 - accuracy: 0.05 - ETA: 1:49 - loss: 3.7953 - accuracy: 0.05 - ETA: 1:48 - loss: 3.7951 - accuracy: 0.05 - ETA: 1:48 - loss: 3.7944 - accuracy: 0.05 - ETA: 1:47 - loss: 3.7941 - accuracy: 0.05 - ETA: 1:47 - loss: 3.7936 - accuracy: 0.05 - ETA: 1:46 - loss: 3.7937 - accuracy: 0.05 - ETA: 1:46 - loss: 3.7948 - accuracy: 0.05 - ETA: 1:46 - loss: 3.7942 - accuracy: 0.06 - ETA: 1:45 - loss: 3.7938 - accuracy: 0.06 - ETA: 1:45 - loss: 3.7931 - accuracy: 0.06 - ETA: 1:44 - loss: 3.7929 - accuracy: 0.06 - ETA: 1:44 - loss: 3.7923 - accuracy: 0.06 - ETA: 1:44 - loss: 3.7922 - accuracy: 0.06 - ETA: 1:43 - loss: 3.7920 - accuracy: 0.06 - ETA: 1:43 - loss: 3.7921 - accuracy: 0.06 - ETA: 1:42 - loss: 3.7921 - accuracy: 0.06 - ETA: 1:42 - loss: 3.7927 - accuracy: 0.05 - ETA: 1:41 - loss: 3.7935 - accuracy: 0.05 - ETA: 1:41 - loss: 3.7945 - accuracy: 0.05 - ETA: 1:41 - loss: 3.7947 - accuracy: 0.05 - ETA: 1:40 - loss: 3.7948 - accuracy: 0.05 - ETA: 1:40 - loss: 3.7951 - accuracy: 0.05 - ETA: 1:39 - loss: 3.7954 - accuracy: 0.05 - ETA: 1:39 - loss: 3.7953 - accuracy: 0.05 - ETA: 1:38 - loss: 3.7952 - accuracy: 0.06 - ETA: 1:38 - loss: 3.7951 - accuracy: 0.05 - ETA: 1:37 - loss: 3.7952 - accuracy: 0.05 - ETA: 1:37 - loss: 3.7949 - accuracy: 0.05 - ETA: 1:37 - loss: 3.7937 - accuracy: 0.05 - ETA: 1:36 - loss: 3.7932 - accuracy: 0.05 - ETA: 1:36 - loss: 3.7936 - accuracy: 0.05 - ETA: 1:35 - loss: 3.7932 - accuracy: 0.06 - ETA: 1:35 - loss: 3.7933 - accuracy: 0.06 - ETA: 1:34 - loss: 3.7939 - accuracy: 0.06 - ETA: 1:34 - loss: 3.7927 - accuracy: 0.06 - ETA: 1:33 - loss: 3.7931 - accuracy: 0.06 - ETA: 1:33 - loss: 3.7929 - accuracy: 0.06 - ETA: 1:32 - loss: 3.7926 - accuracy: 0.06 - ETA: 1:32 - loss: 3.7926 - accuracy: 0.06 - ETA: 1:31 - loss: 3.7923 - accuracy: 0.06 - ETA: 1:31 - loss: 3.7928 - accuracy: 0.06 - ETA: 1:31 - loss: 3.7926 - accuracy: 0.06 - ETA: 1:30 - loss: 3.7926 - accuracy: 0.06 - ETA: 1:30 - loss: 3.7920 - accuracy: 0.06 - ETA: 1:29 - loss: 3.7918 - accuracy: 0.06 - ETA: 1:29 - loss: 3.7917 - accuracy: 0.06 - ETA: 1:28 - loss: 3.7907 - accuracy: 0.06 - ETA: 1:28 - loss: 3.7896 - accuracy: 0.06 - ETA: 1:28 - loss: 3.7899 - accuracy: 0.06 - ETA: 1:27 - loss: 3.7903 - accuracy: 0.06 - ETA: 1:27 - loss: 3.7894 - accuracy: 0.06 - ETA: 1:26 - loss: 3.7895 - accuracy: 0.06 - ETA: 1:26 - loss: 3.7890 - accuracy: 0.06 - ETA: 1:25 - loss: 3.7897 - accuracy: 0.06 - ETA: 1:25 - loss: 3.7893 - accuracy: 0.06 - ETA: 1:24 - loss: 3.7892 - accuracy: 0.06 - ETA: 1:24 - loss: 3.7892 - accuracy: 0.06 - ETA: 1:23 - loss: 3.7888 - accuracy: 0.06 - ETA: 1:23 - loss: 3.7887 - accuracy: 0.06 - ETA: 1:22 - loss: 3.7885 - accuracy: 0.06 - ETA: 1:22 - loss: 3.7892 - accuracy: 0.06 - ETA: 1:21 - loss: 3.7893 - accuracy: 0.06 - ETA: 1:21 - loss: 3.7896 - accuracy: 0.06 - ETA: 1:21 - loss: 3.7899 - accuracy: 0.06 - ETA: 1:20 - loss: 3.7903 - accuracy: 0.06 - ETA: 1:20 - loss: 3.7901 - accuracy: 0.06 - ETA: 1:19 - loss: 3.7904 - accuracy: 0.06 - ETA: 1:19 - loss: 3.7900 - accuracy: 0.06 - ETA: 1:18 - loss: 3.7900 - accuracy: 0.06 - ETA: 1:18 - loss: 3.7895 - accuracy: 0.06 - ETA: 1:17 - loss: 3.7896 - accuracy: 0.06 - ETA: 1:17 - loss: 3.7895 - accuracy: 0.06 - ETA: 1:16 - loss: 3.7893 - accuracy: 0.06 - ETA: 1:16 - loss: 3.7890 - accuracy: 0.06 - ETA: 1:15 - loss: 3.7884 - accuracy: 0.06 - ETA: 1:15 - loss: 3.7891 - accuracy: 0.06 - ETA: 1:15 - loss: 3.7884 - accuracy: 0.06 - ETA: 1:14 - loss: 3.7881 - accuracy: 0.06 - ETA: 1:14 - loss: 3.7881 - accuracy: 0.06 - ETA: 1:13 - loss: 3.7875 - accuracy: 0.06 - ETA: 1:13 - loss: 3.7878 - accuracy: 0.06 - ETA: 1:12 - loss: 3.7879 - accuracy: 0.06 - ETA: 1:12 - loss: 3.7877 - accuracy: 0.06 - ETA: 1:11 - loss: 3.7870 - accuracy: 0.06 - ETA: 1:11 - loss: 3.7873 - accuracy: 0.06 - ETA: 1:11 - loss: 3.7870 - accuracy: 0.06 - ETA: 1:10 - loss: 3.7870 - accuracy: 0.06 - ETA: 1:10 - loss: 3.7865 - accuracy: 0.06 - ETA: 1:09 - loss: 3.7862 - accuracy: 0.06 - ETA: 1:09 - loss: 3.7866 - accuracy: 0.06 - ETA: 1:08 - loss: 3.7865 - accuracy: 0.06 - ETA: 1:08 - loss: 3.7866 - accuracy: 0.06 - ETA: 1:07 - loss: 3.7864 - accuracy: 0.06 - ETA: 1:07 - loss: 3.7866 - accuracy: 0.06 - ETA: 1:06 - loss: 3.7862 - accuracy: 0.06 - ETA: 1:06 - loss: 3.7858 - accuracy: 0.06 - ETA: 1:05 - loss: 3.7852 - accuracy: 0.0614"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:05 - loss: 3.7859 - accuracy: 0.06 - ETA: 1:05 - loss: 3.7863 - accuracy: 0.06 - ETA: 1:04 - loss: 3.7865 - accuracy: 0.06 - ETA: 1:04 - loss: 3.7861 - accuracy: 0.06 - ETA: 1:03 - loss: 3.7860 - accuracy: 0.06 - ETA: 1:03 - loss: 3.7861 - accuracy: 0.06 - ETA: 1:02 - loss: 3.7866 - accuracy: 0.06 - ETA: 1:02 - loss: 3.7864 - accuracy: 0.06 - ETA: 1:01 - loss: 3.7862 - accuracy: 0.06 - ETA: 1:01 - loss: 3.7857 - accuracy: 0.06 - ETA: 1:00 - loss: 3.7857 - accuracy: 0.06 - ETA: 1:00 - loss: 3.7860 - accuracy: 0.06 - ETA: 1:00 - loss: 3.7857 - accuracy: 0.06 - ETA: 59s - loss: 3.7851 - accuracy: 0.0614 - ETA: 59s - loss: 3.7849 - accuracy: 0.061 - ETA: 58s - loss: 3.7854 - accuracy: 0.061 - ETA: 58s - loss: 3.7851 - accuracy: 0.061 - ETA: 57s - loss: 3.7848 - accuracy: 0.061 - ETA: 57s - loss: 3.7844 - accuracy: 0.061 - ETA: 56s - loss: 3.7839 - accuracy: 0.061 - ETA: 56s - loss: 3.7836 - accuracy: 0.061 - ETA: 55s - loss: 3.7844 - accuracy: 0.061 - ETA: 55s - loss: 3.7845 - accuracy: 0.061 - ETA: 55s - loss: 3.7841 - accuracy: 0.061 - ETA: 54s - loss: 3.7837 - accuracy: 0.061 - ETA: 54s - loss: 3.7839 - accuracy: 0.061 - ETA: 53s - loss: 3.7833 - accuracy: 0.061 - ETA: 53s - loss: 3.7830 - accuracy: 0.061 - ETA: 52s - loss: 3.7822 - accuracy: 0.061 - ETA: 52s - loss: 3.7817 - accuracy: 0.062 - ETA: 51s - loss: 3.7817 - accuracy: 0.062 - ETA: 51s - loss: 3.7819 - accuracy: 0.062 - ETA: 50s - loss: 3.7816 - accuracy: 0.062 - ETA: 50s - loss: 3.7812 - accuracy: 0.062 - ETA: 49s - loss: 3.7813 - accuracy: 0.062 - ETA: 49s - loss: 3.7816 - accuracy: 0.062 - ETA: 49s - loss: 3.7817 - accuracy: 0.062 - ETA: 48s - loss: 3.7818 - accuracy: 0.062 - ETA: 48s - loss: 3.7815 - accuracy: 0.062 - ETA: 47s - loss: 3.7817 - accuracy: 0.062 - ETA: 47s - loss: 3.7815 - accuracy: 0.062 - ETA: 46s - loss: 3.7817 - accuracy: 0.062 - ETA: 46s - loss: 3.7812 - accuracy: 0.062 - ETA: 45s - loss: 3.7813 - accuracy: 0.062 - ETA: 45s - loss: 3.7817 - accuracy: 0.062 - ETA: 44s - loss: 3.7813 - accuracy: 0.062 - ETA: 44s - loss: 3.7816 - accuracy: 0.062 - ETA: 44s - loss: 3.7812 - accuracy: 0.062 - ETA: 43s - loss: 3.7806 - accuracy: 0.062 - ETA: 43s - loss: 3.7803 - accuracy: 0.062 - ETA: 42s - loss: 3.7800 - accuracy: 0.062 - ETA: 42s - loss: 3.7799 - accuracy: 0.063 - ETA: 41s - loss: 3.7799 - accuracy: 0.062 - ETA: 41s - loss: 3.7797 - accuracy: 0.063 - ETA: 40s - loss: 3.7795 - accuracy: 0.063 - ETA: 40s - loss: 3.7797 - accuracy: 0.063 - ETA: 40s - loss: 3.7796 - accuracy: 0.063 - ETA: 39s - loss: 3.7796 - accuracy: 0.063 - ETA: 39s - loss: 3.7797 - accuracy: 0.063 - ETA: 38s - loss: 3.7793 - accuracy: 0.063 - ETA: 38s - loss: 3.7795 - accuracy: 0.063 - ETA: 37s - loss: 3.7799 - accuracy: 0.063 - ETA: 37s - loss: 3.7795 - accuracy: 0.063 - ETA: 36s - loss: 3.7796 - accuracy: 0.063 - ETA: 36s - loss: 3.7798 - accuracy: 0.063 - ETA: 35s - loss: 3.7793 - accuracy: 0.063 - ETA: 35s - loss: 3.7794 - accuracy: 0.063 - ETA: 35s - loss: 3.7792 - accuracy: 0.063 - ETA: 34s - loss: 3.7792 - accuracy: 0.063 - ETA: 34s - loss: 3.7787 - accuracy: 0.063 - ETA: 33s - loss: 3.7784 - accuracy: 0.063 - ETA: 33s - loss: 3.7785 - accuracy: 0.063 - ETA: 32s - loss: 3.7783 - accuracy: 0.063 - ETA: 32s - loss: 3.7781 - accuracy: 0.063 - ETA: 31s - loss: 3.7776 - accuracy: 0.063 - ETA: 31s - loss: 3.7779 - accuracy: 0.063 - ETA: 30s - loss: 3.7780 - accuracy: 0.063 - ETA: 30s - loss: 3.7775 - accuracy: 0.063 - ETA: 30s - loss: 3.7774 - accuracy: 0.063 - ETA: 29s - loss: 3.7777 - accuracy: 0.063 - ETA: 29s - loss: 3.7778 - accuracy: 0.063 - ETA: 28s - loss: 3.7775 - accuracy: 0.063 - ETA: 28s - loss: 3.7770 - accuracy: 0.063 - ETA: 27s - loss: 3.7776 - accuracy: 0.063 - ETA: 27s - loss: 3.7775 - accuracy: 0.063 - ETA: 26s - loss: 3.7774 - accuracy: 0.063 - ETA: 26s - loss: 3.7773 - accuracy: 0.063 - ETA: 25s - loss: 3.7770 - accuracy: 0.063 - ETA: 25s - loss: 3.7770 - accuracy: 0.063 - ETA: 25s - loss: 3.7769 - accuracy: 0.064 - ETA: 24s - loss: 3.7765 - accuracy: 0.064 - ETA: 24s - loss: 3.7764 - accuracy: 0.064 - ETA: 23s - loss: 3.7764 - accuracy: 0.064 - ETA: 23s - loss: 3.7764 - accuracy: 0.064 - ETA: 22s - loss: 3.7762 - accuracy: 0.064 - ETA: 22s - loss: 3.7762 - accuracy: 0.064 - ETA: 21s - loss: 3.7761 - accuracy: 0.064 - ETA: 21s - loss: 3.7759 - accuracy: 0.064 - ETA: 20s - loss: 3.7757 - accuracy: 0.064 - ETA: 20s - loss: 3.7757 - accuracy: 0.063 - ETA: 20s - loss: 3.7757 - accuracy: 0.063 - ETA: 19s - loss: 3.7755 - accuracy: 0.064 - ETA: 19s - loss: 3.7752 - accuracy: 0.064 - ETA: 18s - loss: 3.7751 - accuracy: 0.064 - ETA: 18s - loss: 3.7749 - accuracy: 0.064 - ETA: 17s - loss: 3.7746 - accuracy: 0.064 - ETA: 17s - loss: 3.7745 - accuracy: 0.064 - ETA: 16s - loss: 3.7744 - accuracy: 0.064 - ETA: 16s - loss: 3.7743 - accuracy: 0.064 - ETA: 15s - loss: 3.7738 - accuracy: 0.064 - ETA: 15s - loss: 3.7733 - accuracy: 0.064 - ETA: 15s - loss: 3.7730 - accuracy: 0.065 - ETA: 14s - loss: 3.7730 - accuracy: 0.065 - ETA: 14s - loss: 3.7726 - accuracy: 0.065 - ETA: 13s - loss: 3.7723 - accuracy: 0.065 - ETA: 13s - loss: 3.7720 - accuracy: 0.065 - ETA: 12s - loss: 3.7722 - accuracy: 0.065 - ETA: 12s - loss: 3.7725 - accuracy: 0.065 - ETA: 11s - loss: 3.7722 - accuracy: 0.065 - ETA: 11s - loss: 3.7723 - accuracy: 0.065 - ETA: 10s - loss: 3.7721 - accuracy: 0.065 - ETA: 10s - loss: 3.7723 - accuracy: 0.065 - ETA: 10s - loss: 3.7723 - accuracy: 0.065 - ETA: 9s - loss: 3.7720 - accuracy: 0.065 - ETA: 9s - loss: 3.7725 - accuracy: 0.06 - ETA: 8s - loss: 3.7724 - accuracy: 0.06 - ETA: 8s - loss: 3.7720 - accuracy: 0.06 - ETA: 7s - loss: 3.7721 - accuracy: 0.06 - ETA: 7s - loss: 3.7720 - accuracy: 0.06 - ETA: 6s - loss: 3.7718 - accuracy: 0.06 - ETA: 6s - loss: 3.7719 - accuracy: 0.06 - ETA: 5s - loss: 3.7721 - accuracy: 0.06 - ETA: 5s - loss: 3.7722 - accuracy: 0.06 - ETA: 5s - loss: 3.7726 - accuracy: 0.06 - ETA: 4s - loss: 3.7724 - accuracy: 0.06 - ETA: 4s - loss: 3.7721 - accuracy: 0.06 - ETA: 3s - loss: 3.7723 - accuracy: 0.06 - ETA: 3s - loss: 3.7722 - accuracy: 0.06 - ETA: 2s - loss: 3.7723 - accuracy: 0.06 - ETA: 2s - loss: 3.7725 - accuracy: 0.06 - ETA: 1s - loss: 3.7724 - accuracy: 0.06 - ETA: 1s - loss: 3.7722 - accuracy: 0.06 - ETA: 0s - loss: 3.7723 - accuracy: 0.06 - ETA: 0s - loss: 3.7721 - accuracy: 0.06 - ETA: 0s - loss: 3.7721 - accuracy: 0.06 - 161s 4ms/step - loss: 3.7721 - accuracy: 0.0652 - val_loss: 3.9467 - val_accuracy: 0.0334\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:36 - loss: 3.7777 - accuracy: 0.07 - ETA: 2:29 - loss: 3.7878 - accuracy: 0.07 - ETA: 2:27 - loss: 3.7284 - accuracy: 0.09 - ETA: 2:26 - loss: 3.7707 - accuracy: 0.08 - ETA: 2:26 - loss: 3.7501 - accuracy: 0.09 - ETA: 2:25 - loss: 3.7750 - accuracy: 0.08 - ETA: 2:24 - loss: 3.7672 - accuracy: 0.08 - ETA: 2:23 - loss: 3.7623 - accuracy: 0.08 - ETA: 2:23 - loss: 3.7582 - accuracy: 0.08 - ETA: 2:23 - loss: 3.7487 - accuracy: 0.08 - ETA: 2:23 - loss: 3.7403 - accuracy: 0.08 - ETA: 2:22 - loss: 3.7494 - accuracy: 0.08 - ETA: 2:22 - loss: 3.7404 - accuracy: 0.08 - ETA: 2:21 - loss: 3.7422 - accuracy: 0.08 - ETA: 2:21 - loss: 3.7397 - accuracy: 0.07 - ETA: 2:20 - loss: 3.7424 - accuracy: 0.08 - ETA: 2:20 - loss: 3.7362 - accuracy: 0.08 - ETA: 2:20 - loss: 3.7315 - accuracy: 0.08 - ETA: 2:19 - loss: 3.7295 - accuracy: 0.08 - ETA: 2:18 - loss: 3.7391 - accuracy: 0.08 - ETA: 2:19 - loss: 3.7405 - accuracy: 0.08 - ETA: 2:19 - loss: 3.7429 - accuracy: 0.08 - ETA: 2:19 - loss: 3.7470 - accuracy: 0.08 - ETA: 2:18 - loss: 3.7408 - accuracy: 0.08 - ETA: 2:18 - loss: 3.7403 - accuracy: 0.08 - ETA: 2:18 - loss: 3.7372 - accuracy: 0.08 - ETA: 2:18 - loss: 3.7394 - accuracy: 0.08 - ETA: 2:17 - loss: 3.7404 - accuracy: 0.08 - ETA: 2:17 - loss: 3.7419 - accuracy: 0.08 - ETA: 2:16 - loss: 3.7427 - accuracy: 0.08 - ETA: 2:16 - loss: 3.7401 - accuracy: 0.08 - ETA: 2:15 - loss: 3.7392 - accuracy: 0.08 - ETA: 2:15 - loss: 3.7377 - accuracy: 0.08 - ETA: 2:14 - loss: 3.7379 - accuracy: 0.08 - ETA: 2:13 - loss: 3.7385 - accuracy: 0.08 - ETA: 2:13 - loss: 3.7386 - accuracy: 0.08 - ETA: 2:12 - loss: 3.7398 - accuracy: 0.08 - ETA: 2:12 - loss: 3.7411 - accuracy: 0.08 - ETA: 2:11 - loss: 3.7399 - accuracy: 0.08 - ETA: 2:11 - loss: 3.7385 - accuracy: 0.08 - ETA: 2:10 - loss: 3.7390 - accuracy: 0.08 - ETA: 2:10 - loss: 3.7357 - accuracy: 0.08 - ETA: 2:10 - loss: 3.7371 - accuracy: 0.08 - ETA: 2:09 - loss: 3.7357 - accuracy: 0.08 - ETA: 2:09 - loss: 3.7336 - accuracy: 0.08 - ETA: 2:09 - loss: 3.7331 - accuracy: 0.08 - ETA: 2:08 - loss: 3.7353 - accuracy: 0.08 - ETA: 2:08 - loss: 3.7362 - accuracy: 0.08 - ETA: 2:08 - loss: 3.7369 - accuracy: 0.08 - ETA: 2:07 - loss: 3.7351 - accuracy: 0.08 - ETA: 2:07 - loss: 3.7354 - accuracy: 0.08 - ETA: 2:06 - loss: 3.7360 - accuracy: 0.08 - ETA: 2:06 - loss: 3.7361 - accuracy: 0.08 - ETA: 2:05 - loss: 3.7359 - accuracy: 0.07 - ETA: 2:05 - loss: 3.7366 - accuracy: 0.07 - ETA: 2:04 - loss: 3.7339 - accuracy: 0.08 - ETA: 2:04 - loss: 3.7332 - accuracy: 0.08 - ETA: 2:04 - loss: 3.7347 - accuracy: 0.08 - ETA: 2:03 - loss: 3.7326 - accuracy: 0.08 - ETA: 2:03 - loss: 3.7344 - accuracy: 0.08 - ETA: 2:02 - loss: 3.7336 - accuracy: 0.08 - ETA: 2:02 - loss: 3.7343 - accuracy: 0.07 - ETA: 2:02 - loss: 3.7320 - accuracy: 0.08 - ETA: 2:01 - loss: 3.7323 - accuracy: 0.08 - ETA: 2:01 - loss: 3.7318 - accuracy: 0.08 - ETA: 2:00 - loss: 3.7342 - accuracy: 0.08 - ETA: 2:00 - loss: 3.7311 - accuracy: 0.08 - ETA: 1:59 - loss: 3.7315 - accuracy: 0.08 - ETA: 1:59 - loss: 3.7318 - accuracy: 0.08 - ETA: 1:58 - loss: 3.7323 - accuracy: 0.08 - ETA: 1:58 - loss: 3.7343 - accuracy: 0.08 - ETA: 1:57 - loss: 3.7334 - accuracy: 0.08 - ETA: 1:57 - loss: 3.7348 - accuracy: 0.07 - ETA: 1:56 - loss: 3.7351 - accuracy: 0.08 - ETA: 1:56 - loss: 3.7368 - accuracy: 0.07 - ETA: 1:55 - loss: 3.7359 - accuracy: 0.07 - ETA: 1:55 - loss: 3.7374 - accuracy: 0.08 - ETA: 1:54 - loss: 3.7382 - accuracy: 0.08 - ETA: 1:54 - loss: 3.7369 - accuracy: 0.08 - ETA: 1:53 - loss: 3.7381 - accuracy: 0.08 - ETA: 1:53 - loss: 3.7367 - accuracy: 0.08 - ETA: 1:52 - loss: 3.7361 - accuracy: 0.08 - ETA: 1:52 - loss: 3.7360 - accuracy: 0.08 - ETA: 1:51 - loss: 3.7362 - accuracy: 0.08 - ETA: 1:51 - loss: 3.7363 - accuracy: 0.08 - ETA: 1:51 - loss: 3.7368 - accuracy: 0.08 - ETA: 1:50 - loss: 3.7369 - accuracy: 0.08 - ETA: 1:50 - loss: 3.7370 - accuracy: 0.08 - ETA: 1:49 - loss: 3.7369 - accuracy: 0.08 - ETA: 1:49 - loss: 3.7370 - accuracy: 0.08 - ETA: 1:48 - loss: 3.7369 - accuracy: 0.08 - ETA: 1:48 - loss: 3.7377 - accuracy: 0.08 - ETA: 1:47 - loss: 3.7383 - accuracy: 0.08 - ETA: 1:47 - loss: 3.7390 - accuracy: 0.08 - ETA: 1:47 - loss: 3.7376 - accuracy: 0.08 - ETA: 1:46 - loss: 3.7375 - accuracy: 0.07 - ETA: 1:46 - loss: 3.7372 - accuracy: 0.07 - ETA: 1:45 - loss: 3.7371 - accuracy: 0.07 - ETA: 1:45 - loss: 3.7375 - accuracy: 0.07 - ETA: 1:44 - loss: 3.7388 - accuracy: 0.07 - ETA: 1:44 - loss: 3.7388 - accuracy: 0.07 - ETA: 1:44 - loss: 3.7386 - accuracy: 0.07 - ETA: 1:43 - loss: 3.7392 - accuracy: 0.07 - ETA: 1:43 - loss: 3.7396 - accuracy: 0.07 - ETA: 1:42 - loss: 3.7400 - accuracy: 0.07 - ETA: 1:42 - loss: 3.7399 - accuracy: 0.07 - ETA: 1:41 - loss: 3.7411 - accuracy: 0.07 - ETA: 1:41 - loss: 3.7421 - accuracy: 0.07 - ETA: 1:40 - loss: 3.7412 - accuracy: 0.07 - ETA: 1:40 - loss: 3.7410 - accuracy: 0.07 - ETA: 1:39 - loss: 3.7412 - accuracy: 0.07 - ETA: 1:39 - loss: 3.7414 - accuracy: 0.07 - ETA: 1:38 - loss: 3.7406 - accuracy: 0.07 - ETA: 1:38 - loss: 3.7412 - accuracy: 0.07 - ETA: 1:38 - loss: 3.7407 - accuracy: 0.07 - ETA: 1:37 - loss: 3.7404 - accuracy: 0.07 - ETA: 1:37 - loss: 3.7403 - accuracy: 0.07 - ETA: 1:36 - loss: 3.7410 - accuracy: 0.07 - ETA: 1:36 - loss: 3.7412 - accuracy: 0.07 - ETA: 1:35 - loss: 3.7413 - accuracy: 0.07 - ETA: 1:35 - loss: 3.7414 - accuracy: 0.07 - ETA: 1:34 - loss: 3.7411 - accuracy: 0.07 - ETA: 1:34 - loss: 3.7409 - accuracy: 0.07 - ETA: 1:33 - loss: 3.7403 - accuracy: 0.07 - ETA: 1:33 - loss: 3.7401 - accuracy: 0.07 - ETA: 1:32 - loss: 3.7403 - accuracy: 0.07 - ETA: 1:32 - loss: 3.7403 - accuracy: 0.07 - ETA: 1:31 - loss: 3.7397 - accuracy: 0.07 - ETA: 1:31 - loss: 3.7395 - accuracy: 0.07 - ETA: 1:31 - loss: 3.7389 - accuracy: 0.07 - ETA: 1:30 - loss: 3.7388 - accuracy: 0.07 - ETA: 1:30 - loss: 3.7393 - accuracy: 0.07 - ETA: 1:29 - loss: 3.7387 - accuracy: 0.07 - ETA: 1:29 - loss: 3.7385 - accuracy: 0.07 - ETA: 1:28 - loss: 3.7389 - accuracy: 0.07 - ETA: 1:28 - loss: 3.7387 - accuracy: 0.07 - ETA: 1:28 - loss: 3.7387 - accuracy: 0.07 - ETA: 1:27 - loss: 3.7385 - accuracy: 0.07 - ETA: 1:27 - loss: 3.7385 - accuracy: 0.07 - ETA: 1:26 - loss: 3.7381 - accuracy: 0.07 - ETA: 1:26 - loss: 3.7377 - accuracy: 0.07 - ETA: 1:25 - loss: 3.7382 - accuracy: 0.07 - ETA: 1:25 - loss: 3.7382 - accuracy: 0.07 - ETA: 1:24 - loss: 3.7382 - accuracy: 0.07 - ETA: 1:24 - loss: 3.7383 - accuracy: 0.07 - ETA: 1:23 - loss: 3.7386 - accuracy: 0.07 - ETA: 1:23 - loss: 3.7388 - accuracy: 0.07 - ETA: 1:23 - loss: 3.7392 - accuracy: 0.07 - ETA: 1:22 - loss: 3.7398 - accuracy: 0.07 - ETA: 1:22 - loss: 3.7400 - accuracy: 0.07 - ETA: 1:21 - loss: 3.7403 - accuracy: 0.07 - ETA: 1:21 - loss: 3.7398 - accuracy: 0.07 - ETA: 1:20 - loss: 3.7395 - accuracy: 0.07 - ETA: 1:20 - loss: 3.7386 - accuracy: 0.07 - ETA: 1:19 - loss: 3.7383 - accuracy: 0.08 - ETA: 1:19 - loss: 3.7376 - accuracy: 0.08 - ETA: 1:18 - loss: 3.7374 - accuracy: 0.08 - ETA: 1:18 - loss: 3.7370 - accuracy: 0.08 - ETA: 1:18 - loss: 3.7368 - accuracy: 0.08 - ETA: 1:17 - loss: 3.7371 - accuracy: 0.08 - ETA: 1:17 - loss: 3.7371 - accuracy: 0.08 - ETA: 1:16 - loss: 3.7364 - accuracy: 0.08 - ETA: 1:16 - loss: 3.7361 - accuracy: 0.08 - ETA: 1:15 - loss: 3.7356 - accuracy: 0.08 - ETA: 1:15 - loss: 3.7361 - accuracy: 0.08 - ETA: 1:14 - loss: 3.7355 - accuracy: 0.08 - ETA: 1:14 - loss: 3.7360 - accuracy: 0.08 - ETA: 1:14 - loss: 3.7355 - accuracy: 0.08 - ETA: 1:13 - loss: 3.7343 - accuracy: 0.08 - ETA: 1:13 - loss: 3.7342 - accuracy: 0.08 - ETA: 1:12 - loss: 3.7342 - accuracy: 0.08 - ETA: 1:12 - loss: 3.7346 - accuracy: 0.08 - ETA: 1:11 - loss: 3.7349 - accuracy: 0.08 - ETA: 1:11 - loss: 3.7351 - accuracy: 0.08 - ETA: 1:10 - loss: 3.7344 - accuracy: 0.08 - ETA: 1:10 - loss: 3.7347 - accuracy: 0.08 - ETA: 1:09 - loss: 3.7347 - accuracy: 0.08 - ETA: 1:09 - loss: 3.7354 - accuracy: 0.08 - ETA: 1:09 - loss: 3.7354 - accuracy: 0.08 - ETA: 1:08 - loss: 3.7350 - accuracy: 0.08 - ETA: 1:08 - loss: 3.7354 - accuracy: 0.08 - ETA: 1:07 - loss: 3.7354 - accuracy: 0.08 - ETA: 1:07 - loss: 3.7360 - accuracy: 0.07 - ETA: 1:06 - loss: 3.7356 - accuracy: 0.07 - ETA: 1:06 - loss: 3.7356 - accuracy: 0.07 - ETA: 1:05 - loss: 3.7358 - accuracy: 0.0801"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:05 - loss: 3.7362 - accuracy: 0.07 - ETA: 1:04 - loss: 3.7366 - accuracy: 0.07 - ETA: 1:04 - loss: 3.7363 - accuracy: 0.07 - ETA: 1:04 - loss: 3.7358 - accuracy: 0.07 - ETA: 1:03 - loss: 3.7359 - accuracy: 0.07 - ETA: 1:03 - loss: 3.7357 - accuracy: 0.07 - ETA: 1:02 - loss: 3.7355 - accuracy: 0.07 - ETA: 1:02 - loss: 3.7354 - accuracy: 0.07 - ETA: 1:01 - loss: 3.7354 - accuracy: 0.07 - ETA: 1:01 - loss: 3.7349 - accuracy: 0.07 - ETA: 1:00 - loss: 3.7350 - accuracy: 0.07 - ETA: 1:00 - loss: 3.7349 - accuracy: 0.07 - ETA: 1:00 - loss: 3.7343 - accuracy: 0.07 - ETA: 59s - loss: 3.7354 - accuracy: 0.0795 - ETA: 59s - loss: 3.7352 - accuracy: 0.079 - ETA: 58s - loss: 3.7350 - accuracy: 0.079 - ETA: 58s - loss: 3.7346 - accuracy: 0.079 - ETA: 57s - loss: 3.7341 - accuracy: 0.079 - ETA: 57s - loss: 3.7341 - accuracy: 0.079 - ETA: 56s - loss: 3.7341 - accuracy: 0.079 - ETA: 56s - loss: 3.7335 - accuracy: 0.080 - ETA: 55s - loss: 3.7334 - accuracy: 0.080 - ETA: 55s - loss: 3.7321 - accuracy: 0.080 - ETA: 55s - loss: 3.7321 - accuracy: 0.080 - ETA: 54s - loss: 3.7319 - accuracy: 0.080 - ETA: 54s - loss: 3.7317 - accuracy: 0.080 - ETA: 53s - loss: 3.7315 - accuracy: 0.080 - ETA: 53s - loss: 3.7318 - accuracy: 0.080 - ETA: 52s - loss: 3.7307 - accuracy: 0.080 - ETA: 52s - loss: 3.7303 - accuracy: 0.080 - ETA: 51s - loss: 3.7309 - accuracy: 0.080 - ETA: 51s - loss: 3.7310 - accuracy: 0.080 - ETA: 50s - loss: 3.7310 - accuracy: 0.080 - ETA: 50s - loss: 3.7308 - accuracy: 0.080 - ETA: 50s - loss: 3.7308 - accuracy: 0.080 - ETA: 49s - loss: 3.7304 - accuracy: 0.080 - ETA: 49s - loss: 3.7303 - accuracy: 0.081 - ETA: 48s - loss: 3.7294 - accuracy: 0.081 - ETA: 48s - loss: 3.7296 - accuracy: 0.081 - ETA: 47s - loss: 3.7295 - accuracy: 0.081 - ETA: 47s - loss: 3.7294 - accuracy: 0.081 - ETA: 46s - loss: 3.7296 - accuracy: 0.081 - ETA: 46s - loss: 3.7289 - accuracy: 0.081 - ETA: 45s - loss: 3.7287 - accuracy: 0.081 - ETA: 45s - loss: 3.7287 - accuracy: 0.081 - ETA: 44s - loss: 3.7288 - accuracy: 0.081 - ETA: 44s - loss: 3.7291 - accuracy: 0.081 - ETA: 44s - loss: 3.7291 - accuracy: 0.081 - ETA: 43s - loss: 3.7296 - accuracy: 0.081 - ETA: 43s - loss: 3.7289 - accuracy: 0.081 - ETA: 42s - loss: 3.7288 - accuracy: 0.081 - ETA: 42s - loss: 3.7280 - accuracy: 0.081 - ETA: 41s - loss: 3.7279 - accuracy: 0.081 - ETA: 41s - loss: 3.7282 - accuracy: 0.081 - ETA: 40s - loss: 3.7285 - accuracy: 0.081 - ETA: 40s - loss: 3.7278 - accuracy: 0.081 - ETA: 40s - loss: 3.7276 - accuracy: 0.081 - ETA: 39s - loss: 3.7276 - accuracy: 0.081 - ETA: 39s - loss: 3.7275 - accuracy: 0.081 - ETA: 38s - loss: 3.7274 - accuracy: 0.081 - ETA: 38s - loss: 3.7272 - accuracy: 0.081 - ETA: 37s - loss: 3.7273 - accuracy: 0.081 - ETA: 37s - loss: 3.7272 - accuracy: 0.081 - ETA: 36s - loss: 3.7265 - accuracy: 0.081 - ETA: 36s - loss: 3.7268 - accuracy: 0.081 - ETA: 35s - loss: 3.7268 - accuracy: 0.081 - ETA: 35s - loss: 3.7266 - accuracy: 0.081 - ETA: 35s - loss: 3.7268 - accuracy: 0.081 - ETA: 34s - loss: 3.7262 - accuracy: 0.081 - ETA: 34s - loss: 3.7262 - accuracy: 0.081 - ETA: 33s - loss: 3.7263 - accuracy: 0.081 - ETA: 33s - loss: 3.7260 - accuracy: 0.081 - ETA: 32s - loss: 3.7261 - accuracy: 0.081 - ETA: 32s - loss: 3.7262 - accuracy: 0.081 - ETA: 31s - loss: 3.7263 - accuracy: 0.081 - ETA: 31s - loss: 3.7264 - accuracy: 0.081 - ETA: 30s - loss: 3.7263 - accuracy: 0.081 - ETA: 30s - loss: 3.7259 - accuracy: 0.081 - ETA: 29s - loss: 3.7262 - accuracy: 0.081 - ETA: 29s - loss: 3.7261 - accuracy: 0.081 - ETA: 29s - loss: 3.7258 - accuracy: 0.081 - ETA: 28s - loss: 3.7256 - accuracy: 0.081 - ETA: 28s - loss: 3.7255 - accuracy: 0.081 - ETA: 27s - loss: 3.7252 - accuracy: 0.081 - ETA: 27s - loss: 3.7248 - accuracy: 0.081 - ETA: 26s - loss: 3.7248 - accuracy: 0.081 - ETA: 26s - loss: 3.7246 - accuracy: 0.081 - ETA: 25s - loss: 3.7249 - accuracy: 0.081 - ETA: 25s - loss: 3.7252 - accuracy: 0.081 - ETA: 25s - loss: 3.7254 - accuracy: 0.081 - ETA: 24s - loss: 3.7249 - accuracy: 0.081 - ETA: 24s - loss: 3.7243 - accuracy: 0.081 - ETA: 23s - loss: 3.7235 - accuracy: 0.081 - ETA: 23s - loss: 3.7236 - accuracy: 0.081 - ETA: 22s - loss: 3.7238 - accuracy: 0.081 - ETA: 22s - loss: 3.7235 - accuracy: 0.081 - ETA: 21s - loss: 3.7235 - accuracy: 0.081 - ETA: 21s - loss: 3.7233 - accuracy: 0.081 - ETA: 20s - loss: 3.7233 - accuracy: 0.081 - ETA: 20s - loss: 3.7230 - accuracy: 0.081 - ETA: 20s - loss: 3.7230 - accuracy: 0.081 - ETA: 19s - loss: 3.7230 - accuracy: 0.081 - ETA: 19s - loss: 3.7228 - accuracy: 0.081 - ETA: 18s - loss: 3.7221 - accuracy: 0.081 - ETA: 18s - loss: 3.7228 - accuracy: 0.081 - ETA: 17s - loss: 3.7227 - accuracy: 0.081 - ETA: 17s - loss: 3.7231 - accuracy: 0.081 - ETA: 16s - loss: 3.7232 - accuracy: 0.081 - ETA: 16s - loss: 3.7230 - accuracy: 0.081 - ETA: 15s - loss: 3.7225 - accuracy: 0.081 - ETA: 15s - loss: 3.7225 - accuracy: 0.081 - ETA: 15s - loss: 3.7225 - accuracy: 0.081 - ETA: 14s - loss: 3.7226 - accuracy: 0.081 - ETA: 14s - loss: 3.7227 - accuracy: 0.081 - ETA: 13s - loss: 3.7226 - accuracy: 0.081 - ETA: 13s - loss: 3.7228 - accuracy: 0.081 - ETA: 12s - loss: 3.7230 - accuracy: 0.081 - ETA: 12s - loss: 3.7230 - accuracy: 0.081 - ETA: 11s - loss: 3.7229 - accuracy: 0.081 - ETA: 11s - loss: 3.7231 - accuracy: 0.081 - ETA: 10s - loss: 3.7232 - accuracy: 0.081 - ETA: 10s - loss: 3.7234 - accuracy: 0.081 - ETA: 10s - loss: 3.7233 - accuracy: 0.081 - ETA: 9s - loss: 3.7232 - accuracy: 0.081 - ETA: 9s - loss: 3.7230 - accuracy: 0.08 - ETA: 8s - loss: 3.7228 - accuracy: 0.08 - ETA: 8s - loss: 3.7231 - accuracy: 0.08 - ETA: 7s - loss: 3.7230 - accuracy: 0.08 - ETA: 7s - loss: 3.7226 - accuracy: 0.08 - ETA: 6s - loss: 3.7227 - accuracy: 0.08 - ETA: 6s - loss: 3.7225 - accuracy: 0.08 - ETA: 5s - loss: 3.7222 - accuracy: 0.08 - ETA: 5s - loss: 3.7221 - accuracy: 0.08 - ETA: 5s - loss: 3.7219 - accuracy: 0.08 - ETA: 4s - loss: 3.7223 - accuracy: 0.08 - ETA: 4s - loss: 3.7223 - accuracy: 0.08 - ETA: 3s - loss: 3.7224 - accuracy: 0.08 - ETA: 3s - loss: 3.7223 - accuracy: 0.08 - ETA: 2s - loss: 3.7221 - accuracy: 0.08 - ETA: 2s - loss: 3.7222 - accuracy: 0.08 - ETA: 1s - loss: 3.7219 - accuracy: 0.08 - ETA: 1s - loss: 3.7214 - accuracy: 0.08 - ETA: 0s - loss: 3.7213 - accuracy: 0.08 - ETA: 0s - loss: 3.7214 - accuracy: 0.08 - ETA: 0s - loss: 3.7210 - accuracy: 0.08 - 161s 4ms/step - loss: 3.7210 - accuracy: 0.0816 - val_loss: 3.9541 - val_accuracy: 0.0296\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:57 - loss: 3.6779 - accuracy: 0.07 - ETA: 2:38 - loss: 3.5479 - accuracy: 0.10 - ETA: 2:31 - loss: 3.6067 - accuracy: 0.10 - ETA: 2:28 - loss: 3.6465 - accuracy: 0.09 - ETA: 2:27 - loss: 3.6783 - accuracy: 0.09 - ETA: 2:25 - loss: 3.6841 - accuracy: 0.09 - ETA: 2:25 - loss: 3.6935 - accuracy: 0.09 - ETA: 2:25 - loss: 3.6980 - accuracy: 0.08 - ETA: 2:25 - loss: 3.6871 - accuracy: 0.09 - ETA: 2:24 - loss: 3.6920 - accuracy: 0.09 - ETA: 2:24 - loss: 3.6967 - accuracy: 0.09 - ETA: 2:23 - loss: 3.7010 - accuracy: 0.09 - ETA: 2:23 - loss: 3.6941 - accuracy: 0.09 - ETA: 2:23 - loss: 3.6802 - accuracy: 0.09 - ETA: 2:22 - loss: 3.6829 - accuracy: 0.09 - ETA: 2:22 - loss: 3.6834 - accuracy: 0.09 - ETA: 2:22 - loss: 3.6755 - accuracy: 0.09 - ETA: 2:21 - loss: 3.6737 - accuracy: 0.09 - ETA: 2:20 - loss: 3.6761 - accuracy: 0.09 - ETA: 2:20 - loss: 3.6847 - accuracy: 0.09 - ETA: 2:19 - loss: 3.6877 - accuracy: 0.09 - ETA: 2:19 - loss: 3.6866 - accuracy: 0.09 - ETA: 2:19 - loss: 3.6871 - accuracy: 0.09 - ETA: 2:19 - loss: 3.6891 - accuracy: 0.09 - ETA: 2:19 - loss: 3.6931 - accuracy: 0.09 - ETA: 2:18 - loss: 3.6932 - accuracy: 0.09 - ETA: 2:18 - loss: 3.6955 - accuracy: 0.09 - ETA: 2:17 - loss: 3.6929 - accuracy: 0.09 - ETA: 2:17 - loss: 3.6931 - accuracy: 0.09 - ETA: 2:16 - loss: 3.6936 - accuracy: 0.09 - ETA: 2:16 - loss: 3.6941 - accuracy: 0.09 - ETA: 2:15 - loss: 3.6946 - accuracy: 0.09 - ETA: 2:15 - loss: 3.6943 - accuracy: 0.09 - ETA: 2:14 - loss: 3.6946 - accuracy: 0.09 - ETA: 2:14 - loss: 3.6924 - accuracy: 0.09 - ETA: 2:13 - loss: 3.6926 - accuracy: 0.09 - ETA: 2:13 - loss: 3.6919 - accuracy: 0.09 - ETA: 2:12 - loss: 3.6905 - accuracy: 0.09 - ETA: 2:12 - loss: 3.6943 - accuracy: 0.09 - ETA: 2:11 - loss: 3.6945 - accuracy: 0.09 - ETA: 2:11 - loss: 3.6945 - accuracy: 0.09 - ETA: 2:10 - loss: 3.6905 - accuracy: 0.09 - ETA: 2:10 - loss: 3.6921 - accuracy: 0.09 - ETA: 2:09 - loss: 3.6944 - accuracy: 0.09 - ETA: 2:09 - loss: 3.6951 - accuracy: 0.09 - ETA: 2:08 - loss: 3.6966 - accuracy: 0.09 - ETA: 2:08 - loss: 3.6962 - accuracy: 0.09 - ETA: 2:07 - loss: 3.6991 - accuracy: 0.09 - ETA: 2:07 - loss: 3.6973 - accuracy: 0.09 - ETA: 2:06 - loss: 3.6981 - accuracy: 0.09 - ETA: 2:06 - loss: 3.7000 - accuracy: 0.09 - ETA: 2:05 - loss: 3.6991 - accuracy: 0.09 - ETA: 2:05 - loss: 3.7019 - accuracy: 0.09 - ETA: 2:05 - loss: 3.7013 - accuracy: 0.09 - ETA: 2:04 - loss: 3.6995 - accuracy: 0.09 - ETA: 2:04 - loss: 3.6970 - accuracy: 0.09 - ETA: 2:03 - loss: 3.6922 - accuracy: 0.09 - ETA: 2:03 - loss: 3.6950 - accuracy: 0.09 - ETA: 2:03 - loss: 3.6955 - accuracy: 0.09 - ETA: 2:02 - loss: 3.6956 - accuracy: 0.09 - ETA: 2:02 - loss: 3.6941 - accuracy: 0.09 - ETA: 2:01 - loss: 3.6938 - accuracy: 0.09 - ETA: 2:01 - loss: 3.6949 - accuracy: 0.09 - ETA: 2:01 - loss: 3.6946 - accuracy: 0.09 - ETA: 2:00 - loss: 3.6943 - accuracy: 0.09 - ETA: 2:00 - loss: 3.6944 - accuracy: 0.09 - ETA: 1:59 - loss: 3.6949 - accuracy: 0.09 - ETA: 1:59 - loss: 3.6952 - accuracy: 0.09 - ETA: 1:58 - loss: 3.6939 - accuracy: 0.09 - ETA: 1:58 - loss: 3.6957 - accuracy: 0.09 - ETA: 1:58 - loss: 3.6949 - accuracy: 0.09 - ETA: 1:57 - loss: 3.6953 - accuracy: 0.09 - ETA: 1:57 - loss: 3.6959 - accuracy: 0.09 - ETA: 1:56 - loss: 3.6964 - accuracy: 0.08 - ETA: 1:56 - loss: 3.6961 - accuracy: 0.08 - ETA: 1:55 - loss: 3.6970 - accuracy: 0.08 - ETA: 1:55 - loss: 3.6955 - accuracy: 0.08 - ETA: 1:54 - loss: 3.6963 - accuracy: 0.08 - ETA: 1:54 - loss: 3.6958 - accuracy: 0.08 - ETA: 1:53 - loss: 3.6967 - accuracy: 0.08 - ETA: 1:53 - loss: 3.6977 - accuracy: 0.08 - ETA: 1:52 - loss: 3.6971 - accuracy: 0.08 - ETA: 1:52 - loss: 3.6959 - accuracy: 0.08 - ETA: 1:51 - loss: 3.6964 - accuracy: 0.08 - ETA: 1:51 - loss: 3.6938 - accuracy: 0.08 - ETA: 1:50 - loss: 3.6934 - accuracy: 0.08 - ETA: 1:50 - loss: 3.6953 - accuracy: 0.08 - ETA: 1:50 - loss: 3.6937 - accuracy: 0.08 - ETA: 1:49 - loss: 3.6933 - accuracy: 0.08 - ETA: 1:49 - loss: 3.6922 - accuracy: 0.08 - ETA: 1:48 - loss: 3.6919 - accuracy: 0.08 - ETA: 1:48 - loss: 3.6922 - accuracy: 0.08 - ETA: 1:47 - loss: 3.6911 - accuracy: 0.08 - ETA: 1:47 - loss: 3.6904 - accuracy: 0.08 - ETA: 1:47 - loss: 3.6904 - accuracy: 0.08 - ETA: 1:46 - loss: 3.6897 - accuracy: 0.08 - ETA: 1:46 - loss: 3.6905 - accuracy: 0.08 - ETA: 1:45 - loss: 3.6896 - accuracy: 0.08 - ETA: 1:45 - loss: 3.6888 - accuracy: 0.08 - ETA: 1:44 - loss: 3.6887 - accuracy: 0.09 - ETA: 1:44 - loss: 3.6897 - accuracy: 0.08 - ETA: 1:43 - loss: 3.6891 - accuracy: 0.08 - ETA: 1:43 - loss: 3.6882 - accuracy: 0.08 - ETA: 1:43 - loss: 3.6872 - accuracy: 0.08 - ETA: 1:42 - loss: 3.6867 - accuracy: 0.08 - ETA: 1:42 - loss: 3.6871 - accuracy: 0.08 - ETA: 1:41 - loss: 3.6877 - accuracy: 0.08 - ETA: 1:41 - loss: 3.6880 - accuracy: 0.08 - ETA: 1:40 - loss: 3.6885 - accuracy: 0.08 - ETA: 1:40 - loss: 3.6870 - accuracy: 0.08 - ETA: 1:39 - loss: 3.6873 - accuracy: 0.08 - ETA: 1:39 - loss: 3.6868 - accuracy: 0.08 - ETA: 1:38 - loss: 3.6869 - accuracy: 0.08 - ETA: 1:38 - loss: 3.6862 - accuracy: 0.08 - ETA: 1:37 - loss: 3.6857 - accuracy: 0.08 - ETA: 1:37 - loss: 3.6856 - accuracy: 0.09 - ETA: 1:36 - loss: 3.6865 - accuracy: 0.08 - ETA: 1:36 - loss: 3.6860 - accuracy: 0.08 - ETA: 1:36 - loss: 3.6850 - accuracy: 0.08 - ETA: 1:35 - loss: 3.6853 - accuracy: 0.08 - ETA: 1:35 - loss: 3.6854 - accuracy: 0.08 - ETA: 1:34 - loss: 3.6851 - accuracy: 0.08 - ETA: 1:34 - loss: 3.6847 - accuracy: 0.08 - ETA: 1:33 - loss: 3.6849 - accuracy: 0.08 - ETA: 1:33 - loss: 3.6856 - accuracy: 0.08 - ETA: 1:32 - loss: 3.6857 - accuracy: 0.08 - ETA: 1:32 - loss: 3.6858 - accuracy: 0.08 - ETA: 1:32 - loss: 3.6855 - accuracy: 0.08 - ETA: 1:31 - loss: 3.6852 - accuracy: 0.08 - ETA: 1:31 - loss: 3.6850 - accuracy: 0.08 - ETA: 1:31 - loss: 3.6848 - accuracy: 0.08 - ETA: 1:30 - loss: 3.6845 - accuracy: 0.08 - ETA: 1:30 - loss: 3.6842 - accuracy: 0.08 - ETA: 1:29 - loss: 3.6840 - accuracy: 0.08 - ETA: 1:29 - loss: 3.6840 - accuracy: 0.08 - ETA: 1:28 - loss: 3.6834 - accuracy: 0.08 - ETA: 1:28 - loss: 3.6834 - accuracy: 0.08 - ETA: 1:27 - loss: 3.6831 - accuracy: 0.08 - ETA: 1:27 - loss: 3.6823 - accuracy: 0.08 - ETA: 1:26 - loss: 3.6827 - accuracy: 0.08 - ETA: 1:26 - loss: 3.6834 - accuracy: 0.08 - ETA: 1:25 - loss: 3.6842 - accuracy: 0.08 - ETA: 1:25 - loss: 3.6842 - accuracy: 0.08 - ETA: 1:24 - loss: 3.6840 - accuracy: 0.08 - ETA: 1:24 - loss: 3.6844 - accuracy: 0.08 - ETA: 1:24 - loss: 3.6841 - accuracy: 0.08 - ETA: 1:23 - loss: 3.6836 - accuracy: 0.08 - ETA: 1:23 - loss: 3.6838 - accuracy: 0.08 - ETA: 1:22 - loss: 3.6845 - accuracy: 0.08 - ETA: 1:22 - loss: 3.6843 - accuracy: 0.08 - ETA: 1:21 - loss: 3.6838 - accuracy: 0.08 - ETA: 1:21 - loss: 3.6835 - accuracy: 0.08 - ETA: 1:20 - loss: 3.6842 - accuracy: 0.08 - ETA: 1:20 - loss: 3.6845 - accuracy: 0.08 - ETA: 1:19 - loss: 3.6855 - accuracy: 0.08 - ETA: 1:19 - loss: 3.6861 - accuracy: 0.08 - ETA: 1:18 - loss: 3.6870 - accuracy: 0.08 - ETA: 1:18 - loss: 3.6879 - accuracy: 0.08 - ETA: 1:18 - loss: 3.6875 - accuracy: 0.08 - ETA: 1:17 - loss: 3.6875 - accuracy: 0.08 - ETA: 1:17 - loss: 3.6870 - accuracy: 0.08 - ETA: 1:16 - loss: 3.6869 - accuracy: 0.08 - ETA: 1:16 - loss: 3.6869 - accuracy: 0.08 - ETA: 1:15 - loss: 3.6872 - accuracy: 0.08 - ETA: 1:15 - loss: 3.6877 - accuracy: 0.08 - ETA: 1:14 - loss: 3.6884 - accuracy: 0.08 - ETA: 1:14 - loss: 3.6886 - accuracy: 0.08 - ETA: 1:14 - loss: 3.6886 - accuracy: 0.08 - ETA: 1:13 - loss: 3.6887 - accuracy: 0.08 - ETA: 1:13 - loss: 3.6884 - accuracy: 0.08 - ETA: 1:12 - loss: 3.6888 - accuracy: 0.08 - ETA: 1:12 - loss: 3.6891 - accuracy: 0.08 - ETA: 1:11 - loss: 3.6894 - accuracy: 0.08 - ETA: 1:11 - loss: 3.6882 - accuracy: 0.08 - ETA: 1:10 - loss: 3.6882 - accuracy: 0.08 - ETA: 1:10 - loss: 3.6879 - accuracy: 0.08 - ETA: 1:09 - loss: 3.6876 - accuracy: 0.08 - ETA: 1:09 - loss: 3.6884 - accuracy: 0.08 - ETA: 1:08 - loss: 3.6882 - accuracy: 0.08 - ETA: 1:08 - loss: 3.6888 - accuracy: 0.08 - ETA: 1:08 - loss: 3.6885 - accuracy: 0.08 - ETA: 1:07 - loss: 3.6878 - accuracy: 0.08 - ETA: 1:07 - loss: 3.6873 - accuracy: 0.08 - ETA: 1:06 - loss: 3.6869 - accuracy: 0.08 - ETA: 1:06 - loss: 3.6869 - accuracy: 0.08 - ETA: 1:05 - loss: 3.6863 - accuracy: 0.0888"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:05 - loss: 3.6872 - accuracy: 0.08 - ETA: 1:04 - loss: 3.6874 - accuracy: 0.08 - ETA: 1:04 - loss: 3.6877 - accuracy: 0.08 - ETA: 1:03 - loss: 3.6871 - accuracy: 0.08 - ETA: 1:03 - loss: 3.6869 - accuracy: 0.08 - ETA: 1:02 - loss: 3.6862 - accuracy: 0.08 - ETA: 1:02 - loss: 3.6865 - accuracy: 0.08 - ETA: 1:02 - loss: 3.6868 - accuracy: 0.08 - ETA: 1:01 - loss: 3.6866 - accuracy: 0.08 - ETA: 1:01 - loss: 3.6860 - accuracy: 0.08 - ETA: 1:00 - loss: 3.6861 - accuracy: 0.08 - ETA: 1:00 - loss: 3.6859 - accuracy: 0.08 - ETA: 59s - loss: 3.6856 - accuracy: 0.0885 - ETA: 59s - loss: 3.6857 - accuracy: 0.088 - ETA: 58s - loss: 3.6859 - accuracy: 0.088 - ETA: 58s - loss: 3.6867 - accuracy: 0.088 - ETA: 58s - loss: 3.6856 - accuracy: 0.088 - ETA: 57s - loss: 3.6858 - accuracy: 0.088 - ETA: 57s - loss: 3.6853 - accuracy: 0.088 - ETA: 56s - loss: 3.6851 - accuracy: 0.088 - ETA: 56s - loss: 3.6855 - accuracy: 0.088 - ETA: 55s - loss: 3.6858 - accuracy: 0.088 - ETA: 55s - loss: 3.6858 - accuracy: 0.088 - ETA: 54s - loss: 3.6860 - accuracy: 0.088 - ETA: 54s - loss: 3.6865 - accuracy: 0.088 - ETA: 53s - loss: 3.6866 - accuracy: 0.087 - ETA: 53s - loss: 3.6872 - accuracy: 0.087 - ETA: 53s - loss: 3.6869 - accuracy: 0.087 - ETA: 52s - loss: 3.6873 - accuracy: 0.087 - ETA: 52s - loss: 3.6873 - accuracy: 0.087 - ETA: 51s - loss: 3.6876 - accuracy: 0.087 - ETA: 51s - loss: 3.6874 - accuracy: 0.087 - ETA: 50s - loss: 3.6868 - accuracy: 0.087 - ETA: 50s - loss: 3.6863 - accuracy: 0.087 - ETA: 49s - loss: 3.6865 - accuracy: 0.087 - ETA: 49s - loss: 3.6861 - accuracy: 0.087 - ETA: 48s - loss: 3.6862 - accuracy: 0.087 - ETA: 48s - loss: 3.6864 - accuracy: 0.087 - ETA: 48s - loss: 3.6867 - accuracy: 0.087 - ETA: 47s - loss: 3.6868 - accuracy: 0.087 - ETA: 47s - loss: 3.6867 - accuracy: 0.087 - ETA: 46s - loss: 3.6865 - accuracy: 0.087 - ETA: 46s - loss: 3.6856 - accuracy: 0.087 - ETA: 45s - loss: 3.6853 - accuracy: 0.087 - ETA: 45s - loss: 3.6852 - accuracy: 0.087 - ETA: 44s - loss: 3.6846 - accuracy: 0.087 - ETA: 44s - loss: 3.6845 - accuracy: 0.087 - ETA: 43s - loss: 3.6844 - accuracy: 0.087 - ETA: 43s - loss: 3.6848 - accuracy: 0.087 - ETA: 43s - loss: 3.6852 - accuracy: 0.087 - ETA: 42s - loss: 3.6847 - accuracy: 0.087 - ETA: 42s - loss: 3.6851 - accuracy: 0.087 - ETA: 41s - loss: 3.6847 - accuracy: 0.088 - ETA: 41s - loss: 3.6841 - accuracy: 0.088 - ETA: 40s - loss: 3.6843 - accuracy: 0.088 - ETA: 40s - loss: 3.6843 - accuracy: 0.088 - ETA: 39s - loss: 3.6849 - accuracy: 0.088 - ETA: 39s - loss: 3.6851 - accuracy: 0.088 - ETA: 38s - loss: 3.6854 - accuracy: 0.088 - ETA: 38s - loss: 3.6851 - accuracy: 0.088 - ETA: 38s - loss: 3.6849 - accuracy: 0.088 - ETA: 37s - loss: 3.6849 - accuracy: 0.088 - ETA: 37s - loss: 3.6857 - accuracy: 0.087 - ETA: 36s - loss: 3.6860 - accuracy: 0.087 - ETA: 36s - loss: 3.6859 - accuracy: 0.087 - ETA: 35s - loss: 3.6863 - accuracy: 0.087 - ETA: 35s - loss: 3.6860 - accuracy: 0.087 - ETA: 34s - loss: 3.6856 - accuracy: 0.087 - ETA: 34s - loss: 3.6857 - accuracy: 0.087 - ETA: 33s - loss: 3.6860 - accuracy: 0.087 - ETA: 33s - loss: 3.6863 - accuracy: 0.087 - ETA: 33s - loss: 3.6864 - accuracy: 0.087 - ETA: 32s - loss: 3.6863 - accuracy: 0.087 - ETA: 32s - loss: 3.6861 - accuracy: 0.087 - ETA: 31s - loss: 3.6860 - accuracy: 0.087 - ETA: 31s - loss: 3.6861 - accuracy: 0.087 - ETA: 30s - loss: 3.6859 - accuracy: 0.087 - ETA: 30s - loss: 3.6855 - accuracy: 0.087 - ETA: 29s - loss: 3.6857 - accuracy: 0.087 - ETA: 29s - loss: 3.6855 - accuracy: 0.087 - ETA: 28s - loss: 3.6849 - accuracy: 0.087 - ETA: 28s - loss: 3.6850 - accuracy: 0.087 - ETA: 28s - loss: 3.6849 - accuracy: 0.087 - ETA: 27s - loss: 3.6846 - accuracy: 0.087 - ETA: 27s - loss: 3.6847 - accuracy: 0.087 - ETA: 26s - loss: 3.6847 - accuracy: 0.087 - ETA: 26s - loss: 3.6848 - accuracy: 0.087 - ETA: 25s - loss: 3.6849 - accuracy: 0.087 - ETA: 25s - loss: 3.6851 - accuracy: 0.087 - ETA: 24s - loss: 3.6850 - accuracy: 0.087 - ETA: 24s - loss: 3.6851 - accuracy: 0.087 - ETA: 24s - loss: 3.6850 - accuracy: 0.087 - ETA: 23s - loss: 3.6842 - accuracy: 0.087 - ETA: 23s - loss: 3.6842 - accuracy: 0.087 - ETA: 22s - loss: 3.6841 - accuracy: 0.087 - ETA: 22s - loss: 3.6838 - accuracy: 0.087 - ETA: 21s - loss: 3.6839 - accuracy: 0.087 - ETA: 21s - loss: 3.6840 - accuracy: 0.087 - ETA: 20s - loss: 3.6841 - accuracy: 0.087 - ETA: 20s - loss: 3.6839 - accuracy: 0.087 - ETA: 19s - loss: 3.6837 - accuracy: 0.088 - ETA: 19s - loss: 3.6841 - accuracy: 0.087 - ETA: 19s - loss: 3.6840 - accuracy: 0.088 - ETA: 18s - loss: 3.6842 - accuracy: 0.088 - ETA: 18s - loss: 3.6847 - accuracy: 0.088 - ETA: 17s - loss: 3.6842 - accuracy: 0.088 - ETA: 17s - loss: 3.6843 - accuracy: 0.088 - ETA: 16s - loss: 3.6842 - accuracy: 0.088 - ETA: 16s - loss: 3.6844 - accuracy: 0.088 - ETA: 15s - loss: 3.6844 - accuracy: 0.088 - ETA: 15s - loss: 3.6844 - accuracy: 0.088 - ETA: 14s - loss: 3.6841 - accuracy: 0.088 - ETA: 14s - loss: 3.6838 - accuracy: 0.088 - ETA: 14s - loss: 3.6832 - accuracy: 0.088 - ETA: 13s - loss: 3.6832 - accuracy: 0.088 - ETA: 13s - loss: 3.6833 - accuracy: 0.088 - ETA: 12s - loss: 3.6832 - accuracy: 0.088 - ETA: 12s - loss: 3.6831 - accuracy: 0.088 - ETA: 11s - loss: 3.6831 - accuracy: 0.088 - ETA: 11s - loss: 3.6835 - accuracy: 0.088 - ETA: 10s - loss: 3.6832 - accuracy: 0.088 - ETA: 10s - loss: 3.6831 - accuracy: 0.088 - ETA: 10s - loss: 3.6829 - accuracy: 0.088 - ETA: 9s - loss: 3.6828 - accuracy: 0.088 - ETA: 9s - loss: 3.6827 - accuracy: 0.08 - ETA: 8s - loss: 3.6825 - accuracy: 0.08 - ETA: 8s - loss: 3.6827 - accuracy: 0.08 - ETA: 7s - loss: 3.6825 - accuracy: 0.08 - ETA: 7s - loss: 3.6825 - accuracy: 0.08 - ETA: 6s - loss: 3.6820 - accuracy: 0.08 - ETA: 6s - loss: 3.6821 - accuracy: 0.08 - ETA: 5s - loss: 3.6820 - accuracy: 0.08 - ETA: 5s - loss: 3.6817 - accuracy: 0.08 - ETA: 5s - loss: 3.6814 - accuracy: 0.08 - ETA: 4s - loss: 3.6817 - accuracy: 0.08 - ETA: 4s - loss: 3.6819 - accuracy: 0.08 - ETA: 3s - loss: 3.6814 - accuracy: 0.08 - ETA: 3s - loss: 3.6816 - accuracy: 0.08 - ETA: 2s - loss: 3.6819 - accuracy: 0.08 - ETA: 2s - loss: 3.6819 - accuracy: 0.08 - ETA: 1s - loss: 3.6817 - accuracy: 0.08 - ETA: 1s - loss: 3.6818 - accuracy: 0.08 - ETA: 0s - loss: 3.6819 - accuracy: 0.08 - ETA: 0s - loss: 3.6819 - accuracy: 0.08 - ETA: 0s - loss: 3.6822 - accuracy: 0.08 - 160s 4ms/step - loss: 3.6822 - accuracy: 0.0885 - val_loss: 3.9392 - val_accuracy: 0.0341\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:47 - loss: 3.6752 - accuracy: 0.08 - ETA: 2:36 - loss: 3.6745 - accuracy: 0.08 - ETA: 2:34 - loss: 3.7171 - accuracy: 0.06 - ETA: 2:30 - loss: 3.7101 - accuracy: 0.07 - ETA: 2:27 - loss: 3.7086 - accuracy: 0.06 - ETA: 2:26 - loss: 3.6894 - accuracy: 0.07 - ETA: 2:25 - loss: 3.6944 - accuracy: 0.07 - ETA: 2:25 - loss: 3.6816 - accuracy: 0.08 - ETA: 2:25 - loss: 3.6802 - accuracy: 0.08 - ETA: 2:24 - loss: 3.6630 - accuracy: 0.08 - ETA: 2:24 - loss: 3.6701 - accuracy: 0.08 - ETA: 2:23 - loss: 3.6869 - accuracy: 0.08 - ETA: 2:22 - loss: 3.6822 - accuracy: 0.08 - ETA: 2:22 - loss: 3.6743 - accuracy: 0.08 - ETA: 2:21 - loss: 3.6753 - accuracy: 0.08 - ETA: 2:21 - loss: 3.6759 - accuracy: 0.08 - ETA: 2:20 - loss: 3.6758 - accuracy: 0.08 - ETA: 2:20 - loss: 3.6672 - accuracy: 0.08 - ETA: 2:20 - loss: 3.6664 - accuracy: 0.08 - ETA: 2:19 - loss: 3.6595 - accuracy: 0.08 - ETA: 2:19 - loss: 3.6548 - accuracy: 0.09 - ETA: 2:20 - loss: 3.6481 - accuracy: 0.09 - ETA: 2:20 - loss: 3.6523 - accuracy: 0.09 - ETA: 2:19 - loss: 3.6478 - accuracy: 0.09 - ETA: 2:19 - loss: 3.6445 - accuracy: 0.09 - ETA: 2:18 - loss: 3.6431 - accuracy: 0.09 - ETA: 2:18 - loss: 3.6397 - accuracy: 0.09 - ETA: 2:17 - loss: 3.6339 - accuracy: 0.09 - ETA: 2:17 - loss: 3.6302 - accuracy: 0.09 - ETA: 2:17 - loss: 3.6344 - accuracy: 0.09 - ETA: 2:16 - loss: 3.6327 - accuracy: 0.09 - ETA: 2:15 - loss: 3.6367 - accuracy: 0.09 - ETA: 2:15 - loss: 3.6400 - accuracy: 0.09 - ETA: 2:14 - loss: 3.6417 - accuracy: 0.09 - ETA: 2:14 - loss: 3.6429 - accuracy: 0.09 - ETA: 2:13 - loss: 3.6430 - accuracy: 0.09 - ETA: 2:13 - loss: 3.6435 - accuracy: 0.09 - ETA: 2:12 - loss: 3.6471 - accuracy: 0.09 - ETA: 2:12 - loss: 3.6434 - accuracy: 0.09 - ETA: 2:11 - loss: 3.6426 - accuracy: 0.09 - ETA: 2:11 - loss: 3.6395 - accuracy: 0.09 - ETA: 2:10 - loss: 3.6374 - accuracy: 0.09 - ETA: 2:10 - loss: 3.6342 - accuracy: 0.09 - ETA: 2:09 - loss: 3.6322 - accuracy: 0.09 - ETA: 2:09 - loss: 3.6342 - accuracy: 0.09 - ETA: 2:08 - loss: 3.6362 - accuracy: 0.09 - ETA: 2:08 - loss: 3.6385 - accuracy: 0.09 - ETA: 2:07 - loss: 3.6390 - accuracy: 0.09 - ETA: 2:07 - loss: 3.6387 - accuracy: 0.09 - ETA: 2:06 - loss: 3.6397 - accuracy: 0.09 - ETA: 2:06 - loss: 3.6391 - accuracy: 0.09 - ETA: 2:05 - loss: 3.6404 - accuracy: 0.09 - ETA: 2:05 - loss: 3.6417 - accuracy: 0.09 - ETA: 2:04 - loss: 3.6414 - accuracy: 0.09 - ETA: 2:04 - loss: 3.6416 - accuracy: 0.09 - ETA: 2:03 - loss: 3.6414 - accuracy: 0.09 - ETA: 2:03 - loss: 3.6426 - accuracy: 0.09 - ETA: 2:03 - loss: 3.6438 - accuracy: 0.09 - ETA: 2:02 - loss: 3.6451 - accuracy: 0.09 - ETA: 2:02 - loss: 3.6468 - accuracy: 0.09 - ETA: 2:02 - loss: 3.6469 - accuracy: 0.09 - ETA: 2:01 - loss: 3.6459 - accuracy: 0.09 - ETA: 2:01 - loss: 3.6476 - accuracy: 0.09 - ETA: 2:00 - loss: 3.6479 - accuracy: 0.09 - ETA: 2:00 - loss: 3.6471 - accuracy: 0.09 - ETA: 2:00 - loss: 3.6495 - accuracy: 0.09 - ETA: 1:59 - loss: 3.6497 - accuracy: 0.09 - ETA: 1:59 - loss: 3.6495 - accuracy: 0.09 - ETA: 1:58 - loss: 3.6479 - accuracy: 0.09 - ETA: 1:58 - loss: 3.6486 - accuracy: 0.09 - ETA: 1:57 - loss: 3.6474 - accuracy: 0.09 - ETA: 1:57 - loss: 3.6471 - accuracy: 0.09 - ETA: 1:56 - loss: 3.6487 - accuracy: 0.09 - ETA: 1:56 - loss: 3.6488 - accuracy: 0.09 - ETA: 1:55 - loss: 3.6492 - accuracy: 0.09 - ETA: 1:55 - loss: 3.6508 - accuracy: 0.09 - ETA: 1:54 - loss: 3.6495 - accuracy: 0.09 - ETA: 1:54 - loss: 3.6493 - accuracy: 0.09 - ETA: 1:54 - loss: 3.6484 - accuracy: 0.09 - ETA: 1:53 - loss: 3.6480 - accuracy: 0.09 - ETA: 1:53 - loss: 3.6480 - accuracy: 0.09 - ETA: 1:52 - loss: 3.6462 - accuracy: 0.09 - ETA: 1:52 - loss: 3.6470 - accuracy: 0.09 - ETA: 1:51 - loss: 3.6479 - accuracy: 0.09 - ETA: 1:51 - loss: 3.6472 - accuracy: 0.09 - ETA: 1:50 - loss: 3.6480 - accuracy: 0.09 - ETA: 1:50 - loss: 3.6492 - accuracy: 0.09 - ETA: 1:49 - loss: 3.6492 - accuracy: 0.09 - ETA: 1:49 - loss: 3.6491 - accuracy: 0.09 - ETA: 1:48 - loss: 3.6504 - accuracy: 0.09 - ETA: 1:48 - loss: 3.6507 - accuracy: 0.09 - ETA: 1:48 - loss: 3.6505 - accuracy: 0.09 - ETA: 1:47 - loss: 3.6504 - accuracy: 0.09 - ETA: 1:47 - loss: 3.6489 - accuracy: 0.09 - ETA: 1:46 - loss: 3.6503 - accuracy: 0.09 - ETA: 1:46 - loss: 3.6496 - accuracy: 0.09 - ETA: 1:46 - loss: 3.6487 - accuracy: 0.09 - ETA: 1:45 - loss: 3.6474 - accuracy: 0.09 - ETA: 1:45 - loss: 3.6474 - accuracy: 0.09 - ETA: 1:44 - loss: 3.6479 - accuracy: 0.09 - ETA: 1:44 - loss: 3.6492 - accuracy: 0.09 - ETA: 1:43 - loss: 3.6484 - accuracy: 0.09 - ETA: 1:43 - loss: 3.6478 - accuracy: 0.09 - ETA: 1:42 - loss: 3.6469 - accuracy: 0.09 - ETA: 1:42 - loss: 3.6474 - accuracy: 0.09 - ETA: 1:41 - loss: 3.6475 - accuracy: 0.09 - ETA: 1:41 - loss: 3.6480 - accuracy: 0.09 - ETA: 1:40 - loss: 3.6488 - accuracy: 0.09 - ETA: 1:40 - loss: 3.6486 - accuracy: 0.09 - ETA: 1:40 - loss: 3.6484 - accuracy: 0.09 - ETA: 1:39 - loss: 3.6478 - accuracy: 0.09 - ETA: 1:39 - loss: 3.6466 - accuracy: 0.09 - ETA: 1:38 - loss: 3.6459 - accuracy: 0.09 - ETA: 1:38 - loss: 3.6450 - accuracy: 0.09 - ETA: 1:37 - loss: 3.6454 - accuracy: 0.09 - ETA: 1:37 - loss: 3.6451 - accuracy: 0.09 - ETA: 1:36 - loss: 3.6459 - accuracy: 0.09 - ETA: 1:36 - loss: 3.6453 - accuracy: 0.09 - ETA: 1:35 - loss: 3.6458 - accuracy: 0.09 - ETA: 1:35 - loss: 3.6454 - accuracy: 0.09 - ETA: 1:34 - loss: 3.6448 - accuracy: 0.09 - ETA: 1:34 - loss: 3.6456 - accuracy: 0.09 - ETA: 1:34 - loss: 3.6473 - accuracy: 0.09 - ETA: 1:33 - loss: 3.6475 - accuracy: 0.09 - ETA: 1:33 - loss: 3.6478 - accuracy: 0.09 - ETA: 1:32 - loss: 3.6476 - accuracy: 0.09 - ETA: 1:32 - loss: 3.6486 - accuracy: 0.09 - ETA: 1:31 - loss: 3.6489 - accuracy: 0.09 - ETA: 1:31 - loss: 3.6484 - accuracy: 0.09 - ETA: 1:31 - loss: 3.6467 - accuracy: 0.09 - ETA: 1:30 - loss: 3.6470 - accuracy: 0.09 - ETA: 1:30 - loss: 3.6468 - accuracy: 0.09 - ETA: 1:29 - loss: 3.6456 - accuracy: 0.09 - ETA: 1:29 - loss: 3.6448 - accuracy: 0.09 - ETA: 1:28 - loss: 3.6446 - accuracy: 0.09 - ETA: 1:28 - loss: 3.6453 - accuracy: 0.09 - ETA: 1:27 - loss: 3.6461 - accuracy: 0.09 - ETA: 1:27 - loss: 3.6458 - accuracy: 0.09 - ETA: 1:26 - loss: 3.6462 - accuracy: 0.09 - ETA: 1:26 - loss: 3.6452 - accuracy: 0.09 - ETA: 1:25 - loss: 3.6456 - accuracy: 0.09 - ETA: 1:25 - loss: 3.6463 - accuracy: 0.09 - ETA: 1:25 - loss: 3.6462 - accuracy: 0.09 - ETA: 1:24 - loss: 3.6463 - accuracy: 0.09 - ETA: 1:24 - loss: 3.6469 - accuracy: 0.09 - ETA: 1:23 - loss: 3.6466 - accuracy: 0.09 - ETA: 1:23 - loss: 3.6466 - accuracy: 0.09 - ETA: 1:22 - loss: 3.6467 - accuracy: 0.09 - ETA: 1:22 - loss: 3.6458 - accuracy: 0.09 - ETA: 1:21 - loss: 3.6460 - accuracy: 0.09 - ETA: 1:21 - loss: 3.6466 - accuracy: 0.09 - ETA: 1:20 - loss: 3.6469 - accuracy: 0.09 - ETA: 1:20 - loss: 3.6463 - accuracy: 0.09 - ETA: 1:20 - loss: 3.6461 - accuracy: 0.09 - ETA: 1:19 - loss: 3.6468 - accuracy: 0.09 - ETA: 1:19 - loss: 3.6469 - accuracy: 0.09 - ETA: 1:18 - loss: 3.6476 - accuracy: 0.09 - ETA: 1:18 - loss: 3.6475 - accuracy: 0.09 - ETA: 1:17 - loss: 3.6468 - accuracy: 0.09 - ETA: 1:17 - loss: 3.6468 - accuracy: 0.09 - ETA: 1:16 - loss: 3.6477 - accuracy: 0.09 - ETA: 1:16 - loss: 3.6476 - accuracy: 0.09 - ETA: 1:15 - loss: 3.6472 - accuracy: 0.09 - ETA: 1:15 - loss: 3.6469 - accuracy: 0.09 - ETA: 1:15 - loss: 3.6478 - accuracy: 0.09 - ETA: 1:14 - loss: 3.6487 - accuracy: 0.09 - ETA: 1:14 - loss: 3.6491 - accuracy: 0.09 - ETA: 1:13 - loss: 3.6484 - accuracy: 0.09 - ETA: 1:13 - loss: 3.6485 - accuracy: 0.09 - ETA: 1:12 - loss: 3.6496 - accuracy: 0.09 - ETA: 1:12 - loss: 3.6494 - accuracy: 0.09 - ETA: 1:11 - loss: 3.6496 - accuracy: 0.09 - ETA: 1:11 - loss: 3.6496 - accuracy: 0.09 - ETA: 1:11 - loss: 3.6497 - accuracy: 0.09 - ETA: 1:10 - loss: 3.6498 - accuracy: 0.09 - ETA: 1:10 - loss: 3.6500 - accuracy: 0.09 - ETA: 1:09 - loss: 3.6506 - accuracy: 0.09 - ETA: 1:09 - loss: 3.6504 - accuracy: 0.09 - ETA: 1:08 - loss: 3.6510 - accuracy: 0.09 - ETA: 1:08 - loss: 3.6508 - accuracy: 0.09 - ETA: 1:07 - loss: 3.6506 - accuracy: 0.09 - ETA: 1:07 - loss: 3.6507 - accuracy: 0.09 - ETA: 1:06 - loss: 3.6502 - accuracy: 0.09 - ETA: 1:06 - loss: 3.6506 - accuracy: 0.09 - ETA: 1:06 - loss: 3.6500 - accuracy: 0.09 - ETA: 1:05 - loss: 3.6505 - accuracy: 0.0932"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:05 - loss: 3.6500 - accuracy: 0.09 - ETA: 1:04 - loss: 3.6503 - accuracy: 0.09 - ETA: 1:04 - loss: 3.6501 - accuracy: 0.09 - ETA: 1:03 - loss: 3.6497 - accuracy: 0.09 - ETA: 1:03 - loss: 3.6494 - accuracy: 0.09 - ETA: 1:02 - loss: 3.6492 - accuracy: 0.09 - ETA: 1:02 - loss: 3.6492 - accuracy: 0.09 - ETA: 1:01 - loss: 3.6497 - accuracy: 0.09 - ETA: 1:01 - loss: 3.6495 - accuracy: 0.09 - ETA: 1:01 - loss: 3.6496 - accuracy: 0.09 - ETA: 1:00 - loss: 3.6499 - accuracy: 0.09 - ETA: 1:00 - loss: 3.6501 - accuracy: 0.09 - ETA: 59s - loss: 3.6503 - accuracy: 0.0930 - ETA: 59s - loss: 3.6498 - accuracy: 0.093 - ETA: 58s - loss: 3.6501 - accuracy: 0.093 - ETA: 58s - loss: 3.6501 - accuracy: 0.093 - ETA: 57s - loss: 3.6495 - accuracy: 0.093 - ETA: 57s - loss: 3.6491 - accuracy: 0.093 - ETA: 56s - loss: 3.6493 - accuracy: 0.093 - ETA: 56s - loss: 3.6487 - accuracy: 0.093 - ETA: 56s - loss: 3.6476 - accuracy: 0.093 - ETA: 55s - loss: 3.6472 - accuracy: 0.093 - ETA: 55s - loss: 3.6463 - accuracy: 0.093 - ETA: 54s - loss: 3.6461 - accuracy: 0.093 - ETA: 54s - loss: 3.6456 - accuracy: 0.093 - ETA: 53s - loss: 3.6449 - accuracy: 0.093 - ETA: 53s - loss: 3.6449 - accuracy: 0.093 - ETA: 52s - loss: 3.6446 - accuracy: 0.093 - ETA: 52s - loss: 3.6450 - accuracy: 0.093 - ETA: 52s - loss: 3.6453 - accuracy: 0.093 - ETA: 51s - loss: 3.6448 - accuracy: 0.093 - ETA: 51s - loss: 3.6448 - accuracy: 0.093 - ETA: 50s - loss: 3.6446 - accuracy: 0.093 - ETA: 50s - loss: 3.6446 - accuracy: 0.093 - ETA: 49s - loss: 3.6444 - accuracy: 0.093 - ETA: 49s - loss: 3.6442 - accuracy: 0.093 - ETA: 48s - loss: 3.6437 - accuracy: 0.093 - ETA: 48s - loss: 3.6441 - accuracy: 0.093 - ETA: 47s - loss: 3.6440 - accuracy: 0.093 - ETA: 47s - loss: 3.6441 - accuracy: 0.093 - ETA: 47s - loss: 3.6441 - accuracy: 0.093 - ETA: 46s - loss: 3.6443 - accuracy: 0.093 - ETA: 46s - loss: 3.6442 - accuracy: 0.093 - ETA: 45s - loss: 3.6440 - accuracy: 0.093 - ETA: 45s - loss: 3.6442 - accuracy: 0.093 - ETA: 44s - loss: 3.6445 - accuracy: 0.093 - ETA: 44s - loss: 3.6448 - accuracy: 0.093 - ETA: 43s - loss: 3.6452 - accuracy: 0.093 - ETA: 43s - loss: 3.6451 - accuracy: 0.093 - ETA: 43s - loss: 3.6448 - accuracy: 0.093 - ETA: 42s - loss: 3.6447 - accuracy: 0.093 - ETA: 42s - loss: 3.6449 - accuracy: 0.093 - ETA: 41s - loss: 3.6453 - accuracy: 0.093 - ETA: 41s - loss: 3.6448 - accuracy: 0.093 - ETA: 40s - loss: 3.6446 - accuracy: 0.093 - ETA: 40s - loss: 3.6447 - accuracy: 0.093 - ETA: 39s - loss: 3.6446 - accuracy: 0.093 - ETA: 39s - loss: 3.6443 - accuracy: 0.093 - ETA: 38s - loss: 3.6448 - accuracy: 0.093 - ETA: 38s - loss: 3.6448 - accuracy: 0.093 - ETA: 38s - loss: 3.6444 - accuracy: 0.093 - ETA: 37s - loss: 3.6442 - accuracy: 0.093 - ETA: 37s - loss: 3.6444 - accuracy: 0.093 - ETA: 36s - loss: 3.6441 - accuracy: 0.093 - ETA: 36s - loss: 3.6444 - accuracy: 0.093 - ETA: 35s - loss: 3.6441 - accuracy: 0.093 - ETA: 35s - loss: 3.6438 - accuracy: 0.093 - ETA: 34s - loss: 3.6435 - accuracy: 0.093 - ETA: 34s - loss: 3.6434 - accuracy: 0.093 - ETA: 33s - loss: 3.6434 - accuracy: 0.093 - ETA: 33s - loss: 3.6434 - accuracy: 0.093 - ETA: 33s - loss: 3.6432 - accuracy: 0.092 - ETA: 32s - loss: 3.6427 - accuracy: 0.092 - ETA: 32s - loss: 3.6427 - accuracy: 0.092 - ETA: 31s - loss: 3.6431 - accuracy: 0.092 - ETA: 31s - loss: 3.6430 - accuracy: 0.092 - ETA: 30s - loss: 3.6434 - accuracy: 0.092 - ETA: 30s - loss: 3.6437 - accuracy: 0.092 - ETA: 29s - loss: 3.6434 - accuracy: 0.092 - ETA: 29s - loss: 3.6430 - accuracy: 0.092 - ETA: 28s - loss: 3.6435 - accuracy: 0.092 - ETA: 28s - loss: 3.6433 - accuracy: 0.092 - ETA: 28s - loss: 3.6434 - accuracy: 0.092 - ETA: 27s - loss: 3.6438 - accuracy: 0.092 - ETA: 27s - loss: 3.6437 - accuracy: 0.092 - ETA: 26s - loss: 3.6439 - accuracy: 0.092 - ETA: 26s - loss: 3.6441 - accuracy: 0.092 - ETA: 25s - loss: 3.6443 - accuracy: 0.092 - ETA: 25s - loss: 3.6443 - accuracy: 0.092 - ETA: 24s - loss: 3.6439 - accuracy: 0.092 - ETA: 24s - loss: 3.6436 - accuracy: 0.092 - ETA: 24s - loss: 3.6440 - accuracy: 0.092 - ETA: 23s - loss: 3.6439 - accuracy: 0.092 - ETA: 23s - loss: 3.6436 - accuracy: 0.092 - ETA: 22s - loss: 3.6435 - accuracy: 0.092 - ETA: 22s - loss: 3.6438 - accuracy: 0.092 - ETA: 21s - loss: 3.6439 - accuracy: 0.092 - ETA: 21s - loss: 3.6437 - accuracy: 0.092 - ETA: 20s - loss: 3.6435 - accuracy: 0.092 - ETA: 20s - loss: 3.6436 - accuracy: 0.092 - ETA: 19s - loss: 3.6437 - accuracy: 0.092 - ETA: 19s - loss: 3.6436 - accuracy: 0.092 - ETA: 19s - loss: 3.6435 - accuracy: 0.092 - ETA: 18s - loss: 3.6438 - accuracy: 0.092 - ETA: 18s - loss: 3.6436 - accuracy: 0.092 - ETA: 17s - loss: 3.6433 - accuracy: 0.092 - ETA: 17s - loss: 3.6435 - accuracy: 0.092 - ETA: 16s - loss: 3.6433 - accuracy: 0.092 - ETA: 16s - loss: 3.6435 - accuracy: 0.092 - ETA: 15s - loss: 3.6432 - accuracy: 0.092 - ETA: 15s - loss: 3.6430 - accuracy: 0.092 - ETA: 14s - loss: 3.6436 - accuracy: 0.092 - ETA: 14s - loss: 3.6435 - accuracy: 0.092 - ETA: 14s - loss: 3.6436 - accuracy: 0.092 - ETA: 13s - loss: 3.6433 - accuracy: 0.092 - ETA: 13s - loss: 3.6431 - accuracy: 0.092 - ETA: 12s - loss: 3.6432 - accuracy: 0.092 - ETA: 12s - loss: 3.6434 - accuracy: 0.092 - ETA: 11s - loss: 3.6433 - accuracy: 0.092 - ETA: 11s - loss: 3.6432 - accuracy: 0.092 - ETA: 10s - loss: 3.6434 - accuracy: 0.092 - ETA: 10s - loss: 3.6440 - accuracy: 0.092 - ETA: 10s - loss: 3.6439 - accuracy: 0.092 - ETA: 9s - loss: 3.6434 - accuracy: 0.092 - ETA: 9s - loss: 3.6438 - accuracy: 0.09 - ETA: 8s - loss: 3.6435 - accuracy: 0.09 - ETA: 8s - loss: 3.6433 - accuracy: 0.09 - ETA: 7s - loss: 3.6434 - accuracy: 0.09 - ETA: 7s - loss: 3.6434 - accuracy: 0.09 - ETA: 6s - loss: 3.6439 - accuracy: 0.09 - ETA: 6s - loss: 3.6442 - accuracy: 0.09 - ETA: 6s - loss: 3.6444 - accuracy: 0.09 - ETA: 5s - loss: 3.6444 - accuracy: 0.09 - ETA: 5s - loss: 3.6448 - accuracy: 0.09 - ETA: 4s - loss: 3.6448 - accuracy: 0.09 - ETA: 4s - loss: 3.6447 - accuracy: 0.09 - ETA: 3s - loss: 3.6445 - accuracy: 0.09 - ETA: 3s - loss: 3.6447 - accuracy: 0.09 - ETA: 2s - loss: 3.6450 - accuracy: 0.09 - ETA: 2s - loss: 3.6452 - accuracy: 0.09 - ETA: 1s - loss: 3.6451 - accuracy: 0.09 - ETA: 1s - loss: 3.6452 - accuracy: 0.09 - ETA: 0s - loss: 3.6456 - accuracy: 0.09 - ETA: 0s - loss: 3.6456 - accuracy: 0.09 - ETA: 0s - loss: 3.6449 - accuracy: 0.09 - 175s 4ms/step - loss: 3.6450 - accuracy: 0.0922 - val_loss: 3.9399 - val_accuracy: 0.0411\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:00 - loss: 3.5978 - accuracy: 0.09 - ETA: 3:58 - loss: 3.6865 - accuracy: 0.08 - ETA: 3:59 - loss: 3.6270 - accuracy: 0.09 - ETA: 4:01 - loss: 3.6366 - accuracy: 0.09 - ETA: 3:57 - loss: 3.6532 - accuracy: 0.09 - ETA: 3:56 - loss: 3.6373 - accuracy: 0.09 - ETA: 3:56 - loss: 3.6414 - accuracy: 0.08 - ETA: 3:56 - loss: 3.6473 - accuracy: 0.08 - ETA: 3:55 - loss: 3.6668 - accuracy: 0.08 - ETA: 3:55 - loss: 3.6712 - accuracy: 0.08 - ETA: 3:54 - loss: 3.6471 - accuracy: 0.09 - ETA: 3:53 - loss: 3.6507 - accuracy: 0.09 - ETA: 3:53 - loss: 3.6492 - accuracy: 0.09 - ETA: 3:53 - loss: 3.6530 - accuracy: 0.08 - ETA: 3:52 - loss: 3.6512 - accuracy: 0.09 - ETA: 3:51 - loss: 3.6393 - accuracy: 0.09 - ETA: 3:50 - loss: 3.6447 - accuracy: 0.09 - ETA: 3:50 - loss: 3.6433 - accuracy: 0.09 - ETA: 3:49 - loss: 3.6343 - accuracy: 0.09 - ETA: 3:48 - loss: 3.6335 - accuracy: 0.09 - ETA: 3:47 - loss: 3.6352 - accuracy: 0.09 - ETA: 3:47 - loss: 3.6408 - accuracy: 0.09 - ETA: 3:46 - loss: 3.6453 - accuracy: 0.09 - ETA: 3:45 - loss: 3.6485 - accuracy: 0.09 - ETA: 3:44 - loss: 3.6486 - accuracy: 0.09 - ETA: 3:44 - loss: 3.6484 - accuracy: 0.09 - ETA: 3:44 - loss: 3.6494 - accuracy: 0.09 - ETA: 3:43 - loss: 3.6528 - accuracy: 0.09 - ETA: 3:43 - loss: 3.6570 - accuracy: 0.09 - ETA: 3:42 - loss: 3.6545 - accuracy: 0.09 - ETA: 3:41 - loss: 3.6525 - accuracy: 0.09 - ETA: 3:40 - loss: 3.6519 - accuracy: 0.09 - ETA: 3:39 - loss: 3.6519 - accuracy: 0.09 - ETA: 3:39 - loss: 3.6532 - accuracy: 0.09 - ETA: 3:38 - loss: 3.6508 - accuracy: 0.09 - ETA: 3:37 - loss: 3.6479 - accuracy: 0.09 - ETA: 3:36 - loss: 3.6558 - accuracy: 0.09 - ETA: 3:36 - loss: 3.6545 - accuracy: 0.09 - ETA: 3:35 - loss: 3.6533 - accuracy: 0.09 - ETA: 3:34 - loss: 3.6495 - accuracy: 0.09 - ETA: 3:33 - loss: 3.6453 - accuracy: 0.09 - ETA: 3:32 - loss: 3.6449 - accuracy: 0.09 - ETA: 3:31 - loss: 3.6450 - accuracy: 0.09 - ETA: 3:30 - loss: 3.6452 - accuracy: 0.09 - ETA: 3:30 - loss: 3.6454 - accuracy: 0.09 - ETA: 3:29 - loss: 3.6455 - accuracy: 0.09 - ETA: 3:29 - loss: 3.6458 - accuracy: 0.09 - ETA: 3:28 - loss: 3.6448 - accuracy: 0.09 - ETA: 3:27 - loss: 3.6414 - accuracy: 0.09 - ETA: 3:26 - loss: 3.6417 - accuracy: 0.09 - ETA: 3:26 - loss: 3.6410 - accuracy: 0.09 - ETA: 3:25 - loss: 3.6422 - accuracy: 0.09 - ETA: 3:24 - loss: 3.6419 - accuracy: 0.09 - ETA: 3:23 - loss: 3.6428 - accuracy: 0.09 - ETA: 3:23 - loss: 3.6433 - accuracy: 0.09 - ETA: 3:22 - loss: 3.6436 - accuracy: 0.09 - ETA: 3:21 - loss: 3.6435 - accuracy: 0.09 - ETA: 3:20 - loss: 3.6445 - accuracy: 0.09 - ETA: 3:20 - loss: 3.6419 - accuracy: 0.09 - ETA: 3:19 - loss: 3.6441 - accuracy: 0.09 - ETA: 3:18 - loss: 3.6448 - accuracy: 0.09 - ETA: 3:17 - loss: 3.6462 - accuracy: 0.09 - ETA: 3:16 - loss: 3.6459 - accuracy: 0.09 - ETA: 3:16 - loss: 3.6464 - accuracy: 0.09 - ETA: 3:15 - loss: 3.6466 - accuracy: 0.09 - ETA: 3:14 - loss: 3.6455 - accuracy: 0.09 - ETA: 3:14 - loss: 3.6439 - accuracy: 0.09 - ETA: 3:13 - loss: 3.6462 - accuracy: 0.09 - ETA: 3:12 - loss: 3.6461 - accuracy: 0.09 - ETA: 3:12 - loss: 3.6477 - accuracy: 0.09 - ETA: 3:11 - loss: 3.6481 - accuracy: 0.09 - ETA: 3:11 - loss: 3.6456 - accuracy: 0.09 - ETA: 3:10 - loss: 3.6439 - accuracy: 0.09 - ETA: 3:09 - loss: 3.6416 - accuracy: 0.09 - ETA: 3:08 - loss: 3.6394 - accuracy: 0.09 - ETA: 3:08 - loss: 3.6384 - accuracy: 0.09 - ETA: 3:07 - loss: 3.6371 - accuracy: 0.09 - ETA: 3:06 - loss: 3.6370 - accuracy: 0.09 - ETA: 3:05 - loss: 3.6359 - accuracy: 0.09 - ETA: 3:05 - loss: 3.6355 - accuracy: 0.09 - ETA: 3:04 - loss: 3.6341 - accuracy: 0.09 - ETA: 3:03 - loss: 3.6352 - accuracy: 0.09 - ETA: 3:02 - loss: 3.6348 - accuracy: 0.09 - ETA: 3:02 - loss: 3.6340 - accuracy: 0.09 - ETA: 3:01 - loss: 3.6333 - accuracy: 0.09 - ETA: 3:00 - loss: 3.6322 - accuracy: 0.09 - ETA: 2:59 - loss: 3.6332 - accuracy: 0.09 - ETA: 2:58 - loss: 3.6340 - accuracy: 0.09 - ETA: 2:58 - loss: 3.6332 - accuracy: 0.09 - ETA: 2:57 - loss: 3.6342 - accuracy: 0.09 - ETA: 2:56 - loss: 3.6330 - accuracy: 0.09 - ETA: 2:56 - loss: 3.6328 - accuracy: 0.09 - ETA: 2:55 - loss: 3.6329 - accuracy: 0.09 - ETA: 2:54 - loss: 3.6330 - accuracy: 0.09 - ETA: 2:53 - loss: 3.6338 - accuracy: 0.09 - ETA: 2:53 - loss: 3.6331 - accuracy: 0.09 - ETA: 2:52 - loss: 3.6328 - accuracy: 0.09 - ETA: 2:51 - loss: 3.6333 - accuracy: 0.09 - ETA: 2:50 - loss: 3.6354 - accuracy: 0.09 - ETA: 2:50 - loss: 3.6354 - accuracy: 0.09 - ETA: 2:49 - loss: 3.6353 - accuracy: 0.09 - ETA: 2:48 - loss: 3.6344 - accuracy: 0.09 - ETA: 2:47 - loss: 3.6339 - accuracy: 0.09 - ETA: 2:47 - loss: 3.6345 - accuracy: 0.09 - ETA: 2:46 - loss: 3.6347 - accuracy: 0.09 - ETA: 2:45 - loss: 3.6344 - accuracy: 0.09 - ETA: 2:44 - loss: 3.6347 - accuracy: 0.09 - ETA: 2:44 - loss: 3.6351 - accuracy: 0.09 - ETA: 2:43 - loss: 3.6345 - accuracy: 0.09 - ETA: 2:42 - loss: 3.6347 - accuracy: 0.09 - ETA: 2:41 - loss: 3.6337 - accuracy: 0.09 - ETA: 2:41 - loss: 3.6335 - accuracy: 0.09 - ETA: 2:40 - loss: 3.6343 - accuracy: 0.09 - ETA: 2:39 - loss: 3.6341 - accuracy: 0.09 - ETA: 2:39 - loss: 3.6337 - accuracy: 0.09 - ETA: 2:38 - loss: 3.6343 - accuracy: 0.09 - ETA: 2:37 - loss: 3.6344 - accuracy: 0.09 - ETA: 2:36 - loss: 3.6330 - accuracy: 0.09 - ETA: 2:36 - loss: 3.6341 - accuracy: 0.09 - ETA: 2:35 - loss: 3.6333 - accuracy: 0.09 - ETA: 2:34 - loss: 3.6325 - accuracy: 0.09 - ETA: 2:34 - loss: 3.6327 - accuracy: 0.09 - ETA: 2:33 - loss: 3.6319 - accuracy: 0.09 - ETA: 2:32 - loss: 3.6320 - accuracy: 0.09 - ETA: 2:31 - loss: 3.6314 - accuracy: 0.09 - ETA: 2:31 - loss: 3.6315 - accuracy: 0.09 - ETA: 2:30 - loss: 3.6317 - accuracy: 0.09 - ETA: 2:29 - loss: 3.6321 - accuracy: 0.09 - ETA: 2:28 - loss: 3.6320 - accuracy: 0.09 - ETA: 2:28 - loss: 3.6318 - accuracy: 0.09 - ETA: 2:27 - loss: 3.6335 - accuracy: 0.09 - ETA: 2:26 - loss: 3.6326 - accuracy: 0.09 - ETA: 2:25 - loss: 3.6327 - accuracy: 0.09 - ETA: 2:25 - loss: 3.6324 - accuracy: 0.09 - ETA: 2:24 - loss: 3.6319 - accuracy: 0.09 - ETA: 2:23 - loss: 3.6321 - accuracy: 0.09 - ETA: 2:22 - loss: 3.6325 - accuracy: 0.09 - ETA: 2:22 - loss: 3.6326 - accuracy: 0.09 - ETA: 2:21 - loss: 3.6329 - accuracy: 0.09 - ETA: 2:20 - loss: 3.6314 - accuracy: 0.09 - ETA: 2:19 - loss: 3.6313 - accuracy: 0.09 - ETA: 2:19 - loss: 3.6308 - accuracy: 0.09 - ETA: 2:18 - loss: 3.6300 - accuracy: 0.09 - ETA: 2:17 - loss: 3.6311 - accuracy: 0.09 - ETA: 2:16 - loss: 3.6314 - accuracy: 0.09 - ETA: 2:16 - loss: 3.6328 - accuracy: 0.09 - ETA: 2:15 - loss: 3.6319 - accuracy: 0.09 - ETA: 2:14 - loss: 3.6323 - accuracy: 0.09 - ETA: 2:13 - loss: 3.6320 - accuracy: 0.09 - ETA: 2:12 - loss: 3.6316 - accuracy: 0.09 - ETA: 2:12 - loss: 3.6319 - accuracy: 0.09 - ETA: 2:11 - loss: 3.6318 - accuracy: 0.09 - ETA: 2:10 - loss: 3.6321 - accuracy: 0.09 - ETA: 2:10 - loss: 3.6324 - accuracy: 0.09 - ETA: 2:09 - loss: 3.6315 - accuracy: 0.09 - ETA: 2:08 - loss: 3.6317 - accuracy: 0.09 - ETA: 2:07 - loss: 3.6318 - accuracy: 0.09 - ETA: 2:07 - loss: 3.6314 - accuracy: 0.09 - ETA: 2:06 - loss: 3.6311 - accuracy: 0.09 - ETA: 2:05 - loss: 3.6317 - accuracy: 0.09 - ETA: 2:05 - loss: 3.6315 - accuracy: 0.09 - ETA: 2:04 - loss: 3.6318 - accuracy: 0.09 - ETA: 2:03 - loss: 3.6320 - accuracy: 0.09 - ETA: 2:03 - loss: 3.6317 - accuracy: 0.09 - ETA: 2:02 - loss: 3.6317 - accuracy: 0.09 - ETA: 2:01 - loss: 3.6313 - accuracy: 0.09 - ETA: 2:00 - loss: 3.6308 - accuracy: 0.09 - ETA: 2:00 - loss: 3.6296 - accuracy: 0.09 - ETA: 1:59 - loss: 3.6304 - accuracy: 0.09 - ETA: 1:58 - loss: 3.6307 - accuracy: 0.09 - ETA: 1:57 - loss: 3.6306 - accuracy: 0.09 - ETA: 1:57 - loss: 3.6304 - accuracy: 0.09 - ETA: 1:56 - loss: 3.6302 - accuracy: 0.09 - ETA: 1:55 - loss: 3.6297 - accuracy: 0.09 - ETA: 1:54 - loss: 3.6291 - accuracy: 0.09 - ETA: 1:54 - loss: 3.6282 - accuracy: 0.09 - ETA: 1:53 - loss: 3.6281 - accuracy: 0.09 - ETA: 1:52 - loss: 3.6273 - accuracy: 0.09 - ETA: 1:51 - loss: 3.6274 - accuracy: 0.09 - ETA: 1:51 - loss: 3.6264 - accuracy: 0.09 - ETA: 1:50 - loss: 3.6261 - accuracy: 0.09 - ETA: 1:49 - loss: 3.6269 - accuracy: 0.09 - ETA: 1:48 - loss: 3.6260 - accuracy: 0.09 - ETA: 1:48 - loss: 3.6258 - accuracy: 0.09 - ETA: 1:47 - loss: 3.6261 - accuracy: 0.09 - ETA: 1:46 - loss: 3.6263 - accuracy: 0.0954"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.6260 - accuracy: 0.09 - ETA: 1:45 - loss: 3.6262 - accuracy: 0.09 - ETA: 1:44 - loss: 3.6263 - accuracy: 0.09 - ETA: 1:43 - loss: 3.6270 - accuracy: 0.09 - ETA: 1:43 - loss: 3.6272 - accuracy: 0.09 - ETA: 1:42 - loss: 3.6269 - accuracy: 0.09 - ETA: 1:41 - loss: 3.6258 - accuracy: 0.09 - ETA: 1:40 - loss: 3.6257 - accuracy: 0.09 - ETA: 1:40 - loss: 3.6263 - accuracy: 0.09 - ETA: 1:39 - loss: 3.6257 - accuracy: 0.09 - ETA: 1:38 - loss: 3.6254 - accuracy: 0.09 - ETA: 1:37 - loss: 3.6247 - accuracy: 0.09 - ETA: 1:37 - loss: 3.6248 - accuracy: 0.09 - ETA: 1:36 - loss: 3.6248 - accuracy: 0.09 - ETA: 1:35 - loss: 3.6248 - accuracy: 0.09 - ETA: 1:34 - loss: 3.6247 - accuracy: 0.09 - ETA: 1:34 - loss: 3.6252 - accuracy: 0.09 - ETA: 1:33 - loss: 3.6247 - accuracy: 0.09 - ETA: 1:32 - loss: 3.6246 - accuracy: 0.09 - ETA: 1:32 - loss: 3.6239 - accuracy: 0.09 - ETA: 1:31 - loss: 3.6237 - accuracy: 0.09 - ETA: 1:30 - loss: 3.6236 - accuracy: 0.09 - ETA: 1:29 - loss: 3.6240 - accuracy: 0.09 - ETA: 1:29 - loss: 3.6245 - accuracy: 0.09 - ETA: 1:28 - loss: 3.6243 - accuracy: 0.09 - ETA: 1:27 - loss: 3.6239 - accuracy: 0.09 - ETA: 1:26 - loss: 3.6240 - accuracy: 0.09 - ETA: 1:26 - loss: 3.6237 - accuracy: 0.09 - ETA: 1:25 - loss: 3.6241 - accuracy: 0.09 - ETA: 1:24 - loss: 3.6239 - accuracy: 0.09 - ETA: 1:23 - loss: 3.6238 - accuracy: 0.09 - ETA: 1:23 - loss: 3.6237 - accuracy: 0.09 - ETA: 1:22 - loss: 3.6239 - accuracy: 0.09 - ETA: 1:21 - loss: 3.6232 - accuracy: 0.09 - ETA: 1:20 - loss: 3.6229 - accuracy: 0.09 - ETA: 1:20 - loss: 3.6232 - accuracy: 0.09 - ETA: 1:19 - loss: 3.6231 - accuracy: 0.09 - ETA: 1:18 - loss: 3.6234 - accuracy: 0.09 - ETA: 1:18 - loss: 3.6227 - accuracy: 0.09 - ETA: 1:17 - loss: 3.6227 - accuracy: 0.09 - ETA: 1:16 - loss: 3.6228 - accuracy: 0.09 - ETA: 1:15 - loss: 3.6224 - accuracy: 0.09 - ETA: 1:15 - loss: 3.6220 - accuracy: 0.09 - ETA: 1:14 - loss: 3.6213 - accuracy: 0.09 - ETA: 1:13 - loss: 3.6212 - accuracy: 0.09 - ETA: 1:12 - loss: 3.6213 - accuracy: 0.09 - ETA: 1:12 - loss: 3.6210 - accuracy: 0.09 - ETA: 1:11 - loss: 3.6212 - accuracy: 0.09 - ETA: 1:10 - loss: 3.6219 - accuracy: 0.09 - ETA: 1:10 - loss: 3.6215 - accuracy: 0.09 - ETA: 1:09 - loss: 3.6216 - accuracy: 0.09 - ETA: 1:08 - loss: 3.6213 - accuracy: 0.09 - ETA: 1:07 - loss: 3.6210 - accuracy: 0.09 - ETA: 1:07 - loss: 3.6206 - accuracy: 0.09 - ETA: 1:06 - loss: 3.6206 - accuracy: 0.09 - ETA: 1:05 - loss: 3.6207 - accuracy: 0.09 - ETA: 1:04 - loss: 3.6202 - accuracy: 0.09 - ETA: 1:04 - loss: 3.6201 - accuracy: 0.09 - ETA: 1:03 - loss: 3.6206 - accuracy: 0.09 - ETA: 1:02 - loss: 3.6199 - accuracy: 0.09 - ETA: 1:01 - loss: 3.6195 - accuracy: 0.09 - ETA: 1:01 - loss: 3.6189 - accuracy: 0.09 - ETA: 1:00 - loss: 3.6187 - accuracy: 0.09 - ETA: 59s - loss: 3.6187 - accuracy: 0.0958 - ETA: 59s - loss: 3.6188 - accuracy: 0.095 - ETA: 58s - loss: 3.6191 - accuracy: 0.095 - ETA: 57s - loss: 3.6194 - accuracy: 0.095 - ETA: 56s - loss: 3.6194 - accuracy: 0.095 - ETA: 56s - loss: 3.6194 - accuracy: 0.095 - ETA: 55s - loss: 3.6198 - accuracy: 0.095 - ETA: 54s - loss: 3.6196 - accuracy: 0.095 - ETA: 53s - loss: 3.6195 - accuracy: 0.095 - ETA: 53s - loss: 3.6193 - accuracy: 0.095 - ETA: 52s - loss: 3.6190 - accuracy: 0.095 - ETA: 51s - loss: 3.6185 - accuracy: 0.095 - ETA: 50s - loss: 3.6187 - accuracy: 0.095 - ETA: 50s - loss: 3.6184 - accuracy: 0.095 - ETA: 49s - loss: 3.6187 - accuracy: 0.095 - ETA: 48s - loss: 3.6185 - accuracy: 0.096 - ETA: 47s - loss: 3.6181 - accuracy: 0.096 - ETA: 47s - loss: 3.6183 - accuracy: 0.096 - ETA: 46s - loss: 3.6184 - accuracy: 0.096 - ETA: 45s - loss: 3.6179 - accuracy: 0.096 - ETA: 44s - loss: 3.6178 - accuracy: 0.096 - ETA: 44s - loss: 3.6179 - accuracy: 0.096 - ETA: 43s - loss: 3.6181 - accuracy: 0.096 - ETA: 42s - loss: 3.6180 - accuracy: 0.096 - ETA: 41s - loss: 3.6178 - accuracy: 0.096 - ETA: 41s - loss: 3.6178 - accuracy: 0.096 - ETA: 40s - loss: 3.6178 - accuracy: 0.096 - ETA: 39s - loss: 3.6182 - accuracy: 0.095 - ETA: 39s - loss: 3.6182 - accuracy: 0.095 - ETA: 38s - loss: 3.6180 - accuracy: 0.095 - ETA: 37s - loss: 3.6183 - accuracy: 0.095 - ETA: 36s - loss: 3.6184 - accuracy: 0.095 - ETA: 36s - loss: 3.6183 - accuracy: 0.095 - ETA: 35s - loss: 3.6170 - accuracy: 0.096 - ETA: 34s - loss: 3.6169 - accuracy: 0.096 - ETA: 33s - loss: 3.6171 - accuracy: 0.096 - ETA: 33s - loss: 3.6167 - accuracy: 0.096 - ETA: 32s - loss: 3.6170 - accuracy: 0.096 - ETA: 31s - loss: 3.6168 - accuracy: 0.096 - ETA: 30s - loss: 3.6169 - accuracy: 0.096 - ETA: 30s - loss: 3.6172 - accuracy: 0.095 - ETA: 29s - loss: 3.6172 - accuracy: 0.096 - ETA: 28s - loss: 3.6169 - accuracy: 0.096 - ETA: 27s - loss: 3.6164 - accuracy: 0.096 - ETA: 27s - loss: 3.6164 - accuracy: 0.096 - ETA: 26s - loss: 3.6158 - accuracy: 0.096 - ETA: 25s - loss: 3.6156 - accuracy: 0.096 - ETA: 25s - loss: 3.6157 - accuracy: 0.096 - ETA: 24s - loss: 3.6154 - accuracy: 0.096 - ETA: 23s - loss: 3.6151 - accuracy: 0.096 - ETA: 22s - loss: 3.6154 - accuracy: 0.096 - ETA: 22s - loss: 3.6149 - accuracy: 0.096 - ETA: 21s - loss: 3.6150 - accuracy: 0.096 - ETA: 20s - loss: 3.6153 - accuracy: 0.096 - ETA: 19s - loss: 3.6152 - accuracy: 0.096 - ETA: 19s - loss: 3.6154 - accuracy: 0.096 - ETA: 18s - loss: 3.6150 - accuracy: 0.096 - ETA: 17s - loss: 3.6143 - accuracy: 0.096 - ETA: 16s - loss: 3.6145 - accuracy: 0.096 - ETA: 16s - loss: 3.6148 - accuracy: 0.096 - ETA: 15s - loss: 3.6144 - accuracy: 0.096 - ETA: 14s - loss: 3.6141 - accuracy: 0.097 - ETA: 14s - loss: 3.6140 - accuracy: 0.096 - ETA: 13s - loss: 3.6143 - accuracy: 0.096 - ETA: 12s - loss: 3.6144 - accuracy: 0.096 - ETA: 11s - loss: 3.6145 - accuracy: 0.096 - ETA: 11s - loss: 3.6145 - accuracy: 0.096 - ETA: 10s - loss: 3.6142 - accuracy: 0.097 - ETA: 9s - loss: 3.6139 - accuracy: 0.096 - ETA: 8s - loss: 3.6139 - accuracy: 0.09 - ETA: 8s - loss: 3.6136 - accuracy: 0.09 - ETA: 7s - loss: 3.6135 - accuracy: 0.09 - ETA: 6s - loss: 3.6135 - accuracy: 0.09 - ETA: 5s - loss: 3.6133 - accuracy: 0.09 - ETA: 5s - loss: 3.6133 - accuracy: 0.09 - ETA: 4s - loss: 3.6133 - accuracy: 0.09 - ETA: 3s - loss: 3.6130 - accuracy: 0.09 - ETA: 2s - loss: 3.6123 - accuracy: 0.09 - ETA: 2s - loss: 3.6125 - accuracy: 0.09 - ETA: 1s - loss: 3.6122 - accuracy: 0.09 - ETA: 0s - loss: 3.6121 - accuracy: 0.09 - ETA: 0s - loss: 3.6123 - accuracy: 0.09 - 262s 6ms/step - loss: 3.6123 - accuracy: 0.0971 - val_loss: 3.9150 - val_accuracy: 0.0286\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:59 - loss: 3.5406 - accuracy: 0.11 - ETA: 4:02 - loss: 3.6062 - accuracy: 0.10 - ETA: 4:01 - loss: 3.5785 - accuracy: 0.10 - ETA: 4:00 - loss: 3.5763 - accuracy: 0.11 - ETA: 3:59 - loss: 3.5928 - accuracy: 0.10 - ETA: 3:59 - loss: 3.6006 - accuracy: 0.09 - ETA: 3:58 - loss: 3.6180 - accuracy: 0.09 - ETA: 3:58 - loss: 3.6180 - accuracy: 0.09 - ETA: 3:57 - loss: 3.6055 - accuracy: 0.09 - ETA: 3:55 - loss: 3.6054 - accuracy: 0.09 - ETA: 3:56 - loss: 3.5972 - accuracy: 0.09 - ETA: 3:56 - loss: 3.5970 - accuracy: 0.09 - ETA: 3:55 - loss: 3.5852 - accuracy: 0.10 - ETA: 3:54 - loss: 3.5860 - accuracy: 0.10 - ETA: 3:53 - loss: 3.5863 - accuracy: 0.10 - ETA: 3:52 - loss: 3.5938 - accuracy: 0.10 - ETA: 3:51 - loss: 3.5908 - accuracy: 0.10 - ETA: 3:51 - loss: 3.5869 - accuracy: 0.10 - ETA: 3:51 - loss: 3.5892 - accuracy: 0.10 - ETA: 3:50 - loss: 3.5915 - accuracy: 0.10 - ETA: 3:49 - loss: 3.5956 - accuracy: 0.10 - ETA: 3:48 - loss: 3.5938 - accuracy: 0.10 - ETA: 3:47 - loss: 3.5940 - accuracy: 0.10 - ETA: 3:47 - loss: 3.5919 - accuracy: 0.10 - ETA: 3:45 - loss: 3.5910 - accuracy: 0.10 - ETA: 3:44 - loss: 3.5947 - accuracy: 0.10 - ETA: 3:44 - loss: 3.5988 - accuracy: 0.10 - ETA: 3:43 - loss: 3.5997 - accuracy: 0.10 - ETA: 3:42 - loss: 3.5981 - accuracy: 0.10 - ETA: 3:41 - loss: 3.6007 - accuracy: 0.10 - ETA: 3:41 - loss: 3.6042 - accuracy: 0.10 - ETA: 3:41 - loss: 3.6017 - accuracy: 0.10 - ETA: 3:40 - loss: 3.5986 - accuracy: 0.10 - ETA: 3:39 - loss: 3.6013 - accuracy: 0.10 - ETA: 3:39 - loss: 3.5999 - accuracy: 0.10 - ETA: 3:38 - loss: 3.6011 - accuracy: 0.10 - ETA: 3:38 - loss: 3.6036 - accuracy: 0.10 - ETA: 3:37 - loss: 3.6019 - accuracy: 0.10 - ETA: 3:36 - loss: 3.6007 - accuracy: 0.10 - ETA: 3:35 - loss: 3.6024 - accuracy: 0.10 - ETA: 3:35 - loss: 3.6014 - accuracy: 0.10 - ETA: 3:34 - loss: 3.6026 - accuracy: 0.10 - ETA: 3:33 - loss: 3.6000 - accuracy: 0.10 - ETA: 3:32 - loss: 3.6028 - accuracy: 0.10 - ETA: 3:31 - loss: 3.6025 - accuracy: 0.10 - ETA: 3:31 - loss: 3.6035 - accuracy: 0.10 - ETA: 3:30 - loss: 3.6014 - accuracy: 0.10 - ETA: 3:29 - loss: 3.6028 - accuracy: 0.09 - ETA: 3:28 - loss: 3.5995 - accuracy: 0.10 - ETA: 3:27 - loss: 3.5976 - accuracy: 0.10 - ETA: 3:27 - loss: 3.6007 - accuracy: 0.09 - ETA: 3:26 - loss: 3.6014 - accuracy: 0.09 - ETA: 3:25 - loss: 3.6023 - accuracy: 0.09 - ETA: 3:25 - loss: 3.6027 - accuracy: 0.09 - ETA: 3:24 - loss: 3.6039 - accuracy: 0.09 - ETA: 3:23 - loss: 3.6034 - accuracy: 0.09 - ETA: 3:22 - loss: 3.6045 - accuracy: 0.09 - ETA: 3:21 - loss: 3.6055 - accuracy: 0.09 - ETA: 3:21 - loss: 3.6041 - accuracy: 0.09 - ETA: 3:20 - loss: 3.6024 - accuracy: 0.09 - ETA: 3:19 - loss: 3.6031 - accuracy: 0.09 - ETA: 3:18 - loss: 3.6027 - accuracy: 0.09 - ETA: 3:17 - loss: 3.6019 - accuracy: 0.09 - ETA: 3:16 - loss: 3.6015 - accuracy: 0.09 - ETA: 3:16 - loss: 3.6015 - accuracy: 0.09 - ETA: 3:16 - loss: 3.6012 - accuracy: 0.09 - ETA: 3:15 - loss: 3.6011 - accuracy: 0.09 - ETA: 3:14 - loss: 3.6028 - accuracy: 0.09 - ETA: 3:13 - loss: 3.6019 - accuracy: 0.09 - ETA: 3:13 - loss: 3.6025 - accuracy: 0.09 - ETA: 3:12 - loss: 3.6017 - accuracy: 0.09 - ETA: 3:11 - loss: 3.6004 - accuracy: 0.09 - ETA: 3:11 - loss: 3.6010 - accuracy: 0.09 - ETA: 3:10 - loss: 3.5988 - accuracy: 0.09 - ETA: 3:09 - loss: 3.5965 - accuracy: 0.09 - ETA: 3:08 - loss: 3.5957 - accuracy: 0.09 - ETA: 3:07 - loss: 3.5963 - accuracy: 0.09 - ETA: 3:07 - loss: 3.5952 - accuracy: 0.09 - ETA: 3:06 - loss: 3.5969 - accuracy: 0.09 - ETA: 3:05 - loss: 3.5967 - accuracy: 0.09 - ETA: 3:04 - loss: 3.5976 - accuracy: 0.09 - ETA: 3:03 - loss: 3.5983 - accuracy: 0.09 - ETA: 3:03 - loss: 3.5977 - accuracy: 0.09 - ETA: 3:02 - loss: 3.5960 - accuracy: 0.09 - ETA: 3:01 - loss: 3.5957 - accuracy: 0.09 - ETA: 3:00 - loss: 3.5961 - accuracy: 0.09 - ETA: 3:00 - loss: 3.5946 - accuracy: 0.09 - ETA: 2:59 - loss: 3.5958 - accuracy: 0.09 - ETA: 2:58 - loss: 3.5958 - accuracy: 0.09 - ETA: 2:57 - loss: 3.5956 - accuracy: 0.09 - ETA: 2:57 - loss: 3.5950 - accuracy: 0.09 - ETA: 2:56 - loss: 3.5942 - accuracy: 0.09 - ETA: 2:55 - loss: 3.5938 - accuracy: 0.09 - ETA: 2:55 - loss: 3.5956 - accuracy: 0.09 - ETA: 2:54 - loss: 3.5961 - accuracy: 0.09 - ETA: 2:53 - loss: 3.5965 - accuracy: 0.09 - ETA: 2:52 - loss: 3.5969 - accuracy: 0.09 - ETA: 2:52 - loss: 3.5967 - accuracy: 0.09 - ETA: 2:51 - loss: 3.5967 - accuracy: 0.09 - ETA: 2:50 - loss: 3.5980 - accuracy: 0.09 - ETA: 2:49 - loss: 3.5962 - accuracy: 0.09 - ETA: 2:49 - loss: 3.5964 - accuracy: 0.09 - ETA: 2:48 - loss: 3.5957 - accuracy: 0.09 - ETA: 2:47 - loss: 3.5969 - accuracy: 0.09 - ETA: 2:46 - loss: 3.5970 - accuracy: 0.09 - ETA: 2:46 - loss: 3.5957 - accuracy: 0.09 - ETA: 2:45 - loss: 3.5951 - accuracy: 0.10 - ETA: 2:44 - loss: 3.5959 - accuracy: 0.10 - ETA: 2:43 - loss: 3.5953 - accuracy: 0.10 - ETA: 2:43 - loss: 3.5944 - accuracy: 0.10 - ETA: 2:42 - loss: 3.5953 - accuracy: 0.10 - ETA: 2:41 - loss: 3.5952 - accuracy: 0.10 - ETA: 2:40 - loss: 3.5947 - accuracy: 0.10 - ETA: 2:40 - loss: 3.5964 - accuracy: 0.10 - ETA: 2:39 - loss: 3.5974 - accuracy: 0.10 - ETA: 2:38 - loss: 3.5973 - accuracy: 0.09 - ETA: 2:37 - loss: 3.5968 - accuracy: 0.10 - ETA: 2:37 - loss: 3.5967 - accuracy: 0.10 - ETA: 2:36 - loss: 3.5967 - accuracy: 0.10 - ETA: 2:35 - loss: 3.5960 - accuracy: 0.10 - ETA: 2:34 - loss: 3.5951 - accuracy: 0.10 - ETA: 2:34 - loss: 3.5945 - accuracy: 0.10 - ETA: 2:33 - loss: 3.5957 - accuracy: 0.10 - ETA: 2:32 - loss: 3.5957 - accuracy: 0.10 - ETA: 2:31 - loss: 3.5944 - accuracy: 0.10 - ETA: 2:31 - loss: 3.5928 - accuracy: 0.10 - ETA: 2:30 - loss: 3.5919 - accuracy: 0.10 - ETA: 2:29 - loss: 3.5925 - accuracy: 0.10 - ETA: 2:28 - loss: 3.5931 - accuracy: 0.10 - ETA: 2:28 - loss: 3.5937 - accuracy: 0.10 - ETA: 2:27 - loss: 3.5944 - accuracy: 0.10 - ETA: 2:26 - loss: 3.5942 - accuracy: 0.10 - ETA: 2:25 - loss: 3.5938 - accuracy: 0.10 - ETA: 2:25 - loss: 3.5947 - accuracy: 0.10 - ETA: 2:24 - loss: 3.5941 - accuracy: 0.10 - ETA: 2:23 - loss: 3.5948 - accuracy: 0.10 - ETA: 2:22 - loss: 3.5951 - accuracy: 0.10 - ETA: 2:22 - loss: 3.5938 - accuracy: 0.10 - ETA: 2:21 - loss: 3.5935 - accuracy: 0.10 - ETA: 2:20 - loss: 3.5930 - accuracy: 0.10 - ETA: 2:20 - loss: 3.5924 - accuracy: 0.10 - ETA: 2:19 - loss: 3.5922 - accuracy: 0.10 - ETA: 2:18 - loss: 3.5922 - accuracy: 0.10 - ETA: 2:17 - loss: 3.5929 - accuracy: 0.10 - ETA: 2:17 - loss: 3.5936 - accuracy: 0.10 - ETA: 2:16 - loss: 3.5934 - accuracy: 0.10 - ETA: 2:15 - loss: 3.5927 - accuracy: 0.10 - ETA: 2:15 - loss: 3.5925 - accuracy: 0.10 - ETA: 2:14 - loss: 3.5916 - accuracy: 0.10 - ETA: 2:13 - loss: 3.5922 - accuracy: 0.10 - ETA: 2:12 - loss: 3.5925 - accuracy: 0.10 - ETA: 2:12 - loss: 3.5925 - accuracy: 0.10 - ETA: 2:11 - loss: 3.5920 - accuracy: 0.10 - ETA: 2:10 - loss: 3.5921 - accuracy: 0.10 - ETA: 2:09 - loss: 3.5924 - accuracy: 0.10 - ETA: 2:09 - loss: 3.5920 - accuracy: 0.10 - ETA: 2:08 - loss: 3.5922 - accuracy: 0.10 - ETA: 2:07 - loss: 3.5926 - accuracy: 0.10 - ETA: 2:07 - loss: 3.5919 - accuracy: 0.10 - ETA: 2:06 - loss: 3.5918 - accuracy: 0.10 - ETA: 2:05 - loss: 3.5903 - accuracy: 0.10 - ETA: 2:04 - loss: 3.5900 - accuracy: 0.10 - ETA: 2:04 - loss: 3.5915 - accuracy: 0.10 - ETA: 2:03 - loss: 3.5914 - accuracy: 0.10 - ETA: 2:02 - loss: 3.5914 - accuracy: 0.10 - ETA: 2:01 - loss: 3.5902 - accuracy: 0.10 - ETA: 2:01 - loss: 3.5897 - accuracy: 0.10 - ETA: 2:00 - loss: 3.5902 - accuracy: 0.10 - ETA: 1:59 - loss: 3.5900 - accuracy: 0.10 - ETA: 1:58 - loss: 3.5899 - accuracy: 0.10 - ETA: 1:58 - loss: 3.5896 - accuracy: 0.10 - ETA: 1:57 - loss: 3.5874 - accuracy: 0.10 - ETA: 1:56 - loss: 3.5885 - accuracy: 0.10 - ETA: 1:55 - loss: 3.5885 - accuracy: 0.10 - ETA: 1:55 - loss: 3.5891 - accuracy: 0.10 - ETA: 1:54 - loss: 3.5892 - accuracy: 0.10 - ETA: 1:53 - loss: 3.5893 - accuracy: 0.10 - ETA: 1:53 - loss: 3.5894 - accuracy: 0.10 - ETA: 1:52 - loss: 3.5887 - accuracy: 0.10 - ETA: 1:51 - loss: 3.5888 - accuracy: 0.10 - ETA: 1:50 - loss: 3.5892 - accuracy: 0.10 - ETA: 1:50 - loss: 3.5890 - accuracy: 0.10 - ETA: 1:49 - loss: 3.5891 - accuracy: 0.10 - ETA: 1:48 - loss: 3.5894 - accuracy: 0.10 - ETA: 1:47 - loss: 3.5890 - accuracy: 0.10 - ETA: 1:47 - loss: 3.5895 - accuracy: 0.1022"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:46 - loss: 3.5894 - accuracy: 0.10 - ETA: 1:45 - loss: 3.5895 - accuracy: 0.10 - ETA: 1:44 - loss: 3.5901 - accuracy: 0.10 - ETA: 1:44 - loss: 3.5914 - accuracy: 0.10 - ETA: 1:43 - loss: 3.5925 - accuracy: 0.10 - ETA: 1:42 - loss: 3.5929 - accuracy: 0.10 - ETA: 1:41 - loss: 3.5930 - accuracy: 0.10 - ETA: 1:41 - loss: 3.5932 - accuracy: 0.10 - ETA: 1:40 - loss: 3.5928 - accuracy: 0.10 - ETA: 1:39 - loss: 3.5927 - accuracy: 0.10 - ETA: 1:38 - loss: 3.5930 - accuracy: 0.10 - ETA: 1:38 - loss: 3.5933 - accuracy: 0.10 - ETA: 1:37 - loss: 3.5932 - accuracy: 0.10 - ETA: 1:36 - loss: 3.5926 - accuracy: 0.10 - ETA: 1:35 - loss: 3.5934 - accuracy: 0.10 - ETA: 1:35 - loss: 3.5932 - accuracy: 0.10 - ETA: 1:34 - loss: 3.5930 - accuracy: 0.10 - ETA: 1:33 - loss: 3.5926 - accuracy: 0.10 - ETA: 1:33 - loss: 3.5918 - accuracy: 0.10 - ETA: 1:32 - loss: 3.5919 - accuracy: 0.10 - ETA: 1:31 - loss: 3.5924 - accuracy: 0.10 - ETA: 1:30 - loss: 3.5917 - accuracy: 0.10 - ETA: 1:30 - loss: 3.5915 - accuracy: 0.10 - ETA: 1:29 - loss: 3.5911 - accuracy: 0.10 - ETA: 1:28 - loss: 3.5909 - accuracy: 0.10 - ETA: 1:27 - loss: 3.5908 - accuracy: 0.10 - ETA: 1:27 - loss: 3.5911 - accuracy: 0.10 - ETA: 1:26 - loss: 3.5908 - accuracy: 0.10 - ETA: 1:25 - loss: 3.5908 - accuracy: 0.10 - ETA: 1:24 - loss: 3.5909 - accuracy: 0.10 - ETA: 1:24 - loss: 3.5913 - accuracy: 0.10 - ETA: 1:23 - loss: 3.5916 - accuracy: 0.10 - ETA: 1:22 - loss: 3.5912 - accuracy: 0.10 - ETA: 1:21 - loss: 3.5913 - accuracy: 0.10 - ETA: 1:21 - loss: 3.5915 - accuracy: 0.10 - ETA: 1:20 - loss: 3.5916 - accuracy: 0.10 - ETA: 1:19 - loss: 3.5916 - accuracy: 0.10 - ETA: 1:18 - loss: 3.5914 - accuracy: 0.10 - ETA: 1:18 - loss: 3.5909 - accuracy: 0.10 - ETA: 1:17 - loss: 3.5901 - accuracy: 0.10 - ETA: 1:16 - loss: 3.5905 - accuracy: 0.10 - ETA: 1:15 - loss: 3.5904 - accuracy: 0.10 - ETA: 1:15 - loss: 3.5901 - accuracy: 0.10 - ETA: 1:14 - loss: 3.5906 - accuracy: 0.10 - ETA: 1:13 - loss: 3.5904 - accuracy: 0.10 - ETA: 1:13 - loss: 3.5909 - accuracy: 0.10 - ETA: 1:12 - loss: 3.5913 - accuracy: 0.10 - ETA: 1:11 - loss: 3.5910 - accuracy: 0.10 - ETA: 1:10 - loss: 3.5914 - accuracy: 0.10 - ETA: 1:10 - loss: 3.5918 - accuracy: 0.10 - ETA: 1:09 - loss: 3.5913 - accuracy: 0.10 - ETA: 1:08 - loss: 3.5913 - accuracy: 0.10 - ETA: 1:07 - loss: 3.5915 - accuracy: 0.10 - ETA: 1:07 - loss: 3.5912 - accuracy: 0.10 - ETA: 1:06 - loss: 3.5911 - accuracy: 0.10 - ETA: 1:05 - loss: 3.5909 - accuracy: 0.10 - ETA: 1:04 - loss: 3.5918 - accuracy: 0.10 - ETA: 1:04 - loss: 3.5923 - accuracy: 0.10 - ETA: 1:03 - loss: 3.5923 - accuracy: 0.10 - ETA: 1:02 - loss: 3.5922 - accuracy: 0.10 - ETA: 1:02 - loss: 3.5914 - accuracy: 0.10 - ETA: 1:01 - loss: 3.5917 - accuracy: 0.10 - ETA: 1:00 - loss: 3.5913 - accuracy: 0.10 - ETA: 59s - loss: 3.5917 - accuracy: 0.1013 - ETA: 59s - loss: 3.5910 - accuracy: 0.101 - ETA: 58s - loss: 3.5907 - accuracy: 0.101 - ETA: 57s - loss: 3.5911 - accuracy: 0.101 - ETA: 56s - loss: 3.5911 - accuracy: 0.101 - ETA: 56s - loss: 3.5915 - accuracy: 0.101 - ETA: 55s - loss: 3.5910 - accuracy: 0.101 - ETA: 54s - loss: 3.5908 - accuracy: 0.101 - ETA: 53s - loss: 3.5909 - accuracy: 0.101 - ETA: 53s - loss: 3.5907 - accuracy: 0.101 - ETA: 52s - loss: 3.5908 - accuracy: 0.101 - ETA: 51s - loss: 3.5908 - accuracy: 0.101 - ETA: 51s - loss: 3.5900 - accuracy: 0.102 - ETA: 50s - loss: 3.5899 - accuracy: 0.102 - ETA: 49s - loss: 3.5898 - accuracy: 0.102 - ETA: 48s - loss: 3.5899 - accuracy: 0.102 - ETA: 48s - loss: 3.5896 - accuracy: 0.102 - ETA: 47s - loss: 3.5888 - accuracy: 0.102 - ETA: 46s - loss: 3.5891 - accuracy: 0.102 - ETA: 45s - loss: 3.5886 - accuracy: 0.102 - ETA: 45s - loss: 3.5883 - accuracy: 0.102 - ETA: 44s - loss: 3.5878 - accuracy: 0.102 - ETA: 43s - loss: 3.5876 - accuracy: 0.102 - ETA: 42s - loss: 3.5874 - accuracy: 0.102 - ETA: 42s - loss: 3.5877 - accuracy: 0.102 - ETA: 41s - loss: 3.5880 - accuracy: 0.102 - ETA: 40s - loss: 3.5878 - accuracy: 0.102 - ETA: 39s - loss: 3.5883 - accuracy: 0.102 - ETA: 39s - loss: 3.5889 - accuracy: 0.102 - ETA: 38s - loss: 3.5895 - accuracy: 0.102 - ETA: 37s - loss: 3.5892 - accuracy: 0.102 - ETA: 36s - loss: 3.5889 - accuracy: 0.102 - ETA: 36s - loss: 3.5893 - accuracy: 0.102 - ETA: 35s - loss: 3.5893 - accuracy: 0.102 - ETA: 34s - loss: 3.5889 - accuracy: 0.102 - ETA: 34s - loss: 3.5886 - accuracy: 0.102 - ETA: 33s - loss: 3.5879 - accuracy: 0.102 - ETA: 32s - loss: 3.5872 - accuracy: 0.103 - ETA: 31s - loss: 3.5869 - accuracy: 0.103 - ETA: 31s - loss: 3.5871 - accuracy: 0.103 - ETA: 30s - loss: 3.5870 - accuracy: 0.103 - ETA: 29s - loss: 3.5866 - accuracy: 0.103 - ETA: 28s - loss: 3.5864 - accuracy: 0.103 - ETA: 28s - loss: 3.5861 - accuracy: 0.103 - ETA: 27s - loss: 3.5861 - accuracy: 0.103 - ETA: 26s - loss: 3.5856 - accuracy: 0.103 - ETA: 25s - loss: 3.5855 - accuracy: 0.103 - ETA: 25s - loss: 3.5853 - accuracy: 0.103 - ETA: 24s - loss: 3.5849 - accuracy: 0.103 - ETA: 23s - loss: 3.5848 - accuracy: 0.103 - ETA: 22s - loss: 3.5849 - accuracy: 0.103 - ETA: 22s - loss: 3.5848 - accuracy: 0.103 - ETA: 21s - loss: 3.5846 - accuracy: 0.103 - ETA: 20s - loss: 3.5847 - accuracy: 0.103 - ETA: 19s - loss: 3.5846 - accuracy: 0.103 - ETA: 19s - loss: 3.5851 - accuracy: 0.103 - ETA: 18s - loss: 3.5854 - accuracy: 0.103 - ETA: 17s - loss: 3.5858 - accuracy: 0.103 - ETA: 17s - loss: 3.5855 - accuracy: 0.103 - ETA: 16s - loss: 3.5849 - accuracy: 0.103 - ETA: 15s - loss: 3.5850 - accuracy: 0.103 - ETA: 14s - loss: 3.5849 - accuracy: 0.103 - ETA: 14s - loss: 3.5853 - accuracy: 0.103 - ETA: 13s - loss: 3.5855 - accuracy: 0.103 - ETA: 12s - loss: 3.5856 - accuracy: 0.103 - ETA: 11s - loss: 3.5858 - accuracy: 0.103 - ETA: 11s - loss: 3.5855 - accuracy: 0.103 - ETA: 10s - loss: 3.5858 - accuracy: 0.103 - ETA: 9s - loss: 3.5857 - accuracy: 0.103 - ETA: 8s - loss: 3.5855 - accuracy: 0.10 - ETA: 8s - loss: 3.5853 - accuracy: 0.10 - ETA: 7s - loss: 3.5854 - accuracy: 0.10 - ETA: 6s - loss: 3.5853 - accuracy: 0.10 - ETA: 5s - loss: 3.5857 - accuracy: 0.10 - ETA: 5s - loss: 3.5859 - accuracy: 0.10 - ETA: 4s - loss: 3.5857 - accuracy: 0.10 - ETA: 3s - loss: 3.5858 - accuracy: 0.10 - ETA: 3s - loss: 3.5857 - accuracy: 0.10 - ETA: 2s - loss: 3.5855 - accuracy: 0.10 - ETA: 1s - loss: 3.5857 - accuracy: 0.10 - ETA: 0s - loss: 3.5858 - accuracy: 0.10 - ETA: 0s - loss: 3.5860 - accuracy: 0.10 - 263s 6ms/step - loss: 3.5860 - accuracy: 0.1031 - val_loss: 3.9163 - val_accuracy: 0.0302\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:04 - loss: 3.5289 - accuracy: 0.10 - ETA: 3:58 - loss: 3.5093 - accuracy: 0.13 - ETA: 3:59 - loss: 3.4610 - accuracy: 0.13 - ETA: 3:58 - loss: 3.4864 - accuracy: 0.12 - ETA: 3:55 - loss: 3.4612 - accuracy: 0.13 - ETA: 3:56 - loss: 3.4835 - accuracy: 0.13 - ETA: 3:55 - loss: 3.5077 - accuracy: 0.12 - ETA: 3:57 - loss: 3.4912 - accuracy: 0.12 - ETA: 3:56 - loss: 3.4789 - accuracy: 0.12 - ETA: 3:56 - loss: 3.4897 - accuracy: 0.12 - ETA: 3:56 - loss: 3.4958 - accuracy: 0.12 - ETA: 3:55 - loss: 3.4943 - accuracy: 0.12 - ETA: 3:54 - loss: 3.5086 - accuracy: 0.11 - ETA: 3:53 - loss: 3.5138 - accuracy: 0.11 - ETA: 3:53 - loss: 3.5123 - accuracy: 0.11 - ETA: 3:51 - loss: 3.5117 - accuracy: 0.11 - ETA: 3:50 - loss: 3.5104 - accuracy: 0.11 - ETA: 3:50 - loss: 3.5144 - accuracy: 0.11 - ETA: 3:49 - loss: 3.5119 - accuracy: 0.11 - ETA: 3:49 - loss: 3.5218 - accuracy: 0.11 - ETA: 3:48 - loss: 3.5180 - accuracy: 0.11 - ETA: 3:48 - loss: 3.5202 - accuracy: 0.11 - ETA: 3:47 - loss: 3.5228 - accuracy: 0.11 - ETA: 3:46 - loss: 3.5230 - accuracy: 0.11 - ETA: 3:45 - loss: 3.5247 - accuracy: 0.11 - ETA: 3:44 - loss: 3.5237 - accuracy: 0.11 - ETA: 3:43 - loss: 3.5207 - accuracy: 0.11 - ETA: 3:43 - loss: 3.5270 - accuracy: 0.11 - ETA: 3:42 - loss: 3.5345 - accuracy: 0.11 - ETA: 3:41 - loss: 3.5332 - accuracy: 0.11 - ETA: 3:40 - loss: 3.5320 - accuracy: 0.11 - ETA: 3:39 - loss: 3.5340 - accuracy: 0.11 - ETA: 3:38 - loss: 3.5368 - accuracy: 0.11 - ETA: 3:37 - loss: 3.5403 - accuracy: 0.11 - ETA: 3:36 - loss: 3.5403 - accuracy: 0.11 - ETA: 3:36 - loss: 3.5364 - accuracy: 0.11 - ETA: 3:35 - loss: 3.5403 - accuracy: 0.11 - ETA: 3:35 - loss: 3.5375 - accuracy: 0.11 - ETA: 3:34 - loss: 3.5365 - accuracy: 0.11 - ETA: 3:33 - loss: 3.5390 - accuracy: 0.11 - ETA: 3:33 - loss: 3.5410 - accuracy: 0.11 - ETA: 3:32 - loss: 3.5433 - accuracy: 0.11 - ETA: 3:31 - loss: 3.5422 - accuracy: 0.11 - ETA: 3:31 - loss: 3.5420 - accuracy: 0.11 - ETA: 3:30 - loss: 3.5433 - accuracy: 0.11 - ETA: 3:29 - loss: 3.5461 - accuracy: 0.11 - ETA: 3:28 - loss: 3.5440 - accuracy: 0.11 - ETA: 3:28 - loss: 3.5469 - accuracy: 0.11 - ETA: 3:27 - loss: 3.5492 - accuracy: 0.10 - ETA: 3:26 - loss: 3.5482 - accuracy: 0.11 - ETA: 3:25 - loss: 3.5487 - accuracy: 0.10 - ETA: 3:25 - loss: 3.5497 - accuracy: 0.11 - ETA: 3:24 - loss: 3.5460 - accuracy: 0.11 - ETA: 3:23 - loss: 3.5491 - accuracy: 0.11 - ETA: 3:22 - loss: 3.5502 - accuracy: 0.10 - ETA: 3:22 - loss: 3.5509 - accuracy: 0.10 - ETA: 3:21 - loss: 3.5497 - accuracy: 0.11 - ETA: 3:20 - loss: 3.5467 - accuracy: 0.11 - ETA: 3:20 - loss: 3.5472 - accuracy: 0.11 - ETA: 3:19 - loss: 3.5458 - accuracy: 0.11 - ETA: 3:18 - loss: 3.5441 - accuracy: 0.11 - ETA: 3:17 - loss: 3.5453 - accuracy: 0.11 - ETA: 3:17 - loss: 3.5457 - accuracy: 0.11 - ETA: 3:17 - loss: 3.5446 - accuracy: 0.11 - ETA: 3:16 - loss: 3.5461 - accuracy: 0.11 - ETA: 3:16 - loss: 3.5462 - accuracy: 0.11 - ETA: 3:15 - loss: 3.5457 - accuracy: 0.11 - ETA: 3:15 - loss: 3.5459 - accuracy: 0.11 - ETA: 3:14 - loss: 3.5445 - accuracy: 0.11 - ETA: 3:13 - loss: 3.5438 - accuracy: 0.11 - ETA: 3:12 - loss: 3.5433 - accuracy: 0.11 - ETA: 3:11 - loss: 3.5431 - accuracy: 0.11 - ETA: 3:11 - loss: 3.5440 - accuracy: 0.11 - ETA: 3:10 - loss: 3.5442 - accuracy: 0.11 - ETA: 3:09 - loss: 3.5432 - accuracy: 0.11 - ETA: 3:08 - loss: 3.5447 - accuracy: 0.10 - ETA: 3:08 - loss: 3.5437 - accuracy: 0.11 - ETA: 3:07 - loss: 3.5436 - accuracy: 0.11 - ETA: 3:06 - loss: 3.5463 - accuracy: 0.11 - ETA: 3:05 - loss: 3.5466 - accuracy: 0.11 - ETA: 3:04 - loss: 3.5455 - accuracy: 0.11 - ETA: 3:04 - loss: 3.5456 - accuracy: 0.11 - ETA: 3:03 - loss: 3.5458 - accuracy: 0.11 - ETA: 3:02 - loss: 3.5465 - accuracy: 0.10 - ETA: 3:01 - loss: 3.5448 - accuracy: 0.11 - ETA: 3:01 - loss: 3.5435 - accuracy: 0.11 - ETA: 3:00 - loss: 3.5432 - accuracy: 0.11 - ETA: 2:59 - loss: 3.5418 - accuracy: 0.11 - ETA: 2:58 - loss: 3.5420 - accuracy: 0.11 - ETA: 2:58 - loss: 3.5422 - accuracy: 0.11 - ETA: 2:57 - loss: 3.5436 - accuracy: 0.11 - ETA: 2:56 - loss: 3.5435 - accuracy: 0.11 - ETA: 2:55 - loss: 3.5444 - accuracy: 0.11 - ETA: 2:55 - loss: 3.5429 - accuracy: 0.11 - ETA: 2:54 - loss: 3.5430 - accuracy: 0.11 - ETA: 2:53 - loss: 3.5443 - accuracy: 0.11 - ETA: 2:52 - loss: 3.5423 - accuracy: 0.11 - ETA: 2:52 - loss: 3.5423 - accuracy: 0.11 - ETA: 2:51 - loss: 3.5420 - accuracy: 0.11 - ETA: 2:50 - loss: 3.5426 - accuracy: 0.11 - ETA: 2:49 - loss: 3.5437 - accuracy: 0.11 - ETA: 2:49 - loss: 3.5422 - accuracy: 0.11 - ETA: 2:48 - loss: 3.5421 - accuracy: 0.11 - ETA: 2:47 - loss: 3.5435 - accuracy: 0.11 - ETA: 2:46 - loss: 3.5437 - accuracy: 0.11 - ETA: 2:46 - loss: 3.5436 - accuracy: 0.11 - ETA: 2:45 - loss: 3.5446 - accuracy: 0.11 - ETA: 2:44 - loss: 3.5434 - accuracy: 0.11 - ETA: 2:43 - loss: 3.5433 - accuracy: 0.11 - ETA: 2:43 - loss: 3.5438 - accuracy: 0.10 - ETA: 2:42 - loss: 3.5422 - accuracy: 0.11 - ETA: 2:41 - loss: 3.5436 - accuracy: 0.11 - ETA: 2:40 - loss: 3.5425 - accuracy: 0.11 - ETA: 2:39 - loss: 3.5416 - accuracy: 0.11 - ETA: 2:39 - loss: 3.5407 - accuracy: 0.11 - ETA: 2:38 - loss: 3.5393 - accuracy: 0.11 - ETA: 2:37 - loss: 3.5393 - accuracy: 0.11 - ETA: 2:37 - loss: 3.5396 - accuracy: 0.11 - ETA: 2:36 - loss: 3.5390 - accuracy: 0.11 - ETA: 2:35 - loss: 3.5394 - accuracy: 0.11 - ETA: 2:34 - loss: 3.5403 - accuracy: 0.11 - ETA: 2:34 - loss: 3.5410 - accuracy: 0.11 - ETA: 2:33 - loss: 3.5415 - accuracy: 0.11 - ETA: 2:32 - loss: 3.5417 - accuracy: 0.11 - ETA: 2:31 - loss: 3.5424 - accuracy: 0.11 - ETA: 2:31 - loss: 3.5409 - accuracy: 0.11 - ETA: 2:30 - loss: 3.5412 - accuracy: 0.11 - ETA: 2:29 - loss: 3.5407 - accuracy: 0.11 - ETA: 2:28 - loss: 3.5405 - accuracy: 0.11 - ETA: 2:28 - loss: 3.5402 - accuracy: 0.11 - ETA: 2:27 - loss: 3.5401 - accuracy: 0.11 - ETA: 2:26 - loss: 3.5410 - accuracy: 0.11 - ETA: 2:25 - loss: 3.5414 - accuracy: 0.11 - ETA: 2:25 - loss: 3.5409 - accuracy: 0.11 - ETA: 2:24 - loss: 3.5408 - accuracy: 0.11 - ETA: 2:23 - loss: 3.5401 - accuracy: 0.11 - ETA: 2:23 - loss: 3.5410 - accuracy: 0.11 - ETA: 2:22 - loss: 3.5416 - accuracy: 0.11 - ETA: 2:21 - loss: 3.5419 - accuracy: 0.11 - ETA: 2:20 - loss: 3.5417 - accuracy: 0.11 - ETA: 2:20 - loss: 3.5403 - accuracy: 0.11 - ETA: 2:19 - loss: 3.5392 - accuracy: 0.11 - ETA: 2:18 - loss: 3.5385 - accuracy: 0.11 - ETA: 2:17 - loss: 3.5402 - accuracy: 0.11 - ETA: 2:17 - loss: 3.5412 - accuracy: 0.11 - ETA: 2:16 - loss: 3.5414 - accuracy: 0.11 - ETA: 2:15 - loss: 3.5421 - accuracy: 0.11 - ETA: 2:15 - loss: 3.5422 - accuracy: 0.11 - ETA: 2:14 - loss: 3.5420 - accuracy: 0.11 - ETA: 2:13 - loss: 3.5410 - accuracy: 0.11 - ETA: 2:12 - loss: 3.5415 - accuracy: 0.11 - ETA: 2:12 - loss: 3.5421 - accuracy: 0.11 - ETA: 2:11 - loss: 3.5421 - accuracy: 0.11 - ETA: 2:10 - loss: 3.5416 - accuracy: 0.11 - ETA: 2:10 - loss: 3.5425 - accuracy: 0.11 - ETA: 2:09 - loss: 3.5433 - accuracy: 0.11 - ETA: 2:08 - loss: 3.5433 - accuracy: 0.11 - ETA: 2:07 - loss: 3.5435 - accuracy: 0.11 - ETA: 2:07 - loss: 3.5430 - accuracy: 0.11 - ETA: 2:06 - loss: 3.5436 - accuracy: 0.11 - ETA: 2:05 - loss: 3.5432 - accuracy: 0.11 - ETA: 2:04 - loss: 3.5442 - accuracy: 0.11 - ETA: 2:04 - loss: 3.5436 - accuracy: 0.11 - ETA: 2:03 - loss: 3.5433 - accuracy: 0.11 - ETA: 2:02 - loss: 3.5445 - accuracy: 0.11 - ETA: 2:02 - loss: 3.5439 - accuracy: 0.11 - ETA: 2:01 - loss: 3.5438 - accuracy: 0.11 - ETA: 2:00 - loss: 3.5430 - accuracy: 0.11 - ETA: 1:59 - loss: 3.5439 - accuracy: 0.11 - ETA: 1:59 - loss: 3.5451 - accuracy: 0.11 - ETA: 1:58 - loss: 3.5453 - accuracy: 0.11 - ETA: 1:57 - loss: 3.5468 - accuracy: 0.11 - ETA: 1:56 - loss: 3.5465 - accuracy: 0.11 - ETA: 1:56 - loss: 3.5462 - accuracy: 0.11 - ETA: 1:55 - loss: 3.5468 - accuracy: 0.11 - ETA: 1:54 - loss: 3.5467 - accuracy: 0.11 - ETA: 1:53 - loss: 3.5472 - accuracy: 0.11 - ETA: 1:53 - loss: 3.5469 - accuracy: 0.11 - ETA: 1:52 - loss: 3.5454 - accuracy: 0.11 - ETA: 1:51 - loss: 3.5460 - accuracy: 0.11 - ETA: 1:50 - loss: 3.5462 - accuracy: 0.11 - ETA: 1:50 - loss: 3.5465 - accuracy: 0.11 - ETA: 1:49 - loss: 3.5461 - accuracy: 0.11 - ETA: 1:48 - loss: 3.5464 - accuracy: 0.11 - ETA: 1:47 - loss: 3.5464 - accuracy: 0.11 - ETA: 1:47 - loss: 3.5458 - accuracy: 0.1104"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:46 - loss: 3.5455 - accuracy: 0.11 - ETA: 1:45 - loss: 3.5460 - accuracy: 0.11 - ETA: 1:44 - loss: 3.5456 - accuracy: 0.11 - ETA: 1:44 - loss: 3.5448 - accuracy: 0.11 - ETA: 1:43 - loss: 3.5444 - accuracy: 0.11 - ETA: 1:42 - loss: 3.5447 - accuracy: 0.11 - ETA: 1:41 - loss: 3.5443 - accuracy: 0.11 - ETA: 1:41 - loss: 3.5447 - accuracy: 0.11 - ETA: 1:40 - loss: 3.5452 - accuracy: 0.11 - ETA: 1:39 - loss: 3.5456 - accuracy: 0.11 - ETA: 1:38 - loss: 3.5463 - accuracy: 0.11 - ETA: 1:38 - loss: 3.5457 - accuracy: 0.11 - ETA: 1:37 - loss: 3.5453 - accuracy: 0.11 - ETA: 1:36 - loss: 3.5455 - accuracy: 0.11 - ETA: 1:35 - loss: 3.5449 - accuracy: 0.11 - ETA: 1:35 - loss: 3.5444 - accuracy: 0.11 - ETA: 1:34 - loss: 3.5446 - accuracy: 0.11 - ETA: 1:33 - loss: 3.5439 - accuracy: 0.11 - ETA: 1:32 - loss: 3.5448 - accuracy: 0.11 - ETA: 1:32 - loss: 3.5449 - accuracy: 0.11 - ETA: 1:31 - loss: 3.5451 - accuracy: 0.11 - ETA: 1:30 - loss: 3.5452 - accuracy: 0.11 - ETA: 1:30 - loss: 3.5453 - accuracy: 0.11 - ETA: 1:29 - loss: 3.5449 - accuracy: 0.11 - ETA: 1:28 - loss: 3.5448 - accuracy: 0.11 - ETA: 1:27 - loss: 3.5440 - accuracy: 0.11 - ETA: 1:27 - loss: 3.5448 - accuracy: 0.11 - ETA: 1:26 - loss: 3.5441 - accuracy: 0.11 - ETA: 1:25 - loss: 3.5444 - accuracy: 0.11 - ETA: 1:24 - loss: 3.5447 - accuracy: 0.11 - ETA: 1:24 - loss: 3.5448 - accuracy: 0.11 - ETA: 1:23 - loss: 3.5449 - accuracy: 0.11 - ETA: 1:22 - loss: 3.5443 - accuracy: 0.11 - ETA: 1:21 - loss: 3.5446 - accuracy: 0.11 - ETA: 1:21 - loss: 3.5446 - accuracy: 0.11 - ETA: 1:20 - loss: 3.5451 - accuracy: 0.11 - ETA: 1:19 - loss: 3.5445 - accuracy: 0.11 - ETA: 1:18 - loss: 3.5454 - accuracy: 0.11 - ETA: 1:18 - loss: 3.5449 - accuracy: 0.11 - ETA: 1:17 - loss: 3.5460 - accuracy: 0.11 - ETA: 1:16 - loss: 3.5459 - accuracy: 0.11 - ETA: 1:15 - loss: 3.5459 - accuracy: 0.11 - ETA: 1:15 - loss: 3.5457 - accuracy: 0.11 - ETA: 1:14 - loss: 3.5458 - accuracy: 0.11 - ETA: 1:13 - loss: 3.5449 - accuracy: 0.11 - ETA: 1:13 - loss: 3.5451 - accuracy: 0.11 - ETA: 1:12 - loss: 3.5460 - accuracy: 0.11 - ETA: 1:11 - loss: 3.5460 - accuracy: 0.11 - ETA: 1:10 - loss: 3.5461 - accuracy: 0.11 - ETA: 1:10 - loss: 3.5460 - accuracy: 0.11 - ETA: 1:09 - loss: 3.5464 - accuracy: 0.11 - ETA: 1:08 - loss: 3.5463 - accuracy: 0.11 - ETA: 1:07 - loss: 3.5458 - accuracy: 0.11 - ETA: 1:07 - loss: 3.5462 - accuracy: 0.11 - ETA: 1:06 - loss: 3.5472 - accuracy: 0.11 - ETA: 1:05 - loss: 3.5475 - accuracy: 0.10 - ETA: 1:04 - loss: 3.5478 - accuracy: 0.10 - ETA: 1:04 - loss: 3.5471 - accuracy: 0.11 - ETA: 1:03 - loss: 3.5464 - accuracy: 0.11 - ETA: 1:02 - loss: 3.5459 - accuracy: 0.11 - ETA: 1:02 - loss: 3.5458 - accuracy: 0.11 - ETA: 1:01 - loss: 3.5464 - accuracy: 0.11 - ETA: 1:00 - loss: 3.5461 - accuracy: 0.11 - ETA: 59s - loss: 3.5465 - accuracy: 0.1100 - ETA: 59s - loss: 3.5466 - accuracy: 0.109 - ETA: 58s - loss: 3.5460 - accuracy: 0.110 - ETA: 57s - loss: 3.5464 - accuracy: 0.110 - ETA: 56s - loss: 3.5463 - accuracy: 0.110 - ETA: 56s - loss: 3.5458 - accuracy: 0.110 - ETA: 55s - loss: 3.5455 - accuracy: 0.110 - ETA: 54s - loss: 3.5451 - accuracy: 0.110 - ETA: 53s - loss: 3.5453 - accuracy: 0.110 - ETA: 53s - loss: 3.5451 - accuracy: 0.110 - ETA: 52s - loss: 3.5445 - accuracy: 0.110 - ETA: 51s - loss: 3.5448 - accuracy: 0.110 - ETA: 50s - loss: 3.5448 - accuracy: 0.110 - ETA: 50s - loss: 3.5449 - accuracy: 0.110 - ETA: 49s - loss: 3.5449 - accuracy: 0.110 - ETA: 48s - loss: 3.5450 - accuracy: 0.110 - ETA: 48s - loss: 3.5445 - accuracy: 0.110 - ETA: 47s - loss: 3.5445 - accuracy: 0.110 - ETA: 46s - loss: 3.5441 - accuracy: 0.110 - ETA: 45s - loss: 3.5447 - accuracy: 0.110 - ETA: 45s - loss: 3.5446 - accuracy: 0.110 - ETA: 44s - loss: 3.5449 - accuracy: 0.110 - ETA: 43s - loss: 3.5457 - accuracy: 0.109 - ETA: 42s - loss: 3.5459 - accuracy: 0.109 - ETA: 42s - loss: 3.5463 - accuracy: 0.109 - ETA: 41s - loss: 3.5461 - accuracy: 0.109 - ETA: 40s - loss: 3.5461 - accuracy: 0.109 - ETA: 39s - loss: 3.5463 - accuracy: 0.109 - ETA: 39s - loss: 3.5470 - accuracy: 0.109 - ETA: 38s - loss: 3.5472 - accuracy: 0.109 - ETA: 37s - loss: 3.5476 - accuracy: 0.109 - ETA: 36s - loss: 3.5480 - accuracy: 0.109 - ETA: 36s - loss: 3.5479 - accuracy: 0.109 - ETA: 35s - loss: 3.5475 - accuracy: 0.109 - ETA: 34s - loss: 3.5480 - accuracy: 0.109 - ETA: 34s - loss: 3.5485 - accuracy: 0.109 - ETA: 33s - loss: 3.5486 - accuracy: 0.109 - ETA: 32s - loss: 3.5489 - accuracy: 0.109 - ETA: 31s - loss: 3.5497 - accuracy: 0.109 - ETA: 31s - loss: 3.5497 - accuracy: 0.109 - ETA: 30s - loss: 3.5496 - accuracy: 0.109 - ETA: 29s - loss: 3.5503 - accuracy: 0.109 - ETA: 28s - loss: 3.5502 - accuracy: 0.109 - ETA: 28s - loss: 3.5504 - accuracy: 0.109 - ETA: 27s - loss: 3.5505 - accuracy: 0.109 - ETA: 26s - loss: 3.5506 - accuracy: 0.109 - ETA: 25s - loss: 3.5510 - accuracy: 0.109 - ETA: 25s - loss: 3.5506 - accuracy: 0.109 - ETA: 24s - loss: 3.5506 - accuracy: 0.109 - ETA: 23s - loss: 3.5507 - accuracy: 0.109 - ETA: 22s - loss: 3.5507 - accuracy: 0.109 - ETA: 22s - loss: 3.5511 - accuracy: 0.109 - ETA: 21s - loss: 3.5509 - accuracy: 0.109 - ETA: 20s - loss: 3.5508 - accuracy: 0.110 - ETA: 19s - loss: 3.5511 - accuracy: 0.109 - ETA: 19s - loss: 3.5510 - accuracy: 0.110 - ETA: 18s - loss: 3.5518 - accuracy: 0.109 - ETA: 17s - loss: 3.5519 - accuracy: 0.109 - ETA: 17s - loss: 3.5520 - accuracy: 0.109 - ETA: 16s - loss: 3.5518 - accuracy: 0.109 - ETA: 15s - loss: 3.5519 - accuracy: 0.109 - ETA: 14s - loss: 3.5520 - accuracy: 0.109 - ETA: 14s - loss: 3.5518 - accuracy: 0.109 - ETA: 13s - loss: 3.5518 - accuracy: 0.109 - ETA: 12s - loss: 3.5518 - accuracy: 0.109 - ETA: 11s - loss: 3.5514 - accuracy: 0.109 - ETA: 11s - loss: 3.5516 - accuracy: 0.109 - ETA: 10s - loss: 3.5517 - accuracy: 0.109 - ETA: 9s - loss: 3.5516 - accuracy: 0.109 - ETA: 8s - loss: 3.5523 - accuracy: 0.10 - ETA: 8s - loss: 3.5522 - accuracy: 0.10 - ETA: 7s - loss: 3.5522 - accuracy: 0.10 - ETA: 6s - loss: 3.5525 - accuracy: 0.10 - ETA: 5s - loss: 3.5525 - accuracy: 0.10 - ETA: 5s - loss: 3.5526 - accuracy: 0.10 - ETA: 4s - loss: 3.5529 - accuracy: 0.10 - ETA: 3s - loss: 3.5525 - accuracy: 0.10 - ETA: 3s - loss: 3.5528 - accuracy: 0.10 - ETA: 2s - loss: 3.5529 - accuracy: 0.10 - ETA: 1s - loss: 3.5529 - accuracy: 0.10 - ETA: 0s - loss: 3.5531 - accuracy: 0.10 - ETA: 0s - loss: 3.5527 - accuracy: 0.10 - 262s 6ms/step - loss: 3.5528 - accuracy: 0.1094 - val_loss: 3.9203 - val_accuracy: 0.0337\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:11 - loss: 3.4883 - accuracy: 0.10 - ETA: 4:07 - loss: 3.5877 - accuracy: 0.08 - ETA: 4:01 - loss: 3.5587 - accuracy: 0.09 - ETA: 4:01 - loss: 3.5831 - accuracy: 0.09 - ETA: 4:00 - loss: 3.5944 - accuracy: 0.08 - ETA: 4:00 - loss: 3.5807 - accuracy: 0.09 - ETA: 3:58 - loss: 3.5480 - accuracy: 0.10 - ETA: 3:58 - loss: 3.5452 - accuracy: 0.10 - ETA: 3:57 - loss: 3.5544 - accuracy: 0.10 - ETA: 3:57 - loss: 3.5397 - accuracy: 0.10 - ETA: 3:55 - loss: 3.5412 - accuracy: 0.10 - ETA: 3:54 - loss: 3.5557 - accuracy: 0.10 - ETA: 3:53 - loss: 3.5495 - accuracy: 0.10 - ETA: 3:53 - loss: 3.5434 - accuracy: 0.10 - ETA: 3:52 - loss: 3.5534 - accuracy: 0.10 - ETA: 3:51 - loss: 3.5470 - accuracy: 0.10 - ETA: 3:50 - loss: 3.5551 - accuracy: 0.10 - ETA: 3:51 - loss: 3.5601 - accuracy: 0.10 - ETA: 3:50 - loss: 3.5656 - accuracy: 0.10 - ETA: 3:49 - loss: 3.5657 - accuracy: 0.10 - ETA: 3:48 - loss: 3.5635 - accuracy: 0.10 - ETA: 3:47 - loss: 3.5572 - accuracy: 0.10 - ETA: 3:46 - loss: 3.5595 - accuracy: 0.10 - ETA: 3:46 - loss: 3.5570 - accuracy: 0.10 - ETA: 3:45 - loss: 3.5529 - accuracy: 0.10 - ETA: 3:44 - loss: 3.5533 - accuracy: 0.10 - ETA: 3:44 - loss: 3.5482 - accuracy: 0.10 - ETA: 3:43 - loss: 3.5514 - accuracy: 0.10 - ETA: 3:42 - loss: 3.5530 - accuracy: 0.10 - ETA: 3:41 - loss: 3.5528 - accuracy: 0.10 - ETA: 3:41 - loss: 3.5525 - accuracy: 0.10 - ETA: 3:40 - loss: 3.5535 - accuracy: 0.10 - ETA: 3:39 - loss: 3.5570 - accuracy: 0.10 - ETA: 3:38 - loss: 3.5579 - accuracy: 0.10 - ETA: 3:38 - loss: 3.5579 - accuracy: 0.10 - ETA: 3:37 - loss: 3.5566 - accuracy: 0.10 - ETA: 3:36 - loss: 3.5576 - accuracy: 0.10 - ETA: 3:36 - loss: 3.5564 - accuracy: 0.10 - ETA: 3:35 - loss: 3.5555 - accuracy: 0.10 - ETA: 3:34 - loss: 3.5574 - accuracy: 0.10 - ETA: 3:31 - loss: 3.5542 - accuracy: 0.10 - ETA: 3:31 - loss: 3.5518 - accuracy: 0.10 - ETA: 3:30 - loss: 3.5519 - accuracy: 0.10 - ETA: 3:29 - loss: 3.5528 - accuracy: 0.10 - ETA: 3:28 - loss: 3.5531 - accuracy: 0.10 - ETA: 3:28 - loss: 3.5527 - accuracy: 0.10 - ETA: 3:27 - loss: 3.5522 - accuracy: 0.10 - ETA: 3:26 - loss: 3.5511 - accuracy: 0.10 - ETA: 3:25 - loss: 3.5510 - accuracy: 0.10 - ETA: 3:25 - loss: 3.5495 - accuracy: 0.10 - ETA: 3:24 - loss: 3.5503 - accuracy: 0.10 - ETA: 3:23 - loss: 3.5493 - accuracy: 0.10 - ETA: 3:23 - loss: 3.5476 - accuracy: 0.10 - ETA: 3:22 - loss: 3.5468 - accuracy: 0.10 - ETA: 3:21 - loss: 3.5485 - accuracy: 0.10 - ETA: 3:20 - loss: 3.5478 - accuracy: 0.10 - ETA: 3:20 - loss: 3.5459 - accuracy: 0.10 - ETA: 3:19 - loss: 3.5451 - accuracy: 0.10 - ETA: 3:18 - loss: 3.5465 - accuracy: 0.10 - ETA: 3:18 - loss: 3.5459 - accuracy: 0.10 - ETA: 3:17 - loss: 3.5464 - accuracy: 0.10 - ETA: 3:16 - loss: 3.5482 - accuracy: 0.10 - ETA: 3:15 - loss: 3.5486 - accuracy: 0.10 - ETA: 3:15 - loss: 3.5491 - accuracy: 0.10 - ETA: 3:14 - loss: 3.5483 - accuracy: 0.10 - ETA: 3:13 - loss: 3.5459 - accuracy: 0.10 - ETA: 3:13 - loss: 3.5466 - accuracy: 0.10 - ETA: 3:12 - loss: 3.5472 - accuracy: 0.10 - ETA: 3:11 - loss: 3.5474 - accuracy: 0.10 - ETA: 3:11 - loss: 3.5460 - accuracy: 0.10 - ETA: 3:10 - loss: 3.5470 - accuracy: 0.10 - ETA: 3:09 - loss: 3.5468 - accuracy: 0.10 - ETA: 3:09 - loss: 3.5461 - accuracy: 0.10 - ETA: 3:08 - loss: 3.5440 - accuracy: 0.10 - ETA: 3:07 - loss: 3.5432 - accuracy: 0.10 - ETA: 3:06 - loss: 3.5432 - accuracy: 0.10 - ETA: 3:06 - loss: 3.5442 - accuracy: 0.10 - ETA: 3:05 - loss: 3.5449 - accuracy: 0.10 - ETA: 3:04 - loss: 3.5444 - accuracy: 0.10 - ETA: 3:04 - loss: 3.5441 - accuracy: 0.10 - ETA: 3:03 - loss: 3.5457 - accuracy: 0.10 - ETA: 3:02 - loss: 3.5465 - accuracy: 0.10 - ETA: 3:01 - loss: 3.5453 - accuracy: 0.10 - ETA: 3:01 - loss: 3.5433 - accuracy: 0.10 - ETA: 3:00 - loss: 3.5432 - accuracy: 0.10 - ETA: 2:59 - loss: 3.5416 - accuracy: 0.10 - ETA: 2:58 - loss: 3.5403 - accuracy: 0.11 - ETA: 2:58 - loss: 3.5401 - accuracy: 0.10 - ETA: 2:57 - loss: 3.5414 - accuracy: 0.10 - ETA: 2:56 - loss: 3.5401 - accuracy: 0.10 - ETA: 2:55 - loss: 3.5404 - accuracy: 0.10 - ETA: 2:55 - loss: 3.5406 - accuracy: 0.10 - ETA: 2:54 - loss: 3.5406 - accuracy: 0.10 - ETA: 2:53 - loss: 3.5412 - accuracy: 0.10 - ETA: 2:52 - loss: 3.5405 - accuracy: 0.10 - ETA: 2:52 - loss: 3.5421 - accuracy: 0.10 - ETA: 2:51 - loss: 3.5425 - accuracy: 0.10 - ETA: 2:50 - loss: 3.5435 - accuracy: 0.10 - ETA: 2:49 - loss: 3.5428 - accuracy: 0.10 - ETA: 2:49 - loss: 3.5429 - accuracy: 0.10 - ETA: 2:48 - loss: 3.5430 - accuracy: 0.10 - ETA: 2:47 - loss: 3.5416 - accuracy: 0.10 - ETA: 2:46 - loss: 3.5416 - accuracy: 0.10 - ETA: 2:46 - loss: 3.5414 - accuracy: 0.10 - ETA: 2:45 - loss: 3.5417 - accuracy: 0.10 - ETA: 2:44 - loss: 3.5412 - accuracy: 0.10 - ETA: 2:44 - loss: 3.5410 - accuracy: 0.10 - ETA: 2:43 - loss: 3.5411 - accuracy: 0.10 - ETA: 2:42 - loss: 3.5403 - accuracy: 0.10 - ETA: 2:41 - loss: 3.5407 - accuracy: 0.10 - ETA: 2:40 - loss: 3.5391 - accuracy: 0.10 - ETA: 2:40 - loss: 3.5402 - accuracy: 0.10 - ETA: 2:39 - loss: 3.5408 - accuracy: 0.10 - ETA: 2:38 - loss: 3.5397 - accuracy: 0.10 - ETA: 2:38 - loss: 3.5393 - accuracy: 0.10 - ETA: 2:37 - loss: 3.5372 - accuracy: 0.10 - ETA: 2:36 - loss: 3.5361 - accuracy: 0.10 - ETA: 2:35 - loss: 3.5359 - accuracy: 0.10 - ETA: 2:35 - loss: 3.5356 - accuracy: 0.10 - ETA: 2:34 - loss: 3.5337 - accuracy: 0.11 - ETA: 2:33 - loss: 3.5346 - accuracy: 0.11 - ETA: 2:33 - loss: 3.5354 - accuracy: 0.11 - ETA: 2:32 - loss: 3.5345 - accuracy: 0.11 - ETA: 2:31 - loss: 3.5350 - accuracy: 0.11 - ETA: 2:30 - loss: 3.5329 - accuracy: 0.11 - ETA: 2:30 - loss: 3.5321 - accuracy: 0.11 - ETA: 2:29 - loss: 3.5314 - accuracy: 0.11 - ETA: 2:28 - loss: 3.5306 - accuracy: 0.11 - ETA: 2:28 - loss: 3.5302 - accuracy: 0.11 - ETA: 2:27 - loss: 3.5305 - accuracy: 0.11 - ETA: 2:26 - loss: 3.5292 - accuracy: 0.11 - ETA: 2:25 - loss: 3.5287 - accuracy: 0.11 - ETA: 2:25 - loss: 3.5291 - accuracy: 0.11 - ETA: 2:24 - loss: 3.5296 - accuracy: 0.11 - ETA: 2:23 - loss: 3.5287 - accuracy: 0.11 - ETA: 2:22 - loss: 3.5290 - accuracy: 0.11 - ETA: 2:22 - loss: 3.5288 - accuracy: 0.11 - ETA: 2:21 - loss: 3.5281 - accuracy: 0.11 - ETA: 2:20 - loss: 3.5273 - accuracy: 0.11 - ETA: 2:20 - loss: 3.5274 - accuracy: 0.11 - ETA: 2:19 - loss: 3.5269 - accuracy: 0.11 - ETA: 2:18 - loss: 3.5271 - accuracy: 0.11 - ETA: 2:17 - loss: 3.5276 - accuracy: 0.11 - ETA: 2:17 - loss: 3.5280 - accuracy: 0.11 - ETA: 2:16 - loss: 3.5280 - accuracy: 0.11 - ETA: 2:15 - loss: 3.5283 - accuracy: 0.11 - ETA: 2:14 - loss: 3.5280 - accuracy: 0.11 - ETA: 2:14 - loss: 3.5279 - accuracy: 0.11 - ETA: 2:13 - loss: 3.5276 - accuracy: 0.11 - ETA: 2:12 - loss: 3.5277 - accuracy: 0.11 - ETA: 2:11 - loss: 3.5278 - accuracy: 0.11 - ETA: 2:11 - loss: 3.5277 - accuracy: 0.11 - ETA: 2:10 - loss: 3.5271 - accuracy: 0.11 - ETA: 2:09 - loss: 3.5281 - accuracy: 0.11 - ETA: 2:09 - loss: 3.5279 - accuracy: 0.11 - ETA: 2:08 - loss: 3.5282 - accuracy: 0.11 - ETA: 2:07 - loss: 3.5284 - accuracy: 0.11 - ETA: 2:07 - loss: 3.5283 - accuracy: 0.11 - ETA: 2:06 - loss: 3.5271 - accuracy: 0.11 - ETA: 2:05 - loss: 3.5273 - accuracy: 0.11 - ETA: 2:04 - loss: 3.5281 - accuracy: 0.11 - ETA: 2:04 - loss: 3.5271 - accuracy: 0.11 - ETA: 2:03 - loss: 3.5278 - accuracy: 0.11 - ETA: 2:02 - loss: 3.5276 - accuracy: 0.11 - ETA: 2:01 - loss: 3.5278 - accuracy: 0.11 - ETA: 2:01 - loss: 3.5267 - accuracy: 0.11 - ETA: 2:00 - loss: 3.5262 - accuracy: 0.11 - ETA: 1:59 - loss: 3.5262 - accuracy: 0.11 - ETA: 1:59 - loss: 3.5266 - accuracy: 0.11 - ETA: 1:58 - loss: 3.5255 - accuracy: 0.11 - ETA: 1:57 - loss: 3.5251 - accuracy: 0.11 - ETA: 1:56 - loss: 3.5251 - accuracy: 0.11 - ETA: 1:56 - loss: 3.5255 - accuracy: 0.11 - ETA: 1:55 - loss: 3.5250 - accuracy: 0.11 - ETA: 1:54 - loss: 3.5249 - accuracy: 0.11 - ETA: 1:53 - loss: 3.5247 - accuracy: 0.11 - ETA: 1:53 - loss: 3.5257 - accuracy: 0.11 - ETA: 1:52 - loss: 3.5261 - accuracy: 0.11 - ETA: 1:51 - loss: 3.5265 - accuracy: 0.11 - ETA: 1:51 - loss: 3.5263 - accuracy: 0.11 - ETA: 1:50 - loss: 3.5268 - accuracy: 0.11 - ETA: 1:49 - loss: 3.5264 - accuracy: 0.11 - ETA: 1:48 - loss: 3.5264 - accuracy: 0.11 - ETA: 1:48 - loss: 3.5263 - accuracy: 0.11 - ETA: 1:47 - loss: 3.5264 - accuracy: 0.11 - ETA: 1:46 - loss: 3.5256 - accuracy: 0.1128"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.5260 - accuracy: 0.11 - ETA: 1:45 - loss: 3.5267 - accuracy: 0.11 - ETA: 1:44 - loss: 3.5276 - accuracy: 0.11 - ETA: 1:43 - loss: 3.5271 - accuracy: 0.11 - ETA: 1:42 - loss: 3.5269 - accuracy: 0.11 - ETA: 1:42 - loss: 3.5265 - accuracy: 0.11 - ETA: 1:41 - loss: 3.5264 - accuracy: 0.11 - ETA: 1:40 - loss: 3.5267 - accuracy: 0.11 - ETA: 1:40 - loss: 3.5266 - accuracy: 0.11 - ETA: 1:39 - loss: 3.5263 - accuracy: 0.11 - ETA: 1:38 - loss: 3.5265 - accuracy: 0.11 - ETA: 1:37 - loss: 3.5268 - accuracy: 0.11 - ETA: 1:37 - loss: 3.5253 - accuracy: 0.11 - ETA: 1:36 - loss: 3.5253 - accuracy: 0.11 - ETA: 1:35 - loss: 3.5261 - accuracy: 0.11 - ETA: 1:35 - loss: 3.5267 - accuracy: 0.11 - ETA: 1:34 - loss: 3.5257 - accuracy: 0.11 - ETA: 1:33 - loss: 3.5256 - accuracy: 0.11 - ETA: 1:32 - loss: 3.5255 - accuracy: 0.11 - ETA: 1:32 - loss: 3.5259 - accuracy: 0.11 - ETA: 1:31 - loss: 3.5262 - accuracy: 0.11 - ETA: 1:30 - loss: 3.5264 - accuracy: 0.11 - ETA: 1:29 - loss: 3.5262 - accuracy: 0.11 - ETA: 1:29 - loss: 3.5260 - accuracy: 0.11 - ETA: 1:28 - loss: 3.5266 - accuracy: 0.11 - ETA: 1:27 - loss: 3.5267 - accuracy: 0.11 - ETA: 1:26 - loss: 3.5269 - accuracy: 0.11 - ETA: 1:26 - loss: 3.5273 - accuracy: 0.11 - ETA: 1:25 - loss: 3.5281 - accuracy: 0.11 - ETA: 1:24 - loss: 3.5279 - accuracy: 0.11 - ETA: 1:23 - loss: 3.5276 - accuracy: 0.11 - ETA: 1:23 - loss: 3.5272 - accuracy: 0.11 - ETA: 1:22 - loss: 3.5277 - accuracy: 0.11 - ETA: 1:21 - loss: 3.5277 - accuracy: 0.11 - ETA: 1:21 - loss: 3.5283 - accuracy: 0.11 - ETA: 1:20 - loss: 3.5290 - accuracy: 0.11 - ETA: 1:19 - loss: 3.5288 - accuracy: 0.11 - ETA: 1:18 - loss: 3.5291 - accuracy: 0.11 - ETA: 1:18 - loss: 3.5291 - accuracy: 0.11 - ETA: 1:17 - loss: 3.5296 - accuracy: 0.11 - ETA: 1:16 - loss: 3.5298 - accuracy: 0.11 - ETA: 1:15 - loss: 3.5296 - accuracy: 0.11 - ETA: 1:15 - loss: 3.5294 - accuracy: 0.11 - ETA: 1:14 - loss: 3.5294 - accuracy: 0.11 - ETA: 1:13 - loss: 3.5294 - accuracy: 0.11 - ETA: 1:12 - loss: 3.5298 - accuracy: 0.11 - ETA: 1:12 - loss: 3.5299 - accuracy: 0.11 - ETA: 1:11 - loss: 3.5297 - accuracy: 0.11 - ETA: 1:10 - loss: 3.5305 - accuracy: 0.11 - ETA: 1:09 - loss: 3.5311 - accuracy: 0.11 - ETA: 1:09 - loss: 3.5308 - accuracy: 0.11 - ETA: 1:08 - loss: 3.5311 - accuracy: 0.11 - ETA: 1:07 - loss: 3.5313 - accuracy: 0.11 - ETA: 1:07 - loss: 3.5314 - accuracy: 0.11 - ETA: 1:06 - loss: 3.5321 - accuracy: 0.11 - ETA: 1:05 - loss: 3.5321 - accuracy: 0.11 - ETA: 1:04 - loss: 3.5314 - accuracy: 0.11 - ETA: 1:04 - loss: 3.5319 - accuracy: 0.11 - ETA: 1:03 - loss: 3.5320 - accuracy: 0.11 - ETA: 1:02 - loss: 3.5322 - accuracy: 0.11 - ETA: 1:01 - loss: 3.5320 - accuracy: 0.11 - ETA: 1:01 - loss: 3.5324 - accuracy: 0.11 - ETA: 1:00 - loss: 3.5327 - accuracy: 0.11 - ETA: 59s - loss: 3.5330 - accuracy: 0.1127 - ETA: 58s - loss: 3.5335 - accuracy: 0.112 - ETA: 58s - loss: 3.5341 - accuracy: 0.112 - ETA: 57s - loss: 3.5341 - accuracy: 0.112 - ETA: 56s - loss: 3.5335 - accuracy: 0.112 - ETA: 55s - loss: 3.5338 - accuracy: 0.112 - ETA: 55s - loss: 3.5334 - accuracy: 0.112 - ETA: 54s - loss: 3.5333 - accuracy: 0.112 - ETA: 53s - loss: 3.5333 - accuracy: 0.112 - ETA: 53s - loss: 3.5337 - accuracy: 0.112 - ETA: 52s - loss: 3.5330 - accuracy: 0.112 - ETA: 51s - loss: 3.5330 - accuracy: 0.112 - ETA: 50s - loss: 3.5329 - accuracy: 0.112 - ETA: 50s - loss: 3.5329 - accuracy: 0.112 - ETA: 49s - loss: 3.5328 - accuracy: 0.112 - ETA: 48s - loss: 3.5327 - accuracy: 0.112 - ETA: 47s - loss: 3.5322 - accuracy: 0.112 - ETA: 47s - loss: 3.5323 - accuracy: 0.112 - ETA: 46s - loss: 3.5323 - accuracy: 0.112 - ETA: 45s - loss: 3.5322 - accuracy: 0.112 - ETA: 44s - loss: 3.5322 - accuracy: 0.112 - ETA: 44s - loss: 3.5323 - accuracy: 0.112 - ETA: 43s - loss: 3.5317 - accuracy: 0.112 - ETA: 42s - loss: 3.5324 - accuracy: 0.112 - ETA: 41s - loss: 3.5321 - accuracy: 0.112 - ETA: 41s - loss: 3.5318 - accuracy: 0.112 - ETA: 40s - loss: 3.5317 - accuracy: 0.112 - ETA: 39s - loss: 3.5322 - accuracy: 0.111 - ETA: 39s - loss: 3.5323 - accuracy: 0.111 - ETA: 38s - loss: 3.5321 - accuracy: 0.111 - ETA: 37s - loss: 3.5324 - accuracy: 0.111 - ETA: 36s - loss: 3.5320 - accuracy: 0.111 - ETA: 36s - loss: 3.5320 - accuracy: 0.111 - ETA: 35s - loss: 3.5318 - accuracy: 0.112 - ETA: 34s - loss: 3.5317 - accuracy: 0.112 - ETA: 33s - loss: 3.5314 - accuracy: 0.112 - ETA: 33s - loss: 3.5313 - accuracy: 0.112 - ETA: 32s - loss: 3.5318 - accuracy: 0.112 - ETA: 31s - loss: 3.5314 - accuracy: 0.112 - ETA: 30s - loss: 3.5313 - accuracy: 0.112 - ETA: 30s - loss: 3.5312 - accuracy: 0.112 - ETA: 29s - loss: 3.5310 - accuracy: 0.112 - ETA: 28s - loss: 3.5309 - accuracy: 0.112 - ETA: 27s - loss: 3.5308 - accuracy: 0.112 - ETA: 27s - loss: 3.5308 - accuracy: 0.112 - ETA: 26s - loss: 3.5307 - accuracy: 0.112 - ETA: 25s - loss: 3.5307 - accuracy: 0.112 - ETA: 25s - loss: 3.5306 - accuracy: 0.112 - ETA: 24s - loss: 3.5310 - accuracy: 0.112 - ETA: 23s - loss: 3.5309 - accuracy: 0.112 - ETA: 22s - loss: 3.5314 - accuracy: 0.111 - ETA: 22s - loss: 3.5315 - accuracy: 0.111 - ETA: 21s - loss: 3.5312 - accuracy: 0.112 - ETA: 20s - loss: 3.5310 - accuracy: 0.112 - ETA: 19s - loss: 3.5319 - accuracy: 0.111 - ETA: 19s - loss: 3.5319 - accuracy: 0.111 - ETA: 18s - loss: 3.5325 - accuracy: 0.111 - ETA: 17s - loss: 3.5326 - accuracy: 0.111 - ETA: 16s - loss: 3.5327 - accuracy: 0.111 - ETA: 16s - loss: 3.5326 - accuracy: 0.111 - ETA: 15s - loss: 3.5326 - accuracy: 0.111 - ETA: 14s - loss: 3.5322 - accuracy: 0.111 - ETA: 14s - loss: 3.5319 - accuracy: 0.112 - ETA: 13s - loss: 3.5318 - accuracy: 0.112 - ETA: 12s - loss: 3.5319 - accuracy: 0.112 - ETA: 11s - loss: 3.5316 - accuracy: 0.112 - ETA: 11s - loss: 3.5315 - accuracy: 0.112 - ETA: 10s - loss: 3.5317 - accuracy: 0.112 - ETA: 9s - loss: 3.5319 - accuracy: 0.111 - ETA: 8s - loss: 3.5320 - accuracy: 0.11 - ETA: 8s - loss: 3.5319 - accuracy: 0.11 - ETA: 7s - loss: 3.5326 - accuracy: 0.11 - ETA: 6s - loss: 3.5327 - accuracy: 0.11 - ETA: 5s - loss: 3.5329 - accuracy: 0.11 - ETA: 5s - loss: 3.5330 - accuracy: 0.11 - ETA: 4s - loss: 3.5328 - accuracy: 0.11 - ETA: 3s - loss: 3.5327 - accuracy: 0.11 - ETA: 2s - loss: 3.5331 - accuracy: 0.11 - ETA: 2s - loss: 3.5328 - accuracy: 0.11 - ETA: 1s - loss: 3.5328 - accuracy: 0.11 - ETA: 0s - loss: 3.5336 - accuracy: 0.11 - ETA: 0s - loss: 3.5336 - accuracy: 0.11 - 262s 6ms/step - loss: 3.5336 - accuracy: 0.1117 - val_loss: 3.9315 - val_accuracy: 0.0373\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:07 - loss: 3.5613 - accuracy: 0.11 - ETA: 3:59 - loss: 3.6452 - accuracy: 0.10 - ETA: 3:59 - loss: 3.5676 - accuracy: 0.10 - ETA: 3:56 - loss: 3.5593 - accuracy: 0.11 - ETA: 3:55 - loss: 3.5363 - accuracy: 0.12 - ETA: 3:54 - loss: 3.5134 - accuracy: 0.12 - ETA: 3:52 - loss: 3.5185 - accuracy: 0.12 - ETA: 3:55 - loss: 3.5152 - accuracy: 0.12 - ETA: 3:56 - loss: 3.5375 - accuracy: 0.12 - ETA: 3:54 - loss: 3.5451 - accuracy: 0.12 - ETA: 3:53 - loss: 3.5488 - accuracy: 0.12 - ETA: 3:52 - loss: 3.5470 - accuracy: 0.12 - ETA: 3:52 - loss: 3.5331 - accuracy: 0.12 - ETA: 3:51 - loss: 3.5326 - accuracy: 0.12 - ETA: 3:50 - loss: 3.5414 - accuracy: 0.12 - ETA: 3:50 - loss: 3.5290 - accuracy: 0.12 - ETA: 3:49 - loss: 3.5335 - accuracy: 0.12 - ETA: 3:48 - loss: 3.5324 - accuracy: 0.13 - ETA: 3:47 - loss: 3.5411 - accuracy: 0.12 - ETA: 3:46 - loss: 3.5478 - accuracy: 0.12 - ETA: 3:45 - loss: 3.5535 - accuracy: 0.12 - ETA: 3:43 - loss: 3.5584 - accuracy: 0.12 - ETA: 3:43 - loss: 3.5642 - accuracy: 0.11 - ETA: 3:42 - loss: 3.5666 - accuracy: 0.11 - ETA: 3:42 - loss: 3.5690 - accuracy: 0.11 - ETA: 3:41 - loss: 3.5773 - accuracy: 0.11 - ETA: 3:39 - loss: 3.5789 - accuracy: 0.11 - ETA: 3:39 - loss: 3.5763 - accuracy: 0.11 - ETA: 3:39 - loss: 3.5788 - accuracy: 0.11 - ETA: 3:38 - loss: 3.5791 - accuracy: 0.11 - ETA: 3:38 - loss: 3.5789 - accuracy: 0.11 - ETA: 3:37 - loss: 3.5847 - accuracy: 0.11 - ETA: 3:36 - loss: 3.5883 - accuracy: 0.10 - ETA: 3:35 - loss: 3.5917 - accuracy: 0.10 - ETA: 3:34 - loss: 3.5943 - accuracy: 0.10 - ETA: 3:35 - loss: 3.5958 - accuracy: 0.10 - ETA: 3:34 - loss: 3.5940 - accuracy: 0.10 - ETA: 3:33 - loss: 3.5959 - accuracy: 0.10 - ETA: 3:32 - loss: 3.5949 - accuracy: 0.10 - ETA: 3:31 - loss: 3.5972 - accuracy: 0.10 - ETA: 3:31 - loss: 3.5960 - accuracy: 0.10 - ETA: 3:30 - loss: 3.5944 - accuracy: 0.10 - ETA: 3:29 - loss: 3.5916 - accuracy: 0.10 - ETA: 3:29 - loss: 3.5907 - accuracy: 0.10 - ETA: 3:28 - loss: 3.5889 - accuracy: 0.10 - ETA: 3:27 - loss: 3.5851 - accuracy: 0.10 - ETA: 3:27 - loss: 3.5832 - accuracy: 0.10 - ETA: 3:26 - loss: 3.5811 - accuracy: 0.10 - ETA: 3:25 - loss: 3.5816 - accuracy: 0.10 - ETA: 3:24 - loss: 3.5792 - accuracy: 0.10 - ETA: 3:24 - loss: 3.5765 - accuracy: 0.10 - ETA: 3:23 - loss: 3.5759 - accuracy: 0.10 - ETA: 3:22 - loss: 3.5744 - accuracy: 0.10 - ETA: 3:22 - loss: 3.5738 - accuracy: 0.10 - ETA: 3:21 - loss: 3.5723 - accuracy: 0.10 - ETA: 3:20 - loss: 3.5704 - accuracy: 0.10 - ETA: 3:19 - loss: 3.5722 - accuracy: 0.10 - ETA: 3:19 - loss: 3.5716 - accuracy: 0.10 - ETA: 3:18 - loss: 3.5722 - accuracy: 0.10 - ETA: 3:17 - loss: 3.5717 - accuracy: 0.10 - ETA: 3:16 - loss: 3.5724 - accuracy: 0.10 - ETA: 3:16 - loss: 3.5714 - accuracy: 0.10 - ETA: 3:15 - loss: 3.5712 - accuracy: 0.10 - ETA: 3:14 - loss: 3.5721 - accuracy: 0.10 - ETA: 3:13 - loss: 3.5679 - accuracy: 0.10 - ETA: 3:13 - loss: 3.5667 - accuracy: 0.10 - ETA: 3:12 - loss: 3.5689 - accuracy: 0.10 - ETA: 3:12 - loss: 3.5659 - accuracy: 0.10 - ETA: 3:11 - loss: 3.5612 - accuracy: 0.11 - ETA: 3:10 - loss: 3.5630 - accuracy: 0.11 - ETA: 3:10 - loss: 3.5619 - accuracy: 0.11 - ETA: 3:09 - loss: 3.5603 - accuracy: 0.11 - ETA: 3:08 - loss: 3.5579 - accuracy: 0.11 - ETA: 3:08 - loss: 3.5597 - accuracy: 0.11 - ETA: 3:07 - loss: 3.5561 - accuracy: 0.11 - ETA: 3:06 - loss: 3.5553 - accuracy: 0.11 - ETA: 3:05 - loss: 3.5571 - accuracy: 0.11 - ETA: 3:05 - loss: 3.5571 - accuracy: 0.11 - ETA: 3:04 - loss: 3.5561 - accuracy: 0.11 - ETA: 3:03 - loss: 3.5578 - accuracy: 0.11 - ETA: 3:02 - loss: 3.5589 - accuracy: 0.11 - ETA: 3:02 - loss: 3.5597 - accuracy: 0.11 - ETA: 3:01 - loss: 3.5617 - accuracy: 0.11 - ETA: 3:00 - loss: 3.5612 - accuracy: 0.11 - ETA: 3:00 - loss: 3.5603 - accuracy: 0.11 - ETA: 2:59 - loss: 3.5610 - accuracy: 0.11 - ETA: 2:58 - loss: 3.5609 - accuracy: 0.11 - ETA: 2:58 - loss: 3.5611 - accuracy: 0.11 - ETA: 2:57 - loss: 3.5609 - accuracy: 0.11 - ETA: 2:56 - loss: 3.5593 - accuracy: 0.11 - ETA: 2:55 - loss: 3.5576 - accuracy: 0.11 - ETA: 2:55 - loss: 3.5566 - accuracy: 0.11 - ETA: 2:54 - loss: 3.5565 - accuracy: 0.11 - ETA: 2:53 - loss: 3.5556 - accuracy: 0.11 - ETA: 2:53 - loss: 3.5557 - accuracy: 0.11 - ETA: 2:52 - loss: 3.5561 - accuracy: 0.11 - ETA: 2:51 - loss: 3.5565 - accuracy: 0.11 - ETA: 2:50 - loss: 3.5538 - accuracy: 0.11 - ETA: 2:50 - loss: 3.5535 - accuracy: 0.11 - ETA: 2:49 - loss: 3.5535 - accuracy: 0.11 - ETA: 2:48 - loss: 3.5520 - accuracy: 0.11 - ETA: 2:47 - loss: 3.5495 - accuracy: 0.11 - ETA: 2:47 - loss: 3.5479 - accuracy: 0.11 - ETA: 2:46 - loss: 3.5475 - accuracy: 0.11 - ETA: 2:45 - loss: 3.5470 - accuracy: 0.11 - ETA: 2:44 - loss: 3.5462 - accuracy: 0.11 - ETA: 2:44 - loss: 3.5460 - accuracy: 0.11 - ETA: 2:43 - loss: 3.5460 - accuracy: 0.11 - ETA: 2:42 - loss: 3.5473 - accuracy: 0.11 - ETA: 2:42 - loss: 3.5470 - accuracy: 0.11 - ETA: 2:41 - loss: 3.5472 - accuracy: 0.11 - ETA: 2:40 - loss: 3.5461 - accuracy: 0.11 - ETA: 2:40 - loss: 3.5453 - accuracy: 0.11 - ETA: 2:39 - loss: 3.5446 - accuracy: 0.11 - ETA: 2:38 - loss: 3.5432 - accuracy: 0.11 - ETA: 2:37 - loss: 3.5404 - accuracy: 0.11 - ETA: 2:37 - loss: 3.5391 - accuracy: 0.11 - ETA: 2:36 - loss: 3.5394 - accuracy: 0.11 - ETA: 2:35 - loss: 3.5402 - accuracy: 0.11 - ETA: 2:35 - loss: 3.5377 - accuracy: 0.11 - ETA: 2:34 - loss: 3.5370 - accuracy: 0.11 - ETA: 2:33 - loss: 3.5379 - accuracy: 0.11 - ETA: 2:32 - loss: 3.5382 - accuracy: 0.11 - ETA: 2:32 - loss: 3.5375 - accuracy: 0.11 - ETA: 2:31 - loss: 3.5393 - accuracy: 0.11 - ETA: 2:30 - loss: 3.5402 - accuracy: 0.11 - ETA: 2:29 - loss: 3.5389 - accuracy: 0.11 - ETA: 2:28 - loss: 3.5382 - accuracy: 0.11 - ETA: 2:27 - loss: 3.5379 - accuracy: 0.11 - ETA: 2:26 - loss: 3.5378 - accuracy: 0.11 - ETA: 2:25 - loss: 3.5374 - accuracy: 0.11 - ETA: 2:24 - loss: 3.5379 - accuracy: 0.11 - ETA: 2:23 - loss: 3.5382 - accuracy: 0.11 - ETA: 2:22 - loss: 3.5382 - accuracy: 0.11 - ETA: 2:21 - loss: 3.5380 - accuracy: 0.11 - ETA: 2:20 - loss: 3.5372 - accuracy: 0.11 - ETA: 2:19 - loss: 3.5376 - accuracy: 0.11 - ETA: 2:19 - loss: 3.5382 - accuracy: 0.11 - ETA: 2:18 - loss: 3.5382 - accuracy: 0.11 - ETA: 2:17 - loss: 3.5381 - accuracy: 0.11 - ETA: 2:17 - loss: 3.5388 - accuracy: 0.11 - ETA: 2:16 - loss: 3.5394 - accuracy: 0.11 - ETA: 2:15 - loss: 3.5393 - accuracy: 0.11 - ETA: 2:14 - loss: 3.5405 - accuracy: 0.11 - ETA: 2:13 - loss: 3.5401 - accuracy: 0.11 - ETA: 2:12 - loss: 3.5404 - accuracy: 0.11 - ETA: 2:11 - loss: 3.5403 - accuracy: 0.11 - ETA: 2:10 - loss: 3.5393 - accuracy: 0.11 - ETA: 2:09 - loss: 3.5395 - accuracy: 0.11 - ETA: 2:08 - loss: 3.5384 - accuracy: 0.11 - ETA: 2:07 - loss: 3.5381 - accuracy: 0.11 - ETA: 2:06 - loss: 3.5388 - accuracy: 0.11 - ETA: 2:05 - loss: 3.5391 - accuracy: 0.11 - ETA: 2:05 - loss: 3.5381 - accuracy: 0.11 - ETA: 2:04 - loss: 3.5376 - accuracy: 0.11 - ETA: 2:04 - loss: 3.5378 - accuracy: 0.11 - ETA: 2:03 - loss: 3.5361 - accuracy: 0.11 - ETA: 2:02 - loss: 3.5350 - accuracy: 0.11 - ETA: 2:02 - loss: 3.5355 - accuracy: 0.11 - ETA: 2:01 - loss: 3.5347 - accuracy: 0.11 - ETA: 2:00 - loss: 3.5355 - accuracy: 0.11 - ETA: 2:00 - loss: 3.5349 - accuracy: 0.11 - ETA: 1:59 - loss: 3.5345 - accuracy: 0.11 - ETA: 1:58 - loss: 3.5342 - accuracy: 0.11 - ETA: 1:58 - loss: 3.5350 - accuracy: 0.11 - ETA: 1:57 - loss: 3.5349 - accuracy: 0.11 - ETA: 1:56 - loss: 3.5348 - accuracy: 0.11 - ETA: 1:56 - loss: 3.5352 - accuracy: 0.11 - ETA: 1:55 - loss: 3.5356 - accuracy: 0.11 - ETA: 1:54 - loss: 3.5364 - accuracy: 0.11 - ETA: 1:54 - loss: 3.5362 - accuracy: 0.11 - ETA: 1:53 - loss: 3.5355 - accuracy: 0.11 - ETA: 1:52 - loss: 3.5364 - accuracy: 0.11 - ETA: 1:52 - loss: 3.5359 - accuracy: 0.11 - ETA: 1:51 - loss: 3.5360 - accuracy: 0.11 - ETA: 1:50 - loss: 3.5360 - accuracy: 0.11 - ETA: 1:49 - loss: 3.5364 - accuracy: 0.11 - ETA: 1:49 - loss: 3.5360 - accuracy: 0.11 - ETA: 1:48 - loss: 3.5363 - accuracy: 0.11 - ETA: 1:47 - loss: 3.5367 - accuracy: 0.11 - ETA: 1:47 - loss: 3.5362 - accuracy: 0.11 - ETA: 1:46 - loss: 3.5360 - accuracy: 0.11 - ETA: 1:45 - loss: 3.5357 - accuracy: 0.11 - ETA: 1:45 - loss: 3.5346 - accuracy: 0.11 - ETA: 1:44 - loss: 3.5343 - accuracy: 0.11 - ETA: 1:43 - loss: 3.5345 - accuracy: 0.1128"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:43 - loss: 3.5347 - accuracy: 0.11 - ETA: 1:42 - loss: 3.5340 - accuracy: 0.11 - ETA: 1:41 - loss: 3.5341 - accuracy: 0.11 - ETA: 1:40 - loss: 3.5335 - accuracy: 0.11 - ETA: 1:40 - loss: 3.5340 - accuracy: 0.11 - ETA: 1:39 - loss: 3.5347 - accuracy: 0.11 - ETA: 1:38 - loss: 3.5340 - accuracy: 0.11 - ETA: 1:38 - loss: 3.5336 - accuracy: 0.11 - ETA: 1:37 - loss: 3.5334 - accuracy: 0.11 - ETA: 1:36 - loss: 3.5332 - accuracy: 0.11 - ETA: 1:35 - loss: 3.5331 - accuracy: 0.11 - ETA: 1:35 - loss: 3.5332 - accuracy: 0.11 - ETA: 1:34 - loss: 3.5330 - accuracy: 0.11 - ETA: 1:33 - loss: 3.5339 - accuracy: 0.11 - ETA: 1:33 - loss: 3.5335 - accuracy: 0.11 - ETA: 1:32 - loss: 3.5334 - accuracy: 0.11 - ETA: 1:31 - loss: 3.5331 - accuracy: 0.11 - ETA: 1:31 - loss: 3.5326 - accuracy: 0.11 - ETA: 1:30 - loss: 3.5325 - accuracy: 0.11 - ETA: 1:29 - loss: 3.5315 - accuracy: 0.11 - ETA: 1:28 - loss: 3.5317 - accuracy: 0.11 - ETA: 1:28 - loss: 3.5313 - accuracy: 0.11 - ETA: 1:27 - loss: 3.5311 - accuracy: 0.11 - ETA: 1:26 - loss: 3.5313 - accuracy: 0.11 - ETA: 1:26 - loss: 3.5314 - accuracy: 0.11 - ETA: 1:25 - loss: 3.5317 - accuracy: 0.11 - ETA: 1:24 - loss: 3.5306 - accuracy: 0.11 - ETA: 1:24 - loss: 3.5307 - accuracy: 0.11 - ETA: 1:23 - loss: 3.5312 - accuracy: 0.11 - ETA: 1:22 - loss: 3.5312 - accuracy: 0.11 - ETA: 1:21 - loss: 3.5311 - accuracy: 0.11 - ETA: 1:21 - loss: 3.5317 - accuracy: 0.11 - ETA: 1:20 - loss: 3.5318 - accuracy: 0.11 - ETA: 1:19 - loss: 3.5320 - accuracy: 0.11 - ETA: 1:19 - loss: 3.5318 - accuracy: 0.11 - ETA: 1:18 - loss: 3.5320 - accuracy: 0.11 - ETA: 1:17 - loss: 3.5320 - accuracy: 0.11 - ETA: 1:16 - loss: 3.5320 - accuracy: 0.11 - ETA: 1:16 - loss: 3.5311 - accuracy: 0.11 - ETA: 1:15 - loss: 3.5314 - accuracy: 0.11 - ETA: 1:14 - loss: 3.5321 - accuracy: 0.11 - ETA: 1:14 - loss: 3.5323 - accuracy: 0.11 - ETA: 1:13 - loss: 3.5319 - accuracy: 0.11 - ETA: 1:12 - loss: 3.5321 - accuracy: 0.11 - ETA: 1:11 - loss: 3.5319 - accuracy: 0.11 - ETA: 1:11 - loss: 3.5317 - accuracy: 0.11 - ETA: 1:10 - loss: 3.5315 - accuracy: 0.11 - ETA: 1:09 - loss: 3.5317 - accuracy: 0.11 - ETA: 1:09 - loss: 3.5317 - accuracy: 0.11 - ETA: 1:08 - loss: 3.5309 - accuracy: 0.11 - ETA: 1:07 - loss: 3.5306 - accuracy: 0.11 - ETA: 1:06 - loss: 3.5310 - accuracy: 0.11 - ETA: 1:06 - loss: 3.5307 - accuracy: 0.11 - ETA: 1:05 - loss: 3.5307 - accuracy: 0.11 - ETA: 1:04 - loss: 3.5304 - accuracy: 0.11 - ETA: 1:04 - loss: 3.5295 - accuracy: 0.11 - ETA: 1:03 - loss: 3.5293 - accuracy: 0.11 - ETA: 1:02 - loss: 3.5286 - accuracy: 0.11 - ETA: 1:01 - loss: 3.5285 - accuracy: 0.11 - ETA: 1:01 - loss: 3.5284 - accuracy: 0.11 - ETA: 1:00 - loss: 3.5285 - accuracy: 0.11 - ETA: 59s - loss: 3.5289 - accuracy: 0.1135 - ETA: 59s - loss: 3.5296 - accuracy: 0.113 - ETA: 58s - loss: 3.5298 - accuracy: 0.113 - ETA: 57s - loss: 3.5299 - accuracy: 0.113 - ETA: 56s - loss: 3.5305 - accuracy: 0.113 - ETA: 56s - loss: 3.5303 - accuracy: 0.113 - ETA: 55s - loss: 3.5303 - accuracy: 0.113 - ETA: 54s - loss: 3.5301 - accuracy: 0.113 - ETA: 54s - loss: 3.5305 - accuracy: 0.113 - ETA: 53s - loss: 3.5298 - accuracy: 0.113 - ETA: 52s - loss: 3.5294 - accuracy: 0.113 - ETA: 51s - loss: 3.5286 - accuracy: 0.113 - ETA: 51s - loss: 3.5293 - accuracy: 0.113 - ETA: 50s - loss: 3.5295 - accuracy: 0.113 - ETA: 49s - loss: 3.5294 - accuracy: 0.113 - ETA: 49s - loss: 3.5290 - accuracy: 0.113 - ETA: 48s - loss: 3.5291 - accuracy: 0.113 - ETA: 47s - loss: 3.5290 - accuracy: 0.113 - ETA: 46s - loss: 3.5293 - accuracy: 0.113 - ETA: 46s - loss: 3.5294 - accuracy: 0.113 - ETA: 45s - loss: 3.5295 - accuracy: 0.113 - ETA: 44s - loss: 3.5294 - accuracy: 0.113 - ETA: 44s - loss: 3.5289 - accuracy: 0.113 - ETA: 43s - loss: 3.5293 - accuracy: 0.112 - ETA: 42s - loss: 3.5290 - accuracy: 0.113 - ETA: 41s - loss: 3.5294 - accuracy: 0.112 - ETA: 41s - loss: 3.5285 - accuracy: 0.113 - ETA: 40s - loss: 3.5285 - accuracy: 0.112 - ETA: 39s - loss: 3.5288 - accuracy: 0.112 - ETA: 39s - loss: 3.5286 - accuracy: 0.112 - ETA: 38s - loss: 3.5290 - accuracy: 0.112 - ETA: 37s - loss: 3.5291 - accuracy: 0.112 - ETA: 36s - loss: 3.5289 - accuracy: 0.112 - ETA: 36s - loss: 3.5290 - accuracy: 0.112 - ETA: 35s - loss: 3.5291 - accuracy: 0.112 - ETA: 34s - loss: 3.5286 - accuracy: 0.112 - ETA: 33s - loss: 3.5286 - accuracy: 0.112 - ETA: 33s - loss: 3.5284 - accuracy: 0.112 - ETA: 32s - loss: 3.5285 - accuracy: 0.112 - ETA: 31s - loss: 3.5283 - accuracy: 0.112 - ETA: 31s - loss: 3.5276 - accuracy: 0.112 - ETA: 30s - loss: 3.5273 - accuracy: 0.112 - ETA: 29s - loss: 3.5269 - accuracy: 0.112 - ETA: 28s - loss: 3.5263 - accuracy: 0.113 - ETA: 28s - loss: 3.5265 - accuracy: 0.112 - ETA: 27s - loss: 3.5255 - accuracy: 0.113 - ETA: 26s - loss: 3.5251 - accuracy: 0.113 - ETA: 26s - loss: 3.5248 - accuracy: 0.113 - ETA: 25s - loss: 3.5253 - accuracy: 0.113 - ETA: 24s - loss: 3.5246 - accuracy: 0.113 - ETA: 23s - loss: 3.5249 - accuracy: 0.113 - ETA: 23s - loss: 3.5252 - accuracy: 0.113 - ETA: 22s - loss: 3.5253 - accuracy: 0.113 - ETA: 21s - loss: 3.5252 - accuracy: 0.113 - ETA: 21s - loss: 3.5251 - accuracy: 0.113 - ETA: 20s - loss: 3.5245 - accuracy: 0.113 - ETA: 19s - loss: 3.5241 - accuracy: 0.113 - ETA: 18s - loss: 3.5244 - accuracy: 0.113 - ETA: 18s - loss: 3.5244 - accuracy: 0.113 - ETA: 17s - loss: 3.5242 - accuracy: 0.113 - ETA: 16s - loss: 3.5241 - accuracy: 0.113 - ETA: 15s - loss: 3.5241 - accuracy: 0.113 - ETA: 15s - loss: 3.5237 - accuracy: 0.114 - ETA: 14s - loss: 3.5241 - accuracy: 0.113 - ETA: 13s - loss: 3.5236 - accuracy: 0.113 - ETA: 13s - loss: 3.5231 - accuracy: 0.114 - ETA: 12s - loss: 3.5228 - accuracy: 0.114 - ETA: 11s - loss: 3.5229 - accuracy: 0.114 - ETA: 10s - loss: 3.5230 - accuracy: 0.114 - ETA: 10s - loss: 3.5227 - accuracy: 0.114 - ETA: 9s - loss: 3.5218 - accuracy: 0.114 - ETA: 8s - loss: 3.5219 - accuracy: 0.11 - ETA: 8s - loss: 3.5215 - accuracy: 0.11 - ETA: 7s - loss: 3.5220 - accuracy: 0.11 - ETA: 6s - loss: 3.5222 - accuracy: 0.11 - ETA: 5s - loss: 3.5219 - accuracy: 0.11 - ETA: 5s - loss: 3.5216 - accuracy: 0.11 - ETA: 4s - loss: 3.5220 - accuracy: 0.11 - ETA: 3s - loss: 3.5221 - accuracy: 0.11 - ETA: 2s - loss: 3.5220 - accuracy: 0.11 - ETA: 2s - loss: 3.5221 - accuracy: 0.11 - ETA: 1s - loss: 3.5222 - accuracy: 0.11 - ETA: 0s - loss: 3.5215 - accuracy: 0.11 - ETA: 0s - loss: 3.5212 - accuracy: 0.11 - 258s 6ms/step - loss: 3.5211 - accuracy: 0.1145 - val_loss: 3.9015 - val_accuracy: 0.0324\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:52 - loss: 3.3839 - accuracy: 0.13 - ETA: 3:53 - loss: 3.4087 - accuracy: 0.13 - ETA: 3:55 - loss: 3.4138 - accuracy: 0.12 - ETA: 3:53 - loss: 3.4235 - accuracy: 0.11 - ETA: 3:54 - loss: 3.3986 - accuracy: 0.11 - ETA: 3:54 - loss: 3.3906 - accuracy: 0.11 - ETA: 3:53 - loss: 3.4245 - accuracy: 0.11 - ETA: 3:54 - loss: 3.4154 - accuracy: 0.11 - ETA: 3:52 - loss: 3.4209 - accuracy: 0.11 - ETA: 3:51 - loss: 3.4263 - accuracy: 0.11 - ETA: 3:50 - loss: 3.4222 - accuracy: 0.11 - ETA: 3:50 - loss: 3.4192 - accuracy: 0.11 - ETA: 3:49 - loss: 3.4173 - accuracy: 0.12 - ETA: 3:47 - loss: 3.4251 - accuracy: 0.12 - ETA: 3:47 - loss: 3.4338 - accuracy: 0.11 - ETA: 3:47 - loss: 3.4330 - accuracy: 0.11 - ETA: 3:46 - loss: 3.4466 - accuracy: 0.11 - ETA: 3:46 - loss: 3.4604 - accuracy: 0.11 - ETA: 3:45 - loss: 3.4529 - accuracy: 0.11 - ETA: 3:44 - loss: 3.4566 - accuracy: 0.11 - ETA: 3:43 - loss: 3.4599 - accuracy: 0.11 - ETA: 3:43 - loss: 3.4596 - accuracy: 0.11 - ETA: 3:43 - loss: 3.4529 - accuracy: 0.11 - ETA: 3:42 - loss: 3.4584 - accuracy: 0.11 - ETA: 3:41 - loss: 3.4594 - accuracy: 0.11 - ETA: 3:40 - loss: 3.4615 - accuracy: 0.11 - ETA: 3:40 - loss: 3.4626 - accuracy: 0.11 - ETA: 3:39 - loss: 3.4652 - accuracy: 0.11 - ETA: 3:39 - loss: 3.4701 - accuracy: 0.11 - ETA: 3:38 - loss: 3.4737 - accuracy: 0.11 - ETA: 3:37 - loss: 3.4758 - accuracy: 0.11 - ETA: 3:37 - loss: 3.4797 - accuracy: 0.11 - ETA: 3:36 - loss: 3.4805 - accuracy: 0.11 - ETA: 3:36 - loss: 3.4801 - accuracy: 0.11 - ETA: 3:35 - loss: 3.4784 - accuracy: 0.11 - ETA: 3:34 - loss: 3.4757 - accuracy: 0.11 - ETA: 3:34 - loss: 3.4761 - accuracy: 0.11 - ETA: 3:33 - loss: 3.4760 - accuracy: 0.11 - ETA: 3:32 - loss: 3.4737 - accuracy: 0.11 - ETA: 3:32 - loss: 3.4753 - accuracy: 0.11 - ETA: 3:31 - loss: 3.4748 - accuracy: 0.11 - ETA: 3:31 - loss: 3.4760 - accuracy: 0.11 - ETA: 3:30 - loss: 3.4799 - accuracy: 0.11 - ETA: 3:29 - loss: 3.4832 - accuracy: 0.11 - ETA: 3:28 - loss: 3.4826 - accuracy: 0.11 - ETA: 3:28 - loss: 3.4824 - accuracy: 0.11 - ETA: 3:27 - loss: 3.4805 - accuracy: 0.11 - ETA: 3:26 - loss: 3.4824 - accuracy: 0.11 - ETA: 3:26 - loss: 3.4836 - accuracy: 0.11 - ETA: 3:25 - loss: 3.4829 - accuracy: 0.11 - ETA: 3:24 - loss: 3.4830 - accuracy: 0.11 - ETA: 3:23 - loss: 3.4850 - accuracy: 0.11 - ETA: 3:23 - loss: 3.4870 - accuracy: 0.11 - ETA: 3:22 - loss: 3.4904 - accuracy: 0.11 - ETA: 3:21 - loss: 3.4919 - accuracy: 0.11 - ETA: 3:21 - loss: 3.4950 - accuracy: 0.11 - ETA: 3:20 - loss: 3.4983 - accuracy: 0.11 - ETA: 3:19 - loss: 3.4982 - accuracy: 0.11 - ETA: 3:19 - loss: 3.5002 - accuracy: 0.11 - ETA: 3:18 - loss: 3.5021 - accuracy: 0.11 - ETA: 3:17 - loss: 3.5019 - accuracy: 0.11 - ETA: 3:17 - loss: 3.5023 - accuracy: 0.11 - ETA: 3:16 - loss: 3.5036 - accuracy: 0.11 - ETA: 3:15 - loss: 3.5083 - accuracy: 0.11 - ETA: 3:14 - loss: 3.5061 - accuracy: 0.11 - ETA: 3:14 - loss: 3.5072 - accuracy: 0.11 - ETA: 3:13 - loss: 3.5078 - accuracy: 0.11 - ETA: 3:12 - loss: 3.5051 - accuracy: 0.11 - ETA: 3:12 - loss: 3.5052 - accuracy: 0.11 - ETA: 3:12 - loss: 3.5057 - accuracy: 0.11 - ETA: 3:11 - loss: 3.5062 - accuracy: 0.11 - ETA: 3:11 - loss: 3.5062 - accuracy: 0.11 - ETA: 3:10 - loss: 3.5060 - accuracy: 0.11 - ETA: 3:09 - loss: 3.5060 - accuracy: 0.11 - ETA: 3:09 - loss: 3.5038 - accuracy: 0.11 - ETA: 3:08 - loss: 3.5026 - accuracy: 0.11 - ETA: 3:07 - loss: 3.5022 - accuracy: 0.11 - ETA: 3:07 - loss: 3.5009 - accuracy: 0.11 - ETA: 3:06 - loss: 3.5021 - accuracy: 0.11 - ETA: 3:05 - loss: 3.5059 - accuracy: 0.11 - ETA: 3:04 - loss: 3.5059 - accuracy: 0.11 - ETA: 3:03 - loss: 3.5075 - accuracy: 0.11 - ETA: 3:03 - loss: 3.5062 - accuracy: 0.11 - ETA: 3:02 - loss: 3.5057 - accuracy: 0.11 - ETA: 3:01 - loss: 3.5053 - accuracy: 0.11 - ETA: 3:00 - loss: 3.5050 - accuracy: 0.11 - ETA: 3:00 - loss: 3.5071 - accuracy: 0.11 - ETA: 2:59 - loss: 3.5075 - accuracy: 0.11 - ETA: 2:58 - loss: 3.5099 - accuracy: 0.11 - ETA: 2:58 - loss: 3.5099 - accuracy: 0.11 - ETA: 2:57 - loss: 3.5100 - accuracy: 0.11 - ETA: 2:56 - loss: 3.5091 - accuracy: 0.11 - ETA: 2:55 - loss: 3.5099 - accuracy: 0.11 - ETA: 2:54 - loss: 3.5095 - accuracy: 0.11 - ETA: 2:54 - loss: 3.5103 - accuracy: 0.11 - ETA: 2:53 - loss: 3.5099 - accuracy: 0.11 - ETA: 2:52 - loss: 3.5110 - accuracy: 0.11 - ETA: 2:51 - loss: 3.5098 - accuracy: 0.11 - ETA: 2:51 - loss: 3.5089 - accuracy: 0.11 - ETA: 2:50 - loss: 3.5087 - accuracy: 0.11 - ETA: 2:49 - loss: 3.5084 - accuracy: 0.11 - ETA: 2:48 - loss: 3.5098 - accuracy: 0.11 - ETA: 2:47 - loss: 3.5119 - accuracy: 0.11 - ETA: 2:47 - loss: 3.5124 - accuracy: 0.11 - ETA: 2:46 - loss: 3.5121 - accuracy: 0.11 - ETA: 2:45 - loss: 3.5104 - accuracy: 0.11 - ETA: 2:44 - loss: 3.5106 - accuracy: 0.11 - ETA: 2:44 - loss: 3.5099 - accuracy: 0.11 - ETA: 2:43 - loss: 3.5128 - accuracy: 0.11 - ETA: 2:42 - loss: 3.5139 - accuracy: 0.11 - ETA: 2:41 - loss: 3.5144 - accuracy: 0.11 - ETA: 2:41 - loss: 3.5134 - accuracy: 0.11 - ETA: 2:40 - loss: 3.5142 - accuracy: 0.11 - ETA: 2:39 - loss: 3.5142 - accuracy: 0.11 - ETA: 2:38 - loss: 3.5140 - accuracy: 0.11 - ETA: 2:37 - loss: 3.5147 - accuracy: 0.11 - ETA: 2:37 - loss: 3.5148 - accuracy: 0.11 - ETA: 2:36 - loss: 3.5149 - accuracy: 0.11 - ETA: 2:35 - loss: 3.5148 - accuracy: 0.11 - ETA: 2:34 - loss: 3.5136 - accuracy: 0.11 - ETA: 2:34 - loss: 3.5139 - accuracy: 0.11 - ETA: 2:33 - loss: 3.5135 - accuracy: 0.11 - ETA: 2:32 - loss: 3.5130 - accuracy: 0.11 - ETA: 2:31 - loss: 3.5140 - accuracy: 0.11 - ETA: 2:31 - loss: 3.5145 - accuracy: 0.11 - ETA: 2:30 - loss: 3.5154 - accuracy: 0.11 - ETA: 2:29 - loss: 3.5146 - accuracy: 0.11 - ETA: 2:28 - loss: 3.5131 - accuracy: 0.11 - ETA: 2:28 - loss: 3.5131 - accuracy: 0.11 - ETA: 2:27 - loss: 3.5127 - accuracy: 0.11 - ETA: 2:26 - loss: 3.5114 - accuracy: 0.11 - ETA: 2:26 - loss: 3.5116 - accuracy: 0.11 - ETA: 2:25 - loss: 3.5112 - accuracy: 0.11 - ETA: 2:24 - loss: 3.5109 - accuracy: 0.11 - ETA: 2:23 - loss: 3.5113 - accuracy: 0.11 - ETA: 2:23 - loss: 3.5125 - accuracy: 0.11 - ETA: 2:22 - loss: 3.5118 - accuracy: 0.11 - ETA: 2:21 - loss: 3.5114 - accuracy: 0.11 - ETA: 2:20 - loss: 3.5106 - accuracy: 0.11 - ETA: 2:20 - loss: 3.5113 - accuracy: 0.11 - ETA: 2:19 - loss: 3.5108 - accuracy: 0.11 - ETA: 2:18 - loss: 3.5105 - accuracy: 0.11 - ETA: 2:18 - loss: 3.5103 - accuracy: 0.11 - ETA: 2:17 - loss: 3.5107 - accuracy: 0.11 - ETA: 2:16 - loss: 3.5105 - accuracy: 0.11 - ETA: 2:15 - loss: 3.5102 - accuracy: 0.11 - ETA: 2:15 - loss: 3.5098 - accuracy: 0.11 - ETA: 2:14 - loss: 3.5094 - accuracy: 0.11 - ETA: 2:13 - loss: 3.5095 - accuracy: 0.11 - ETA: 2:13 - loss: 3.5100 - accuracy: 0.11 - ETA: 2:12 - loss: 3.5095 - accuracy: 0.11 - ETA: 2:11 - loss: 3.5089 - accuracy: 0.11 - ETA: 2:10 - loss: 3.5087 - accuracy: 0.11 - ETA: 2:10 - loss: 3.5090 - accuracy: 0.11 - ETA: 2:09 - loss: 3.5075 - accuracy: 0.11 - ETA: 2:08 - loss: 3.5074 - accuracy: 0.11 - ETA: 2:07 - loss: 3.5073 - accuracy: 0.11 - ETA: 2:07 - loss: 3.5067 - accuracy: 0.11 - ETA: 2:06 - loss: 3.5068 - accuracy: 0.11 - ETA: 2:05 - loss: 3.5073 - accuracy: 0.11 - ETA: 2:05 - loss: 3.5066 - accuracy: 0.11 - ETA: 2:04 - loss: 3.5064 - accuracy: 0.11 - ETA: 2:03 - loss: 3.5071 - accuracy: 0.11 - ETA: 2:02 - loss: 3.5072 - accuracy: 0.11 - ETA: 2:02 - loss: 3.5073 - accuracy: 0.11 - ETA: 2:01 - loss: 3.5072 - accuracy: 0.11 - ETA: 2:00 - loss: 3.5080 - accuracy: 0.11 - ETA: 1:59 - loss: 3.5074 - accuracy: 0.11 - ETA: 1:59 - loss: 3.5074 - accuracy: 0.11 - ETA: 1:58 - loss: 3.5068 - accuracy: 0.11 - ETA: 1:57 - loss: 3.5069 - accuracy: 0.11 - ETA: 1:57 - loss: 3.5076 - accuracy: 0.11 - ETA: 1:56 - loss: 3.5069 - accuracy: 0.11 - ETA: 1:55 - loss: 3.5075 - accuracy: 0.11 - ETA: 1:54 - loss: 3.5074 - accuracy: 0.11 - ETA: 1:54 - loss: 3.5075 - accuracy: 0.11 - ETA: 1:53 - loss: 3.5082 - accuracy: 0.11 - ETA: 1:52 - loss: 3.5076 - accuracy: 0.11 - ETA: 1:51 - loss: 3.5067 - accuracy: 0.11 - ETA: 1:51 - loss: 3.5074 - accuracy: 0.11 - ETA: 1:50 - loss: 3.5073 - accuracy: 0.11 - ETA: 1:49 - loss: 3.5069 - accuracy: 0.11 - ETA: 1:49 - loss: 3.5067 - accuracy: 0.11 - ETA: 1:48 - loss: 3.5064 - accuracy: 0.11 - ETA: 1:47 - loss: 3.5075 - accuracy: 0.11 - ETA: 1:46 - loss: 3.5074 - accuracy: 0.1131"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:46 - loss: 3.5071 - accuracy: 0.11 - ETA: 1:45 - loss: 3.5069 - accuracy: 0.11 - ETA: 1:44 - loss: 3.5065 - accuracy: 0.11 - ETA: 1:43 - loss: 3.5049 - accuracy: 0.11 - ETA: 1:43 - loss: 3.5047 - accuracy: 0.11 - ETA: 1:42 - loss: 3.5057 - accuracy: 0.11 - ETA: 1:41 - loss: 3.5059 - accuracy: 0.11 - ETA: 1:40 - loss: 3.5059 - accuracy: 0.11 - ETA: 1:40 - loss: 3.5058 - accuracy: 0.11 - ETA: 1:39 - loss: 3.5059 - accuracy: 0.11 - ETA: 1:38 - loss: 3.5054 - accuracy: 0.11 - ETA: 1:37 - loss: 3.5055 - accuracy: 0.11 - ETA: 1:37 - loss: 3.5053 - accuracy: 0.11 - ETA: 1:36 - loss: 3.5053 - accuracy: 0.11 - ETA: 1:35 - loss: 3.5049 - accuracy: 0.11 - ETA: 1:34 - loss: 3.5046 - accuracy: 0.11 - ETA: 1:34 - loss: 3.5054 - accuracy: 0.11 - ETA: 1:33 - loss: 3.5050 - accuracy: 0.11 - ETA: 1:32 - loss: 3.5050 - accuracy: 0.11 - ETA: 1:32 - loss: 3.5046 - accuracy: 0.11 - ETA: 1:31 - loss: 3.5048 - accuracy: 0.11 - ETA: 1:30 - loss: 3.5048 - accuracy: 0.11 - ETA: 1:29 - loss: 3.5047 - accuracy: 0.11 - ETA: 1:29 - loss: 3.5050 - accuracy: 0.11 - ETA: 1:28 - loss: 3.5050 - accuracy: 0.11 - ETA: 1:27 - loss: 3.5046 - accuracy: 0.11 - ETA: 1:26 - loss: 3.5037 - accuracy: 0.11 - ETA: 1:26 - loss: 3.5036 - accuracy: 0.11 - ETA: 1:25 - loss: 3.5039 - accuracy: 0.11 - ETA: 1:24 - loss: 3.5037 - accuracy: 0.11 - ETA: 1:23 - loss: 3.5036 - accuracy: 0.11 - ETA: 1:23 - loss: 3.5037 - accuracy: 0.11 - ETA: 1:22 - loss: 3.5035 - accuracy: 0.11 - ETA: 1:21 - loss: 3.5033 - accuracy: 0.11 - ETA: 1:21 - loss: 3.5034 - accuracy: 0.11 - ETA: 1:20 - loss: 3.5037 - accuracy: 0.11 - ETA: 1:19 - loss: 3.5028 - accuracy: 0.11 - ETA: 1:18 - loss: 3.5026 - accuracy: 0.11 - ETA: 1:18 - loss: 3.5022 - accuracy: 0.11 - ETA: 1:17 - loss: 3.5020 - accuracy: 0.11 - ETA: 1:16 - loss: 3.5024 - accuracy: 0.11 - ETA: 1:15 - loss: 3.5016 - accuracy: 0.11 - ETA: 1:15 - loss: 3.5008 - accuracy: 0.11 - ETA: 1:14 - loss: 3.5011 - accuracy: 0.11 - ETA: 1:13 - loss: 3.5010 - accuracy: 0.11 - ETA: 1:12 - loss: 3.5011 - accuracy: 0.11 - ETA: 1:12 - loss: 3.5014 - accuracy: 0.11 - ETA: 1:11 - loss: 3.5015 - accuracy: 0.11 - ETA: 1:10 - loss: 3.5016 - accuracy: 0.11 - ETA: 1:09 - loss: 3.5018 - accuracy: 0.11 - ETA: 1:09 - loss: 3.5015 - accuracy: 0.11 - ETA: 1:08 - loss: 3.5009 - accuracy: 0.11 - ETA: 1:07 - loss: 3.5009 - accuracy: 0.11 - ETA: 1:07 - loss: 3.5009 - accuracy: 0.11 - ETA: 1:06 - loss: 3.5012 - accuracy: 0.11 - ETA: 1:05 - loss: 3.5015 - accuracy: 0.11 - ETA: 1:04 - loss: 3.5013 - accuracy: 0.11 - ETA: 1:04 - loss: 3.5013 - accuracy: 0.11 - ETA: 1:03 - loss: 3.5005 - accuracy: 0.11 - ETA: 1:02 - loss: 3.5007 - accuracy: 0.11 - ETA: 1:01 - loss: 3.5007 - accuracy: 0.11 - ETA: 1:01 - loss: 3.5008 - accuracy: 0.11 - ETA: 1:00 - loss: 3.5009 - accuracy: 0.11 - ETA: 59s - loss: 3.5010 - accuracy: 0.1147 - ETA: 58s - loss: 3.5010 - accuracy: 0.114 - ETA: 58s - loss: 3.5005 - accuracy: 0.115 - ETA: 57s - loss: 3.5007 - accuracy: 0.114 - ETA: 56s - loss: 3.5005 - accuracy: 0.114 - ETA: 56s - loss: 3.5009 - accuracy: 0.114 - ETA: 55s - loss: 3.5009 - accuracy: 0.114 - ETA: 54s - loss: 3.5003 - accuracy: 0.114 - ETA: 53s - loss: 3.5002 - accuracy: 0.114 - ETA: 53s - loss: 3.5006 - accuracy: 0.114 - ETA: 52s - loss: 3.5008 - accuracy: 0.114 - ETA: 51s - loss: 3.5006 - accuracy: 0.114 - ETA: 50s - loss: 3.5003 - accuracy: 0.114 - ETA: 50s - loss: 3.4998 - accuracy: 0.115 - ETA: 49s - loss: 3.5000 - accuracy: 0.115 - ETA: 48s - loss: 3.5000 - accuracy: 0.115 - ETA: 47s - loss: 3.5004 - accuracy: 0.114 - ETA: 47s - loss: 3.5006 - accuracy: 0.114 - ETA: 46s - loss: 3.4999 - accuracy: 0.115 - ETA: 45s - loss: 3.4999 - accuracy: 0.115 - ETA: 44s - loss: 3.5000 - accuracy: 0.115 - ETA: 44s - loss: 3.5001 - accuracy: 0.115 - ETA: 43s - loss: 3.5003 - accuracy: 0.115 - ETA: 42s - loss: 3.4997 - accuracy: 0.115 - ETA: 42s - loss: 3.5000 - accuracy: 0.115 - ETA: 41s - loss: 3.5007 - accuracy: 0.115 - ETA: 40s - loss: 3.5008 - accuracy: 0.115 - ETA: 39s - loss: 3.5009 - accuracy: 0.115 - ETA: 39s - loss: 3.5003 - accuracy: 0.115 - ETA: 38s - loss: 3.5004 - accuracy: 0.115 - ETA: 37s - loss: 3.4999 - accuracy: 0.115 - ETA: 36s - loss: 3.5004 - accuracy: 0.115 - ETA: 36s - loss: 3.5004 - accuracy: 0.115 - ETA: 35s - loss: 3.5004 - accuracy: 0.115 - ETA: 34s - loss: 3.5001 - accuracy: 0.115 - ETA: 33s - loss: 3.5000 - accuracy: 0.115 - ETA: 33s - loss: 3.4996 - accuracy: 0.115 - ETA: 32s - loss: 3.4992 - accuracy: 0.115 - ETA: 31s - loss: 3.4993 - accuracy: 0.115 - ETA: 30s - loss: 3.4989 - accuracy: 0.115 - ETA: 30s - loss: 3.4989 - accuracy: 0.115 - ETA: 29s - loss: 3.4988 - accuracy: 0.116 - ETA: 28s - loss: 3.4985 - accuracy: 0.116 - ETA: 28s - loss: 3.4984 - accuracy: 0.116 - ETA: 27s - loss: 3.4979 - accuracy: 0.116 - ETA: 26s - loss: 3.4978 - accuracy: 0.116 - ETA: 25s - loss: 3.4980 - accuracy: 0.116 - ETA: 25s - loss: 3.4972 - accuracy: 0.116 - ETA: 24s - loss: 3.4966 - accuracy: 0.116 - ETA: 23s - loss: 3.4970 - accuracy: 0.116 - ETA: 22s - loss: 3.4971 - accuracy: 0.116 - ETA: 22s - loss: 3.4972 - accuracy: 0.116 - ETA: 21s - loss: 3.4972 - accuracy: 0.116 - ETA: 20s - loss: 3.4976 - accuracy: 0.116 - ETA: 19s - loss: 3.4974 - accuracy: 0.116 - ETA: 19s - loss: 3.4969 - accuracy: 0.116 - ETA: 18s - loss: 3.4970 - accuracy: 0.116 - ETA: 17s - loss: 3.4969 - accuracy: 0.116 - ETA: 16s - loss: 3.4968 - accuracy: 0.116 - ETA: 16s - loss: 3.4969 - accuracy: 0.116 - ETA: 15s - loss: 3.4968 - accuracy: 0.116 - ETA: 14s - loss: 3.4966 - accuracy: 0.116 - ETA: 14s - loss: 3.4971 - accuracy: 0.116 - ETA: 13s - loss: 3.4968 - accuracy: 0.116 - ETA: 12s - loss: 3.4970 - accuracy: 0.116 - ETA: 11s - loss: 3.4972 - accuracy: 0.116 - ETA: 11s - loss: 3.4974 - accuracy: 0.116 - ETA: 10s - loss: 3.4977 - accuracy: 0.116 - ETA: 9s - loss: 3.4976 - accuracy: 0.116 - ETA: 8s - loss: 3.4969 - accuracy: 0.11 - ETA: 8s - loss: 3.4973 - accuracy: 0.11 - ETA: 7s - loss: 3.4974 - accuracy: 0.11 - ETA: 6s - loss: 3.4972 - accuracy: 0.11 - ETA: 5s - loss: 3.4971 - accuracy: 0.11 - ETA: 5s - loss: 3.4975 - accuracy: 0.11 - ETA: 4s - loss: 3.4971 - accuracy: 0.11 - ETA: 3s - loss: 3.4971 - accuracy: 0.11 - ETA: 2s - loss: 3.4968 - accuracy: 0.11 - ETA: 2s - loss: 3.4966 - accuracy: 0.11 - ETA: 1s - loss: 3.4969 - accuracy: 0.11 - ETA: 0s - loss: 3.4968 - accuracy: 0.11 - ETA: 0s - loss: 3.4967 - accuracy: 0.11 - 262s 6ms/step - loss: 3.4967 - accuracy: 0.1173 - val_loss: 3.8999 - val_accuracy: 0.0273\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:57 - loss: 3.6497 - accuracy: 0.10 - ETA: 4:00 - loss: 3.5029 - accuracy: 0.12 - ETA: 4:04 - loss: 3.4145 - accuracy: 0.13 - ETA: 4:04 - loss: 3.4237 - accuracy: 0.12 - ETA: 4:02 - loss: 3.4069 - accuracy: 0.12 - ETA: 4:01 - loss: 3.3940 - accuracy: 0.13 - ETA: 3:59 - loss: 3.4085 - accuracy: 0.13 - ETA: 3:58 - loss: 3.4208 - accuracy: 0.12 - ETA: 3:58 - loss: 3.4266 - accuracy: 0.12 - ETA: 3:57 - loss: 3.4174 - accuracy: 0.12 - ETA: 3:55 - loss: 3.4279 - accuracy: 0.13 - ETA: 3:54 - loss: 3.4244 - accuracy: 0.13 - ETA: 3:53 - loss: 3.4361 - accuracy: 0.12 - ETA: 3:51 - loss: 3.4383 - accuracy: 0.12 - ETA: 3:50 - loss: 3.4359 - accuracy: 0.12 - ETA: 3:50 - loss: 3.4422 - accuracy: 0.12 - ETA: 3:49 - loss: 3.4410 - accuracy: 0.12 - ETA: 3:49 - loss: 3.4450 - accuracy: 0.12 - ETA: 3:48 - loss: 3.4395 - accuracy: 0.12 - ETA: 3:47 - loss: 3.4353 - accuracy: 0.12 - ETA: 3:46 - loss: 3.4383 - accuracy: 0.12 - ETA: 3:45 - loss: 3.4455 - accuracy: 0.12 - ETA: 3:45 - loss: 3.4399 - accuracy: 0.12 - ETA: 3:44 - loss: 3.4365 - accuracy: 0.12 - ETA: 3:44 - loss: 3.4398 - accuracy: 0.12 - ETA: 3:43 - loss: 3.4393 - accuracy: 0.12 - ETA: 3:42 - loss: 3.4413 - accuracy: 0.12 - ETA: 3:42 - loss: 3.4364 - accuracy: 0.13 - ETA: 3:42 - loss: 3.4372 - accuracy: 0.13 - ETA: 3:42 - loss: 3.4353 - accuracy: 0.13 - ETA: 3:41 - loss: 3.4371 - accuracy: 0.13 - ETA: 3:40 - loss: 3.4349 - accuracy: 0.13 - ETA: 3:39 - loss: 3.4360 - accuracy: 0.13 - ETA: 3:38 - loss: 3.4342 - accuracy: 0.13 - ETA: 3:37 - loss: 3.4378 - accuracy: 0.13 - ETA: 3:37 - loss: 3.4429 - accuracy: 0.13 - ETA: 3:37 - loss: 3.4437 - accuracy: 0.13 - ETA: 3:36 - loss: 3.4455 - accuracy: 0.13 - ETA: 3:35 - loss: 3.4465 - accuracy: 0.13 - ETA: 3:34 - loss: 3.4458 - accuracy: 0.13 - ETA: 3:33 - loss: 3.4457 - accuracy: 0.13 - ETA: 3:32 - loss: 3.4504 - accuracy: 0.12 - ETA: 3:32 - loss: 3.4505 - accuracy: 0.12 - ETA: 3:31 - loss: 3.4530 - accuracy: 0.12 - ETA: 3:30 - loss: 3.4547 - accuracy: 0.12 - ETA: 3:29 - loss: 3.4531 - accuracy: 0.12 - ETA: 3:28 - loss: 3.4551 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4531 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4537 - accuracy: 0.12 - ETA: 3:26 - loss: 3.4559 - accuracy: 0.12 - ETA: 3:25 - loss: 3.4567 - accuracy: 0.12 - ETA: 3:25 - loss: 3.4569 - accuracy: 0.12 - ETA: 3:24 - loss: 3.4604 - accuracy: 0.12 - ETA: 3:23 - loss: 3.4614 - accuracy: 0.12 - ETA: 3:23 - loss: 3.4598 - accuracy: 0.12 - ETA: 3:22 - loss: 3.4618 - accuracy: 0.12 - ETA: 3:21 - loss: 3.4652 - accuracy: 0.12 - ETA: 3:20 - loss: 3.4670 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4666 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4657 - accuracy: 0.12 - ETA: 3:18 - loss: 3.4661 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4676 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4663 - accuracy: 0.12 - ETA: 3:16 - loss: 3.4688 - accuracy: 0.12 - ETA: 3:16 - loss: 3.4656 - accuracy: 0.12 - ETA: 3:15 - loss: 3.4641 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4626 - accuracy: 0.12 - ETA: 3:13 - loss: 3.4617 - accuracy: 0.12 - ETA: 3:13 - loss: 3.4641 - accuracy: 0.12 - ETA: 3:12 - loss: 3.4645 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4636 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4623 - accuracy: 0.12 - ETA: 3:10 - loss: 3.4632 - accuracy: 0.12 - ETA: 3:09 - loss: 3.4643 - accuracy: 0.12 - ETA: 3:08 - loss: 3.4619 - accuracy: 0.12 - ETA: 3:08 - loss: 3.4623 - accuracy: 0.12 - ETA: 3:07 - loss: 3.4628 - accuracy: 0.12 - ETA: 3:06 - loss: 3.4619 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4638 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4637 - accuracy: 0.12 - ETA: 3:04 - loss: 3.4634 - accuracy: 0.12 - ETA: 3:03 - loss: 3.4622 - accuracy: 0.12 - ETA: 3:02 - loss: 3.4637 - accuracy: 0.12 - ETA: 3:02 - loss: 3.4635 - accuracy: 0.12 - ETA: 3:01 - loss: 3.4628 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4626 - accuracy: 0.12 - ETA: 2:59 - loss: 3.4616 - accuracy: 0.12 - ETA: 2:58 - loss: 3.4627 - accuracy: 0.12 - ETA: 2:58 - loss: 3.4636 - accuracy: 0.12 - ETA: 2:57 - loss: 3.4638 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4645 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4647 - accuracy: 0.12 - ETA: 2:55 - loss: 3.4656 - accuracy: 0.12 - ETA: 2:54 - loss: 3.4669 - accuracy: 0.12 - ETA: 2:53 - loss: 3.4679 - accuracy: 0.12 - ETA: 2:53 - loss: 3.4677 - accuracy: 0.12 - ETA: 2:52 - loss: 3.4676 - accuracy: 0.12 - ETA: 2:51 - loss: 3.4692 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4698 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4704 - accuracy: 0.12 - ETA: 2:49 - loss: 3.4711 - accuracy: 0.12 - ETA: 2:48 - loss: 3.4713 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4711 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4707 - accuracy: 0.12 - ETA: 2:46 - loss: 3.4695 - accuracy: 0.12 - ETA: 2:45 - loss: 3.4710 - accuracy: 0.12 - ETA: 2:44 - loss: 3.4718 - accuracy: 0.12 - ETA: 2:44 - loss: 3.4723 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4728 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4722 - accuracy: 0.12 - ETA: 2:41 - loss: 3.4703 - accuracy: 0.12 - ETA: 2:41 - loss: 3.4690 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4708 - accuracy: 0.12 - ETA: 2:39 - loss: 3.4717 - accuracy: 0.12 - ETA: 2:38 - loss: 3.4715 - accuracy: 0.12 - ETA: 2:38 - loss: 3.4710 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4719 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4721 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4721 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4740 - accuracy: 0.12 - ETA: 2:34 - loss: 3.4735 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4716 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4716 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4718 - accuracy: 0.12 - ETA: 2:31 - loss: 3.4721 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4711 - accuracy: 0.12 - ETA: 2:29 - loss: 3.4714 - accuracy: 0.12 - ETA: 2:29 - loss: 3.4712 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4702 - accuracy: 0.12 - ETA: 2:27 - loss: 3.4709 - accuracy: 0.12 - ETA: 2:27 - loss: 3.4708 - accuracy: 0.12 - ETA: 2:26 - loss: 3.4709 - accuracy: 0.12 - ETA: 2:25 - loss: 3.4711 - accuracy: 0.12 - ETA: 2:25 - loss: 3.4712 - accuracy: 0.12 - ETA: 2:24 - loss: 3.4717 - accuracy: 0.12 - ETA: 2:23 - loss: 3.4733 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4727 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4729 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4726 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4726 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4739 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4728 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4726 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4729 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4728 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4743 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4736 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4736 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4738 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4744 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4745 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4746 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4756 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4764 - accuracy: 0.11 - ETA: 2:09 - loss: 3.4767 - accuracy: 0.12 - ETA: 2:08 - loss: 3.4757 - accuracy: 0.12 - ETA: 2:08 - loss: 3.4765 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4762 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4755 - accuracy: 0.12 - ETA: 2:05 - loss: 3.4747 - accuracy: 0.12 - ETA: 2:05 - loss: 3.4745 - accuracy: 0.12 - ETA: 2:04 - loss: 3.4744 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4750 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4752 - accuracy: 0.12 - ETA: 2:02 - loss: 3.4754 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4755 - accuracy: 0.11 - ETA: 2:00 - loss: 3.4754 - accuracy: 0.11 - ETA: 2:00 - loss: 3.4760 - accuracy: 0.11 - ETA: 1:59 - loss: 3.4756 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4759 - accuracy: 0.11 - ETA: 1:58 - loss: 3.4758 - accuracy: 0.11 - ETA: 1:57 - loss: 3.4753 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4748 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4744 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4737 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4727 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4732 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4741 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4734 - accuracy: 0.12 - ETA: 1:51 - loss: 3.4730 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4734 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4743 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4738 - accuracy: 0.12 - ETA: 1:48 - loss: 3.4740 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4736 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4743 - accuracy: 0.1204"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:46 - loss: 3.4750 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4758 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4755 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4772 - accuracy: 0.12 - ETA: 1:43 - loss: 3.4766 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4769 - accuracy: 0.11 - ETA: 1:42 - loss: 3.4765 - accuracy: 0.11 - ETA: 1:41 - loss: 3.4775 - accuracy: 0.11 - ETA: 1:40 - loss: 3.4776 - accuracy: 0.11 - ETA: 1:39 - loss: 3.4781 - accuracy: 0.11 - ETA: 1:39 - loss: 3.4778 - accuracy: 0.11 - ETA: 1:38 - loss: 3.4782 - accuracy: 0.11 - ETA: 1:37 - loss: 3.4781 - accuracy: 0.11 - ETA: 1:36 - loss: 3.4790 - accuracy: 0.11 - ETA: 1:36 - loss: 3.4797 - accuracy: 0.11 - ETA: 1:35 - loss: 3.4795 - accuracy: 0.11 - ETA: 1:34 - loss: 3.4799 - accuracy: 0.11 - ETA: 1:33 - loss: 3.4794 - accuracy: 0.11 - ETA: 1:33 - loss: 3.4785 - accuracy: 0.11 - ETA: 1:32 - loss: 3.4781 - accuracy: 0.11 - ETA: 1:31 - loss: 3.4777 - accuracy: 0.11 - ETA: 1:30 - loss: 3.4780 - accuracy: 0.11 - ETA: 1:30 - loss: 3.4782 - accuracy: 0.11 - ETA: 1:29 - loss: 3.4787 - accuracy: 0.11 - ETA: 1:28 - loss: 3.4788 - accuracy: 0.11 - ETA: 1:28 - loss: 3.4791 - accuracy: 0.11 - ETA: 1:27 - loss: 3.4799 - accuracy: 0.11 - ETA: 1:26 - loss: 3.4806 - accuracy: 0.11 - ETA: 1:25 - loss: 3.4814 - accuracy: 0.11 - ETA: 1:25 - loss: 3.4813 - accuracy: 0.11 - ETA: 1:24 - loss: 3.4818 - accuracy: 0.11 - ETA: 1:23 - loss: 3.4813 - accuracy: 0.11 - ETA: 1:22 - loss: 3.4815 - accuracy: 0.11 - ETA: 1:22 - loss: 3.4810 - accuracy: 0.11 - ETA: 1:21 - loss: 3.4811 - accuracy: 0.11 - ETA: 1:20 - loss: 3.4809 - accuracy: 0.11 - ETA: 1:19 - loss: 3.4811 - accuracy: 0.11 - ETA: 1:19 - loss: 3.4805 - accuracy: 0.11 - ETA: 1:18 - loss: 3.4799 - accuracy: 0.11 - ETA: 1:17 - loss: 3.4801 - accuracy: 0.11 - ETA: 1:16 - loss: 3.4804 - accuracy: 0.11 - ETA: 1:16 - loss: 3.4799 - accuracy: 0.11 - ETA: 1:15 - loss: 3.4802 - accuracy: 0.11 - ETA: 1:14 - loss: 3.4810 - accuracy: 0.11 - ETA: 1:14 - loss: 3.4807 - accuracy: 0.11 - ETA: 1:13 - loss: 3.4811 - accuracy: 0.11 - ETA: 1:12 - loss: 3.4812 - accuracy: 0.11 - ETA: 1:11 - loss: 3.4813 - accuracy: 0.11 - ETA: 1:11 - loss: 3.4814 - accuracy: 0.11 - ETA: 1:10 - loss: 3.4809 - accuracy: 0.11 - ETA: 1:09 - loss: 3.4805 - accuracy: 0.11 - ETA: 1:08 - loss: 3.4803 - accuracy: 0.11 - ETA: 1:08 - loss: 3.4797 - accuracy: 0.11 - ETA: 1:07 - loss: 3.4805 - accuracy: 0.11 - ETA: 1:06 - loss: 3.4802 - accuracy: 0.11 - ETA: 1:05 - loss: 3.4808 - accuracy: 0.11 - ETA: 1:05 - loss: 3.4805 - accuracy: 0.11 - ETA: 1:04 - loss: 3.4806 - accuracy: 0.11 - ETA: 1:03 - loss: 3.4802 - accuracy: 0.11 - ETA: 1:02 - loss: 3.4805 - accuracy: 0.11 - ETA: 1:02 - loss: 3.4799 - accuracy: 0.11 - ETA: 1:01 - loss: 3.4803 - accuracy: 0.11 - ETA: 1:00 - loss: 3.4805 - accuracy: 0.11 - ETA: 1:00 - loss: 3.4806 - accuracy: 0.11 - ETA: 59s - loss: 3.4807 - accuracy: 0.1192 - ETA: 58s - loss: 3.4804 - accuracy: 0.119 - ETA: 57s - loss: 3.4811 - accuracy: 0.119 - ETA: 57s - loss: 3.4811 - accuracy: 0.119 - ETA: 56s - loss: 3.4809 - accuracy: 0.119 - ETA: 55s - loss: 3.4804 - accuracy: 0.119 - ETA: 54s - loss: 3.4805 - accuracy: 0.119 - ETA: 54s - loss: 3.4812 - accuracy: 0.118 - ETA: 53s - loss: 3.4819 - accuracy: 0.118 - ETA: 52s - loss: 3.4823 - accuracy: 0.118 - ETA: 52s - loss: 3.4829 - accuracy: 0.118 - ETA: 51s - loss: 3.4826 - accuracy: 0.118 - ETA: 50s - loss: 3.4825 - accuracy: 0.118 - ETA: 49s - loss: 3.4829 - accuracy: 0.118 - ETA: 49s - loss: 3.4833 - accuracy: 0.118 - ETA: 48s - loss: 3.4829 - accuracy: 0.118 - ETA: 47s - loss: 3.4831 - accuracy: 0.118 - ETA: 46s - loss: 3.4831 - accuracy: 0.118 - ETA: 46s - loss: 3.4837 - accuracy: 0.118 - ETA: 45s - loss: 3.4844 - accuracy: 0.118 - ETA: 44s - loss: 3.4842 - accuracy: 0.118 - ETA: 43s - loss: 3.4844 - accuracy: 0.118 - ETA: 43s - loss: 3.4845 - accuracy: 0.118 - ETA: 42s - loss: 3.4847 - accuracy: 0.118 - ETA: 41s - loss: 3.4847 - accuracy: 0.118 - ETA: 40s - loss: 3.4853 - accuracy: 0.118 - ETA: 40s - loss: 3.4849 - accuracy: 0.118 - ETA: 39s - loss: 3.4853 - accuracy: 0.118 - ETA: 38s - loss: 3.4856 - accuracy: 0.118 - ETA: 37s - loss: 3.4857 - accuracy: 0.118 - ETA: 37s - loss: 3.4858 - accuracy: 0.118 - ETA: 36s - loss: 3.4859 - accuracy: 0.118 - ETA: 35s - loss: 3.4859 - accuracy: 0.118 - ETA: 35s - loss: 3.4864 - accuracy: 0.117 - ETA: 34s - loss: 3.4864 - accuracy: 0.118 - ETA: 33s - loss: 3.4868 - accuracy: 0.117 - ETA: 32s - loss: 3.4869 - accuracy: 0.118 - ETA: 32s - loss: 3.4869 - accuracy: 0.117 - ETA: 31s - loss: 3.4870 - accuracy: 0.117 - ETA: 30s - loss: 3.4875 - accuracy: 0.117 - ETA: 29s - loss: 3.4878 - accuracy: 0.117 - ETA: 29s - loss: 3.4882 - accuracy: 0.117 - ETA: 28s - loss: 3.4885 - accuracy: 0.117 - ETA: 27s - loss: 3.4883 - accuracy: 0.117 - ETA: 26s - loss: 3.4883 - accuracy: 0.117 - ETA: 26s - loss: 3.4882 - accuracy: 0.117 - ETA: 25s - loss: 3.4880 - accuracy: 0.117 - ETA: 24s - loss: 3.4884 - accuracy: 0.117 - ETA: 23s - loss: 3.4882 - accuracy: 0.117 - ETA: 23s - loss: 3.4883 - accuracy: 0.117 - ETA: 22s - loss: 3.4882 - accuracy: 0.117 - ETA: 21s - loss: 3.4887 - accuracy: 0.117 - ETA: 20s - loss: 3.4887 - accuracy: 0.117 - ETA: 20s - loss: 3.4884 - accuracy: 0.117 - ETA: 19s - loss: 3.4885 - accuracy: 0.117 - ETA: 18s - loss: 3.4887 - accuracy: 0.117 - ETA: 17s - loss: 3.4889 - accuracy: 0.117 - ETA: 17s - loss: 3.4894 - accuracy: 0.117 - ETA: 16s - loss: 3.4891 - accuracy: 0.117 - ETA: 15s - loss: 3.4881 - accuracy: 0.117 - ETA: 14s - loss: 3.4878 - accuracy: 0.117 - ETA: 14s - loss: 3.4880 - accuracy: 0.117 - ETA: 13s - loss: 3.4878 - accuracy: 0.117 - ETA: 12s - loss: 3.4875 - accuracy: 0.117 - ETA: 12s - loss: 3.4875 - accuracy: 0.117 - ETA: 11s - loss: 3.4871 - accuracy: 0.117 - ETA: 10s - loss: 3.4874 - accuracy: 0.117 - ETA: 9s - loss: 3.4872 - accuracy: 0.117 - ETA: 9s - loss: 3.4876 - accuracy: 0.11 - ETA: 8s - loss: 3.4875 - accuracy: 0.11 - ETA: 7s - loss: 3.4873 - accuracy: 0.11 - ETA: 6s - loss: 3.4871 - accuracy: 0.11 - ETA: 6s - loss: 3.4874 - accuracy: 0.11 - ETA: 5s - loss: 3.4875 - accuracy: 0.11 - ETA: 4s - loss: 3.4875 - accuracy: 0.11 - ETA: 3s - loss: 3.4876 - accuracy: 0.11 - ETA: 3s - loss: 3.4874 - accuracy: 0.11 - ETA: 2s - loss: 3.4876 - accuracy: 0.11 - ETA: 1s - loss: 3.4875 - accuracy: 0.11 - ETA: 0s - loss: 3.4880 - accuracy: 0.11 - ETA: 0s - loss: 3.4880 - accuracy: 0.11 - 267s 6ms/step - loss: 3.4880 - accuracy: 0.1172 - val_loss: 3.9092 - val_accuracy: 0.0309\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:04 - loss: 3.5567 - accuracy: 0.09 - ETA: 4:20 - loss: 3.5422 - accuracy: 0.10 - ETA: 4:17 - loss: 3.4749 - accuracy: 0.11 - ETA: 4:18 - loss: 3.4258 - accuracy: 0.12 - ETA: 4:15 - loss: 3.4388 - accuracy: 0.12 - ETA: 4:15 - loss: 3.4470 - accuracy: 0.11 - ETA: 4:14 - loss: 3.4540 - accuracy: 0.12 - ETA: 4:13 - loss: 3.4554 - accuracy: 0.11 - ETA: 4:11 - loss: 3.4469 - accuracy: 0.11 - ETA: 4:09 - loss: 3.4479 - accuracy: 0.11 - ETA: 4:07 - loss: 3.4385 - accuracy: 0.12 - ETA: 4:05 - loss: 3.4405 - accuracy: 0.11 - ETA: 4:06 - loss: 3.4496 - accuracy: 0.11 - ETA: 4:06 - loss: 3.4640 - accuracy: 0.11 - ETA: 4:05 - loss: 3.4695 - accuracy: 0.11 - ETA: 4:04 - loss: 3.4660 - accuracy: 0.11 - ETA: 4:04 - loss: 3.4722 - accuracy: 0.11 - ETA: 4:03 - loss: 3.4704 - accuracy: 0.11 - ETA: 4:03 - loss: 3.4735 - accuracy: 0.11 - ETA: 4:02 - loss: 3.4792 - accuracy: 0.11 - ETA: 4:02 - loss: 3.4881 - accuracy: 0.11 - ETA: 4:01 - loss: 3.4882 - accuracy: 0.11 - ETA: 4:00 - loss: 3.4839 - accuracy: 0.11 - ETA: 4:00 - loss: 3.4790 - accuracy: 0.11 - ETA: 3:58 - loss: 3.4780 - accuracy: 0.11 - ETA: 3:58 - loss: 3.4836 - accuracy: 0.11 - ETA: 3:57 - loss: 3.4884 - accuracy: 0.11 - ETA: 3:57 - loss: 3.4888 - accuracy: 0.11 - ETA: 3:56 - loss: 3.4923 - accuracy: 0.11 - ETA: 3:54 - loss: 3.4928 - accuracy: 0.11 - ETA: 3:53 - loss: 3.4942 - accuracy: 0.10 - ETA: 3:52 - loss: 3.4942 - accuracy: 0.10 - ETA: 3:51 - loss: 3.4935 - accuracy: 0.10 - ETA: 3:50 - loss: 3.4915 - accuracy: 0.10 - ETA: 3:49 - loss: 3.4901 - accuracy: 0.10 - ETA: 3:49 - loss: 3.4917 - accuracy: 0.10 - ETA: 3:48 - loss: 3.4957 - accuracy: 0.10 - ETA: 3:48 - loss: 3.4899 - accuracy: 0.10 - ETA: 3:47 - loss: 3.4894 - accuracy: 0.11 - ETA: 3:47 - loss: 3.4821 - accuracy: 0.11 - ETA: 3:46 - loss: 3.4810 - accuracy: 0.11 - ETA: 3:45 - loss: 3.4789 - accuracy: 0.11 - ETA: 3:45 - loss: 3.4788 - accuracy: 0.11 - ETA: 3:44 - loss: 3.4795 - accuracy: 0.11 - ETA: 3:43 - loss: 3.4764 - accuracy: 0.11 - ETA: 3:43 - loss: 3.4774 - accuracy: 0.11 - ETA: 3:43 - loss: 3.4731 - accuracy: 0.11 - ETA: 3:42 - loss: 3.4752 - accuracy: 0.11 - ETA: 3:41 - loss: 3.4758 - accuracy: 0.11 - ETA: 3:40 - loss: 3.4768 - accuracy: 0.11 - ETA: 3:39 - loss: 3.4757 - accuracy: 0.11 - ETA: 3:38 - loss: 3.4783 - accuracy: 0.11 - ETA: 3:37 - loss: 3.4784 - accuracy: 0.11 - ETA: 3:36 - loss: 3.4812 - accuracy: 0.11 - ETA: 3:36 - loss: 3.4795 - accuracy: 0.11 - ETA: 3:35 - loss: 3.4806 - accuracy: 0.11 - ETA: 3:34 - loss: 3.4802 - accuracy: 0.11 - ETA: 3:33 - loss: 3.4781 - accuracy: 0.11 - ETA: 3:32 - loss: 3.4791 - accuracy: 0.11 - ETA: 3:32 - loss: 3.4780 - accuracy: 0.11 - ETA: 3:31 - loss: 3.4785 - accuracy: 0.11 - ETA: 3:30 - loss: 3.4781 - accuracy: 0.11 - ETA: 3:29 - loss: 3.4774 - accuracy: 0.11 - ETA: 3:29 - loss: 3.4779 - accuracy: 0.11 - ETA: 3:28 - loss: 3.4794 - accuracy: 0.11 - ETA: 3:27 - loss: 3.4779 - accuracy: 0.11 - ETA: 3:27 - loss: 3.4780 - accuracy: 0.11 - ETA: 3:26 - loss: 3.4796 - accuracy: 0.11 - ETA: 3:25 - loss: 3.4805 - accuracy: 0.11 - ETA: 3:25 - loss: 3.4789 - accuracy: 0.11 - ETA: 3:24 - loss: 3.4771 - accuracy: 0.11 - ETA: 3:23 - loss: 3.4762 - accuracy: 0.11 - ETA: 3:22 - loss: 3.4744 - accuracy: 0.11 - ETA: 3:21 - loss: 3.4738 - accuracy: 0.11 - ETA: 3:20 - loss: 3.4718 - accuracy: 0.11 - ETA: 3:19 - loss: 3.4738 - accuracy: 0.11 - ETA: 3:19 - loss: 3.4735 - accuracy: 0.11 - ETA: 3:18 - loss: 3.4753 - accuracy: 0.11 - ETA: 3:17 - loss: 3.4770 - accuracy: 0.11 - ETA: 3:16 - loss: 3.4773 - accuracy: 0.11 - ETA: 3:16 - loss: 3.4775 - accuracy: 0.11 - ETA: 3:15 - loss: 3.4770 - accuracy: 0.11 - ETA: 3:14 - loss: 3.4744 - accuracy: 0.11 - ETA: 3:13 - loss: 3.4733 - accuracy: 0.12 - ETA: 3:12 - loss: 3.4737 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4742 - accuracy: 0.11 - ETA: 3:11 - loss: 3.4732 - accuracy: 0.12 - ETA: 3:10 - loss: 3.4724 - accuracy: 0.12 - ETA: 3:09 - loss: 3.4729 - accuracy: 0.12 - ETA: 3:08 - loss: 3.4726 - accuracy: 0.12 - ETA: 3:07 - loss: 3.4739 - accuracy: 0.11 - ETA: 3:06 - loss: 3.4742 - accuracy: 0.11 - ETA: 3:05 - loss: 3.4763 - accuracy: 0.11 - ETA: 3:04 - loss: 3.4757 - accuracy: 0.11 - ETA: 3:03 - loss: 3.4766 - accuracy: 0.11 - ETA: 3:02 - loss: 3.4773 - accuracy: 0.11 - ETA: 3:02 - loss: 3.4749 - accuracy: 0.11 - ETA: 3:01 - loss: 3.4755 - accuracy: 0.11 - ETA: 3:00 - loss: 3.4766 - accuracy: 0.11 - ETA: 2:59 - loss: 3.4761 - accuracy: 0.11 - ETA: 2:59 - loss: 3.4762 - accuracy: 0.11 - ETA: 2:58 - loss: 3.4762 - accuracy: 0.11 - ETA: 2:57 - loss: 3.4758 - accuracy: 0.11 - ETA: 2:56 - loss: 3.4767 - accuracy: 0.11 - ETA: 2:56 - loss: 3.4772 - accuracy: 0.11 - ETA: 2:55 - loss: 3.4766 - accuracy: 0.11 - ETA: 2:54 - loss: 3.4769 - accuracy: 0.11 - ETA: 2:53 - loss: 3.4775 - accuracy: 0.11 - ETA: 2:53 - loss: 3.4778 - accuracy: 0.11 - ETA: 2:52 - loss: 3.4764 - accuracy: 0.11 - ETA: 2:51 - loss: 3.4763 - accuracy: 0.11 - ETA: 2:50 - loss: 3.4774 - accuracy: 0.11 - ETA: 2:49 - loss: 3.4764 - accuracy: 0.11 - ETA: 2:48 - loss: 3.4771 - accuracy: 0.11 - ETA: 2:47 - loss: 3.4762 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4768 - accuracy: 0.11 - ETA: 2:46 - loss: 3.4765 - accuracy: 0.11 - ETA: 2:45 - loss: 3.4762 - accuracy: 0.12 - ETA: 2:44 - loss: 3.4751 - accuracy: 0.12 - ETA: 2:44 - loss: 3.4759 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4766 - accuracy: 0.11 - ETA: 2:42 - loss: 3.4758 - accuracy: 0.11 - ETA: 2:41 - loss: 3.4762 - accuracy: 0.11 - ETA: 2:41 - loss: 3.4767 - accuracy: 0.11 - ETA: 2:40 - loss: 3.4762 - accuracy: 0.11 - ETA: 2:39 - loss: 3.4766 - accuracy: 0.11 - ETA: 2:38 - loss: 3.4763 - accuracy: 0.11 - ETA: 2:38 - loss: 3.4757 - accuracy: 0.11 - ETA: 2:37 - loss: 3.4759 - accuracy: 0.11 - ETA: 2:36 - loss: 3.4756 - accuracy: 0.11 - ETA: 2:35 - loss: 3.4754 - accuracy: 0.11 - ETA: 2:34 - loss: 3.4755 - accuracy: 0.11 - ETA: 2:34 - loss: 3.4755 - accuracy: 0.11 - ETA: 2:33 - loss: 3.4756 - accuracy: 0.11 - ETA: 2:32 - loss: 3.4764 - accuracy: 0.11 - ETA: 2:31 - loss: 3.4764 - accuracy: 0.11 - ETA: 2:30 - loss: 3.4770 - accuracy: 0.11 - ETA: 2:30 - loss: 3.4763 - accuracy: 0.11 - ETA: 2:29 - loss: 3.4776 - accuracy: 0.11 - ETA: 2:28 - loss: 3.4773 - accuracy: 0.11 - ETA: 2:27 - loss: 3.4782 - accuracy: 0.11 - ETA: 2:26 - loss: 3.4778 - accuracy: 0.11 - ETA: 2:26 - loss: 3.4781 - accuracy: 0.11 - ETA: 2:25 - loss: 3.4783 - accuracy: 0.11 - ETA: 2:24 - loss: 3.4786 - accuracy: 0.11 - ETA: 2:24 - loss: 3.4783 - accuracy: 0.11 - ETA: 2:23 - loss: 3.4792 - accuracy: 0.11 - ETA: 2:22 - loss: 3.4801 - accuracy: 0.11 - ETA: 2:21 - loss: 3.4799 - accuracy: 0.11 - ETA: 2:20 - loss: 3.4811 - accuracy: 0.11 - ETA: 2:20 - loss: 3.4810 - accuracy: 0.11 - ETA: 2:19 - loss: 3.4805 - accuracy: 0.11 - ETA: 2:18 - loss: 3.4796 - accuracy: 0.11 - ETA: 2:17 - loss: 3.4808 - accuracy: 0.11 - ETA: 2:16 - loss: 3.4811 - accuracy: 0.11 - ETA: 2:16 - loss: 3.4819 - accuracy: 0.11 - ETA: 2:15 - loss: 3.4815 - accuracy: 0.11 - ETA: 2:14 - loss: 3.4815 - accuracy: 0.11 - ETA: 2:13 - loss: 3.4810 - accuracy: 0.11 - ETA: 2:12 - loss: 3.4813 - accuracy: 0.11 - ETA: 2:12 - loss: 3.4816 - accuracy: 0.11 - ETA: 2:11 - loss: 3.4816 - accuracy: 0.11 - ETA: 2:10 - loss: 3.4824 - accuracy: 0.11 - ETA: 2:09 - loss: 3.4831 - accuracy: 0.11 - ETA: 2:08 - loss: 3.4831 - accuracy: 0.11 - ETA: 2:08 - loss: 3.4834 - accuracy: 0.11 - ETA: 2:07 - loss: 3.4837 - accuracy: 0.11 - ETA: 2:06 - loss: 3.4842 - accuracy: 0.11 - ETA: 2:05 - loss: 3.4829 - accuracy: 0.11 - ETA: 2:05 - loss: 3.4833 - accuracy: 0.11 - ETA: 2:04 - loss: 3.4832 - accuracy: 0.11 - ETA: 2:03 - loss: 3.4830 - accuracy: 0.11 - ETA: 2:02 - loss: 3.4823 - accuracy: 0.11 - ETA: 2:02 - loss: 3.4825 - accuracy: 0.11 - ETA: 2:01 - loss: 3.4824 - accuracy: 0.11 - ETA: 2:00 - loss: 3.4822 - accuracy: 0.11 - ETA: 1:59 - loss: 3.4822 - accuracy: 0.11 - ETA: 1:58 - loss: 3.4810 - accuracy: 0.11 - ETA: 1:58 - loss: 3.4812 - accuracy: 0.11 - ETA: 1:57 - loss: 3.4807 - accuracy: 0.11 - ETA: 1:56 - loss: 3.4810 - accuracy: 0.11 - ETA: 1:55 - loss: 3.4812 - accuracy: 0.11 - ETA: 1:55 - loss: 3.4816 - accuracy: 0.11 - ETA: 1:54 - loss: 3.4816 - accuracy: 0.11 - ETA: 1:53 - loss: 3.4812 - accuracy: 0.11 - ETA: 1:52 - loss: 3.4804 - accuracy: 0.1185"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:52 - loss: 3.4814 - accuracy: 0.11 - ETA: 1:51 - loss: 3.4819 - accuracy: 0.11 - ETA: 1:50 - loss: 3.4816 - accuracy: 0.11 - ETA: 1:49 - loss: 3.4804 - accuracy: 0.11 - ETA: 1:49 - loss: 3.4800 - accuracy: 0.11 - ETA: 1:48 - loss: 3.4802 - accuracy: 0.11 - ETA: 1:47 - loss: 3.4796 - accuracy: 0.11 - ETA: 1:46 - loss: 3.4788 - accuracy: 0.11 - ETA: 1:45 - loss: 3.4789 - accuracy: 0.11 - ETA: 1:45 - loss: 3.4773 - accuracy: 0.11 - ETA: 1:44 - loss: 3.4779 - accuracy: 0.11 - ETA: 1:43 - loss: 3.4777 - accuracy: 0.11 - ETA: 1:42 - loss: 3.4774 - accuracy: 0.11 - ETA: 1:41 - loss: 3.4770 - accuracy: 0.11 - ETA: 1:41 - loss: 3.4771 - accuracy: 0.11 - ETA: 1:40 - loss: 3.4772 - accuracy: 0.11 - ETA: 1:39 - loss: 3.4773 - accuracy: 0.11 - ETA: 1:38 - loss: 3.4770 - accuracy: 0.11 - ETA: 1:38 - loss: 3.4768 - accuracy: 0.11 - ETA: 1:37 - loss: 3.4770 - accuracy: 0.11 - ETA: 1:36 - loss: 3.4767 - accuracy: 0.11 - ETA: 1:35 - loss: 3.4772 - accuracy: 0.11 - ETA: 1:35 - loss: 3.4770 - accuracy: 0.11 - ETA: 1:34 - loss: 3.4776 - accuracy: 0.11 - ETA: 1:33 - loss: 3.4784 - accuracy: 0.11 - ETA: 1:32 - loss: 3.4782 - accuracy: 0.11 - ETA: 1:31 - loss: 3.4786 - accuracy: 0.11 - ETA: 1:31 - loss: 3.4783 - accuracy: 0.11 - ETA: 1:30 - loss: 3.4778 - accuracy: 0.11 - ETA: 1:29 - loss: 3.4772 - accuracy: 0.11 - ETA: 1:28 - loss: 3.4773 - accuracy: 0.11 - ETA: 1:27 - loss: 3.4770 - accuracy: 0.11 - ETA: 1:27 - loss: 3.4770 - accuracy: 0.11 - ETA: 1:26 - loss: 3.4777 - accuracy: 0.11 - ETA: 1:25 - loss: 3.4769 - accuracy: 0.11 - ETA: 1:24 - loss: 3.4760 - accuracy: 0.11 - ETA: 1:24 - loss: 3.4764 - accuracy: 0.11 - ETA: 1:23 - loss: 3.4757 - accuracy: 0.11 - ETA: 1:22 - loss: 3.4758 - accuracy: 0.11 - ETA: 1:21 - loss: 3.4758 - accuracy: 0.11 - ETA: 1:20 - loss: 3.4758 - accuracy: 0.11 - ETA: 1:20 - loss: 3.4756 - accuracy: 0.11 - ETA: 1:19 - loss: 3.4757 - accuracy: 0.11 - ETA: 1:18 - loss: 3.4754 - accuracy: 0.11 - ETA: 1:17 - loss: 3.4745 - accuracy: 0.11 - ETA: 1:17 - loss: 3.4742 - accuracy: 0.11 - ETA: 1:16 - loss: 3.4734 - accuracy: 0.11 - ETA: 1:15 - loss: 3.4730 - accuracy: 0.11 - ETA: 1:14 - loss: 3.4735 - accuracy: 0.11 - ETA: 1:13 - loss: 3.4740 - accuracy: 0.11 - ETA: 1:13 - loss: 3.4745 - accuracy: 0.11 - ETA: 1:12 - loss: 3.4750 - accuracy: 0.11 - ETA: 1:11 - loss: 3.4745 - accuracy: 0.11 - ETA: 1:10 - loss: 3.4750 - accuracy: 0.11 - ETA: 1:10 - loss: 3.4750 - accuracy: 0.11 - ETA: 1:09 - loss: 3.4747 - accuracy: 0.11 - ETA: 1:08 - loss: 3.4757 - accuracy: 0.11 - ETA: 1:07 - loss: 3.4761 - accuracy: 0.11 - ETA: 1:06 - loss: 3.4761 - accuracy: 0.11 - ETA: 1:06 - loss: 3.4757 - accuracy: 0.11 - ETA: 1:05 - loss: 3.4761 - accuracy: 0.11 - ETA: 1:04 - loss: 3.4762 - accuracy: 0.11 - ETA: 1:03 - loss: 3.4764 - accuracy: 0.11 - ETA: 1:03 - loss: 3.4763 - accuracy: 0.11 - ETA: 1:02 - loss: 3.4765 - accuracy: 0.11 - ETA: 1:01 - loss: 3.4769 - accuracy: 0.11 - ETA: 1:00 - loss: 3.4776 - accuracy: 0.11 - ETA: 59s - loss: 3.4780 - accuracy: 0.1186 - ETA: 59s - loss: 3.4780 - accuracy: 0.118 - ETA: 58s - loss: 3.4776 - accuracy: 0.118 - ETA: 57s - loss: 3.4778 - accuracy: 0.118 - ETA: 56s - loss: 3.4776 - accuracy: 0.118 - ETA: 56s - loss: 3.4776 - accuracy: 0.118 - ETA: 55s - loss: 3.4777 - accuracy: 0.118 - ETA: 54s - loss: 3.4776 - accuracy: 0.118 - ETA: 53s - loss: 3.4777 - accuracy: 0.118 - ETA: 52s - loss: 3.4781 - accuracy: 0.118 - ETA: 52s - loss: 3.4786 - accuracy: 0.118 - ETA: 51s - loss: 3.4786 - accuracy: 0.118 - ETA: 50s - loss: 3.4790 - accuracy: 0.118 - ETA: 49s - loss: 3.4787 - accuracy: 0.118 - ETA: 49s - loss: 3.4791 - accuracy: 0.118 - ETA: 48s - loss: 3.4792 - accuracy: 0.118 - ETA: 47s - loss: 3.4797 - accuracy: 0.117 - ETA: 46s - loss: 3.4795 - accuracy: 0.118 - ETA: 45s - loss: 3.4793 - accuracy: 0.118 - ETA: 45s - loss: 3.4797 - accuracy: 0.118 - ETA: 44s - loss: 3.4797 - accuracy: 0.118 - ETA: 43s - loss: 3.4808 - accuracy: 0.117 - ETA: 42s - loss: 3.4806 - accuracy: 0.118 - ETA: 42s - loss: 3.4807 - accuracy: 0.118 - ETA: 41s - loss: 3.4804 - accuracy: 0.118 - ETA: 40s - loss: 3.4806 - accuracy: 0.118 - ETA: 39s - loss: 3.4804 - accuracy: 0.118 - ETA: 38s - loss: 3.4800 - accuracy: 0.118 - ETA: 38s - loss: 3.4803 - accuracy: 0.118 - ETA: 37s - loss: 3.4803 - accuracy: 0.118 - ETA: 36s - loss: 3.4807 - accuracy: 0.118 - ETA: 35s - loss: 3.4811 - accuracy: 0.118 - ETA: 35s - loss: 3.4814 - accuracy: 0.117 - ETA: 34s - loss: 3.4814 - accuracy: 0.118 - ETA: 33s - loss: 3.4816 - accuracy: 0.118 - ETA: 32s - loss: 3.4818 - accuracy: 0.117 - ETA: 31s - loss: 3.4818 - accuracy: 0.118 - ETA: 31s - loss: 3.4816 - accuracy: 0.118 - ETA: 30s - loss: 3.4818 - accuracy: 0.117 - ETA: 29s - loss: 3.4819 - accuracy: 0.117 - ETA: 28s - loss: 3.4822 - accuracy: 0.117 - ETA: 28s - loss: 3.4826 - accuracy: 0.117 - ETA: 27s - loss: 3.4822 - accuracy: 0.117 - ETA: 26s - loss: 3.4825 - accuracy: 0.117 - ETA: 25s - loss: 3.4816 - accuracy: 0.117 - ETA: 24s - loss: 3.4820 - accuracy: 0.117 - ETA: 24s - loss: 3.4822 - accuracy: 0.117 - ETA: 23s - loss: 3.4821 - accuracy: 0.117 - ETA: 22s - loss: 3.4819 - accuracy: 0.117 - ETA: 21s - loss: 3.4822 - accuracy: 0.117 - ETA: 21s - loss: 3.4819 - accuracy: 0.117 - ETA: 20s - loss: 3.4813 - accuracy: 0.117 - ETA: 19s - loss: 3.4813 - accuracy: 0.117 - ETA: 18s - loss: 3.4807 - accuracy: 0.117 - ETA: 17s - loss: 3.4812 - accuracy: 0.117 - ETA: 17s - loss: 3.4812 - accuracy: 0.117 - ETA: 16s - loss: 3.4810 - accuracy: 0.117 - ETA: 15s - loss: 3.4808 - accuracy: 0.117 - ETA: 14s - loss: 3.4810 - accuracy: 0.117 - ETA: 14s - loss: 3.4807 - accuracy: 0.117 - ETA: 13s - loss: 3.4812 - accuracy: 0.117 - ETA: 12s - loss: 3.4812 - accuracy: 0.117 - ETA: 11s - loss: 3.4811 - accuracy: 0.117 - ETA: 10s - loss: 3.4804 - accuracy: 0.118 - ETA: 10s - loss: 3.4808 - accuracy: 0.117 - ETA: 9s - loss: 3.4807 - accuracy: 0.117 - ETA: 8s - loss: 3.4807 - accuracy: 0.11 - ETA: 7s - loss: 3.4812 - accuracy: 0.11 - ETA: 7s - loss: 3.4815 - accuracy: 0.11 - ETA: 6s - loss: 3.4815 - accuracy: 0.11 - ETA: 5s - loss: 3.4810 - accuracy: 0.11 - ETA: 4s - loss: 3.4806 - accuracy: 0.11 - ETA: 3s - loss: 3.4803 - accuracy: 0.11 - ETA: 3s - loss: 3.4800 - accuracy: 0.11 - ETA: 2s - loss: 3.4804 - accuracy: 0.11 - ETA: 1s - loss: 3.4800 - accuracy: 0.11 - ETA: 0s - loss: 3.4796 - accuracy: 0.11 - ETA: 0s - loss: 3.4795 - accuracy: 0.11 - 278s 7ms/step - loss: 3.4796 - accuracy: 0.1182 - val_loss: 3.9139 - val_accuracy: 0.0331\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:17 - loss: 3.4167 - accuracy: 0.15 - ETA: 4:02 - loss: 3.4427 - accuracy: 0.12 - ETA: 3:58 - loss: 3.4628 - accuracy: 0.14 - ETA: 3:57 - loss: 3.5020 - accuracy: 0.12 - ETA: 3:57 - loss: 3.4999 - accuracy: 0.11 - ETA: 4:01 - loss: 3.4879 - accuracy: 0.10 - ETA: 4:02 - loss: 3.4805 - accuracy: 0.11 - ETA: 4:03 - loss: 3.4875 - accuracy: 0.11 - ETA: 4:04 - loss: 3.4712 - accuracy: 0.11 - ETA: 4:03 - loss: 3.4699 - accuracy: 0.11 - ETA: 4:03 - loss: 3.4688 - accuracy: 0.11 - ETA: 4:01 - loss: 3.4663 - accuracy: 0.11 - ETA: 4:00 - loss: 3.4628 - accuracy: 0.11 - ETA: 4:01 - loss: 3.4635 - accuracy: 0.11 - ETA: 4:00 - loss: 3.4663 - accuracy: 0.11 - ETA: 4:00 - loss: 3.4728 - accuracy: 0.11 - ETA: 3:59 - loss: 3.4643 - accuracy: 0.11 - ETA: 3:58 - loss: 3.4635 - accuracy: 0.11 - ETA: 3:56 - loss: 3.4651 - accuracy: 0.11 - ETA: 3:57 - loss: 3.4704 - accuracy: 0.11 - ETA: 3:56 - loss: 3.4732 - accuracy: 0.11 - ETA: 3:54 - loss: 3.4711 - accuracy: 0.11 - ETA: 3:54 - loss: 3.4722 - accuracy: 0.11 - ETA: 3:53 - loss: 3.4738 - accuracy: 0.11 - ETA: 3:52 - loss: 3.4746 - accuracy: 0.11 - ETA: 3:51 - loss: 3.4687 - accuracy: 0.11 - ETA: 3:51 - loss: 3.4662 - accuracy: 0.11 - ETA: 3:50 - loss: 3.4683 - accuracy: 0.11 - ETA: 3:49 - loss: 3.4690 - accuracy: 0.11 - ETA: 3:48 - loss: 3.4676 - accuracy: 0.11 - ETA: 3:47 - loss: 3.4733 - accuracy: 0.11 - ETA: 3:47 - loss: 3.4754 - accuracy: 0.11 - ETA: 3:46 - loss: 3.4716 - accuracy: 0.11 - ETA: 3:46 - loss: 3.4779 - accuracy: 0.11 - ETA: 3:45 - loss: 3.4803 - accuracy: 0.11 - ETA: 3:44 - loss: 3.4802 - accuracy: 0.11 - ETA: 3:44 - loss: 3.4799 - accuracy: 0.11 - ETA: 3:43 - loss: 3.4778 - accuracy: 0.11 - ETA: 3:42 - loss: 3.4790 - accuracy: 0.11 - ETA: 3:42 - loss: 3.4827 - accuracy: 0.11 - ETA: 3:41 - loss: 3.4808 - accuracy: 0.11 - ETA: 3:40 - loss: 3.4831 - accuracy: 0.11 - ETA: 3:40 - loss: 3.4843 - accuracy: 0.11 - ETA: 3:39 - loss: 3.4851 - accuracy: 0.11 - ETA: 3:38 - loss: 3.4863 - accuracy: 0.11 - ETA: 3:37 - loss: 3.4849 - accuracy: 0.11 - ETA: 3:36 - loss: 3.4829 - accuracy: 0.11 - ETA: 3:35 - loss: 3.4819 - accuracy: 0.11 - ETA: 3:35 - loss: 3.4818 - accuracy: 0.11 - ETA: 3:34 - loss: 3.4802 - accuracy: 0.11 - ETA: 3:34 - loss: 3.4771 - accuracy: 0.11 - ETA: 3:33 - loss: 3.4767 - accuracy: 0.11 - ETA: 3:32 - loss: 3.4751 - accuracy: 0.11 - ETA: 3:31 - loss: 3.4747 - accuracy: 0.11 - ETA: 3:31 - loss: 3.4719 - accuracy: 0.12 - ETA: 3:30 - loss: 3.4745 - accuracy: 0.11 - ETA: 3:29 - loss: 3.4705 - accuracy: 0.11 - ETA: 3:29 - loss: 3.4655 - accuracy: 0.12 - ETA: 3:28 - loss: 3.4626 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4633 - accuracy: 0.12 - ETA: 3:26 - loss: 3.4650 - accuracy: 0.12 - ETA: 3:26 - loss: 3.4635 - accuracy: 0.12 - ETA: 3:25 - loss: 3.4646 - accuracy: 0.12 - ETA: 3:24 - loss: 3.4638 - accuracy: 0.12 - ETA: 3:23 - loss: 3.4657 - accuracy: 0.11 - ETA: 3:22 - loss: 3.4668 - accuracy: 0.11 - ETA: 3:21 - loss: 3.4659 - accuracy: 0.11 - ETA: 3:21 - loss: 3.4651 - accuracy: 0.11 - ETA: 3:20 - loss: 3.4643 - accuracy: 0.11 - ETA: 3:19 - loss: 3.4632 - accuracy: 0.11 - ETA: 3:19 - loss: 3.4656 - accuracy: 0.11 - ETA: 3:18 - loss: 3.4644 - accuracy: 0.11 - ETA: 3:17 - loss: 3.4644 - accuracy: 0.11 - ETA: 3:16 - loss: 3.4659 - accuracy: 0.11 - ETA: 3:15 - loss: 3.4647 - accuracy: 0.11 - ETA: 3:15 - loss: 3.4634 - accuracy: 0.11 - ETA: 3:14 - loss: 3.4635 - accuracy: 0.11 - ETA: 3:14 - loss: 3.4634 - accuracy: 0.11 - ETA: 3:13 - loss: 3.4618 - accuracy: 0.11 - ETA: 3:12 - loss: 3.4635 - accuracy: 0.11 - ETA: 3:11 - loss: 3.4647 - accuracy: 0.11 - ETA: 3:11 - loss: 3.4664 - accuracy: 0.11 - ETA: 3:10 - loss: 3.4663 - accuracy: 0.11 - ETA: 3:09 - loss: 3.4674 - accuracy: 0.11 - ETA: 3:08 - loss: 3.4690 - accuracy: 0.11 - ETA: 3:07 - loss: 3.4694 - accuracy: 0.11 - ETA: 3:06 - loss: 3.4692 - accuracy: 0.11 - ETA: 3:05 - loss: 3.4683 - accuracy: 0.11 - ETA: 3:05 - loss: 3.4689 - accuracy: 0.11 - ETA: 3:04 - loss: 3.4688 - accuracy: 0.11 - ETA: 3:04 - loss: 3.4685 - accuracy: 0.11 - ETA: 3:03 - loss: 3.4699 - accuracy: 0.11 - ETA: 3:02 - loss: 3.4700 - accuracy: 0.11 - ETA: 3:01 - loss: 3.4711 - accuracy: 0.11 - ETA: 3:01 - loss: 3.4704 - accuracy: 0.11 - ETA: 3:00 - loss: 3.4706 - accuracy: 0.11 - ETA: 2:59 - loss: 3.4720 - accuracy: 0.11 - ETA: 2:58 - loss: 3.4730 - accuracy: 0.11 - ETA: 2:58 - loss: 3.4722 - accuracy: 0.11 - ETA: 2:57 - loss: 3.4712 - accuracy: 0.11 - ETA: 2:56 - loss: 3.4706 - accuracy: 0.11 - ETA: 2:56 - loss: 3.4704 - accuracy: 0.11 - ETA: 2:55 - loss: 3.4691 - accuracy: 0.11 - ETA: 2:54 - loss: 3.4689 - accuracy: 0.11 - ETA: 2:53 - loss: 3.4686 - accuracy: 0.11 - ETA: 2:52 - loss: 3.4693 - accuracy: 0.11 - ETA: 2:52 - loss: 3.4683 - accuracy: 0.11 - ETA: 2:51 - loss: 3.4694 - accuracy: 0.11 - ETA: 2:50 - loss: 3.4686 - accuracy: 0.11 - ETA: 2:49 - loss: 3.4699 - accuracy: 0.11 - ETA: 2:48 - loss: 3.4690 - accuracy: 0.11 - ETA: 2:48 - loss: 3.4704 - accuracy: 0.11 - ETA: 2:47 - loss: 3.4695 - accuracy: 0.11 - ETA: 2:46 - loss: 3.4695 - accuracy: 0.11 - ETA: 2:46 - loss: 3.4691 - accuracy: 0.11 - ETA: 2:45 - loss: 3.4680 - accuracy: 0.11 - ETA: 2:44 - loss: 3.4673 - accuracy: 0.11 - ETA: 2:43 - loss: 3.4682 - accuracy: 0.11 - ETA: 2:43 - loss: 3.4700 - accuracy: 0.11 - ETA: 2:42 - loss: 3.4693 - accuracy: 0.11 - ETA: 2:41 - loss: 3.4686 - accuracy: 0.11 - ETA: 2:41 - loss: 3.4687 - accuracy: 0.11 - ETA: 2:40 - loss: 3.4689 - accuracy: 0.11 - ETA: 2:39 - loss: 3.4696 - accuracy: 0.11 - ETA: 2:38 - loss: 3.4704 - accuracy: 0.11 - ETA: 2:38 - loss: 3.4712 - accuracy: 0.11 - ETA: 2:37 - loss: 3.4721 - accuracy: 0.11 - ETA: 2:36 - loss: 3.4725 - accuracy: 0.11 - ETA: 2:35 - loss: 3.4726 - accuracy: 0.11 - ETA: 2:34 - loss: 3.4707 - accuracy: 0.11 - ETA: 2:34 - loss: 3.4714 - accuracy: 0.11 - ETA: 2:33 - loss: 3.4708 - accuracy: 0.11 - ETA: 2:32 - loss: 3.4719 - accuracy: 0.11 - ETA: 2:32 - loss: 3.4717 - accuracy: 0.11 - ETA: 2:31 - loss: 3.4718 - accuracy: 0.11 - ETA: 2:30 - loss: 3.4716 - accuracy: 0.11 - ETA: 2:29 - loss: 3.4700 - accuracy: 0.11 - ETA: 2:29 - loss: 3.4697 - accuracy: 0.11 - ETA: 2:28 - loss: 3.4692 - accuracy: 0.11 - ETA: 2:27 - loss: 3.4690 - accuracy: 0.11 - ETA: 2:26 - loss: 3.4690 - accuracy: 0.11 - ETA: 2:26 - loss: 3.4685 - accuracy: 0.11 - ETA: 2:25 - loss: 3.4681 - accuracy: 0.11 - ETA: 2:24 - loss: 3.4683 - accuracy: 0.11 - ETA: 2:23 - loss: 3.4672 - accuracy: 0.11 - ETA: 2:22 - loss: 3.4641 - accuracy: 0.11 - ETA: 2:22 - loss: 3.4636 - accuracy: 0.11 - ETA: 2:21 - loss: 3.4631 - accuracy: 0.11 - ETA: 2:20 - loss: 3.4623 - accuracy: 0.11 - ETA: 2:19 - loss: 3.4627 - accuracy: 0.11 - ETA: 2:18 - loss: 3.4618 - accuracy: 0.11 - ETA: 2:18 - loss: 3.4611 - accuracy: 0.11 - ETA: 2:17 - loss: 3.4604 - accuracy: 0.11 - ETA: 2:16 - loss: 3.4594 - accuracy: 0.11 - ETA: 2:15 - loss: 3.4604 - accuracy: 0.11 - ETA: 2:15 - loss: 3.4606 - accuracy: 0.11 - ETA: 2:14 - loss: 3.4610 - accuracy: 0.11 - ETA: 2:13 - loss: 3.4598 - accuracy: 0.11 - ETA: 2:12 - loss: 3.4595 - accuracy: 0.11 - ETA: 2:12 - loss: 3.4592 - accuracy: 0.11 - ETA: 2:11 - loss: 3.4588 - accuracy: 0.11 - ETA: 2:10 - loss: 3.4589 - accuracy: 0.11 - ETA: 2:09 - loss: 3.4591 - accuracy: 0.11 - ETA: 2:08 - loss: 3.4591 - accuracy: 0.11 - ETA: 2:08 - loss: 3.4585 - accuracy: 0.11 - ETA: 2:07 - loss: 3.4585 - accuracy: 0.11 - ETA: 2:06 - loss: 3.4585 - accuracy: 0.11 - ETA: 2:05 - loss: 3.4587 - accuracy: 0.11 - ETA: 2:05 - loss: 3.4591 - accuracy: 0.11 - ETA: 2:04 - loss: 3.4589 - accuracy: 0.11 - ETA: 2:03 - loss: 3.4586 - accuracy: 0.12 - ETA: 2:02 - loss: 3.4586 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4598 - accuracy: 0.11 - ETA: 2:01 - loss: 3.4588 - accuracy: 0.11 - ETA: 2:00 - loss: 3.4581 - accuracy: 0.11 - ETA: 1:59 - loss: 3.4573 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4571 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4564 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4561 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4559 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4558 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4566 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4559 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4553 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4557 - accuracy: 0.12 - ETA: 1:51 - loss: 3.4555 - accuracy: 0.1206"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:51 - loss: 3.4554 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4561 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4553 - accuracy: 0.12 - ETA: 1:48 - loss: 3.4552 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4554 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4554 - accuracy: 0.12 - ETA: 1:46 - loss: 3.4549 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4546 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4545 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4537 - accuracy: 0.12 - ETA: 1:43 - loss: 3.4539 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4534 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4529 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4533 - accuracy: 0.12 - ETA: 1:40 - loss: 3.4536 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4540 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4540 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4550 - accuracy: 0.12 - ETA: 1:37 - loss: 3.4545 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4546 - accuracy: 0.12 - ETA: 1:35 - loss: 3.4544 - accuracy: 0.12 - ETA: 1:35 - loss: 3.4536 - accuracy: 0.12 - ETA: 1:34 - loss: 3.4541 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4541 - accuracy: 0.12 - ETA: 1:32 - loss: 3.4533 - accuracy: 0.12 - ETA: 1:31 - loss: 3.4531 - accuracy: 0.12 - ETA: 1:31 - loss: 3.4526 - accuracy: 0.12 - ETA: 1:30 - loss: 3.4523 - accuracy: 0.12 - ETA: 1:29 - loss: 3.4519 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4512 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4504 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4506 - accuracy: 0.12 - ETA: 1:26 - loss: 3.4509 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4511 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4512 - accuracy: 0.12 - ETA: 1:24 - loss: 3.4519 - accuracy: 0.12 - ETA: 1:23 - loss: 3.4521 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4518 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4520 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4521 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4522 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4518 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4513 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4510 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4513 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4516 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4515 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4511 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4513 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4512 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4508 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4512 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4520 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4520 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4519 - accuracy: 0.12 - ETA: 1:08 - loss: 3.4511 - accuracy: 0.12 - ETA: 1:08 - loss: 3.4510 - accuracy: 0.12 - ETA: 1:07 - loss: 3.4511 - accuracy: 0.12 - ETA: 1:06 - loss: 3.4509 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4504 - accuracy: 0.12 - ETA: 1:04 - loss: 3.4504 - accuracy: 0.12 - ETA: 1:04 - loss: 3.4498 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4495 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4497 - accuracy: 0.12 - ETA: 1:01 - loss: 3.4493 - accuracy: 0.12 - ETA: 1:01 - loss: 3.4495 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4490 - accuracy: 0.12 - ETA: 59s - loss: 3.4497 - accuracy: 0.1223 - ETA: 58s - loss: 3.4495 - accuracy: 0.122 - ETA: 57s - loss: 3.4490 - accuracy: 0.122 - ETA: 57s - loss: 3.4495 - accuracy: 0.122 - ETA: 56s - loss: 3.4497 - accuracy: 0.122 - ETA: 55s - loss: 3.4492 - accuracy: 0.122 - ETA: 54s - loss: 3.4499 - accuracy: 0.122 - ETA: 54s - loss: 3.4489 - accuracy: 0.122 - ETA: 53s - loss: 3.4485 - accuracy: 0.122 - ETA: 52s - loss: 3.4485 - accuracy: 0.122 - ETA: 51s - loss: 3.4485 - accuracy: 0.122 - ETA: 51s - loss: 3.4483 - accuracy: 0.122 - ETA: 50s - loss: 3.4487 - accuracy: 0.122 - ETA: 49s - loss: 3.4488 - accuracy: 0.122 - ETA: 48s - loss: 3.4483 - accuracy: 0.122 - ETA: 47s - loss: 3.4476 - accuracy: 0.122 - ETA: 47s - loss: 3.4477 - accuracy: 0.122 - ETA: 46s - loss: 3.4473 - accuracy: 0.122 - ETA: 45s - loss: 3.4465 - accuracy: 0.123 - ETA: 44s - loss: 3.4464 - accuracy: 0.123 - ETA: 44s - loss: 3.4467 - accuracy: 0.122 - ETA: 43s - loss: 3.4466 - accuracy: 0.123 - ETA: 42s - loss: 3.4465 - accuracy: 0.123 - ETA: 41s - loss: 3.4467 - accuracy: 0.123 - ETA: 40s - loss: 3.4464 - accuracy: 0.123 - ETA: 40s - loss: 3.4461 - accuracy: 0.123 - ETA: 39s - loss: 3.4467 - accuracy: 0.123 - ETA: 38s - loss: 3.4465 - accuracy: 0.123 - ETA: 37s - loss: 3.4472 - accuracy: 0.123 - ETA: 37s - loss: 3.4475 - accuracy: 0.123 - ETA: 36s - loss: 3.4475 - accuracy: 0.123 - ETA: 35s - loss: 3.4477 - accuracy: 0.123 - ETA: 34s - loss: 3.4472 - accuracy: 0.123 - ETA: 34s - loss: 3.4471 - accuracy: 0.123 - ETA: 33s - loss: 3.4470 - accuracy: 0.123 - ETA: 32s - loss: 3.4475 - accuracy: 0.123 - ETA: 31s - loss: 3.4479 - accuracy: 0.123 - ETA: 30s - loss: 3.4477 - accuracy: 0.123 - ETA: 30s - loss: 3.4477 - accuracy: 0.123 - ETA: 29s - loss: 3.4472 - accuracy: 0.123 - ETA: 28s - loss: 3.4473 - accuracy: 0.123 - ETA: 27s - loss: 3.4480 - accuracy: 0.123 - ETA: 27s - loss: 3.4479 - accuracy: 0.123 - ETA: 26s - loss: 3.4477 - accuracy: 0.123 - ETA: 25s - loss: 3.4482 - accuracy: 0.123 - ETA: 24s - loss: 3.4483 - accuracy: 0.123 - ETA: 23s - loss: 3.4483 - accuracy: 0.123 - ETA: 23s - loss: 3.4487 - accuracy: 0.123 - ETA: 22s - loss: 3.4483 - accuracy: 0.123 - ETA: 21s - loss: 3.4483 - accuracy: 0.123 - ETA: 20s - loss: 3.4481 - accuracy: 0.123 - ETA: 20s - loss: 3.4484 - accuracy: 0.123 - ETA: 19s - loss: 3.4487 - accuracy: 0.123 - ETA: 18s - loss: 3.4488 - accuracy: 0.123 - ETA: 17s - loss: 3.4482 - accuracy: 0.123 - ETA: 17s - loss: 3.4484 - accuracy: 0.123 - ETA: 16s - loss: 3.4478 - accuracy: 0.123 - ETA: 15s - loss: 3.4480 - accuracy: 0.123 - ETA: 14s - loss: 3.4482 - accuracy: 0.123 - ETA: 13s - loss: 3.4480 - accuracy: 0.123 - ETA: 13s - loss: 3.4483 - accuracy: 0.123 - ETA: 12s - loss: 3.4484 - accuracy: 0.123 - ETA: 11s - loss: 3.4480 - accuracy: 0.123 - ETA: 10s - loss: 3.4478 - accuracy: 0.123 - ETA: 10s - loss: 3.4475 - accuracy: 0.123 - ETA: 9s - loss: 3.4471 - accuracy: 0.123 - ETA: 8s - loss: 3.4469 - accuracy: 0.12 - ETA: 7s - loss: 3.4469 - accuracy: 0.12 - ETA: 7s - loss: 3.4471 - accuracy: 0.12 - ETA: 6s - loss: 3.4469 - accuracy: 0.12 - ETA: 5s - loss: 3.4465 - accuracy: 0.12 - ETA: 4s - loss: 3.4467 - accuracy: 0.12 - ETA: 3s - loss: 3.4467 - accuracy: 0.12 - ETA: 3s - loss: 3.4470 - accuracy: 0.12 - ETA: 2s - loss: 3.4467 - accuracy: 0.12 - ETA: 1s - loss: 3.4466 - accuracy: 0.12 - ETA: 0s - loss: 3.4466 - accuracy: 0.12 - ETA: 0s - loss: 3.4464 - accuracy: 0.12 - 275s 6ms/step - loss: 3.4464 - accuracy: 0.1239 - val_loss: 3.8621 - val_accuracy: 0.0270\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:10 - loss: 3.3858 - accuracy: 0.14 - ETA: 4:15 - loss: 3.4267 - accuracy: 0.14 - ETA: 4:13 - loss: 3.4326 - accuracy: 0.13 - ETA: 4:13 - loss: 3.4490 - accuracy: 0.12 - ETA: 4:13 - loss: 3.4466 - accuracy: 0.12 - ETA: 4:15 - loss: 3.4499 - accuracy: 0.12 - ETA: 4:15 - loss: 3.4712 - accuracy: 0.11 - ETA: 4:15 - loss: 3.4438 - accuracy: 0.12 - ETA: 4:15 - loss: 3.4315 - accuracy: 0.12 - ETA: 4:13 - loss: 3.4187 - accuracy: 0.12 - ETA: 4:11 - loss: 3.4195 - accuracy: 0.13 - ETA: 4:11 - loss: 3.4286 - accuracy: 0.12 - ETA: 4:11 - loss: 3.4282 - accuracy: 0.12 - ETA: 4:10 - loss: 3.4326 - accuracy: 0.12 - ETA: 4:08 - loss: 3.4319 - accuracy: 0.12 - ETA: 4:06 - loss: 3.4252 - accuracy: 0.12 - ETA: 4:04 - loss: 3.4199 - accuracy: 0.12 - ETA: 4:03 - loss: 3.4201 - accuracy: 0.12 - ETA: 4:01 - loss: 3.4102 - accuracy: 0.13 - ETA: 4:01 - loss: 3.4040 - accuracy: 0.13 - ETA: 4:00 - loss: 3.4049 - accuracy: 0.13 - ETA: 3:59 - loss: 3.4045 - accuracy: 0.13 - ETA: 3:59 - loss: 3.4062 - accuracy: 0.13 - ETA: 3:58 - loss: 3.4127 - accuracy: 0.12 - ETA: 3:58 - loss: 3.4079 - accuracy: 0.12 - ETA: 3:57 - loss: 3.4068 - accuracy: 0.12 - ETA: 3:57 - loss: 3.4068 - accuracy: 0.12 - ETA: 3:56 - loss: 3.4105 - accuracy: 0.12 - ETA: 3:56 - loss: 3.4112 - accuracy: 0.12 - ETA: 3:55 - loss: 3.4140 - accuracy: 0.12 - ETA: 3:55 - loss: 3.4198 - accuracy: 0.12 - ETA: 3:54 - loss: 3.4163 - accuracy: 0.12 - ETA: 3:52 - loss: 3.4192 - accuracy: 0.12 - ETA: 3:52 - loss: 3.4238 - accuracy: 0.12 - ETA: 3:51 - loss: 3.4216 - accuracy: 0.12 - ETA: 3:50 - loss: 3.4240 - accuracy: 0.12 - ETA: 3:49 - loss: 3.4240 - accuracy: 0.12 - ETA: 3:47 - loss: 3.4221 - accuracy: 0.12 - ETA: 3:46 - loss: 3.4203 - accuracy: 0.12 - ETA: 3:45 - loss: 3.4211 - accuracy: 0.12 - ETA: 3:45 - loss: 3.4230 - accuracy: 0.12 - ETA: 3:44 - loss: 3.4223 - accuracy: 0.12 - ETA: 3:43 - loss: 3.4210 - accuracy: 0.12 - ETA: 3:42 - loss: 3.4216 - accuracy: 0.12 - ETA: 3:42 - loss: 3.4211 - accuracy: 0.12 - ETA: 3:41 - loss: 3.4183 - accuracy: 0.13 - ETA: 3:41 - loss: 3.4195 - accuracy: 0.13 - ETA: 3:40 - loss: 3.4227 - accuracy: 0.13 - ETA: 3:39 - loss: 3.4221 - accuracy: 0.12 - ETA: 3:38 - loss: 3.4226 - accuracy: 0.12 - ETA: 3:37 - loss: 3.4244 - accuracy: 0.12 - ETA: 3:36 - loss: 3.4240 - accuracy: 0.12 - ETA: 3:36 - loss: 3.4238 - accuracy: 0.12 - ETA: 3:35 - loss: 3.4238 - accuracy: 0.12 - ETA: 3:34 - loss: 3.4239 - accuracy: 0.12 - ETA: 3:34 - loss: 3.4222 - accuracy: 0.12 - ETA: 3:32 - loss: 3.4200 - accuracy: 0.12 - ETA: 3:32 - loss: 3.4222 - accuracy: 0.12 - ETA: 3:30 - loss: 3.4215 - accuracy: 0.12 - ETA: 3:30 - loss: 3.4216 - accuracy: 0.12 - ETA: 3:29 - loss: 3.4215 - accuracy: 0.12 - ETA: 3:28 - loss: 3.4231 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4267 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4252 - accuracy: 0.12 - ETA: 3:26 - loss: 3.4242 - accuracy: 0.12 - ETA: 3:25 - loss: 3.4247 - accuracy: 0.12 - ETA: 3:24 - loss: 3.4244 - accuracy: 0.12 - ETA: 3:24 - loss: 3.4214 - accuracy: 0.12 - ETA: 3:23 - loss: 3.4225 - accuracy: 0.12 - ETA: 3:22 - loss: 3.4213 - accuracy: 0.12 - ETA: 3:22 - loss: 3.4223 - accuracy: 0.12 - ETA: 3:21 - loss: 3.4201 - accuracy: 0.12 - ETA: 3:20 - loss: 3.4200 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4229 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4229 - accuracy: 0.12 - ETA: 3:18 - loss: 3.4229 - accuracy: 0.13 - ETA: 3:17 - loss: 3.4211 - accuracy: 0.13 - ETA: 3:16 - loss: 3.4214 - accuracy: 0.13 - ETA: 3:15 - loss: 3.4206 - accuracy: 0.13 - ETA: 3:14 - loss: 3.4197 - accuracy: 0.13 - ETA: 3:14 - loss: 3.4202 - accuracy: 0.13 - ETA: 3:13 - loss: 3.4215 - accuracy: 0.13 - ETA: 3:12 - loss: 3.4213 - accuracy: 0.13 - ETA: 3:12 - loss: 3.4222 - accuracy: 0.13 - ETA: 3:11 - loss: 3.4216 - accuracy: 0.13 - ETA: 3:10 - loss: 3.4213 - accuracy: 0.13 - ETA: 3:09 - loss: 3.4196 - accuracy: 0.13 - ETA: 3:08 - loss: 3.4201 - accuracy: 0.13 - ETA: 3:08 - loss: 3.4203 - accuracy: 0.13 - ETA: 3:07 - loss: 3.4203 - accuracy: 0.13 - ETA: 3:06 - loss: 3.4208 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4189 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4183 - accuracy: 0.13 - ETA: 3:04 - loss: 3.4188 - accuracy: 0.13 - ETA: 3:03 - loss: 3.4181 - accuracy: 0.13 - ETA: 3:02 - loss: 3.4178 - accuracy: 0.13 - ETA: 3:02 - loss: 3.4162 - accuracy: 0.13 - ETA: 3:01 - loss: 3.4149 - accuracy: 0.13 - ETA: 3:00 - loss: 3.4148 - accuracy: 0.13 - ETA: 2:59 - loss: 3.4147 - accuracy: 0.13 - ETA: 2:58 - loss: 3.4136 - accuracy: 0.13 - ETA: 2:57 - loss: 3.4132 - accuracy: 0.13 - ETA: 2:57 - loss: 3.4149 - accuracy: 0.13 - ETA: 2:56 - loss: 3.4149 - accuracy: 0.13 - ETA: 2:55 - loss: 3.4147 - accuracy: 0.13 - ETA: 2:54 - loss: 3.4156 - accuracy: 0.13 - ETA: 2:54 - loss: 3.4163 - accuracy: 0.13 - ETA: 2:53 - loss: 3.4173 - accuracy: 0.13 - ETA: 2:52 - loss: 3.4151 - accuracy: 0.13 - ETA: 2:51 - loss: 3.4161 - accuracy: 0.13 - ETA: 2:51 - loss: 3.4161 - accuracy: 0.13 - ETA: 2:50 - loss: 3.4158 - accuracy: 0.13 - ETA: 2:49 - loss: 3.4162 - accuracy: 0.13 - ETA: 2:48 - loss: 3.4156 - accuracy: 0.13 - ETA: 2:47 - loss: 3.4161 - accuracy: 0.13 - ETA: 2:47 - loss: 3.4158 - accuracy: 0.13 - ETA: 2:46 - loss: 3.4173 - accuracy: 0.13 - ETA: 2:45 - loss: 3.4179 - accuracy: 0.13 - ETA: 2:44 - loss: 3.4178 - accuracy: 0.13 - ETA: 2:43 - loss: 3.4184 - accuracy: 0.13 - ETA: 2:43 - loss: 3.4190 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4189 - accuracy: 0.13 - ETA: 2:41 - loss: 3.4206 - accuracy: 0.13 - ETA: 2:40 - loss: 3.4208 - accuracy: 0.13 - ETA: 2:39 - loss: 3.4201 - accuracy: 0.13 - ETA: 2:39 - loss: 3.4203 - accuracy: 0.13 - ETA: 2:38 - loss: 3.4205 - accuracy: 0.13 - ETA: 2:37 - loss: 3.4207 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4198 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4198 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4195 - accuracy: 0.12 - ETA: 2:34 - loss: 3.4179 - accuracy: 0.13 - ETA: 2:33 - loss: 3.4187 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4188 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4181 - accuracy: 0.12 - ETA: 2:31 - loss: 3.4184 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4198 - accuracy: 0.12 - ETA: 2:29 - loss: 3.4201 - accuracy: 0.12 - ETA: 2:29 - loss: 3.4200 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4207 - accuracy: 0.12 - ETA: 2:27 - loss: 3.4204 - accuracy: 0.12 - ETA: 2:26 - loss: 3.4214 - accuracy: 0.12 - ETA: 2:25 - loss: 3.4223 - accuracy: 0.12 - ETA: 2:24 - loss: 3.4230 - accuracy: 0.12 - ETA: 2:24 - loss: 3.4230 - accuracy: 0.12 - ETA: 2:23 - loss: 3.4229 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4232 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4238 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4233 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4242 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4237 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4242 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4241 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4236 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4235 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4231 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4234 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4239 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4244 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4240 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4234 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4232 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4228 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4218 - accuracy: 0.12 - ETA: 2:08 - loss: 3.4231 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4223 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4222 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4208 - accuracy: 0.12 - ETA: 2:05 - loss: 3.4208 - accuracy: 0.12 - ETA: 2:04 - loss: 3.4208 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4206 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4209 - accuracy: 0.12 - ETA: 2:02 - loss: 3.4216 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4219 - accuracy: 0.12 - ETA: 2:00 - loss: 3.4213 - accuracy: 0.12 - ETA: 2:00 - loss: 3.4214 - accuracy: 0.12 - ETA: 1:59 - loss: 3.4220 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4228 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4222 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4223 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4238 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4238 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4234 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4226 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4228 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4229 - accuracy: 0.1283"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:51 - loss: 3.4228 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4219 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4220 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4225 - accuracy: 0.12 - ETA: 1:48 - loss: 3.4226 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4227 - accuracy: 0.12 - ETA: 1:46 - loss: 3.4221 - accuracy: 0.12 - ETA: 1:46 - loss: 3.4214 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4217 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4217 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4213 - accuracy: 0.12 - ETA: 1:43 - loss: 3.4211 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4216 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4229 - accuracy: 0.12 - ETA: 1:40 - loss: 3.4231 - accuracy: 0.12 - ETA: 1:40 - loss: 3.4229 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4223 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4218 - accuracy: 0.12 - ETA: 1:37 - loss: 3.4215 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4221 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4230 - accuracy: 0.12 - ETA: 1:35 - loss: 3.4237 - accuracy: 0.12 - ETA: 1:34 - loss: 3.4229 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4231 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4226 - accuracy: 0.12 - ETA: 1:32 - loss: 3.4222 - accuracy: 0.12 - ETA: 1:31 - loss: 3.4207 - accuracy: 0.12 - ETA: 1:30 - loss: 3.4208 - accuracy: 0.12 - ETA: 1:30 - loss: 3.4207 - accuracy: 0.12 - ETA: 1:29 - loss: 3.4202 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4207 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4216 - accuracy: 0.12 - ETA: 1:26 - loss: 3.4222 - accuracy: 0.12 - ETA: 1:26 - loss: 3.4217 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4221 - accuracy: 0.12 - ETA: 1:24 - loss: 3.4224 - accuracy: 0.12 - ETA: 1:23 - loss: 3.4220 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4210 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4209 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4211 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4204 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4209 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4211 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4211 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4210 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4210 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4210 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4214 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4210 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4210 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4211 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4208 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4211 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4214 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4216 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4218 - accuracy: 0.12 - ETA: 1:08 - loss: 3.4223 - accuracy: 0.12 - ETA: 1:07 - loss: 3.4226 - accuracy: 0.12 - ETA: 1:06 - loss: 3.4232 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4230 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4232 - accuracy: 0.12 - ETA: 1:04 - loss: 3.4230 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4234 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4237 - accuracy: 0.12 - ETA: 1:01 - loss: 3.4243 - accuracy: 0.12 - ETA: 1:01 - loss: 3.4243 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4243 - accuracy: 0.12 - ETA: 59s - loss: 3.4244 - accuracy: 0.1261 - ETA: 58s - loss: 3.4245 - accuracy: 0.125 - ETA: 58s - loss: 3.4241 - accuracy: 0.126 - ETA: 57s - loss: 3.4244 - accuracy: 0.126 - ETA: 56s - loss: 3.4245 - accuracy: 0.125 - ETA: 55s - loss: 3.4250 - accuracy: 0.125 - ETA: 54s - loss: 3.4249 - accuracy: 0.125 - ETA: 54s - loss: 3.4250 - accuracy: 0.125 - ETA: 53s - loss: 3.4250 - accuracy: 0.125 - ETA: 52s - loss: 3.4253 - accuracy: 0.125 - ETA: 51s - loss: 3.4248 - accuracy: 0.125 - ETA: 51s - loss: 3.4251 - accuracy: 0.125 - ETA: 50s - loss: 3.4246 - accuracy: 0.126 - ETA: 49s - loss: 3.4251 - accuracy: 0.125 - ETA: 48s - loss: 3.4253 - accuracy: 0.126 - ETA: 48s - loss: 3.4254 - accuracy: 0.126 - ETA: 47s - loss: 3.4256 - accuracy: 0.125 - ETA: 46s - loss: 3.4256 - accuracy: 0.125 - ETA: 45s - loss: 3.4252 - accuracy: 0.125 - ETA: 44s - loss: 3.4255 - accuracy: 0.125 - ETA: 44s - loss: 3.4259 - accuracy: 0.125 - ETA: 43s - loss: 3.4266 - accuracy: 0.125 - ETA: 42s - loss: 3.4263 - accuracy: 0.126 - ETA: 41s - loss: 3.4260 - accuracy: 0.126 - ETA: 41s - loss: 3.4253 - accuracy: 0.126 - ETA: 40s - loss: 3.4250 - accuracy: 0.126 - ETA: 39s - loss: 3.4255 - accuracy: 0.126 - ETA: 38s - loss: 3.4255 - accuracy: 0.126 - ETA: 38s - loss: 3.4252 - accuracy: 0.126 - ETA: 37s - loss: 3.4252 - accuracy: 0.126 - ETA: 36s - loss: 3.4250 - accuracy: 0.126 - ETA: 35s - loss: 3.4241 - accuracy: 0.126 - ETA: 34s - loss: 3.4246 - accuracy: 0.126 - ETA: 34s - loss: 3.4240 - accuracy: 0.126 - ETA: 33s - loss: 3.4241 - accuracy: 0.126 - ETA: 32s - loss: 3.4242 - accuracy: 0.126 - ETA: 31s - loss: 3.4244 - accuracy: 0.126 - ETA: 31s - loss: 3.4241 - accuracy: 0.126 - ETA: 30s - loss: 3.4237 - accuracy: 0.126 - ETA: 29s - loss: 3.4238 - accuracy: 0.126 - ETA: 28s - loss: 3.4242 - accuracy: 0.126 - ETA: 27s - loss: 3.4250 - accuracy: 0.126 - ETA: 27s - loss: 3.4253 - accuracy: 0.126 - ETA: 26s - loss: 3.4249 - accuracy: 0.126 - ETA: 25s - loss: 3.4252 - accuracy: 0.126 - ETA: 24s - loss: 3.4255 - accuracy: 0.126 - ETA: 24s - loss: 3.4261 - accuracy: 0.126 - ETA: 23s - loss: 3.4258 - accuracy: 0.126 - ETA: 22s - loss: 3.4258 - accuracy: 0.126 - ETA: 21s - loss: 3.4259 - accuracy: 0.126 - ETA: 21s - loss: 3.4260 - accuracy: 0.126 - ETA: 20s - loss: 3.4259 - accuracy: 0.126 - ETA: 19s - loss: 3.4259 - accuracy: 0.126 - ETA: 18s - loss: 3.4261 - accuracy: 0.126 - ETA: 17s - loss: 3.4261 - accuracy: 0.126 - ETA: 17s - loss: 3.4264 - accuracy: 0.126 - ETA: 16s - loss: 3.4262 - accuracy: 0.126 - ETA: 15s - loss: 3.4268 - accuracy: 0.126 - ETA: 14s - loss: 3.4268 - accuracy: 0.126 - ETA: 14s - loss: 3.4268 - accuracy: 0.126 - ETA: 13s - loss: 3.4267 - accuracy: 0.126 - ETA: 12s - loss: 3.4264 - accuracy: 0.126 - ETA: 11s - loss: 3.4261 - accuracy: 0.126 - ETA: 10s - loss: 3.4262 - accuracy: 0.126 - ETA: 10s - loss: 3.4266 - accuracy: 0.126 - ETA: 9s - loss: 3.4263 - accuracy: 0.126 - ETA: 8s - loss: 3.4260 - accuracy: 0.12 - ETA: 7s - loss: 3.4260 - accuracy: 0.12 - ETA: 7s - loss: 3.4261 - accuracy: 0.12 - ETA: 6s - loss: 3.4260 - accuracy: 0.12 - ETA: 5s - loss: 3.4264 - accuracy: 0.12 - ETA: 4s - loss: 3.4262 - accuracy: 0.12 - ETA: 3s - loss: 3.4266 - accuracy: 0.12 - ETA: 3s - loss: 3.4260 - accuracy: 0.12 - ETA: 2s - loss: 3.4262 - accuracy: 0.12 - ETA: 1s - loss: 3.4265 - accuracy: 0.12 - ETA: 0s - loss: 3.4264 - accuracy: 0.12 - ETA: 0s - loss: 3.4264 - accuracy: 0.12 - 276s 7ms/step - loss: 3.4264 - accuracy: 0.1263 - val_loss: 3.9126 - val_accuracy: 0.0267\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:11 - loss: 3.2483 - accuracy: 0.16 - ETA: 4:10 - loss: 3.2851 - accuracy: 0.15 - ETA: 4:11 - loss: 3.3419 - accuracy: 0.14 - ETA: 4:11 - loss: 3.3621 - accuracy: 0.14 - ETA: 4:13 - loss: 3.3541 - accuracy: 0.14 - ETA: 4:13 - loss: 3.3637 - accuracy: 0.14 - ETA: 4:15 - loss: 3.3613 - accuracy: 0.14 - ETA: 4:14 - loss: 3.3562 - accuracy: 0.14 - ETA: 4:15 - loss: 3.3634 - accuracy: 0.14 - ETA: 4:12 - loss: 3.3663 - accuracy: 0.14 - ETA: 4:09 - loss: 3.3599 - accuracy: 0.14 - ETA: 4:07 - loss: 3.3569 - accuracy: 0.14 - ETA: 4:06 - loss: 3.3608 - accuracy: 0.14 - ETA: 4:05 - loss: 3.3806 - accuracy: 0.14 - ETA: 4:04 - loss: 3.3729 - accuracy: 0.14 - ETA: 4:05 - loss: 3.3735 - accuracy: 0.14 - ETA: 4:03 - loss: 3.3733 - accuracy: 0.14 - ETA: 4:02 - loss: 3.3732 - accuracy: 0.14 - ETA: 4:01 - loss: 3.3744 - accuracy: 0.14 - ETA: 4:01 - loss: 3.3773 - accuracy: 0.13 - ETA: 4:00 - loss: 3.3732 - accuracy: 0.13 - ETA: 3:59 - loss: 3.3815 - accuracy: 0.13 - ETA: 3:59 - loss: 3.3916 - accuracy: 0.13 - ETA: 3:57 - loss: 3.3900 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3953 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3877 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3899 - accuracy: 0.13 - ETA: 3:53 - loss: 3.3922 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3914 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3930 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3956 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3978 - accuracy: 0.13 - ETA: 3:47 - loss: 3.4022 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3985 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3979 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3990 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3968 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3983 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3994 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3998 - accuracy: 0.13 - ETA: 3:43 - loss: 3.4037 - accuracy: 0.13 - ETA: 3:42 - loss: 3.4046 - accuracy: 0.13 - ETA: 3:41 - loss: 3.4049 - accuracy: 0.13 - ETA: 3:41 - loss: 3.4067 - accuracy: 0.13 - ETA: 3:40 - loss: 3.4061 - accuracy: 0.12 - ETA: 3:39 - loss: 3.4050 - accuracy: 0.12 - ETA: 3:39 - loss: 3.4051 - accuracy: 0.12 - ETA: 3:38 - loss: 3.4024 - accuracy: 0.12 - ETA: 3:37 - loss: 3.3969 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3965 - accuracy: 0.13 - ETA: 3:36 - loss: 3.4001 - accuracy: 0.12 - ETA: 3:35 - loss: 3.3986 - accuracy: 0.13 - ETA: 3:34 - loss: 3.4019 - accuracy: 0.12 - ETA: 3:33 - loss: 3.4024 - accuracy: 0.12 - ETA: 3:32 - loss: 3.4073 - accuracy: 0.12 - ETA: 3:31 - loss: 3.4102 - accuracy: 0.12 - ETA: 3:31 - loss: 3.4126 - accuracy: 0.12 - ETA: 3:30 - loss: 3.4103 - accuracy: 0.12 - ETA: 3:29 - loss: 3.4107 - accuracy: 0.12 - ETA: 3:29 - loss: 3.4090 - accuracy: 0.12 - ETA: 3:28 - loss: 3.4071 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4083 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4087 - accuracy: 0.12 - ETA: 3:26 - loss: 3.4066 - accuracy: 0.13 - ETA: 3:25 - loss: 3.4052 - accuracy: 0.13 - ETA: 3:25 - loss: 3.4045 - accuracy: 0.13 - ETA: 3:24 - loss: 3.4036 - accuracy: 0.13 - ETA: 3:24 - loss: 3.4031 - accuracy: 0.13 - ETA: 3:23 - loss: 3.4049 - accuracy: 0.13 - ETA: 3:22 - loss: 3.4058 - accuracy: 0.13 - ETA: 3:21 - loss: 3.4053 - accuracy: 0.13 - ETA: 3:20 - loss: 3.4063 - accuracy: 0.13 - ETA: 3:20 - loss: 3.4063 - accuracy: 0.13 - ETA: 3:19 - loss: 3.4079 - accuracy: 0.13 - ETA: 3:18 - loss: 3.4100 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4096 - accuracy: 0.12 - ETA: 3:16 - loss: 3.4089 - accuracy: 0.13 - ETA: 3:15 - loss: 3.4098 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4100 - accuracy: 0.13 - ETA: 3:14 - loss: 3.4117 - accuracy: 0.12 - ETA: 3:13 - loss: 3.4111 - accuracy: 0.13 - ETA: 3:12 - loss: 3.4110 - accuracy: 0.12 - ETA: 3:12 - loss: 3.4105 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4113 - accuracy: 0.12 - ETA: 3:10 - loss: 3.4131 - accuracy: 0.12 - ETA: 3:10 - loss: 3.4131 - accuracy: 0.12 - ETA: 3:09 - loss: 3.4111 - accuracy: 0.12 - ETA: 3:08 - loss: 3.4098 - accuracy: 0.12 - ETA: 3:07 - loss: 3.4088 - accuracy: 0.12 - ETA: 3:07 - loss: 3.4070 - accuracy: 0.12 - ETA: 3:06 - loss: 3.4064 - accuracy: 0.13 - ETA: 3:05 - loss: 3.4064 - accuracy: 0.13 - ETA: 3:05 - loss: 3.4057 - accuracy: 0.13 - ETA: 3:04 - loss: 3.4054 - accuracy: 0.13 - ETA: 3:03 - loss: 3.4050 - accuracy: 0.13 - ETA: 3:02 - loss: 3.4071 - accuracy: 0.13 - ETA: 3:01 - loss: 3.4084 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4069 - accuracy: 0.13 - ETA: 3:00 - loss: 3.4073 - accuracy: 0.13 - ETA: 2:59 - loss: 3.4074 - accuracy: 0.13 - ETA: 2:58 - loss: 3.4073 - accuracy: 0.13 - ETA: 2:57 - loss: 3.4073 - accuracy: 0.13 - ETA: 2:57 - loss: 3.4058 - accuracy: 0.13 - ETA: 2:56 - loss: 3.4064 - accuracy: 0.13 - ETA: 2:55 - loss: 3.4080 - accuracy: 0.12 - ETA: 2:54 - loss: 3.4077 - accuracy: 0.12 - ETA: 2:54 - loss: 3.4071 - accuracy: 0.13 - ETA: 2:53 - loss: 3.4059 - accuracy: 0.13 - ETA: 2:52 - loss: 3.4058 - accuracy: 0.13 - ETA: 2:51 - loss: 3.4057 - accuracy: 0.13 - ETA: 2:51 - loss: 3.4038 - accuracy: 0.13 - ETA: 2:50 - loss: 3.4052 - accuracy: 0.13 - ETA: 2:49 - loss: 3.4046 - accuracy: 0.13 - ETA: 2:48 - loss: 3.4053 - accuracy: 0.13 - ETA: 2:47 - loss: 3.4061 - accuracy: 0.13 - ETA: 2:46 - loss: 3.4062 - accuracy: 0.13 - ETA: 2:46 - loss: 3.4053 - accuracy: 0.13 - ETA: 2:45 - loss: 3.4072 - accuracy: 0.13 - ETA: 2:44 - loss: 3.4073 - accuracy: 0.13 - ETA: 2:43 - loss: 3.4066 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4075 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4090 - accuracy: 0.12 - ETA: 2:41 - loss: 3.4101 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4095 - accuracy: 0.12 - ETA: 2:39 - loss: 3.4077 - accuracy: 0.12 - ETA: 2:38 - loss: 3.4078 - accuracy: 0.12 - ETA: 2:38 - loss: 3.4080 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4075 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4082 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4078 - accuracy: 0.12 - ETA: 2:34 - loss: 3.4062 - accuracy: 0.12 - ETA: 2:34 - loss: 3.4050 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4051 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4044 - accuracy: 0.12 - ETA: 2:31 - loss: 3.4049 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4055 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4041 - accuracy: 0.13 - ETA: 2:29 - loss: 3.4034 - accuracy: 0.13 - ETA: 2:28 - loss: 3.4027 - accuracy: 0.13 - ETA: 2:27 - loss: 3.4031 - accuracy: 0.13 - ETA: 2:26 - loss: 3.4028 - accuracy: 0.13 - ETA: 2:26 - loss: 3.4022 - accuracy: 0.13 - ETA: 2:25 - loss: 3.4013 - accuracy: 0.13 - ETA: 2:24 - loss: 3.4015 - accuracy: 0.13 - ETA: 2:23 - loss: 3.4013 - accuracy: 0.13 - ETA: 2:22 - loss: 3.4015 - accuracy: 0.13 - ETA: 2:22 - loss: 3.4013 - accuracy: 0.13 - ETA: 2:21 - loss: 3.4012 - accuracy: 0.13 - ETA: 2:20 - loss: 3.4017 - accuracy: 0.13 - ETA: 2:19 - loss: 3.4014 - accuracy: 0.13 - ETA: 2:19 - loss: 3.4007 - accuracy: 0.13 - ETA: 2:18 - loss: 3.4020 - accuracy: 0.13 - ETA: 2:17 - loss: 3.4029 - accuracy: 0.13 - ETA: 2:16 - loss: 3.4032 - accuracy: 0.13 - ETA: 2:15 - loss: 3.4030 - accuracy: 0.13 - ETA: 2:15 - loss: 3.4041 - accuracy: 0.13 - ETA: 2:14 - loss: 3.4032 - accuracy: 0.13 - ETA: 2:13 - loss: 3.4036 - accuracy: 0.13 - ETA: 2:12 - loss: 3.4044 - accuracy: 0.13 - ETA: 2:11 - loss: 3.4037 - accuracy: 0.13 - ETA: 2:11 - loss: 3.4018 - accuracy: 0.13 - ETA: 2:10 - loss: 3.4016 - accuracy: 0.13 - ETA: 2:09 - loss: 3.4012 - accuracy: 0.13 - ETA: 2:08 - loss: 3.4013 - accuracy: 0.13 - ETA: 2:07 - loss: 3.4029 - accuracy: 0.13 - ETA: 2:07 - loss: 3.4037 - accuracy: 0.13 - ETA: 2:06 - loss: 3.4057 - accuracy: 0.13 - ETA: 2:05 - loss: 3.4061 - accuracy: 0.13 - ETA: 2:04 - loss: 3.4066 - accuracy: 0.13 - ETA: 2:03 - loss: 3.4064 - accuracy: 0.13 - ETA: 2:03 - loss: 3.4053 - accuracy: 0.13 - ETA: 2:02 - loss: 3.4053 - accuracy: 0.13 - ETA: 2:01 - loss: 3.4050 - accuracy: 0.13 - ETA: 2:00 - loss: 3.4061 - accuracy: 0.13 - ETA: 1:59 - loss: 3.4063 - accuracy: 0.13 - ETA: 1:59 - loss: 3.4059 - accuracy: 0.13 - ETA: 1:58 - loss: 3.4063 - accuracy: 0.13 - ETA: 1:57 - loss: 3.4071 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4070 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4065 - accuracy: 0.13 - ETA: 1:55 - loss: 3.4063 - accuracy: 0.13 - ETA: 1:54 - loss: 3.4073 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4069 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4077 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4079 - accuracy: 0.12 - ETA: 1:51 - loss: 3.4079 - accuracy: 0.1294"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:50 - loss: 3.4078 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4085 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4081 - accuracy: 0.12 - ETA: 1:48 - loss: 3.4083 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4086 - accuracy: 0.12 - ETA: 1:46 - loss: 3.4092 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4091 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4094 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4096 - accuracy: 0.12 - ETA: 1:43 - loss: 3.4098 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4097 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4094 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4100 - accuracy: 0.12 - ETA: 1:40 - loss: 3.4098 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4098 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4096 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4095 - accuracy: 0.12 - ETA: 1:37 - loss: 3.4098 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4100 - accuracy: 0.12 - ETA: 1:35 - loss: 3.4099 - accuracy: 0.12 - ETA: 1:34 - loss: 3.4103 - accuracy: 0.12 - ETA: 1:34 - loss: 3.4097 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4104 - accuracy: 0.12 - ETA: 1:32 - loss: 3.4107 - accuracy: 0.12 - ETA: 1:31 - loss: 3.4112 - accuracy: 0.12 - ETA: 1:30 - loss: 3.4107 - accuracy: 0.12 - ETA: 1:30 - loss: 3.4114 - accuracy: 0.12 - ETA: 1:29 - loss: 3.4115 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4118 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4128 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4131 - accuracy: 0.12 - ETA: 1:26 - loss: 3.4128 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4129 - accuracy: 0.12 - ETA: 1:24 - loss: 3.4130 - accuracy: 0.12 - ETA: 1:23 - loss: 3.4128 - accuracy: 0.12 - ETA: 1:23 - loss: 3.4126 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4130 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4130 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4130 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4136 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4144 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4143 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4150 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4147 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4151 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4155 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4162 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4162 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4163 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4160 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4162 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4161 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4161 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4159 - accuracy: 0.12 - ETA: 1:08 - loss: 3.4155 - accuracy: 0.12 - ETA: 1:07 - loss: 3.4149 - accuracy: 0.12 - ETA: 1:06 - loss: 3.4149 - accuracy: 0.12 - ETA: 1:06 - loss: 3.4148 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4154 - accuracy: 0.12 - ETA: 1:04 - loss: 3.4145 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4148 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4153 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4156 - accuracy: 0.12 - ETA: 1:01 - loss: 3.4158 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4153 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4160 - accuracy: 0.12 - ETA: 59s - loss: 3.4157 - accuracy: 0.1256 - ETA: 58s - loss: 3.4157 - accuracy: 0.125 - ETA: 57s - loss: 3.4158 - accuracy: 0.125 - ETA: 56s - loss: 3.4162 - accuracy: 0.125 - ETA: 56s - loss: 3.4160 - accuracy: 0.125 - ETA: 55s - loss: 3.4154 - accuracy: 0.125 - ETA: 54s - loss: 3.4151 - accuracy: 0.125 - ETA: 53s - loss: 3.4158 - accuracy: 0.125 - ETA: 53s - loss: 3.4161 - accuracy: 0.125 - ETA: 52s - loss: 3.4157 - accuracy: 0.125 - ETA: 51s - loss: 3.4155 - accuracy: 0.125 - ETA: 50s - loss: 3.4150 - accuracy: 0.125 - ETA: 50s - loss: 3.4153 - accuracy: 0.125 - ETA: 49s - loss: 3.4154 - accuracy: 0.125 - ETA: 48s - loss: 3.4150 - accuracy: 0.125 - ETA: 47s - loss: 3.4146 - accuracy: 0.125 - ETA: 47s - loss: 3.4141 - accuracy: 0.125 - ETA: 46s - loss: 3.4136 - accuracy: 0.125 - ETA: 45s - loss: 3.4131 - accuracy: 0.125 - ETA: 44s - loss: 3.4127 - accuracy: 0.126 - ETA: 44s - loss: 3.4129 - accuracy: 0.126 - ETA: 43s - loss: 3.4131 - accuracy: 0.126 - ETA: 42s - loss: 3.4131 - accuracy: 0.126 - ETA: 41s - loss: 3.4135 - accuracy: 0.126 - ETA: 40s - loss: 3.4127 - accuracy: 0.126 - ETA: 40s - loss: 3.4125 - accuracy: 0.126 - ETA: 39s - loss: 3.4128 - accuracy: 0.126 - ETA: 38s - loss: 3.4135 - accuracy: 0.126 - ETA: 37s - loss: 3.4131 - accuracy: 0.126 - ETA: 37s - loss: 3.4131 - accuracy: 0.126 - ETA: 36s - loss: 3.4126 - accuracy: 0.126 - ETA: 35s - loss: 3.4123 - accuracy: 0.126 - ETA: 34s - loss: 3.4121 - accuracy: 0.126 - ETA: 34s - loss: 3.4122 - accuracy: 0.126 - ETA: 33s - loss: 3.4118 - accuracy: 0.126 - ETA: 32s - loss: 3.4116 - accuracy: 0.126 - ETA: 31s - loss: 3.4120 - accuracy: 0.126 - ETA: 31s - loss: 3.4120 - accuracy: 0.126 - ETA: 30s - loss: 3.4119 - accuracy: 0.126 - ETA: 29s - loss: 3.4122 - accuracy: 0.126 - ETA: 28s - loss: 3.4125 - accuracy: 0.126 - ETA: 28s - loss: 3.4130 - accuracy: 0.126 - ETA: 27s - loss: 3.4129 - accuracy: 0.126 - ETA: 26s - loss: 3.4132 - accuracy: 0.126 - ETA: 25s - loss: 3.4136 - accuracy: 0.125 - ETA: 25s - loss: 3.4127 - accuracy: 0.126 - ETA: 24s - loss: 3.4127 - accuracy: 0.126 - ETA: 23s - loss: 3.4132 - accuracy: 0.125 - ETA: 22s - loss: 3.4135 - accuracy: 0.125 - ETA: 21s - loss: 3.4136 - accuracy: 0.125 - ETA: 21s - loss: 3.4133 - accuracy: 0.126 - ETA: 20s - loss: 3.4134 - accuracy: 0.126 - ETA: 19s - loss: 3.4132 - accuracy: 0.125 - ETA: 18s - loss: 3.4130 - accuracy: 0.126 - ETA: 18s - loss: 3.4131 - accuracy: 0.125 - ETA: 17s - loss: 3.4136 - accuracy: 0.125 - ETA: 16s - loss: 3.4135 - accuracy: 0.125 - ETA: 15s - loss: 3.4140 - accuracy: 0.125 - ETA: 15s - loss: 3.4137 - accuracy: 0.125 - ETA: 14s - loss: 3.4142 - accuracy: 0.125 - ETA: 13s - loss: 3.4148 - accuracy: 0.125 - ETA: 12s - loss: 3.4150 - accuracy: 0.125 - ETA: 12s - loss: 3.4152 - accuracy: 0.125 - ETA: 11s - loss: 3.4156 - accuracy: 0.125 - ETA: 10s - loss: 3.4153 - accuracy: 0.125 - ETA: 9s - loss: 3.4149 - accuracy: 0.126 - ETA: 9s - loss: 3.4146 - accuracy: 0.12 - ETA: 8s - loss: 3.4146 - accuracy: 0.12 - ETA: 7s - loss: 3.4145 - accuracy: 0.12 - ETA: 6s - loss: 3.4141 - accuracy: 0.12 - ETA: 6s - loss: 3.4144 - accuracy: 0.12 - ETA: 5s - loss: 3.4140 - accuracy: 0.12 - ETA: 4s - loss: 3.4139 - accuracy: 0.12 - ETA: 3s - loss: 3.4140 - accuracy: 0.12 - ETA: 3s - loss: 3.4143 - accuracy: 0.12 - ETA: 2s - loss: 3.4144 - accuracy: 0.12 - ETA: 1s - loss: 3.4148 - accuracy: 0.12 - ETA: 0s - loss: 3.4147 - accuracy: 0.12 - ETA: 0s - loss: 3.4153 - accuracy: 0.12 - 267s 6ms/step - loss: 3.4152 - accuracy: 0.1256 - val_loss: 3.9275 - val_accuracy: 0.0317\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:07 - loss: 3.5044 - accuracy: 0.11 - ETA: 4:06 - loss: 3.4831 - accuracy: 0.12 - ETA: 4:11 - loss: 3.4900 - accuracy: 0.11 - ETA: 4:09 - loss: 3.4446 - accuracy: 0.11 - ETA: 4:07 - loss: 3.4392 - accuracy: 0.11 - ETA: 4:06 - loss: 3.4016 - accuracy: 0.12 - ETA: 4:02 - loss: 3.3956 - accuracy: 0.12 - ETA: 4:00 - loss: 3.4034 - accuracy: 0.12 - ETA: 3:59 - loss: 3.3934 - accuracy: 0.13 - ETA: 3:58 - loss: 3.3803 - accuracy: 0.12 - ETA: 3:57 - loss: 3.3946 - accuracy: 0.12 - ETA: 3:56 - loss: 3.3967 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3980 - accuracy: 0.12 - ETA: 3:54 - loss: 3.4024 - accuracy: 0.12 - ETA: 3:54 - loss: 3.3972 - accuracy: 0.12 - ETA: 3:54 - loss: 3.4068 - accuracy: 0.12 - ETA: 3:53 - loss: 3.4140 - accuracy: 0.12 - ETA: 3:52 - loss: 3.4140 - accuracy: 0.12 - ETA: 3:51 - loss: 3.4065 - accuracy: 0.12 - ETA: 3:50 - loss: 3.4126 - accuracy: 0.12 - ETA: 3:50 - loss: 3.4061 - accuracy: 0.12 - ETA: 3:49 - loss: 3.4025 - accuracy: 0.12 - ETA: 3:48 - loss: 3.3982 - accuracy: 0.12 - ETA: 3:47 - loss: 3.3989 - accuracy: 0.12 - ETA: 3:47 - loss: 3.3946 - accuracy: 0.12 - ETA: 3:46 - loss: 3.3955 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3980 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3985 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3995 - accuracy: 0.12 - ETA: 3:42 - loss: 3.3993 - accuracy: 0.12 - ETA: 3:42 - loss: 3.3983 - accuracy: 0.12 - ETA: 3:41 - loss: 3.4020 - accuracy: 0.12 - ETA: 3:40 - loss: 3.4041 - accuracy: 0.12 - ETA: 3:39 - loss: 3.4059 - accuracy: 0.12 - ETA: 3:38 - loss: 3.4062 - accuracy: 0.12 - ETA: 3:38 - loss: 3.4084 - accuracy: 0.12 - ETA: 3:37 - loss: 3.4097 - accuracy: 0.12 - ETA: 3:37 - loss: 3.4112 - accuracy: 0.12 - ETA: 3:36 - loss: 3.4149 - accuracy: 0.12 - ETA: 3:35 - loss: 3.4121 - accuracy: 0.12 - ETA: 3:34 - loss: 3.4128 - accuracy: 0.12 - ETA: 3:33 - loss: 3.4107 - accuracy: 0.12 - ETA: 3:33 - loss: 3.4128 - accuracy: 0.12 - ETA: 3:32 - loss: 3.4164 - accuracy: 0.12 - ETA: 3:31 - loss: 3.4179 - accuracy: 0.12 - ETA: 3:30 - loss: 3.4203 - accuracy: 0.12 - ETA: 3:29 - loss: 3.4195 - accuracy: 0.12 - ETA: 3:29 - loss: 3.4193 - accuracy: 0.12 - ETA: 3:29 - loss: 3.4175 - accuracy: 0.12 - ETA: 3:28 - loss: 3.4144 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4178 - accuracy: 0.12 - ETA: 3:26 - loss: 3.4132 - accuracy: 0.12 - ETA: 3:25 - loss: 3.4125 - accuracy: 0.12 - ETA: 3:25 - loss: 3.4136 - accuracy: 0.12 - ETA: 3:24 - loss: 3.4103 - accuracy: 0.12 - ETA: 3:23 - loss: 3.4097 - accuracy: 0.12 - ETA: 3:22 - loss: 3.4089 - accuracy: 0.12 - ETA: 3:22 - loss: 3.4094 - accuracy: 0.12 - ETA: 3:21 - loss: 3.4084 - accuracy: 0.12 - ETA: 3:20 - loss: 3.4087 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4082 - accuracy: 0.12 - ETA: 3:18 - loss: 3.4050 - accuracy: 0.12 - ETA: 3:18 - loss: 3.4051 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4043 - accuracy: 0.12 - ETA: 3:16 - loss: 3.4061 - accuracy: 0.12 - ETA: 3:15 - loss: 3.4057 - accuracy: 0.12 - ETA: 3:15 - loss: 3.4045 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4070 - accuracy: 0.12 - ETA: 3:13 - loss: 3.4069 - accuracy: 0.12 - ETA: 3:12 - loss: 3.4078 - accuracy: 0.12 - ETA: 3:12 - loss: 3.4090 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4104 - accuracy: 0.12 - ETA: 3:10 - loss: 3.4113 - accuracy: 0.12 - ETA: 3:09 - loss: 3.4128 - accuracy: 0.12 - ETA: 3:09 - loss: 3.4136 - accuracy: 0.12 - ETA: 3:08 - loss: 3.4119 - accuracy: 0.12 - ETA: 3:07 - loss: 3.4107 - accuracy: 0.12 - ETA: 3:06 - loss: 3.4126 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4128 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4135 - accuracy: 0.12 - ETA: 3:04 - loss: 3.4134 - accuracy: 0.12 - ETA: 3:03 - loss: 3.4146 - accuracy: 0.12 - ETA: 3:02 - loss: 3.4142 - accuracy: 0.12 - ETA: 3:02 - loss: 3.4130 - accuracy: 0.12 - ETA: 3:01 - loss: 3.4097 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4110 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4113 - accuracy: 0.12 - ETA: 2:59 - loss: 3.4105 - accuracy: 0.12 - ETA: 2:58 - loss: 3.4116 - accuracy: 0.12 - ETA: 2:58 - loss: 3.4112 - accuracy: 0.12 - ETA: 2:57 - loss: 3.4125 - accuracy: 0.12 - ETA: 2:57 - loss: 3.4127 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4125 - accuracy: 0.12 - ETA: 2:55 - loss: 3.4122 - accuracy: 0.12 - ETA: 2:54 - loss: 3.4127 - accuracy: 0.12 - ETA: 2:54 - loss: 3.4132 - accuracy: 0.12 - ETA: 2:53 - loss: 3.4146 - accuracy: 0.12 - ETA: 2:52 - loss: 3.4142 - accuracy: 0.12 - ETA: 2:51 - loss: 3.4150 - accuracy: 0.12 - ETA: 2:51 - loss: 3.4156 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4164 - accuracy: 0.12 - ETA: 2:49 - loss: 3.4165 - accuracy: 0.12 - ETA: 2:48 - loss: 3.4171 - accuracy: 0.12 - ETA: 2:48 - loss: 3.4172 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4181 - accuracy: 0.12 - ETA: 2:46 - loss: 3.4164 - accuracy: 0.12 - ETA: 2:45 - loss: 3.4172 - accuracy: 0.12 - ETA: 2:45 - loss: 3.4164 - accuracy: 0.12 - ETA: 2:44 - loss: 3.4166 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4168 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4181 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4186 - accuracy: 0.12 - ETA: 2:41 - loss: 3.4196 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4202 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4221 - accuracy: 0.12 - ETA: 2:39 - loss: 3.4227 - accuracy: 0.12 - ETA: 2:38 - loss: 3.4229 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4234 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4233 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4225 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4225 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4226 - accuracy: 0.12 - ETA: 2:34 - loss: 3.4230 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4225 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4214 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4207 - accuracy: 0.12 - ETA: 2:31 - loss: 3.4225 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4231 - accuracy: 0.12 - ETA: 2:29 - loss: 3.4225 - accuracy: 0.12 - ETA: 2:29 - loss: 3.4223 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4226 - accuracy: 0.12 - ETA: 2:27 - loss: 3.4226 - accuracy: 0.12 - ETA: 2:27 - loss: 3.4228 - accuracy: 0.12 - ETA: 2:26 - loss: 3.4228 - accuracy: 0.12 - ETA: 2:25 - loss: 3.4240 - accuracy: 0.12 - ETA: 2:24 - loss: 3.4233 - accuracy: 0.12 - ETA: 2:24 - loss: 3.4235 - accuracy: 0.12 - ETA: 2:23 - loss: 3.4232 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4244 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4252 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4239 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4233 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4233 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4230 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4240 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4237 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4222 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4229 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4221 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4220 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4224 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4217 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4210 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4209 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4207 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4210 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4214 - accuracy: 0.12 - ETA: 2:08 - loss: 3.4218 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4227 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4221 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4221 - accuracy: 0.12 - ETA: 2:05 - loss: 3.4225 - accuracy: 0.12 - ETA: 2:04 - loss: 3.4231 - accuracy: 0.12 - ETA: 2:04 - loss: 3.4236 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4240 - accuracy: 0.12 - ETA: 2:02 - loss: 3.4234 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4237 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4231 - accuracy: 0.12 - ETA: 2:00 - loss: 3.4236 - accuracy: 0.12 - ETA: 1:59 - loss: 3.4236 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4241 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4240 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4252 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4260 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4258 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4250 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4244 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4247 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4246 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4257 - accuracy: 0.12 - ETA: 1:51 - loss: 3.4261 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4265 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4263 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4267 - accuracy: 0.12 - ETA: 1:48 - loss: 3.4279 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4277 - accuracy: 0.1206"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:47 - loss: 3.4286 - accuracy: 0.12 - ETA: 1:46 - loss: 3.4289 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4286 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4287 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4285 - accuracy: 0.12 - ETA: 1:43 - loss: 3.4280 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4276 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4273 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4279 - accuracy: 0.12 - ETA: 1:40 - loss: 3.4283 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4281 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4275 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4274 - accuracy: 0.12 - ETA: 1:37 - loss: 3.4276 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4275 - accuracy: 0.12 - ETA: 1:35 - loss: 3.4280 - accuracy: 0.12 - ETA: 1:35 - loss: 3.4279 - accuracy: 0.12 - ETA: 1:34 - loss: 3.4277 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4283 - accuracy: 0.12 - ETA: 1:32 - loss: 3.4284 - accuracy: 0.12 - ETA: 1:32 - loss: 3.4290 - accuracy: 0.12 - ETA: 1:31 - loss: 3.4289 - accuracy: 0.12 - ETA: 1:30 - loss: 3.4287 - accuracy: 0.12 - ETA: 1:29 - loss: 3.4285 - accuracy: 0.12 - ETA: 1:29 - loss: 3.4285 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4286 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4287 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4283 - accuracy: 0.12 - ETA: 1:26 - loss: 3.4283 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4277 - accuracy: 0.12 - ETA: 1:24 - loss: 3.4290 - accuracy: 0.12 - ETA: 1:24 - loss: 3.4291 - accuracy: 0.12 - ETA: 1:23 - loss: 3.4288 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4284 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4285 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4288 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4291 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4287 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4286 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4283 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4293 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4294 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4291 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4289 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4286 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4287 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4287 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4280 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4271 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4264 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4261 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4260 - accuracy: 0.12 - ETA: 1:08 - loss: 3.4264 - accuracy: 0.12 - ETA: 1:07 - loss: 3.4265 - accuracy: 0.12 - ETA: 1:06 - loss: 3.4268 - accuracy: 0.12 - ETA: 1:06 - loss: 3.4269 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4258 - accuracy: 0.12 - ETA: 1:04 - loss: 3.4257 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4262 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4269 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4274 - accuracy: 0.12 - ETA: 1:01 - loss: 3.4271 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4278 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4282 - accuracy: 0.12 - ETA: 59s - loss: 3.4285 - accuracy: 0.1230 - ETA: 58s - loss: 3.4287 - accuracy: 0.123 - ETA: 57s - loss: 3.4286 - accuracy: 0.123 - ETA: 57s - loss: 3.4293 - accuracy: 0.123 - ETA: 56s - loss: 3.4293 - accuracy: 0.122 - ETA: 55s - loss: 3.4294 - accuracy: 0.122 - ETA: 55s - loss: 3.4295 - accuracy: 0.122 - ETA: 54s - loss: 3.4300 - accuracy: 0.122 - ETA: 53s - loss: 3.4297 - accuracy: 0.122 - ETA: 52s - loss: 3.4299 - accuracy: 0.122 - ETA: 52s - loss: 3.4299 - accuracy: 0.122 - ETA: 51s - loss: 3.4299 - accuracy: 0.122 - ETA: 50s - loss: 3.4298 - accuracy: 0.122 - ETA: 49s - loss: 3.4296 - accuracy: 0.123 - ETA: 49s - loss: 3.4293 - accuracy: 0.123 - ETA: 48s - loss: 3.4291 - accuracy: 0.123 - ETA: 47s - loss: 3.4289 - accuracy: 0.123 - ETA: 46s - loss: 3.4294 - accuracy: 0.122 - ETA: 46s - loss: 3.4293 - accuracy: 0.123 - ETA: 45s - loss: 3.4295 - accuracy: 0.123 - ETA: 44s - loss: 3.4294 - accuracy: 0.123 - ETA: 43s - loss: 3.4300 - accuracy: 0.122 - ETA: 43s - loss: 3.4303 - accuracy: 0.122 - ETA: 42s - loss: 3.4303 - accuracy: 0.122 - ETA: 41s - loss: 3.4303 - accuracy: 0.122 - ETA: 40s - loss: 3.4308 - accuracy: 0.122 - ETA: 40s - loss: 3.4307 - accuracy: 0.122 - ETA: 39s - loss: 3.4308 - accuracy: 0.122 - ETA: 38s - loss: 3.4309 - accuracy: 0.122 - ETA: 37s - loss: 3.4312 - accuracy: 0.122 - ETA: 37s - loss: 3.4306 - accuracy: 0.122 - ETA: 36s - loss: 3.4305 - accuracy: 0.122 - ETA: 35s - loss: 3.4303 - accuracy: 0.122 - ETA: 34s - loss: 3.4295 - accuracy: 0.122 - ETA: 34s - loss: 3.4296 - accuracy: 0.122 - ETA: 33s - loss: 3.4302 - accuracy: 0.122 - ETA: 32s - loss: 3.4304 - accuracy: 0.122 - ETA: 31s - loss: 3.4300 - accuracy: 0.122 - ETA: 31s - loss: 3.4296 - accuracy: 0.122 - ETA: 30s - loss: 3.4300 - accuracy: 0.122 - ETA: 29s - loss: 3.4304 - accuracy: 0.122 - ETA: 29s - loss: 3.4302 - accuracy: 0.122 - ETA: 28s - loss: 3.4299 - accuracy: 0.122 - ETA: 27s - loss: 3.4299 - accuracy: 0.122 - ETA: 26s - loss: 3.4301 - accuracy: 0.122 - ETA: 26s - loss: 3.4298 - accuracy: 0.122 - ETA: 25s - loss: 3.4294 - accuracy: 0.122 - ETA: 24s - loss: 3.4292 - accuracy: 0.122 - ETA: 23s - loss: 3.4298 - accuracy: 0.122 - ETA: 23s - loss: 3.4291 - accuracy: 0.122 - ETA: 22s - loss: 3.4288 - accuracy: 0.122 - ETA: 21s - loss: 3.4286 - accuracy: 0.122 - ETA: 20s - loss: 3.4285 - accuracy: 0.122 - ETA: 20s - loss: 3.4284 - accuracy: 0.122 - ETA: 19s - loss: 3.4285 - accuracy: 0.122 - ETA: 18s - loss: 3.4283 - accuracy: 0.122 - ETA: 17s - loss: 3.4282 - accuracy: 0.122 - ETA: 17s - loss: 3.4277 - accuracy: 0.122 - ETA: 16s - loss: 3.4280 - accuracy: 0.122 - ETA: 15s - loss: 3.4285 - accuracy: 0.122 - ETA: 14s - loss: 3.4281 - accuracy: 0.122 - ETA: 14s - loss: 3.4278 - accuracy: 0.122 - ETA: 13s - loss: 3.4273 - accuracy: 0.122 - ETA: 12s - loss: 3.4273 - accuracy: 0.122 - ETA: 11s - loss: 3.4272 - accuracy: 0.122 - ETA: 11s - loss: 3.4276 - accuracy: 0.122 - ETA: 10s - loss: 3.4271 - accuracy: 0.122 - ETA: 9s - loss: 3.4271 - accuracy: 0.122 - ETA: 8s - loss: 3.4269 - accuracy: 0.12 - ETA: 8s - loss: 3.4268 - accuracy: 0.12 - ETA: 7s - loss: 3.4271 - accuracy: 0.12 - ETA: 6s - loss: 3.4267 - accuracy: 0.12 - ETA: 5s - loss: 3.4270 - accuracy: 0.12 - ETA: 5s - loss: 3.4274 - accuracy: 0.12 - ETA: 4s - loss: 3.4275 - accuracy: 0.12 - ETA: 3s - loss: 3.4273 - accuracy: 0.12 - ETA: 3s - loss: 3.4278 - accuracy: 0.12 - ETA: 2s - loss: 3.4276 - accuracy: 0.12 - ETA: 1s - loss: 3.4274 - accuracy: 0.12 - ETA: 0s - loss: 3.4274 - accuracy: 0.12 - ETA: 0s - loss: 3.4271 - accuracy: 0.12 - 264s 6ms/step - loss: 3.4272 - accuracy: 0.1225 - val_loss: 3.9239 - val_accuracy: 0.0277\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:18 - loss: 3.3989 - accuracy: 0.14 - ETA: 4:20 - loss: 3.4740 - accuracy: 0.11 - ETA: 4:14 - loss: 3.4672 - accuracy: 0.10 - ETA: 4:09 - loss: 3.4366 - accuracy: 0.11 - ETA: 4:06 - loss: 3.4427 - accuracy: 0.12 - ETA: 4:05 - loss: 3.4330 - accuracy: 0.12 - ETA: 4:05 - loss: 3.4243 - accuracy: 0.13 - ETA: 4:03 - loss: 3.4240 - accuracy: 0.12 - ETA: 4:00 - loss: 3.4293 - accuracy: 0.12 - ETA: 3:58 - loss: 3.4369 - accuracy: 0.12 - ETA: 3:56 - loss: 3.4352 - accuracy: 0.12 - ETA: 3:55 - loss: 3.4317 - accuracy: 0.12 - ETA: 3:54 - loss: 3.4333 - accuracy: 0.12 - ETA: 3:53 - loss: 3.4306 - accuracy: 0.12 - ETA: 3:52 - loss: 3.4323 - accuracy: 0.12 - ETA: 3:52 - loss: 3.4233 - accuracy: 0.12 - ETA: 3:51 - loss: 3.4160 - accuracy: 0.12 - ETA: 3:50 - loss: 3.4048 - accuracy: 0.12 - ETA: 3:49 - loss: 3.4039 - accuracy: 0.12 - ETA: 3:48 - loss: 3.4064 - accuracy: 0.12 - ETA: 3:47 - loss: 3.4016 - accuracy: 0.13 - ETA: 3:46 - loss: 3.4024 - accuracy: 0.13 - ETA: 3:46 - loss: 3.4093 - accuracy: 0.12 - ETA: 3:45 - loss: 3.4038 - accuracy: 0.12 - ETA: 3:44 - loss: 3.4048 - accuracy: 0.12 - ETA: 3:43 - loss: 3.4016 - accuracy: 0.12 - ETA: 3:43 - loss: 3.4079 - accuracy: 0.12 - ETA: 3:42 - loss: 3.4075 - accuracy: 0.13 - ETA: 3:41 - loss: 3.4121 - accuracy: 0.12 - ETA: 3:40 - loss: 3.4109 - accuracy: 0.12 - ETA: 3:39 - loss: 3.4122 - accuracy: 0.12 - ETA: 3:39 - loss: 3.4118 - accuracy: 0.12 - ETA: 3:38 - loss: 3.4147 - accuracy: 0.12 - ETA: 3:37 - loss: 3.4105 - accuracy: 0.12 - ETA: 3:36 - loss: 3.4080 - accuracy: 0.12 - ETA: 3:35 - loss: 3.4042 - accuracy: 0.13 - ETA: 3:35 - loss: 3.4030 - accuracy: 0.13 - ETA: 3:34 - loss: 3.4023 - accuracy: 0.13 - ETA: 3:34 - loss: 3.4029 - accuracy: 0.13 - ETA: 3:33 - loss: 3.4070 - accuracy: 0.13 - ETA: 3:32 - loss: 3.4067 - accuracy: 0.13 - ETA: 3:31 - loss: 3.4075 - accuracy: 0.13 - ETA: 3:30 - loss: 3.4054 - accuracy: 0.13 - ETA: 3:30 - loss: 3.4068 - accuracy: 0.13 - ETA: 3:29 - loss: 3.4030 - accuracy: 0.13 - ETA: 3:28 - loss: 3.4032 - accuracy: 0.13 - ETA: 3:28 - loss: 3.4032 - accuracy: 0.13 - ETA: 3:27 - loss: 3.4037 - accuracy: 0.13 - ETA: 3:26 - loss: 3.4018 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3985 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3985 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3991 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3987 - accuracy: 0.13 - ETA: 3:22 - loss: 3.4013 - accuracy: 0.12 - ETA: 3:21 - loss: 3.3990 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3991 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3992 - accuracy: 0.12 - ETA: 3:20 - loss: 3.4006 - accuracy: 0.12 - ETA: 3:19 - loss: 3.3994 - accuracy: 0.13 - ETA: 3:18 - loss: 3.4004 - accuracy: 0.13 - ETA: 3:17 - loss: 3.4032 - accuracy: 0.13 - ETA: 3:17 - loss: 3.4052 - accuracy: 0.12 - ETA: 3:16 - loss: 3.4038 - accuracy: 0.12 - ETA: 3:15 - loss: 3.4055 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4039 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4067 - accuracy: 0.12 - ETA: 3:13 - loss: 3.4080 - accuracy: 0.12 - ETA: 3:12 - loss: 3.4058 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4063 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4067 - accuracy: 0.12 - ETA: 3:10 - loss: 3.4087 - accuracy: 0.12 - ETA: 3:09 - loss: 3.4110 - accuracy: 0.12 - ETA: 3:09 - loss: 3.4111 - accuracy: 0.12 - ETA: 3:08 - loss: 3.4118 - accuracy: 0.12 - ETA: 3:07 - loss: 3.4144 - accuracy: 0.12 - ETA: 3:06 - loss: 3.4149 - accuracy: 0.12 - ETA: 3:06 - loss: 3.4175 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4197 - accuracy: 0.12 - ETA: 3:04 - loss: 3.4208 - accuracy: 0.12 - ETA: 3:04 - loss: 3.4203 - accuracy: 0.12 - ETA: 3:03 - loss: 3.4212 - accuracy: 0.12 - ETA: 3:02 - loss: 3.4228 - accuracy: 0.12 - ETA: 3:01 - loss: 3.4211 - accuracy: 0.12 - ETA: 3:01 - loss: 3.4213 - accuracy: 0.12 - ETA: 3:01 - loss: 3.4218 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4216 - accuracy: 0.12 - ETA: 2:59 - loss: 3.4220 - accuracy: 0.12 - ETA: 2:59 - loss: 3.4223 - accuracy: 0.12 - ETA: 2:58 - loss: 3.4217 - accuracy: 0.12 - ETA: 2:57 - loss: 3.4218 - accuracy: 0.12 - ETA: 2:57 - loss: 3.4218 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4220 - accuracy: 0.12 - ETA: 2:55 - loss: 3.4218 - accuracy: 0.12 - ETA: 2:55 - loss: 3.4246 - accuracy: 0.12 - ETA: 2:54 - loss: 3.4254 - accuracy: 0.12 - ETA: 2:53 - loss: 3.4243 - accuracy: 0.12 - ETA: 2:53 - loss: 3.4241 - accuracy: 0.12 - ETA: 2:52 - loss: 3.4246 - accuracy: 0.12 - ETA: 2:51 - loss: 3.4261 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4251 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4255 - accuracy: 0.12 - ETA: 2:49 - loss: 3.4259 - accuracy: 0.12 - ETA: 2:48 - loss: 3.4253 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4259 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4265 - accuracy: 0.12 - ETA: 2:46 - loss: 3.4250 - accuracy: 0.12 - ETA: 2:45 - loss: 3.4245 - accuracy: 0.12 - ETA: 2:45 - loss: 3.4231 - accuracy: 0.12 - ETA: 2:44 - loss: 3.4228 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4233 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4229 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4251 - accuracy: 0.12 - ETA: 2:41 - loss: 3.4237 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4220 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4229 - accuracy: 0.12 - ETA: 2:39 - loss: 3.4224 - accuracy: 0.12 - ETA: 2:38 - loss: 3.4243 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4220 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4234 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4234 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4219 - accuracy: 0.12 - ETA: 2:34 - loss: 3.4211 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4204 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4204 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4203 - accuracy: 0.12 - ETA: 2:31 - loss: 3.4209 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4211 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4222 - accuracy: 0.12 - ETA: 2:29 - loss: 3.4227 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4231 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4227 - accuracy: 0.12 - ETA: 2:27 - loss: 3.4230 - accuracy: 0.12 - ETA: 2:26 - loss: 3.4215 - accuracy: 0.12 - ETA: 2:25 - loss: 3.4215 - accuracy: 0.12 - ETA: 2:25 - loss: 3.4207 - accuracy: 0.12 - ETA: 2:24 - loss: 3.4202 - accuracy: 0.12 - ETA: 2:23 - loss: 3.4211 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4216 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4209 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4206 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4201 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4200 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4199 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4206 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4205 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4208 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4195 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4196 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4199 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4207 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4210 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4205 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4208 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4204 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4195 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4200 - accuracy: 0.12 - ETA: 2:08 - loss: 3.4209 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4215 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4218 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4209 - accuracy: 0.12 - ETA: 2:05 - loss: 3.4206 - accuracy: 0.12 - ETA: 2:04 - loss: 3.4205 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4218 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4224 - accuracy: 0.12 - ETA: 2:02 - loss: 3.4222 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4222 - accuracy: 0.12 - ETA: 2:00 - loss: 3.4224 - accuracy: 0.12 - ETA: 2:00 - loss: 3.4216 - accuracy: 0.12 - ETA: 1:59 - loss: 3.4217 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4219 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4203 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4202 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4200 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4201 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4193 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4201 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4201 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4204 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4199 - accuracy: 0.12 - ETA: 1:51 - loss: 3.4199 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4196 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4186 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4182 - accuracy: 0.12 - ETA: 1:48 - loss: 3.4177 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4182 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4182 - accuracy: 0.1232"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:46 - loss: 3.4180 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4174 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4184 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4176 - accuracy: 0.12 - ETA: 1:43 - loss: 3.4174 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4175 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4167 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4162 - accuracy: 0.12 - ETA: 1:40 - loss: 3.4158 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4160 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4160 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4166 - accuracy: 0.12 - ETA: 1:37 - loss: 3.4168 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4168 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4164 - accuracy: 0.12 - ETA: 1:35 - loss: 3.4162 - accuracy: 0.12 - ETA: 1:34 - loss: 3.4157 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4158 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4162 - accuracy: 0.12 - ETA: 1:32 - loss: 3.4159 - accuracy: 0.12 - ETA: 1:31 - loss: 3.4153 - accuracy: 0.12 - ETA: 1:30 - loss: 3.4157 - accuracy: 0.12 - ETA: 1:30 - loss: 3.4166 - accuracy: 0.12 - ETA: 1:29 - loss: 3.4167 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4172 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4171 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4164 - accuracy: 0.12 - ETA: 1:26 - loss: 3.4164 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4166 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4163 - accuracy: 0.12 - ETA: 1:24 - loss: 3.4162 - accuracy: 0.12 - ETA: 1:23 - loss: 3.4156 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4154 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4150 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4158 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4161 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4162 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4164 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4158 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4157 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4157 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4155 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4151 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4147 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4146 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4141 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4132 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4123 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4128 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4127 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4129 - accuracy: 0.12 - ETA: 1:08 - loss: 3.4124 - accuracy: 0.12 - ETA: 1:07 - loss: 3.4125 - accuracy: 0.12 - ETA: 1:07 - loss: 3.4122 - accuracy: 0.12 - ETA: 1:06 - loss: 3.4129 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4125 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4125 - accuracy: 0.12 - ETA: 1:04 - loss: 3.4126 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4121 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4110 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4107 - accuracy: 0.12 - ETA: 1:01 - loss: 3.4112 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4116 - accuracy: 0.12 - ETA: 59s - loss: 3.4119 - accuracy: 0.1246 - ETA: 59s - loss: 3.4116 - accuracy: 0.124 - ETA: 58s - loss: 3.4115 - accuracy: 0.124 - ETA: 57s - loss: 3.4116 - accuracy: 0.124 - ETA: 56s - loss: 3.4114 - accuracy: 0.124 - ETA: 56s - loss: 3.4111 - accuracy: 0.124 - ETA: 55s - loss: 3.4113 - accuracy: 0.124 - ETA: 54s - loss: 3.4108 - accuracy: 0.124 - ETA: 53s - loss: 3.4110 - accuracy: 0.124 - ETA: 53s - loss: 3.4112 - accuracy: 0.124 - ETA: 52s - loss: 3.4120 - accuracy: 0.124 - ETA: 51s - loss: 3.4117 - accuracy: 0.124 - ETA: 50s - loss: 3.4119 - accuracy: 0.124 - ETA: 50s - loss: 3.4118 - accuracy: 0.124 - ETA: 49s - loss: 3.4113 - accuracy: 0.124 - ETA: 48s - loss: 3.4115 - accuracy: 0.124 - ETA: 48s - loss: 3.4116 - accuracy: 0.124 - ETA: 47s - loss: 3.4110 - accuracy: 0.124 - ETA: 46s - loss: 3.4112 - accuracy: 0.124 - ETA: 45s - loss: 3.4112 - accuracy: 0.124 - ETA: 45s - loss: 3.4116 - accuracy: 0.124 - ETA: 44s - loss: 3.4115 - accuracy: 0.124 - ETA: 43s - loss: 3.4117 - accuracy: 0.124 - ETA: 42s - loss: 3.4116 - accuracy: 0.124 - ETA: 42s - loss: 3.4118 - accuracy: 0.124 - ETA: 41s - loss: 3.4113 - accuracy: 0.124 - ETA: 40s - loss: 3.4110 - accuracy: 0.124 - ETA: 39s - loss: 3.4115 - accuracy: 0.124 - ETA: 39s - loss: 3.4118 - accuracy: 0.124 - ETA: 38s - loss: 3.4116 - accuracy: 0.124 - ETA: 37s - loss: 3.4113 - accuracy: 0.124 - ETA: 36s - loss: 3.4112 - accuracy: 0.124 - ETA: 36s - loss: 3.4112 - accuracy: 0.124 - ETA: 35s - loss: 3.4106 - accuracy: 0.124 - ETA: 34s - loss: 3.4109 - accuracy: 0.124 - ETA: 34s - loss: 3.4105 - accuracy: 0.124 - ETA: 33s - loss: 3.4099 - accuracy: 0.124 - ETA: 32s - loss: 3.4096 - accuracy: 0.124 - ETA: 31s - loss: 3.4096 - accuracy: 0.124 - ETA: 31s - loss: 3.4100 - accuracy: 0.124 - ETA: 30s - loss: 3.4099 - accuracy: 0.124 - ETA: 29s - loss: 3.4093 - accuracy: 0.124 - ETA: 28s - loss: 3.4090 - accuracy: 0.124 - ETA: 28s - loss: 3.4082 - accuracy: 0.124 - ETA: 27s - loss: 3.4083 - accuracy: 0.124 - ETA: 26s - loss: 3.4087 - accuracy: 0.124 - ETA: 25s - loss: 3.4091 - accuracy: 0.124 - ETA: 25s - loss: 3.4093 - accuracy: 0.124 - ETA: 24s - loss: 3.4090 - accuracy: 0.124 - ETA: 23s - loss: 3.4092 - accuracy: 0.124 - ETA: 22s - loss: 3.4094 - accuracy: 0.124 - ETA: 22s - loss: 3.4088 - accuracy: 0.124 - ETA: 21s - loss: 3.4083 - accuracy: 0.124 - ETA: 20s - loss: 3.4083 - accuracy: 0.124 - ETA: 20s - loss: 3.4085 - accuracy: 0.124 - ETA: 19s - loss: 3.4086 - accuracy: 0.124 - ETA: 18s - loss: 3.4084 - accuracy: 0.125 - ETA: 17s - loss: 3.4082 - accuracy: 0.125 - ETA: 17s - loss: 3.4084 - accuracy: 0.125 - ETA: 16s - loss: 3.4079 - accuracy: 0.125 - ETA: 15s - loss: 3.4076 - accuracy: 0.125 - ETA: 14s - loss: 3.4080 - accuracy: 0.125 - ETA: 14s - loss: 3.4085 - accuracy: 0.125 - ETA: 13s - loss: 3.4086 - accuracy: 0.125 - ETA: 12s - loss: 3.4086 - accuracy: 0.125 - ETA: 11s - loss: 3.4088 - accuracy: 0.125 - ETA: 11s - loss: 3.4085 - accuracy: 0.125 - ETA: 10s - loss: 3.4086 - accuracy: 0.125 - ETA: 9s - loss: 3.4086 - accuracy: 0.125 - ETA: 8s - loss: 3.4088 - accuracy: 0.12 - ETA: 8s - loss: 3.4088 - accuracy: 0.12 - ETA: 7s - loss: 3.4088 - accuracy: 0.12 - ETA: 6s - loss: 3.4082 - accuracy: 0.12 - ETA: 5s - loss: 3.4080 - accuracy: 0.12 - ETA: 5s - loss: 3.4077 - accuracy: 0.12 - ETA: 4s - loss: 3.4083 - accuracy: 0.12 - ETA: 3s - loss: 3.4087 - accuracy: 0.12 - ETA: 3s - loss: 3.4083 - accuracy: 0.12 - ETA: 2s - loss: 3.4080 - accuracy: 0.12 - ETA: 1s - loss: 3.4079 - accuracy: 0.12 - ETA: 0s - loss: 3.4078 - accuracy: 0.12 - ETA: 0s - loss: 3.4077 - accuracy: 0.12 - 263s 6ms/step - loss: 3.4076 - accuracy: 0.1252 - val_loss: 4.0950 - val_accuracy: 0.0357\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:16 - loss: 3.6267 - accuracy: 0.09 - ETA: 4:05 - loss: 3.5658 - accuracy: 0.09 - ETA: 4:02 - loss: 3.5236 - accuracy: 0.10 - ETA: 3:59 - loss: 3.4344 - accuracy: 0.12 - ETA: 3:58 - loss: 3.4195 - accuracy: 0.12 - ETA: 3:59 - loss: 3.4026 - accuracy: 0.13 - ETA: 3:58 - loss: 3.4073 - accuracy: 0.13 - ETA: 3:56 - loss: 3.4252 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3981 - accuracy: 0.12 - ETA: 3:54 - loss: 3.3932 - accuracy: 0.12 - ETA: 3:53 - loss: 3.4023 - accuracy: 0.12 - ETA: 3:53 - loss: 3.3928 - accuracy: 0.13 - ETA: 3:52 - loss: 3.4133 - accuracy: 0.12 - ETA: 3:52 - loss: 3.3985 - accuracy: 0.12 - ETA: 3:51 - loss: 3.4016 - accuracy: 0.12 - ETA: 3:50 - loss: 3.3973 - accuracy: 0.12 - ETA: 3:50 - loss: 3.4065 - accuracy: 0.12 - ETA: 3:49 - loss: 3.4060 - accuracy: 0.12 - ETA: 3:47 - loss: 3.4025 - accuracy: 0.12 - ETA: 3:46 - loss: 3.4082 - accuracy: 0.12 - ETA: 3:46 - loss: 3.4127 - accuracy: 0.12 - ETA: 3:46 - loss: 3.4161 - accuracy: 0.12 - ETA: 3:45 - loss: 3.4171 - accuracy: 0.12 - ETA: 3:44 - loss: 3.4205 - accuracy: 0.12 - ETA: 3:43 - loss: 3.4290 - accuracy: 0.12 - ETA: 3:43 - loss: 3.4238 - accuracy: 0.12 - ETA: 3:42 - loss: 3.4262 - accuracy: 0.12 - ETA: 3:42 - loss: 3.4208 - accuracy: 0.12 - ETA: 3:41 - loss: 3.4217 - accuracy: 0.12 - ETA: 3:40 - loss: 3.4229 - accuracy: 0.12 - ETA: 3:39 - loss: 3.4226 - accuracy: 0.12 - ETA: 3:38 - loss: 3.4220 - accuracy: 0.12 - ETA: 3:38 - loss: 3.4182 - accuracy: 0.12 - ETA: 3:37 - loss: 3.4160 - accuracy: 0.12 - ETA: 3:37 - loss: 3.4188 - accuracy: 0.12 - ETA: 3:36 - loss: 3.4208 - accuracy: 0.12 - ETA: 3:35 - loss: 3.4195 - accuracy: 0.12 - ETA: 3:35 - loss: 3.4221 - accuracy: 0.12 - ETA: 3:34 - loss: 3.4212 - accuracy: 0.12 - ETA: 3:34 - loss: 3.4217 - accuracy: 0.12 - ETA: 3:33 - loss: 3.4258 - accuracy: 0.12 - ETA: 3:32 - loss: 3.4248 - accuracy: 0.12 - ETA: 3:31 - loss: 3.4264 - accuracy: 0.12 - ETA: 3:31 - loss: 3.4292 - accuracy: 0.12 - ETA: 3:30 - loss: 3.4266 - accuracy: 0.12 - ETA: 3:30 - loss: 3.4276 - accuracy: 0.12 - ETA: 3:29 - loss: 3.4279 - accuracy: 0.12 - ETA: 3:28 - loss: 3.4247 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4259 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4266 - accuracy: 0.12 - ETA: 3:26 - loss: 3.4289 - accuracy: 0.12 - ETA: 3:25 - loss: 3.4323 - accuracy: 0.12 - ETA: 3:24 - loss: 3.4310 - accuracy: 0.12 - ETA: 3:24 - loss: 3.4285 - accuracy: 0.12 - ETA: 3:23 - loss: 3.4236 - accuracy: 0.12 - ETA: 3:22 - loss: 3.4239 - accuracy: 0.12 - ETA: 3:21 - loss: 3.4224 - accuracy: 0.12 - ETA: 3:20 - loss: 3.4229 - accuracy: 0.12 - ETA: 3:20 - loss: 3.4210 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4220 - accuracy: 0.12 - ETA: 3:18 - loss: 3.4251 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4249 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4248 - accuracy: 0.12 - ETA: 3:16 - loss: 3.4238 - accuracy: 0.12 - ETA: 3:15 - loss: 3.4241 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4252 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4237 - accuracy: 0.12 - ETA: 3:13 - loss: 3.4241 - accuracy: 0.12 - ETA: 3:12 - loss: 3.4240 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4222 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4205 - accuracy: 0.12 - ETA: 3:10 - loss: 3.4200 - accuracy: 0.12 - ETA: 3:09 - loss: 3.4191 - accuracy: 0.12 - ETA: 3:09 - loss: 3.4191 - accuracy: 0.12 - ETA: 3:08 - loss: 3.4158 - accuracy: 0.12 - ETA: 3:07 - loss: 3.4169 - accuracy: 0.12 - ETA: 3:07 - loss: 3.4178 - accuracy: 0.12 - ETA: 3:06 - loss: 3.4167 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4148 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4151 - accuracy: 0.12 - ETA: 3:04 - loss: 3.4130 - accuracy: 0.12 - ETA: 3:03 - loss: 3.4131 - accuracy: 0.12 - ETA: 3:02 - loss: 3.4117 - accuracy: 0.12 - ETA: 3:02 - loss: 3.4092 - accuracy: 0.12 - ETA: 3:01 - loss: 3.4074 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4048 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4044 - accuracy: 0.12 - ETA: 2:59 - loss: 3.4047 - accuracy: 0.12 - ETA: 2:58 - loss: 3.4033 - accuracy: 0.12 - ETA: 2:58 - loss: 3.4031 - accuracy: 0.12 - ETA: 2:57 - loss: 3.4030 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4036 - accuracy: 0.12 - ETA: 2:55 - loss: 3.4037 - accuracy: 0.12 - ETA: 2:55 - loss: 3.4039 - accuracy: 0.12 - ETA: 2:54 - loss: 3.4033 - accuracy: 0.12 - ETA: 2:53 - loss: 3.4029 - accuracy: 0.12 - ETA: 2:52 - loss: 3.4037 - accuracy: 0.12 - ETA: 2:52 - loss: 3.4033 - accuracy: 0.12 - ETA: 2:51 - loss: 3.4022 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4004 - accuracy: 0.12 - ETA: 2:50 - loss: 3.3996 - accuracy: 0.12 - ETA: 2:49 - loss: 3.4000 - accuracy: 0.12 - ETA: 2:48 - loss: 3.3991 - accuracy: 0.12 - ETA: 2:47 - loss: 3.3979 - accuracy: 0.12 - ETA: 2:47 - loss: 3.3980 - accuracy: 0.12 - ETA: 2:46 - loss: 3.3976 - accuracy: 0.12 - ETA: 2:45 - loss: 3.3976 - accuracy: 0.12 - ETA: 2:44 - loss: 3.3984 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4004 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4006 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4003 - accuracy: 0.12 - ETA: 2:41 - loss: 3.4007 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4003 - accuracy: 0.12 - ETA: 2:40 - loss: 3.3988 - accuracy: 0.12 - ETA: 2:39 - loss: 3.3985 - accuracy: 0.12 - ETA: 2:38 - loss: 3.3991 - accuracy: 0.12 - ETA: 2:37 - loss: 3.3999 - accuracy: 0.12 - ETA: 2:37 - loss: 3.3996 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4002 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4003 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4000 - accuracy: 0.12 - ETA: 2:34 - loss: 3.3998 - accuracy: 0.12 - ETA: 2:33 - loss: 3.3993 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4008 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4012 - accuracy: 0.12 - ETA: 2:31 - loss: 3.3996 - accuracy: 0.12 - ETA: 2:30 - loss: 3.3991 - accuracy: 0.12 - ETA: 2:29 - loss: 3.3974 - accuracy: 0.12 - ETA: 2:29 - loss: 3.3966 - accuracy: 0.12 - ETA: 2:28 - loss: 3.3972 - accuracy: 0.12 - ETA: 2:27 - loss: 3.3977 - accuracy: 0.12 - ETA: 2:26 - loss: 3.3976 - accuracy: 0.12 - ETA: 2:26 - loss: 3.3981 - accuracy: 0.12 - ETA: 2:25 - loss: 3.3986 - accuracy: 0.12 - ETA: 2:24 - loss: 3.3987 - accuracy: 0.12 - ETA: 2:24 - loss: 3.3993 - accuracy: 0.12 - ETA: 2:23 - loss: 3.3998 - accuracy: 0.12 - ETA: 2:22 - loss: 3.3988 - accuracy: 0.12 - ETA: 2:21 - loss: 3.3986 - accuracy: 0.12 - ETA: 2:21 - loss: 3.3986 - accuracy: 0.12 - ETA: 2:20 - loss: 3.3987 - accuracy: 0.12 - ETA: 2:19 - loss: 3.3983 - accuracy: 0.12 - ETA: 2:18 - loss: 3.3990 - accuracy: 0.12 - ETA: 2:18 - loss: 3.3987 - accuracy: 0.12 - ETA: 2:17 - loss: 3.3974 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3972 - accuracy: 0.12 - ETA: 2:15 - loss: 3.3980 - accuracy: 0.12 - ETA: 2:15 - loss: 3.3986 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4000 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4006 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4011 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4009 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4021 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4016 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4014 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4029 - accuracy: 0.12 - ETA: 2:08 - loss: 3.4026 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4030 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4029 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4030 - accuracy: 0.12 - ETA: 2:05 - loss: 3.4035 - accuracy: 0.12 - ETA: 2:04 - loss: 3.4041 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4041 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4036 - accuracy: 0.12 - ETA: 2:02 - loss: 3.4041 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4040 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4043 - accuracy: 0.12 - ETA: 2:00 - loss: 3.4047 - accuracy: 0.12 - ETA: 1:59 - loss: 3.4045 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4047 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4044 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4052 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4053 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4045 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4051 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4040 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4040 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4032 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4037 - accuracy: 0.12 - ETA: 1:51 - loss: 3.4045 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4044 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4038 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4041 - accuracy: 0.12 - ETA: 1:48 - loss: 3.4044 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4045 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4042 - accuracy: 0.1275"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:46 - loss: 3.4038 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4042 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4046 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4046 - accuracy: 0.12 - ETA: 1:43 - loss: 3.4054 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4062 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4059 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4061 - accuracy: 0.12 - ETA: 1:40 - loss: 3.4062 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4061 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4061 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4067 - accuracy: 0.12 - ETA: 1:37 - loss: 3.4062 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4063 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4058 - accuracy: 0.12 - ETA: 1:35 - loss: 3.4055 - accuracy: 0.12 - ETA: 1:34 - loss: 3.4054 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4047 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4048 - accuracy: 0.12 - ETA: 1:32 - loss: 3.4050 - accuracy: 0.12 - ETA: 1:31 - loss: 3.4047 - accuracy: 0.12 - ETA: 1:31 - loss: 3.4039 - accuracy: 0.12 - ETA: 1:30 - loss: 3.4035 - accuracy: 0.12 - ETA: 1:29 - loss: 3.4039 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4039 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4031 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4027 - accuracy: 0.12 - ETA: 1:26 - loss: 3.4026 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4025 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4024 - accuracy: 0.12 - ETA: 1:24 - loss: 3.4025 - accuracy: 0.12 - ETA: 1:23 - loss: 3.4028 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4032 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4035 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4033 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4030 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4030 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4028 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4037 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4038 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4045 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4047 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4048 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4045 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4046 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4045 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4040 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4040 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4040 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4043 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4043 - accuracy: 0.12 - ETA: 1:08 - loss: 3.4047 - accuracy: 0.12 - ETA: 1:08 - loss: 3.4044 - accuracy: 0.12 - ETA: 1:07 - loss: 3.4042 - accuracy: 0.12 - ETA: 1:06 - loss: 3.4041 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4039 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4045 - accuracy: 0.12 - ETA: 1:04 - loss: 3.4041 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4038 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4039 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4044 - accuracy: 0.12 - ETA: 1:01 - loss: 3.4042 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4047 - accuracy: 0.12 - ETA: 59s - loss: 3.4035 - accuracy: 0.1260 - ETA: 59s - loss: 3.4039 - accuracy: 0.126 - ETA: 58s - loss: 3.4037 - accuracy: 0.126 - ETA: 57s - loss: 3.4039 - accuracy: 0.126 - ETA: 56s - loss: 3.4044 - accuracy: 0.125 - ETA: 56s - loss: 3.4044 - accuracy: 0.125 - ETA: 55s - loss: 3.4044 - accuracy: 0.125 - ETA: 54s - loss: 3.4044 - accuracy: 0.126 - ETA: 54s - loss: 3.4048 - accuracy: 0.126 - ETA: 53s - loss: 3.4052 - accuracy: 0.126 - ETA: 52s - loss: 3.4052 - accuracy: 0.126 - ETA: 51s - loss: 3.4055 - accuracy: 0.126 - ETA: 51s - loss: 3.4052 - accuracy: 0.126 - ETA: 50s - loss: 3.4054 - accuracy: 0.126 - ETA: 49s - loss: 3.4046 - accuracy: 0.126 - ETA: 48s - loss: 3.4045 - accuracy: 0.126 - ETA: 48s - loss: 3.4048 - accuracy: 0.126 - ETA: 47s - loss: 3.4052 - accuracy: 0.125 - ETA: 46s - loss: 3.4054 - accuracy: 0.125 - ETA: 45s - loss: 3.4056 - accuracy: 0.125 - ETA: 45s - loss: 3.4057 - accuracy: 0.125 - ETA: 44s - loss: 3.4056 - accuracy: 0.125 - ETA: 43s - loss: 3.4050 - accuracy: 0.125 - ETA: 42s - loss: 3.4048 - accuracy: 0.125 - ETA: 42s - loss: 3.4048 - accuracy: 0.125 - ETA: 41s - loss: 3.4047 - accuracy: 0.125 - ETA: 40s - loss: 3.4053 - accuracy: 0.125 - ETA: 40s - loss: 3.4058 - accuracy: 0.125 - ETA: 39s - loss: 3.4064 - accuracy: 0.125 - ETA: 38s - loss: 3.4072 - accuracy: 0.125 - ETA: 37s - loss: 3.4074 - accuracy: 0.125 - ETA: 37s - loss: 3.4074 - accuracy: 0.125 - ETA: 36s - loss: 3.4075 - accuracy: 0.125 - ETA: 35s - loss: 3.4067 - accuracy: 0.125 - ETA: 34s - loss: 3.4068 - accuracy: 0.125 - ETA: 34s - loss: 3.4069 - accuracy: 0.125 - ETA: 33s - loss: 3.4074 - accuracy: 0.125 - ETA: 32s - loss: 3.4072 - accuracy: 0.124 - ETA: 31s - loss: 3.4073 - accuracy: 0.124 - ETA: 31s - loss: 3.4070 - accuracy: 0.125 - ETA: 30s - loss: 3.4067 - accuracy: 0.125 - ETA: 29s - loss: 3.4068 - accuracy: 0.125 - ETA: 28s - loss: 3.4073 - accuracy: 0.125 - ETA: 28s - loss: 3.4073 - accuracy: 0.125 - ETA: 27s - loss: 3.4074 - accuracy: 0.124 - ETA: 26s - loss: 3.4071 - accuracy: 0.124 - ETA: 25s - loss: 3.4079 - accuracy: 0.124 - ETA: 25s - loss: 3.4077 - accuracy: 0.124 - ETA: 24s - loss: 3.4081 - accuracy: 0.124 - ETA: 23s - loss: 3.4079 - accuracy: 0.124 - ETA: 23s - loss: 3.4073 - accuracy: 0.125 - ETA: 22s - loss: 3.4077 - accuracy: 0.124 - ETA: 21s - loss: 3.4076 - accuracy: 0.124 - ETA: 20s - loss: 3.4080 - accuracy: 0.124 - ETA: 20s - loss: 3.4084 - accuracy: 0.124 - ETA: 19s - loss: 3.4083 - accuracy: 0.124 - ETA: 18s - loss: 3.4081 - accuracy: 0.124 - ETA: 17s - loss: 3.4083 - accuracy: 0.124 - ETA: 17s - loss: 3.4083 - accuracy: 0.124 - ETA: 16s - loss: 3.4086 - accuracy: 0.124 - ETA: 15s - loss: 3.4086 - accuracy: 0.124 - ETA: 14s - loss: 3.4088 - accuracy: 0.124 - ETA: 14s - loss: 3.4086 - accuracy: 0.124 - ETA: 13s - loss: 3.4091 - accuracy: 0.124 - ETA: 12s - loss: 3.4093 - accuracy: 0.124 - ETA: 11s - loss: 3.4090 - accuracy: 0.124 - ETA: 11s - loss: 3.4088 - accuracy: 0.124 - ETA: 10s - loss: 3.4086 - accuracy: 0.124 - ETA: 9s - loss: 3.4085 - accuracy: 0.124 - ETA: 8s - loss: 3.4082 - accuracy: 0.12 - ETA: 8s - loss: 3.4088 - accuracy: 0.12 - ETA: 7s - loss: 3.4092 - accuracy: 0.12 - ETA: 6s - loss: 3.4094 - accuracy: 0.12 - ETA: 5s - loss: 3.4096 - accuracy: 0.12 - ETA: 5s - loss: 3.4098 - accuracy: 0.12 - ETA: 4s - loss: 3.4100 - accuracy: 0.12 - ETA: 3s - loss: 3.4101 - accuracy: 0.12 - ETA: 3s - loss: 3.4101 - accuracy: 0.12 - ETA: 2s - loss: 3.4104 - accuracy: 0.12 - ETA: 1s - loss: 3.4102 - accuracy: 0.12 - ETA: 0s - loss: 3.4103 - accuracy: 0.12 - ETA: 0s - loss: 3.4101 - accuracy: 0.12 - 263s 6ms/step - loss: 3.4100 - accuracy: 0.1240 - val_loss: 3.9514 - val_accuracy: 0.0354\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:05 - loss: 3.2774 - accuracy: 0.15 - ETA: 4:04 - loss: 3.3421 - accuracy: 0.13 - ETA: 4:00 - loss: 3.3545 - accuracy: 0.13 - ETA: 3:59 - loss: 3.3649 - accuracy: 0.14 - ETA: 3:59 - loss: 3.4071 - accuracy: 0.12 - ETA: 3:56 - loss: 3.4016 - accuracy: 0.12 - ETA: 3:56 - loss: 3.3614 - accuracy: 0.13 - ETA: 3:57 - loss: 3.3449 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3563 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3642 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3671 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3775 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3631 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3625 - accuracy: 0.13 - ETA: 3:53 - loss: 3.3611 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3613 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3574 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3687 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3712 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3801 - accuracy: 0.12 - ETA: 3:49 - loss: 3.3887 - accuracy: 0.12 - ETA: 3:48 - loss: 3.3866 - accuracy: 0.12 - ETA: 3:48 - loss: 3.3840 - accuracy: 0.12 - ETA: 3:47 - loss: 3.3856 - accuracy: 0.12 - ETA: 3:46 - loss: 3.3871 - accuracy: 0.12 - ETA: 3:46 - loss: 3.3873 - accuracy: 0.12 - ETA: 3:45 - loss: 3.3870 - accuracy: 0.12 - ETA: 3:44 - loss: 3.3911 - accuracy: 0.12 - ETA: 3:43 - loss: 3.3885 - accuracy: 0.12 - ETA: 3:42 - loss: 3.3939 - accuracy: 0.12 - ETA: 3:41 - loss: 3.3900 - accuracy: 0.12 - ETA: 3:40 - loss: 3.3885 - accuracy: 0.12 - ETA: 3:40 - loss: 3.3860 - accuracy: 0.12 - ETA: 3:39 - loss: 3.3831 - accuracy: 0.12 - ETA: 3:39 - loss: 3.3867 - accuracy: 0.12 - ETA: 3:38 - loss: 3.3807 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3833 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3831 - accuracy: 0.12 - ETA: 3:35 - loss: 3.3843 - accuracy: 0.12 - ETA: 3:35 - loss: 3.3858 - accuracy: 0.12 - ETA: 3:34 - loss: 3.3874 - accuracy: 0.12 - ETA: 3:34 - loss: 3.3861 - accuracy: 0.12 - ETA: 3:33 - loss: 3.3880 - accuracy: 0.12 - ETA: 3:32 - loss: 3.3885 - accuracy: 0.12 - ETA: 3:31 - loss: 3.3889 - accuracy: 0.12 - ETA: 3:31 - loss: 3.3886 - accuracy: 0.12 - ETA: 3:30 - loss: 3.3927 - accuracy: 0.12 - ETA: 3:29 - loss: 3.3899 - accuracy: 0.12 - ETA: 3:29 - loss: 3.3896 - accuracy: 0.12 - ETA: 3:28 - loss: 3.3920 - accuracy: 0.12 - ETA: 3:27 - loss: 3.3917 - accuracy: 0.12 - ETA: 3:26 - loss: 3.3945 - accuracy: 0.12 - ETA: 3:26 - loss: 3.3958 - accuracy: 0.12 - ETA: 3:25 - loss: 3.3961 - accuracy: 0.12 - ETA: 3:24 - loss: 3.3957 - accuracy: 0.12 - ETA: 3:23 - loss: 3.3962 - accuracy: 0.12 - ETA: 3:22 - loss: 3.3932 - accuracy: 0.12 - ETA: 3:22 - loss: 3.3966 - accuracy: 0.12 - ETA: 3:21 - loss: 3.3985 - accuracy: 0.12 - ETA: 3:20 - loss: 3.3997 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4012 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4026 - accuracy: 0.12 - ETA: 3:18 - loss: 3.4026 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4008 - accuracy: 0.12 - ETA: 3:16 - loss: 3.4006 - accuracy: 0.12 - ETA: 3:16 - loss: 3.4000 - accuracy: 0.12 - ETA: 3:15 - loss: 3.3984 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4000 - accuracy: 0.12 - ETA: 3:13 - loss: 3.3990 - accuracy: 0.12 - ETA: 3:12 - loss: 3.4006 - accuracy: 0.12 - ETA: 3:12 - loss: 3.4001 - accuracy: 0.12 - ETA: 3:11 - loss: 3.3998 - accuracy: 0.12 - ETA: 3:10 - loss: 3.4005 - accuracy: 0.12 - ETA: 3:10 - loss: 3.3994 - accuracy: 0.12 - ETA: 3:09 - loss: 3.4000 - accuracy: 0.12 - ETA: 3:08 - loss: 3.3991 - accuracy: 0.12 - ETA: 3:07 - loss: 3.3977 - accuracy: 0.12 - ETA: 3:06 - loss: 3.3995 - accuracy: 0.12 - ETA: 3:06 - loss: 3.4001 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4014 - accuracy: 0.12 - ETA: 3:04 - loss: 3.4035 - accuracy: 0.12 - ETA: 3:03 - loss: 3.4027 - accuracy: 0.12 - ETA: 3:03 - loss: 3.4030 - accuracy: 0.12 - ETA: 3:02 - loss: 3.4033 - accuracy: 0.12 - ETA: 3:02 - loss: 3.4046 - accuracy: 0.12 - ETA: 3:01 - loss: 3.4052 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4053 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4050 - accuracy: 0.12 - ETA: 2:59 - loss: 3.4039 - accuracy: 0.12 - ETA: 2:58 - loss: 3.4037 - accuracy: 0.12 - ETA: 2:58 - loss: 3.4037 - accuracy: 0.12 - ETA: 2:57 - loss: 3.4032 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4032 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4048 - accuracy: 0.12 - ETA: 2:55 - loss: 3.4066 - accuracy: 0.12 - ETA: 2:54 - loss: 3.4082 - accuracy: 0.12 - ETA: 2:53 - loss: 3.4095 - accuracy: 0.12 - ETA: 2:52 - loss: 3.4083 - accuracy: 0.12 - ETA: 2:52 - loss: 3.4095 - accuracy: 0.12 - ETA: 2:51 - loss: 3.4103 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4119 - accuracy: 0.12 - ETA: 2:49 - loss: 3.4116 - accuracy: 0.12 - ETA: 2:49 - loss: 3.4118 - accuracy: 0.12 - ETA: 2:48 - loss: 3.4105 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4109 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4100 - accuracy: 0.12 - ETA: 2:46 - loss: 3.4096 - accuracy: 0.12 - ETA: 2:45 - loss: 3.4102 - accuracy: 0.12 - ETA: 2:44 - loss: 3.4109 - accuracy: 0.12 - ETA: 2:44 - loss: 3.4106 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4103 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4102 - accuracy: 0.12 - ETA: 2:41 - loss: 3.4103 - accuracy: 0.12 - ETA: 2:41 - loss: 3.4101 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4090 - accuracy: 0.12 - ETA: 2:39 - loss: 3.4085 - accuracy: 0.12 - ETA: 2:38 - loss: 3.4065 - accuracy: 0.12 - ETA: 2:38 - loss: 3.4058 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4063 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4068 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4072 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4069 - accuracy: 0.12 - ETA: 2:34 - loss: 3.4063 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4067 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4061 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4061 - accuracy: 0.12 - ETA: 2:31 - loss: 3.4045 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4046 - accuracy: 0.12 - ETA: 2:29 - loss: 3.4041 - accuracy: 0.12 - ETA: 2:29 - loss: 3.4043 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4041 - accuracy: 0.12 - ETA: 2:27 - loss: 3.4037 - accuracy: 0.12 - ETA: 2:26 - loss: 3.4028 - accuracy: 0.12 - ETA: 2:26 - loss: 3.4023 - accuracy: 0.12 - ETA: 2:25 - loss: 3.4024 - accuracy: 0.12 - ETA: 2:24 - loss: 3.4032 - accuracy: 0.12 - ETA: 2:23 - loss: 3.4026 - accuracy: 0.12 - ETA: 2:23 - loss: 3.4042 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4039 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4039 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4034 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4031 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4029 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4020 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4030 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4034 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4028 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4029 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4021 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4017 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4017 - accuracy: 0.12 - ETA: 2:12 - loss: 3.3996 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4007 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3994 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4006 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4006 - accuracy: 0.12 - ETA: 2:09 - loss: 3.3999 - accuracy: 0.12 - ETA: 2:08 - loss: 3.3997 - accuracy: 0.12 - ETA: 2:07 - loss: 3.3984 - accuracy: 0.12 - ETA: 2:06 - loss: 3.3989 - accuracy: 0.12 - ETA: 2:06 - loss: 3.3991 - accuracy: 0.12 - ETA: 2:05 - loss: 3.3991 - accuracy: 0.12 - ETA: 2:04 - loss: 3.3995 - accuracy: 0.12 - ETA: 2:03 - loss: 3.3993 - accuracy: 0.12 - ETA: 2:03 - loss: 3.3985 - accuracy: 0.12 - ETA: 2:02 - loss: 3.3992 - accuracy: 0.12 - ETA: 2:01 - loss: 3.3988 - accuracy: 0.12 - ETA: 2:00 - loss: 3.3995 - accuracy: 0.12 - ETA: 2:00 - loss: 3.3987 - accuracy: 0.12 - ETA: 1:59 - loss: 3.3992 - accuracy: 0.12 - ETA: 1:58 - loss: 3.3980 - accuracy: 0.12 - ETA: 1:57 - loss: 3.3974 - accuracy: 0.12 - ETA: 1:57 - loss: 3.3972 - accuracy: 0.12 - ETA: 1:56 - loss: 3.3971 - accuracy: 0.12 - ETA: 1:55 - loss: 3.3971 - accuracy: 0.12 - ETA: 1:55 - loss: 3.3974 - accuracy: 0.12 - ETA: 1:54 - loss: 3.3966 - accuracy: 0.12 - ETA: 1:53 - loss: 3.3960 - accuracy: 0.12 - ETA: 1:52 - loss: 3.3963 - accuracy: 0.12 - ETA: 1:52 - loss: 3.3958 - accuracy: 0.12 - ETA: 1:51 - loss: 3.3964 - accuracy: 0.12 - ETA: 1:50 - loss: 3.3971 - accuracy: 0.12 - ETA: 1:49 - loss: 3.3968 - accuracy: 0.12 - ETA: 1:49 - loss: 3.3968 - accuracy: 0.12 - ETA: 1:48 - loss: 3.3981 - accuracy: 0.12 - ETA: 1:47 - loss: 3.3983 - accuracy: 0.1261"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:46 - loss: 3.3986 - accuracy: 0.12 - ETA: 1:46 - loss: 3.3988 - accuracy: 0.12 - ETA: 1:45 - loss: 3.3984 - accuracy: 0.12 - ETA: 1:44 - loss: 3.3993 - accuracy: 0.12 - ETA: 1:43 - loss: 3.3996 - accuracy: 0.12 - ETA: 1:43 - loss: 3.4008 - accuracy: 0.12 - ETA: 1:42 - loss: 3.3999 - accuracy: 0.12 - ETA: 1:41 - loss: 3.3993 - accuracy: 0.12 - ETA: 1:40 - loss: 3.3995 - accuracy: 0.12 - ETA: 1:40 - loss: 3.3999 - accuracy: 0.12 - ETA: 1:39 - loss: 3.3995 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4001 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4004 - accuracy: 0.12 - ETA: 1:37 - loss: 3.3994 - accuracy: 0.12 - ETA: 1:36 - loss: 3.3999 - accuracy: 0.12 - ETA: 1:35 - loss: 3.3992 - accuracy: 0.12 - ETA: 1:34 - loss: 3.4000 - accuracy: 0.12 - ETA: 1:34 - loss: 3.3996 - accuracy: 0.12 - ETA: 1:33 - loss: 3.3990 - accuracy: 0.12 - ETA: 1:32 - loss: 3.3995 - accuracy: 0.12 - ETA: 1:31 - loss: 3.3988 - accuracy: 0.12 - ETA: 1:31 - loss: 3.3995 - accuracy: 0.12 - ETA: 1:30 - loss: 3.3997 - accuracy: 0.12 - ETA: 1:29 - loss: 3.3997 - accuracy: 0.12 - ETA: 1:29 - loss: 3.4002 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4007 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4010 - accuracy: 0.12 - ETA: 1:26 - loss: 3.4009 - accuracy: 0.12 - ETA: 1:26 - loss: 3.4008 - accuracy: 0.12 - ETA: 1:25 - loss: 3.3999 - accuracy: 0.12 - ETA: 1:24 - loss: 3.3998 - accuracy: 0.12 - ETA: 1:23 - loss: 3.3998 - accuracy: 0.12 - ETA: 1:23 - loss: 3.3995 - accuracy: 0.12 - ETA: 1:22 - loss: 3.3999 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4007 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4005 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4006 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4005 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4005 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4011 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4012 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4011 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4018 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4015 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4021 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4020 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4020 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4016 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4024 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4025 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4024 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4028 - accuracy: 0.12 - ETA: 1:08 - loss: 3.4040 - accuracy: 0.12 - ETA: 1:07 - loss: 3.4040 - accuracy: 0.12 - ETA: 1:06 - loss: 3.4038 - accuracy: 0.12 - ETA: 1:06 - loss: 3.4058 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4062 - accuracy: 0.12 - ETA: 1:04 - loss: 3.4057 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4058 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4057 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4058 - accuracy: 0.12 - ETA: 1:01 - loss: 3.4053 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4052 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4049 - accuracy: 0.12 - ETA: 59s - loss: 3.4042 - accuracy: 0.1247 - ETA: 58s - loss: 3.4046 - accuracy: 0.124 - ETA: 57s - loss: 3.4043 - accuracy: 0.124 - ETA: 57s - loss: 3.4040 - accuracy: 0.124 - ETA: 56s - loss: 3.4040 - accuracy: 0.124 - ETA: 55s - loss: 3.4037 - accuracy: 0.124 - ETA: 54s - loss: 3.4038 - accuracy: 0.124 - ETA: 54s - loss: 3.4042 - accuracy: 0.124 - ETA: 53s - loss: 3.4046 - accuracy: 0.124 - ETA: 52s - loss: 3.4043 - accuracy: 0.124 - ETA: 51s - loss: 3.4042 - accuracy: 0.124 - ETA: 51s - loss: 3.4041 - accuracy: 0.124 - ETA: 50s - loss: 3.4041 - accuracy: 0.124 - ETA: 49s - loss: 3.4044 - accuracy: 0.124 - ETA: 49s - loss: 3.4047 - accuracy: 0.124 - ETA: 48s - loss: 3.4046 - accuracy: 0.124 - ETA: 47s - loss: 3.4048 - accuracy: 0.124 - ETA: 46s - loss: 3.4047 - accuracy: 0.124 - ETA: 46s - loss: 3.4045 - accuracy: 0.124 - ETA: 45s - loss: 3.4042 - accuracy: 0.124 - ETA: 44s - loss: 3.4040 - accuracy: 0.124 - ETA: 43s - loss: 3.4043 - accuracy: 0.124 - ETA: 43s - loss: 3.4046 - accuracy: 0.124 - ETA: 42s - loss: 3.4046 - accuracy: 0.124 - ETA: 41s - loss: 3.4046 - accuracy: 0.124 - ETA: 40s - loss: 3.4041 - accuracy: 0.124 - ETA: 40s - loss: 3.4040 - accuracy: 0.124 - ETA: 39s - loss: 3.4032 - accuracy: 0.124 - ETA: 38s - loss: 3.4034 - accuracy: 0.124 - ETA: 37s - loss: 3.4034 - accuracy: 0.124 - ETA: 37s - loss: 3.4038 - accuracy: 0.124 - ETA: 36s - loss: 3.4036 - accuracy: 0.124 - ETA: 35s - loss: 3.4038 - accuracy: 0.124 - ETA: 34s - loss: 3.4041 - accuracy: 0.124 - ETA: 34s - loss: 3.4037 - accuracy: 0.124 - ETA: 33s - loss: 3.4029 - accuracy: 0.124 - ETA: 32s - loss: 3.4027 - accuracy: 0.124 - ETA: 31s - loss: 3.4019 - accuracy: 0.125 - ETA: 31s - loss: 3.4019 - accuracy: 0.124 - ETA: 30s - loss: 3.4020 - accuracy: 0.124 - ETA: 29s - loss: 3.4019 - accuracy: 0.124 - ETA: 28s - loss: 3.4025 - accuracy: 0.124 - ETA: 28s - loss: 3.4020 - accuracy: 0.124 - ETA: 27s - loss: 3.4016 - accuracy: 0.124 - ETA: 26s - loss: 3.4019 - accuracy: 0.124 - ETA: 26s - loss: 3.4015 - accuracy: 0.124 - ETA: 25s - loss: 3.4016 - accuracy: 0.124 - ETA: 24s - loss: 3.4017 - accuracy: 0.124 - ETA: 23s - loss: 3.4014 - accuracy: 0.124 - ETA: 23s - loss: 3.4014 - accuracy: 0.124 - ETA: 22s - loss: 3.4015 - accuracy: 0.124 - ETA: 21s - loss: 3.4012 - accuracy: 0.124 - ETA: 20s - loss: 3.4006 - accuracy: 0.125 - ETA: 20s - loss: 3.4005 - accuracy: 0.125 - ETA: 19s - loss: 3.4004 - accuracy: 0.125 - ETA: 18s - loss: 3.4003 - accuracy: 0.125 - ETA: 17s - loss: 3.4003 - accuracy: 0.125 - ETA: 17s - loss: 3.4007 - accuracy: 0.124 - ETA: 16s - loss: 3.4010 - accuracy: 0.124 - ETA: 15s - loss: 3.4004 - accuracy: 0.124 - ETA: 14s - loss: 3.4006 - accuracy: 0.124 - ETA: 14s - loss: 3.3999 - accuracy: 0.125 - ETA: 13s - loss: 3.3996 - accuracy: 0.125 - ETA: 12s - loss: 3.3991 - accuracy: 0.125 - ETA: 11s - loss: 3.3989 - accuracy: 0.125 - ETA: 11s - loss: 3.3988 - accuracy: 0.125 - ETA: 10s - loss: 3.3991 - accuracy: 0.125 - ETA: 9s - loss: 3.3991 - accuracy: 0.125 - ETA: 8s - loss: 3.3996 - accuracy: 0.12 - ETA: 8s - loss: 3.3995 - accuracy: 0.12 - ETA: 7s - loss: 3.3996 - accuracy: 0.12 - ETA: 6s - loss: 3.3996 - accuracy: 0.12 - ETA: 5s - loss: 3.3990 - accuracy: 0.12 - ETA: 5s - loss: 3.3985 - accuracy: 0.12 - ETA: 4s - loss: 3.3982 - accuracy: 0.12 - ETA: 3s - loss: 3.3982 - accuracy: 0.12 - ETA: 3s - loss: 3.3982 - accuracy: 0.12 - ETA: 2s - loss: 3.3982 - accuracy: 0.12 - ETA: 1s - loss: 3.3978 - accuracy: 0.12 - ETA: 0s - loss: 3.3977 - accuracy: 0.12 - ETA: 0s - loss: 3.3975 - accuracy: 0.12 - 264s 6ms/step - loss: 3.3975 - accuracy: 0.1253 - val_loss: 4.0416 - val_accuracy: 0.0351\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:02 - loss: 3.5445 - accuracy: 0.08 - ETA: 4:01 - loss: 3.5176 - accuracy: 0.08 - ETA: 3:57 - loss: 3.4917 - accuracy: 0.11 - ETA: 3:56 - loss: 3.4293 - accuracy: 0.11 - ETA: 3:57 - loss: 3.4114 - accuracy: 0.12 - ETA: 3:56 - loss: 3.4055 - accuracy: 0.11 - ETA: 3:56 - loss: 3.3763 - accuracy: 0.12 - ETA: 3:58 - loss: 3.3720 - accuracy: 0.12 - ETA: 3:56 - loss: 3.3717 - accuracy: 0.12 - ETA: 3:57 - loss: 3.4011 - accuracy: 0.11 - ETA: 3:55 - loss: 3.4113 - accuracy: 0.11 - ETA: 3:54 - loss: 3.4164 - accuracy: 0.11 - ETA: 3:53 - loss: 3.4229 - accuracy: 0.11 - ETA: 3:53 - loss: 3.4170 - accuracy: 0.11 - ETA: 3:52 - loss: 3.4132 - accuracy: 0.11 - ETA: 3:51 - loss: 3.4138 - accuracy: 0.11 - ETA: 3:50 - loss: 3.4187 - accuracy: 0.11 - ETA: 3:50 - loss: 3.4180 - accuracy: 0.11 - ETA: 3:49 - loss: 3.4190 - accuracy: 0.11 - ETA: 3:48 - loss: 3.4172 - accuracy: 0.11 - ETA: 3:47 - loss: 3.4189 - accuracy: 0.11 - ETA: 3:46 - loss: 3.4054 - accuracy: 0.12 - ETA: 3:45 - loss: 3.4017 - accuracy: 0.12 - ETA: 3:45 - loss: 3.4010 - accuracy: 0.12 - ETA: 3:44 - loss: 3.4038 - accuracy: 0.12 - ETA: 3:44 - loss: 3.3980 - accuracy: 0.12 - ETA: 3:43 - loss: 3.3979 - accuracy: 0.12 - ETA: 3:42 - loss: 3.3977 - accuracy: 0.12 - ETA: 3:41 - loss: 3.3953 - accuracy: 0.12 - ETA: 3:41 - loss: 3.3949 - accuracy: 0.12 - ETA: 3:40 - loss: 3.3964 - accuracy: 0.12 - ETA: 3:39 - loss: 3.3980 - accuracy: 0.12 - ETA: 3:38 - loss: 3.3985 - accuracy: 0.12 - ETA: 3:38 - loss: 3.4009 - accuracy: 0.12 - ETA: 3:37 - loss: 3.4058 - accuracy: 0.12 - ETA: 3:36 - loss: 3.4050 - accuracy: 0.12 - ETA: 3:35 - loss: 3.3992 - accuracy: 0.12 - ETA: 3:35 - loss: 3.3969 - accuracy: 0.12 - ETA: 3:34 - loss: 3.3957 - accuracy: 0.12 - ETA: 3:34 - loss: 3.3991 - accuracy: 0.12 - ETA: 3:33 - loss: 3.3934 - accuracy: 0.12 - ETA: 3:32 - loss: 3.3956 - accuracy: 0.12 - ETA: 3:31 - loss: 3.3987 - accuracy: 0.12 - ETA: 3:31 - loss: 3.3986 - accuracy: 0.12 - ETA: 3:30 - loss: 3.3957 - accuracy: 0.12 - ETA: 3:29 - loss: 3.3982 - accuracy: 0.12 - ETA: 3:28 - loss: 3.3989 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4017 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4016 - accuracy: 0.12 - ETA: 3:26 - loss: 3.4050 - accuracy: 0.12 - ETA: 3:26 - loss: 3.4062 - accuracy: 0.12 - ETA: 3:25 - loss: 3.4079 - accuracy: 0.12 - ETA: 3:24 - loss: 3.4071 - accuracy: 0.12 - ETA: 3:23 - loss: 3.4073 - accuracy: 0.12 - ETA: 3:22 - loss: 3.4072 - accuracy: 0.12 - ETA: 3:22 - loss: 3.4084 - accuracy: 0.12 - ETA: 3:21 - loss: 3.4068 - accuracy: 0.12 - ETA: 3:20 - loss: 3.4042 - accuracy: 0.12 - ETA: 3:20 - loss: 3.4029 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4033 - accuracy: 0.12 - ETA: 3:18 - loss: 3.4040 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4037 - accuracy: 0.12 - ETA: 3:16 - loss: 3.4037 - accuracy: 0.12 - ETA: 3:16 - loss: 3.4042 - accuracy: 0.12 - ETA: 3:15 - loss: 3.4051 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4047 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4038 - accuracy: 0.12 - ETA: 3:13 - loss: 3.4033 - accuracy: 0.12 - ETA: 3:12 - loss: 3.4038 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4044 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4058 - accuracy: 0.12 - ETA: 3:10 - loss: 3.4044 - accuracy: 0.12 - ETA: 3:09 - loss: 3.4039 - accuracy: 0.12 - ETA: 3:08 - loss: 3.4037 - accuracy: 0.12 - ETA: 3:07 - loss: 3.4045 - accuracy: 0.12 - ETA: 3:07 - loss: 3.4048 - accuracy: 0.12 - ETA: 3:06 - loss: 3.4021 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4007 - accuracy: 0.12 - ETA: 3:04 - loss: 3.4006 - accuracy: 0.12 - ETA: 3:04 - loss: 3.4012 - accuracy: 0.12 - ETA: 3:03 - loss: 3.4015 - accuracy: 0.12 - ETA: 3:02 - loss: 3.4021 - accuracy: 0.12 - ETA: 3:02 - loss: 3.4036 - accuracy: 0.12 - ETA: 3:01 - loss: 3.4036 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4015 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4015 - accuracy: 0.12 - ETA: 2:59 - loss: 3.4016 - accuracy: 0.12 - ETA: 2:58 - loss: 3.3999 - accuracy: 0.12 - ETA: 2:58 - loss: 3.3996 - accuracy: 0.12 - ETA: 2:57 - loss: 3.4016 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4018 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4014 - accuracy: 0.12 - ETA: 2:55 - loss: 3.4020 - accuracy: 0.12 - ETA: 2:54 - loss: 3.4027 - accuracy: 0.12 - ETA: 2:53 - loss: 3.4024 - accuracy: 0.12 - ETA: 2:53 - loss: 3.4029 - accuracy: 0.12 - ETA: 2:52 - loss: 3.4033 - accuracy: 0.12 - ETA: 2:51 - loss: 3.4064 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4059 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4050 - accuracy: 0.12 - ETA: 2:49 - loss: 3.4052 - accuracy: 0.12 - ETA: 2:48 - loss: 3.4053 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4052 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4067 - accuracy: 0.12 - ETA: 2:46 - loss: 3.4053 - accuracy: 0.12 - ETA: 2:45 - loss: 3.4049 - accuracy: 0.12 - ETA: 2:44 - loss: 3.4067 - accuracy: 0.12 - ETA: 2:44 - loss: 3.4055 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4053 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4044 - accuracy: 0.12 - ETA: 2:41 - loss: 3.4041 - accuracy: 0.12 - ETA: 2:41 - loss: 3.4042 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4052 - accuracy: 0.12 - ETA: 2:39 - loss: 3.4062 - accuracy: 0.12 - ETA: 2:39 - loss: 3.4068 - accuracy: 0.12 - ETA: 2:38 - loss: 3.4072 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4070 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4055 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4070 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4074 - accuracy: 0.12 - ETA: 2:34 - loss: 3.4073 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4076 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4080 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4080 - accuracy: 0.12 - ETA: 2:31 - loss: 3.4089 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4096 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4090 - accuracy: 0.12 - ETA: 2:29 - loss: 3.4103 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4107 - accuracy: 0.12 - ETA: 2:27 - loss: 3.4098 - accuracy: 0.12 - ETA: 2:27 - loss: 3.4096 - accuracy: 0.12 - ETA: 2:26 - loss: 3.4106 - accuracy: 0.12 - ETA: 2:25 - loss: 3.4097 - accuracy: 0.12 - ETA: 2:24 - loss: 3.4095 - accuracy: 0.12 - ETA: 2:24 - loss: 3.4090 - accuracy: 0.12 - ETA: 2:23 - loss: 3.4077 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4070 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4063 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4056 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4053 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4059 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4067 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4069 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4070 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4073 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4071 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4070 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4081 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4082 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4076 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4072 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4069 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4064 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4059 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4050 - accuracy: 0.12 - ETA: 2:08 - loss: 3.4042 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4040 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4031 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4035 - accuracy: 0.12 - ETA: 2:05 - loss: 3.4030 - accuracy: 0.12 - ETA: 2:04 - loss: 3.4024 - accuracy: 0.12 - ETA: 2:04 - loss: 3.4024 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4023 - accuracy: 0.12 - ETA: 2:02 - loss: 3.4019 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4020 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4016 - accuracy: 0.12 - ETA: 2:00 - loss: 3.4018 - accuracy: 0.12 - ETA: 1:59 - loss: 3.4016 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4011 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4014 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4014 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4020 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4010 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4018 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4005 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4000 - accuracy: 0.12 - ETA: 1:53 - loss: 3.3999 - accuracy: 0.12 - ETA: 1:52 - loss: 3.3993 - accuracy: 0.12 - ETA: 1:51 - loss: 3.3992 - accuracy: 0.12 - ETA: 1:51 - loss: 3.3989 - accuracy: 0.12 - ETA: 1:50 - loss: 3.3986 - accuracy: 0.12 - ETA: 1:49 - loss: 3.3979 - accuracy: 0.12 - ETA: 1:48 - loss: 3.3976 - accuracy: 0.12 - ETA: 1:48 - loss: 3.3962 - accuracy: 0.12 - ETA: 1:47 - loss: 3.3960 - accuracy: 0.12 - ETA: 1:46 - loss: 3.3962 - accuracy: 0.1255"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3970 - accuracy: 0.12 - ETA: 1:45 - loss: 3.3961 - accuracy: 0.12 - ETA: 1:44 - loss: 3.3960 - accuracy: 0.12 - ETA: 1:43 - loss: 3.3964 - accuracy: 0.12 - ETA: 1:43 - loss: 3.3955 - accuracy: 0.12 - ETA: 1:42 - loss: 3.3964 - accuracy: 0.12 - ETA: 1:41 - loss: 3.3963 - accuracy: 0.12 - ETA: 1:40 - loss: 3.3954 - accuracy: 0.12 - ETA: 1:40 - loss: 3.3945 - accuracy: 0.12 - ETA: 1:39 - loss: 3.3940 - accuracy: 0.12 - ETA: 1:38 - loss: 3.3935 - accuracy: 0.12 - ETA: 1:37 - loss: 3.3938 - accuracy: 0.12 - ETA: 1:37 - loss: 3.3935 - accuracy: 0.12 - ETA: 1:36 - loss: 3.3934 - accuracy: 0.12 - ETA: 1:35 - loss: 3.3934 - accuracy: 0.12 - ETA: 1:34 - loss: 3.3926 - accuracy: 0.12 - ETA: 1:34 - loss: 3.3920 - accuracy: 0.12 - ETA: 1:33 - loss: 3.3917 - accuracy: 0.12 - ETA: 1:32 - loss: 3.3921 - accuracy: 0.12 - ETA: 1:32 - loss: 3.3916 - accuracy: 0.12 - ETA: 1:31 - loss: 3.3914 - accuracy: 0.12 - ETA: 1:30 - loss: 3.3914 - accuracy: 0.12 - ETA: 1:29 - loss: 3.3909 - accuracy: 0.12 - ETA: 1:29 - loss: 3.3912 - accuracy: 0.12 - ETA: 1:28 - loss: 3.3905 - accuracy: 0.12 - ETA: 1:27 - loss: 3.3905 - accuracy: 0.12 - ETA: 1:26 - loss: 3.3901 - accuracy: 0.12 - ETA: 1:26 - loss: 3.3903 - accuracy: 0.12 - ETA: 1:25 - loss: 3.3903 - accuracy: 0.12 - ETA: 1:24 - loss: 3.3906 - accuracy: 0.12 - ETA: 1:23 - loss: 3.3909 - accuracy: 0.12 - ETA: 1:23 - loss: 3.3912 - accuracy: 0.12 - ETA: 1:22 - loss: 3.3904 - accuracy: 0.12 - ETA: 1:21 - loss: 3.3902 - accuracy: 0.12 - ETA: 1:21 - loss: 3.3906 - accuracy: 0.12 - ETA: 1:20 - loss: 3.3906 - accuracy: 0.12 - ETA: 1:19 - loss: 3.3892 - accuracy: 0.12 - ETA: 1:18 - loss: 3.3897 - accuracy: 0.12 - ETA: 1:18 - loss: 3.3897 - accuracy: 0.12 - ETA: 1:17 - loss: 3.3898 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3899 - accuracy: 0.12 - ETA: 1:15 - loss: 3.3896 - accuracy: 0.12 - ETA: 1:15 - loss: 3.3896 - accuracy: 0.12 - ETA: 1:14 - loss: 3.3886 - accuracy: 0.12 - ETA: 1:13 - loss: 3.3888 - accuracy: 0.12 - ETA: 1:12 - loss: 3.3888 - accuracy: 0.12 - ETA: 1:12 - loss: 3.3882 - accuracy: 0.12 - ETA: 1:11 - loss: 3.3889 - accuracy: 0.12 - ETA: 1:10 - loss: 3.3884 - accuracy: 0.12 - ETA: 1:09 - loss: 3.3882 - accuracy: 0.12 - ETA: 1:09 - loss: 3.3876 - accuracy: 0.12 - ETA: 1:08 - loss: 3.3874 - accuracy: 0.12 - ETA: 1:07 - loss: 3.3874 - accuracy: 0.12 - ETA: 1:07 - loss: 3.3875 - accuracy: 0.12 - ETA: 1:06 - loss: 3.3871 - accuracy: 0.12 - ETA: 1:05 - loss: 3.3871 - accuracy: 0.12 - ETA: 1:04 - loss: 3.3869 - accuracy: 0.12 - ETA: 1:04 - loss: 3.3883 - accuracy: 0.12 - ETA: 1:03 - loss: 3.3884 - accuracy: 0.12 - ETA: 1:02 - loss: 3.3888 - accuracy: 0.12 - ETA: 1:01 - loss: 3.3886 - accuracy: 0.12 - ETA: 1:01 - loss: 3.3888 - accuracy: 0.12 - ETA: 1:00 - loss: 3.3883 - accuracy: 0.12 - ETA: 59s - loss: 3.3882 - accuracy: 0.1277 - ETA: 58s - loss: 3.3883 - accuracy: 0.127 - ETA: 58s - loss: 3.3883 - accuracy: 0.127 - ETA: 57s - loss: 3.3882 - accuracy: 0.127 - ETA: 56s - loss: 3.3888 - accuracy: 0.127 - ETA: 55s - loss: 3.3896 - accuracy: 0.127 - ETA: 55s - loss: 3.3898 - accuracy: 0.127 - ETA: 54s - loss: 3.3891 - accuracy: 0.127 - ETA: 53s - loss: 3.3892 - accuracy: 0.127 - ETA: 53s - loss: 3.3888 - accuracy: 0.127 - ETA: 52s - loss: 3.3890 - accuracy: 0.127 - ETA: 51s - loss: 3.3890 - accuracy: 0.127 - ETA: 50s - loss: 3.3887 - accuracy: 0.127 - ETA: 50s - loss: 3.3890 - accuracy: 0.127 - ETA: 49s - loss: 3.3888 - accuracy: 0.127 - ETA: 48s - loss: 3.3890 - accuracy: 0.127 - ETA: 47s - loss: 3.3894 - accuracy: 0.127 - ETA: 47s - loss: 3.3890 - accuracy: 0.127 - ETA: 46s - loss: 3.3891 - accuracy: 0.127 - ETA: 45s - loss: 3.3887 - accuracy: 0.127 - ETA: 45s - loss: 3.3890 - accuracy: 0.127 - ETA: 44s - loss: 3.3886 - accuracy: 0.127 - ETA: 43s - loss: 3.3883 - accuracy: 0.127 - ETA: 42s - loss: 3.3879 - accuracy: 0.127 - ETA: 42s - loss: 3.3882 - accuracy: 0.127 - ETA: 41s - loss: 3.3879 - accuracy: 0.127 - ETA: 40s - loss: 3.3877 - accuracy: 0.127 - ETA: 39s - loss: 3.3872 - accuracy: 0.127 - ETA: 39s - loss: 3.3871 - accuracy: 0.127 - ETA: 38s - loss: 3.3875 - accuracy: 0.127 - ETA: 37s - loss: 3.3872 - accuracy: 0.127 - ETA: 36s - loss: 3.3871 - accuracy: 0.127 - ETA: 36s - loss: 3.3874 - accuracy: 0.127 - ETA: 35s - loss: 3.3877 - accuracy: 0.127 - ETA: 34s - loss: 3.3870 - accuracy: 0.127 - ETA: 33s - loss: 3.3874 - accuracy: 0.127 - ETA: 33s - loss: 3.3866 - accuracy: 0.127 - ETA: 32s - loss: 3.3862 - accuracy: 0.127 - ETA: 31s - loss: 3.3862 - accuracy: 0.127 - ETA: 30s - loss: 3.3862 - accuracy: 0.127 - ETA: 30s - loss: 3.3861 - accuracy: 0.127 - ETA: 29s - loss: 3.3855 - accuracy: 0.127 - ETA: 28s - loss: 3.3852 - accuracy: 0.127 - ETA: 28s - loss: 3.3850 - accuracy: 0.127 - ETA: 27s - loss: 3.3856 - accuracy: 0.127 - ETA: 26s - loss: 3.3855 - accuracy: 0.127 - ETA: 25s - loss: 3.3854 - accuracy: 0.127 - ETA: 25s - loss: 3.3843 - accuracy: 0.127 - ETA: 24s - loss: 3.3840 - accuracy: 0.128 - ETA: 23s - loss: 3.3843 - accuracy: 0.127 - ETA: 22s - loss: 3.3846 - accuracy: 0.127 - ETA: 22s - loss: 3.3850 - accuracy: 0.127 - ETA: 21s - loss: 3.3850 - accuracy: 0.127 - ETA: 20s - loss: 3.3848 - accuracy: 0.127 - ETA: 19s - loss: 3.3853 - accuracy: 0.127 - ETA: 19s - loss: 3.3852 - accuracy: 0.127 - ETA: 18s - loss: 3.3855 - accuracy: 0.127 - ETA: 17s - loss: 3.3858 - accuracy: 0.127 - ETA: 17s - loss: 3.3859 - accuracy: 0.127 - ETA: 16s - loss: 3.3855 - accuracy: 0.127 - ETA: 15s - loss: 3.3855 - accuracy: 0.127 - ETA: 14s - loss: 3.3853 - accuracy: 0.127 - ETA: 14s - loss: 3.3856 - accuracy: 0.127 - ETA: 13s - loss: 3.3851 - accuracy: 0.127 - ETA: 12s - loss: 3.3846 - accuracy: 0.127 - ETA: 11s - loss: 3.3848 - accuracy: 0.127 - ETA: 11s - loss: 3.3854 - accuracy: 0.127 - ETA: 10s - loss: 3.3854 - accuracy: 0.127 - ETA: 9s - loss: 3.3857 - accuracy: 0.127 - ETA: 8s - loss: 3.3858 - accuracy: 0.12 - ETA: 8s - loss: 3.3859 - accuracy: 0.12 - ETA: 7s - loss: 3.3858 - accuracy: 0.12 - ETA: 6s - loss: 3.3860 - accuracy: 0.12 - ETA: 5s - loss: 3.3858 - accuracy: 0.12 - ETA: 5s - loss: 3.3854 - accuracy: 0.12 - ETA: 4s - loss: 3.3857 - accuracy: 0.12 - ETA: 3s - loss: 3.3863 - accuracy: 0.12 - ETA: 3s - loss: 3.3862 - accuracy: 0.12 - ETA: 2s - loss: 3.3865 - accuracy: 0.12 - ETA: 1s - loss: 3.3869 - accuracy: 0.12 - ETA: 0s - loss: 3.3872 - accuracy: 0.12 - ETA: 0s - loss: 3.3874 - accuracy: 0.12 - 262s 6ms/step - loss: 3.3873 - accuracy: 0.1270 - val_loss: 3.9912 - val_accuracy: 0.0356\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:03 - loss: 3.4085 - accuracy: 0.12 - ETA: 3:59 - loss: 3.3482 - accuracy: 0.12 - ETA: 3:56 - loss: 3.3855 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3925 - accuracy: 0.12 - ETA: 3:58 - loss: 3.3808 - accuracy: 0.12 - ETA: 3:57 - loss: 3.3813 - accuracy: 0.12 - ETA: 3:58 - loss: 3.4023 - accuracy: 0.12 - ETA: 3:56 - loss: 3.3935 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3885 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3862 - accuracy: 0.12 - ETA: 3:54 - loss: 3.3861 - accuracy: 0.12 - ETA: 3:52 - loss: 3.3867 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3842 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3939 - accuracy: 0.12 - ETA: 3:51 - loss: 3.3845 - accuracy: 0.12 - ETA: 3:51 - loss: 3.3811 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3884 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3831 - accuracy: 0.12 - ETA: 3:48 - loss: 3.3829 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3848 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3826 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3781 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3815 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3845 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3828 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3824 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3771 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3714 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3689 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3689 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3689 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3739 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3726 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3725 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3726 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3704 - accuracy: 0.13 - ETA: 3:33 - loss: 3.3696 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3651 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3649 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3612 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3608 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3623 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3593 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3624 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3599 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3585 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3606 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3594 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3591 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3616 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3628 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3634 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3662 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3683 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3683 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3662 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3680 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3686 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3668 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3654 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3645 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3631 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3637 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3639 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3642 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3673 - accuracy: 0.13 - ETA: 3:12 - loss: 3.3692 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3706 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3690 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3693 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3701 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3690 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3692 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3696 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3698 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3683 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3669 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3670 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3680 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3695 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3697 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3720 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3719 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3733 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3723 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3729 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3735 - accuracy: 0.12 - ETA: 2:57 - loss: 3.3759 - accuracy: 0.12 - ETA: 2:57 - loss: 3.3779 - accuracy: 0.12 - ETA: 2:56 - loss: 3.3801 - accuracy: 0.12 - ETA: 2:55 - loss: 3.3781 - accuracy: 0.12 - ETA: 2:54 - loss: 3.3769 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3775 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3775 - accuracy: 0.12 - ETA: 2:52 - loss: 3.3779 - accuracy: 0.12 - ETA: 2:52 - loss: 3.3783 - accuracy: 0.12 - ETA: 2:51 - loss: 3.3785 - accuracy: 0.12 - ETA: 2:50 - loss: 3.3783 - accuracy: 0.12 - ETA: 2:49 - loss: 3.3769 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3768 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3751 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3754 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3737 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3727 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3729 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3726 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3725 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3730 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3746 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3728 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3737 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3729 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3721 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3731 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3738 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3744 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3748 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3736 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3733 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3740 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3729 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3730 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3730 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3735 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3736 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3731 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3747 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3738 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3746 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3753 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3760 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3763 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3762 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3761 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3757 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3748 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3743 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3744 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3758 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3766 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3767 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3772 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3769 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3776 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3789 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3804 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3835 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3848 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3839 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3850 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3849 - accuracy: 0.12 - ETA: 2:10 - loss: 3.3848 - accuracy: 0.12 - ETA: 2:10 - loss: 3.3836 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3838 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3832 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3841 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3839 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3836 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3841 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3844 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3852 - accuracy: 0.12 - ETA: 2:03 - loss: 3.3853 - accuracy: 0.12 - ETA: 2:02 - loss: 3.3856 - accuracy: 0.12 - ETA: 2:02 - loss: 3.3858 - accuracy: 0.12 - ETA: 2:01 - loss: 3.3868 - accuracy: 0.12 - ETA: 2:00 - loss: 3.3868 - accuracy: 0.12 - ETA: 1:59 - loss: 3.3867 - accuracy: 0.12 - ETA: 1:59 - loss: 3.3867 - accuracy: 0.12 - ETA: 1:58 - loss: 3.3864 - accuracy: 0.12 - ETA: 1:57 - loss: 3.3862 - accuracy: 0.12 - ETA: 1:56 - loss: 3.3870 - accuracy: 0.12 - ETA: 1:56 - loss: 3.3871 - accuracy: 0.12 - ETA: 1:55 - loss: 3.3869 - accuracy: 0.12 - ETA: 1:54 - loss: 3.3872 - accuracy: 0.12 - ETA: 1:54 - loss: 3.3874 - accuracy: 0.12 - ETA: 1:53 - loss: 3.3872 - accuracy: 0.12 - ETA: 1:52 - loss: 3.3879 - accuracy: 0.12 - ETA: 1:52 - loss: 3.3874 - accuracy: 0.12 - ETA: 1:51 - loss: 3.3872 - accuracy: 0.12 - ETA: 1:50 - loss: 3.3868 - accuracy: 0.12 - ETA: 1:49 - loss: 3.3875 - accuracy: 0.12 - ETA: 1:49 - loss: 3.3873 - accuracy: 0.12 - ETA: 1:48 - loss: 3.3875 - accuracy: 0.12 - ETA: 1:47 - loss: 3.3862 - accuracy: 0.12 - ETA: 1:46 - loss: 3.3861 - accuracy: 0.12 - ETA: 1:46 - loss: 3.3868 - accuracy: 0.1296"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3866 - accuracy: 0.12 - ETA: 1:44 - loss: 3.3865 - accuracy: 0.12 - ETA: 1:44 - loss: 3.3866 - accuracy: 0.12 - ETA: 1:43 - loss: 3.3869 - accuracy: 0.12 - ETA: 1:42 - loss: 3.3869 - accuracy: 0.12 - ETA: 1:41 - loss: 3.3871 - accuracy: 0.12 - ETA: 1:41 - loss: 3.3866 - accuracy: 0.12 - ETA: 1:40 - loss: 3.3861 - accuracy: 0.12 - ETA: 1:39 - loss: 3.3865 - accuracy: 0.12 - ETA: 1:38 - loss: 3.3865 - accuracy: 0.12 - ETA: 1:38 - loss: 3.3869 - accuracy: 0.12 - ETA: 1:37 - loss: 3.3866 - accuracy: 0.12 - ETA: 1:36 - loss: 3.3874 - accuracy: 0.12 - ETA: 1:36 - loss: 3.3870 - accuracy: 0.12 - ETA: 1:35 - loss: 3.3865 - accuracy: 0.12 - ETA: 1:34 - loss: 3.3863 - accuracy: 0.12 - ETA: 1:33 - loss: 3.3865 - accuracy: 0.12 - ETA: 1:33 - loss: 3.3867 - accuracy: 0.12 - ETA: 1:32 - loss: 3.3860 - accuracy: 0.12 - ETA: 1:31 - loss: 3.3862 - accuracy: 0.12 - ETA: 1:30 - loss: 3.3855 - accuracy: 0.12 - ETA: 1:30 - loss: 3.3852 - accuracy: 0.12 - ETA: 1:29 - loss: 3.3848 - accuracy: 0.12 - ETA: 1:28 - loss: 3.3857 - accuracy: 0.12 - ETA: 1:27 - loss: 3.3857 - accuracy: 0.12 - ETA: 1:27 - loss: 3.3859 - accuracy: 0.12 - ETA: 1:26 - loss: 3.3870 - accuracy: 0.12 - ETA: 1:25 - loss: 3.3877 - accuracy: 0.12 - ETA: 1:24 - loss: 3.3870 - accuracy: 0.12 - ETA: 1:24 - loss: 3.3865 - accuracy: 0.12 - ETA: 1:23 - loss: 3.3874 - accuracy: 0.12 - ETA: 1:22 - loss: 3.3868 - accuracy: 0.12 - ETA: 1:22 - loss: 3.3861 - accuracy: 0.12 - ETA: 1:21 - loss: 3.3860 - accuracy: 0.12 - ETA: 1:20 - loss: 3.3858 - accuracy: 0.12 - ETA: 1:19 - loss: 3.3858 - accuracy: 0.12 - ETA: 1:19 - loss: 3.3853 - accuracy: 0.12 - ETA: 1:18 - loss: 3.3857 - accuracy: 0.12 - ETA: 1:17 - loss: 3.3857 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3851 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3863 - accuracy: 0.12 - ETA: 1:15 - loss: 3.3863 - accuracy: 0.12 - ETA: 1:14 - loss: 3.3863 - accuracy: 0.12 - ETA: 1:13 - loss: 3.3865 - accuracy: 0.12 - ETA: 1:13 - loss: 3.3871 - accuracy: 0.12 - ETA: 1:12 - loss: 3.3869 - accuracy: 0.12 - ETA: 1:11 - loss: 3.3863 - accuracy: 0.12 - ETA: 1:11 - loss: 3.3856 - accuracy: 0.12 - ETA: 1:10 - loss: 3.3858 - accuracy: 0.12 - ETA: 1:09 - loss: 3.3858 - accuracy: 0.12 - ETA: 1:08 - loss: 3.3856 - accuracy: 0.12 - ETA: 1:08 - loss: 3.3851 - accuracy: 0.12 - ETA: 1:07 - loss: 3.3849 - accuracy: 0.12 - ETA: 1:06 - loss: 3.3846 - accuracy: 0.12 - ETA: 1:05 - loss: 3.3848 - accuracy: 0.12 - ETA: 1:05 - loss: 3.3846 - accuracy: 0.12 - ETA: 1:04 - loss: 3.3847 - accuracy: 0.12 - ETA: 1:03 - loss: 3.3852 - accuracy: 0.12 - ETA: 1:03 - loss: 3.3848 - accuracy: 0.12 - ETA: 1:02 - loss: 3.3839 - accuracy: 0.12 - ETA: 1:01 - loss: 3.3839 - accuracy: 0.12 - ETA: 1:00 - loss: 3.3831 - accuracy: 0.12 - ETA: 1:00 - loss: 3.3830 - accuracy: 0.12 - ETA: 59s - loss: 3.3833 - accuracy: 0.1290 - ETA: 58s - loss: 3.3838 - accuracy: 0.128 - ETA: 57s - loss: 3.3840 - accuracy: 0.128 - ETA: 57s - loss: 3.3834 - accuracy: 0.129 - ETA: 56s - loss: 3.3833 - accuracy: 0.129 - ETA: 55s - loss: 3.3826 - accuracy: 0.129 - ETA: 54s - loss: 3.3824 - accuracy: 0.128 - ETA: 54s - loss: 3.3817 - accuracy: 0.129 - ETA: 53s - loss: 3.3811 - accuracy: 0.129 - ETA: 52s - loss: 3.3805 - accuracy: 0.129 - ETA: 52s - loss: 3.3802 - accuracy: 0.129 - ETA: 51s - loss: 3.3803 - accuracy: 0.129 - ETA: 50s - loss: 3.3794 - accuracy: 0.129 - ETA: 49s - loss: 3.3792 - accuracy: 0.129 - ETA: 49s - loss: 3.3791 - accuracy: 0.129 - ETA: 48s - loss: 3.3797 - accuracy: 0.129 - ETA: 47s - loss: 3.3798 - accuracy: 0.129 - ETA: 46s - loss: 3.3788 - accuracy: 0.129 - ETA: 46s - loss: 3.3784 - accuracy: 0.129 - ETA: 45s - loss: 3.3779 - accuracy: 0.129 - ETA: 44s - loss: 3.3782 - accuracy: 0.129 - ETA: 44s - loss: 3.3779 - accuracy: 0.129 - ETA: 43s - loss: 3.3766 - accuracy: 0.129 - ETA: 42s - loss: 3.3772 - accuracy: 0.129 - ETA: 41s - loss: 3.3766 - accuracy: 0.129 - ETA: 41s - loss: 3.3765 - accuracy: 0.129 - ETA: 40s - loss: 3.3765 - accuracy: 0.129 - ETA: 39s - loss: 3.3763 - accuracy: 0.129 - ETA: 38s - loss: 3.3762 - accuracy: 0.129 - ETA: 38s - loss: 3.3763 - accuracy: 0.129 - ETA: 37s - loss: 3.3759 - accuracy: 0.129 - ETA: 36s - loss: 3.3761 - accuracy: 0.129 - ETA: 35s - loss: 3.3759 - accuracy: 0.129 - ETA: 35s - loss: 3.3759 - accuracy: 0.129 - ETA: 34s - loss: 3.3765 - accuracy: 0.129 - ETA: 33s - loss: 3.3759 - accuracy: 0.129 - ETA: 33s - loss: 3.3762 - accuracy: 0.129 - ETA: 32s - loss: 3.3762 - accuracy: 0.129 - ETA: 31s - loss: 3.3764 - accuracy: 0.129 - ETA: 30s - loss: 3.3756 - accuracy: 0.129 - ETA: 30s - loss: 3.3757 - accuracy: 0.129 - ETA: 29s - loss: 3.3760 - accuracy: 0.129 - ETA: 28s - loss: 3.3759 - accuracy: 0.129 - ETA: 27s - loss: 3.3763 - accuracy: 0.129 - ETA: 27s - loss: 3.3761 - accuracy: 0.129 - ETA: 26s - loss: 3.3763 - accuracy: 0.129 - ETA: 25s - loss: 3.3762 - accuracy: 0.129 - ETA: 24s - loss: 3.3766 - accuracy: 0.129 - ETA: 24s - loss: 3.3765 - accuracy: 0.129 - ETA: 23s - loss: 3.3767 - accuracy: 0.129 - ETA: 22s - loss: 3.3767 - accuracy: 0.128 - ETA: 22s - loss: 3.3767 - accuracy: 0.128 - ETA: 21s - loss: 3.3763 - accuracy: 0.128 - ETA: 20s - loss: 3.3762 - accuracy: 0.128 - ETA: 19s - loss: 3.3760 - accuracy: 0.128 - ETA: 19s - loss: 3.3755 - accuracy: 0.128 - ETA: 18s - loss: 3.3755 - accuracy: 0.128 - ETA: 17s - loss: 3.3752 - accuracy: 0.128 - ETA: 16s - loss: 3.3751 - accuracy: 0.128 - ETA: 16s - loss: 3.3750 - accuracy: 0.128 - ETA: 15s - loss: 3.3752 - accuracy: 0.128 - ETA: 14s - loss: 3.3751 - accuracy: 0.128 - ETA: 13s - loss: 3.3754 - accuracy: 0.128 - ETA: 13s - loss: 3.3748 - accuracy: 0.128 - ETA: 12s - loss: 3.3745 - accuracy: 0.129 - ETA: 11s - loss: 3.3742 - accuracy: 0.129 - ETA: 11s - loss: 3.3737 - accuracy: 0.129 - ETA: 10s - loss: 3.3739 - accuracy: 0.129 - ETA: 9s - loss: 3.3736 - accuracy: 0.129 - ETA: 8s - loss: 3.3735 - accuracy: 0.12 - ETA: 8s - loss: 3.3728 - accuracy: 0.12 - ETA: 7s - loss: 3.3726 - accuracy: 0.12 - ETA: 6s - loss: 3.3723 - accuracy: 0.12 - ETA: 5s - loss: 3.3724 - accuracy: 0.12 - ETA: 5s - loss: 3.3728 - accuracy: 0.12 - ETA: 4s - loss: 3.3733 - accuracy: 0.12 - ETA: 3s - loss: 3.3733 - accuracy: 0.12 - ETA: 2s - loss: 3.3735 - accuracy: 0.12 - ETA: 2s - loss: 3.3737 - accuracy: 0.12 - ETA: 1s - loss: 3.3734 - accuracy: 0.12 - ETA: 0s - loss: 3.3736 - accuracy: 0.12 - ETA: 0s - loss: 3.3733 - accuracy: 0.12 - 261s 6ms/step - loss: 3.3733 - accuracy: 0.1290 - val_loss: 3.9622 - val_accuracy: 0.0290\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:04 - loss: 3.4141 - accuracy: 0.09 - ETA: 4:07 - loss: 3.4435 - accuracy: 0.11 - ETA: 4:04 - loss: 3.5835 - accuracy: 0.09 - ETA: 4:02 - loss: 3.5184 - accuracy: 0.11 - ETA: 4:02 - loss: 3.4767 - accuracy: 0.12 - ETA: 4:00 - loss: 3.4440 - accuracy: 0.12 - ETA: 4:00 - loss: 3.4473 - accuracy: 0.13 - ETA: 3:58 - loss: 3.4516 - accuracy: 0.12 - ETA: 3:57 - loss: 3.4619 - accuracy: 0.12 - ETA: 3:56 - loss: 3.4583 - accuracy: 0.12 - ETA: 3:56 - loss: 3.4647 - accuracy: 0.11 - ETA: 3:56 - loss: 3.4685 - accuracy: 0.11 - ETA: 3:56 - loss: 3.4619 - accuracy: 0.11 - ETA: 3:56 - loss: 3.4513 - accuracy: 0.12 - ETA: 3:54 - loss: 3.4466 - accuracy: 0.12 - ETA: 3:53 - loss: 3.4579 - accuracy: 0.11 - ETA: 3:53 - loss: 3.4663 - accuracy: 0.11 - ETA: 3:53 - loss: 3.4665 - accuracy: 0.11 - ETA: 3:52 - loss: 3.4591 - accuracy: 0.11 - ETA: 3:51 - loss: 3.4604 - accuracy: 0.11 - ETA: 3:50 - loss: 3.4646 - accuracy: 0.11 - ETA: 3:49 - loss: 3.4695 - accuracy: 0.11 - ETA: 3:49 - loss: 3.4740 - accuracy: 0.11 - ETA: 3:48 - loss: 3.4717 - accuracy: 0.11 - ETA: 3:47 - loss: 3.4702 - accuracy: 0.11 - ETA: 3:46 - loss: 3.4637 - accuracy: 0.11 - ETA: 3:45 - loss: 3.4662 - accuracy: 0.11 - ETA: 3:45 - loss: 3.4650 - accuracy: 0.11 - ETA: 3:43 - loss: 3.4682 - accuracy: 0.11 - ETA: 3:42 - loss: 3.4665 - accuracy: 0.11 - ETA: 3:42 - loss: 3.4633 - accuracy: 0.11 - ETA: 3:42 - loss: 3.4605 - accuracy: 0.11 - ETA: 3:41 - loss: 3.4542 - accuracy: 0.12 - ETA: 3:40 - loss: 3.4538 - accuracy: 0.12 - ETA: 3:39 - loss: 3.4505 - accuracy: 0.12 - ETA: 3:38 - loss: 3.4492 - accuracy: 0.12 - ETA: 3:37 - loss: 3.4476 - accuracy: 0.12 - ETA: 3:37 - loss: 3.4463 - accuracy: 0.12 - ETA: 3:36 - loss: 3.4445 - accuracy: 0.12 - ETA: 3:35 - loss: 3.4472 - accuracy: 0.12 - ETA: 3:34 - loss: 3.4445 - accuracy: 0.12 - ETA: 3:33 - loss: 3.4442 - accuracy: 0.12 - ETA: 3:32 - loss: 3.4442 - accuracy: 0.12 - ETA: 3:32 - loss: 3.4440 - accuracy: 0.12 - ETA: 3:31 - loss: 3.4415 - accuracy: 0.12 - ETA: 3:30 - loss: 3.4425 - accuracy: 0.12 - ETA: 3:29 - loss: 3.4412 - accuracy: 0.12 - ETA: 3:28 - loss: 3.4385 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4378 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4361 - accuracy: 0.12 - ETA: 3:26 - loss: 3.4326 - accuracy: 0.12 - ETA: 3:25 - loss: 3.4308 - accuracy: 0.12 - ETA: 3:24 - loss: 3.4272 - accuracy: 0.12 - ETA: 3:24 - loss: 3.4276 - accuracy: 0.12 - ETA: 3:23 - loss: 3.4245 - accuracy: 0.12 - ETA: 3:22 - loss: 3.4256 - accuracy: 0.12 - ETA: 3:21 - loss: 3.4247 - accuracy: 0.12 - ETA: 3:21 - loss: 3.4210 - accuracy: 0.12 - ETA: 3:20 - loss: 3.4200 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4180 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4188 - accuracy: 0.12 - ETA: 3:18 - loss: 3.4157 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4138 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4167 - accuracy: 0.12 - ETA: 3:16 - loss: 3.4189 - accuracy: 0.12 - ETA: 3:15 - loss: 3.4199 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4210 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4226 - accuracy: 0.12 - ETA: 3:13 - loss: 3.4224 - accuracy: 0.12 - ETA: 3:12 - loss: 3.4230 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4230 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4222 - accuracy: 0.12 - ETA: 3:10 - loss: 3.4222 - accuracy: 0.12 - ETA: 3:09 - loss: 3.4206 - accuracy: 0.12 - ETA: 3:08 - loss: 3.4198 - accuracy: 0.12 - ETA: 3:08 - loss: 3.4197 - accuracy: 0.12 - ETA: 3:07 - loss: 3.4196 - accuracy: 0.12 - ETA: 3:06 - loss: 3.4198 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4173 - accuracy: 0.12 - ETA: 3:04 - loss: 3.4179 - accuracy: 0.12 - ETA: 3:04 - loss: 3.4164 - accuracy: 0.12 - ETA: 3:03 - loss: 3.4143 - accuracy: 0.12 - ETA: 3:02 - loss: 3.4145 - accuracy: 0.12 - ETA: 3:01 - loss: 3.4132 - accuracy: 0.12 - ETA: 3:01 - loss: 3.4111 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4114 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4122 - accuracy: 0.12 - ETA: 2:59 - loss: 3.4119 - accuracy: 0.12 - ETA: 2:58 - loss: 3.4096 - accuracy: 0.12 - ETA: 2:58 - loss: 3.4105 - accuracy: 0.12 - ETA: 2:57 - loss: 3.4098 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4108 - accuracy: 0.12 - ETA: 2:55 - loss: 3.4103 - accuracy: 0.12 - ETA: 2:55 - loss: 3.4086 - accuracy: 0.12 - ETA: 2:54 - loss: 3.4084 - accuracy: 0.12 - ETA: 2:53 - loss: 3.4069 - accuracy: 0.12 - ETA: 2:52 - loss: 3.4076 - accuracy: 0.12 - ETA: 2:51 - loss: 3.4066 - accuracy: 0.12 - ETA: 2:51 - loss: 3.4060 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4054 - accuracy: 0.12 - ETA: 2:49 - loss: 3.4072 - accuracy: 0.12 - ETA: 2:48 - loss: 3.4067 - accuracy: 0.12 - ETA: 2:48 - loss: 3.4064 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4055 - accuracy: 0.12 - ETA: 2:46 - loss: 3.4050 - accuracy: 0.12 - ETA: 2:45 - loss: 3.4054 - accuracy: 0.12 - ETA: 2:45 - loss: 3.4066 - accuracy: 0.12 - ETA: 2:44 - loss: 3.4075 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4077 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4060 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4056 - accuracy: 0.12 - ETA: 2:41 - loss: 3.4056 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4049 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4055 - accuracy: 0.12 - ETA: 2:39 - loss: 3.4047 - accuracy: 0.12 - ETA: 2:38 - loss: 3.4052 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4057 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4065 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4059 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4062 - accuracy: 0.12 - ETA: 2:34 - loss: 3.4053 - accuracy: 0.12 - ETA: 2:34 - loss: 3.4051 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4044 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4042 - accuracy: 0.12 - ETA: 2:31 - loss: 3.4036 - accuracy: 0.12 - ETA: 2:31 - loss: 3.4018 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4010 - accuracy: 0.12 - ETA: 2:29 - loss: 3.4007 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4015 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4007 - accuracy: 0.12 - ETA: 2:27 - loss: 3.3996 - accuracy: 0.12 - ETA: 2:26 - loss: 3.3994 - accuracy: 0.12 - ETA: 2:26 - loss: 3.3987 - accuracy: 0.12 - ETA: 2:25 - loss: 3.3986 - accuracy: 0.12 - ETA: 2:24 - loss: 3.3972 - accuracy: 0.12 - ETA: 2:23 - loss: 3.3979 - accuracy: 0.12 - ETA: 2:23 - loss: 3.3980 - accuracy: 0.12 - ETA: 2:22 - loss: 3.3979 - accuracy: 0.12 - ETA: 2:21 - loss: 3.3981 - accuracy: 0.12 - ETA: 2:21 - loss: 3.3958 - accuracy: 0.12 - ETA: 2:20 - loss: 3.3948 - accuracy: 0.12 - ETA: 2:19 - loss: 3.3952 - accuracy: 0.12 - ETA: 2:18 - loss: 3.3954 - accuracy: 0.12 - ETA: 2:18 - loss: 3.3955 - accuracy: 0.12 - ETA: 2:17 - loss: 3.3973 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3974 - accuracy: 0.12 - ETA: 2:15 - loss: 3.3977 - accuracy: 0.12 - ETA: 2:15 - loss: 3.3971 - accuracy: 0.12 - ETA: 2:14 - loss: 3.3971 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3970 - accuracy: 0.12 - ETA: 2:12 - loss: 3.3985 - accuracy: 0.12 - ETA: 2:12 - loss: 3.3981 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3980 - accuracy: 0.12 - ETA: 2:10 - loss: 3.3985 - accuracy: 0.12 - ETA: 2:10 - loss: 3.3971 - accuracy: 0.12 - ETA: 2:09 - loss: 3.3951 - accuracy: 0.12 - ETA: 2:08 - loss: 3.3937 - accuracy: 0.12 - ETA: 2:07 - loss: 3.3929 - accuracy: 0.12 - ETA: 2:07 - loss: 3.3922 - accuracy: 0.12 - ETA: 2:06 - loss: 3.3923 - accuracy: 0.12 - ETA: 2:05 - loss: 3.3924 - accuracy: 0.12 - ETA: 2:04 - loss: 3.3932 - accuracy: 0.12 - ETA: 2:04 - loss: 3.3941 - accuracy: 0.12 - ETA: 2:03 - loss: 3.3946 - accuracy: 0.12 - ETA: 2:02 - loss: 3.3946 - accuracy: 0.12 - ETA: 2:01 - loss: 3.3955 - accuracy: 0.12 - ETA: 2:01 - loss: 3.3963 - accuracy: 0.12 - ETA: 2:00 - loss: 3.3967 - accuracy: 0.12 - ETA: 1:59 - loss: 3.3962 - accuracy: 0.12 - ETA: 1:58 - loss: 3.3966 - accuracy: 0.12 - ETA: 1:58 - loss: 3.3962 - accuracy: 0.12 - ETA: 1:57 - loss: 3.3961 - accuracy: 0.12 - ETA: 1:56 - loss: 3.3962 - accuracy: 0.12 - ETA: 1:55 - loss: 3.3962 - accuracy: 0.12 - ETA: 1:55 - loss: 3.3960 - accuracy: 0.12 - ETA: 1:54 - loss: 3.3964 - accuracy: 0.12 - ETA: 1:53 - loss: 3.3965 - accuracy: 0.12 - ETA: 1:53 - loss: 3.3961 - accuracy: 0.12 - ETA: 1:52 - loss: 3.3958 - accuracy: 0.12 - ETA: 1:51 - loss: 3.3965 - accuracy: 0.12 - ETA: 1:50 - loss: 3.3965 - accuracy: 0.12 - ETA: 1:50 - loss: 3.3960 - accuracy: 0.12 - ETA: 1:49 - loss: 3.3952 - accuracy: 0.12 - ETA: 1:48 - loss: 3.3951 - accuracy: 0.12 - ETA: 1:47 - loss: 3.3957 - accuracy: 0.12 - ETA: 1:47 - loss: 3.3971 - accuracy: 0.1264"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:46 - loss: 3.3970 - accuracy: 0.12 - ETA: 1:45 - loss: 3.3966 - accuracy: 0.12 - ETA: 1:44 - loss: 3.3967 - accuracy: 0.12 - ETA: 1:44 - loss: 3.3956 - accuracy: 0.12 - ETA: 1:43 - loss: 3.3958 - accuracy: 0.12 - ETA: 1:42 - loss: 3.3964 - accuracy: 0.12 - ETA: 1:42 - loss: 3.3957 - accuracy: 0.12 - ETA: 1:41 - loss: 3.3948 - accuracy: 0.12 - ETA: 1:40 - loss: 3.3943 - accuracy: 0.12 - ETA: 1:39 - loss: 3.3942 - accuracy: 0.12 - ETA: 1:39 - loss: 3.3951 - accuracy: 0.12 - ETA: 1:38 - loss: 3.3947 - accuracy: 0.12 - ETA: 1:37 - loss: 3.3938 - accuracy: 0.12 - ETA: 1:36 - loss: 3.3940 - accuracy: 0.12 - ETA: 1:36 - loss: 3.3942 - accuracy: 0.12 - ETA: 1:35 - loss: 3.3935 - accuracy: 0.12 - ETA: 1:34 - loss: 3.3933 - accuracy: 0.12 - ETA: 1:33 - loss: 3.3931 - accuracy: 0.12 - ETA: 1:33 - loss: 3.3930 - accuracy: 0.12 - ETA: 1:32 - loss: 3.3932 - accuracy: 0.12 - ETA: 1:31 - loss: 3.3938 - accuracy: 0.12 - ETA: 1:30 - loss: 3.3950 - accuracy: 0.12 - ETA: 1:30 - loss: 3.3939 - accuracy: 0.12 - ETA: 1:29 - loss: 3.3944 - accuracy: 0.12 - ETA: 1:28 - loss: 3.3937 - accuracy: 0.12 - ETA: 1:27 - loss: 3.3946 - accuracy: 0.12 - ETA: 1:27 - loss: 3.3944 - accuracy: 0.12 - ETA: 1:26 - loss: 3.3941 - accuracy: 0.12 - ETA: 1:25 - loss: 3.3936 - accuracy: 0.12 - ETA: 1:24 - loss: 3.3941 - accuracy: 0.12 - ETA: 1:24 - loss: 3.3936 - accuracy: 0.12 - ETA: 1:23 - loss: 3.3937 - accuracy: 0.12 - ETA: 1:22 - loss: 3.3941 - accuracy: 0.12 - ETA: 1:21 - loss: 3.3943 - accuracy: 0.12 - ETA: 1:21 - loss: 3.3941 - accuracy: 0.12 - ETA: 1:20 - loss: 3.3942 - accuracy: 0.12 - ETA: 1:19 - loss: 3.3939 - accuracy: 0.12 - ETA: 1:19 - loss: 3.3933 - accuracy: 0.12 - ETA: 1:18 - loss: 3.3929 - accuracy: 0.12 - ETA: 1:17 - loss: 3.3926 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3920 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3921 - accuracy: 0.12 - ETA: 1:15 - loss: 3.3916 - accuracy: 0.12 - ETA: 1:14 - loss: 3.3913 - accuracy: 0.12 - ETA: 1:13 - loss: 3.3909 - accuracy: 0.12 - ETA: 1:13 - loss: 3.3907 - accuracy: 0.12 - ETA: 1:12 - loss: 3.3900 - accuracy: 0.12 - ETA: 1:11 - loss: 3.3901 - accuracy: 0.12 - ETA: 1:10 - loss: 3.3894 - accuracy: 0.12 - ETA: 1:10 - loss: 3.3899 - accuracy: 0.12 - ETA: 1:09 - loss: 3.3897 - accuracy: 0.12 - ETA: 1:08 - loss: 3.3896 - accuracy: 0.12 - ETA: 1:07 - loss: 3.3893 - accuracy: 0.12 - ETA: 1:07 - loss: 3.3889 - accuracy: 0.12 - ETA: 1:06 - loss: 3.3875 - accuracy: 0.12 - ETA: 1:05 - loss: 3.3867 - accuracy: 0.12 - ETA: 1:04 - loss: 3.3864 - accuracy: 0.12 - ETA: 1:04 - loss: 3.3863 - accuracy: 0.12 - ETA: 1:03 - loss: 3.3865 - accuracy: 0.12 - ETA: 1:02 - loss: 3.3861 - accuracy: 0.12 - ETA: 1:02 - loss: 3.3860 - accuracy: 0.12 - ETA: 1:01 - loss: 3.3867 - accuracy: 0.12 - ETA: 1:00 - loss: 3.3867 - accuracy: 0.12 - ETA: 59s - loss: 3.3872 - accuracy: 0.1262 - ETA: 59s - loss: 3.3867 - accuracy: 0.126 - ETA: 58s - loss: 3.3864 - accuracy: 0.126 - ETA: 57s - loss: 3.3857 - accuracy: 0.126 - ETA: 56s - loss: 3.3857 - accuracy: 0.126 - ETA: 56s - loss: 3.3858 - accuracy: 0.126 - ETA: 55s - loss: 3.3854 - accuracy: 0.126 - ETA: 54s - loss: 3.3858 - accuracy: 0.126 - ETA: 53s - loss: 3.3860 - accuracy: 0.126 - ETA: 53s - loss: 3.3859 - accuracy: 0.126 - ETA: 52s - loss: 3.3849 - accuracy: 0.127 - ETA: 51s - loss: 3.3845 - accuracy: 0.127 - ETA: 50s - loss: 3.3838 - accuracy: 0.127 - ETA: 50s - loss: 3.3836 - accuracy: 0.127 - ETA: 49s - loss: 3.3830 - accuracy: 0.128 - ETA: 48s - loss: 3.3823 - accuracy: 0.128 - ETA: 48s - loss: 3.3823 - accuracy: 0.128 - ETA: 47s - loss: 3.3811 - accuracy: 0.128 - ETA: 46s - loss: 3.3811 - accuracy: 0.128 - ETA: 45s - loss: 3.3808 - accuracy: 0.128 - ETA: 45s - loss: 3.3805 - accuracy: 0.128 - ETA: 44s - loss: 3.3805 - accuracy: 0.128 - ETA: 43s - loss: 3.3806 - accuracy: 0.128 - ETA: 42s - loss: 3.3798 - accuracy: 0.128 - ETA: 42s - loss: 3.3803 - accuracy: 0.128 - ETA: 41s - loss: 3.3800 - accuracy: 0.128 - ETA: 40s - loss: 3.3802 - accuracy: 0.128 - ETA: 39s - loss: 3.3805 - accuracy: 0.128 - ETA: 39s - loss: 3.3800 - accuracy: 0.128 - ETA: 38s - loss: 3.3803 - accuracy: 0.128 - ETA: 37s - loss: 3.3802 - accuracy: 0.128 - ETA: 36s - loss: 3.3802 - accuracy: 0.128 - ETA: 36s - loss: 3.3799 - accuracy: 0.128 - ETA: 35s - loss: 3.3795 - accuracy: 0.128 - ETA: 34s - loss: 3.3789 - accuracy: 0.128 - ETA: 34s - loss: 3.3789 - accuracy: 0.128 - ETA: 33s - loss: 3.3788 - accuracy: 0.128 - ETA: 32s - loss: 3.3787 - accuracy: 0.128 - ETA: 31s - loss: 3.3786 - accuracy: 0.128 - ETA: 31s - loss: 3.3789 - accuracy: 0.128 - ETA: 30s - loss: 3.3794 - accuracy: 0.128 - ETA: 29s - loss: 3.3790 - accuracy: 0.128 - ETA: 28s - loss: 3.3791 - accuracy: 0.128 - ETA: 28s - loss: 3.3789 - accuracy: 0.128 - ETA: 27s - loss: 3.3796 - accuracy: 0.128 - ETA: 26s - loss: 3.3792 - accuracy: 0.128 - ETA: 25s - loss: 3.3789 - accuracy: 0.128 - ETA: 25s - loss: 3.3785 - accuracy: 0.128 - ETA: 24s - loss: 3.3787 - accuracy: 0.128 - ETA: 23s - loss: 3.3784 - accuracy: 0.128 - ETA: 22s - loss: 3.3786 - accuracy: 0.128 - ETA: 22s - loss: 3.3782 - accuracy: 0.128 - ETA: 21s - loss: 3.3787 - accuracy: 0.128 - ETA: 20s - loss: 3.3786 - accuracy: 0.128 - ETA: 19s - loss: 3.3789 - accuracy: 0.128 - ETA: 19s - loss: 3.3786 - accuracy: 0.128 - ETA: 18s - loss: 3.3789 - accuracy: 0.128 - ETA: 17s - loss: 3.3795 - accuracy: 0.127 - ETA: 16s - loss: 3.3800 - accuracy: 0.127 - ETA: 16s - loss: 3.3797 - accuracy: 0.128 - ETA: 15s - loss: 3.3801 - accuracy: 0.127 - ETA: 14s - loss: 3.3806 - accuracy: 0.127 - ETA: 13s - loss: 3.3810 - accuracy: 0.127 - ETA: 13s - loss: 3.3810 - accuracy: 0.127 - ETA: 12s - loss: 3.3809 - accuracy: 0.127 - ETA: 11s - loss: 3.3809 - accuracy: 0.127 - ETA: 11s - loss: 3.3810 - accuracy: 0.127 - ETA: 10s - loss: 3.3814 - accuracy: 0.127 - ETA: 9s - loss: 3.3811 - accuracy: 0.127 - ETA: 8s - loss: 3.3806 - accuracy: 0.12 - ETA: 8s - loss: 3.3805 - accuracy: 0.12 - ETA: 7s - loss: 3.3801 - accuracy: 0.12 - ETA: 6s - loss: 3.3804 - accuracy: 0.12 - ETA: 5s - loss: 3.3808 - accuracy: 0.12 - ETA: 5s - loss: 3.3809 - accuracy: 0.12 - ETA: 4s - loss: 3.3804 - accuracy: 0.12 - ETA: 3s - loss: 3.3801 - accuracy: 0.12 - ETA: 2s - loss: 3.3805 - accuracy: 0.12 - ETA: 2s - loss: 3.3807 - accuracy: 0.12 - ETA: 1s - loss: 3.3809 - accuracy: 0.12 - ETA: 0s - loss: 3.3815 - accuracy: 0.12 - ETA: 0s - loss: 3.3812 - accuracy: 0.12 - 257s 6ms/step - loss: 3.3812 - accuracy: 0.1274 - val_loss: 3.8906 - val_accuracy: 0.0275\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:04 - loss: 3.2962 - accuracy: 0.14 - ETA: 3:57 - loss: 3.3712 - accuracy: 0.11 - ETA: 3:58 - loss: 3.3970 - accuracy: 0.11 - ETA: 3:58 - loss: 3.4117 - accuracy: 0.11 - ETA: 3:58 - loss: 3.3891 - accuracy: 0.11 - ETA: 4:01 - loss: 3.3756 - accuracy: 0.13 - ETA: 4:01 - loss: 3.3757 - accuracy: 0.12 - ETA: 4:00 - loss: 3.3875 - accuracy: 0.12 - ETA: 4:02 - loss: 3.4089 - accuracy: 0.12 - ETA: 4:03 - loss: 3.4180 - accuracy: 0.12 - ETA: 4:02 - loss: 3.4239 - accuracy: 0.12 - ETA: 4:00 - loss: 3.4338 - accuracy: 0.12 - ETA: 3:59 - loss: 3.4408 - accuracy: 0.12 - ETA: 3:58 - loss: 3.4314 - accuracy: 0.12 - ETA: 3:57 - loss: 3.4293 - accuracy: 0.12 - ETA: 3:56 - loss: 3.4297 - accuracy: 0.12 - ETA: 3:55 - loss: 3.4343 - accuracy: 0.12 - ETA: 3:54 - loss: 3.4381 - accuracy: 0.12 - ETA: 3:54 - loss: 3.4417 - accuracy: 0.12 - ETA: 3:53 - loss: 3.4350 - accuracy: 0.12 - ETA: 3:51 - loss: 3.4299 - accuracy: 0.12 - ETA: 3:50 - loss: 3.4372 - accuracy: 0.12 - ETA: 3:49 - loss: 3.4376 - accuracy: 0.12 - ETA: 3:48 - loss: 3.4343 - accuracy: 0.12 - ETA: 3:47 - loss: 3.4310 - accuracy: 0.12 - ETA: 3:46 - loss: 3.4367 - accuracy: 0.12 - ETA: 3:45 - loss: 3.4406 - accuracy: 0.12 - ETA: 3:44 - loss: 3.4425 - accuracy: 0.12 - ETA: 3:43 - loss: 3.4392 - accuracy: 0.12 - ETA: 3:43 - loss: 3.4352 - accuracy: 0.12 - ETA: 3:42 - loss: 3.4373 - accuracy: 0.12 - ETA: 3:41 - loss: 3.4395 - accuracy: 0.12 - ETA: 3:40 - loss: 3.4459 - accuracy: 0.12 - ETA: 3:39 - loss: 3.4530 - accuracy: 0.12 - ETA: 3:38 - loss: 3.4538 - accuracy: 0.12 - ETA: 3:38 - loss: 3.4546 - accuracy: 0.12 - ETA: 3:37 - loss: 3.4568 - accuracy: 0.12 - ETA: 3:36 - loss: 3.4627 - accuracy: 0.11 - ETA: 3:36 - loss: 3.4636 - accuracy: 0.11 - ETA: 3:35 - loss: 3.4614 - accuracy: 0.12 - ETA: 3:34 - loss: 3.4598 - accuracy: 0.12 - ETA: 3:34 - loss: 3.4622 - accuracy: 0.12 - ETA: 3:33 - loss: 3.4599 - accuracy: 0.12 - ETA: 3:32 - loss: 3.4613 - accuracy: 0.12 - ETA: 3:32 - loss: 3.4637 - accuracy: 0.12 - ETA: 3:31 - loss: 3.4650 - accuracy: 0.12 - ETA: 3:30 - loss: 3.4633 - accuracy: 0.12 - ETA: 3:29 - loss: 3.4631 - accuracy: 0.12 - ETA: 3:28 - loss: 3.4629 - accuracy: 0.12 - ETA: 3:28 - loss: 3.4605 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4605 - accuracy: 0.12 - ETA: 3:26 - loss: 3.4613 - accuracy: 0.12 - ETA: 3:25 - loss: 3.4630 - accuracy: 0.12 - ETA: 3:25 - loss: 3.4637 - accuracy: 0.12 - ETA: 3:24 - loss: 3.4624 - accuracy: 0.12 - ETA: 3:23 - loss: 3.4607 - accuracy: 0.12 - ETA: 3:22 - loss: 3.4599 - accuracy: 0.12 - ETA: 3:21 - loss: 3.4581 - accuracy: 0.12 - ETA: 3:20 - loss: 3.4561 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4550 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4538 - accuracy: 0.12 - ETA: 3:18 - loss: 3.4523 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4504 - accuracy: 0.12 - ETA: 3:16 - loss: 3.4488 - accuracy: 0.12 - ETA: 3:15 - loss: 3.4509 - accuracy: 0.12 - ETA: 3:15 - loss: 3.4514 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4528 - accuracy: 0.12 - ETA: 3:13 - loss: 3.4530 - accuracy: 0.12 - ETA: 3:12 - loss: 3.4517 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4543 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4514 - accuracy: 0.12 - ETA: 3:10 - loss: 3.4504 - accuracy: 0.12 - ETA: 3:09 - loss: 3.4505 - accuracy: 0.12 - ETA: 3:08 - loss: 3.4487 - accuracy: 0.12 - ETA: 3:08 - loss: 3.4481 - accuracy: 0.12 - ETA: 3:07 - loss: 3.4467 - accuracy: 0.12 - ETA: 3:06 - loss: 3.4483 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4473 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4464 - accuracy: 0.12 - ETA: 3:04 - loss: 3.4462 - accuracy: 0.12 - ETA: 3:03 - loss: 3.4457 - accuracy: 0.12 - ETA: 3:02 - loss: 3.4438 - accuracy: 0.12 - ETA: 3:01 - loss: 3.4423 - accuracy: 0.12 - ETA: 3:01 - loss: 3.4431 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4400 - accuracy: 0.12 - ETA: 2:59 - loss: 3.4407 - accuracy: 0.12 - ETA: 2:59 - loss: 3.4410 - accuracy: 0.12 - ETA: 2:58 - loss: 3.4398 - accuracy: 0.12 - ETA: 2:57 - loss: 3.4396 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4394 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4387 - accuracy: 0.12 - ETA: 2:55 - loss: 3.4385 - accuracy: 0.12 - ETA: 2:54 - loss: 3.4355 - accuracy: 0.12 - ETA: 2:53 - loss: 3.4356 - accuracy: 0.12 - ETA: 2:53 - loss: 3.4342 - accuracy: 0.12 - ETA: 2:52 - loss: 3.4351 - accuracy: 0.12 - ETA: 2:52 - loss: 3.4362 - accuracy: 0.12 - ETA: 2:51 - loss: 3.4353 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4352 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4341 - accuracy: 0.12 - ETA: 2:49 - loss: 3.4341 - accuracy: 0.12 - ETA: 2:48 - loss: 3.4326 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4324 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4333 - accuracy: 0.12 - ETA: 2:46 - loss: 3.4334 - accuracy: 0.12 - ETA: 2:45 - loss: 3.4331 - accuracy: 0.12 - ETA: 2:44 - loss: 3.4337 - accuracy: 0.12 - ETA: 2:44 - loss: 3.4328 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4320 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4323 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4311 - accuracy: 0.12 - ETA: 2:41 - loss: 3.4320 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4305 - accuracy: 0.12 - ETA: 2:39 - loss: 3.4286 - accuracy: 0.12 - ETA: 2:39 - loss: 3.4274 - accuracy: 0.12 - ETA: 2:38 - loss: 3.4262 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4267 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4268 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4253 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4238 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4244 - accuracy: 0.12 - ETA: 2:34 - loss: 3.4247 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4253 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4249 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4233 - accuracy: 0.12 - ETA: 2:31 - loss: 3.4228 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4233 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4229 - accuracy: 0.12 - ETA: 2:29 - loss: 3.4220 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4219 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4204 - accuracy: 0.12 - ETA: 2:27 - loss: 3.4198 - accuracy: 0.12 - ETA: 2:26 - loss: 3.4199 - accuracy: 0.12 - ETA: 2:25 - loss: 3.4206 - accuracy: 0.12 - ETA: 2:25 - loss: 3.4206 - accuracy: 0.12 - ETA: 2:24 - loss: 3.4183 - accuracy: 0.12 - ETA: 2:23 - loss: 3.4163 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4165 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4175 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4171 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4168 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4167 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4160 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4158 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4168 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4166 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4165 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4169 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4165 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4157 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4154 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4145 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4140 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4144 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4137 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4141 - accuracy: 0.12 - ETA: 2:08 - loss: 3.4139 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4130 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4136 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4145 - accuracy: 0.12 - ETA: 2:05 - loss: 3.4142 - accuracy: 0.12 - ETA: 2:04 - loss: 3.4142 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4162 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4158 - accuracy: 0.12 - ETA: 2:02 - loss: 3.4148 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4147 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4146 - accuracy: 0.12 - ETA: 2:00 - loss: 3.4129 - accuracy: 0.12 - ETA: 1:59 - loss: 3.4124 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4118 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4115 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4117 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4108 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4118 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4119 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4104 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4101 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4105 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4095 - accuracy: 0.12 - ETA: 1:51 - loss: 3.4079 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4074 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4076 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4073 - accuracy: 0.12 - ETA: 1:48 - loss: 3.4072 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4077 - accuracy: 0.12 - ETA: 1:46 - loss: 3.4073 - accuracy: 0.1251"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:46 - loss: 3.4063 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4061 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4067 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4065 - accuracy: 0.12 - ETA: 1:43 - loss: 3.4066 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4067 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4068 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4074 - accuracy: 0.12 - ETA: 1:40 - loss: 3.4079 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4077 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4076 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4078 - accuracy: 0.12 - ETA: 1:37 - loss: 3.4083 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4086 - accuracy: 0.12 - ETA: 1:35 - loss: 3.4084 - accuracy: 0.12 - ETA: 1:35 - loss: 3.4091 - accuracy: 0.12 - ETA: 1:34 - loss: 3.4088 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4090 - accuracy: 0.12 - ETA: 1:32 - loss: 3.4085 - accuracy: 0.12 - ETA: 1:32 - loss: 3.4083 - accuracy: 0.12 - ETA: 1:31 - loss: 3.4082 - accuracy: 0.12 - ETA: 1:30 - loss: 3.4082 - accuracy: 0.12 - ETA: 1:29 - loss: 3.4084 - accuracy: 0.12 - ETA: 1:29 - loss: 3.4082 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4085 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4082 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4077 - accuracy: 0.12 - ETA: 1:26 - loss: 3.4071 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4068 - accuracy: 0.12 - ETA: 1:24 - loss: 3.4061 - accuracy: 0.12 - ETA: 1:24 - loss: 3.4059 - accuracy: 0.12 - ETA: 1:23 - loss: 3.4059 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4054 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4063 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4053 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4039 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4032 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4029 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4029 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4022 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4029 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4024 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4022 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4023 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4019 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4015 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4022 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4013 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4016 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4015 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4018 - accuracy: 0.12 - ETA: 1:08 - loss: 3.4025 - accuracy: 0.12 - ETA: 1:07 - loss: 3.4027 - accuracy: 0.12 - ETA: 1:07 - loss: 3.4034 - accuracy: 0.12 - ETA: 1:06 - loss: 3.4033 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4031 - accuracy: 0.12 - ETA: 1:04 - loss: 3.4029 - accuracy: 0.12 - ETA: 1:04 - loss: 3.4027 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4030 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4018 - accuracy: 0.12 - ETA: 1:01 - loss: 3.4015 - accuracy: 0.12 - ETA: 1:01 - loss: 3.4010 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4009 - accuracy: 0.12 - ETA: 59s - loss: 3.4001 - accuracy: 0.1265 - ETA: 58s - loss: 3.4000 - accuracy: 0.126 - ETA: 58s - loss: 3.3999 - accuracy: 0.126 - ETA: 57s - loss: 3.3998 - accuracy: 0.126 - ETA: 56s - loss: 3.3997 - accuracy: 0.126 - ETA: 56s - loss: 3.3996 - accuracy: 0.126 - ETA: 55s - loss: 3.3993 - accuracy: 0.126 - ETA: 54s - loss: 3.3992 - accuracy: 0.126 - ETA: 53s - loss: 3.3991 - accuracy: 0.127 - ETA: 53s - loss: 3.3996 - accuracy: 0.126 - ETA: 52s - loss: 3.4003 - accuracy: 0.126 - ETA: 51s - loss: 3.4004 - accuracy: 0.126 - ETA: 50s - loss: 3.4002 - accuracy: 0.126 - ETA: 50s - loss: 3.3999 - accuracy: 0.126 - ETA: 49s - loss: 3.3996 - accuracy: 0.126 - ETA: 48s - loss: 3.3995 - accuracy: 0.126 - ETA: 47s - loss: 3.3997 - accuracy: 0.126 - ETA: 47s - loss: 3.4000 - accuracy: 0.126 - ETA: 46s - loss: 3.4003 - accuracy: 0.126 - ETA: 45s - loss: 3.4003 - accuracy: 0.126 - ETA: 45s - loss: 3.4006 - accuracy: 0.126 - ETA: 44s - loss: 3.4004 - accuracy: 0.126 - ETA: 43s - loss: 3.4008 - accuracy: 0.126 - ETA: 42s - loss: 3.4015 - accuracy: 0.126 - ETA: 42s - loss: 3.4013 - accuracy: 0.126 - ETA: 41s - loss: 3.4010 - accuracy: 0.126 - ETA: 40s - loss: 3.4005 - accuracy: 0.126 - ETA: 39s - loss: 3.4007 - accuracy: 0.126 - ETA: 39s - loss: 3.4004 - accuracy: 0.126 - ETA: 38s - loss: 3.4005 - accuracy: 0.126 - ETA: 37s - loss: 3.4004 - accuracy: 0.126 - ETA: 36s - loss: 3.4003 - accuracy: 0.126 - ETA: 36s - loss: 3.4000 - accuracy: 0.126 - ETA: 35s - loss: 3.3996 - accuracy: 0.126 - ETA: 34s - loss: 3.3993 - accuracy: 0.126 - ETA: 33s - loss: 3.3997 - accuracy: 0.126 - ETA: 33s - loss: 3.3993 - accuracy: 0.126 - ETA: 32s - loss: 3.3985 - accuracy: 0.126 - ETA: 31s - loss: 3.3989 - accuracy: 0.126 - ETA: 31s - loss: 3.3990 - accuracy: 0.126 - ETA: 30s - loss: 3.3987 - accuracy: 0.126 - ETA: 29s - loss: 3.3989 - accuracy: 0.126 - ETA: 28s - loss: 3.3993 - accuracy: 0.126 - ETA: 28s - loss: 3.3991 - accuracy: 0.126 - ETA: 27s - loss: 3.3992 - accuracy: 0.126 - ETA: 26s - loss: 3.3993 - accuracy: 0.126 - ETA: 25s - loss: 3.3991 - accuracy: 0.126 - ETA: 25s - loss: 3.3991 - accuracy: 0.126 - ETA: 24s - loss: 3.3989 - accuracy: 0.126 - ETA: 23s - loss: 3.3988 - accuracy: 0.126 - ETA: 22s - loss: 3.3989 - accuracy: 0.126 - ETA: 22s - loss: 3.3990 - accuracy: 0.126 - ETA: 21s - loss: 3.3994 - accuracy: 0.126 - ETA: 20s - loss: 3.3995 - accuracy: 0.126 - ETA: 19s - loss: 3.3994 - accuracy: 0.126 - ETA: 19s - loss: 3.3991 - accuracy: 0.126 - ETA: 18s - loss: 3.3992 - accuracy: 0.126 - ETA: 17s - loss: 3.3993 - accuracy: 0.125 - ETA: 17s - loss: 3.3987 - accuracy: 0.126 - ETA: 16s - loss: 3.3983 - accuracy: 0.126 - ETA: 15s - loss: 3.3977 - accuracy: 0.126 - ETA: 14s - loss: 3.3972 - accuracy: 0.126 - ETA: 14s - loss: 3.3968 - accuracy: 0.126 - ETA: 13s - loss: 3.3964 - accuracy: 0.126 - ETA: 12s - loss: 3.3966 - accuracy: 0.126 - ETA: 11s - loss: 3.3961 - accuracy: 0.126 - ETA: 11s - loss: 3.3963 - accuracy: 0.126 - ETA: 10s - loss: 3.3962 - accuracy: 0.126 - ETA: 9s - loss: 3.3958 - accuracy: 0.126 - ETA: 8s - loss: 3.3953 - accuracy: 0.12 - ETA: 8s - loss: 3.3953 - accuracy: 0.12 - ETA: 7s - loss: 3.3953 - accuracy: 0.12 - ETA: 6s - loss: 3.3946 - accuracy: 0.12 - ETA: 5s - loss: 3.3941 - accuracy: 0.12 - ETA: 5s - loss: 3.3937 - accuracy: 0.12 - ETA: 4s - loss: 3.3934 - accuracy: 0.12 - ETA: 3s - loss: 3.3934 - accuracy: 0.12 - ETA: 3s - loss: 3.3932 - accuracy: 0.12 - ETA: 2s - loss: 3.3929 - accuracy: 0.12 - ETA: 1s - loss: 3.3925 - accuracy: 0.12 - ETA: 0s - loss: 3.3927 - accuracy: 0.12 - ETA: 0s - loss: 3.3920 - accuracy: 0.12 - 262s 6ms/step - loss: 3.3919 - accuracy: 0.1275 - val_loss: 3.9805 - val_accuracy: 0.0345\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:54 - loss: 3.3349 - accuracy: 0.10 - ETA: 3:55 - loss: 3.3484 - accuracy: 0.11 - ETA: 3:56 - loss: 3.3373 - accuracy: 0.11 - ETA: 3:56 - loss: 3.3531 - accuracy: 0.11 - ETA: 4:03 - loss: 3.3805 - accuracy: 0.10 - ETA: 4:05 - loss: 3.4155 - accuracy: 0.11 - ETA: 4:05 - loss: 3.4249 - accuracy: 0.11 - ETA: 4:05 - loss: 3.4265 - accuracy: 0.12 - ETA: 4:04 - loss: 3.4100 - accuracy: 0.12 - ETA: 4:05 - loss: 3.4059 - accuracy: 0.12 - ETA: 4:03 - loss: 3.4047 - accuracy: 0.12 - ETA: 4:01 - loss: 3.4129 - accuracy: 0.11 - ETA: 3:59 - loss: 3.4115 - accuracy: 0.11 - ETA: 3:58 - loss: 3.4076 - accuracy: 0.12 - ETA: 3:56 - loss: 3.3949 - accuracy: 0.12 - ETA: 3:56 - loss: 3.3962 - accuracy: 0.12 - ETA: 3:54 - loss: 3.4031 - accuracy: 0.12 - ETA: 3:53 - loss: 3.3960 - accuracy: 0.12 - ETA: 3:52 - loss: 3.3922 - accuracy: 0.12 - ETA: 3:50 - loss: 3.3900 - accuracy: 0.12 - ETA: 3:50 - loss: 3.3859 - accuracy: 0.12 - ETA: 3:49 - loss: 3.3810 - accuracy: 0.12 - ETA: 3:48 - loss: 3.3849 - accuracy: 0.12 - ETA: 3:47 - loss: 3.3777 - accuracy: 0.12 - ETA: 3:46 - loss: 3.3778 - accuracy: 0.12 - ETA: 3:45 - loss: 3.3774 - accuracy: 0.12 - ETA: 3:44 - loss: 3.3761 - accuracy: 0.12 - ETA: 3:43 - loss: 3.3708 - accuracy: 0.12 - ETA: 3:43 - loss: 3.3679 - accuracy: 0.12 - ETA: 3:42 - loss: 3.3650 - accuracy: 0.12 - ETA: 3:41 - loss: 3.3712 - accuracy: 0.12 - ETA: 3:40 - loss: 3.3708 - accuracy: 0.12 - ETA: 3:39 - loss: 3.3737 - accuracy: 0.12 - ETA: 3:39 - loss: 3.3726 - accuracy: 0.12 - ETA: 3:38 - loss: 3.3726 - accuracy: 0.12 - ETA: 3:37 - loss: 3.3683 - accuracy: 0.12 - ETA: 3:36 - loss: 3.3675 - accuracy: 0.12 - ETA: 3:35 - loss: 3.3665 - accuracy: 0.12 - ETA: 3:34 - loss: 3.3646 - accuracy: 0.12 - ETA: 3:33 - loss: 3.3691 - accuracy: 0.12 - ETA: 3:33 - loss: 3.3680 - accuracy: 0.12 - ETA: 3:32 - loss: 3.3697 - accuracy: 0.12 - ETA: 3:31 - loss: 3.3678 - accuracy: 0.12 - ETA: 3:31 - loss: 3.3664 - accuracy: 0.12 - ETA: 3:30 - loss: 3.3666 - accuracy: 0.12 - ETA: 3:29 - loss: 3.3647 - accuracy: 0.12 - ETA: 3:28 - loss: 3.3621 - accuracy: 0.12 - ETA: 3:28 - loss: 3.3610 - accuracy: 0.12 - ETA: 3:27 - loss: 3.3621 - accuracy: 0.12 - ETA: 3:26 - loss: 3.3618 - accuracy: 0.12 - ETA: 3:25 - loss: 3.3622 - accuracy: 0.12 - ETA: 3:25 - loss: 3.3637 - accuracy: 0.12 - ETA: 3:24 - loss: 3.3627 - accuracy: 0.12 - ETA: 3:23 - loss: 3.3622 - accuracy: 0.12 - ETA: 3:22 - loss: 3.3620 - accuracy: 0.12 - ETA: 3:21 - loss: 3.3613 - accuracy: 0.12 - ETA: 3:21 - loss: 3.3617 - accuracy: 0.12 - ETA: 3:20 - loss: 3.3633 - accuracy: 0.12 - ETA: 3:19 - loss: 3.3627 - accuracy: 0.12 - ETA: 3:18 - loss: 3.3664 - accuracy: 0.12 - ETA: 3:17 - loss: 3.3661 - accuracy: 0.12 - ETA: 3:17 - loss: 3.3678 - accuracy: 0.12 - ETA: 3:16 - loss: 3.3692 - accuracy: 0.12 - ETA: 3:15 - loss: 3.3709 - accuracy: 0.12 - ETA: 3:14 - loss: 3.3728 - accuracy: 0.12 - ETA: 3:14 - loss: 3.3731 - accuracy: 0.12 - ETA: 3:13 - loss: 3.3728 - accuracy: 0.12 - ETA: 3:12 - loss: 3.3756 - accuracy: 0.12 - ETA: 3:11 - loss: 3.3735 - accuracy: 0.12 - ETA: 3:11 - loss: 3.3721 - accuracy: 0.12 - ETA: 3:10 - loss: 3.3719 - accuracy: 0.12 - ETA: 3:10 - loss: 3.3693 - accuracy: 0.12 - ETA: 3:09 - loss: 3.3676 - accuracy: 0.12 - ETA: 3:08 - loss: 3.3679 - accuracy: 0.12 - ETA: 3:08 - loss: 3.3687 - accuracy: 0.12 - ETA: 3:07 - loss: 3.3703 - accuracy: 0.12 - ETA: 3:06 - loss: 3.3698 - accuracy: 0.12 - ETA: 3:06 - loss: 3.3707 - accuracy: 0.12 - ETA: 3:05 - loss: 3.3724 - accuracy: 0.12 - ETA: 3:04 - loss: 3.3731 - accuracy: 0.12 - ETA: 3:03 - loss: 3.3756 - accuracy: 0.12 - ETA: 3:03 - loss: 3.3737 - accuracy: 0.12 - ETA: 3:02 - loss: 3.3734 - accuracy: 0.12 - ETA: 3:01 - loss: 3.3724 - accuracy: 0.12 - ETA: 3:00 - loss: 3.3715 - accuracy: 0.12 - ETA: 3:00 - loss: 3.3725 - accuracy: 0.12 - ETA: 2:59 - loss: 3.3715 - accuracy: 0.12 - ETA: 2:58 - loss: 3.3720 - accuracy: 0.12 - ETA: 2:57 - loss: 3.3722 - accuracy: 0.12 - ETA: 2:57 - loss: 3.3720 - accuracy: 0.12 - ETA: 2:56 - loss: 3.3725 - accuracy: 0.12 - ETA: 2:55 - loss: 3.3725 - accuracy: 0.12 - ETA: 2:54 - loss: 3.3737 - accuracy: 0.12 - ETA: 2:54 - loss: 3.3748 - accuracy: 0.12 - ETA: 2:53 - loss: 3.3764 - accuracy: 0.12 - ETA: 2:52 - loss: 3.3756 - accuracy: 0.12 - ETA: 2:52 - loss: 3.3759 - accuracy: 0.12 - ETA: 2:51 - loss: 3.3750 - accuracy: 0.12 - ETA: 2:51 - loss: 3.3766 - accuracy: 0.12 - ETA: 2:50 - loss: 3.3782 - accuracy: 0.12 - ETA: 2:49 - loss: 3.3792 - accuracy: 0.12 - ETA: 2:48 - loss: 3.3794 - accuracy: 0.12 - ETA: 2:48 - loss: 3.3787 - accuracy: 0.12 - ETA: 2:47 - loss: 3.3801 - accuracy: 0.12 - ETA: 2:46 - loss: 3.3802 - accuracy: 0.12 - ETA: 2:46 - loss: 3.3781 - accuracy: 0.12 - ETA: 2:45 - loss: 3.3779 - accuracy: 0.12 - ETA: 2:44 - loss: 3.3790 - accuracy: 0.12 - ETA: 2:43 - loss: 3.3805 - accuracy: 0.12 - ETA: 2:42 - loss: 3.3802 - accuracy: 0.12 - ETA: 2:42 - loss: 3.3789 - accuracy: 0.12 - ETA: 2:41 - loss: 3.3780 - accuracy: 0.12 - ETA: 2:40 - loss: 3.3765 - accuracy: 0.12 - ETA: 2:39 - loss: 3.3753 - accuracy: 0.12 - ETA: 2:39 - loss: 3.3767 - accuracy: 0.12 - ETA: 2:38 - loss: 3.3761 - accuracy: 0.12 - ETA: 2:37 - loss: 3.3762 - accuracy: 0.12 - ETA: 2:36 - loss: 3.3773 - accuracy: 0.12 - ETA: 2:36 - loss: 3.3770 - accuracy: 0.12 - ETA: 2:35 - loss: 3.3766 - accuracy: 0.12 - ETA: 2:34 - loss: 3.3767 - accuracy: 0.12 - ETA: 2:33 - loss: 3.3772 - accuracy: 0.12 - ETA: 2:33 - loss: 3.3784 - accuracy: 0.12 - ETA: 2:32 - loss: 3.3792 - accuracy: 0.12 - ETA: 2:31 - loss: 3.3779 - accuracy: 0.12 - ETA: 2:30 - loss: 3.3795 - accuracy: 0.12 - ETA: 2:30 - loss: 3.3791 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3787 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3780 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3794 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3803 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3804 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3793 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3795 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3794 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3793 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3803 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3797 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3787 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3768 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3770 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3764 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3769 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3764 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3765 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3758 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3754 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3752 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3765 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3769 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3774 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3771 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3768 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3774 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3791 - accuracy: 0.12 - ETA: 2:08 - loss: 3.3794 - accuracy: 0.12 - ETA: 2:07 - loss: 3.3781 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3785 - accuracy: 0.12 - ETA: 2:06 - loss: 3.3779 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3779 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3773 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3783 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3780 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3777 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3779 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3760 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3759 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3755 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3749 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3744 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3739 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3737 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3749 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3739 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3742 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3742 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3744 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3746 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3732 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3736 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3738 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3741 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3750 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3756 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3758 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3754 - accuracy: 0.1301"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3744 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3745 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3735 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3722 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3719 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3719 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3719 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3722 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3717 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3716 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3717 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3722 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3714 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3707 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3709 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3712 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3709 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3707 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3705 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3707 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3695 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3692 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3692 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3699 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3685 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3686 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3685 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3674 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3672 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3666 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3661 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3663 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3663 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3663 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3663 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3665 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3667 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3665 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3666 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3669 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3677 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3678 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3679 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3680 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3684 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3686 - accuracy: 0.12 - ETA: 1:12 - loss: 3.3692 - accuracy: 0.12 - ETA: 1:11 - loss: 3.3685 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3680 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3680 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3675 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3672 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3671 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3663 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3664 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3664 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3666 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3659 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3662 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3664 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3665 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3667 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3664 - accuracy: 0.13 - ETA: 59s - loss: 3.3658 - accuracy: 0.1306 - ETA: 58s - loss: 3.3667 - accuracy: 0.130 - ETA: 58s - loss: 3.3669 - accuracy: 0.130 - ETA: 57s - loss: 3.3672 - accuracy: 0.130 - ETA: 56s - loss: 3.3666 - accuracy: 0.130 - ETA: 55s - loss: 3.3667 - accuracy: 0.130 - ETA: 55s - loss: 3.3669 - accuracy: 0.129 - ETA: 54s - loss: 3.3671 - accuracy: 0.130 - ETA: 53s - loss: 3.3664 - accuracy: 0.130 - ETA: 52s - loss: 3.3669 - accuracy: 0.130 - ETA: 52s - loss: 3.3670 - accuracy: 0.130 - ETA: 51s - loss: 3.3670 - accuracy: 0.130 - ETA: 50s - loss: 3.3671 - accuracy: 0.130 - ETA: 49s - loss: 3.3674 - accuracy: 0.130 - ETA: 49s - loss: 3.3670 - accuracy: 0.130 - ETA: 48s - loss: 3.3669 - accuracy: 0.130 - ETA: 47s - loss: 3.3666 - accuracy: 0.130 - ETA: 47s - loss: 3.3675 - accuracy: 0.129 - ETA: 46s - loss: 3.3675 - accuracy: 0.129 - ETA: 45s - loss: 3.3676 - accuracy: 0.129 - ETA: 44s - loss: 3.3675 - accuracy: 0.129 - ETA: 44s - loss: 3.3673 - accuracy: 0.129 - ETA: 43s - loss: 3.3670 - accuracy: 0.129 - ETA: 42s - loss: 3.3667 - accuracy: 0.130 - ETA: 41s - loss: 3.3673 - accuracy: 0.129 - ETA: 41s - loss: 3.3672 - accuracy: 0.129 - ETA: 40s - loss: 3.3670 - accuracy: 0.129 - ETA: 39s - loss: 3.3663 - accuracy: 0.129 - ETA: 38s - loss: 3.3663 - accuracy: 0.129 - ETA: 38s - loss: 3.3663 - accuracy: 0.129 - ETA: 37s - loss: 3.3659 - accuracy: 0.129 - ETA: 36s - loss: 3.3660 - accuracy: 0.129 - ETA: 36s - loss: 3.3666 - accuracy: 0.129 - ETA: 35s - loss: 3.3666 - accuracy: 0.129 - ETA: 34s - loss: 3.3668 - accuracy: 0.129 - ETA: 33s - loss: 3.3667 - accuracy: 0.129 - ETA: 33s - loss: 3.3666 - accuracy: 0.129 - ETA: 32s - loss: 3.3665 - accuracy: 0.129 - ETA: 31s - loss: 3.3670 - accuracy: 0.129 - ETA: 30s - loss: 3.3670 - accuracy: 0.129 - ETA: 30s - loss: 3.3666 - accuracy: 0.129 - ETA: 29s - loss: 3.3666 - accuracy: 0.129 - ETA: 28s - loss: 3.3667 - accuracy: 0.129 - ETA: 27s - loss: 3.3669 - accuracy: 0.129 - ETA: 27s - loss: 3.3672 - accuracy: 0.129 - ETA: 26s - loss: 3.3667 - accuracy: 0.130 - ETA: 25s - loss: 3.3673 - accuracy: 0.129 - ETA: 25s - loss: 3.3668 - accuracy: 0.130 - ETA: 24s - loss: 3.3669 - accuracy: 0.130 - ETA: 23s - loss: 3.3665 - accuracy: 0.130 - ETA: 22s - loss: 3.3665 - accuracy: 0.130 - ETA: 22s - loss: 3.3662 - accuracy: 0.130 - ETA: 21s - loss: 3.3661 - accuracy: 0.130 - ETA: 20s - loss: 3.3655 - accuracy: 0.130 - ETA: 19s - loss: 3.3651 - accuracy: 0.130 - ETA: 19s - loss: 3.3645 - accuracy: 0.130 - ETA: 18s - loss: 3.3646 - accuracy: 0.130 - ETA: 17s - loss: 3.3646 - accuracy: 0.130 - ETA: 16s - loss: 3.3645 - accuracy: 0.130 - ETA: 16s - loss: 3.3640 - accuracy: 0.130 - ETA: 15s - loss: 3.3638 - accuracy: 0.130 - ETA: 14s - loss: 3.3637 - accuracy: 0.130 - ETA: 14s - loss: 3.3638 - accuracy: 0.130 - ETA: 13s - loss: 3.3631 - accuracy: 0.130 - ETA: 12s - loss: 3.3630 - accuracy: 0.130 - ETA: 11s - loss: 3.3625 - accuracy: 0.130 - ETA: 11s - loss: 3.3631 - accuracy: 0.130 - ETA: 10s - loss: 3.3628 - accuracy: 0.130 - ETA: 9s - loss: 3.3631 - accuracy: 0.130 - ETA: 8s - loss: 3.3634 - accuracy: 0.13 - ETA: 8s - loss: 3.3632 - accuracy: 0.13 - ETA: 7s - loss: 3.3629 - accuracy: 0.13 - ETA: 6s - loss: 3.3633 - accuracy: 0.13 - ETA: 5s - loss: 3.3632 - accuracy: 0.13 - ETA: 5s - loss: 3.3634 - accuracy: 0.13 - ETA: 4s - loss: 3.3634 - accuracy: 0.13 - ETA: 3s - loss: 3.3636 - accuracy: 0.13 - ETA: 2s - loss: 3.3635 - accuracy: 0.13 - ETA: 2s - loss: 3.3630 - accuracy: 0.13 - ETA: 1s - loss: 3.3633 - accuracy: 0.13 - ETA: 0s - loss: 3.3631 - accuracy: 0.13 - ETA: 0s - loss: 3.3631 - accuracy: 0.13 - 261s 6ms/step - loss: 3.3630 - accuracy: 0.1305 - val_loss: 3.8933 - val_accuracy: 0.0283\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:08 - loss: 3.2263 - accuracy: 0.20 - ETA: 4:00 - loss: 3.3474 - accuracy: 0.16 - ETA: 4:01 - loss: 3.3611 - accuracy: 0.15 - ETA: 3:58 - loss: 3.3879 - accuracy: 0.14 - ETA: 3:58 - loss: 3.3785 - accuracy: 0.13 - ETA: 3:57 - loss: 3.3865 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3869 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3562 - accuracy: 0.13 - ETA: 3:58 - loss: 3.3552 - accuracy: 0.13 - ETA: 3:57 - loss: 3.3478 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3238 - accuracy: 0.13 - ETA: 3:57 - loss: 3.3293 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3334 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3247 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3320 - accuracy: 0.14 - ETA: 3:53 - loss: 3.3247 - accuracy: 0.14 - ETA: 3:54 - loss: 3.3319 - accuracy: 0.14 - ETA: 3:53 - loss: 3.3252 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3186 - accuracy: 0.14 - ETA: 3:51 - loss: 3.3188 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3264 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3324 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3407 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3434 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3425 - accuracy: 0.14 - ETA: 3:45 - loss: 3.3418 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3477 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3548 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3543 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3614 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3592 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3553 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3582 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3595 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3622 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3652 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3647 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3621 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3653 - accuracy: 0.13 - ETA: 3:33 - loss: 3.3646 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3626 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3610 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3589 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3582 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3608 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3618 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3623 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3655 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3680 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3684 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3674 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3700 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3699 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3707 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3715 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3682 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3704 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3714 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3732 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3734 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3739 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3757 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3741 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3746 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3755 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3764 - accuracy: 0.13 - ETA: 3:12 - loss: 3.3750 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3745 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3746 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3751 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3733 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3724 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3719 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3691 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3702 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3698 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3723 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3749 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3736 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3732 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3721 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3735 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3731 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3734 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3747 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3775 - accuracy: 0.12 - ETA: 2:58 - loss: 3.3788 - accuracy: 0.12 - ETA: 2:57 - loss: 3.3790 - accuracy: 0.12 - ETA: 2:57 - loss: 3.3797 - accuracy: 0.12 - ETA: 2:56 - loss: 3.3786 - accuracy: 0.12 - ETA: 2:55 - loss: 3.3763 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3754 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3757 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3762 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3763 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3756 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3750 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3748 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3730 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3724 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3720 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3716 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3715 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3697 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3694 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3700 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3712 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3704 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3707 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3689 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3683 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3671 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3667 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3675 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3658 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3654 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3651 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3651 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3646 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3646 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3647 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3643 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3649 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3652 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3655 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3656 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3669 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3667 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3674 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3679 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3661 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3660 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3666 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3680 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3684 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3683 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3676 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3683 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3697 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3697 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3716 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3735 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3732 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3724 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3731 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3727 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3736 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3731 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3747 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3756 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3753 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3754 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3757 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3759 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3759 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3751 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3746 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3745 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3744 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3743 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3749 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3744 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3754 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3755 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3753 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3762 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3765 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3763 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3759 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3764 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3762 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3752 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3753 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3753 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3753 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3755 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3759 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3760 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3757 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3749 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3748 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3741 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3740 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3737 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3739 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3725 - accuracy: 0.1313"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3732 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3728 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3738 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3740 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3728 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3733 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3734 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3737 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3732 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3728 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3734 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3736 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3740 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3749 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3748 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3746 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3749 - accuracy: 0.12 - ETA: 1:33 - loss: 3.3754 - accuracy: 0.12 - ETA: 1:32 - loss: 3.3756 - accuracy: 0.12 - ETA: 1:32 - loss: 3.3752 - accuracy: 0.12 - ETA: 1:31 - loss: 3.3749 - accuracy: 0.12 - ETA: 1:30 - loss: 3.3752 - accuracy: 0.12 - ETA: 1:29 - loss: 3.3751 - accuracy: 0.12 - ETA: 1:29 - loss: 3.3755 - accuracy: 0.12 - ETA: 1:28 - loss: 3.3754 - accuracy: 0.12 - ETA: 1:27 - loss: 3.3747 - accuracy: 0.12 - ETA: 1:27 - loss: 3.3746 - accuracy: 0.12 - ETA: 1:26 - loss: 3.3742 - accuracy: 0.12 - ETA: 1:25 - loss: 3.3745 - accuracy: 0.12 - ETA: 1:24 - loss: 3.3744 - accuracy: 0.12 - ETA: 1:24 - loss: 3.3754 - accuracy: 0.12 - ETA: 1:23 - loss: 3.3747 - accuracy: 0.12 - ETA: 1:22 - loss: 3.3745 - accuracy: 0.12 - ETA: 1:21 - loss: 3.3753 - accuracy: 0.12 - ETA: 1:21 - loss: 3.3768 - accuracy: 0.12 - ETA: 1:20 - loss: 3.3761 - accuracy: 0.12 - ETA: 1:19 - loss: 3.3759 - accuracy: 0.12 - ETA: 1:18 - loss: 3.3754 - accuracy: 0.12 - ETA: 1:18 - loss: 3.3753 - accuracy: 0.12 - ETA: 1:17 - loss: 3.3747 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3746 - accuracy: 0.12 - ETA: 1:15 - loss: 3.3742 - accuracy: 0.12 - ETA: 1:15 - loss: 3.3741 - accuracy: 0.12 - ETA: 1:14 - loss: 3.3747 - accuracy: 0.12 - ETA: 1:13 - loss: 3.3748 - accuracy: 0.12 - ETA: 1:12 - loss: 3.3750 - accuracy: 0.12 - ETA: 1:12 - loss: 3.3747 - accuracy: 0.12 - ETA: 1:11 - loss: 3.3740 - accuracy: 0.12 - ETA: 1:10 - loss: 3.3731 - accuracy: 0.12 - ETA: 1:10 - loss: 3.3732 - accuracy: 0.12 - ETA: 1:09 - loss: 3.3729 - accuracy: 0.12 - ETA: 1:08 - loss: 3.3735 - accuracy: 0.12 - ETA: 1:07 - loss: 3.3729 - accuracy: 0.12 - ETA: 1:07 - loss: 3.3734 - accuracy: 0.12 - ETA: 1:06 - loss: 3.3734 - accuracy: 0.12 - ETA: 1:05 - loss: 3.3732 - accuracy: 0.12 - ETA: 1:04 - loss: 3.3729 - accuracy: 0.12 - ETA: 1:04 - loss: 3.3737 - accuracy: 0.12 - ETA: 1:03 - loss: 3.3735 - accuracy: 0.12 - ETA: 1:02 - loss: 3.3728 - accuracy: 0.12 - ETA: 1:01 - loss: 3.3731 - accuracy: 0.12 - ETA: 1:01 - loss: 3.3720 - accuracy: 0.12 - ETA: 1:00 - loss: 3.3717 - accuracy: 0.12 - ETA: 59s - loss: 3.3711 - accuracy: 0.1299 - ETA: 59s - loss: 3.3711 - accuracy: 0.129 - ETA: 58s - loss: 3.3711 - accuracy: 0.129 - ETA: 57s - loss: 3.3715 - accuracy: 0.129 - ETA: 56s - loss: 3.3714 - accuracy: 0.129 - ETA: 56s - loss: 3.3716 - accuracy: 0.129 - ETA: 55s - loss: 3.3712 - accuracy: 0.129 - ETA: 54s - loss: 3.3710 - accuracy: 0.129 - ETA: 53s - loss: 3.3702 - accuracy: 0.129 - ETA: 53s - loss: 3.3703 - accuracy: 0.129 - ETA: 52s - loss: 3.3708 - accuracy: 0.129 - ETA: 51s - loss: 3.3710 - accuracy: 0.129 - ETA: 50s - loss: 3.3707 - accuracy: 0.129 - ETA: 50s - loss: 3.3707 - accuracy: 0.129 - ETA: 49s - loss: 3.3704 - accuracy: 0.129 - ETA: 48s - loss: 3.3699 - accuracy: 0.129 - ETA: 47s - loss: 3.3700 - accuracy: 0.129 - ETA: 47s - loss: 3.3700 - accuracy: 0.129 - ETA: 46s - loss: 3.3696 - accuracy: 0.129 - ETA: 45s - loss: 3.3693 - accuracy: 0.129 - ETA: 45s - loss: 3.3688 - accuracy: 0.129 - ETA: 44s - loss: 3.3676 - accuracy: 0.129 - ETA: 43s - loss: 3.3671 - accuracy: 0.129 - ETA: 42s - loss: 3.3665 - accuracy: 0.130 - ETA: 42s - loss: 3.3662 - accuracy: 0.130 - ETA: 41s - loss: 3.3661 - accuracy: 0.129 - ETA: 40s - loss: 3.3661 - accuracy: 0.130 - ETA: 39s - loss: 3.3663 - accuracy: 0.130 - ETA: 39s - loss: 3.3665 - accuracy: 0.129 - ETA: 38s - loss: 3.3660 - accuracy: 0.129 - ETA: 37s - loss: 3.3656 - accuracy: 0.129 - ETA: 36s - loss: 3.3654 - accuracy: 0.130 - ETA: 36s - loss: 3.3656 - accuracy: 0.129 - ETA: 35s - loss: 3.3651 - accuracy: 0.130 - ETA: 34s - loss: 3.3646 - accuracy: 0.130 - ETA: 33s - loss: 3.3641 - accuracy: 0.130 - ETA: 33s - loss: 3.3638 - accuracy: 0.130 - ETA: 32s - loss: 3.3635 - accuracy: 0.130 - ETA: 31s - loss: 3.3629 - accuracy: 0.130 - ETA: 31s - loss: 3.3630 - accuracy: 0.130 - ETA: 30s - loss: 3.3619 - accuracy: 0.130 - ETA: 29s - loss: 3.3616 - accuracy: 0.131 - ETA: 28s - loss: 3.3611 - accuracy: 0.131 - ETA: 28s - loss: 3.3611 - accuracy: 0.131 - ETA: 27s - loss: 3.3605 - accuracy: 0.131 - ETA: 26s - loss: 3.3610 - accuracy: 0.131 - ETA: 25s - loss: 3.3612 - accuracy: 0.131 - ETA: 25s - loss: 3.3604 - accuracy: 0.131 - ETA: 24s - loss: 3.3604 - accuracy: 0.131 - ETA: 23s - loss: 3.3602 - accuracy: 0.131 - ETA: 22s - loss: 3.3600 - accuracy: 0.131 - ETA: 22s - loss: 3.3598 - accuracy: 0.131 - ETA: 21s - loss: 3.3596 - accuracy: 0.131 - ETA: 20s - loss: 3.3598 - accuracy: 0.131 - ETA: 19s - loss: 3.3595 - accuracy: 0.131 - ETA: 19s - loss: 3.3589 - accuracy: 0.131 - ETA: 18s - loss: 3.3587 - accuracy: 0.131 - ETA: 17s - loss: 3.3585 - accuracy: 0.131 - ETA: 17s - loss: 3.3589 - accuracy: 0.131 - ETA: 16s - loss: 3.3589 - accuracy: 0.131 - ETA: 15s - loss: 3.3594 - accuracy: 0.131 - ETA: 14s - loss: 3.3590 - accuracy: 0.131 - ETA: 14s - loss: 3.3593 - accuracy: 0.131 - ETA: 13s - loss: 3.3592 - accuracy: 0.131 - ETA: 12s - loss: 3.3590 - accuracy: 0.131 - ETA: 11s - loss: 3.3596 - accuracy: 0.131 - ETA: 11s - loss: 3.3593 - accuracy: 0.131 - ETA: 10s - loss: 3.3588 - accuracy: 0.131 - ETA: 9s - loss: 3.3586 - accuracy: 0.131 - ETA: 8s - loss: 3.3586 - accuracy: 0.13 - ETA: 8s - loss: 3.3587 - accuracy: 0.13 - ETA: 7s - loss: 3.3585 - accuracy: 0.13 - ETA: 6s - loss: 3.3584 - accuracy: 0.13 - ETA: 5s - loss: 3.3583 - accuracy: 0.13 - ETA: 5s - loss: 3.3579 - accuracy: 0.13 - ETA: 4s - loss: 3.3579 - accuracy: 0.13 - ETA: 3s - loss: 3.3578 - accuracy: 0.13 - ETA: 3s - loss: 3.3583 - accuracy: 0.13 - ETA: 2s - loss: 3.3586 - accuracy: 0.13 - ETA: 1s - loss: 3.3593 - accuracy: 0.13 - ETA: 0s - loss: 3.3592 - accuracy: 0.13 - ETA: 0s - loss: 3.3594 - accuracy: 0.13 - 262s 6ms/step - loss: 3.3593 - accuracy: 0.1313 - val_loss: 3.9289 - val_accuracy: 0.0277\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:00 - loss: 3.2979 - accuracy: 0.13 - ETA: 3:59 - loss: 3.2570 - accuracy: 0.14 - ETA: 3:59 - loss: 3.3754 - accuracy: 0.12 - ETA: 3:59 - loss: 3.4032 - accuracy: 0.11 - ETA: 3:58 - loss: 3.3543 - accuracy: 0.12 - ETA: 3:56 - loss: 3.3476 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3572 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3507 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3491 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3429 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3277 - accuracy: 0.14 - ETA: 3:56 - loss: 3.3390 - accuracy: 0.13 - ETA: 3:57 - loss: 3.3418 - accuracy: 0.13 - ETA: 3:57 - loss: 3.3501 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3353 - accuracy: 0.14 - ETA: 3:57 - loss: 3.3365 - accuracy: 0.14 - ETA: 3:57 - loss: 3.3446 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3552 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3540 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3393 - accuracy: 0.14 - ETA: 3:54 - loss: 3.3516 - accuracy: 0.13 - ETA: 3:53 - loss: 3.3546 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3558 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3539 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3510 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3499 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3514 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3513 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3568 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3520 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3419 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3392 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3397 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3401 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3401 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3429 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3430 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3408 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3437 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3413 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3393 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3392 - accuracy: 0.14 - ETA: 3:35 - loss: 3.3370 - accuracy: 0.14 - ETA: 3:35 - loss: 3.3392 - accuracy: 0.14 - ETA: 3:34 - loss: 3.3411 - accuracy: 0.14 - ETA: 3:33 - loss: 3.3461 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3454 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3498 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3516 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3538 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3563 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3586 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3594 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3605 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3607 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3654 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3663 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3647 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3648 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3661 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3654 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3675 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3684 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3700 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3690 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3677 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3675 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3673 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3700 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3711 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3694 - accuracy: 0.13 - ETA: 3:12 - loss: 3.3721 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3709 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3720 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3719 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3724 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3726 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3730 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3739 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3756 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3758 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3793 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3803 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3795 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3804 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3793 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3785 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3789 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3773 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3763 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3750 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3751 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3741 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3747 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3755 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3779 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3766 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3776 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3785 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3780 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3776 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3779 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3781 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3794 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3798 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3798 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3792 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3775 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3773 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3787 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3773 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3763 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3774 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3771 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3772 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3770 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3758 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3764 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3763 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3750 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3741 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3748 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3725 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3709 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3708 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3719 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3731 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3734 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3739 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3744 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3724 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3723 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3736 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3735 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3748 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3757 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3757 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3756 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3765 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3778 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3779 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3782 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3802 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3800 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3804 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3806 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3804 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3798 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3810 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3809 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3810 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3806 - accuracy: 0.12 - ETA: 2:12 - loss: 3.3792 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3795 - accuracy: 0.12 - ETA: 2:10 - loss: 3.3802 - accuracy: 0.12 - ETA: 2:10 - loss: 3.3818 - accuracy: 0.12 - ETA: 2:09 - loss: 3.3821 - accuracy: 0.12 - ETA: 2:08 - loss: 3.3832 - accuracy: 0.12 - ETA: 2:07 - loss: 3.3825 - accuracy: 0.12 - ETA: 2:07 - loss: 3.3829 - accuracy: 0.12 - ETA: 2:06 - loss: 3.3841 - accuracy: 0.12 - ETA: 2:05 - loss: 3.3851 - accuracy: 0.12 - ETA: 2:05 - loss: 3.3855 - accuracy: 0.12 - ETA: 2:04 - loss: 3.3854 - accuracy: 0.12 - ETA: 2:03 - loss: 3.3850 - accuracy: 0.12 - ETA: 2:02 - loss: 3.3845 - accuracy: 0.12 - ETA: 2:02 - loss: 3.3859 - accuracy: 0.12 - ETA: 2:01 - loss: 3.3855 - accuracy: 0.12 - ETA: 2:00 - loss: 3.3855 - accuracy: 0.12 - ETA: 1:59 - loss: 3.3849 - accuracy: 0.12 - ETA: 1:59 - loss: 3.3851 - accuracy: 0.12 - ETA: 1:58 - loss: 3.3842 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3851 - accuracy: 0.12 - ETA: 1:56 - loss: 3.3852 - accuracy: 0.12 - ETA: 1:56 - loss: 3.3854 - accuracy: 0.12 - ETA: 1:55 - loss: 3.3857 - accuracy: 0.12 - ETA: 1:54 - loss: 3.3866 - accuracy: 0.12 - ETA: 1:53 - loss: 3.3874 - accuracy: 0.12 - ETA: 1:53 - loss: 3.3866 - accuracy: 0.12 - ETA: 1:52 - loss: 3.3865 - accuracy: 0.12 - ETA: 1:51 - loss: 3.3857 - accuracy: 0.12 - ETA: 1:50 - loss: 3.3854 - accuracy: 0.12 - ETA: 1:50 - loss: 3.3859 - accuracy: 0.12 - ETA: 1:49 - loss: 3.3859 - accuracy: 0.12 - ETA: 1:48 - loss: 3.3852 - accuracy: 0.12 - ETA: 1:47 - loss: 3.3863 - accuracy: 0.1295"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:47 - loss: 3.3853 - accuracy: 0.12 - ETA: 1:46 - loss: 3.3848 - accuracy: 0.12 - ETA: 1:45 - loss: 3.3842 - accuracy: 0.12 - ETA: 1:45 - loss: 3.3843 - accuracy: 0.12 - ETA: 1:44 - loss: 3.3845 - accuracy: 0.12 - ETA: 1:43 - loss: 3.3843 - accuracy: 0.12 - ETA: 1:42 - loss: 3.3842 - accuracy: 0.12 - ETA: 1:42 - loss: 3.3846 - accuracy: 0.12 - ETA: 1:41 - loss: 3.3844 - accuracy: 0.12 - ETA: 1:40 - loss: 3.3842 - accuracy: 0.12 - ETA: 1:39 - loss: 3.3838 - accuracy: 0.12 - ETA: 1:39 - loss: 3.3836 - accuracy: 0.12 - ETA: 1:38 - loss: 3.3829 - accuracy: 0.12 - ETA: 1:37 - loss: 3.3827 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3826 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3827 - accuracy: 0.12 - ETA: 1:35 - loss: 3.3832 - accuracy: 0.12 - ETA: 1:34 - loss: 3.3834 - accuracy: 0.12 - ETA: 1:34 - loss: 3.3836 - accuracy: 0.12 - ETA: 1:33 - loss: 3.3829 - accuracy: 0.12 - ETA: 1:32 - loss: 3.3831 - accuracy: 0.12 - ETA: 1:31 - loss: 3.3832 - accuracy: 0.12 - ETA: 1:31 - loss: 3.3835 - accuracy: 0.12 - ETA: 1:30 - loss: 3.3841 - accuracy: 0.12 - ETA: 1:29 - loss: 3.3838 - accuracy: 0.12 - ETA: 1:28 - loss: 3.3843 - accuracy: 0.12 - ETA: 1:27 - loss: 3.3849 - accuracy: 0.12 - ETA: 1:27 - loss: 3.3850 - accuracy: 0.12 - ETA: 1:26 - loss: 3.3851 - accuracy: 0.12 - ETA: 1:25 - loss: 3.3848 - accuracy: 0.12 - ETA: 1:25 - loss: 3.3846 - accuracy: 0.12 - ETA: 1:24 - loss: 3.3846 - accuracy: 0.12 - ETA: 1:23 - loss: 3.3841 - accuracy: 0.12 - ETA: 1:22 - loss: 3.3844 - accuracy: 0.12 - ETA: 1:21 - loss: 3.3841 - accuracy: 0.12 - ETA: 1:21 - loss: 3.3840 - accuracy: 0.12 - ETA: 1:20 - loss: 3.3844 - accuracy: 0.12 - ETA: 1:19 - loss: 3.3842 - accuracy: 0.12 - ETA: 1:18 - loss: 3.3842 - accuracy: 0.12 - ETA: 1:18 - loss: 3.3837 - accuracy: 0.12 - ETA: 1:17 - loss: 3.3839 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3836 - accuracy: 0.12 - ETA: 1:15 - loss: 3.3838 - accuracy: 0.12 - ETA: 1:15 - loss: 3.3833 - accuracy: 0.12 - ETA: 1:14 - loss: 3.3830 - accuracy: 0.12 - ETA: 1:13 - loss: 3.3825 - accuracy: 0.12 - ETA: 1:13 - loss: 3.3817 - accuracy: 0.12 - ETA: 1:12 - loss: 3.3811 - accuracy: 0.12 - ETA: 1:11 - loss: 3.3819 - accuracy: 0.12 - ETA: 1:10 - loss: 3.3817 - accuracy: 0.12 - ETA: 1:10 - loss: 3.3820 - accuracy: 0.12 - ETA: 1:09 - loss: 3.3813 - accuracy: 0.12 - ETA: 1:08 - loss: 3.3808 - accuracy: 0.12 - ETA: 1:07 - loss: 3.3805 - accuracy: 0.12 - ETA: 1:07 - loss: 3.3806 - accuracy: 0.12 - ETA: 1:06 - loss: 3.3803 - accuracy: 0.12 - ETA: 1:05 - loss: 3.3793 - accuracy: 0.12 - ETA: 1:04 - loss: 3.3789 - accuracy: 0.12 - ETA: 1:04 - loss: 3.3790 - accuracy: 0.12 - ETA: 1:03 - loss: 3.3787 - accuracy: 0.12 - ETA: 1:02 - loss: 3.3779 - accuracy: 0.12 - ETA: 1:01 - loss: 3.3780 - accuracy: 0.12 - ETA: 1:01 - loss: 3.3780 - accuracy: 0.12 - ETA: 1:00 - loss: 3.3775 - accuracy: 0.12 - ETA: 59s - loss: 3.3778 - accuracy: 0.1297 - ETA: 58s - loss: 3.3769 - accuracy: 0.129 - ETA: 58s - loss: 3.3768 - accuracy: 0.129 - ETA: 57s - loss: 3.3766 - accuracy: 0.129 - ETA: 56s - loss: 3.3756 - accuracy: 0.130 - ETA: 55s - loss: 3.3750 - accuracy: 0.130 - ETA: 55s - loss: 3.3743 - accuracy: 0.130 - ETA: 54s - loss: 3.3746 - accuracy: 0.130 - ETA: 53s - loss: 3.3751 - accuracy: 0.130 - ETA: 52s - loss: 3.3749 - accuracy: 0.130 - ETA: 52s - loss: 3.3749 - accuracy: 0.130 - ETA: 51s - loss: 3.3754 - accuracy: 0.130 - ETA: 50s - loss: 3.3752 - accuracy: 0.130 - ETA: 49s - loss: 3.3758 - accuracy: 0.130 - ETA: 49s - loss: 3.3759 - accuracy: 0.130 - ETA: 48s - loss: 3.3755 - accuracy: 0.130 - ETA: 47s - loss: 3.3758 - accuracy: 0.130 - ETA: 46s - loss: 3.3759 - accuracy: 0.130 - ETA: 46s - loss: 3.3761 - accuracy: 0.130 - ETA: 45s - loss: 3.3765 - accuracy: 0.130 - ETA: 44s - loss: 3.3761 - accuracy: 0.130 - ETA: 43s - loss: 3.3760 - accuracy: 0.130 - ETA: 43s - loss: 3.3758 - accuracy: 0.130 - ETA: 42s - loss: 3.3762 - accuracy: 0.130 - ETA: 41s - loss: 3.3765 - accuracy: 0.130 - ETA: 40s - loss: 3.3765 - accuracy: 0.129 - ETA: 40s - loss: 3.3767 - accuracy: 0.129 - ETA: 39s - loss: 3.3769 - accuracy: 0.129 - ETA: 38s - loss: 3.3772 - accuracy: 0.129 - ETA: 38s - loss: 3.3773 - accuracy: 0.129 - ETA: 37s - loss: 3.3774 - accuracy: 0.129 - ETA: 36s - loss: 3.3779 - accuracy: 0.129 - ETA: 35s - loss: 3.3784 - accuracy: 0.129 - ETA: 35s - loss: 3.3786 - accuracy: 0.128 - ETA: 34s - loss: 3.3787 - accuracy: 0.128 - ETA: 33s - loss: 3.3792 - accuracy: 0.128 - ETA: 32s - loss: 3.3794 - accuracy: 0.128 - ETA: 32s - loss: 3.3793 - accuracy: 0.128 - ETA: 31s - loss: 3.3790 - accuracy: 0.128 - ETA: 30s - loss: 3.3799 - accuracy: 0.128 - ETA: 29s - loss: 3.3798 - accuracy: 0.128 - ETA: 29s - loss: 3.3798 - accuracy: 0.128 - ETA: 28s - loss: 3.3798 - accuracy: 0.128 - ETA: 27s - loss: 3.3802 - accuracy: 0.128 - ETA: 26s - loss: 3.3798 - accuracy: 0.128 - ETA: 26s - loss: 3.3795 - accuracy: 0.128 - ETA: 25s - loss: 3.3796 - accuracy: 0.128 - ETA: 24s - loss: 3.3795 - accuracy: 0.128 - ETA: 23s - loss: 3.3795 - accuracy: 0.128 - ETA: 23s - loss: 3.3796 - accuracy: 0.128 - ETA: 22s - loss: 3.3790 - accuracy: 0.128 - ETA: 21s - loss: 3.3793 - accuracy: 0.128 - ETA: 20s - loss: 3.3797 - accuracy: 0.128 - ETA: 20s - loss: 3.3797 - accuracy: 0.128 - ETA: 19s - loss: 3.3797 - accuracy: 0.128 - ETA: 18s - loss: 3.3798 - accuracy: 0.128 - ETA: 17s - loss: 3.3793 - accuracy: 0.128 - ETA: 17s - loss: 3.3795 - accuracy: 0.128 - ETA: 16s - loss: 3.3802 - accuracy: 0.128 - ETA: 15s - loss: 3.3798 - accuracy: 0.128 - ETA: 14s - loss: 3.3802 - accuracy: 0.128 - ETA: 14s - loss: 3.3803 - accuracy: 0.128 - ETA: 13s - loss: 3.3809 - accuracy: 0.128 - ETA: 12s - loss: 3.3814 - accuracy: 0.128 - ETA: 11s - loss: 3.3819 - accuracy: 0.128 - ETA: 11s - loss: 3.3824 - accuracy: 0.128 - ETA: 10s - loss: 3.3826 - accuracy: 0.128 - ETA: 9s - loss: 3.3821 - accuracy: 0.128 - ETA: 9s - loss: 3.3823 - accuracy: 0.12 - ETA: 8s - loss: 3.3825 - accuracy: 0.12 - ETA: 7s - loss: 3.3830 - accuracy: 0.12 - ETA: 6s - loss: 3.3837 - accuracy: 0.12 - ETA: 6s - loss: 3.3838 - accuracy: 0.12 - ETA: 5s - loss: 3.3837 - accuracy: 0.12 - ETA: 4s - loss: 3.3840 - accuracy: 0.12 - ETA: 3s - loss: 3.3840 - accuracy: 0.12 - ETA: 3s - loss: 3.3841 - accuracy: 0.12 - ETA: 2s - loss: 3.3840 - accuracy: 0.12 - ETA: 1s - loss: 3.3843 - accuracy: 0.12 - ETA: 0s - loss: 3.3846 - accuracy: 0.12 - ETA: 0s - loss: 3.3846 - accuracy: 0.12 - 265s 6ms/step - loss: 3.3846 - accuracy: 0.1279 - val_loss: 4.1235 - val_accuracy: 0.0327\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:05 - loss: 3.1893 - accuracy: 0.15 - ETA: 4:01 - loss: 3.3235 - accuracy: 0.15 - ETA: 3:59 - loss: 3.4058 - accuracy: 0.13 - ETA: 3:55 - loss: 3.4629 - accuracy: 0.13 - ETA: 3:54 - loss: 3.5022 - accuracy: 0.13 - ETA: 3:54 - loss: 3.4956 - accuracy: 0.12 - ETA: 3:55 - loss: 3.5192 - accuracy: 0.12 - ETA: 3:58 - loss: 3.5235 - accuracy: 0.11 - ETA: 3:58 - loss: 3.5098 - accuracy: 0.12 - ETA: 3:58 - loss: 3.4918 - accuracy: 0.12 - ETA: 3:58 - loss: 3.4965 - accuracy: 0.12 - ETA: 3:57 - loss: 3.4811 - accuracy: 0.13 - ETA: 3:57 - loss: 3.4772 - accuracy: 0.12 - ETA: 3:56 - loss: 3.4735 - accuracy: 0.12 - ETA: 3:54 - loss: 3.4716 - accuracy: 0.12 - ETA: 3:53 - loss: 3.4659 - accuracy: 0.12 - ETA: 3:52 - loss: 3.4595 - accuracy: 0.12 - ETA: 3:51 - loss: 3.4492 - accuracy: 0.12 - ETA: 3:50 - loss: 3.4455 - accuracy: 0.12 - ETA: 3:49 - loss: 3.4334 - accuracy: 0.12 - ETA: 3:48 - loss: 3.4327 - accuracy: 0.12 - ETA: 3:48 - loss: 3.4323 - accuracy: 0.12 - ETA: 3:47 - loss: 3.4298 - accuracy: 0.12 - ETA: 3:46 - loss: 3.4282 - accuracy: 0.12 - ETA: 3:45 - loss: 3.4299 - accuracy: 0.12 - ETA: 3:44 - loss: 3.4277 - accuracy: 0.12 - ETA: 3:43 - loss: 3.4265 - accuracy: 0.12 - ETA: 3:42 - loss: 3.4277 - accuracy: 0.12 - ETA: 3:41 - loss: 3.4235 - accuracy: 0.12 - ETA: 3:40 - loss: 3.4166 - accuracy: 0.12 - ETA: 3:39 - loss: 3.4180 - accuracy: 0.12 - ETA: 3:39 - loss: 3.4199 - accuracy: 0.12 - ETA: 3:38 - loss: 3.4191 - accuracy: 0.12 - ETA: 3:37 - loss: 3.4182 - accuracy: 0.12 - ETA: 3:36 - loss: 3.4167 - accuracy: 0.12 - ETA: 3:35 - loss: 3.4182 - accuracy: 0.12 - ETA: 3:35 - loss: 3.4158 - accuracy: 0.12 - ETA: 3:34 - loss: 3.4155 - accuracy: 0.12 - ETA: 3:33 - loss: 3.4147 - accuracy: 0.12 - ETA: 3:32 - loss: 3.4127 - accuracy: 0.12 - ETA: 3:32 - loss: 3.4105 - accuracy: 0.12 - ETA: 3:31 - loss: 3.4121 - accuracy: 0.12 - ETA: 3:30 - loss: 3.4145 - accuracy: 0.12 - ETA: 3:29 - loss: 3.4164 - accuracy: 0.12 - ETA: 3:28 - loss: 3.4149 - accuracy: 0.12 - ETA: 3:28 - loss: 3.4105 - accuracy: 0.12 - ETA: 3:27 - loss: 3.4080 - accuracy: 0.12 - ETA: 3:26 - loss: 3.4082 - accuracy: 0.12 - ETA: 3:25 - loss: 3.4063 - accuracy: 0.12 - ETA: 3:24 - loss: 3.4061 - accuracy: 0.12 - ETA: 3:24 - loss: 3.4081 - accuracy: 0.12 - ETA: 3:23 - loss: 3.4084 - accuracy: 0.12 - ETA: 3:22 - loss: 3.4086 - accuracy: 0.12 - ETA: 3:22 - loss: 3.4085 - accuracy: 0.12 - ETA: 3:21 - loss: 3.4095 - accuracy: 0.12 - ETA: 3:20 - loss: 3.4093 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4062 - accuracy: 0.12 - ETA: 3:19 - loss: 3.4064 - accuracy: 0.12 - ETA: 3:18 - loss: 3.4100 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4106 - accuracy: 0.12 - ETA: 3:17 - loss: 3.4099 - accuracy: 0.12 - ETA: 3:16 - loss: 3.4112 - accuracy: 0.12 - ETA: 3:15 - loss: 3.4112 - accuracy: 0.12 - ETA: 3:14 - loss: 3.4118 - accuracy: 0.12 - ETA: 3:13 - loss: 3.4130 - accuracy: 0.12 - ETA: 3:13 - loss: 3.4120 - accuracy: 0.12 - ETA: 3:12 - loss: 3.4123 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4106 - accuracy: 0.12 - ETA: 3:11 - loss: 3.4138 - accuracy: 0.12 - ETA: 3:10 - loss: 3.4166 - accuracy: 0.12 - ETA: 3:09 - loss: 3.4175 - accuracy: 0.11 - ETA: 3:08 - loss: 3.4178 - accuracy: 0.11 - ETA: 3:07 - loss: 3.4181 - accuracy: 0.11 - ETA: 3:07 - loss: 3.4177 - accuracy: 0.11 - ETA: 3:06 - loss: 3.4147 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4123 - accuracy: 0.12 - ETA: 3:05 - loss: 3.4102 - accuracy: 0.12 - ETA: 3:04 - loss: 3.4098 - accuracy: 0.12 - ETA: 3:03 - loss: 3.4102 - accuracy: 0.12 - ETA: 3:03 - loss: 3.4102 - accuracy: 0.12 - ETA: 3:02 - loss: 3.4100 - accuracy: 0.12 - ETA: 3:01 - loss: 3.4112 - accuracy: 0.12 - ETA: 3:01 - loss: 3.4121 - accuracy: 0.12 - ETA: 3:00 - loss: 3.4121 - accuracy: 0.12 - ETA: 2:59 - loss: 3.4130 - accuracy: 0.12 - ETA: 2:58 - loss: 3.4126 - accuracy: 0.12 - ETA: 2:58 - loss: 3.4124 - accuracy: 0.12 - ETA: 2:57 - loss: 3.4131 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4133 - accuracy: 0.12 - ETA: 2:56 - loss: 3.4135 - accuracy: 0.12 - ETA: 2:55 - loss: 3.4151 - accuracy: 0.12 - ETA: 2:54 - loss: 3.4138 - accuracy: 0.12 - ETA: 2:53 - loss: 3.4133 - accuracy: 0.12 - ETA: 2:52 - loss: 3.4113 - accuracy: 0.12 - ETA: 2:52 - loss: 3.4116 - accuracy: 0.12 - ETA: 2:51 - loss: 3.4105 - accuracy: 0.12 - ETA: 2:51 - loss: 3.4106 - accuracy: 0.12 - ETA: 2:50 - loss: 3.4102 - accuracy: 0.12 - ETA: 2:49 - loss: 3.4079 - accuracy: 0.12 - ETA: 2:49 - loss: 3.4080 - accuracy: 0.12 - ETA: 2:48 - loss: 3.4094 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4102 - accuracy: 0.12 - ETA: 2:47 - loss: 3.4101 - accuracy: 0.12 - ETA: 2:46 - loss: 3.4103 - accuracy: 0.12 - ETA: 2:46 - loss: 3.4114 - accuracy: 0.12 - ETA: 2:45 - loss: 3.4126 - accuracy: 0.11 - ETA: 2:44 - loss: 3.4121 - accuracy: 0.12 - ETA: 2:43 - loss: 3.4110 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4110 - accuracy: 0.12 - ETA: 2:42 - loss: 3.4100 - accuracy: 0.12 - ETA: 2:41 - loss: 3.4109 - accuracy: 0.12 - ETA: 2:40 - loss: 3.4101 - accuracy: 0.12 - ETA: 2:39 - loss: 3.4110 - accuracy: 0.12 - ETA: 2:39 - loss: 3.4106 - accuracy: 0.12 - ETA: 2:38 - loss: 3.4103 - accuracy: 0.12 - ETA: 2:37 - loss: 3.4112 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4122 - accuracy: 0.12 - ETA: 2:36 - loss: 3.4124 - accuracy: 0.12 - ETA: 2:35 - loss: 3.4129 - accuracy: 0.12 - ETA: 2:34 - loss: 3.4130 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4140 - accuracy: 0.12 - ETA: 2:33 - loss: 3.4141 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4141 - accuracy: 0.12 - ETA: 2:31 - loss: 3.4139 - accuracy: 0.12 - ETA: 2:30 - loss: 3.4146 - accuracy: 0.11 - ETA: 2:30 - loss: 3.4164 - accuracy: 0.11 - ETA: 2:29 - loss: 3.4170 - accuracy: 0.11 - ETA: 2:28 - loss: 3.4156 - accuracy: 0.11 - ETA: 2:28 - loss: 3.4145 - accuracy: 0.11 - ETA: 2:27 - loss: 3.4142 - accuracy: 0.11 - ETA: 2:26 - loss: 3.4134 - accuracy: 0.11 - ETA: 2:25 - loss: 3.4134 - accuracy: 0.12 - ETA: 2:25 - loss: 3.4142 - accuracy: 0.11 - ETA: 2:24 - loss: 3.4126 - accuracy: 0.12 - ETA: 2:23 - loss: 3.4114 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4104 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4103 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4097 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4096 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4095 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4082 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4081 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4085 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4060 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4050 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4054 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4049 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4063 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4064 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4065 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4060 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4054 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4032 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4028 - accuracy: 0.12 - ETA: 2:08 - loss: 3.4017 - accuracy: 0.12 - ETA: 2:08 - loss: 3.4013 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4010 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4000 - accuracy: 0.12 - ETA: 2:06 - loss: 3.3990 - accuracy: 0.12 - ETA: 2:05 - loss: 3.3988 - accuracy: 0.12 - ETA: 2:04 - loss: 3.3996 - accuracy: 0.12 - ETA: 2:03 - loss: 3.3996 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4002 - accuracy: 0.12 - ETA: 2:02 - loss: 3.3992 - accuracy: 0.12 - ETA: 2:01 - loss: 3.3987 - accuracy: 0.12 - ETA: 2:00 - loss: 3.3989 - accuracy: 0.12 - ETA: 2:00 - loss: 3.3990 - accuracy: 0.12 - ETA: 1:59 - loss: 3.4025 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4021 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4023 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4019 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4027 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4034 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4038 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4034 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4027 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4026 - accuracy: 0.12 - ETA: 1:52 - loss: 3.4012 - accuracy: 0.12 - ETA: 1:51 - loss: 3.4016 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4020 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4020 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4014 - accuracy: 0.12 - ETA: 1:48 - loss: 3.4018 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4013 - accuracy: 0.12 - ETA: 1:46 - loss: 3.4009 - accuracy: 0.12 - ETA: 1:46 - loss: 3.4008 - accuracy: 0.1233"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.4013 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4018 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4015 - accuracy: 0.12 - ETA: 1:43 - loss: 3.4007 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4003 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4006 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4010 - accuracy: 0.12 - ETA: 1:40 - loss: 3.4011 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4005 - accuracy: 0.12 - ETA: 1:39 - loss: 3.3999 - accuracy: 0.12 - ETA: 1:38 - loss: 3.3998 - accuracy: 0.12 - ETA: 1:37 - loss: 3.4002 - accuracy: 0.12 - ETA: 1:36 - loss: 3.3998 - accuracy: 0.12 - ETA: 1:36 - loss: 3.3987 - accuracy: 0.12 - ETA: 1:35 - loss: 3.3990 - accuracy: 0.12 - ETA: 1:34 - loss: 3.3981 - accuracy: 0.12 - ETA: 1:34 - loss: 3.3975 - accuracy: 0.12 - ETA: 1:33 - loss: 3.3976 - accuracy: 0.12 - ETA: 1:32 - loss: 3.3975 - accuracy: 0.12 - ETA: 1:31 - loss: 3.3967 - accuracy: 0.12 - ETA: 1:31 - loss: 3.3964 - accuracy: 0.12 - ETA: 1:30 - loss: 3.3962 - accuracy: 0.12 - ETA: 1:29 - loss: 3.3952 - accuracy: 0.12 - ETA: 1:28 - loss: 3.3950 - accuracy: 0.12 - ETA: 1:28 - loss: 3.3947 - accuracy: 0.12 - ETA: 1:27 - loss: 3.3942 - accuracy: 0.12 - ETA: 1:26 - loss: 3.3939 - accuracy: 0.12 - ETA: 1:26 - loss: 3.3940 - accuracy: 0.12 - ETA: 1:25 - loss: 3.3939 - accuracy: 0.12 - ETA: 1:24 - loss: 3.3940 - accuracy: 0.12 - ETA: 1:23 - loss: 3.3938 - accuracy: 0.12 - ETA: 1:23 - loss: 3.3939 - accuracy: 0.12 - ETA: 1:22 - loss: 3.3945 - accuracy: 0.12 - ETA: 1:21 - loss: 3.3946 - accuracy: 0.12 - ETA: 1:20 - loss: 3.3947 - accuracy: 0.12 - ETA: 1:20 - loss: 3.3945 - accuracy: 0.12 - ETA: 1:19 - loss: 3.3949 - accuracy: 0.12 - ETA: 1:18 - loss: 3.3956 - accuracy: 0.12 - ETA: 1:17 - loss: 3.3963 - accuracy: 0.12 - ETA: 1:17 - loss: 3.3962 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3965 - accuracy: 0.12 - ETA: 1:15 - loss: 3.3963 - accuracy: 0.12 - ETA: 1:15 - loss: 3.3959 - accuracy: 0.12 - ETA: 1:14 - loss: 3.3957 - accuracy: 0.12 - ETA: 1:13 - loss: 3.3955 - accuracy: 0.12 - ETA: 1:12 - loss: 3.3960 - accuracy: 0.12 - ETA: 1:12 - loss: 3.3961 - accuracy: 0.12 - ETA: 1:11 - loss: 3.3958 - accuracy: 0.12 - ETA: 1:10 - loss: 3.3956 - accuracy: 0.12 - ETA: 1:09 - loss: 3.3955 - accuracy: 0.12 - ETA: 1:09 - loss: 3.3956 - accuracy: 0.12 - ETA: 1:08 - loss: 3.3958 - accuracy: 0.12 - ETA: 1:07 - loss: 3.3955 - accuracy: 0.12 - ETA: 1:06 - loss: 3.3959 - accuracy: 0.12 - ETA: 1:06 - loss: 3.3960 - accuracy: 0.12 - ETA: 1:05 - loss: 3.3953 - accuracy: 0.12 - ETA: 1:04 - loss: 3.3949 - accuracy: 0.12 - ETA: 1:04 - loss: 3.3941 - accuracy: 0.12 - ETA: 1:03 - loss: 3.3943 - accuracy: 0.12 - ETA: 1:02 - loss: 3.3945 - accuracy: 0.12 - ETA: 1:01 - loss: 3.3946 - accuracy: 0.12 - ETA: 1:01 - loss: 3.3948 - accuracy: 0.12 - ETA: 1:00 - loss: 3.3950 - accuracy: 0.12 - ETA: 59s - loss: 3.3948 - accuracy: 0.1249 - ETA: 58s - loss: 3.3945 - accuracy: 0.125 - ETA: 58s - loss: 3.3944 - accuracy: 0.125 - ETA: 57s - loss: 3.3944 - accuracy: 0.124 - ETA: 56s - loss: 3.3942 - accuracy: 0.125 - ETA: 55s - loss: 3.3942 - accuracy: 0.125 - ETA: 55s - loss: 3.3938 - accuracy: 0.125 - ETA: 54s - loss: 3.3935 - accuracy: 0.125 - ETA: 53s - loss: 3.3939 - accuracy: 0.125 - ETA: 52s - loss: 3.3933 - accuracy: 0.125 - ETA: 52s - loss: 3.3937 - accuracy: 0.125 - ETA: 51s - loss: 3.3933 - accuracy: 0.125 - ETA: 50s - loss: 3.3933 - accuracy: 0.125 - ETA: 50s - loss: 3.3932 - accuracy: 0.125 - ETA: 49s - loss: 3.3938 - accuracy: 0.125 - ETA: 48s - loss: 3.3935 - accuracy: 0.125 - ETA: 47s - loss: 3.3936 - accuracy: 0.125 - ETA: 47s - loss: 3.3934 - accuracy: 0.125 - ETA: 46s - loss: 3.3932 - accuracy: 0.125 - ETA: 45s - loss: 3.3923 - accuracy: 0.125 - ETA: 44s - loss: 3.3923 - accuracy: 0.125 - ETA: 44s - loss: 3.3926 - accuracy: 0.125 - ETA: 43s - loss: 3.3929 - accuracy: 0.126 - ETA: 42s - loss: 3.3929 - accuracy: 0.126 - ETA: 41s - loss: 3.3932 - accuracy: 0.125 - ETA: 41s - loss: 3.3931 - accuracy: 0.125 - ETA: 40s - loss: 3.3932 - accuracy: 0.125 - ETA: 39s - loss: 3.3935 - accuracy: 0.125 - ETA: 39s - loss: 3.3937 - accuracy: 0.125 - ETA: 38s - loss: 3.3933 - accuracy: 0.125 - ETA: 37s - loss: 3.3927 - accuracy: 0.125 - ETA: 36s - loss: 3.3922 - accuracy: 0.125 - ETA: 36s - loss: 3.3923 - accuracy: 0.125 - ETA: 35s - loss: 3.3922 - accuracy: 0.126 - ETA: 34s - loss: 3.3921 - accuracy: 0.126 - ETA: 33s - loss: 3.3920 - accuracy: 0.126 - ETA: 33s - loss: 3.3918 - accuracy: 0.126 - ETA: 32s - loss: 3.3910 - accuracy: 0.126 - ETA: 31s - loss: 3.3906 - accuracy: 0.126 - ETA: 30s - loss: 3.3909 - accuracy: 0.126 - ETA: 30s - loss: 3.3912 - accuracy: 0.126 - ETA: 29s - loss: 3.3914 - accuracy: 0.126 - ETA: 28s - loss: 3.3914 - accuracy: 0.126 - ETA: 27s - loss: 3.3916 - accuracy: 0.126 - ETA: 27s - loss: 3.3921 - accuracy: 0.126 - ETA: 26s - loss: 3.3922 - accuracy: 0.126 - ETA: 25s - loss: 3.3918 - accuracy: 0.126 - ETA: 25s - loss: 3.3918 - accuracy: 0.126 - ETA: 24s - loss: 3.3915 - accuracy: 0.126 - ETA: 23s - loss: 3.3914 - accuracy: 0.126 - ETA: 22s - loss: 3.3912 - accuracy: 0.126 - ETA: 22s - loss: 3.3915 - accuracy: 0.125 - ETA: 21s - loss: 3.3912 - accuracy: 0.126 - ETA: 20s - loss: 3.3910 - accuracy: 0.126 - ETA: 19s - loss: 3.3909 - accuracy: 0.126 - ETA: 19s - loss: 3.3911 - accuracy: 0.126 - ETA: 18s - loss: 3.3922 - accuracy: 0.126 - ETA: 17s - loss: 3.3925 - accuracy: 0.126 - ETA: 16s - loss: 3.3922 - accuracy: 0.126 - ETA: 16s - loss: 3.3921 - accuracy: 0.126 - ETA: 15s - loss: 3.3928 - accuracy: 0.126 - ETA: 14s - loss: 3.3927 - accuracy: 0.126 - ETA: 14s - loss: 3.3926 - accuracy: 0.126 - ETA: 13s - loss: 3.3918 - accuracy: 0.126 - ETA: 12s - loss: 3.3921 - accuracy: 0.126 - ETA: 11s - loss: 3.3920 - accuracy: 0.126 - ETA: 11s - loss: 3.3923 - accuracy: 0.126 - ETA: 10s - loss: 3.3923 - accuracy: 0.126 - ETA: 9s - loss: 3.3920 - accuracy: 0.126 - ETA: 8s - loss: 3.3913 - accuracy: 0.12 - ETA: 8s - loss: 3.3911 - accuracy: 0.12 - ETA: 7s - loss: 3.3915 - accuracy: 0.12 - ETA: 6s - loss: 3.3919 - accuracy: 0.12 - ETA: 5s - loss: 3.3919 - accuracy: 0.12 - ETA: 5s - loss: 3.3924 - accuracy: 0.12 - ETA: 4s - loss: 3.3926 - accuracy: 0.12 - ETA: 3s - loss: 3.3927 - accuracy: 0.12 - ETA: 3s - loss: 3.3927 - accuracy: 0.12 - ETA: 2s - loss: 3.3928 - accuracy: 0.12 - ETA: 1s - loss: 3.3923 - accuracy: 0.12 - ETA: 0s - loss: 3.3923 - accuracy: 0.12 - ETA: 0s - loss: 3.3917 - accuracy: 0.12 - 261s 6ms/step - loss: 3.3916 - accuracy: 0.1264 - val_loss: 4.0131 - val_accuracy: 0.0296\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:10 - loss: 3.3128 - accuracy: 0.13 - ETA: 3:59 - loss: 3.2972 - accuracy: 0.14 - ETA: 3:57 - loss: 3.2713 - accuracy: 0.15 - ETA: 4:00 - loss: 3.2839 - accuracy: 0.15 - ETA: 3:58 - loss: 3.3223 - accuracy: 0.14 - ETA: 3:54 - loss: 3.2835 - accuracy: 0.14 - ETA: 3:54 - loss: 3.3035 - accuracy: 0.14 - ETA: 3:54 - loss: 3.3172 - accuracy: 0.14 - ETA: 3:54 - loss: 3.3040 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3059 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3218 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3177 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3244 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3200 - accuracy: 0.13 - ETA: 3:57 - loss: 3.3134 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3142 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3151 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3209 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3192 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3182 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3149 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3110 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3191 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3275 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3246 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3240 - accuracy: 0.14 - ETA: 3:45 - loss: 3.3290 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3338 - accuracy: 0.14 - ETA: 3:43 - loss: 3.3381 - accuracy: 0.14 - ETA: 3:42 - loss: 3.3409 - accuracy: 0.14 - ETA: 3:41 - loss: 3.3461 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3468 - accuracy: 0.14 - ETA: 3:39 - loss: 3.3527 - accuracy: 0.14 - ETA: 3:38 - loss: 3.3546 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3560 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3575 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3596 - accuracy: 0.14 - ETA: 3:35 - loss: 3.3589 - accuracy: 0.14 - ETA: 3:34 - loss: 3.3618 - accuracy: 0.14 - ETA: 3:33 - loss: 3.3584 - accuracy: 0.14 - ETA: 3:33 - loss: 3.3602 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3609 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3588 - accuracy: 0.14 - ETA: 3:31 - loss: 3.3600 - accuracy: 0.14 - ETA: 3:30 - loss: 3.3563 - accuracy: 0.14 - ETA: 3:29 - loss: 3.3597 - accuracy: 0.14 - ETA: 3:28 - loss: 3.3612 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3606 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3566 - accuracy: 0.14 - ETA: 3:26 - loss: 3.3583 - accuracy: 0.14 - ETA: 3:25 - loss: 3.3593 - accuracy: 0.14 - ETA: 3:25 - loss: 3.3595 - accuracy: 0.14 - ETA: 3:24 - loss: 3.3601 - accuracy: 0.14 - ETA: 3:23 - loss: 3.3594 - accuracy: 0.14 - ETA: 3:23 - loss: 3.3610 - accuracy: 0.14 - ETA: 3:22 - loss: 3.3587 - accuracy: 0.14 - ETA: 3:21 - loss: 3.3595 - accuracy: 0.14 - ETA: 3:20 - loss: 3.3597 - accuracy: 0.14 - ETA: 3:20 - loss: 3.3574 - accuracy: 0.14 - ETA: 3:19 - loss: 3.3584 - accuracy: 0.14 - ETA: 3:18 - loss: 3.3584 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3596 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3591 - accuracy: 0.14 - ETA: 3:16 - loss: 3.3605 - accuracy: 0.14 - ETA: 3:15 - loss: 3.3576 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3565 - accuracy: 0.14 - ETA: 3:13 - loss: 3.3558 - accuracy: 0.14 - ETA: 3:13 - loss: 3.3565 - accuracy: 0.14 - ETA: 3:12 - loss: 3.3575 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3584 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3566 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3544 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3546 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3541 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3543 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3554 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3560 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3549 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3552 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3556 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3548 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3554 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3560 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3557 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3558 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3578 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3585 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3594 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3585 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3570 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3578 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3579 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3569 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3552 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3552 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3562 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3558 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3562 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3568 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3564 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3575 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3575 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3585 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3607 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3601 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3593 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3580 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3582 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3599 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3588 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3583 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3570 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3564 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3570 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3563 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3542 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3554 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3553 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3552 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3568 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3564 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3575 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3555 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3564 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3571 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3568 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3559 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3559 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3563 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3560 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3561 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3574 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3583 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3578 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3577 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3597 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3599 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3594 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3603 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3592 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3588 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3590 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3588 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3582 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3587 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3593 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3607 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3611 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3619 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3622 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3624 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3614 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3605 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3597 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3589 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3590 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3594 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3598 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3594 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3593 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3583 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3579 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3587 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3584 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3592 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3587 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3573 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3582 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3580 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3575 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3585 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3584 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3591 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3586 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3596 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3591 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3600 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3600 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3594 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3592 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3593 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3591 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3592 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3594 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3591 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3586 - accuracy: 0.1343"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3588 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3593 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3600 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3589 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3587 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3590 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3576 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3575 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3581 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3584 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3581 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3580 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3583 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3579 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3569 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3573 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3573 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3575 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3584 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3579 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3576 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3575 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3579 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3573 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3574 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3572 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3570 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3577 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3578 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3582 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3582 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3578 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3575 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3575 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3569 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3563 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3560 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3565 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3558 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3549 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3554 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3551 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3550 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3557 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3552 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3546 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3547 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3552 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3548 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3549 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3546 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3542 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3538 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3544 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3543 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3547 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3550 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3546 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3545 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3544 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3547 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3548 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3553 - accuracy: 0.13 - ETA: 59s - loss: 3.3551 - accuracy: 0.1358 - ETA: 58s - loss: 3.3553 - accuracy: 0.135 - ETA: 58s - loss: 3.3557 - accuracy: 0.135 - ETA: 57s - loss: 3.3550 - accuracy: 0.136 - ETA: 56s - loss: 3.3550 - accuracy: 0.136 - ETA: 55s - loss: 3.3544 - accuracy: 0.136 - ETA: 55s - loss: 3.3546 - accuracy: 0.136 - ETA: 54s - loss: 3.3544 - accuracy: 0.136 - ETA: 53s - loss: 3.3545 - accuracy: 0.136 - ETA: 52s - loss: 3.3549 - accuracy: 0.136 - ETA: 52s - loss: 3.3552 - accuracy: 0.136 - ETA: 51s - loss: 3.3558 - accuracy: 0.135 - ETA: 50s - loss: 3.3566 - accuracy: 0.135 - ETA: 50s - loss: 3.3565 - accuracy: 0.135 - ETA: 49s - loss: 3.3564 - accuracy: 0.135 - ETA: 48s - loss: 3.3565 - accuracy: 0.135 - ETA: 47s - loss: 3.3565 - accuracy: 0.135 - ETA: 47s - loss: 3.3574 - accuracy: 0.135 - ETA: 46s - loss: 3.3576 - accuracy: 0.135 - ETA: 45s - loss: 3.3577 - accuracy: 0.135 - ETA: 44s - loss: 3.3577 - accuracy: 0.135 - ETA: 44s - loss: 3.3579 - accuracy: 0.135 - ETA: 43s - loss: 3.3580 - accuracy: 0.135 - ETA: 42s - loss: 3.3590 - accuracy: 0.134 - ETA: 41s - loss: 3.3592 - accuracy: 0.134 - ETA: 41s - loss: 3.3598 - accuracy: 0.134 - ETA: 40s - loss: 3.3600 - accuracy: 0.134 - ETA: 39s - loss: 3.3614 - accuracy: 0.134 - ETA: 39s - loss: 3.3611 - accuracy: 0.134 - ETA: 38s - loss: 3.3609 - accuracy: 0.134 - ETA: 37s - loss: 3.3612 - accuracy: 0.134 - ETA: 36s - loss: 3.3614 - accuracy: 0.134 - ETA: 36s - loss: 3.3614 - accuracy: 0.134 - ETA: 35s - loss: 3.3619 - accuracy: 0.134 - ETA: 34s - loss: 3.3626 - accuracy: 0.134 - ETA: 33s - loss: 3.3627 - accuracy: 0.134 - ETA: 33s - loss: 3.3632 - accuracy: 0.134 - ETA: 32s - loss: 3.3632 - accuracy: 0.134 - ETA: 31s - loss: 3.3636 - accuracy: 0.133 - ETA: 30s - loss: 3.3642 - accuracy: 0.133 - ETA: 30s - loss: 3.3649 - accuracy: 0.133 - ETA: 29s - loss: 3.3652 - accuracy: 0.133 - ETA: 28s - loss: 3.3653 - accuracy: 0.133 - ETA: 28s - loss: 3.3656 - accuracy: 0.133 - ETA: 27s - loss: 3.3654 - accuracy: 0.133 - ETA: 26s - loss: 3.3656 - accuracy: 0.133 - ETA: 25s - loss: 3.3648 - accuracy: 0.133 - ETA: 25s - loss: 3.3647 - accuracy: 0.133 - ETA: 24s - loss: 3.3646 - accuracy: 0.133 - ETA: 23s - loss: 3.3645 - accuracy: 0.133 - ETA: 22s - loss: 3.3648 - accuracy: 0.133 - ETA: 22s - loss: 3.3656 - accuracy: 0.133 - ETA: 21s - loss: 3.3656 - accuracy: 0.133 - ETA: 20s - loss: 3.3657 - accuracy: 0.133 - ETA: 19s - loss: 3.3655 - accuracy: 0.133 - ETA: 19s - loss: 3.3659 - accuracy: 0.133 - ETA: 18s - loss: 3.3661 - accuracy: 0.133 - ETA: 17s - loss: 3.3664 - accuracy: 0.133 - ETA: 16s - loss: 3.3664 - accuracy: 0.133 - ETA: 16s - loss: 3.3666 - accuracy: 0.133 - ETA: 15s - loss: 3.3669 - accuracy: 0.133 - ETA: 14s - loss: 3.3672 - accuracy: 0.133 - ETA: 14s - loss: 3.3670 - accuracy: 0.133 - ETA: 13s - loss: 3.3669 - accuracy: 0.133 - ETA: 12s - loss: 3.3671 - accuracy: 0.133 - ETA: 11s - loss: 3.3674 - accuracy: 0.133 - ETA: 11s - loss: 3.3671 - accuracy: 0.133 - ETA: 10s - loss: 3.3663 - accuracy: 0.133 - ETA: 9s - loss: 3.3665 - accuracy: 0.133 - ETA: 8s - loss: 3.3665 - accuracy: 0.13 - ETA: 8s - loss: 3.3663 - accuracy: 0.13 - ETA: 7s - loss: 3.3663 - accuracy: 0.13 - ETA: 6s - loss: 3.3663 - accuracy: 0.13 - ETA: 5s - loss: 3.3658 - accuracy: 0.13 - ETA: 5s - loss: 3.3661 - accuracy: 0.13 - ETA: 4s - loss: 3.3663 - accuracy: 0.13 - ETA: 3s - loss: 3.3660 - accuracy: 0.13 - ETA: 3s - loss: 3.3663 - accuracy: 0.13 - ETA: 2s - loss: 3.3657 - accuracy: 0.13 - ETA: 1s - loss: 3.3659 - accuracy: 0.13 - ETA: 0s - loss: 3.3659 - accuracy: 0.13 - ETA: 0s - loss: 3.3656 - accuracy: 0.13 - 261s 6ms/step - loss: 3.3657 - accuracy: 0.1332 - val_loss: 3.9674 - val_accuracy: 0.0373\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:03 - loss: 3.1232 - accuracy: 0.21 - ETA: 3:54 - loss: 3.2791 - accuracy: 0.17 - ETA: 3:55 - loss: 3.3434 - accuracy: 0.16 - ETA: 3:53 - loss: 3.3469 - accuracy: 0.15 - ETA: 3:54 - loss: 3.3350 - accuracy: 0.15 - ETA: 3:57 - loss: 3.3233 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3394 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3326 - accuracy: 0.14 - ETA: 3:53 - loss: 3.3284 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3458 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3477 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3595 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3571 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3550 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3530 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3489 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3511 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3519 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3386 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3408 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3417 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3475 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3449 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3505 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3530 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3507 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3537 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3550 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3557 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3578 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3481 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3504 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3436 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3449 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3466 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3471 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3489 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3475 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3463 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3435 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3444 - accuracy: 0.13 - ETA: 3:33 - loss: 3.3428 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3402 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3393 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3389 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3385 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3387 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3372 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3338 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3334 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3351 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3348 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3333 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3351 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3384 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3365 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3387 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3375 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3384 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3359 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3353 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3387 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3383 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3387 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3382 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3400 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3422 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3410 - accuracy: 0.13 - ETA: 3:12 - loss: 3.3398 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3406 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3410 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3433 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3439 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3442 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3444 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3433 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3447 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3437 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3433 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3434 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3449 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3518 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3530 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3544 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3560 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3546 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3539 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3530 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3527 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3518 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3518 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3527 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3530 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3538 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3547 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3563 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3573 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3568 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3574 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3586 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3595 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3610 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3637 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3638 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3644 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3635 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3644 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3661 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3650 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3652 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3634 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3628 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3627 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3628 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3603 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3610 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3602 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3616 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3603 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3619 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3636 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3639 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3646 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3643 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3641 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3632 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3626 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3619 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3630 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3638 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3637 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3630 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3628 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3623 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3618 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3621 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3615 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3611 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3612 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3608 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3617 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3614 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3602 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3596 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3598 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3605 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3610 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3611 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3610 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3609 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3609 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3609 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3616 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3623 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3635 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3637 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3639 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3639 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3638 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3648 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3661 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3661 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3654 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3652 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3648 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3655 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3655 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3640 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3639 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3645 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3649 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3649 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3657 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3658 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3652 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3642 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3647 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3640 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3634 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3619 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3628 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3617 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3626 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3639 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3652 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3653 - accuracy: 0.1325"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:46 - loss: 3.3649 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3650 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3644 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3651 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3649 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3649 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3649 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3646 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3648 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3647 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3650 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3651 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3652 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3658 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3655 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3664 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3657 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3656 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3659 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3656 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3654 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3648 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3652 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3646 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3648 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3657 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3653 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3660 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3660 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3656 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3653 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3657 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3658 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3664 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3663 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3668 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3669 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3665 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3663 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3653 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3648 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3642 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3643 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3642 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3655 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3650 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3652 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3641 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3640 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3637 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3643 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3645 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3642 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3639 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3628 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3626 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3626 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3623 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3615 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3614 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3613 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3613 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3613 - accuracy: 0.13 - ETA: 59s - loss: 3.3614 - accuracy: 0.1314 - ETA: 58s - loss: 3.3610 - accuracy: 0.131 - ETA: 58s - loss: 3.3604 - accuracy: 0.131 - ETA: 57s - loss: 3.3611 - accuracy: 0.131 - ETA: 56s - loss: 3.3613 - accuracy: 0.131 - ETA: 56s - loss: 3.3615 - accuracy: 0.131 - ETA: 55s - loss: 3.3608 - accuracy: 0.131 - ETA: 54s - loss: 3.3603 - accuracy: 0.131 - ETA: 53s - loss: 3.3600 - accuracy: 0.131 - ETA: 53s - loss: 3.3597 - accuracy: 0.131 - ETA: 52s - loss: 3.3601 - accuracy: 0.131 - ETA: 51s - loss: 3.3601 - accuracy: 0.131 - ETA: 50s - loss: 3.3601 - accuracy: 0.131 - ETA: 50s - loss: 3.3600 - accuracy: 0.131 - ETA: 49s - loss: 3.3603 - accuracy: 0.131 - ETA: 48s - loss: 3.3603 - accuracy: 0.131 - ETA: 47s - loss: 3.3599 - accuracy: 0.131 - ETA: 47s - loss: 3.3604 - accuracy: 0.131 - ETA: 46s - loss: 3.3598 - accuracy: 0.131 - ETA: 45s - loss: 3.3599 - accuracy: 0.131 - ETA: 45s - loss: 3.3596 - accuracy: 0.131 - ETA: 44s - loss: 3.3594 - accuracy: 0.131 - ETA: 43s - loss: 3.3598 - accuracy: 0.131 - ETA: 42s - loss: 3.3595 - accuracy: 0.131 - ETA: 42s - loss: 3.3594 - accuracy: 0.131 - ETA: 41s - loss: 3.3594 - accuracy: 0.131 - ETA: 40s - loss: 3.3593 - accuracy: 0.131 - ETA: 39s - loss: 3.3589 - accuracy: 0.131 - ETA: 39s - loss: 3.3586 - accuracy: 0.131 - ETA: 38s - loss: 3.3588 - accuracy: 0.131 - ETA: 37s - loss: 3.3584 - accuracy: 0.131 - ETA: 36s - loss: 3.3579 - accuracy: 0.131 - ETA: 36s - loss: 3.3581 - accuracy: 0.131 - ETA: 35s - loss: 3.3582 - accuracy: 0.131 - ETA: 34s - loss: 3.3579 - accuracy: 0.131 - ETA: 33s - loss: 3.3577 - accuracy: 0.131 - ETA: 33s - loss: 3.3567 - accuracy: 0.132 - ETA: 32s - loss: 3.3567 - accuracy: 0.132 - ETA: 31s - loss: 3.3569 - accuracy: 0.132 - ETA: 31s - loss: 3.3571 - accuracy: 0.131 - ETA: 30s - loss: 3.3572 - accuracy: 0.131 - ETA: 29s - loss: 3.3574 - accuracy: 0.131 - ETA: 28s - loss: 3.3578 - accuracy: 0.131 - ETA: 28s - loss: 3.3579 - accuracy: 0.131 - ETA: 27s - loss: 3.3575 - accuracy: 0.131 - ETA: 26s - loss: 3.3575 - accuracy: 0.131 - ETA: 25s - loss: 3.3578 - accuracy: 0.131 - ETA: 25s - loss: 3.3579 - accuracy: 0.131 - ETA: 24s - loss: 3.3575 - accuracy: 0.131 - ETA: 23s - loss: 3.3573 - accuracy: 0.131 - ETA: 22s - loss: 3.3576 - accuracy: 0.131 - ETA: 22s - loss: 3.3579 - accuracy: 0.131 - ETA: 21s - loss: 3.3585 - accuracy: 0.131 - ETA: 20s - loss: 3.3576 - accuracy: 0.131 - ETA: 19s - loss: 3.3573 - accuracy: 0.131 - ETA: 19s - loss: 3.3581 - accuracy: 0.131 - ETA: 18s - loss: 3.3570 - accuracy: 0.131 - ETA: 17s - loss: 3.3573 - accuracy: 0.131 - ETA: 17s - loss: 3.3572 - accuracy: 0.131 - ETA: 16s - loss: 3.3572 - accuracy: 0.131 - ETA: 15s - loss: 3.3576 - accuracy: 0.131 - ETA: 14s - loss: 3.3573 - accuracy: 0.131 - ETA: 14s - loss: 3.3571 - accuracy: 0.131 - ETA: 13s - loss: 3.3573 - accuracy: 0.131 - ETA: 12s - loss: 3.3578 - accuracy: 0.131 - ETA: 11s - loss: 3.3578 - accuracy: 0.131 - ETA: 11s - loss: 3.3577 - accuracy: 0.131 - ETA: 10s - loss: 3.3579 - accuracy: 0.131 - ETA: 9s - loss: 3.3578 - accuracy: 0.131 - ETA: 8s - loss: 3.3587 - accuracy: 0.13 - ETA: 8s - loss: 3.3581 - accuracy: 0.13 - ETA: 7s - loss: 3.3584 - accuracy: 0.13 - ETA: 6s - loss: 3.3587 - accuracy: 0.13 - ETA: 5s - loss: 3.3590 - accuracy: 0.13 - ETA: 5s - loss: 3.3587 - accuracy: 0.13 - ETA: 4s - loss: 3.3587 - accuracy: 0.13 - ETA: 3s - loss: 3.3580 - accuracy: 0.13 - ETA: 3s - loss: 3.3581 - accuracy: 0.13 - ETA: 2s - loss: 3.3583 - accuracy: 0.13 - ETA: 1s - loss: 3.3582 - accuracy: 0.13 - ETA: 0s - loss: 3.3579 - accuracy: 0.13 - ETA: 0s - loss: 3.3575 - accuracy: 0.13 - 262s 6ms/step - loss: 3.3575 - accuracy: 0.1313 - val_loss: 4.0016 - val_accuracy: 0.0362\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:13 - loss: 3.3895 - accuracy: 0.10 - ETA: 4:10 - loss: 3.3429 - accuracy: 0.13 - ETA: 4:06 - loss: 3.3053 - accuracy: 0.13 - ETA: 4:02 - loss: 3.3019 - accuracy: 0.13 - ETA: 3:59 - loss: 3.3179 - accuracy: 0.13 - ETA: 3:59 - loss: 3.3307 - accuracy: 0.13 - ETA: 3:58 - loss: 3.3541 - accuracy: 0.13 - ETA: 3:58 - loss: 3.3438 - accuracy: 0.13 - ETA: 3:59 - loss: 3.3484 - accuracy: 0.13 - ETA: 3:59 - loss: 3.3531 - accuracy: 0.13 - ETA: 3:58 - loss: 3.3526 - accuracy: 0.13 - ETA: 3:57 - loss: 3.3508 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3691 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3751 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3757 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3809 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3758 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3699 - accuracy: 0.13 - ETA: 3:53 - loss: 3.3713 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3643 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3578 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3548 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3591 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3611 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3572 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3558 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3542 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3570 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3632 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3644 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3637 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3641 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3629 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3644 - accuracy: 0.12 - ETA: 3:39 - loss: 3.3656 - accuracy: 0.12 - ETA: 3:38 - loss: 3.3620 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3602 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3610 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3606 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3617 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3590 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3578 - accuracy: 0.13 - ETA: 3:33 - loss: 3.3555 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3503 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3486 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3482 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3495 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3507 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3507 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3531 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3546 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3570 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3588 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3555 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3564 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3602 - accuracy: 0.12 - ETA: 3:23 - loss: 3.3628 - accuracy: 0.12 - ETA: 3:22 - loss: 3.3615 - accuracy: 0.12 - ETA: 3:21 - loss: 3.3628 - accuracy: 0.12 - ETA: 3:21 - loss: 3.3637 - accuracy: 0.12 - ETA: 3:20 - loss: 3.3644 - accuracy: 0.12 - ETA: 3:19 - loss: 3.3640 - accuracy: 0.12 - ETA: 3:18 - loss: 3.3630 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3626 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3624 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3616 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3604 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3593 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3581 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3600 - accuracy: 0.13 - ETA: 3:12 - loss: 3.3614 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3662 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3674 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3660 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3659 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3644 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3667 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3672 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3670 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3681 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3688 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3706 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3708 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3718 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3718 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3711 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3715 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3724 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3752 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3771 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3775 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3786 - accuracy: 0.12 - ETA: 2:56 - loss: 3.3802 - accuracy: 0.12 - ETA: 2:55 - loss: 3.3824 - accuracy: 0.12 - ETA: 2:55 - loss: 3.3823 - accuracy: 0.12 - ETA: 2:54 - loss: 3.3824 - accuracy: 0.12 - ETA: 2:53 - loss: 3.3821 - accuracy: 0.12 - ETA: 2:53 - loss: 3.3833 - accuracy: 0.12 - ETA: 2:52 - loss: 3.3828 - accuracy: 0.12 - ETA: 2:51 - loss: 3.3846 - accuracy: 0.12 - ETA: 2:50 - loss: 3.3839 - accuracy: 0.12 - ETA: 2:50 - loss: 3.3851 - accuracy: 0.12 - ETA: 2:49 - loss: 3.3842 - accuracy: 0.12 - ETA: 2:48 - loss: 3.3858 - accuracy: 0.12 - ETA: 2:48 - loss: 3.3876 - accuracy: 0.12 - ETA: 2:47 - loss: 3.3855 - accuracy: 0.12 - ETA: 2:46 - loss: 3.3858 - accuracy: 0.12 - ETA: 2:46 - loss: 3.3847 - accuracy: 0.12 - ETA: 2:45 - loss: 3.3843 - accuracy: 0.12 - ETA: 2:44 - loss: 3.3860 - accuracy: 0.12 - ETA: 2:44 - loss: 3.3852 - accuracy: 0.12 - ETA: 2:43 - loss: 3.3846 - accuracy: 0.12 - ETA: 2:42 - loss: 3.3835 - accuracy: 0.12 - ETA: 2:42 - loss: 3.3841 - accuracy: 0.12 - ETA: 2:41 - loss: 3.3849 - accuracy: 0.12 - ETA: 2:40 - loss: 3.3849 - accuracy: 0.12 - ETA: 2:39 - loss: 3.3857 - accuracy: 0.12 - ETA: 2:39 - loss: 3.3851 - accuracy: 0.12 - ETA: 2:38 - loss: 3.3839 - accuracy: 0.12 - ETA: 2:37 - loss: 3.3826 - accuracy: 0.12 - ETA: 2:36 - loss: 3.3823 - accuracy: 0.12 - ETA: 2:36 - loss: 3.3830 - accuracy: 0.12 - ETA: 2:35 - loss: 3.3827 - accuracy: 0.12 - ETA: 2:34 - loss: 3.3843 - accuracy: 0.12 - ETA: 2:34 - loss: 3.3833 - accuracy: 0.12 - ETA: 2:33 - loss: 3.3831 - accuracy: 0.12 - ETA: 2:32 - loss: 3.3822 - accuracy: 0.12 - ETA: 2:31 - loss: 3.3831 - accuracy: 0.12 - ETA: 2:30 - loss: 3.3831 - accuracy: 0.12 - ETA: 2:30 - loss: 3.3828 - accuracy: 0.12 - ETA: 2:29 - loss: 3.3824 - accuracy: 0.12 - ETA: 2:28 - loss: 3.3810 - accuracy: 0.12 - ETA: 2:27 - loss: 3.3800 - accuracy: 0.12 - ETA: 2:27 - loss: 3.3786 - accuracy: 0.12 - ETA: 2:26 - loss: 3.3793 - accuracy: 0.12 - ETA: 2:25 - loss: 3.3788 - accuracy: 0.12 - ETA: 2:24 - loss: 3.3781 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3787 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3788 - accuracy: 0.12 - ETA: 2:22 - loss: 3.3785 - accuracy: 0.12 - ETA: 2:21 - loss: 3.3782 - accuracy: 0.12 - ETA: 2:21 - loss: 3.3783 - accuracy: 0.12 - ETA: 2:20 - loss: 3.3781 - accuracy: 0.12 - ETA: 2:19 - loss: 3.3784 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3774 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3763 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3752 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3764 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3755 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3754 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3739 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3733 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3732 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3728 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3731 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3728 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3719 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3723 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3716 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3724 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3729 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3724 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3730 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3736 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3741 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3744 - accuracy: 0.12 - ETA: 2:01 - loss: 3.3741 - accuracy: 0.12 - ETA: 2:01 - loss: 3.3736 - accuracy: 0.12 - ETA: 2:00 - loss: 3.3735 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3729 - accuracy: 0.12 - ETA: 1:58 - loss: 3.3737 - accuracy: 0.12 - ETA: 1:58 - loss: 3.3734 - accuracy: 0.12 - ETA: 1:57 - loss: 3.3737 - accuracy: 0.12 - ETA: 1:56 - loss: 3.3734 - accuracy: 0.12 - ETA: 1:55 - loss: 3.3741 - accuracy: 0.12 - ETA: 1:55 - loss: 3.3744 - accuracy: 0.12 - ETA: 1:54 - loss: 3.3737 - accuracy: 0.12 - ETA: 1:53 - loss: 3.3740 - accuracy: 0.12 - ETA: 1:52 - loss: 3.3745 - accuracy: 0.12 - ETA: 1:52 - loss: 3.3753 - accuracy: 0.12 - ETA: 1:51 - loss: 3.3757 - accuracy: 0.12 - ETA: 1:50 - loss: 3.3753 - accuracy: 0.12 - ETA: 1:49 - loss: 3.3746 - accuracy: 0.12 - ETA: 1:49 - loss: 3.3746 - accuracy: 0.12 - ETA: 1:48 - loss: 3.3745 - accuracy: 0.12 - ETA: 1:47 - loss: 3.3748 - accuracy: 0.1288"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:46 - loss: 3.3749 - accuracy: 0.12 - ETA: 1:46 - loss: 3.3750 - accuracy: 0.12 - ETA: 1:45 - loss: 3.3750 - accuracy: 0.12 - ETA: 1:44 - loss: 3.3743 - accuracy: 0.12 - ETA: 1:43 - loss: 3.3736 - accuracy: 0.12 - ETA: 1:43 - loss: 3.3732 - accuracy: 0.12 - ETA: 1:42 - loss: 3.3732 - accuracy: 0.12 - ETA: 1:41 - loss: 3.3745 - accuracy: 0.12 - ETA: 1:41 - loss: 3.3743 - accuracy: 0.12 - ETA: 1:40 - loss: 3.3743 - accuracy: 0.12 - ETA: 1:39 - loss: 3.3738 - accuracy: 0.12 - ETA: 1:38 - loss: 3.3733 - accuracy: 0.12 - ETA: 1:38 - loss: 3.3736 - accuracy: 0.12 - ETA: 1:37 - loss: 3.3733 - accuracy: 0.12 - ETA: 1:36 - loss: 3.3728 - accuracy: 0.12 - ETA: 1:35 - loss: 3.3723 - accuracy: 0.12 - ETA: 1:35 - loss: 3.3718 - accuracy: 0.12 - ETA: 1:34 - loss: 3.3707 - accuracy: 0.12 - ETA: 1:33 - loss: 3.3712 - accuracy: 0.12 - ETA: 1:32 - loss: 3.3712 - accuracy: 0.12 - ETA: 1:32 - loss: 3.3712 - accuracy: 0.12 - ETA: 1:31 - loss: 3.3705 - accuracy: 0.12 - ETA: 1:30 - loss: 3.3700 - accuracy: 0.12 - ETA: 1:29 - loss: 3.3700 - accuracy: 0.12 - ETA: 1:29 - loss: 3.3696 - accuracy: 0.12 - ETA: 1:28 - loss: 3.3694 - accuracy: 0.12 - ETA: 1:27 - loss: 3.3691 - accuracy: 0.12 - ETA: 1:26 - loss: 3.3694 - accuracy: 0.12 - ETA: 1:26 - loss: 3.3686 - accuracy: 0.12 - ETA: 1:25 - loss: 3.3677 - accuracy: 0.12 - ETA: 1:24 - loss: 3.3678 - accuracy: 0.12 - ETA: 1:23 - loss: 3.3677 - accuracy: 0.12 - ETA: 1:23 - loss: 3.3674 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3667 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3664 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3666 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3670 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3678 - accuracy: 0.12 - ETA: 1:18 - loss: 3.3681 - accuracy: 0.12 - ETA: 1:17 - loss: 3.3671 - accuracy: 0.12 - ETA: 1:17 - loss: 3.3674 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3679 - accuracy: 0.12 - ETA: 1:15 - loss: 3.3678 - accuracy: 0.12 - ETA: 1:15 - loss: 3.3677 - accuracy: 0.12 - ETA: 1:14 - loss: 3.3670 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3669 - accuracy: 0.12 - ETA: 1:12 - loss: 3.3674 - accuracy: 0.12 - ETA: 1:12 - loss: 3.3675 - accuracy: 0.12 - ETA: 1:11 - loss: 3.3677 - accuracy: 0.12 - ETA: 1:10 - loss: 3.3677 - accuracy: 0.12 - ETA: 1:09 - loss: 3.3677 - accuracy: 0.12 - ETA: 1:09 - loss: 3.3680 - accuracy: 0.12 - ETA: 1:08 - loss: 3.3675 - accuracy: 0.12 - ETA: 1:07 - loss: 3.3673 - accuracy: 0.12 - ETA: 1:06 - loss: 3.3670 - accuracy: 0.12 - ETA: 1:06 - loss: 3.3692 - accuracy: 0.12 - ETA: 1:05 - loss: 3.3695 - accuracy: 0.12 - ETA: 1:04 - loss: 3.3697 - accuracy: 0.12 - ETA: 1:03 - loss: 3.3692 - accuracy: 0.12 - ETA: 1:03 - loss: 3.3689 - accuracy: 0.12 - ETA: 1:02 - loss: 3.3680 - accuracy: 0.12 - ETA: 1:01 - loss: 3.3680 - accuracy: 0.12 - ETA: 1:00 - loss: 3.3681 - accuracy: 0.12 - ETA: 1:00 - loss: 3.3676 - accuracy: 0.12 - ETA: 59s - loss: 3.3680 - accuracy: 0.1293 - ETA: 58s - loss: 3.3686 - accuracy: 0.129 - ETA: 57s - loss: 3.3678 - accuracy: 0.129 - ETA: 57s - loss: 3.3683 - accuracy: 0.129 - ETA: 56s - loss: 3.3685 - accuracy: 0.129 - ETA: 55s - loss: 3.3683 - accuracy: 0.128 - ETA: 54s - loss: 3.3681 - accuracy: 0.129 - ETA: 54s - loss: 3.3685 - accuracy: 0.128 - ETA: 53s - loss: 3.3684 - accuracy: 0.128 - ETA: 52s - loss: 3.3686 - accuracy: 0.128 - ETA: 51s - loss: 3.3687 - accuracy: 0.128 - ETA: 51s - loss: 3.3687 - accuracy: 0.128 - ETA: 50s - loss: 3.3679 - accuracy: 0.129 - ETA: 49s - loss: 3.3677 - accuracy: 0.129 - ETA: 48s - loss: 3.3673 - accuracy: 0.129 - ETA: 48s - loss: 3.3670 - accuracy: 0.129 - ETA: 47s - loss: 3.3664 - accuracy: 0.129 - ETA: 46s - loss: 3.3662 - accuracy: 0.129 - ETA: 45s - loss: 3.3660 - accuracy: 0.129 - ETA: 45s - loss: 3.3658 - accuracy: 0.129 - ETA: 44s - loss: 3.3659 - accuracy: 0.129 - ETA: 43s - loss: 3.3659 - accuracy: 0.129 - ETA: 42s - loss: 3.3667 - accuracy: 0.129 - ETA: 42s - loss: 3.3669 - accuracy: 0.129 - ETA: 41s - loss: 3.3667 - accuracy: 0.129 - ETA: 40s - loss: 3.3667 - accuracy: 0.128 - ETA: 40s - loss: 3.3668 - accuracy: 0.128 - ETA: 39s - loss: 3.3666 - accuracy: 0.129 - ETA: 38s - loss: 3.3668 - accuracy: 0.128 - ETA: 37s - loss: 3.3668 - accuracy: 0.128 - ETA: 37s - loss: 3.3674 - accuracy: 0.128 - ETA: 36s - loss: 3.3676 - accuracy: 0.128 - ETA: 35s - loss: 3.3677 - accuracy: 0.128 - ETA: 34s - loss: 3.3675 - accuracy: 0.128 - ETA: 34s - loss: 3.3670 - accuracy: 0.128 - ETA: 33s - loss: 3.3673 - accuracy: 0.128 - ETA: 32s - loss: 3.3669 - accuracy: 0.128 - ETA: 31s - loss: 3.3667 - accuracy: 0.128 - ETA: 31s - loss: 3.3669 - accuracy: 0.128 - ETA: 30s - loss: 3.3661 - accuracy: 0.129 - ETA: 29s - loss: 3.3659 - accuracy: 0.129 - ETA: 28s - loss: 3.3662 - accuracy: 0.129 - ETA: 28s - loss: 3.3660 - accuracy: 0.129 - ETA: 27s - loss: 3.3662 - accuracy: 0.129 - ETA: 26s - loss: 3.3662 - accuracy: 0.129 - ETA: 25s - loss: 3.3660 - accuracy: 0.129 - ETA: 25s - loss: 3.3656 - accuracy: 0.129 - ETA: 24s - loss: 3.3653 - accuracy: 0.129 - ETA: 23s - loss: 3.3657 - accuracy: 0.129 - ETA: 23s - loss: 3.3659 - accuracy: 0.129 - ETA: 22s - loss: 3.3658 - accuracy: 0.129 - ETA: 21s - loss: 3.3656 - accuracy: 0.129 - ETA: 20s - loss: 3.3657 - accuracy: 0.129 - ETA: 20s - loss: 3.3647 - accuracy: 0.129 - ETA: 19s - loss: 3.3648 - accuracy: 0.129 - ETA: 18s - loss: 3.3648 - accuracy: 0.129 - ETA: 17s - loss: 3.3652 - accuracy: 0.128 - ETA: 17s - loss: 3.3648 - accuracy: 0.129 - ETA: 16s - loss: 3.3642 - accuracy: 0.129 - ETA: 15s - loss: 3.3645 - accuracy: 0.129 - ETA: 14s - loss: 3.3648 - accuracy: 0.129 - ETA: 14s - loss: 3.3643 - accuracy: 0.129 - ETA: 13s - loss: 3.3640 - accuracy: 0.129 - ETA: 12s - loss: 3.3642 - accuracy: 0.129 - ETA: 11s - loss: 3.3638 - accuracy: 0.129 - ETA: 11s - loss: 3.3633 - accuracy: 0.129 - ETA: 10s - loss: 3.3630 - accuracy: 0.129 - ETA: 9s - loss: 3.3631 - accuracy: 0.129 - ETA: 8s - loss: 3.3630 - accuracy: 0.12 - ETA: 8s - loss: 3.3630 - accuracy: 0.12 - ETA: 7s - loss: 3.3626 - accuracy: 0.12 - ETA: 6s - loss: 3.3630 - accuracy: 0.12 - ETA: 5s - loss: 3.3621 - accuracy: 0.12 - ETA: 5s - loss: 3.3620 - accuracy: 0.12 - ETA: 4s - loss: 3.3619 - accuracy: 0.12 - ETA: 3s - loss: 3.3615 - accuracy: 0.12 - ETA: 3s - loss: 3.3611 - accuracy: 0.12 - ETA: 2s - loss: 3.3610 - accuracy: 0.12 - ETA: 1s - loss: 3.3608 - accuracy: 0.12 - ETA: 0s - loss: 3.3609 - accuracy: 0.12 - ETA: 0s - loss: 3.3608 - accuracy: 0.12 - 263s 6ms/step - loss: 3.3607 - accuracy: 0.1291 - val_loss: 3.9738 - val_accuracy: 0.0311\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:07 - loss: 3.2642 - accuracy: 0.16 - ETA: 4:03 - loss: 3.2494 - accuracy: 0.16 - ETA: 4:01 - loss: 3.2447 - accuracy: 0.16 - ETA: 3:59 - loss: 3.2557 - accuracy: 0.15 - ETA: 3:58 - loss: 3.2849 - accuracy: 0.14 - ETA: 3:57 - loss: 3.3321 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3605 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3510 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3419 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3410 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3478 - accuracy: 0.12 - ETA: 3:54 - loss: 3.3538 - accuracy: 0.12 - ETA: 3:54 - loss: 3.3561 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3644 - accuracy: 0.12 - ETA: 3:54 - loss: 3.3565 - accuracy: 0.12 - ETA: 3:54 - loss: 3.3626 - accuracy: 0.12 - ETA: 3:53 - loss: 3.3624 - accuracy: 0.12 - ETA: 3:53 - loss: 3.3647 - accuracy: 0.12 - ETA: 3:52 - loss: 3.3610 - accuracy: 0.12 - ETA: 3:52 - loss: 3.3622 - accuracy: 0.12 - ETA: 3:51 - loss: 3.3613 - accuracy: 0.12 - ETA: 3:50 - loss: 3.3668 - accuracy: 0.12 - ETA: 3:49 - loss: 3.3694 - accuracy: 0.12 - ETA: 3:49 - loss: 3.3723 - accuracy: 0.12 - ETA: 3:47 - loss: 3.3698 - accuracy: 0.12 - ETA: 3:46 - loss: 3.3628 - accuracy: 0.12 - ETA: 3:45 - loss: 3.3520 - accuracy: 0.12 - ETA: 3:45 - loss: 3.3493 - accuracy: 0.12 - ETA: 3:44 - loss: 3.3568 - accuracy: 0.12 - ETA: 3:43 - loss: 3.3548 - accuracy: 0.12 - ETA: 3:42 - loss: 3.3548 - accuracy: 0.12 - ETA: 3:41 - loss: 3.3518 - accuracy: 0.12 - ETA: 3:40 - loss: 3.3558 - accuracy: 0.12 - ETA: 3:40 - loss: 3.3542 - accuracy: 0.12 - ETA: 3:39 - loss: 3.3614 - accuracy: 0.12 - ETA: 3:38 - loss: 3.3642 - accuracy: 0.12 - ETA: 3:38 - loss: 3.3622 - accuracy: 0.12 - ETA: 3:37 - loss: 3.3632 - accuracy: 0.12 - ETA: 3:36 - loss: 3.3619 - accuracy: 0.12 - ETA: 3:36 - loss: 3.3623 - accuracy: 0.12 - ETA: 3:35 - loss: 3.3625 - accuracy: 0.12 - ETA: 3:34 - loss: 3.3635 - accuracy: 0.12 - ETA: 3:33 - loss: 3.3654 - accuracy: 0.12 - ETA: 3:32 - loss: 3.3666 - accuracy: 0.12 - ETA: 3:32 - loss: 3.3653 - accuracy: 0.12 - ETA: 3:31 - loss: 3.3665 - accuracy: 0.12 - ETA: 3:30 - loss: 3.3663 - accuracy: 0.12 - ETA: 3:29 - loss: 3.3662 - accuracy: 0.12 - ETA: 3:28 - loss: 3.3631 - accuracy: 0.12 - ETA: 3:27 - loss: 3.3620 - accuracy: 0.12 - ETA: 3:26 - loss: 3.3590 - accuracy: 0.12 - ETA: 3:26 - loss: 3.3597 - accuracy: 0.12 - ETA: 3:25 - loss: 3.3587 - accuracy: 0.12 - ETA: 3:24 - loss: 3.3580 - accuracy: 0.12 - ETA: 3:23 - loss: 3.3609 - accuracy: 0.12 - ETA: 3:22 - loss: 3.3595 - accuracy: 0.12 - ETA: 3:22 - loss: 3.3543 - accuracy: 0.12 - ETA: 3:21 - loss: 3.3568 - accuracy: 0.12 - ETA: 3:20 - loss: 3.3576 - accuracy: 0.12 - ETA: 3:19 - loss: 3.3578 - accuracy: 0.12 - ETA: 3:18 - loss: 3.3577 - accuracy: 0.12 - ETA: 3:18 - loss: 3.3591 - accuracy: 0.12 - ETA: 3:17 - loss: 3.3581 - accuracy: 0.12 - ETA: 3:16 - loss: 3.3543 - accuracy: 0.12 - ETA: 3:15 - loss: 3.3570 - accuracy: 0.12 - ETA: 3:15 - loss: 3.3572 - accuracy: 0.12 - ETA: 3:14 - loss: 3.3582 - accuracy: 0.12 - ETA: 3:13 - loss: 3.3568 - accuracy: 0.12 - ETA: 3:12 - loss: 3.3567 - accuracy: 0.12 - ETA: 3:12 - loss: 3.3572 - accuracy: 0.12 - ETA: 3:11 - loss: 3.3582 - accuracy: 0.12 - ETA: 3:10 - loss: 3.3602 - accuracy: 0.12 - ETA: 3:09 - loss: 3.3632 - accuracy: 0.12 - ETA: 3:09 - loss: 3.3646 - accuracy: 0.12 - ETA: 3:08 - loss: 3.3646 - accuracy: 0.12 - ETA: 3:07 - loss: 3.3640 - accuracy: 0.12 - ETA: 3:06 - loss: 3.3655 - accuracy: 0.12 - ETA: 3:05 - loss: 3.3663 - accuracy: 0.12 - ETA: 3:05 - loss: 3.3662 - accuracy: 0.12 - ETA: 3:04 - loss: 3.3646 - accuracy: 0.12 - ETA: 3:03 - loss: 3.3639 - accuracy: 0.12 - ETA: 3:03 - loss: 3.3622 - accuracy: 0.12 - ETA: 3:02 - loss: 3.3618 - accuracy: 0.12 - ETA: 3:01 - loss: 3.3608 - accuracy: 0.12 - ETA: 3:00 - loss: 3.3604 - accuracy: 0.12 - ETA: 2:59 - loss: 3.3585 - accuracy: 0.12 - ETA: 2:59 - loss: 3.3567 - accuracy: 0.12 - ETA: 2:58 - loss: 3.3574 - accuracy: 0.12 - ETA: 2:58 - loss: 3.3555 - accuracy: 0.12 - ETA: 2:57 - loss: 3.3551 - accuracy: 0.12 - ETA: 2:56 - loss: 3.3548 - accuracy: 0.12 - ETA: 2:55 - loss: 3.3556 - accuracy: 0.12 - ETA: 2:55 - loss: 3.3540 - accuracy: 0.12 - ETA: 2:54 - loss: 3.3531 - accuracy: 0.12 - ETA: 2:53 - loss: 3.3528 - accuracy: 0.12 - ETA: 2:52 - loss: 3.3539 - accuracy: 0.12 - ETA: 2:52 - loss: 3.3535 - accuracy: 0.12 - ETA: 2:51 - loss: 3.3525 - accuracy: 0.12 - ETA: 2:50 - loss: 3.3523 - accuracy: 0.12 - ETA: 2:49 - loss: 3.3523 - accuracy: 0.12 - ETA: 2:49 - loss: 3.3511 - accuracy: 0.12 - ETA: 2:48 - loss: 3.3498 - accuracy: 0.12 - ETA: 2:47 - loss: 3.3512 - accuracy: 0.12 - ETA: 2:47 - loss: 3.3501 - accuracy: 0.12 - ETA: 2:46 - loss: 3.3508 - accuracy: 0.12 - ETA: 2:45 - loss: 3.3506 - accuracy: 0.12 - ETA: 2:45 - loss: 3.3512 - accuracy: 0.12 - ETA: 2:44 - loss: 3.3515 - accuracy: 0.12 - ETA: 2:43 - loss: 3.3516 - accuracy: 0.12 - ETA: 2:42 - loss: 3.3528 - accuracy: 0.12 - ETA: 2:42 - loss: 3.3528 - accuracy: 0.12 - ETA: 2:41 - loss: 3.3533 - accuracy: 0.12 - ETA: 2:40 - loss: 3.3531 - accuracy: 0.12 - ETA: 2:39 - loss: 3.3527 - accuracy: 0.12 - ETA: 2:39 - loss: 3.3532 - accuracy: 0.12 - ETA: 2:38 - loss: 3.3522 - accuracy: 0.12 - ETA: 2:37 - loss: 3.3523 - accuracy: 0.12 - ETA: 2:36 - loss: 3.3533 - accuracy: 0.12 - ETA: 2:36 - loss: 3.3516 - accuracy: 0.12 - ETA: 2:35 - loss: 3.3502 - accuracy: 0.12 - ETA: 2:34 - loss: 3.3497 - accuracy: 0.12 - ETA: 2:33 - loss: 3.3498 - accuracy: 0.12 - ETA: 2:33 - loss: 3.3499 - accuracy: 0.12 - ETA: 2:32 - loss: 3.3495 - accuracy: 0.12 - ETA: 2:31 - loss: 3.3498 - accuracy: 0.12 - ETA: 2:30 - loss: 3.3490 - accuracy: 0.12 - ETA: 2:30 - loss: 3.3493 - accuracy: 0.12 - ETA: 2:29 - loss: 3.3502 - accuracy: 0.12 - ETA: 2:28 - loss: 3.3490 - accuracy: 0.12 - ETA: 2:28 - loss: 3.3493 - accuracy: 0.12 - ETA: 2:27 - loss: 3.3480 - accuracy: 0.12 - ETA: 2:26 - loss: 3.3477 - accuracy: 0.12 - ETA: 2:25 - loss: 3.3466 - accuracy: 0.12 - ETA: 2:25 - loss: 3.3472 - accuracy: 0.12 - ETA: 2:24 - loss: 3.3468 - accuracy: 0.12 - ETA: 2:23 - loss: 3.3452 - accuracy: 0.12 - ETA: 2:22 - loss: 3.3458 - accuracy: 0.12 - ETA: 2:22 - loss: 3.3456 - accuracy: 0.12 - ETA: 2:21 - loss: 3.3452 - accuracy: 0.12 - ETA: 2:20 - loss: 3.3449 - accuracy: 0.12 - ETA: 2:19 - loss: 3.3450 - accuracy: 0.12 - ETA: 2:19 - loss: 3.3455 - accuracy: 0.12 - ETA: 2:18 - loss: 3.3467 - accuracy: 0.12 - ETA: 2:17 - loss: 3.3469 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3472 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3476 - accuracy: 0.12 - ETA: 2:15 - loss: 3.3474 - accuracy: 0.12 - ETA: 2:14 - loss: 3.3469 - accuracy: 0.12 - ETA: 2:14 - loss: 3.3467 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3470 - accuracy: 0.12 - ETA: 2:12 - loss: 3.3480 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3488 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3500 - accuracy: 0.12 - ETA: 2:10 - loss: 3.3498 - accuracy: 0.12 - ETA: 2:09 - loss: 3.3490 - accuracy: 0.12 - ETA: 2:08 - loss: 3.3480 - accuracy: 0.12 - ETA: 2:08 - loss: 3.3486 - accuracy: 0.12 - ETA: 2:07 - loss: 3.3486 - accuracy: 0.12 - ETA: 2:06 - loss: 3.3498 - accuracy: 0.12 - ETA: 2:05 - loss: 3.3506 - accuracy: 0.12 - ETA: 2:05 - loss: 3.3509 - accuracy: 0.12 - ETA: 2:04 - loss: 3.3505 - accuracy: 0.12 - ETA: 2:03 - loss: 3.3506 - accuracy: 0.12 - ETA: 2:02 - loss: 3.3510 - accuracy: 0.12 - ETA: 2:02 - loss: 3.3513 - accuracy: 0.12 - ETA: 2:01 - loss: 3.3525 - accuracy: 0.12 - ETA: 2:00 - loss: 3.3540 - accuracy: 0.12 - ETA: 1:59 - loss: 3.3533 - accuracy: 0.12 - ETA: 1:59 - loss: 3.3539 - accuracy: 0.12 - ETA: 1:58 - loss: 3.3534 - accuracy: 0.12 - ETA: 1:57 - loss: 3.3534 - accuracy: 0.12 - ETA: 1:57 - loss: 3.3542 - accuracy: 0.12 - ETA: 1:56 - loss: 3.3543 - accuracy: 0.12 - ETA: 1:55 - loss: 3.3548 - accuracy: 0.12 - ETA: 1:54 - loss: 3.3559 - accuracy: 0.12 - ETA: 1:54 - loss: 3.3579 - accuracy: 0.12 - ETA: 1:53 - loss: 3.3574 - accuracy: 0.12 - ETA: 1:52 - loss: 3.3587 - accuracy: 0.12 - ETA: 1:51 - loss: 3.3591 - accuracy: 0.12 - ETA: 1:51 - loss: 3.3592 - accuracy: 0.12 - ETA: 1:50 - loss: 3.3600 - accuracy: 0.12 - ETA: 1:49 - loss: 3.3614 - accuracy: 0.12 - ETA: 1:48 - loss: 3.3616 - accuracy: 0.12 - ETA: 1:48 - loss: 3.3617 - accuracy: 0.12 - ETA: 1:47 - loss: 3.3628 - accuracy: 0.12 - ETA: 1:46 - loss: 3.3620 - accuracy: 0.1274"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3617 - accuracy: 0.12 - ETA: 1:45 - loss: 3.3622 - accuracy: 0.12 - ETA: 1:44 - loss: 3.3627 - accuracy: 0.12 - ETA: 1:43 - loss: 3.3621 - accuracy: 0.12 - ETA: 1:43 - loss: 3.3625 - accuracy: 0.12 - ETA: 1:42 - loss: 3.3625 - accuracy: 0.12 - ETA: 1:41 - loss: 3.3619 - accuracy: 0.12 - ETA: 1:40 - loss: 3.3616 - accuracy: 0.12 - ETA: 1:40 - loss: 3.3613 - accuracy: 0.12 - ETA: 1:39 - loss: 3.3617 - accuracy: 0.12 - ETA: 1:38 - loss: 3.3620 - accuracy: 0.12 - ETA: 1:38 - loss: 3.3617 - accuracy: 0.12 - ETA: 1:37 - loss: 3.3621 - accuracy: 0.12 - ETA: 1:36 - loss: 3.3619 - accuracy: 0.12 - ETA: 1:35 - loss: 3.3614 - accuracy: 0.12 - ETA: 1:35 - loss: 3.3607 - accuracy: 0.12 - ETA: 1:34 - loss: 3.3606 - accuracy: 0.12 - ETA: 1:33 - loss: 3.3606 - accuracy: 0.12 - ETA: 1:32 - loss: 3.3604 - accuracy: 0.12 - ETA: 1:32 - loss: 3.3594 - accuracy: 0.12 - ETA: 1:31 - loss: 3.3595 - accuracy: 0.12 - ETA: 1:30 - loss: 3.3586 - accuracy: 0.12 - ETA: 1:30 - loss: 3.3591 - accuracy: 0.12 - ETA: 1:29 - loss: 3.3586 - accuracy: 0.12 - ETA: 1:28 - loss: 3.3585 - accuracy: 0.12 - ETA: 1:27 - loss: 3.3585 - accuracy: 0.12 - ETA: 1:27 - loss: 3.3594 - accuracy: 0.12 - ETA: 1:26 - loss: 3.3597 - accuracy: 0.12 - ETA: 1:25 - loss: 3.3593 - accuracy: 0.12 - ETA: 1:24 - loss: 3.3597 - accuracy: 0.12 - ETA: 1:24 - loss: 3.3596 - accuracy: 0.12 - ETA: 1:23 - loss: 3.3603 - accuracy: 0.12 - ETA: 1:22 - loss: 3.3596 - accuracy: 0.12 - ETA: 1:21 - loss: 3.3591 - accuracy: 0.12 - ETA: 1:21 - loss: 3.3595 - accuracy: 0.12 - ETA: 1:20 - loss: 3.3593 - accuracy: 0.12 - ETA: 1:19 - loss: 3.3588 - accuracy: 0.12 - ETA: 1:18 - loss: 3.3587 - accuracy: 0.12 - ETA: 1:18 - loss: 3.3590 - accuracy: 0.12 - ETA: 1:17 - loss: 3.3587 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3574 - accuracy: 0.12 - ETA: 1:15 - loss: 3.3575 - accuracy: 0.12 - ETA: 1:15 - loss: 3.3569 - accuracy: 0.12 - ETA: 1:14 - loss: 3.3569 - accuracy: 0.12 - ETA: 1:13 - loss: 3.3569 - accuracy: 0.12 - ETA: 1:13 - loss: 3.3570 - accuracy: 0.12 - ETA: 1:12 - loss: 3.3563 - accuracy: 0.12 - ETA: 1:11 - loss: 3.3568 - accuracy: 0.12 - ETA: 1:10 - loss: 3.3571 - accuracy: 0.12 - ETA: 1:10 - loss: 3.3570 - accuracy: 0.12 - ETA: 1:09 - loss: 3.3573 - accuracy: 0.12 - ETA: 1:08 - loss: 3.3562 - accuracy: 0.12 - ETA: 1:07 - loss: 3.3557 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3553 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3547 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3549 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3550 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3544 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3536 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3533 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3531 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3530 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3533 - accuracy: 0.13 - ETA: 59s - loss: 3.3535 - accuracy: 0.1309 - ETA: 59s - loss: 3.3533 - accuracy: 0.130 - ETA: 58s - loss: 3.3526 - accuracy: 0.130 - ETA: 57s - loss: 3.3517 - accuracy: 0.131 - ETA: 56s - loss: 3.3517 - accuracy: 0.131 - ETA: 56s - loss: 3.3520 - accuracy: 0.130 - ETA: 55s - loss: 3.3523 - accuracy: 0.130 - ETA: 54s - loss: 3.3521 - accuracy: 0.131 - ETA: 53s - loss: 3.3521 - accuracy: 0.131 - ETA: 53s - loss: 3.3513 - accuracy: 0.131 - ETA: 52s - loss: 3.3517 - accuracy: 0.131 - ETA: 51s - loss: 3.3521 - accuracy: 0.131 - ETA: 50s - loss: 3.3523 - accuracy: 0.131 - ETA: 50s - loss: 3.3513 - accuracy: 0.131 - ETA: 49s - loss: 3.3519 - accuracy: 0.131 - ETA: 48s - loss: 3.3512 - accuracy: 0.131 - ETA: 47s - loss: 3.3507 - accuracy: 0.131 - ETA: 47s - loss: 3.3506 - accuracy: 0.131 - ETA: 46s - loss: 3.3510 - accuracy: 0.131 - ETA: 45s - loss: 3.3513 - accuracy: 0.131 - ETA: 44s - loss: 3.3514 - accuracy: 0.131 - ETA: 44s - loss: 3.3517 - accuracy: 0.131 - ETA: 43s - loss: 3.3510 - accuracy: 0.131 - ETA: 42s - loss: 3.3507 - accuracy: 0.131 - ETA: 42s - loss: 3.3507 - accuracy: 0.131 - ETA: 41s - loss: 3.3510 - accuracy: 0.131 - ETA: 40s - loss: 3.3511 - accuracy: 0.131 - ETA: 39s - loss: 3.3521 - accuracy: 0.131 - ETA: 39s - loss: 3.3527 - accuracy: 0.131 - ETA: 38s - loss: 3.3525 - accuracy: 0.131 - ETA: 37s - loss: 3.3527 - accuracy: 0.131 - ETA: 36s - loss: 3.3528 - accuracy: 0.131 - ETA: 36s - loss: 3.3524 - accuracy: 0.131 - ETA: 35s - loss: 3.3522 - accuracy: 0.131 - ETA: 34s - loss: 3.3518 - accuracy: 0.131 - ETA: 33s - loss: 3.3517 - accuracy: 0.131 - ETA: 33s - loss: 3.3511 - accuracy: 0.131 - ETA: 32s - loss: 3.3511 - accuracy: 0.131 - ETA: 31s - loss: 3.3505 - accuracy: 0.131 - ETA: 31s - loss: 3.3502 - accuracy: 0.132 - ETA: 30s - loss: 3.3501 - accuracy: 0.132 - ETA: 29s - loss: 3.3504 - accuracy: 0.132 - ETA: 28s - loss: 3.3504 - accuracy: 0.132 - ETA: 28s - loss: 3.3508 - accuracy: 0.132 - ETA: 27s - loss: 3.3510 - accuracy: 0.132 - ETA: 26s - loss: 3.3506 - accuracy: 0.132 - ETA: 25s - loss: 3.3511 - accuracy: 0.132 - ETA: 25s - loss: 3.3511 - accuracy: 0.132 - ETA: 24s - loss: 3.3511 - accuracy: 0.132 - ETA: 23s - loss: 3.3515 - accuracy: 0.132 - ETA: 22s - loss: 3.3519 - accuracy: 0.132 - ETA: 22s - loss: 3.3519 - accuracy: 0.131 - ETA: 21s - loss: 3.3520 - accuracy: 0.131 - ETA: 20s - loss: 3.3516 - accuracy: 0.131 - ETA: 19s - loss: 3.3509 - accuracy: 0.132 - ETA: 19s - loss: 3.3512 - accuracy: 0.132 - ETA: 18s - loss: 3.3515 - accuracy: 0.131 - ETA: 17s - loss: 3.3519 - accuracy: 0.131 - ETA: 17s - loss: 3.3515 - accuracy: 0.131 - ETA: 16s - loss: 3.3520 - accuracy: 0.131 - ETA: 15s - loss: 3.3524 - accuracy: 0.131 - ETA: 14s - loss: 3.3520 - accuracy: 0.131 - ETA: 14s - loss: 3.3518 - accuracy: 0.131 - ETA: 13s - loss: 3.3517 - accuracy: 0.131 - ETA: 12s - loss: 3.3521 - accuracy: 0.131 - ETA: 11s - loss: 3.3523 - accuracy: 0.131 - ETA: 11s - loss: 3.3523 - accuracy: 0.131 - ETA: 10s - loss: 3.3526 - accuracy: 0.131 - ETA: 9s - loss: 3.3527 - accuracy: 0.131 - ETA: 8s - loss: 3.3524 - accuracy: 0.13 - ETA: 8s - loss: 3.3525 - accuracy: 0.13 - ETA: 7s - loss: 3.3526 - accuracy: 0.13 - ETA: 6s - loss: 3.3531 - accuracy: 0.13 - ETA: 5s - loss: 3.3534 - accuracy: 0.13 - ETA: 5s - loss: 3.3535 - accuracy: 0.13 - ETA: 4s - loss: 3.3537 - accuracy: 0.13 - ETA: 3s - loss: 3.3533 - accuracy: 0.13 - ETA: 3s - loss: 3.3532 - accuracy: 0.13 - ETA: 2s - loss: 3.3531 - accuracy: 0.13 - ETA: 1s - loss: 3.3534 - accuracy: 0.13 - ETA: 0s - loss: 3.3536 - accuracy: 0.13 - ETA: 0s - loss: 3.3536 - accuracy: 0.13 - 262s 6ms/step - loss: 3.3537 - accuracy: 0.1311 - val_loss: 4.0952 - val_accuracy: 0.0311\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:01 - loss: 3.4063 - accuracy: 0.12 - ETA: 4:02 - loss: 3.3266 - accuracy: 0.13 - ETA: 4:01 - loss: 3.3674 - accuracy: 0.12 - ETA: 4:00 - loss: 3.3285 - accuracy: 0.13 - ETA: 3:57 - loss: 3.2823 - accuracy: 0.14 - ETA: 3:57 - loss: 3.3058 - accuracy: 0.13 - ETA: 3:56 - loss: 3.2942 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3081 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3177 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3146 - accuracy: 0.14 - ETA: 3:53 - loss: 3.3258 - accuracy: 0.13 - ETA: 3:53 - loss: 3.3342 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3396 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3408 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3366 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3471 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3364 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3357 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3456 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3461 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3473 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3441 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3373 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3419 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3398 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3459 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3483 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3504 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3507 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3519 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3587 - accuracy: 0.12 - ETA: 3:41 - loss: 3.3567 - accuracy: 0.12 - ETA: 3:40 - loss: 3.3555 - accuracy: 0.12 - ETA: 3:39 - loss: 3.3568 - accuracy: 0.12 - ETA: 3:38 - loss: 3.3587 - accuracy: 0.12 - ETA: 3:38 - loss: 3.3589 - accuracy: 0.12 - ETA: 3:37 - loss: 3.3620 - accuracy: 0.12 - ETA: 3:36 - loss: 3.3612 - accuracy: 0.12 - ETA: 3:36 - loss: 3.3640 - accuracy: 0.12 - ETA: 3:35 - loss: 3.3666 - accuracy: 0.12 - ETA: 3:34 - loss: 3.3663 - accuracy: 0.12 - ETA: 3:33 - loss: 3.3647 - accuracy: 0.12 - ETA: 3:33 - loss: 3.3611 - accuracy: 0.12 - ETA: 3:32 - loss: 3.3595 - accuracy: 0.12 - ETA: 3:31 - loss: 3.3598 - accuracy: 0.12 - ETA: 3:30 - loss: 3.3599 - accuracy: 0.12 - ETA: 3:29 - loss: 3.3588 - accuracy: 0.12 - ETA: 3:28 - loss: 3.3547 - accuracy: 0.12 - ETA: 3:28 - loss: 3.3534 - accuracy: 0.12 - ETA: 3:28 - loss: 3.3532 - accuracy: 0.12 - ETA: 3:27 - loss: 3.3501 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3515 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3493 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3491 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3496 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3511 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3537 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3566 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3561 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3559 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3556 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3564 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3559 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3579 - accuracy: 0.12 - ETA: 3:17 - loss: 3.3581 - accuracy: 0.12 - ETA: 3:16 - loss: 3.3596 - accuracy: 0.12 - ETA: 3:15 - loss: 3.3585 - accuracy: 0.12 - ETA: 3:14 - loss: 3.3583 - accuracy: 0.12 - ETA: 3:14 - loss: 3.3604 - accuracy: 0.12 - ETA: 3:13 - loss: 3.3587 - accuracy: 0.12 - ETA: 3:12 - loss: 3.3601 - accuracy: 0.12 - ETA: 3:11 - loss: 3.3609 - accuracy: 0.12 - ETA: 3:10 - loss: 3.3614 - accuracy: 0.12 - ETA: 3:10 - loss: 3.3595 - accuracy: 0.12 - ETA: 3:09 - loss: 3.3576 - accuracy: 0.12 - ETA: 3:08 - loss: 3.3584 - accuracy: 0.12 - ETA: 3:08 - loss: 3.3604 - accuracy: 0.12 - ETA: 3:07 - loss: 3.3606 - accuracy: 0.12 - ETA: 3:06 - loss: 3.3613 - accuracy: 0.12 - ETA: 3:05 - loss: 3.3619 - accuracy: 0.12 - ETA: 3:05 - loss: 3.3653 - accuracy: 0.12 - ETA: 3:04 - loss: 3.3653 - accuracy: 0.12 - ETA: 3:03 - loss: 3.3657 - accuracy: 0.12 - ETA: 3:02 - loss: 3.3690 - accuracy: 0.12 - ETA: 3:01 - loss: 3.3700 - accuracy: 0.12 - ETA: 3:01 - loss: 3.3706 - accuracy: 0.12 - ETA: 3:00 - loss: 3.3693 - accuracy: 0.12 - ETA: 2:59 - loss: 3.3698 - accuracy: 0.12 - ETA: 2:58 - loss: 3.3700 - accuracy: 0.12 - ETA: 2:58 - loss: 3.3695 - accuracy: 0.12 - ETA: 2:57 - loss: 3.3681 - accuracy: 0.12 - ETA: 2:56 - loss: 3.3705 - accuracy: 0.12 - ETA: 2:56 - loss: 3.3717 - accuracy: 0.12 - ETA: 2:55 - loss: 3.3727 - accuracy: 0.12 - ETA: 2:54 - loss: 3.3715 - accuracy: 0.12 - ETA: 2:53 - loss: 3.3716 - accuracy: 0.12 - ETA: 2:53 - loss: 3.3720 - accuracy: 0.12 - ETA: 2:52 - loss: 3.3736 - accuracy: 0.12 - ETA: 2:51 - loss: 3.3726 - accuracy: 0.12 - ETA: 2:50 - loss: 3.3729 - accuracy: 0.12 - ETA: 2:50 - loss: 3.3725 - accuracy: 0.12 - ETA: 2:49 - loss: 3.3738 - accuracy: 0.12 - ETA: 2:48 - loss: 3.3736 - accuracy: 0.12 - ETA: 2:47 - loss: 3.3733 - accuracy: 0.12 - ETA: 2:47 - loss: 3.3734 - accuracy: 0.12 - ETA: 2:46 - loss: 3.3736 - accuracy: 0.12 - ETA: 2:45 - loss: 3.3743 - accuracy: 0.12 - ETA: 2:45 - loss: 3.3738 - accuracy: 0.12 - ETA: 2:44 - loss: 3.3739 - accuracy: 0.12 - ETA: 2:43 - loss: 3.3716 - accuracy: 0.12 - ETA: 2:43 - loss: 3.3734 - accuracy: 0.12 - ETA: 2:42 - loss: 3.3728 - accuracy: 0.12 - ETA: 2:41 - loss: 3.3720 - accuracy: 0.12 - ETA: 2:40 - loss: 3.3725 - accuracy: 0.12 - ETA: 2:40 - loss: 3.3733 - accuracy: 0.12 - ETA: 2:39 - loss: 3.3731 - accuracy: 0.12 - ETA: 2:38 - loss: 3.3736 - accuracy: 0.12 - ETA: 2:37 - loss: 3.3739 - accuracy: 0.12 - ETA: 2:37 - loss: 3.3738 - accuracy: 0.12 - ETA: 2:36 - loss: 3.3727 - accuracy: 0.12 - ETA: 2:35 - loss: 3.3718 - accuracy: 0.12 - ETA: 2:34 - loss: 3.3721 - accuracy: 0.12 - ETA: 2:34 - loss: 3.3714 - accuracy: 0.12 - ETA: 2:33 - loss: 3.3701 - accuracy: 0.12 - ETA: 2:32 - loss: 3.3706 - accuracy: 0.12 - ETA: 2:31 - loss: 3.3709 - accuracy: 0.12 - ETA: 2:31 - loss: 3.3714 - accuracy: 0.12 - ETA: 2:30 - loss: 3.3725 - accuracy: 0.12 - ETA: 2:29 - loss: 3.3740 - accuracy: 0.12 - ETA: 2:28 - loss: 3.3737 - accuracy: 0.12 - ETA: 2:28 - loss: 3.3736 - accuracy: 0.12 - ETA: 2:27 - loss: 3.3737 - accuracy: 0.12 - ETA: 2:26 - loss: 3.3749 - accuracy: 0.12 - ETA: 2:25 - loss: 3.3753 - accuracy: 0.12 - ETA: 2:25 - loss: 3.3740 - accuracy: 0.12 - ETA: 2:24 - loss: 3.3738 - accuracy: 0.12 - ETA: 2:23 - loss: 3.3733 - accuracy: 0.12 - ETA: 2:22 - loss: 3.3734 - accuracy: 0.12 - ETA: 2:22 - loss: 3.3736 - accuracy: 0.12 - ETA: 2:21 - loss: 3.3731 - accuracy: 0.12 - ETA: 2:20 - loss: 3.3733 - accuracy: 0.12 - ETA: 2:19 - loss: 3.3737 - accuracy: 0.12 - ETA: 2:19 - loss: 3.3742 - accuracy: 0.12 - ETA: 2:18 - loss: 3.3759 - accuracy: 0.12 - ETA: 2:17 - loss: 3.3760 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3759 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3749 - accuracy: 0.12 - ETA: 2:15 - loss: 3.3760 - accuracy: 0.12 - ETA: 2:14 - loss: 3.3769 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3774 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3778 - accuracy: 0.12 - ETA: 2:12 - loss: 3.3769 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3764 - accuracy: 0.12 - ETA: 2:10 - loss: 3.3757 - accuracy: 0.12 - ETA: 2:09 - loss: 3.3755 - accuracy: 0.12 - ETA: 2:09 - loss: 3.3750 - accuracy: 0.12 - ETA: 2:08 - loss: 3.3756 - accuracy: 0.12 - ETA: 2:07 - loss: 3.3761 - accuracy: 0.12 - ETA: 2:07 - loss: 3.3748 - accuracy: 0.12 - ETA: 2:06 - loss: 3.3739 - accuracy: 0.12 - ETA: 2:05 - loss: 3.3733 - accuracy: 0.12 - ETA: 2:04 - loss: 3.3730 - accuracy: 0.12 - ETA: 2:04 - loss: 3.3738 - accuracy: 0.12 - ETA: 2:03 - loss: 3.3735 - accuracy: 0.12 - ETA: 2:02 - loss: 3.3734 - accuracy: 0.12 - ETA: 2:01 - loss: 3.3728 - accuracy: 0.12 - ETA: 2:01 - loss: 3.3733 - accuracy: 0.12 - ETA: 2:00 - loss: 3.3731 - accuracy: 0.12 - ETA: 1:59 - loss: 3.3723 - accuracy: 0.12 - ETA: 1:59 - loss: 3.3716 - accuracy: 0.12 - ETA: 1:58 - loss: 3.3712 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3708 - accuracy: 0.12 - ETA: 1:56 - loss: 3.3715 - accuracy: 0.12 - ETA: 1:56 - loss: 3.3709 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3705 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3725 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3719 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3713 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3717 - accuracy: 0.12 - ETA: 1:51 - loss: 3.3714 - accuracy: 0.12 - ETA: 1:50 - loss: 3.3719 - accuracy: 0.12 - ETA: 1:50 - loss: 3.3711 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3707 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3700 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3686 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3689 - accuracy: 0.1306"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:46 - loss: 3.3692 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3688 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3687 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3690 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3690 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3683 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3682 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3688 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3687 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3676 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3681 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3682 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3676 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3686 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3682 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3687 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3692 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3692 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3693 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3692 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3703 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3700 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3703 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3705 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3703 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3704 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3697 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3702 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3705 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3711 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3714 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3716 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3716 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3723 - accuracy: 0.12 - ETA: 1:21 - loss: 3.3722 - accuracy: 0.12 - ETA: 1:20 - loss: 3.3717 - accuracy: 0.12 - ETA: 1:19 - loss: 3.3715 - accuracy: 0.12 - ETA: 1:19 - loss: 3.3719 - accuracy: 0.12 - ETA: 1:18 - loss: 3.3718 - accuracy: 0.12 - ETA: 1:17 - loss: 3.3715 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3718 - accuracy: 0.12 - ETA: 1:16 - loss: 3.3714 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3718 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3719 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3719 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3719 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3724 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3724 - accuracy: 0.12 - ETA: 1:10 - loss: 3.3722 - accuracy: 0.12 - ETA: 1:10 - loss: 3.3726 - accuracy: 0.12 - ETA: 1:09 - loss: 3.3731 - accuracy: 0.12 - ETA: 1:08 - loss: 3.3736 - accuracy: 0.12 - ETA: 1:08 - loss: 3.3737 - accuracy: 0.12 - ETA: 1:07 - loss: 3.3732 - accuracy: 0.12 - ETA: 1:06 - loss: 3.3727 - accuracy: 0.12 - ETA: 1:05 - loss: 3.3727 - accuracy: 0.12 - ETA: 1:05 - loss: 3.3723 - accuracy: 0.12 - ETA: 1:04 - loss: 3.3722 - accuracy: 0.12 - ETA: 1:03 - loss: 3.3727 - accuracy: 0.12 - ETA: 1:02 - loss: 3.3733 - accuracy: 0.12 - ETA: 1:02 - loss: 3.3730 - accuracy: 0.12 - ETA: 1:01 - loss: 3.3730 - accuracy: 0.12 - ETA: 1:00 - loss: 3.3720 - accuracy: 0.12 - ETA: 59s - loss: 3.3716 - accuracy: 0.1297 - ETA: 59s - loss: 3.3710 - accuracy: 0.129 - ETA: 58s - loss: 3.3708 - accuracy: 0.129 - ETA: 57s - loss: 3.3712 - accuracy: 0.129 - ETA: 57s - loss: 3.3702 - accuracy: 0.129 - ETA: 56s - loss: 3.3707 - accuracy: 0.129 - ETA: 55s - loss: 3.3703 - accuracy: 0.129 - ETA: 54s - loss: 3.3701 - accuracy: 0.130 - ETA: 54s - loss: 3.3695 - accuracy: 0.130 - ETA: 53s - loss: 3.3691 - accuracy: 0.130 - ETA: 52s - loss: 3.3684 - accuracy: 0.130 - ETA: 51s - loss: 3.3683 - accuracy: 0.130 - ETA: 51s - loss: 3.3682 - accuracy: 0.130 - ETA: 50s - loss: 3.3688 - accuracy: 0.130 - ETA: 49s - loss: 3.3684 - accuracy: 0.130 - ETA: 48s - loss: 3.3687 - accuracy: 0.130 - ETA: 48s - loss: 3.3690 - accuracy: 0.130 - ETA: 47s - loss: 3.3688 - accuracy: 0.130 - ETA: 46s - loss: 3.3686 - accuracy: 0.130 - ETA: 45s - loss: 3.3686 - accuracy: 0.130 - ETA: 45s - loss: 3.3685 - accuracy: 0.129 - ETA: 44s - loss: 3.3692 - accuracy: 0.129 - ETA: 43s - loss: 3.3686 - accuracy: 0.129 - ETA: 42s - loss: 3.3679 - accuracy: 0.130 - ETA: 42s - loss: 3.3678 - accuracy: 0.130 - ETA: 41s - loss: 3.3675 - accuracy: 0.130 - ETA: 40s - loss: 3.3665 - accuracy: 0.130 - ETA: 39s - loss: 3.3665 - accuracy: 0.130 - ETA: 39s - loss: 3.3659 - accuracy: 0.130 - ETA: 38s - loss: 3.3655 - accuracy: 0.130 - ETA: 37s - loss: 3.3654 - accuracy: 0.130 - ETA: 37s - loss: 3.3658 - accuracy: 0.130 - ETA: 36s - loss: 3.3660 - accuracy: 0.130 - ETA: 35s - loss: 3.3658 - accuracy: 0.130 - ETA: 34s - loss: 3.3660 - accuracy: 0.130 - ETA: 34s - loss: 3.3657 - accuracy: 0.130 - ETA: 33s - loss: 3.3657 - accuracy: 0.130 - ETA: 32s - loss: 3.3654 - accuracy: 0.130 - ETA: 31s - loss: 3.3655 - accuracy: 0.130 - ETA: 31s - loss: 3.3655 - accuracy: 0.130 - ETA: 30s - loss: 3.3653 - accuracy: 0.130 - ETA: 29s - loss: 3.3657 - accuracy: 0.130 - ETA: 28s - loss: 3.3655 - accuracy: 0.130 - ETA: 28s - loss: 3.3657 - accuracy: 0.130 - ETA: 27s - loss: 3.3655 - accuracy: 0.130 - ETA: 26s - loss: 3.3658 - accuracy: 0.130 - ETA: 25s - loss: 3.3664 - accuracy: 0.130 - ETA: 25s - loss: 3.3661 - accuracy: 0.130 - ETA: 24s - loss: 3.3654 - accuracy: 0.130 - ETA: 23s - loss: 3.3659 - accuracy: 0.130 - ETA: 23s - loss: 3.3655 - accuracy: 0.130 - ETA: 22s - loss: 3.3651 - accuracy: 0.130 - ETA: 21s - loss: 3.3649 - accuracy: 0.130 - ETA: 20s - loss: 3.3648 - accuracy: 0.130 - ETA: 20s - loss: 3.3637 - accuracy: 0.130 - ETA: 19s - loss: 3.3630 - accuracy: 0.131 - ETA: 18s - loss: 3.3627 - accuracy: 0.131 - ETA: 17s - loss: 3.3623 - accuracy: 0.131 - ETA: 17s - loss: 3.3623 - accuracy: 0.131 - ETA: 16s - loss: 3.3619 - accuracy: 0.131 - ETA: 15s - loss: 3.3618 - accuracy: 0.131 - ETA: 14s - loss: 3.3611 - accuracy: 0.131 - ETA: 14s - loss: 3.3612 - accuracy: 0.131 - ETA: 13s - loss: 3.3614 - accuracy: 0.131 - ETA: 12s - loss: 3.3614 - accuracy: 0.131 - ETA: 11s - loss: 3.3616 - accuracy: 0.131 - ETA: 11s - loss: 3.3617 - accuracy: 0.131 - ETA: 10s - loss: 3.3618 - accuracy: 0.131 - ETA: 9s - loss: 3.3622 - accuracy: 0.131 - ETA: 8s - loss: 3.3631 - accuracy: 0.13 - ETA: 8s - loss: 3.3635 - accuracy: 0.13 - ETA: 7s - loss: 3.3641 - accuracy: 0.13 - ETA: 6s - loss: 3.3635 - accuracy: 0.13 - ETA: 5s - loss: 3.3631 - accuracy: 0.13 - ETA: 5s - loss: 3.3631 - accuracy: 0.13 - ETA: 4s - loss: 3.3633 - accuracy: 0.13 - ETA: 3s - loss: 3.3635 - accuracy: 0.13 - ETA: 3s - loss: 3.3629 - accuracy: 0.13 - ETA: 2s - loss: 3.3630 - accuracy: 0.13 - ETA: 1s - loss: 3.3635 - accuracy: 0.13 - ETA: 0s - loss: 3.3640 - accuracy: 0.13 - ETA: 0s - loss: 3.3639 - accuracy: 0.13 - 263s 6ms/step - loss: 3.3639 - accuracy: 0.1309 - val_loss: 3.9512 - val_accuracy: 0.0334\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:10 - loss: 3.3563 - accuracy: 0.13 - ETA: 4:07 - loss: 3.3915 - accuracy: 0.13 - ETA: 4:03 - loss: 3.3548 - accuracy: 0.12 - ETA: 3:59 - loss: 3.3735 - accuracy: 0.11 - ETA: 3:59 - loss: 3.3683 - accuracy: 0.12 - ETA: 3:59 - loss: 3.3590 - accuracy: 0.13 - ETA: 3:58 - loss: 3.3469 - accuracy: 0.13 - ETA: 3:57 - loss: 3.3115 - accuracy: 0.14 - ETA: 3:58 - loss: 3.3017 - accuracy: 0.14 - ETA: 3:57 - loss: 3.3161 - accuracy: 0.14 - ETA: 3:56 - loss: 3.3280 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3360 - accuracy: 0.14 - ETA: 3:54 - loss: 3.3233 - accuracy: 0.14 - ETA: 3:53 - loss: 3.3243 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3139 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3210 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3219 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3149 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3069 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3082 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3183 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3141 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3123 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3212 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3192 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3231 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3307 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3349 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3360 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3367 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3398 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3402 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3351 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3394 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3451 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3496 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3534 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3523 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3522 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3510 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3492 - accuracy: 0.13 - ETA: 3:33 - loss: 3.3491 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3457 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3465 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3453 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3464 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3470 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3439 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3429 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3423 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3370 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3380 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3396 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3406 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3382 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3381 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3362 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3338 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3345 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3326 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3308 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3277 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3299 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3323 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3284 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3290 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3266 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3282 - accuracy: 0.13 - ETA: 3:12 - loss: 3.3292 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3303 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3295 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3306 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3242 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3231 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3251 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3278 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3289 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3284 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3266 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3259 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3245 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3259 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3250 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3253 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3256 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3258 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3264 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3255 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3253 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3252 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3239 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3245 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3241 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3232 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3230 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3231 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3243 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3243 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3235 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3250 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3257 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3255 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3263 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3238 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3232 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3242 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3229 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3244 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3236 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3239 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3221 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3233 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3221 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3220 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3219 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3233 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3235 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3223 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3228 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3223 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3223 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3222 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3224 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3232 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3239 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3238 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3226 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3212 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3213 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3214 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3212 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3225 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3226 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3220 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3211 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3209 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3202 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3206 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3206 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3198 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3204 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3216 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3214 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3207 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3212 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3220 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3215 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3197 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3199 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3186 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3165 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3179 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3178 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3182 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3180 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3173 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3179 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3174 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3177 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3175 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3168 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3177 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3183 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3183 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3192 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3200 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3188 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3194 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3192 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3203 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3199 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3185 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3186 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3184 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3190 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3191 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3198 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3193 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3191 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3194 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3197 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3198 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3206 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3211 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3213 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3217 - accuracy: 0.1376"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:46 - loss: 3.3218 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3230 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3228 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3236 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3237 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3240 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3237 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3233 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3230 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3228 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3222 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3220 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3223 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3226 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3227 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3231 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3228 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3229 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3223 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3220 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3212 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3207 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3209 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3210 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3205 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3206 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3203 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3199 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3205 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3212 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3212 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3211 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3203 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3205 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3217 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3215 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3220 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3206 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3210 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3203 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3208 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3205 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3198 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3201 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3201 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3201 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3198 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3198 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3203 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3201 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3203 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3204 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3202 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3197 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3196 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3193 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3195 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3198 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3197 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3196 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3190 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3194 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3205 - accuracy: 0.13 - ETA: 59s - loss: 3.3209 - accuracy: 0.1370 - ETA: 58s - loss: 3.3204 - accuracy: 0.137 - ETA: 58s - loss: 3.3204 - accuracy: 0.137 - ETA: 57s - loss: 3.3210 - accuracy: 0.137 - ETA: 56s - loss: 3.3218 - accuracy: 0.137 - ETA: 56s - loss: 3.3221 - accuracy: 0.136 - ETA: 55s - loss: 3.3226 - accuracy: 0.136 - ETA: 54s - loss: 3.3227 - accuracy: 0.136 - ETA: 53s - loss: 3.3224 - accuracy: 0.136 - ETA: 53s - loss: 3.3224 - accuracy: 0.136 - ETA: 52s - loss: 3.3229 - accuracy: 0.136 - ETA: 51s - loss: 3.3232 - accuracy: 0.136 - ETA: 50s - loss: 3.3235 - accuracy: 0.136 - ETA: 50s - loss: 3.3241 - accuracy: 0.136 - ETA: 49s - loss: 3.3246 - accuracy: 0.136 - ETA: 48s - loss: 3.3251 - accuracy: 0.136 - ETA: 47s - loss: 3.3257 - accuracy: 0.136 - ETA: 47s - loss: 3.3259 - accuracy: 0.136 - ETA: 46s - loss: 3.3268 - accuracy: 0.136 - ETA: 45s - loss: 3.3270 - accuracy: 0.136 - ETA: 44s - loss: 3.3263 - accuracy: 0.136 - ETA: 44s - loss: 3.3262 - accuracy: 0.136 - ETA: 43s - loss: 3.3263 - accuracy: 0.136 - ETA: 42s - loss: 3.3261 - accuracy: 0.136 - ETA: 42s - loss: 3.3266 - accuracy: 0.136 - ETA: 41s - loss: 3.3257 - accuracy: 0.136 - ETA: 40s - loss: 3.3259 - accuracy: 0.136 - ETA: 39s - loss: 3.3248 - accuracy: 0.136 - ETA: 39s - loss: 3.3253 - accuracy: 0.136 - ETA: 38s - loss: 3.3253 - accuracy: 0.136 - ETA: 37s - loss: 3.3251 - accuracy: 0.136 - ETA: 36s - loss: 3.3249 - accuracy: 0.136 - ETA: 36s - loss: 3.3247 - accuracy: 0.136 - ETA: 35s - loss: 3.3245 - accuracy: 0.136 - ETA: 34s - loss: 3.3245 - accuracy: 0.136 - ETA: 33s - loss: 3.3242 - accuracy: 0.136 - ETA: 33s - loss: 3.3242 - accuracy: 0.136 - ETA: 32s - loss: 3.3245 - accuracy: 0.136 - ETA: 31s - loss: 3.3244 - accuracy: 0.136 - ETA: 31s - loss: 3.3243 - accuracy: 0.136 - ETA: 30s - loss: 3.3242 - accuracy: 0.136 - ETA: 29s - loss: 3.3242 - accuracy: 0.136 - ETA: 28s - loss: 3.3243 - accuracy: 0.136 - ETA: 28s - loss: 3.3248 - accuracy: 0.136 - ETA: 27s - loss: 3.3247 - accuracy: 0.136 - ETA: 26s - loss: 3.3239 - accuracy: 0.136 - ETA: 25s - loss: 3.3238 - accuracy: 0.136 - ETA: 25s - loss: 3.3236 - accuracy: 0.136 - ETA: 24s - loss: 3.3238 - accuracy: 0.136 - ETA: 23s - loss: 3.3234 - accuracy: 0.136 - ETA: 22s - loss: 3.3232 - accuracy: 0.136 - ETA: 22s - loss: 3.3225 - accuracy: 0.136 - ETA: 21s - loss: 3.3230 - accuracy: 0.136 - ETA: 20s - loss: 3.3225 - accuracy: 0.136 - ETA: 20s - loss: 3.3228 - accuracy: 0.136 - ETA: 19s - loss: 3.3230 - accuracy: 0.136 - ETA: 18s - loss: 3.3229 - accuracy: 0.136 - ETA: 17s - loss: 3.3226 - accuracy: 0.136 - ETA: 17s - loss: 3.3225 - accuracy: 0.136 - ETA: 16s - loss: 3.3222 - accuracy: 0.136 - ETA: 15s - loss: 3.3226 - accuracy: 0.136 - ETA: 14s - loss: 3.3231 - accuracy: 0.136 - ETA: 14s - loss: 3.3240 - accuracy: 0.136 - ETA: 13s - loss: 3.3244 - accuracy: 0.136 - ETA: 12s - loss: 3.3240 - accuracy: 0.136 - ETA: 11s - loss: 3.3239 - accuracy: 0.136 - ETA: 11s - loss: 3.3247 - accuracy: 0.136 - ETA: 10s - loss: 3.3246 - accuracy: 0.136 - ETA: 9s - loss: 3.3244 - accuracy: 0.136 - ETA: 8s - loss: 3.3242 - accuracy: 0.13 - ETA: 8s - loss: 3.3245 - accuracy: 0.13 - ETA: 7s - loss: 3.3246 - accuracy: 0.13 - ETA: 6s - loss: 3.3248 - accuracy: 0.13 - ETA: 5s - loss: 3.3247 - accuracy: 0.13 - ETA: 5s - loss: 3.3241 - accuracy: 0.13 - ETA: 4s - loss: 3.3242 - accuracy: 0.13 - ETA: 3s - loss: 3.3241 - accuracy: 0.13 - ETA: 3s - loss: 3.3242 - accuracy: 0.13 - ETA: 2s - loss: 3.3245 - accuracy: 0.13 - ETA: 1s - loss: 3.3249 - accuracy: 0.13 - ETA: 0s - loss: 3.3248 - accuracy: 0.13 - ETA: 0s - loss: 3.3248 - accuracy: 0.13 - 263s 6ms/step - loss: 3.3248 - accuracy: 0.1361 - val_loss: 4.1568 - val_accuracy: 0.0357\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:10 - loss: 3.3265 - accuracy: 0.12 - ETA: 4:09 - loss: 3.4247 - accuracy: 0.10 - ETA: 4:05 - loss: 3.3770 - accuracy: 0.11 - ETA: 4:03 - loss: 3.3675 - accuracy: 0.11 - ETA: 4:03 - loss: 3.3629 - accuracy: 0.12 - ETA: 4:01 - loss: 3.3305 - accuracy: 0.13 - ETA: 3:59 - loss: 3.3379 - accuracy: 0.12 - ETA: 3:58 - loss: 3.3332 - accuracy: 0.12 - ETA: 3:58 - loss: 3.3518 - accuracy: 0.12 - ETA: 3:56 - loss: 3.3538 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3533 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3673 - accuracy: 0.12 - ETA: 3:53 - loss: 3.3616 - accuracy: 0.12 - ETA: 3:53 - loss: 3.3764 - accuracy: 0.12 - ETA: 3:53 - loss: 3.3632 - accuracy: 0.12 - ETA: 3:53 - loss: 3.3565 - accuracy: 0.12 - ETA: 3:53 - loss: 3.3562 - accuracy: 0.12 - ETA: 3:53 - loss: 3.3580 - accuracy: 0.12 - ETA: 3:53 - loss: 3.3573 - accuracy: 0.12 - ETA: 3:52 - loss: 3.3499 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3450 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3351 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3343 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3383 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3301 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3349 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3400 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3459 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3448 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3444 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3423 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3457 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3387 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3456 - accuracy: 0.12 - ETA: 3:39 - loss: 3.3479 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3480 - accuracy: 0.12 - ETA: 3:37 - loss: 3.3523 - accuracy: 0.12 - ETA: 3:37 - loss: 3.3498 - accuracy: 0.12 - ETA: 3:36 - loss: 3.3505 - accuracy: 0.12 - ETA: 3:35 - loss: 3.3509 - accuracy: 0.12 - ETA: 3:34 - loss: 3.3486 - accuracy: 0.12 - ETA: 3:33 - loss: 3.3474 - accuracy: 0.12 - ETA: 3:32 - loss: 3.3478 - accuracy: 0.12 - ETA: 3:32 - loss: 3.3459 - accuracy: 0.12 - ETA: 3:31 - loss: 3.3425 - accuracy: 0.12 - ETA: 3:30 - loss: 3.3398 - accuracy: 0.12 - ETA: 3:29 - loss: 3.3404 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3412 - accuracy: 0.12 - ETA: 3:28 - loss: 3.3428 - accuracy: 0.12 - ETA: 3:27 - loss: 3.3427 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3445 - accuracy: 0.12 - ETA: 3:25 - loss: 3.3427 - accuracy: 0.12 - ETA: 3:25 - loss: 3.3425 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3420 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3433 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3395 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3372 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3367 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3366 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3389 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3379 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3331 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3319 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3306 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3311 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3328 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3341 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3340 - accuracy: 0.13 - ETA: 3:12 - loss: 3.3328 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3310 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3318 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3312 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3319 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3312 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3322 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3316 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3324 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3309 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3316 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3329 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3316 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3322 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3303 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3300 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3312 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3301 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3275 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3271 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3270 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3277 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3285 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3276 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3286 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3299 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3295 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3289 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3281 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3275 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3273 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3275 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3277 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3282 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3288 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3292 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3302 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3295 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3295 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3284 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3280 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3283 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3275 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3293 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3291 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3290 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3274 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3274 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3282 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3260 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3254 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3266 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3271 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3262 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3272 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3256 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3258 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3276 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3273 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3275 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3257 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3254 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3261 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3274 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3283 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3284 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3294 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3290 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3290 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3297 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3299 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3302 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3290 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3312 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3318 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3327 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3325 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3323 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3325 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3337 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3339 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3343 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3349 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3348 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3364 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3372 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3381 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3382 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3387 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3386 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3393 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3407 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3415 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3419 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3423 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3460 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3460 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3463 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3473 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3482 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3480 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3487 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3479 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3483 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3479 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3489 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3484 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3481 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3479 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3487 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3487 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3488 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3489 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3489 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3493 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3493 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3491 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3491 - accuracy: 0.1350"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3497 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3501 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3508 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3505 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3506 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3501 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3504 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3504 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3507 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3500 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3494 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3507 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3500 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3502 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3501 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3501 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3498 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3491 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3499 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3500 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3503 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3507 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3506 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3513 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3509 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3507 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3508 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3510 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3512 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3518 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3514 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3516 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3522 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3526 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3527 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3532 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3534 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3528 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3530 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3537 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3537 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3539 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3531 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3540 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3547 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3545 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3543 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3536 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3534 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3535 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3535 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3538 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3538 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3538 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3536 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3528 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3526 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3523 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3525 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3519 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3518 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3516 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3517 - accuracy: 0.13 - ETA: 59s - loss: 3.3518 - accuracy: 0.1341 - ETA: 58s - loss: 3.3515 - accuracy: 0.134 - ETA: 58s - loss: 3.3512 - accuracy: 0.134 - ETA: 57s - loss: 3.3511 - accuracy: 0.134 - ETA: 56s - loss: 3.3515 - accuracy: 0.134 - ETA: 55s - loss: 3.3519 - accuracy: 0.134 - ETA: 55s - loss: 3.3513 - accuracy: 0.134 - ETA: 54s - loss: 3.3511 - accuracy: 0.134 - ETA: 53s - loss: 3.3511 - accuracy: 0.134 - ETA: 53s - loss: 3.3513 - accuracy: 0.134 - ETA: 52s - loss: 3.3513 - accuracy: 0.134 - ETA: 51s - loss: 3.3514 - accuracy: 0.134 - ETA: 50s - loss: 3.3514 - accuracy: 0.133 - ETA: 50s - loss: 3.3518 - accuracy: 0.133 - ETA: 49s - loss: 3.3516 - accuracy: 0.134 - ETA: 48s - loss: 3.3520 - accuracy: 0.133 - ETA: 47s - loss: 3.3515 - accuracy: 0.134 - ETA: 47s - loss: 3.3510 - accuracy: 0.134 - ETA: 46s - loss: 3.3511 - accuracy: 0.134 - ETA: 45s - loss: 3.3513 - accuracy: 0.134 - ETA: 44s - loss: 3.3509 - accuracy: 0.134 - ETA: 44s - loss: 3.3503 - accuracy: 0.134 - ETA: 43s - loss: 3.3504 - accuracy: 0.134 - ETA: 42s - loss: 3.3513 - accuracy: 0.134 - ETA: 41s - loss: 3.3512 - accuracy: 0.134 - ETA: 41s - loss: 3.3513 - accuracy: 0.134 - ETA: 40s - loss: 3.3516 - accuracy: 0.134 - ETA: 39s - loss: 3.3517 - accuracy: 0.134 - ETA: 39s - loss: 3.3517 - accuracy: 0.134 - ETA: 38s - loss: 3.3513 - accuracy: 0.134 - ETA: 37s - loss: 3.3508 - accuracy: 0.134 - ETA: 36s - loss: 3.3505 - accuracy: 0.134 - ETA: 36s - loss: 3.3506 - accuracy: 0.134 - ETA: 35s - loss: 3.3510 - accuracy: 0.134 - ETA: 34s - loss: 3.3508 - accuracy: 0.134 - ETA: 33s - loss: 3.3503 - accuracy: 0.134 - ETA: 33s - loss: 3.3506 - accuracy: 0.134 - ETA: 32s - loss: 3.3504 - accuracy: 0.134 - ETA: 31s - loss: 3.3502 - accuracy: 0.134 - ETA: 30s - loss: 3.3503 - accuracy: 0.134 - ETA: 30s - loss: 3.3505 - accuracy: 0.134 - ETA: 29s - loss: 3.3508 - accuracy: 0.134 - ETA: 28s - loss: 3.3506 - accuracy: 0.134 - ETA: 28s - loss: 3.3504 - accuracy: 0.134 - ETA: 27s - loss: 3.3500 - accuracy: 0.134 - ETA: 26s - loss: 3.3497 - accuracy: 0.134 - ETA: 25s - loss: 3.3491 - accuracy: 0.134 - ETA: 25s - loss: 3.3488 - accuracy: 0.134 - ETA: 24s - loss: 3.3487 - accuracy: 0.134 - ETA: 23s - loss: 3.3489 - accuracy: 0.134 - ETA: 22s - loss: 3.3487 - accuracy: 0.134 - ETA: 22s - loss: 3.3479 - accuracy: 0.134 - ETA: 21s - loss: 3.3476 - accuracy: 0.134 - ETA: 20s - loss: 3.3474 - accuracy: 0.134 - ETA: 19s - loss: 3.3476 - accuracy: 0.134 - ETA: 19s - loss: 3.3478 - accuracy: 0.134 - ETA: 18s - loss: 3.3479 - accuracy: 0.134 - ETA: 17s - loss: 3.3482 - accuracy: 0.134 - ETA: 16s - loss: 3.3483 - accuracy: 0.134 - ETA: 16s - loss: 3.3484 - accuracy: 0.134 - ETA: 15s - loss: 3.3482 - accuracy: 0.134 - ETA: 14s - loss: 3.3482 - accuracy: 0.134 - ETA: 14s - loss: 3.3485 - accuracy: 0.133 - ETA: 13s - loss: 3.3489 - accuracy: 0.133 - ETA: 12s - loss: 3.3497 - accuracy: 0.133 - ETA: 11s - loss: 3.3500 - accuracy: 0.133 - ETA: 11s - loss: 3.3500 - accuracy: 0.133 - ETA: 10s - loss: 3.3498 - accuracy: 0.133 - ETA: 9s - loss: 3.3503 - accuracy: 0.133 - ETA: 8s - loss: 3.3507 - accuracy: 0.13 - ETA: 8s - loss: 3.3507 - accuracy: 0.13 - ETA: 7s - loss: 3.3513 - accuracy: 0.13 - ETA: 6s - loss: 3.3513 - accuracy: 0.13 - ETA: 5s - loss: 3.3519 - accuracy: 0.13 - ETA: 5s - loss: 3.3522 - accuracy: 0.13 - ETA: 4s - loss: 3.3526 - accuracy: 0.13 - ETA: 3s - loss: 3.3524 - accuracy: 0.13 - ETA: 3s - loss: 3.3524 - accuracy: 0.13 - ETA: 2s - loss: 3.3522 - accuracy: 0.13 - ETA: 1s - loss: 3.3522 - accuracy: 0.13 - ETA: 0s - loss: 3.3528 - accuracy: 0.13 - ETA: 0s - loss: 3.3536 - accuracy: 0.13 - 262s 6ms/step - loss: 3.3536 - accuracy: 0.1328 - val_loss: 3.9741 - val_accuracy: 0.0243\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:05 - loss: 3.3909 - accuracy: 0.11 - ETA: 4:02 - loss: 3.4169 - accuracy: 0.12 - ETA: 4:04 - loss: 3.4472 - accuracy: 0.11 - ETA: 4:03 - loss: 3.4470 - accuracy: 0.11 - ETA: 4:03 - loss: 3.4680 - accuracy: 0.10 - ETA: 4:01 - loss: 3.4382 - accuracy: 0.10 - ETA: 3:58 - loss: 3.4047 - accuracy: 0.10 - ETA: 3:57 - loss: 3.4024 - accuracy: 0.10 - ETA: 3:57 - loss: 3.3919 - accuracy: 0.11 - ETA: 3:55 - loss: 3.3734 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3679 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3608 - accuracy: 0.12 - ETA: 3:52 - loss: 3.3525 - accuracy: 0.12 - ETA: 3:51 - loss: 3.3538 - accuracy: 0.12 - ETA: 3:51 - loss: 3.3583 - accuracy: 0.12 - ETA: 3:50 - loss: 3.3578 - accuracy: 0.12 - ETA: 3:49 - loss: 3.3518 - accuracy: 0.12 - ETA: 3:50 - loss: 3.3651 - accuracy: 0.12 - ETA: 3:50 - loss: 3.3620 - accuracy: 0.12 - ETA: 3:49 - loss: 3.3642 - accuracy: 0.12 - ETA: 3:49 - loss: 3.3517 - accuracy: 0.12 - ETA: 3:49 - loss: 3.3570 - accuracy: 0.12 - ETA: 3:48 - loss: 3.3628 - accuracy: 0.12 - ETA: 3:47 - loss: 3.3588 - accuracy: 0.12 - ETA: 3:46 - loss: 3.3603 - accuracy: 0.12 - ETA: 3:46 - loss: 3.3620 - accuracy: 0.12 - ETA: 3:45 - loss: 3.3635 - accuracy: 0.12 - ETA: 3:45 - loss: 3.3648 - accuracy: 0.12 - ETA: 3:44 - loss: 3.3677 - accuracy: 0.12 - ETA: 3:43 - loss: 3.3684 - accuracy: 0.12 - ETA: 3:42 - loss: 3.3633 - accuracy: 0.12 - ETA: 3:41 - loss: 3.3648 - accuracy: 0.12 - ETA: 3:40 - loss: 3.3590 - accuracy: 0.12 - ETA: 3:39 - loss: 3.3572 - accuracy: 0.12 - ETA: 3:38 - loss: 3.3596 - accuracy: 0.12 - ETA: 3:38 - loss: 3.3548 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3520 - accuracy: 0.12 - ETA: 3:36 - loss: 3.3531 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3498 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3954 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3945 - accuracy: 0.13 - ETA: 3:33 - loss: 3.3920 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3954 - accuracy: 0.12 - ETA: 3:32 - loss: 3.3896 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3893 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3862 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3850 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3827 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3856 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3844 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3839 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3809 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3785 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3756 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3778 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3765 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3759 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3776 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3792 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3779 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3797 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3802 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3776 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3778 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3812 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3809 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3778 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3781 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3786 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3776 - accuracy: 0.13 - ETA: 3:12 - loss: 3.3787 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3777 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3747 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3752 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3756 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3744 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3751 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3728 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3719 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3700 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3667 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3655 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3649 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3653 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3641 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3619 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3627 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3643 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3604 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3613 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3601 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3605 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3600 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3599 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3600 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3594 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3592 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3596 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3585 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3583 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3574 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3561 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3559 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3562 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3543 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3543 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3532 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3517 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3507 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3498 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3481 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3470 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3466 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3459 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3473 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3466 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3472 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3462 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3457 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3463 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3466 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3480 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3479 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3474 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3465 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3478 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3488 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3484 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3488 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3474 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3469 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3447 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3452 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3458 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3458 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3469 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3454 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3460 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3458 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3475 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3476 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3471 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3465 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3478 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3481 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3477 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3474 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3466 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3455 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3450 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3448 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3454 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3454 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3448 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3450 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3454 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3450 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3443 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3446 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3444 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3452 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3460 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3456 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3457 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3451 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3441 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3430 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3428 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3424 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3428 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3437 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3433 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3427 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3426 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3424 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3418 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3413 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3412 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3404 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3405 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3404 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3399 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3399 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3404 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3416 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3424 - accuracy: 0.1362"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:46 - loss: 3.3433 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3439 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3448 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3457 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3466 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3473 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3477 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3480 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3481 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3486 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3486 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3482 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3479 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3492 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3495 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3494 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3497 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3501 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3502 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3493 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3484 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3484 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3488 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3483 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3481 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3474 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3466 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3471 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3465 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3463 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3461 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3459 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3462 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3471 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3474 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3470 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3469 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3476 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3479 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3481 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3478 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3475 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3473 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3466 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3473 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3477 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3472 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3475 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3471 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3472 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3469 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3466 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3464 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3466 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3456 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3450 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3446 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3448 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3445 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3450 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3456 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3454 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3457 - accuracy: 0.13 - ETA: 59s - loss: 3.3456 - accuracy: 0.1367 - ETA: 59s - loss: 3.3457 - accuracy: 0.136 - ETA: 58s - loss: 3.3453 - accuracy: 0.136 - ETA: 57s - loss: 3.3449 - accuracy: 0.136 - ETA: 56s - loss: 3.3443 - accuracy: 0.136 - ETA: 56s - loss: 3.3445 - accuracy: 0.136 - ETA: 55s - loss: 3.3445 - accuracy: 0.136 - ETA: 54s - loss: 3.3446 - accuracy: 0.136 - ETA: 53s - loss: 3.3447 - accuracy: 0.136 - ETA: 53s - loss: 3.3455 - accuracy: 0.136 - ETA: 52s - loss: 3.3451 - accuracy: 0.136 - ETA: 51s - loss: 3.3451 - accuracy: 0.136 - ETA: 51s - loss: 3.3452 - accuracy: 0.136 - ETA: 50s - loss: 3.3450 - accuracy: 0.136 - ETA: 49s - loss: 3.3450 - accuracy: 0.136 - ETA: 48s - loss: 3.3451 - accuracy: 0.136 - ETA: 48s - loss: 3.3456 - accuracy: 0.136 - ETA: 47s - loss: 3.3459 - accuracy: 0.136 - ETA: 46s - loss: 3.3454 - accuracy: 0.135 - ETA: 45s - loss: 3.3450 - accuracy: 0.135 - ETA: 45s - loss: 3.3447 - accuracy: 0.136 - ETA: 44s - loss: 3.3449 - accuracy: 0.136 - ETA: 43s - loss: 3.3444 - accuracy: 0.136 - ETA: 42s - loss: 3.3448 - accuracy: 0.136 - ETA: 42s - loss: 3.3452 - accuracy: 0.136 - ETA: 41s - loss: 3.3448 - accuracy: 0.136 - ETA: 40s - loss: 3.3446 - accuracy: 0.135 - ETA: 39s - loss: 3.3441 - accuracy: 0.136 - ETA: 39s - loss: 3.3440 - accuracy: 0.136 - ETA: 38s - loss: 3.3445 - accuracy: 0.135 - ETA: 37s - loss: 3.3449 - accuracy: 0.135 - ETA: 36s - loss: 3.3451 - accuracy: 0.135 - ETA: 36s - loss: 3.3442 - accuracy: 0.135 - ETA: 35s - loss: 3.3443 - accuracy: 0.135 - ETA: 34s - loss: 3.3439 - accuracy: 0.135 - ETA: 34s - loss: 3.3435 - accuracy: 0.136 - ETA: 33s - loss: 3.3439 - accuracy: 0.136 - ETA: 32s - loss: 3.3441 - accuracy: 0.135 - ETA: 31s - loss: 3.3442 - accuracy: 0.136 - ETA: 31s - loss: 3.3440 - accuracy: 0.135 - ETA: 30s - loss: 3.3441 - accuracy: 0.136 - ETA: 29s - loss: 3.3445 - accuracy: 0.135 - ETA: 28s - loss: 3.3445 - accuracy: 0.135 - ETA: 28s - loss: 3.3445 - accuracy: 0.135 - ETA: 27s - loss: 3.3443 - accuracy: 0.135 - ETA: 26s - loss: 3.3440 - accuracy: 0.136 - ETA: 25s - loss: 3.3433 - accuracy: 0.136 - ETA: 25s - loss: 3.3425 - accuracy: 0.136 - ETA: 24s - loss: 3.3421 - accuracy: 0.136 - ETA: 23s - loss: 3.3421 - accuracy: 0.136 - ETA: 22s - loss: 3.3416 - accuracy: 0.136 - ETA: 22s - loss: 3.3421 - accuracy: 0.136 - ETA: 21s - loss: 3.3437 - accuracy: 0.136 - ETA: 20s - loss: 3.3433 - accuracy: 0.136 - ETA: 20s - loss: 3.3443 - accuracy: 0.136 - ETA: 19s - loss: 3.3449 - accuracy: 0.135 - ETA: 18s - loss: 3.3452 - accuracy: 0.136 - ETA: 17s - loss: 3.3446 - accuracy: 0.136 - ETA: 17s - loss: 3.3447 - accuracy: 0.136 - ETA: 16s - loss: 3.3448 - accuracy: 0.136 - ETA: 15s - loss: 3.3447 - accuracy: 0.136 - ETA: 14s - loss: 3.3451 - accuracy: 0.136 - ETA: 14s - loss: 3.3453 - accuracy: 0.136 - ETA: 13s - loss: 3.3454 - accuracy: 0.136 - ETA: 12s - loss: 3.3453 - accuracy: 0.136 - ETA: 11s - loss: 3.3454 - accuracy: 0.136 - ETA: 11s - loss: 3.3452 - accuracy: 0.136 - ETA: 10s - loss: 3.3450 - accuracy: 0.136 - ETA: 9s - loss: 3.3451 - accuracy: 0.136 - ETA: 8s - loss: 3.3454 - accuracy: 0.13 - ETA: 8s - loss: 3.3450 - accuracy: 0.13 - ETA: 7s - loss: 3.3452 - accuracy: 0.13 - ETA: 6s - loss: 3.3452 - accuracy: 0.13 - ETA: 5s - loss: 3.3450 - accuracy: 0.13 - ETA: 5s - loss: 3.3445 - accuracy: 0.13 - ETA: 4s - loss: 3.3446 - accuracy: 0.13 - ETA: 3s - loss: 3.3450 - accuracy: 0.13 - ETA: 3s - loss: 3.3447 - accuracy: 0.13 - ETA: 2s - loss: 3.3445 - accuracy: 0.13 - ETA: 1s - loss: 3.3448 - accuracy: 0.13 - ETA: 0s - loss: 3.3445 - accuracy: 0.13 - ETA: 0s - loss: 3.3445 - accuracy: 0.13 - 262s 6ms/step - loss: 3.3444 - accuracy: 0.1365 - val_loss: 3.9543 - val_accuracy: 0.0333\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:03 - loss: 3.3151 - accuracy: 0.14 - ETA: 4:02 - loss: 3.3339 - accuracy: 0.14 - ETA: 4:00 - loss: 3.3353 - accuracy: 0.14 - ETA: 3:58 - loss: 3.2854 - accuracy: 0.14 - ETA: 3:58 - loss: 3.3065 - accuracy: 0.13 - ETA: 3:59 - loss: 3.3331 - accuracy: 0.14 - ETA: 4:00 - loss: 3.3381 - accuracy: 0.13 - ETA: 3:57 - loss: 3.3426 - accuracy: 0.13 - ETA: 3:57 - loss: 3.3427 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3503 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3752 - accuracy: 0.12 - ETA: 3:53 - loss: 3.3766 - accuracy: 0.12 - ETA: 3:52 - loss: 3.3642 - accuracy: 0.12 - ETA: 3:52 - loss: 3.3559 - accuracy: 0.12 - ETA: 3:51 - loss: 3.3588 - accuracy: 0.12 - ETA: 3:50 - loss: 3.3570 - accuracy: 0.12 - ETA: 3:51 - loss: 3.3487 - accuracy: 0.12 - ETA: 3:50 - loss: 3.3419 - accuracy: 0.12 - ETA: 3:50 - loss: 3.3440 - accuracy: 0.12 - ETA: 3:49 - loss: 3.3467 - accuracy: 0.12 - ETA: 3:48 - loss: 3.3482 - accuracy: 0.12 - ETA: 3:47 - loss: 3.3520 - accuracy: 0.12 - ETA: 3:47 - loss: 3.3462 - accuracy: 0.12 - ETA: 3:46 - loss: 3.3486 - accuracy: 0.12 - ETA: 3:46 - loss: 3.3505 - accuracy: 0.12 - ETA: 3:45 - loss: 3.3492 - accuracy: 0.12 - ETA: 3:44 - loss: 3.3483 - accuracy: 0.12 - ETA: 3:43 - loss: 3.3453 - accuracy: 0.12 - ETA: 3:42 - loss: 3.3423 - accuracy: 0.12 - ETA: 3:41 - loss: 3.3432 - accuracy: 0.12 - ETA: 3:40 - loss: 3.3429 - accuracy: 0.12 - ETA: 3:39 - loss: 3.3469 - accuracy: 0.12 - ETA: 3:39 - loss: 3.3416 - accuracy: 0.12 - ETA: 3:38 - loss: 3.3407 - accuracy: 0.12 - ETA: 3:37 - loss: 3.3389 - accuracy: 0.12 - ETA: 3:37 - loss: 3.3406 - accuracy: 0.12 - ETA: 3:36 - loss: 3.3365 - accuracy: 0.12 - ETA: 3:35 - loss: 3.3316 - accuracy: 0.12 - ETA: 3:34 - loss: 3.3311 - accuracy: 0.12 - ETA: 3:34 - loss: 3.3276 - accuracy: 0.12 - ETA: 3:33 - loss: 3.3254 - accuracy: 0.12 - ETA: 3:32 - loss: 3.3247 - accuracy: 0.12 - ETA: 3:31 - loss: 3.3223 - accuracy: 0.12 - ETA: 3:31 - loss: 3.3235 - accuracy: 0.12 - ETA: 3:30 - loss: 3.3210 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3210 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3242 - accuracy: 0.12 - ETA: 3:28 - loss: 3.3248 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3244 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3200 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3206 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3190 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3172 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3174 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3200 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3211 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3203 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3207 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3204 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3186 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3173 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3188 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3201 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3177 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3179 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3172 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3182 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3146 - accuracy: 0.13 - ETA: 3:12 - loss: 3.3130 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3136 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3124 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3100 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3095 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3101 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3087 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3095 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3094 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3111 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3099 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3118 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3124 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3111 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3125 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3128 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3123 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3110 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3089 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3096 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3089 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3092 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3096 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3100 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3097 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3096 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3114 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3111 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3114 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3122 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3139 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3118 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3116 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3128 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3122 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3150 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3153 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3146 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3130 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3125 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3112 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3127 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3143 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3138 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3117 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3128 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3124 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3120 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3114 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3125 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3123 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3128 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3131 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3130 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3144 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3151 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3158 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3148 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3144 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3143 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3140 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3126 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3117 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3121 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3127 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3123 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3130 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3131 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3119 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3112 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3108 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3121 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3131 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3134 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3146 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3144 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3142 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3131 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3141 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3152 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3157 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3150 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3158 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3159 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3169 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3176 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3176 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3174 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3181 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3184 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3193 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3205 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3199 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3191 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3191 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3185 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3178 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3167 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3170 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3170 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3163 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3161 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3158 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3159 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3159 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3150 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3155 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3151 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3167 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3172 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3172 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3177 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3172 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3175 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3163 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3172 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3169 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3176 - accuracy: 0.1341"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3173 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3185 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3180 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3181 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3176 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3180 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3184 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3182 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3179 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3181 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3185 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3176 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3172 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3175 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3178 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3178 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3180 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3180 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3180 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3178 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3172 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3166 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3163 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3165 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3165 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3160 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3165 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3179 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3186 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3178 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3171 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3173 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3176 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3181 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3181 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3181 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3188 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3196 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3201 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3198 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3193 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3193 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3196 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3201 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3201 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3201 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3196 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3196 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3190 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3191 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3192 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3195 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3200 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3195 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3191 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3199 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3199 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3195 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3202 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3208 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3214 - accuracy: 0.13 - ETA: 59s - loss: 3.3209 - accuracy: 0.1353 - ETA: 58s - loss: 3.3213 - accuracy: 0.135 - ETA: 58s - loss: 3.3213 - accuracy: 0.135 - ETA: 57s - loss: 3.3207 - accuracy: 0.135 - ETA: 56s - loss: 3.3204 - accuracy: 0.135 - ETA: 56s - loss: 3.3196 - accuracy: 0.135 - ETA: 55s - loss: 3.3203 - accuracy: 0.135 - ETA: 54s - loss: 3.3192 - accuracy: 0.135 - ETA: 53s - loss: 3.3188 - accuracy: 0.135 - ETA: 53s - loss: 3.3189 - accuracy: 0.135 - ETA: 52s - loss: 3.3197 - accuracy: 0.135 - ETA: 51s - loss: 3.3198 - accuracy: 0.135 - ETA: 51s - loss: 3.3203 - accuracy: 0.135 - ETA: 50s - loss: 3.3210 - accuracy: 0.135 - ETA: 49s - loss: 3.3216 - accuracy: 0.135 - ETA: 48s - loss: 3.3218 - accuracy: 0.135 - ETA: 48s - loss: 3.3219 - accuracy: 0.135 - ETA: 47s - loss: 3.3217 - accuracy: 0.135 - ETA: 46s - loss: 3.3220 - accuracy: 0.135 - ETA: 45s - loss: 3.3221 - accuracy: 0.135 - ETA: 45s - loss: 3.3226 - accuracy: 0.135 - ETA: 44s - loss: 3.3226 - accuracy: 0.135 - ETA: 43s - loss: 3.3233 - accuracy: 0.135 - ETA: 43s - loss: 3.3228 - accuracy: 0.135 - ETA: 42s - loss: 3.3227 - accuracy: 0.135 - ETA: 41s - loss: 3.3223 - accuracy: 0.135 - ETA: 40s - loss: 3.3221 - accuracy: 0.135 - ETA: 40s - loss: 3.3216 - accuracy: 0.135 - ETA: 39s - loss: 3.3218 - accuracy: 0.135 - ETA: 38s - loss: 3.3219 - accuracy: 0.135 - ETA: 38s - loss: 3.3222 - accuracy: 0.134 - ETA: 37s - loss: 3.3229 - accuracy: 0.134 - ETA: 36s - loss: 3.3226 - accuracy: 0.135 - ETA: 35s - loss: 3.3225 - accuracy: 0.135 - ETA: 35s - loss: 3.3233 - accuracy: 0.134 - ETA: 34s - loss: 3.3234 - accuracy: 0.134 - ETA: 33s - loss: 3.3237 - accuracy: 0.134 - ETA: 33s - loss: 3.3241 - accuracy: 0.134 - ETA: 32s - loss: 3.3239 - accuracy: 0.134 - ETA: 31s - loss: 3.3236 - accuracy: 0.134 - ETA: 30s - loss: 3.3237 - accuracy: 0.134 - ETA: 30s - loss: 3.3235 - accuracy: 0.134 - ETA: 29s - loss: 3.3231 - accuracy: 0.134 - ETA: 28s - loss: 3.3229 - accuracy: 0.134 - ETA: 28s - loss: 3.3229 - accuracy: 0.134 - ETA: 27s - loss: 3.3232 - accuracy: 0.135 - ETA: 26s - loss: 3.3230 - accuracy: 0.135 - ETA: 25s - loss: 3.3235 - accuracy: 0.134 - ETA: 25s - loss: 3.3238 - accuracy: 0.134 - ETA: 24s - loss: 3.3236 - accuracy: 0.134 - ETA: 23s - loss: 3.3240 - accuracy: 0.134 - ETA: 23s - loss: 3.3240 - accuracy: 0.134 - ETA: 22s - loss: 3.3236 - accuracy: 0.134 - ETA: 21s - loss: 3.3236 - accuracy: 0.134 - ETA: 20s - loss: 3.3231 - accuracy: 0.134 - ETA: 20s - loss: 3.3233 - accuracy: 0.134 - ETA: 19s - loss: 3.3224 - accuracy: 0.135 - ETA: 18s - loss: 3.3221 - accuracy: 0.135 - ETA: 18s - loss: 3.3225 - accuracy: 0.134 - ETA: 17s - loss: 3.3218 - accuracy: 0.135 - ETA: 16s - loss: 3.3216 - accuracy: 0.135 - ETA: 15s - loss: 3.3216 - accuracy: 0.135 - ETA: 15s - loss: 3.3212 - accuracy: 0.135 - ETA: 14s - loss: 3.3215 - accuracy: 0.135 - ETA: 13s - loss: 3.3218 - accuracy: 0.135 - ETA: 13s - loss: 3.3210 - accuracy: 0.135 - ETA: 12s - loss: 3.3209 - accuracy: 0.135 - ETA: 11s - loss: 3.3207 - accuracy: 0.135 - ETA: 10s - loss: 3.3202 - accuracy: 0.135 - ETA: 10s - loss: 3.3205 - accuracy: 0.135 - ETA: 9s - loss: 3.3205 - accuracy: 0.135 - ETA: 8s - loss: 3.3205 - accuracy: 0.13 - ETA: 7s - loss: 3.3205 - accuracy: 0.13 - ETA: 7s - loss: 3.3208 - accuracy: 0.13 - ETA: 6s - loss: 3.3213 - accuracy: 0.13 - ETA: 5s - loss: 3.3212 - accuracy: 0.13 - ETA: 5s - loss: 3.3213 - accuracy: 0.13 - ETA: 4s - loss: 3.3215 - accuracy: 0.13 - ETA: 3s - loss: 3.3219 - accuracy: 0.13 - ETA: 2s - loss: 3.3219 - accuracy: 0.13 - ETA: 2s - loss: 3.3221 - accuracy: 0.13 - ETA: 1s - loss: 3.3218 - accuracy: 0.13 - ETA: 0s - loss: 3.3218 - accuracy: 0.13 - ETA: 0s - loss: 3.3221 - accuracy: 0.13 - 256s 6ms/step - loss: 3.3222 - accuracy: 0.1352 - val_loss: 3.9630 - val_accuracy: 0.0302\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:59 - loss: 3.2617 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3482 - accuracy: 0.12 - ETA: 4:00 - loss: 3.4136 - accuracy: 0.11 - ETA: 3:57 - loss: 3.3817 - accuracy: 0.12 - ETA: 3:56 - loss: 3.4041 - accuracy: 0.11 - ETA: 3:56 - loss: 3.3546 - accuracy: 0.12 - ETA: 3:56 - loss: 3.3328 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3416 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3302 - accuracy: 0.12 - ETA: 3:54 - loss: 3.3159 - accuracy: 0.13 - ETA: 3:53 - loss: 3.3160 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3245 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3161 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3114 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3125 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3158 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3053 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3008 - accuracy: 0.13 - ETA: 3:46 - loss: 3.2963 - accuracy: 0.14 - ETA: 3:46 - loss: 3.2976 - accuracy: 0.14 - ETA: 3:45 - loss: 3.3009 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3146 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3167 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3195 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3183 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3137 - accuracy: 0.14 - ETA: 3:41 - loss: 3.3157 - accuracy: 0.14 - ETA: 3:41 - loss: 3.3104 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3168 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3115 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3069 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3115 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3066 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3059 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3074 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3060 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3051 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3059 - accuracy: 0.13 - ETA: 3:33 - loss: 3.3067 - accuracy: 0.13 - ETA: 3:33 - loss: 3.3035 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3013 - accuracy: 0.13 - ETA: 3:31 - loss: 3.2996 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3004 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3001 - accuracy: 0.13 - ETA: 3:29 - loss: 3.2942 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2979 - accuracy: 0.13 - ETA: 3:27 - loss: 3.2987 - accuracy: 0.13 - ETA: 3:27 - loss: 3.2938 - accuracy: 0.13 - ETA: 3:26 - loss: 3.2947 - accuracy: 0.13 - ETA: 3:25 - loss: 3.2969 - accuracy: 0.13 - ETA: 3:24 - loss: 3.2939 - accuracy: 0.13 - ETA: 3:24 - loss: 3.2954 - accuracy: 0.13 - ETA: 3:23 - loss: 3.2946 - accuracy: 0.13 - ETA: 3:22 - loss: 3.2986 - accuracy: 0.13 - ETA: 3:21 - loss: 3.2955 - accuracy: 0.13 - ETA: 3:20 - loss: 3.2951 - accuracy: 0.13 - ETA: 3:19 - loss: 3.2961 - accuracy: 0.13 - ETA: 3:19 - loss: 3.2981 - accuracy: 0.13 - ETA: 3:18 - loss: 3.2962 - accuracy: 0.13 - ETA: 3:17 - loss: 3.2941 - accuracy: 0.13 - ETA: 3:17 - loss: 3.2928 - accuracy: 0.13 - ETA: 3:16 - loss: 3.2935 - accuracy: 0.13 - ETA: 3:15 - loss: 3.2945 - accuracy: 0.13 - ETA: 3:14 - loss: 3.2942 - accuracy: 0.13 - ETA: 3:14 - loss: 3.2943 - accuracy: 0.13 - ETA: 3:13 - loss: 3.2968 - accuracy: 0.13 - ETA: 3:12 - loss: 3.2957 - accuracy: 0.13 - ETA: 3:12 - loss: 3.2961 - accuracy: 0.13 - ETA: 3:11 - loss: 3.2988 - accuracy: 0.13 - ETA: 3:10 - loss: 3.2976 - accuracy: 0.13 - ETA: 3:09 - loss: 3.2963 - accuracy: 0.13 - ETA: 3:08 - loss: 3.2969 - accuracy: 0.13 - ETA: 3:08 - loss: 3.2974 - accuracy: 0.13 - ETA: 3:07 - loss: 3.2991 - accuracy: 0.13 - ETA: 3:06 - loss: 3.2983 - accuracy: 0.13 - ETA: 3:06 - loss: 3.2976 - accuracy: 0.13 - ETA: 3:05 - loss: 3.2996 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3002 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3022 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3024 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3009 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3018 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3015 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3024 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3026 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3041 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3048 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3045 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3031 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3007 - accuracy: 0.13 - ETA: 2:54 - loss: 3.2991 - accuracy: 0.13 - ETA: 2:54 - loss: 3.2996 - accuracy: 0.13 - ETA: 2:53 - loss: 3.2994 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3004 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3011 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3016 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3009 - accuracy: 0.13 - ETA: 2:49 - loss: 3.2999 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3009 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3000 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3012 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3024 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3024 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3039 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3031 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3038 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3047 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3057 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3060 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3056 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3041 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3040 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3041 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3041 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3035 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3045 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3042 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3032 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3019 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3012 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3023 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3029 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3027 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3047 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3055 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3062 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3062 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3075 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3074 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3070 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3050 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3052 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3052 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3067 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3081 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3092 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3093 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3099 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3088 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3096 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3100 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3106 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3104 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3097 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3088 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3100 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3095 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3108 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3095 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3094 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3093 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3100 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3098 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3086 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3091 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3095 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3091 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3091 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3093 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3093 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3086 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3105 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3100 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3102 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3101 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3082 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3082 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3091 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3087 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3092 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3091 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3087 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3092 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3098 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3095 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3089 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3093 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3097 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3099 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3133 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3133 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3132 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3124 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3125 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3126 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3128 - accuracy: 0.1363"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3131 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3139 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3151 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3160 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3153 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3168 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3170 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3170 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3180 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3187 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3188 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3190 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3187 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3184 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3186 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3181 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3187 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3186 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3186 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3187 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3202 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3199 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3199 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3200 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3196 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3195 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3199 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3204 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3206 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3208 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3215 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3210 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3211 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3211 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3223 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3226 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3226 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3226 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3230 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3229 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3233 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3247 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3249 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3255 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3247 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3244 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3251 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3253 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3252 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3256 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3260 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3261 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3268 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3269 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3270 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3271 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3274 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3282 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3274 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3268 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3266 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3262 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3264 - accuracy: 0.13 - ETA: 59s - loss: 3.3266 - accuracy: 0.1340 - ETA: 58s - loss: 3.3269 - accuracy: 0.133 - ETA: 58s - loss: 3.3273 - accuracy: 0.133 - ETA: 57s - loss: 3.3274 - accuracy: 0.133 - ETA: 56s - loss: 3.3279 - accuracy: 0.133 - ETA: 55s - loss: 3.3278 - accuracy: 0.133 - ETA: 55s - loss: 3.3279 - accuracy: 0.133 - ETA: 54s - loss: 3.3284 - accuracy: 0.133 - ETA: 53s - loss: 3.3283 - accuracy: 0.133 - ETA: 52s - loss: 3.3288 - accuracy: 0.133 - ETA: 52s - loss: 3.3284 - accuracy: 0.133 - ETA: 51s - loss: 3.3288 - accuracy: 0.133 - ETA: 50s - loss: 3.3288 - accuracy: 0.133 - ETA: 49s - loss: 3.3291 - accuracy: 0.133 - ETA: 49s - loss: 3.3294 - accuracy: 0.133 - ETA: 48s - loss: 3.3300 - accuracy: 0.133 - ETA: 47s - loss: 3.3300 - accuracy: 0.133 - ETA: 47s - loss: 3.3299 - accuracy: 0.133 - ETA: 46s - loss: 3.3293 - accuracy: 0.133 - ETA: 45s - loss: 3.3296 - accuracy: 0.133 - ETA: 44s - loss: 3.3299 - accuracy: 0.133 - ETA: 44s - loss: 3.3303 - accuracy: 0.133 - ETA: 43s - loss: 3.3306 - accuracy: 0.133 - ETA: 42s - loss: 3.3299 - accuracy: 0.133 - ETA: 41s - loss: 3.3298 - accuracy: 0.133 - ETA: 41s - loss: 3.3290 - accuracy: 0.133 - ETA: 40s - loss: 3.3295 - accuracy: 0.133 - ETA: 39s - loss: 3.3297 - accuracy: 0.133 - ETA: 38s - loss: 3.3298 - accuracy: 0.133 - ETA: 38s - loss: 3.3300 - accuracy: 0.133 - ETA: 37s - loss: 3.3300 - accuracy: 0.133 - ETA: 36s - loss: 3.3303 - accuracy: 0.133 - ETA: 35s - loss: 3.3307 - accuracy: 0.133 - ETA: 35s - loss: 3.3302 - accuracy: 0.133 - ETA: 34s - loss: 3.3307 - accuracy: 0.133 - ETA: 33s - loss: 3.3308 - accuracy: 0.133 - ETA: 33s - loss: 3.3310 - accuracy: 0.133 - ETA: 32s - loss: 3.3305 - accuracy: 0.133 - ETA: 31s - loss: 3.3302 - accuracy: 0.133 - ETA: 30s - loss: 3.3306 - accuracy: 0.133 - ETA: 30s - loss: 3.3302 - accuracy: 0.133 - ETA: 29s - loss: 3.3299 - accuracy: 0.133 - ETA: 28s - loss: 3.3302 - accuracy: 0.133 - ETA: 27s - loss: 3.3308 - accuracy: 0.133 - ETA: 27s - loss: 3.3309 - accuracy: 0.133 - ETA: 26s - loss: 3.3306 - accuracy: 0.133 - ETA: 25s - loss: 3.3304 - accuracy: 0.133 - ETA: 24s - loss: 3.3303 - accuracy: 0.133 - ETA: 24s - loss: 3.3298 - accuracy: 0.133 - ETA: 23s - loss: 3.3301 - accuracy: 0.133 - ETA: 22s - loss: 3.3300 - accuracy: 0.133 - ETA: 22s - loss: 3.3300 - accuracy: 0.133 - ETA: 21s - loss: 3.3300 - accuracy: 0.133 - ETA: 20s - loss: 3.3303 - accuracy: 0.133 - ETA: 19s - loss: 3.3297 - accuracy: 0.133 - ETA: 19s - loss: 3.3298 - accuracy: 0.133 - ETA: 18s - loss: 3.3301 - accuracy: 0.133 - ETA: 17s - loss: 3.3300 - accuracy: 0.133 - ETA: 16s - loss: 3.3293 - accuracy: 0.133 - ETA: 16s - loss: 3.3301 - accuracy: 0.133 - ETA: 15s - loss: 3.3296 - accuracy: 0.133 - ETA: 14s - loss: 3.3297 - accuracy: 0.133 - ETA: 13s - loss: 3.3291 - accuracy: 0.133 - ETA: 13s - loss: 3.3293 - accuracy: 0.133 - ETA: 12s - loss: 3.3292 - accuracy: 0.133 - ETA: 11s - loss: 3.3295 - accuracy: 0.133 - ETA: 11s - loss: 3.3294 - accuracy: 0.133 - ETA: 10s - loss: 3.3302 - accuracy: 0.133 - ETA: 9s - loss: 3.3307 - accuracy: 0.133 - ETA: 8s - loss: 3.3302 - accuracy: 0.13 - ETA: 8s - loss: 3.3303 - accuracy: 0.13 - ETA: 7s - loss: 3.3300 - accuracy: 0.13 - ETA: 6s - loss: 3.3299 - accuracy: 0.13 - ETA: 5s - loss: 3.3295 - accuracy: 0.13 - ETA: 5s - loss: 3.3295 - accuracy: 0.13 - ETA: 4s - loss: 3.3299 - accuracy: 0.13 - ETA: 3s - loss: 3.3294 - accuracy: 0.13 - ETA: 2s - loss: 3.3299 - accuracy: 0.13 - ETA: 2s - loss: 3.3299 - accuracy: 0.13 - ETA: 1s - loss: 3.3293 - accuracy: 0.13 - ETA: 0s - loss: 3.3293 - accuracy: 0.13 - ETA: 0s - loss: 3.3296 - accuracy: 0.13 - 261s 6ms/step - loss: 3.3297 - accuracy: 0.1341 - val_loss: 4.0728 - val_accuracy: 0.0311\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:00 - loss: 3.3777 - accuracy: 0.10 - ETA: 3:56 - loss: 3.3082 - accuracy: 0.12 - ETA: 4:01 - loss: 3.2358 - accuracy: 0.13 - ETA: 3:59 - loss: 3.2828 - accuracy: 0.14 - ETA: 3:58 - loss: 3.2917 - accuracy: 0.13 - ETA: 3:58 - loss: 3.3214 - accuracy: 0.13 - ETA: 3:56 - loss: 3.2943 - accuracy: 0.13 - ETA: 3:55 - loss: 3.2952 - accuracy: 0.13 - ETA: 3:54 - loss: 3.2866 - accuracy: 0.13 - ETA: 3:53 - loss: 3.2928 - accuracy: 0.13 - ETA: 3:52 - loss: 3.2921 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3099 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3062 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3126 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3106 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3044 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3033 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3076 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3073 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3074 - accuracy: 0.14 - ETA: 3:45 - loss: 3.3147 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3222 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3203 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3199 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3164 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3196 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3210 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3222 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3170 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3203 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3169 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3196 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3194 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3139 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3117 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3126 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3122 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3142 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3151 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3134 - accuracy: 0.13 - ETA: 3:33 - loss: 3.3140 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3150 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3121 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3106 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3112 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3065 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3081 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3089 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3071 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3091 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3117 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3103 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3087 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3093 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3131 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3162 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3170 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3183 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3165 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3177 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3196 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3214 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3193 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3189 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3199 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3183 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3185 - accuracy: 0.13 - ETA: 3:12 - loss: 3.3179 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3145 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3156 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3155 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3123 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3091 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3100 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3074 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3089 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3074 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3086 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3082 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3096 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3088 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3115 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3127 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3106 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3090 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3074 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3068 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3062 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3049 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3061 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3056 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3035 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3039 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3040 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3043 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3041 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3045 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3045 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3046 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3051 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3044 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3034 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3038 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3058 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3057 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3056 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3062 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3074 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3062 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3056 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3060 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3064 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3057 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3053 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3064 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3085 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3081 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3081 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3079 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3075 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3076 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3074 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3080 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3074 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3076 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3075 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3067 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3069 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3088 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3095 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3105 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3104 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3105 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3095 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3090 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3099 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3090 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3085 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3083 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3081 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3081 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3085 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3075 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3075 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3070 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3067 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3074 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3077 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3083 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3090 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3088 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3092 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3101 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3111 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3108 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3103 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3104 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3113 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3118 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3112 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3111 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3117 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3113 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3106 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3114 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3110 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3099 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3102 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3101 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3109 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3096 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3105 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3098 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3098 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3095 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3097 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3094 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3102 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3105 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3096 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3092 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3096 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3096 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3097 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3096 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3094 - accuracy: 0.1376"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3101 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3100 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3099 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3102 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3103 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3110 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3109 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3113 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3105 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3101 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3098 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3096 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3092 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3088 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3087 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3092 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3095 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3091 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3097 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3101 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3103 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3101 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3105 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3108 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3103 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3106 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3113 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3113 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3157 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3153 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3153 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3160 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3166 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3164 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3166 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3167 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3168 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3174 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3173 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3178 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3168 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3167 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3162 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3165 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3167 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3167 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3173 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3177 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3177 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3172 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3176 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3176 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3181 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3178 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3176 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3170 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3177 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3181 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3178 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3180 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3182 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3188 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3188 - accuracy: 0.13 - ETA: 59s - loss: 3.3190 - accuracy: 0.1380 - ETA: 58s - loss: 3.3192 - accuracy: 0.137 - ETA: 58s - loss: 3.3193 - accuracy: 0.137 - ETA: 57s - loss: 3.3188 - accuracy: 0.137 - ETA: 56s - loss: 3.3192 - accuracy: 0.137 - ETA: 55s - loss: 3.3187 - accuracy: 0.137 - ETA: 55s - loss: 3.3185 - accuracy: 0.138 - ETA: 54s - loss: 3.3189 - accuracy: 0.137 - ETA: 53s - loss: 3.3191 - accuracy: 0.137 - ETA: 52s - loss: 3.3188 - accuracy: 0.137 - ETA: 52s - loss: 3.3194 - accuracy: 0.137 - ETA: 51s - loss: 3.3195 - accuracy: 0.137 - ETA: 50s - loss: 3.3187 - accuracy: 0.138 - ETA: 50s - loss: 3.3186 - accuracy: 0.138 - ETA: 49s - loss: 3.3187 - accuracy: 0.138 - ETA: 48s - loss: 3.3191 - accuracy: 0.137 - ETA: 47s - loss: 3.3196 - accuracy: 0.137 - ETA: 47s - loss: 3.3198 - accuracy: 0.137 - ETA: 46s - loss: 3.3194 - accuracy: 0.137 - ETA: 45s - loss: 3.3194 - accuracy: 0.137 - ETA: 44s - loss: 3.3196 - accuracy: 0.137 - ETA: 44s - loss: 3.3194 - accuracy: 0.137 - ETA: 43s - loss: 3.3198 - accuracy: 0.137 - ETA: 42s - loss: 3.3198 - accuracy: 0.137 - ETA: 41s - loss: 3.3197 - accuracy: 0.137 - ETA: 41s - loss: 3.3201 - accuracy: 0.137 - ETA: 40s - loss: 3.3198 - accuracy: 0.137 - ETA: 39s - loss: 3.3201 - accuracy: 0.137 - ETA: 39s - loss: 3.3205 - accuracy: 0.137 - ETA: 38s - loss: 3.3206 - accuracy: 0.137 - ETA: 37s - loss: 3.3206 - accuracy: 0.137 - ETA: 36s - loss: 3.3207 - accuracy: 0.137 - ETA: 36s - loss: 3.3212 - accuracy: 0.137 - ETA: 35s - loss: 3.3210 - accuracy: 0.137 - ETA: 34s - loss: 3.3212 - accuracy: 0.136 - ETA: 33s - loss: 3.3213 - accuracy: 0.136 - ETA: 33s - loss: 3.3213 - accuracy: 0.136 - ETA: 32s - loss: 3.3209 - accuracy: 0.136 - ETA: 31s - loss: 3.3211 - accuracy: 0.136 - ETA: 30s - loss: 3.3210 - accuracy: 0.136 - ETA: 30s - loss: 3.3201 - accuracy: 0.137 - ETA: 29s - loss: 3.3208 - accuracy: 0.137 - ETA: 28s - loss: 3.3206 - accuracy: 0.137 - ETA: 27s - loss: 3.3210 - accuracy: 0.137 - ETA: 27s - loss: 3.3211 - accuracy: 0.137 - ETA: 26s - loss: 3.3213 - accuracy: 0.137 - ETA: 25s - loss: 3.3208 - accuracy: 0.137 - ETA: 25s - loss: 3.3210 - accuracy: 0.137 - ETA: 24s - loss: 3.3211 - accuracy: 0.137 - ETA: 23s - loss: 3.3207 - accuracy: 0.137 - ETA: 22s - loss: 3.3210 - accuracy: 0.137 - ETA: 22s - loss: 3.3205 - accuracy: 0.137 - ETA: 21s - loss: 3.3204 - accuracy: 0.137 - ETA: 20s - loss: 3.3200 - accuracy: 0.137 - ETA: 19s - loss: 3.3205 - accuracy: 0.137 - ETA: 19s - loss: 3.3200 - accuracy: 0.137 - ETA: 18s - loss: 3.3201 - accuracy: 0.137 - ETA: 17s - loss: 3.3198 - accuracy: 0.137 - ETA: 16s - loss: 3.3202 - accuracy: 0.137 - ETA: 16s - loss: 3.3199 - accuracy: 0.137 - ETA: 15s - loss: 3.3200 - accuracy: 0.137 - ETA: 14s - loss: 3.3201 - accuracy: 0.137 - ETA: 14s - loss: 3.3201 - accuracy: 0.137 - ETA: 13s - loss: 3.3204 - accuracy: 0.137 - ETA: 12s - loss: 3.3204 - accuracy: 0.137 - ETA: 11s - loss: 3.3204 - accuracy: 0.137 - ETA: 11s - loss: 3.3207 - accuracy: 0.137 - ETA: 10s - loss: 3.3206 - accuracy: 0.137 - ETA: 9s - loss: 3.3209 - accuracy: 0.137 - ETA: 8s - loss: 3.3208 - accuracy: 0.13 - ETA: 8s - loss: 3.3211 - accuracy: 0.13 - ETA: 7s - loss: 3.3207 - accuracy: 0.13 - ETA: 6s - loss: 3.3203 - accuracy: 0.13 - ETA: 5s - loss: 3.3204 - accuracy: 0.13 - ETA: 5s - loss: 3.3205 - accuracy: 0.13 - ETA: 4s - loss: 3.3204 - accuracy: 0.13 - ETA: 3s - loss: 3.3198 - accuracy: 0.13 - ETA: 2s - loss: 3.3198 - accuracy: 0.13 - ETA: 2s - loss: 3.3200 - accuracy: 0.13 - ETA: 1s - loss: 3.3203 - accuracy: 0.13 - ETA: 0s - loss: 3.3203 - accuracy: 0.13 - ETA: 0s - loss: 3.3204 - accuracy: 0.13 - 261s 6ms/step - loss: 3.3204 - accuracy: 0.1376 - val_loss: 4.0601 - val_accuracy: 0.0351\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:07 - loss: 3.0943 - accuracy: 0.17 - ETA: 3:59 - loss: 3.1806 - accuracy: 0.14 - ETA: 3:56 - loss: 3.1894 - accuracy: 0.15 - ETA: 3:57 - loss: 3.2416 - accuracy: 0.14 - ETA: 3:55 - loss: 3.2885 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3070 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3005 - accuracy: 0.13 - ETA: 3:54 - loss: 3.2936 - accuracy: 0.13 - ETA: 3:53 - loss: 3.2755 - accuracy: 0.13 - ETA: 3:52 - loss: 3.2955 - accuracy: 0.13 - ETA: 3:51 - loss: 3.2923 - accuracy: 0.13 - ETA: 3:49 - loss: 3.2878 - accuracy: 0.13 - ETA: 3:48 - loss: 3.2988 - accuracy: 0.13 - ETA: 3:48 - loss: 3.2918 - accuracy: 0.13 - ETA: 3:47 - loss: 3.2961 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3085 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3088 - accuracy: 0.13 - ETA: 3:44 - loss: 3.2993 - accuracy: 0.13 - ETA: 3:44 - loss: 3.2915 - accuracy: 0.14 - ETA: 3:44 - loss: 3.2920 - accuracy: 0.13 - ETA: 3:43 - loss: 3.2916 - accuracy: 0.13 - ETA: 3:43 - loss: 3.2893 - accuracy: 0.14 - ETA: 3:42 - loss: 3.2958 - accuracy: 0.14 - ETA: 3:41 - loss: 3.2950 - accuracy: 0.14 - ETA: 3:41 - loss: 3.3009 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3014 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3101 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3119 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3083 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3070 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3000 - accuracy: 0.14 - ETA: 3:37 - loss: 3.2990 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2971 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2945 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2949 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2982 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2981 - accuracy: 0.14 - ETA: 3:34 - loss: 3.3004 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2990 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2959 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2932 - accuracy: 0.14 - ETA: 3:31 - loss: 3.2908 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2880 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2891 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2912 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2903 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2912 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2925 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2938 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2961 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2950 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2966 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2961 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2965 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2942 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2926 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2949 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2945 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2959 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2951 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2977 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2970 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2954 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2921 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2922 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2884 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2863 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2870 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2885 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2900 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2901 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2893 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2893 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2889 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2865 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2872 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2860 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2868 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2858 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2858 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2864 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2854 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2838 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2841 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2853 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2853 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2839 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2849 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2827 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2838 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2837 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2831 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2815 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2823 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2820 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2825 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2826 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2834 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2849 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2827 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2824 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2815 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2802 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2808 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2795 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2800 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2800 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2802 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2802 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2799 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2802 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2798 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2795 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2801 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2798 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2812 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2799 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2798 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2818 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2844 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2840 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2843 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2845 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2835 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2831 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2821 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2826 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2831 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2839 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2850 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2857 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2869 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2856 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2850 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2848 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2849 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2860 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2872 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2886 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2899 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2902 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2908 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2914 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2926 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2933 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2949 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2956 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2974 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2984 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2992 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3018 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3027 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3039 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3044 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3046 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3052 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3059 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3060 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3057 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3057 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3065 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3073 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3082 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3077 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3083 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3084 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3100 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3105 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3115 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3129 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3145 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3148 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3155 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3175 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3191 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3195 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3204 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3201 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3207 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3206 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3204 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3216 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3224 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3232 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3241 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3250 - accuracy: 0.1374"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3255 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3252 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3249 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3252 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3254 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3256 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3255 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3263 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3258 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3263 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3263 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3264 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3270 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3274 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3284 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3279 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3278 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3277 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3281 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3282 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3288 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3286 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3285 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3287 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3287 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3294 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3295 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3298 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3298 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3294 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3294 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3287 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3276 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3280 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3278 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3285 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3280 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3284 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3287 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3294 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3292 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3289 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3287 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3296 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3295 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3296 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3293 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3293 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3297 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3296 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3296 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3302 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3305 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3308 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3312 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3315 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3319 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3318 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3321 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3318 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3318 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3320 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3324 - accuracy: 0.13 - ETA: 59s - loss: 3.3323 - accuracy: 0.1373 - ETA: 58s - loss: 3.3326 - accuracy: 0.137 - ETA: 57s - loss: 3.3328 - accuracy: 0.137 - ETA: 57s - loss: 3.3332 - accuracy: 0.137 - ETA: 56s - loss: 3.3334 - accuracy: 0.137 - ETA: 55s - loss: 3.3336 - accuracy: 0.137 - ETA: 54s - loss: 3.3338 - accuracy: 0.137 - ETA: 54s - loss: 3.3341 - accuracy: 0.137 - ETA: 53s - loss: 3.3354 - accuracy: 0.137 - ETA: 52s - loss: 3.3352 - accuracy: 0.137 - ETA: 52s - loss: 3.3352 - accuracy: 0.137 - ETA: 51s - loss: 3.3350 - accuracy: 0.137 - ETA: 50s - loss: 3.3350 - accuracy: 0.137 - ETA: 49s - loss: 3.3351 - accuracy: 0.137 - ETA: 49s - loss: 3.3353 - accuracy: 0.137 - ETA: 48s - loss: 3.3352 - accuracy: 0.137 - ETA: 47s - loss: 3.3348 - accuracy: 0.137 - ETA: 46s - loss: 3.3349 - accuracy: 0.137 - ETA: 46s - loss: 3.3350 - accuracy: 0.136 - ETA: 45s - loss: 3.3342 - accuracy: 0.136 - ETA: 44s - loss: 3.3340 - accuracy: 0.137 - ETA: 43s - loss: 3.3339 - accuracy: 0.137 - ETA: 43s - loss: 3.3340 - accuracy: 0.137 - ETA: 42s - loss: 3.3341 - accuracy: 0.136 - ETA: 41s - loss: 3.3340 - accuracy: 0.136 - ETA: 41s - loss: 3.3333 - accuracy: 0.137 - ETA: 40s - loss: 3.3333 - accuracy: 0.137 - ETA: 39s - loss: 3.3334 - accuracy: 0.137 - ETA: 38s - loss: 3.3325 - accuracy: 0.137 - ETA: 38s - loss: 3.3325 - accuracy: 0.137 - ETA: 37s - loss: 3.3324 - accuracy: 0.137 - ETA: 36s - loss: 3.3326 - accuracy: 0.137 - ETA: 35s - loss: 3.3330 - accuracy: 0.137 - ETA: 35s - loss: 3.3340 - accuracy: 0.137 - ETA: 34s - loss: 3.3336 - accuracy: 0.137 - ETA: 33s - loss: 3.3341 - accuracy: 0.137 - ETA: 33s - loss: 3.3334 - accuracy: 0.137 - ETA: 32s - loss: 3.3335 - accuracy: 0.137 - ETA: 31s - loss: 3.3336 - accuracy: 0.137 - ETA: 30s - loss: 3.3332 - accuracy: 0.137 - ETA: 30s - loss: 3.3330 - accuracy: 0.137 - ETA: 29s - loss: 3.3327 - accuracy: 0.137 - ETA: 28s - loss: 3.3326 - accuracy: 0.137 - ETA: 27s - loss: 3.3320 - accuracy: 0.137 - ETA: 27s - loss: 3.3321 - accuracy: 0.137 - ETA: 26s - loss: 3.3317 - accuracy: 0.137 - ETA: 25s - loss: 3.3312 - accuracy: 0.137 - ETA: 24s - loss: 3.3308 - accuracy: 0.137 - ETA: 24s - loss: 3.3303 - accuracy: 0.137 - ETA: 23s - loss: 3.3306 - accuracy: 0.137 - ETA: 22s - loss: 3.3305 - accuracy: 0.137 - ETA: 22s - loss: 3.3299 - accuracy: 0.137 - ETA: 21s - loss: 3.3302 - accuracy: 0.137 - ETA: 20s - loss: 3.3301 - accuracy: 0.137 - ETA: 19s - loss: 3.3300 - accuracy: 0.137 - ETA: 19s - loss: 3.3297 - accuracy: 0.137 - ETA: 18s - loss: 3.3297 - accuracy: 0.137 - ETA: 17s - loss: 3.3288 - accuracy: 0.137 - ETA: 16s - loss: 3.3287 - accuracy: 0.137 - ETA: 16s - loss: 3.3287 - accuracy: 0.137 - ETA: 15s - loss: 3.3287 - accuracy: 0.137 - ETA: 14s - loss: 3.3287 - accuracy: 0.137 - ETA: 14s - loss: 3.3284 - accuracy: 0.137 - ETA: 13s - loss: 3.3278 - accuracy: 0.138 - ETA: 12s - loss: 3.3275 - accuracy: 0.138 - ETA: 11s - loss: 3.3276 - accuracy: 0.138 - ETA: 11s - loss: 3.3272 - accuracy: 0.138 - ETA: 10s - loss: 3.3274 - accuracy: 0.138 - ETA: 9s - loss: 3.3278 - accuracy: 0.137 - ETA: 8s - loss: 3.3280 - accuracy: 0.13 - ETA: 8s - loss: 3.3283 - accuracy: 0.13 - ETA: 7s - loss: 3.3284 - accuracy: 0.13 - ETA: 6s - loss: 3.3284 - accuracy: 0.13 - ETA: 5s - loss: 3.3286 - accuracy: 0.13 - ETA: 5s - loss: 3.3280 - accuracy: 0.13 - ETA: 4s - loss: 3.3280 - accuracy: 0.13 - ETA: 3s - loss: 3.3281 - accuracy: 0.13 - ETA: 2s - loss: 3.3282 - accuracy: 0.13 - ETA: 2s - loss: 3.3283 - accuracy: 0.13 - ETA: 1s - loss: 3.3283 - accuracy: 0.13 - ETA: 0s - loss: 3.3279 - accuracy: 0.13 - ETA: 0s - loss: 3.3277 - accuracy: 0.13 - 261s 6ms/step - loss: 3.3276 - accuracy: 0.1377 - val_loss: 4.0143 - val_accuracy: 0.0344\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:11 - loss: 3.2160 - accuracy: 0.15 - ETA: 4:02 - loss: 3.2638 - accuracy: 0.14 - ETA: 3:57 - loss: 3.2649 - accuracy: 0.14 - ETA: 3:58 - loss: 3.3018 - accuracy: 0.14 - ETA: 3:58 - loss: 3.2940 - accuracy: 0.14 - ETA: 3:58 - loss: 3.3137 - accuracy: 0.14 - ETA: 3:58 - loss: 3.3055 - accuracy: 0.14 - ETA: 3:57 - loss: 3.3037 - accuracy: 0.13 - ETA: 3:55 - loss: 3.2904 - accuracy: 0.14 - ETA: 3:56 - loss: 3.2987 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3058 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3235 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3333 - accuracy: 0.12 - ETA: 3:52 - loss: 3.3315 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3311 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3337 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3387 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3411 - accuracy: 0.12 - ETA: 3:49 - loss: 3.3345 - accuracy: 0.12 - ETA: 3:48 - loss: 3.3280 - accuracy: 0.12 - ETA: 3:47 - loss: 3.3361 - accuracy: 0.12 - ETA: 3:46 - loss: 3.3409 - accuracy: 0.12 - ETA: 3:45 - loss: 3.3336 - accuracy: 0.12 - ETA: 3:45 - loss: 3.3361 - accuracy: 0.12 - ETA: 3:44 - loss: 3.3340 - accuracy: 0.12 - ETA: 3:44 - loss: 3.3410 - accuracy: 0.12 - ETA: 3:43 - loss: 3.3457 - accuracy: 0.12 - ETA: 3:42 - loss: 3.3391 - accuracy: 0.12 - ETA: 3:41 - loss: 3.3414 - accuracy: 0.12 - ETA: 3:41 - loss: 3.3453 - accuracy: 0.12 - ETA: 3:40 - loss: 3.3417 - accuracy: 0.12 - ETA: 3:39 - loss: 3.3383 - accuracy: 0.12 - ETA: 3:39 - loss: 3.3446 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3446 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3502 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3448 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3467 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3498 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3509 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3501 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3504 - accuracy: 0.13 - ETA: 3:33 - loss: 3.3509 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3506 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3548 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3529 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3494 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3529 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3556 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3580 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3579 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3620 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3662 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3641 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3625 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3637 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3634 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3642 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3625 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3658 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3671 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3697 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3673 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3670 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3666 - accuracy: 0.12 - ETA: 3:15 - loss: 3.3665 - accuracy: 0.12 - ETA: 3:15 - loss: 3.3631 - accuracy: 0.12 - ETA: 3:14 - loss: 3.3689 - accuracy: 0.12 - ETA: 3:13 - loss: 3.3671 - accuracy: 0.12 - ETA: 3:13 - loss: 3.3661 - accuracy: 0.12 - ETA: 3:12 - loss: 3.3685 - accuracy: 0.12 - ETA: 3:11 - loss: 3.3688 - accuracy: 0.12 - ETA: 3:10 - loss: 3.3668 - accuracy: 0.12 - ETA: 3:09 - loss: 3.3669 - accuracy: 0.12 - ETA: 3:08 - loss: 3.3651 - accuracy: 0.12 - ETA: 3:08 - loss: 3.3654 - accuracy: 0.12 - ETA: 3:07 - loss: 3.3634 - accuracy: 0.12 - ETA: 3:06 - loss: 3.3630 - accuracy: 0.12 - ETA: 3:05 - loss: 3.3624 - accuracy: 0.12 - ETA: 3:05 - loss: 3.3742 - accuracy: 0.12 - ETA: 3:04 - loss: 3.3737 - accuracy: 0.12 - ETA: 3:03 - loss: 3.3729 - accuracy: 0.12 - ETA: 3:02 - loss: 3.3727 - accuracy: 0.12 - ETA: 3:01 - loss: 3.3737 - accuracy: 0.12 - ETA: 3:01 - loss: 3.3729 - accuracy: 0.12 - ETA: 3:00 - loss: 3.3700 - accuracy: 0.12 - ETA: 2:59 - loss: 3.3714 - accuracy: 0.12 - ETA: 2:59 - loss: 3.3709 - accuracy: 0.12 - ETA: 2:58 - loss: 3.3701 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3707 - accuracy: 0.12 - ETA: 2:56 - loss: 3.3705 - accuracy: 0.12 - ETA: 2:56 - loss: 3.3704 - accuracy: 0.12 - ETA: 2:55 - loss: 3.3712 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3720 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3720 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3723 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3700 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3707 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3712 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3704 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3697 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3700 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3703 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3709 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3696 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3703 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3720 - accuracy: 0.12 - ETA: 2:44 - loss: 3.3720 - accuracy: 0.12 - ETA: 2:43 - loss: 3.3713 - accuracy: 0.12 - ETA: 2:42 - loss: 3.3715 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3721 - accuracy: 0.12 - ETA: 2:41 - loss: 3.3718 - accuracy: 0.12 - ETA: 2:40 - loss: 3.3718 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3721 - accuracy: 0.12 - ETA: 2:39 - loss: 3.3713 - accuracy: 0.12 - ETA: 2:38 - loss: 3.3726 - accuracy: 0.12 - ETA: 2:37 - loss: 3.3713 - accuracy: 0.12 - ETA: 2:37 - loss: 3.3700 - accuracy: 0.12 - ETA: 2:36 - loss: 3.3706 - accuracy: 0.12 - ETA: 2:35 - loss: 3.3691 - accuracy: 0.12 - ETA: 2:34 - loss: 3.3699 - accuracy: 0.12 - ETA: 2:34 - loss: 3.3687 - accuracy: 0.12 - ETA: 2:33 - loss: 3.3688 - accuracy: 0.12 - ETA: 2:32 - loss: 3.3692 - accuracy: 0.12 - ETA: 2:32 - loss: 3.3700 - accuracy: 0.12 - ETA: 2:31 - loss: 3.3699 - accuracy: 0.12 - ETA: 2:30 - loss: 3.3698 - accuracy: 0.12 - ETA: 2:30 - loss: 3.3685 - accuracy: 0.12 - ETA: 2:29 - loss: 3.3683 - accuracy: 0.12 - ETA: 2:28 - loss: 3.3695 - accuracy: 0.12 - ETA: 2:27 - loss: 3.3689 - accuracy: 0.12 - ETA: 2:27 - loss: 3.3694 - accuracy: 0.12 - ETA: 2:26 - loss: 3.3693 - accuracy: 0.12 - ETA: 2:25 - loss: 3.3689 - accuracy: 0.12 - ETA: 2:25 - loss: 3.3693 - accuracy: 0.12 - ETA: 2:24 - loss: 3.3686 - accuracy: 0.12 - ETA: 2:23 - loss: 3.3690 - accuracy: 0.12 - ETA: 2:22 - loss: 3.3686 - accuracy: 0.12 - ETA: 2:22 - loss: 3.3684 - accuracy: 0.12 - ETA: 2:21 - loss: 3.3684 - accuracy: 0.12 - ETA: 2:20 - loss: 3.3694 - accuracy: 0.12 - ETA: 2:19 - loss: 3.3692 - accuracy: 0.12 - ETA: 2:19 - loss: 3.3695 - accuracy: 0.12 - ETA: 2:18 - loss: 3.3684 - accuracy: 0.12 - ETA: 2:17 - loss: 3.3681 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3683 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3686 - accuracy: 0.12 - ETA: 2:15 - loss: 3.3681 - accuracy: 0.12 - ETA: 2:14 - loss: 3.3674 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3664 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3664 - accuracy: 0.12 - ETA: 2:12 - loss: 3.3657 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3669 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3657 - accuracy: 0.12 - ETA: 2:10 - loss: 3.3653 - accuracy: 0.12 - ETA: 2:09 - loss: 3.3649 - accuracy: 0.12 - ETA: 2:08 - loss: 3.3631 - accuracy: 0.12 - ETA: 2:08 - loss: 3.3627 - accuracy: 0.12 - ETA: 2:07 - loss: 3.3614 - accuracy: 0.12 - ETA: 2:06 - loss: 3.3609 - accuracy: 0.12 - ETA: 2:05 - loss: 3.3607 - accuracy: 0.12 - ETA: 2:05 - loss: 3.3603 - accuracy: 0.12 - ETA: 2:04 - loss: 3.3599 - accuracy: 0.12 - ETA: 2:03 - loss: 3.3603 - accuracy: 0.12 - ETA: 2:02 - loss: 3.3590 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3586 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3570 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3564 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3566 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3567 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3570 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3583 - accuracy: 0.12 - ETA: 1:57 - loss: 3.3576 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3575 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3575 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3577 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3577 - accuracy: 0.12 - ETA: 1:53 - loss: 3.3572 - accuracy: 0.12 - ETA: 1:52 - loss: 3.3572 - accuracy: 0.12 - ETA: 1:51 - loss: 3.3562 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3551 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3556 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3551 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3546 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3545 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3548 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3542 - accuracy: 0.1299"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3550 - accuracy: 0.12 - ETA: 1:45 - loss: 3.3538 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3540 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3532 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3532 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3533 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3528 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3513 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3508 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3508 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3505 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3509 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3497 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3495 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3488 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3495 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3492 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3483 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3473 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3466 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3474 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3475 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3463 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3454 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3456 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3460 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3462 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3455 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3450 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3446 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3448 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3447 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3451 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3439 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3433 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3434 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3430 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3428 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3437 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3427 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3420 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3414 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3415 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3417 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3416 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3408 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3403 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3397 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3399 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3397 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3403 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3405 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3417 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3415 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3411 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3405 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3403 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3407 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3411 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3418 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3414 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3410 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3407 - accuracy: 0.13 - ETA: 59s - loss: 3.3409 - accuracy: 0.1336 - ETA: 58s - loss: 3.3410 - accuracy: 0.133 - ETA: 58s - loss: 3.3412 - accuracy: 0.133 - ETA: 57s - loss: 3.3409 - accuracy: 0.133 - ETA: 56s - loss: 3.3408 - accuracy: 0.133 - ETA: 55s - loss: 3.3410 - accuracy: 0.133 - ETA: 55s - loss: 3.3407 - accuracy: 0.133 - ETA: 54s - loss: 3.3401 - accuracy: 0.133 - ETA: 53s - loss: 3.3398 - accuracy: 0.133 - ETA: 52s - loss: 3.3389 - accuracy: 0.134 - ETA: 52s - loss: 3.3388 - accuracy: 0.134 - ETA: 51s - loss: 3.3382 - accuracy: 0.134 - ETA: 50s - loss: 3.3377 - accuracy: 0.134 - ETA: 50s - loss: 3.3375 - accuracy: 0.134 - ETA: 49s - loss: 3.3375 - accuracy: 0.134 - ETA: 48s - loss: 3.3372 - accuracy: 0.134 - ETA: 47s - loss: 3.3378 - accuracy: 0.134 - ETA: 47s - loss: 3.3376 - accuracy: 0.134 - ETA: 46s - loss: 3.3379 - accuracy: 0.134 - ETA: 45s - loss: 3.3374 - accuracy: 0.134 - ETA: 44s - loss: 3.3375 - accuracy: 0.134 - ETA: 44s - loss: 3.3370 - accuracy: 0.134 - ETA: 43s - loss: 3.3367 - accuracy: 0.134 - ETA: 42s - loss: 3.3363 - accuracy: 0.134 - ETA: 41s - loss: 3.3360 - accuracy: 0.134 - ETA: 41s - loss: 3.3355 - accuracy: 0.134 - ETA: 40s - loss: 3.3352 - accuracy: 0.134 - ETA: 39s - loss: 3.3354 - accuracy: 0.134 - ETA: 39s - loss: 3.3352 - accuracy: 0.134 - ETA: 38s - loss: 3.3344 - accuracy: 0.135 - ETA: 37s - loss: 3.3340 - accuracy: 0.135 - ETA: 36s - loss: 3.3340 - accuracy: 0.135 - ETA: 36s - loss: 3.3340 - accuracy: 0.135 - ETA: 35s - loss: 3.3341 - accuracy: 0.134 - ETA: 34s - loss: 3.3338 - accuracy: 0.135 - ETA: 33s - loss: 3.3335 - accuracy: 0.135 - ETA: 33s - loss: 3.3333 - accuracy: 0.135 - ETA: 32s - loss: 3.3334 - accuracy: 0.135 - ETA: 31s - loss: 3.3332 - accuracy: 0.135 - ETA: 30s - loss: 3.3324 - accuracy: 0.135 - ETA: 30s - loss: 3.3328 - accuracy: 0.135 - ETA: 29s - loss: 3.3332 - accuracy: 0.135 - ETA: 28s - loss: 3.3335 - accuracy: 0.135 - ETA: 28s - loss: 3.3334 - accuracy: 0.135 - ETA: 27s - loss: 3.3336 - accuracy: 0.135 - ETA: 26s - loss: 3.3339 - accuracy: 0.135 - ETA: 25s - loss: 3.3339 - accuracy: 0.135 - ETA: 25s - loss: 3.3331 - accuracy: 0.135 - ETA: 24s - loss: 3.3330 - accuracy: 0.134 - ETA: 23s - loss: 3.3330 - accuracy: 0.135 - ETA: 22s - loss: 3.3323 - accuracy: 0.135 - ETA: 22s - loss: 3.3319 - accuracy: 0.135 - ETA: 21s - loss: 3.3317 - accuracy: 0.135 - ETA: 20s - loss: 3.3317 - accuracy: 0.135 - ETA: 19s - loss: 3.3313 - accuracy: 0.135 - ETA: 19s - loss: 3.3312 - accuracy: 0.135 - ETA: 18s - loss: 3.3313 - accuracy: 0.135 - ETA: 17s - loss: 3.3314 - accuracy: 0.135 - ETA: 17s - loss: 3.3307 - accuracy: 0.135 - ETA: 16s - loss: 3.3307 - accuracy: 0.135 - ETA: 15s - loss: 3.3300 - accuracy: 0.135 - ETA: 14s - loss: 3.3298 - accuracy: 0.135 - ETA: 14s - loss: 3.3298 - accuracy: 0.135 - ETA: 13s - loss: 3.3295 - accuracy: 0.135 - ETA: 12s - loss: 3.3296 - accuracy: 0.135 - ETA: 11s - loss: 3.3288 - accuracy: 0.136 - ETA: 11s - loss: 3.3286 - accuracy: 0.136 - ETA: 10s - loss: 3.3285 - accuracy: 0.136 - ETA: 9s - loss: 3.3280 - accuracy: 0.136 - ETA: 8s - loss: 3.3279 - accuracy: 0.13 - ETA: 8s - loss: 3.3282 - accuracy: 0.13 - ETA: 7s - loss: 3.3285 - accuracy: 0.13 - ETA: 6s - loss: 3.3282 - accuracy: 0.13 - ETA: 5s - loss: 3.3281 - accuracy: 0.13 - ETA: 5s - loss: 3.3286 - accuracy: 0.13 - ETA: 4s - loss: 3.3279 - accuracy: 0.13 - ETA: 3s - loss: 3.3276 - accuracy: 0.13 - ETA: 3s - loss: 3.3273 - accuracy: 0.13 - ETA: 2s - loss: 3.3267 - accuracy: 0.13 - ETA: 1s - loss: 3.3272 - accuracy: 0.13 - ETA: 0s - loss: 3.3266 - accuracy: 0.13 - ETA: 0s - loss: 3.3271 - accuracy: 0.13 - 262s 6ms/step - loss: 3.3270 - accuracy: 0.1363 - val_loss: 4.0893 - val_accuracy: 0.0315\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:55 - loss: 3.2296 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3697 - accuracy: 0.10 - ETA: 3:55 - loss: 3.3020 - accuracy: 0.11 - ETA: 3:57 - loss: 3.3178 - accuracy: 0.12 - ETA: 3:56 - loss: 3.3305 - accuracy: 0.12 - ETA: 3:56 - loss: 3.3115 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3014 - accuracy: 0.13 - ETA: 3:55 - loss: 3.2991 - accuracy: 0.13 - ETA: 3:53 - loss: 3.3154 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3321 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3318 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3368 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3356 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3152 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3025 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3157 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3163 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3230 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3212 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3264 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3259 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3239 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3169 - accuracy: 0.14 - ETA: 3:43 - loss: 3.3090 - accuracy: 0.14 - ETA: 3:42 - loss: 3.3053 - accuracy: 0.14 - ETA: 3:41 - loss: 3.2989 - accuracy: 0.14 - ETA: 3:41 - loss: 3.3004 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3023 - accuracy: 0.14 - ETA: 3:39 - loss: 3.3039 - accuracy: 0.14 - ETA: 3:38 - loss: 3.3021 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2950 - accuracy: 0.14 - ETA: 3:37 - loss: 3.2955 - accuracy: 0.14 - ETA: 3:37 - loss: 3.2931 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2935 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2933 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2882 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2905 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2927 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2885 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2879 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2866 - accuracy: 0.14 - ETA: 3:31 - loss: 3.2843 - accuracy: 0.14 - ETA: 3:31 - loss: 3.2857 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2852 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2827 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2841 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2840 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2840 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2820 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2774 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2780 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2789 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2830 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2817 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2826 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2870 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2882 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2908 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2958 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2935 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2896 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2940 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2950 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2945 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2957 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2952 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2966 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2934 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2944 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2950 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2958 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2971 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2979 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2983 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2979 - accuracy: 0.13 - ETA: 3:06 - loss: 3.2989 - accuracy: 0.13 - ETA: 3:06 - loss: 3.2989 - accuracy: 0.13 - ETA: 3:05 - loss: 3.2996 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3010 - accuracy: 0.13 - ETA: 3:04 - loss: 3.2995 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3006 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2997 - accuracy: 0.14 - ETA: 3:02 - loss: 3.3013 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3014 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3024 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3029 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3053 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3052 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3061 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3063 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3064 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3065 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3054 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3023 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3047 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3056 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3052 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3052 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3065 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3068 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3049 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3044 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3054 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3049 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3037 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3033 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3030 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3030 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3029 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3038 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3032 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3028 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3047 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3055 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3052 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3052 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3053 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3052 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3046 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3038 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3038 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3019 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3013 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3020 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3024 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3022 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3023 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3017 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3024 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3031 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3033 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3039 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3055 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3062 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3116 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3114 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3116 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3125 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3130 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3142 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3149 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3146 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3139 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3144 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3138 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3135 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3137 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3133 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3127 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3130 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3136 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3133 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3122 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3126 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3119 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3121 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3111 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3106 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3093 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3088 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3094 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3083 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3080 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3081 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3084 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3078 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3071 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3078 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3073 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3080 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3080 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3077 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3065 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3064 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3061 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3058 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3056 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3050 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3056 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3061 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3064 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3058 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3051 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3048 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3052 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3057 - accuracy: 0.1370"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3048 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3043 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3051 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3049 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3049 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3054 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3051 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3049 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3047 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3040 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3053 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3070 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3088 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3085 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3080 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3077 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3076 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3078 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3084 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3090 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3094 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3096 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3102 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3104 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3106 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3110 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3113 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3120 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3127 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3128 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3125 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3120 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3120 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3118 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3123 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3123 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3124 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3128 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3131 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3139 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3141 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3148 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3148 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3146 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3148 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3149 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3157 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3171 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3166 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3170 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3173 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3187 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3195 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3201 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3213 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3212 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3213 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3213 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3218 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3219 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3224 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3258 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3255 - accuracy: 0.13 - ETA: 59s - loss: 3.3262 - accuracy: 0.1364 - ETA: 59s - loss: 3.3269 - accuracy: 0.136 - ETA: 58s - loss: 3.3268 - accuracy: 0.136 - ETA: 57s - loss: 3.3267 - accuracy: 0.136 - ETA: 56s - loss: 3.3265 - accuracy: 0.136 - ETA: 56s - loss: 3.3261 - accuracy: 0.136 - ETA: 55s - loss: 3.3262 - accuracy: 0.136 - ETA: 54s - loss: 3.3259 - accuracy: 0.136 - ETA: 53s - loss: 3.3257 - accuracy: 0.136 - ETA: 53s - loss: 3.3257 - accuracy: 0.136 - ETA: 52s - loss: 3.3261 - accuracy: 0.136 - ETA: 51s - loss: 3.3265 - accuracy: 0.136 - ETA: 50s - loss: 3.3268 - accuracy: 0.136 - ETA: 50s - loss: 3.3270 - accuracy: 0.136 - ETA: 49s - loss: 3.3274 - accuracy: 0.136 - ETA: 48s - loss: 3.3283 - accuracy: 0.136 - ETA: 48s - loss: 3.3284 - accuracy: 0.136 - ETA: 47s - loss: 3.3287 - accuracy: 0.136 - ETA: 46s - loss: 3.3288 - accuracy: 0.136 - ETA: 45s - loss: 3.3298 - accuracy: 0.136 - ETA: 45s - loss: 3.3300 - accuracy: 0.136 - ETA: 44s - loss: 3.3305 - accuracy: 0.135 - ETA: 43s - loss: 3.3315 - accuracy: 0.135 - ETA: 42s - loss: 3.3318 - accuracy: 0.135 - ETA: 42s - loss: 3.3314 - accuracy: 0.135 - ETA: 41s - loss: 3.3311 - accuracy: 0.135 - ETA: 40s - loss: 3.3311 - accuracy: 0.135 - ETA: 39s - loss: 3.3315 - accuracy: 0.135 - ETA: 39s - loss: 3.3315 - accuracy: 0.135 - ETA: 38s - loss: 3.3308 - accuracy: 0.135 - ETA: 37s - loss: 3.3309 - accuracy: 0.135 - ETA: 36s - loss: 3.3309 - accuracy: 0.135 - ETA: 36s - loss: 3.3311 - accuracy: 0.135 - ETA: 35s - loss: 3.3312 - accuracy: 0.135 - ETA: 34s - loss: 3.3312 - accuracy: 0.135 - ETA: 33s - loss: 3.3312 - accuracy: 0.135 - ETA: 33s - loss: 3.3315 - accuracy: 0.135 - ETA: 32s - loss: 3.3309 - accuracy: 0.135 - ETA: 31s - loss: 3.3310 - accuracy: 0.135 - ETA: 31s - loss: 3.3311 - accuracy: 0.135 - ETA: 30s - loss: 3.3315 - accuracy: 0.135 - ETA: 29s - loss: 3.3319 - accuracy: 0.135 - ETA: 28s - loss: 3.3317 - accuracy: 0.135 - ETA: 28s - loss: 3.3313 - accuracy: 0.135 - ETA: 27s - loss: 3.3317 - accuracy: 0.135 - ETA: 26s - loss: 3.3318 - accuracy: 0.135 - ETA: 25s - loss: 3.3317 - accuracy: 0.135 - ETA: 25s - loss: 3.3313 - accuracy: 0.135 - ETA: 24s - loss: 3.3312 - accuracy: 0.135 - ETA: 23s - loss: 3.3310 - accuracy: 0.135 - ETA: 22s - loss: 3.3311 - accuracy: 0.135 - ETA: 22s - loss: 3.3313 - accuracy: 0.135 - ETA: 21s - loss: 3.3303 - accuracy: 0.136 - ETA: 20s - loss: 3.3306 - accuracy: 0.135 - ETA: 19s - loss: 3.3307 - accuracy: 0.136 - ETA: 19s - loss: 3.3299 - accuracy: 0.136 - ETA: 18s - loss: 3.3297 - accuracy: 0.136 - ETA: 17s - loss: 3.3296 - accuracy: 0.136 - ETA: 17s - loss: 3.3294 - accuracy: 0.136 - ETA: 16s - loss: 3.3288 - accuracy: 0.136 - ETA: 15s - loss: 3.3292 - accuracy: 0.136 - ETA: 14s - loss: 3.3289 - accuracy: 0.136 - ETA: 14s - loss: 3.3284 - accuracy: 0.136 - ETA: 13s - loss: 3.3285 - accuracy: 0.136 - ETA: 12s - loss: 3.3280 - accuracy: 0.136 - ETA: 11s - loss: 3.3281 - accuracy: 0.136 - ETA: 11s - loss: 3.3282 - accuracy: 0.136 - ETA: 10s - loss: 3.3275 - accuracy: 0.136 - ETA: 9s - loss: 3.3277 - accuracy: 0.136 - ETA: 8s - loss: 3.3278 - accuracy: 0.13 - ETA: 8s - loss: 3.3271 - accuracy: 0.13 - ETA: 7s - loss: 3.3270 - accuracy: 0.13 - ETA: 6s - loss: 3.3274 - accuracy: 0.13 - ETA: 5s - loss: 3.3275 - accuracy: 0.13 - ETA: 5s - loss: 3.3274 - accuracy: 0.13 - ETA: 4s - loss: 3.3277 - accuracy: 0.13 - ETA: 3s - loss: 3.3282 - accuracy: 0.13 - ETA: 3s - loss: 3.3283 - accuracy: 0.13 - ETA: 2s - loss: 3.3288 - accuracy: 0.13 - ETA: 1s - loss: 3.3289 - accuracy: 0.13 - ETA: 0s - loss: 3.3293 - accuracy: 0.13 - ETA: 0s - loss: 3.3294 - accuracy: 0.13 - 261s 6ms/step - loss: 3.3294 - accuracy: 0.1363 - val_loss: 3.9433 - val_accuracy: 0.0312\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:59 - loss: 3.3098 - accuracy: 0.08 - ETA: 3:58 - loss: 3.3339 - accuracy: 0.08 - ETA: 3:57 - loss: 3.3479 - accuracy: 0.11 - ETA: 3:55 - loss: 3.3710 - accuracy: 0.11 - ETA: 3:59 - loss: 3.3719 - accuracy: 0.11 - ETA: 3:59 - loss: 3.3306 - accuracy: 0.12 - ETA: 3:58 - loss: 3.3506 - accuracy: 0.12 - ETA: 3:57 - loss: 3.3351 - accuracy: 0.12 - ETA: 3:57 - loss: 3.3405 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3290 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3426 - accuracy: 0.12 - ETA: 3:53 - loss: 3.3621 - accuracy: 0.12 - ETA: 3:53 - loss: 3.3720 - accuracy: 0.12 - ETA: 3:52 - loss: 3.3632 - accuracy: 0.12 - ETA: 3:51 - loss: 3.3582 - accuracy: 0.12 - ETA: 3:51 - loss: 3.3545 - accuracy: 0.12 - ETA: 3:50 - loss: 3.3457 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3456 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3466 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3511 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3375 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3411 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3439 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3389 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3445 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3346 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3281 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3279 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3321 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3236 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3223 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3216 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3252 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3301 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3277 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3224 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3219 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3245 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3168 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3160 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3172 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3187 - accuracy: 0.13 - ETA: 3:33 - loss: 3.3200 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3147 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3130 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3116 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3126 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3107 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3110 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3077 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3066 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3056 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3089 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3081 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3090 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3102 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3073 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3075 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3090 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3085 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3101 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3123 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3121 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3130 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3138 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3126 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3107 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3144 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3151 - accuracy: 0.13 - ETA: 3:12 - loss: 3.3120 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3115 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3112 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3109 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3129 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3154 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3147 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3155 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3171 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3186 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3189 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3198 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3211 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3234 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3226 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3235 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3226 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3226 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3244 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3260 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3259 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3280 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3273 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3275 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3272 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3271 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3257 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3262 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3266 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3264 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3263 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3267 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3275 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3271 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3272 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3269 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3270 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3262 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3252 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3244 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3249 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3235 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3238 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3231 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3238 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3232 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3212 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3211 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3201 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3209 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3214 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3223 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3241 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3241 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3255 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3253 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3237 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3235 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3219 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3234 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3230 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3229 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3232 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3238 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3242 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3239 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3234 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3238 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3236 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3232 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3217 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3214 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3204 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3196 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3201 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3204 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3211 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3220 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3223 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3249 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3260 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3259 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3251 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3263 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3268 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3278 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3270 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3270 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3272 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3277 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3275 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3273 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3273 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3272 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3277 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3271 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3272 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3268 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3264 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3267 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3278 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3270 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3276 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3276 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3274 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3278 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3281 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3270 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3253 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3246 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3240 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3240 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3247 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3242 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3241 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3245 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3252 - accuracy: 0.1374"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3245 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3239 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3242 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3239 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3243 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3239 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3234 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3219 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3214 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3211 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3204 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3194 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3194 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3191 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3197 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3193 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3188 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3192 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3184 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3183 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3179 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3177 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3178 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3172 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3173 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3166 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3154 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3152 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3153 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3146 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3140 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3134 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3134 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3131 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3128 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3131 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3132 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3127 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3128 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3124 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3130 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3125 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3129 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3120 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3117 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3111 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3111 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3109 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3102 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3099 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3101 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3100 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3096 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3102 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3102 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3107 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3113 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3111 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3110 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3107 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3102 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3103 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3098 - accuracy: 0.13 - ETA: 59s - loss: 3.3093 - accuracy: 0.1398 - ETA: 58s - loss: 3.3093 - accuracy: 0.139 - ETA: 58s - loss: 3.3099 - accuracy: 0.139 - ETA: 57s - loss: 3.3102 - accuracy: 0.139 - ETA: 56s - loss: 3.3102 - accuracy: 0.139 - ETA: 55s - loss: 3.3096 - accuracy: 0.139 - ETA: 55s - loss: 3.3096 - accuracy: 0.139 - ETA: 54s - loss: 3.3109 - accuracy: 0.139 - ETA: 53s - loss: 3.3110 - accuracy: 0.139 - ETA: 52s - loss: 3.3106 - accuracy: 0.139 - ETA: 52s - loss: 3.3106 - accuracy: 0.139 - ETA: 51s - loss: 3.3099 - accuracy: 0.139 - ETA: 50s - loss: 3.3104 - accuracy: 0.139 - ETA: 50s - loss: 3.3105 - accuracy: 0.139 - ETA: 49s - loss: 3.3107 - accuracy: 0.139 - ETA: 48s - loss: 3.3114 - accuracy: 0.139 - ETA: 47s - loss: 3.3111 - accuracy: 0.139 - ETA: 47s - loss: 3.3111 - accuracy: 0.139 - ETA: 46s - loss: 3.3114 - accuracy: 0.139 - ETA: 45s - loss: 3.3103 - accuracy: 0.139 - ETA: 44s - loss: 3.3105 - accuracy: 0.139 - ETA: 44s - loss: 3.3104 - accuracy: 0.139 - ETA: 43s - loss: 3.3101 - accuracy: 0.139 - ETA: 42s - loss: 3.3102 - accuracy: 0.139 - ETA: 41s - loss: 3.3105 - accuracy: 0.139 - ETA: 41s - loss: 3.3103 - accuracy: 0.139 - ETA: 40s - loss: 3.3102 - accuracy: 0.139 - ETA: 39s - loss: 3.3100 - accuracy: 0.139 - ETA: 39s - loss: 3.3095 - accuracy: 0.139 - ETA: 38s - loss: 3.3091 - accuracy: 0.139 - ETA: 37s - loss: 3.3097 - accuracy: 0.139 - ETA: 36s - loss: 3.3096 - accuracy: 0.139 - ETA: 36s - loss: 3.3097 - accuracy: 0.139 - ETA: 35s - loss: 3.3093 - accuracy: 0.139 - ETA: 34s - loss: 3.3099 - accuracy: 0.139 - ETA: 33s - loss: 3.3102 - accuracy: 0.139 - ETA: 33s - loss: 3.3104 - accuracy: 0.139 - ETA: 32s - loss: 3.3102 - accuracy: 0.139 - ETA: 31s - loss: 3.3106 - accuracy: 0.139 - ETA: 30s - loss: 3.3105 - accuracy: 0.139 - ETA: 30s - loss: 3.3109 - accuracy: 0.139 - ETA: 29s - loss: 3.3112 - accuracy: 0.139 - ETA: 28s - loss: 3.3107 - accuracy: 0.139 - ETA: 27s - loss: 3.3104 - accuracy: 0.139 - ETA: 27s - loss: 3.3102 - accuracy: 0.139 - ETA: 26s - loss: 3.3103 - accuracy: 0.139 - ETA: 25s - loss: 3.3099 - accuracy: 0.139 - ETA: 25s - loss: 3.3103 - accuracy: 0.139 - ETA: 24s - loss: 3.3103 - accuracy: 0.139 - ETA: 23s - loss: 3.3106 - accuracy: 0.139 - ETA: 22s - loss: 3.3102 - accuracy: 0.139 - ETA: 22s - loss: 3.3105 - accuracy: 0.139 - ETA: 21s - loss: 3.3103 - accuracy: 0.139 - ETA: 20s - loss: 3.3104 - accuracy: 0.139 - ETA: 19s - loss: 3.3104 - accuracy: 0.139 - ETA: 19s - loss: 3.3106 - accuracy: 0.139 - ETA: 18s - loss: 3.3113 - accuracy: 0.139 - ETA: 17s - loss: 3.3110 - accuracy: 0.139 - ETA: 16s - loss: 3.3109 - accuracy: 0.139 - ETA: 16s - loss: 3.3112 - accuracy: 0.139 - ETA: 15s - loss: 3.3106 - accuracy: 0.139 - ETA: 14s - loss: 3.3100 - accuracy: 0.139 - ETA: 14s - loss: 3.3099 - accuracy: 0.139 - ETA: 13s - loss: 3.3103 - accuracy: 0.139 - ETA: 12s - loss: 3.3106 - accuracy: 0.139 - ETA: 11s - loss: 3.3105 - accuracy: 0.139 - ETA: 11s - loss: 3.3107 - accuracy: 0.139 - ETA: 10s - loss: 3.3099 - accuracy: 0.140 - ETA: 9s - loss: 3.3100 - accuracy: 0.140 - ETA: 8s - loss: 3.3103 - accuracy: 0.14 - ETA: 8s - loss: 3.3102 - accuracy: 0.13 - ETA: 7s - loss: 3.3094 - accuracy: 0.14 - ETA: 6s - loss: 3.3090 - accuracy: 0.14 - ETA: 5s - loss: 3.3086 - accuracy: 0.14 - ETA: 5s - loss: 3.3083 - accuracy: 0.14 - ETA: 4s - loss: 3.3078 - accuracy: 0.14 - ETA: 3s - loss: 3.3083 - accuracy: 0.14 - ETA: 2s - loss: 3.3081 - accuracy: 0.14 - ETA: 2s - loss: 3.3075 - accuracy: 0.14 - ETA: 1s - loss: 3.3079 - accuracy: 0.14 - ETA: 0s - loss: 3.3079 - accuracy: 0.14 - ETA: 0s - loss: 3.3076 - accuracy: 0.14 - 260s 6ms/step - loss: 3.3076 - accuracy: 0.1406 - val_loss: 3.9668 - val_accuracy: 0.0254\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:15 - loss: 3.3565 - accuracy: 0.13 - ETA: 4:05 - loss: 3.4032 - accuracy: 0.12 - ETA: 4:03 - loss: 3.3503 - accuracy: 0.13 - ETA: 4:00 - loss: 3.3115 - accuracy: 0.14 - ETA: 3:58 - loss: 3.3172 - accuracy: 0.14 - ETA: 3:56 - loss: 3.2992 - accuracy: 0.14 - ETA: 3:59 - loss: 3.3069 - accuracy: 0.14 - ETA: 3:57 - loss: 3.3215 - accuracy: 0.14 - ETA: 3:56 - loss: 3.3295 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3222 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3180 - accuracy: 0.14 - ETA: 3:54 - loss: 3.3106 - accuracy: 0.14 - ETA: 3:53 - loss: 3.2946 - accuracy: 0.15 - ETA: 3:52 - loss: 3.2971 - accuracy: 0.15 - ETA: 3:51 - loss: 3.2992 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3010 - accuracy: 0.14 - ETA: 3:50 - loss: 3.2977 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3033 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3112 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3026 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3003 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3019 - accuracy: 0.14 - ETA: 3:45 - loss: 3.3078 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3089 - accuracy: 0.15 - ETA: 3:44 - loss: 3.3074 - accuracy: 0.15 - ETA: 3:43 - loss: 3.3056 - accuracy: 0.15 - ETA: 3:43 - loss: 3.2999 - accuracy: 0.15 - ETA: 3:42 - loss: 3.3014 - accuracy: 0.15 - ETA: 3:41 - loss: 3.2997 - accuracy: 0.15 - ETA: 3:40 - loss: 3.3049 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3047 - accuracy: 0.14 - ETA: 3:39 - loss: 3.3011 - accuracy: 0.14 - ETA: 3:38 - loss: 3.3033 - accuracy: 0.15 - ETA: 3:38 - loss: 3.2992 - accuracy: 0.15 - ETA: 3:37 - loss: 3.2981 - accuracy: 0.15 - ETA: 3:36 - loss: 3.2932 - accuracy: 0.15 - ETA: 3:35 - loss: 3.2944 - accuracy: 0.15 - ETA: 3:35 - loss: 3.2955 - accuracy: 0.15 - ETA: 3:34 - loss: 3.2962 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2968 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2981 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3004 - accuracy: 0.14 - ETA: 3:31 - loss: 3.3005 - accuracy: 0.14 - ETA: 3:31 - loss: 3.2976 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2987 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2995 - accuracy: 0.14 - ETA: 3:29 - loss: 3.3003 - accuracy: 0.14 - ETA: 3:28 - loss: 3.3047 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3044 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3052 - accuracy: 0.14 - ETA: 3:26 - loss: 3.3021 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2962 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2983 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2981 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2974 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2951 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2954 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2942 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2964 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2966 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2972 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2952 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2954 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2949 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2934 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2952 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2976 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2974 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2972 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2979 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2972 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2978 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2990 - accuracy: 0.14 - ETA: 3:08 - loss: 3.3004 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2989 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2982 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2993 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2988 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2979 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2964 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2984 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2979 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2967 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2970 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2963 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2966 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2980 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2988 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2990 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2960 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2965 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2976 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2968 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2981 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2981 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2991 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2993 - accuracy: 0.14 - ETA: 2:51 - loss: 3.3005 - accuracy: 0.14 - ETA: 2:50 - loss: 3.3020 - accuracy: 0.14 - ETA: 2:49 - loss: 3.3016 - accuracy: 0.14 - ETA: 2:49 - loss: 3.3027 - accuracy: 0.14 - ETA: 2:48 - loss: 3.3026 - accuracy: 0.14 - ETA: 2:47 - loss: 3.3030 - accuracy: 0.14 - ETA: 2:46 - loss: 3.3031 - accuracy: 0.14 - ETA: 2:46 - loss: 3.3032 - accuracy: 0.14 - ETA: 2:45 - loss: 3.3033 - accuracy: 0.14 - ETA: 2:44 - loss: 3.3041 - accuracy: 0.14 - ETA: 2:44 - loss: 3.3031 - accuracy: 0.14 - ETA: 2:43 - loss: 3.3014 - accuracy: 0.14 - ETA: 2:42 - loss: 3.3015 - accuracy: 0.14 - ETA: 2:41 - loss: 3.3001 - accuracy: 0.14 - ETA: 2:40 - loss: 3.3007 - accuracy: 0.14 - ETA: 2:40 - loss: 3.3012 - accuracy: 0.14 - ETA: 2:39 - loss: 3.3008 - accuracy: 0.14 - ETA: 2:38 - loss: 3.3015 - accuracy: 0.14 - ETA: 2:37 - loss: 3.3008 - accuracy: 0.14 - ETA: 2:37 - loss: 3.3020 - accuracy: 0.14 - ETA: 2:36 - loss: 3.3021 - accuracy: 0.14 - ETA: 2:35 - loss: 3.3009 - accuracy: 0.14 - ETA: 2:34 - loss: 3.3001 - accuracy: 0.14 - ETA: 2:34 - loss: 3.3007 - accuracy: 0.14 - ETA: 2:33 - loss: 3.3024 - accuracy: 0.14 - ETA: 2:32 - loss: 3.3027 - accuracy: 0.14 - ETA: 2:31 - loss: 3.3016 - accuracy: 0.14 - ETA: 2:31 - loss: 3.3018 - accuracy: 0.14 - ETA: 2:30 - loss: 3.3013 - accuracy: 0.14 - ETA: 2:29 - loss: 3.3013 - accuracy: 0.14 - ETA: 2:28 - loss: 3.3006 - accuracy: 0.14 - ETA: 2:28 - loss: 3.3003 - accuracy: 0.14 - ETA: 2:27 - loss: 3.3011 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3004 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3001 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2992 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2994 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3003 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2989 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2990 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2992 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2992 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2991 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2986 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2986 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2984 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2993 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3002 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3011 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3013 - accuracy: 0.14 - ETA: 2:14 - loss: 3.3016 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3028 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3030 - accuracy: 0.14 - ETA: 2:12 - loss: 3.3035 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3034 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3038 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3049 - accuracy: 0.14 - ETA: 2:09 - loss: 3.3040 - accuracy: 0.14 - ETA: 2:08 - loss: 3.3031 - accuracy: 0.14 - ETA: 2:07 - loss: 3.3033 - accuracy: 0.14 - ETA: 2:07 - loss: 3.3026 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3024 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3022 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3019 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3025 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3038 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3031 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3025 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3016 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3017 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3020 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3027 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3029 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3023 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3022 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3036 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3038 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3039 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3049 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3052 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3053 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3049 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3052 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3042 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3041 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3034 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3026 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3024 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3017 - accuracy: 0.1412"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3025 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3031 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3036 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3036 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3036 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3030 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3036 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3033 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3044 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3054 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3059 - accuracy: 0.14 - ETA: 1:37 - loss: 3.3055 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3056 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3059 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3062 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3068 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3064 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3066 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3067 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3066 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3066 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3059 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3060 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3059 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3057 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3050 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3053 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3054 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3046 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3048 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3042 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3036 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3035 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3036 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3040 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3037 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3033 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3034 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3042 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3033 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3033 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3031 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3026 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3028 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3027 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3027 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3021 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3023 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3020 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3023 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3023 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3019 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3024 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3014 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3014 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3016 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3007 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3006 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3000 - accuracy: 0.13 - ETA: 1:02 - loss: 3.2997 - accuracy: 0.13 - ETA: 1:01 - loss: 3.2995 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3003 - accuracy: 0.13 - ETA: 1:00 - loss: 3.2994 - accuracy: 0.13 - ETA: 59s - loss: 3.2993 - accuracy: 0.1401 - ETA: 58s - loss: 3.2998 - accuracy: 0.140 - ETA: 58s - loss: 3.3004 - accuracy: 0.140 - ETA: 57s - loss: 3.3002 - accuracy: 0.140 - ETA: 56s - loss: 3.3004 - accuracy: 0.140 - ETA: 55s - loss: 3.3005 - accuracy: 0.140 - ETA: 55s - loss: 3.3005 - accuracy: 0.140 - ETA: 54s - loss: 3.3008 - accuracy: 0.140 - ETA: 53s - loss: 3.3007 - accuracy: 0.140 - ETA: 52s - loss: 3.3008 - accuracy: 0.140 - ETA: 52s - loss: 3.3009 - accuracy: 0.139 - ETA: 51s - loss: 3.3004 - accuracy: 0.139 - ETA: 50s - loss: 3.2995 - accuracy: 0.140 - ETA: 50s - loss: 3.3004 - accuracy: 0.140 - ETA: 49s - loss: 3.3004 - accuracy: 0.140 - ETA: 48s - loss: 3.3007 - accuracy: 0.139 - ETA: 47s - loss: 3.3010 - accuracy: 0.139 - ETA: 47s - loss: 3.3009 - accuracy: 0.140 - ETA: 46s - loss: 3.3006 - accuracy: 0.140 - ETA: 45s - loss: 3.3011 - accuracy: 0.140 - ETA: 44s - loss: 3.3005 - accuracy: 0.140 - ETA: 44s - loss: 3.3001 - accuracy: 0.140 - ETA: 43s - loss: 3.2999 - accuracy: 0.140 - ETA: 42s - loss: 3.3004 - accuracy: 0.140 - ETA: 41s - loss: 3.2999 - accuracy: 0.140 - ETA: 41s - loss: 3.3003 - accuracy: 0.140 - ETA: 40s - loss: 3.3004 - accuracy: 0.140 - ETA: 39s - loss: 3.3000 - accuracy: 0.140 - ETA: 39s - loss: 3.3001 - accuracy: 0.140 - ETA: 38s - loss: 3.2999 - accuracy: 0.140 - ETA: 37s - loss: 3.3000 - accuracy: 0.140 - ETA: 36s - loss: 3.3003 - accuracy: 0.140 - ETA: 36s - loss: 3.3009 - accuracy: 0.140 - ETA: 35s - loss: 3.3007 - accuracy: 0.140 - ETA: 34s - loss: 3.3008 - accuracy: 0.140 - ETA: 33s - loss: 3.3010 - accuracy: 0.140 - ETA: 33s - loss: 3.3017 - accuracy: 0.140 - ETA: 32s - loss: 3.3018 - accuracy: 0.140 - ETA: 31s - loss: 3.3018 - accuracy: 0.140 - ETA: 30s - loss: 3.3015 - accuracy: 0.140 - ETA: 30s - loss: 3.3021 - accuracy: 0.140 - ETA: 29s - loss: 3.3018 - accuracy: 0.140 - ETA: 28s - loss: 3.3022 - accuracy: 0.140 - ETA: 27s - loss: 3.3023 - accuracy: 0.140 - ETA: 27s - loss: 3.3027 - accuracy: 0.140 - ETA: 26s - loss: 3.3025 - accuracy: 0.140 - ETA: 25s - loss: 3.3023 - accuracy: 0.140 - ETA: 25s - loss: 3.3017 - accuracy: 0.140 - ETA: 24s - loss: 3.3016 - accuracy: 0.140 - ETA: 23s - loss: 3.3017 - accuracy: 0.140 - ETA: 22s - loss: 3.3014 - accuracy: 0.140 - ETA: 22s - loss: 3.3021 - accuracy: 0.139 - ETA: 21s - loss: 3.3023 - accuracy: 0.140 - ETA: 20s - loss: 3.3022 - accuracy: 0.140 - ETA: 19s - loss: 3.3019 - accuracy: 0.140 - ETA: 19s - loss: 3.3018 - accuracy: 0.140 - ETA: 18s - loss: 3.3017 - accuracy: 0.140 - ETA: 17s - loss: 3.3021 - accuracy: 0.140 - ETA: 16s - loss: 3.3021 - accuracy: 0.140 - ETA: 16s - loss: 3.3026 - accuracy: 0.139 - ETA: 15s - loss: 3.3030 - accuracy: 0.139 - ETA: 14s - loss: 3.3032 - accuracy: 0.139 - ETA: 14s - loss: 3.3030 - accuracy: 0.139 - ETA: 13s - loss: 3.3026 - accuracy: 0.139 - ETA: 12s - loss: 3.3029 - accuracy: 0.139 - ETA: 11s - loss: 3.3029 - accuracy: 0.139 - ETA: 11s - loss: 3.3023 - accuracy: 0.140 - ETA: 10s - loss: 3.3025 - accuracy: 0.139 - ETA: 9s - loss: 3.3028 - accuracy: 0.139 - ETA: 8s - loss: 3.3028 - accuracy: 0.13 - ETA: 8s - loss: 3.3027 - accuracy: 0.13 - ETA: 7s - loss: 3.3027 - accuracy: 0.13 - ETA: 6s - loss: 3.3028 - accuracy: 0.13 - ETA: 5s - loss: 3.3031 - accuracy: 0.13 - ETA: 5s - loss: 3.3027 - accuracy: 0.13 - ETA: 4s - loss: 3.3019 - accuracy: 0.14 - ETA: 3s - loss: 3.3019 - accuracy: 0.13 - ETA: 2s - loss: 3.3016 - accuracy: 0.14 - ETA: 2s - loss: 3.3013 - accuracy: 0.14 - ETA: 1s - loss: 3.3016 - accuracy: 0.14 - ETA: 0s - loss: 3.3021 - accuracy: 0.13 - ETA: 0s - loss: 3.3021 - accuracy: 0.13 - 261s 6ms/step - loss: 3.3021 - accuracy: 0.1399 - val_loss: 4.0618 - val_accuracy: 0.0308\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:06 - loss: 3.3682 - accuracy: 0.15 - ETA: 4:03 - loss: 3.2643 - accuracy: 0.17 - ETA: 4:03 - loss: 3.2464 - accuracy: 0.16 - ETA: 4:00 - loss: 3.2457 - accuracy: 0.16 - ETA: 3:57 - loss: 3.2416 - accuracy: 0.15 - ETA: 3:55 - loss: 3.2464 - accuracy: 0.14 - ETA: 3:55 - loss: 3.2824 - accuracy: 0.14 - ETA: 3:54 - loss: 3.2824 - accuracy: 0.14 - ETA: 3:54 - loss: 3.2915 - accuracy: 0.14 - ETA: 3:53 - loss: 3.2984 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3080 - accuracy: 0.14 - ETA: 3:51 - loss: 3.3113 - accuracy: 0.14 - ETA: 3:51 - loss: 3.3059 - accuracy: 0.14 - ETA: 3:50 - loss: 3.2951 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3076 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3157 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3070 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3059 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3051 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3009 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3027 - accuracy: 0.14 - ETA: 3:46 - loss: 3.2999 - accuracy: 0.14 - ETA: 3:46 - loss: 3.2939 - accuracy: 0.14 - ETA: 3:45 - loss: 3.2936 - accuracy: 0.14 - ETA: 3:44 - loss: 3.2958 - accuracy: 0.14 - ETA: 3:44 - loss: 3.2991 - accuracy: 0.14 - ETA: 3:43 - loss: 3.2970 - accuracy: 0.14 - ETA: 3:42 - loss: 3.3003 - accuracy: 0.14 - ETA: 3:41 - loss: 3.2982 - accuracy: 0.14 - ETA: 3:40 - loss: 3.2951 - accuracy: 0.14 - ETA: 3:39 - loss: 3.2961 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2943 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2989 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3008 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3033 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2990 - accuracy: 0.14 - ETA: 3:34 - loss: 3.3005 - accuracy: 0.14 - ETA: 3:33 - loss: 3.3000 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2951 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3002 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2986 - accuracy: 0.14 - ETA: 3:31 - loss: 3.2982 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2932 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2910 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2929 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2915 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2887 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2934 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2928 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2915 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2883 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2843 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2845 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2867 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2871 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2839 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2844 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2859 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2874 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2895 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2941 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2952 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2976 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2970 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3007 - accuracy: 0.14 - ETA: 3:13 - loss: 3.3018 - accuracy: 0.14 - ETA: 3:12 - loss: 3.3020 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2973 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3000 - accuracy: 0.14 - ETA: 3:10 - loss: 3.3004 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2995 - accuracy: 0.14 - ETA: 3:09 - loss: 3.3010 - accuracy: 0.14 - ETA: 3:08 - loss: 3.3020 - accuracy: 0.14 - ETA: 3:07 - loss: 3.3002 - accuracy: 0.14 - ETA: 3:07 - loss: 3.3006 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3028 - accuracy: 0.14 - ETA: 3:05 - loss: 3.3049 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3047 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3022 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3006 - accuracy: 0.14 - ETA: 3:02 - loss: 3.3021 - accuracy: 0.14 - ETA: 3:01 - loss: 3.3003 - accuracy: 0.14 - ETA: 3:00 - loss: 3.3005 - accuracy: 0.14 - ETA: 3:00 - loss: 3.3000 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2986 - accuracy: 0.14 - ETA: 2:58 - loss: 3.3005 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2987 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2987 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2964 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2964 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2969 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2984 - accuracy: 0.14 - ETA: 2:53 - loss: 3.3015 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2989 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2974 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2969 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2942 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2931 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2930 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2923 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2933 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2935 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2953 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2948 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2943 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2952 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2954 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2949 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2946 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2950 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2943 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2950 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2968 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2970 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2964 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2981 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2982 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2978 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2985 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2981 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2989 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2982 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2990 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2993 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2997 - accuracy: 0.14 - ETA: 2:29 - loss: 3.3010 - accuracy: 0.14 - ETA: 2:28 - loss: 3.3005 - accuracy: 0.14 - ETA: 2:28 - loss: 3.3011 - accuracy: 0.14 - ETA: 2:27 - loss: 3.3011 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3029 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3028 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3032 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3039 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3015 - accuracy: 0.14 - ETA: 2:23 - loss: 3.3018 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3049 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3046 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3055 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3066 - accuracy: 0.14 - ETA: 2:19 - loss: 3.3071 - accuracy: 0.14 - ETA: 2:19 - loss: 3.3076 - accuracy: 0.14 - ETA: 2:18 - loss: 3.3072 - accuracy: 0.14 - ETA: 2:17 - loss: 3.3080 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3070 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3075 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3078 - accuracy: 0.14 - ETA: 2:14 - loss: 3.3076 - accuracy: 0.14 - ETA: 2:14 - loss: 3.3073 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3074 - accuracy: 0.14 - ETA: 2:12 - loss: 3.3064 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3078 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3082 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3090 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3083 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3080 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3088 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3099 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3098 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3097 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3101 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3099 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3098 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3106 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3097 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3104 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3108 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3111 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3119 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3118 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3122 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3136 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3131 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3120 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3119 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3112 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3112 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3109 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3109 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3094 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3093 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3090 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3087 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3097 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3108 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3115 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3115 - accuracy: 0.1395"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3115 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3105 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3101 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3108 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3111 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3108 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3110 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3107 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3102 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3099 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3098 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3092 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3095 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3102 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3095 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3100 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3104 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3109 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3108 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3104 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3099 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3092 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3087 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3080 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3079 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3077 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3078 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3081 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3078 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3076 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3068 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3063 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3067 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3064 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3064 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3068 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3071 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3061 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3057 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3055 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3054 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3051 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3047 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3048 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3050 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3047 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3046 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3039 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3042 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3049 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3053 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3058 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3065 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3057 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3068 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3071 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3067 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3070 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3069 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3071 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3069 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3063 - accuracy: 0.13 - ETA: 59s - loss: 3.3064 - accuracy: 0.1387 - ETA: 59s - loss: 3.3059 - accuracy: 0.138 - ETA: 58s - loss: 3.3056 - accuracy: 0.138 - ETA: 57s - loss: 3.3057 - accuracy: 0.138 - ETA: 56s - loss: 3.3058 - accuracy: 0.138 - ETA: 56s - loss: 3.3057 - accuracy: 0.138 - ETA: 55s - loss: 3.3054 - accuracy: 0.139 - ETA: 54s - loss: 3.3044 - accuracy: 0.139 - ETA: 54s - loss: 3.3041 - accuracy: 0.139 - ETA: 53s - loss: 3.3038 - accuracy: 0.139 - ETA: 52s - loss: 3.3038 - accuracy: 0.139 - ETA: 51s - loss: 3.3037 - accuracy: 0.139 - ETA: 51s - loss: 3.3035 - accuracy: 0.139 - ETA: 50s - loss: 3.3043 - accuracy: 0.139 - ETA: 49s - loss: 3.3044 - accuracy: 0.139 - ETA: 48s - loss: 3.3038 - accuracy: 0.139 - ETA: 48s - loss: 3.3045 - accuracy: 0.139 - ETA: 47s - loss: 3.3044 - accuracy: 0.139 - ETA: 46s - loss: 3.3038 - accuracy: 0.139 - ETA: 46s - loss: 3.3035 - accuracy: 0.139 - ETA: 45s - loss: 3.3040 - accuracy: 0.139 - ETA: 44s - loss: 3.3037 - accuracy: 0.139 - ETA: 43s - loss: 3.3029 - accuracy: 0.139 - ETA: 43s - loss: 3.3026 - accuracy: 0.139 - ETA: 42s - loss: 3.3023 - accuracy: 0.139 - ETA: 41s - loss: 3.3028 - accuracy: 0.139 - ETA: 40s - loss: 3.3031 - accuracy: 0.139 - ETA: 40s - loss: 3.3029 - accuracy: 0.139 - ETA: 39s - loss: 3.3031 - accuracy: 0.139 - ETA: 38s - loss: 3.3034 - accuracy: 0.139 - ETA: 37s - loss: 3.3031 - accuracy: 0.139 - ETA: 37s - loss: 3.3034 - accuracy: 0.139 - ETA: 36s - loss: 3.3040 - accuracy: 0.139 - ETA: 35s - loss: 3.3036 - accuracy: 0.139 - ETA: 35s - loss: 3.3027 - accuracy: 0.139 - ETA: 34s - loss: 3.3023 - accuracy: 0.139 - ETA: 33s - loss: 3.3023 - accuracy: 0.139 - ETA: 32s - loss: 3.3022 - accuracy: 0.140 - ETA: 32s - loss: 3.3017 - accuracy: 0.140 - ETA: 31s - loss: 3.3015 - accuracy: 0.140 - ETA: 30s - loss: 3.3019 - accuracy: 0.139 - ETA: 29s - loss: 3.3017 - accuracy: 0.139 - ETA: 29s - loss: 3.3012 - accuracy: 0.140 - ETA: 28s - loss: 3.3008 - accuracy: 0.140 - ETA: 27s - loss: 3.3015 - accuracy: 0.140 - ETA: 27s - loss: 3.3017 - accuracy: 0.140 - ETA: 26s - loss: 3.3020 - accuracy: 0.140 - ETA: 25s - loss: 3.3014 - accuracy: 0.140 - ETA: 24s - loss: 3.3011 - accuracy: 0.140 - ETA: 24s - loss: 3.3014 - accuracy: 0.140 - ETA: 23s - loss: 3.3012 - accuracy: 0.140 - ETA: 22s - loss: 3.3010 - accuracy: 0.140 - ETA: 21s - loss: 3.3012 - accuracy: 0.140 - ETA: 21s - loss: 3.3012 - accuracy: 0.140 - ETA: 20s - loss: 3.3018 - accuracy: 0.140 - ETA: 19s - loss: 3.3018 - accuracy: 0.140 - ETA: 19s - loss: 3.3020 - accuracy: 0.140 - ETA: 18s - loss: 3.3015 - accuracy: 0.140 - ETA: 17s - loss: 3.3016 - accuracy: 0.140 - ETA: 16s - loss: 3.3012 - accuracy: 0.141 - ETA: 16s - loss: 3.3006 - accuracy: 0.141 - ETA: 15s - loss: 3.3006 - accuracy: 0.141 - ETA: 14s - loss: 3.3005 - accuracy: 0.141 - ETA: 13s - loss: 3.3006 - accuracy: 0.141 - ETA: 13s - loss: 3.3007 - accuracy: 0.141 - ETA: 12s - loss: 3.3004 - accuracy: 0.141 - ETA: 11s - loss: 3.3010 - accuracy: 0.140 - ETA: 11s - loss: 3.3010 - accuracy: 0.140 - ETA: 10s - loss: 3.3014 - accuracy: 0.140 - ETA: 9s - loss: 3.3016 - accuracy: 0.140 - ETA: 8s - loss: 3.3017 - accuracy: 0.14 - ETA: 8s - loss: 3.3016 - accuracy: 0.14 - ETA: 7s - loss: 3.3014 - accuracy: 0.14 - ETA: 6s - loss: 3.3018 - accuracy: 0.14 - ETA: 5s - loss: 3.3023 - accuracy: 0.14 - ETA: 5s - loss: 3.3019 - accuracy: 0.14 - ETA: 4s - loss: 3.3015 - accuracy: 0.14 - ETA: 3s - loss: 3.3011 - accuracy: 0.14 - ETA: 2s - loss: 3.3011 - accuracy: 0.14 - ETA: 2s - loss: 3.3011 - accuracy: 0.14 - ETA: 1s - loss: 3.3008 - accuracy: 0.14 - ETA: 0s - loss: 3.3004 - accuracy: 0.14 - ETA: 0s - loss: 3.2998 - accuracy: 0.14 - 260s 6ms/step - loss: 3.2999 - accuracy: 0.1412 - val_loss: 3.9866 - val_accuracy: 0.0293\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:00 - loss: 3.3872 - accuracy: 0.12 - ETA: 4:00 - loss: 3.2581 - accuracy: 0.14 - ETA: 3:54 - loss: 3.2772 - accuracy: 0.13 - ETA: 3:53 - loss: 3.2663 - accuracy: 0.13 - ETA: 3:53 - loss: 3.2541 - accuracy: 0.13 - ETA: 3:52 - loss: 3.2654 - accuracy: 0.13 - ETA: 3:52 - loss: 3.2697 - accuracy: 0.13 - ETA: 3:51 - loss: 3.2671 - accuracy: 0.13 - ETA: 3:51 - loss: 3.2916 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3005 - accuracy: 0.12 - ETA: 3:49 - loss: 3.3030 - accuracy: 0.12 - ETA: 3:48 - loss: 3.3165 - accuracy: 0.12 - ETA: 3:48 - loss: 3.3036 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3056 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3027 - accuracy: 0.13 - ETA: 3:46 - loss: 3.2938 - accuracy: 0.13 - ETA: 3:45 - loss: 3.2853 - accuracy: 0.14 - ETA: 3:45 - loss: 3.2952 - accuracy: 0.13 - ETA: 3:44 - loss: 3.2993 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3004 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3135 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3187 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3260 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3306 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3314 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3259 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3223 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3144 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3115 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3122 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3127 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3153 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3152 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3153 - accuracy: 0.13 - ETA: 3:33 - loss: 3.3161 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3176 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3180 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3190 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3202 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3194 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3185 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3187 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3134 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3100 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3097 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3091 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3075 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3063 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3061 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3053 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3034 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3034 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3002 - accuracy: 0.13 - ETA: 3:22 - loss: 3.2960 - accuracy: 0.13 - ETA: 3:21 - loss: 3.2986 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3012 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3025 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3015 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3011 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3016 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3041 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3012 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3013 - accuracy: 0.13 - ETA: 3:14 - loss: 3.2989 - accuracy: 0.13 - ETA: 3:13 - loss: 3.2984 - accuracy: 0.13 - ETA: 3:13 - loss: 3.2979 - accuracy: 0.13 - ETA: 3:12 - loss: 3.2982 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3018 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3031 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3046 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3048 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3053 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3076 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3080 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3091 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3085 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3076 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3101 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3089 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3137 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3130 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3144 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3146 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3140 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3155 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3151 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3161 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3175 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3184 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3182 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3179 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3158 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3145 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3158 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3156 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3180 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3190 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3198 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3189 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3209 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3211 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3216 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3214 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3212 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3214 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3214 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3219 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3214 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3215 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3216 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3221 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3223 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3219 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3239 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3238 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3247 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3245 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3244 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3245 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3252 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3262 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3262 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3250 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3242 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3255 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3262 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3284 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3296 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3286 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3280 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3271 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3268 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3259 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3255 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3248 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3250 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3254 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3255 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3247 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3245 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3241 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3227 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3235 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3234 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3239 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3245 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3243 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3248 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3241 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3251 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3232 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3237 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3236 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3240 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3238 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3242 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3230 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3228 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3237 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3242 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3236 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3235 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3227 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3242 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3250 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3243 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3236 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3230 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3242 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3240 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3251 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3244 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3240 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3251 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3261 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3263 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3273 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3260 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3257 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3243 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3249 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3234 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3233 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3225 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3223 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3218 - accuracy: 0.1386"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3209 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3201 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3193 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3188 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3190 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3198 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3201 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3192 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3186 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3187 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3183 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3186 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3180 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3168 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3157 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3158 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3162 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3162 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3170 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3167 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3171 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3176 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3171 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3175 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3171 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3166 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3176 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3176 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3180 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3179 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3170 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3177 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3174 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3171 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3167 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3164 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3160 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3160 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3172 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3174 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3179 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3178 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3175 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3179 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3185 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3187 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3186 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3181 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3182 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3180 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3175 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3177 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3168 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3165 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3161 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3149 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3144 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3141 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3140 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3138 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3138 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3141 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3138 - accuracy: 0.13 - ETA: 59s - loss: 3.3135 - accuracy: 0.1400 - ETA: 58s - loss: 3.3133 - accuracy: 0.140 - ETA: 57s - loss: 3.3128 - accuracy: 0.140 - ETA: 57s - loss: 3.3124 - accuracy: 0.140 - ETA: 56s - loss: 3.3125 - accuracy: 0.140 - ETA: 55s - loss: 3.3122 - accuracy: 0.140 - ETA: 55s - loss: 3.3123 - accuracy: 0.139 - ETA: 54s - loss: 3.3131 - accuracy: 0.139 - ETA: 53s - loss: 3.3127 - accuracy: 0.139 - ETA: 52s - loss: 3.3130 - accuracy: 0.139 - ETA: 52s - loss: 3.3127 - accuracy: 0.139 - ETA: 51s - loss: 3.3132 - accuracy: 0.139 - ETA: 50s - loss: 3.3135 - accuracy: 0.139 - ETA: 49s - loss: 3.3139 - accuracy: 0.139 - ETA: 49s - loss: 3.3140 - accuracy: 0.139 - ETA: 48s - loss: 3.3145 - accuracy: 0.139 - ETA: 47s - loss: 3.3146 - accuracy: 0.139 - ETA: 46s - loss: 3.3140 - accuracy: 0.139 - ETA: 46s - loss: 3.3135 - accuracy: 0.139 - ETA: 45s - loss: 3.3139 - accuracy: 0.139 - ETA: 44s - loss: 3.3132 - accuracy: 0.140 - ETA: 44s - loss: 3.3129 - accuracy: 0.140 - ETA: 43s - loss: 3.3124 - accuracy: 0.140 - ETA: 42s - loss: 3.3130 - accuracy: 0.140 - ETA: 41s - loss: 3.3134 - accuracy: 0.139 - ETA: 41s - loss: 3.3137 - accuracy: 0.139 - ETA: 40s - loss: 3.3132 - accuracy: 0.139 - ETA: 39s - loss: 3.3138 - accuracy: 0.139 - ETA: 38s - loss: 3.3136 - accuracy: 0.140 - ETA: 38s - loss: 3.3136 - accuracy: 0.140 - ETA: 37s - loss: 3.3134 - accuracy: 0.140 - ETA: 36s - loss: 3.3138 - accuracy: 0.140 - ETA: 35s - loss: 3.3136 - accuracy: 0.140 - ETA: 35s - loss: 3.3132 - accuracy: 0.140 - ETA: 34s - loss: 3.3135 - accuracy: 0.140 - ETA: 33s - loss: 3.3134 - accuracy: 0.140 - ETA: 33s - loss: 3.3130 - accuracy: 0.140 - ETA: 32s - loss: 3.3124 - accuracy: 0.140 - ETA: 31s - loss: 3.3122 - accuracy: 0.140 - ETA: 30s - loss: 3.3119 - accuracy: 0.140 - ETA: 30s - loss: 3.3117 - accuracy: 0.140 - ETA: 29s - loss: 3.3113 - accuracy: 0.140 - ETA: 28s - loss: 3.3115 - accuracy: 0.140 - ETA: 27s - loss: 3.3106 - accuracy: 0.141 - ETA: 27s - loss: 3.3101 - accuracy: 0.141 - ETA: 26s - loss: 3.3102 - accuracy: 0.141 - ETA: 25s - loss: 3.3105 - accuracy: 0.141 - ETA: 24s - loss: 3.3104 - accuracy: 0.141 - ETA: 24s - loss: 3.3105 - accuracy: 0.141 - ETA: 23s - loss: 3.3103 - accuracy: 0.141 - ETA: 22s - loss: 3.3099 - accuracy: 0.141 - ETA: 22s - loss: 3.3095 - accuracy: 0.141 - ETA: 21s - loss: 3.3096 - accuracy: 0.141 - ETA: 20s - loss: 3.3081 - accuracy: 0.141 - ETA: 19s - loss: 3.3085 - accuracy: 0.141 - ETA: 19s - loss: 3.3085 - accuracy: 0.141 - ETA: 18s - loss: 3.3086 - accuracy: 0.141 - ETA: 17s - loss: 3.3080 - accuracy: 0.141 - ETA: 16s - loss: 3.3081 - accuracy: 0.141 - ETA: 16s - loss: 3.3072 - accuracy: 0.141 - ETA: 15s - loss: 3.3071 - accuracy: 0.141 - ETA: 14s - loss: 3.3069 - accuracy: 0.141 - ETA: 13s - loss: 3.3064 - accuracy: 0.141 - ETA: 13s - loss: 3.3069 - accuracy: 0.141 - ETA: 12s - loss: 3.3066 - accuracy: 0.141 - ETA: 11s - loss: 3.3068 - accuracy: 0.141 - ETA: 11s - loss: 3.3070 - accuracy: 0.141 - ETA: 10s - loss: 3.3068 - accuracy: 0.141 - ETA: 9s - loss: 3.3070 - accuracy: 0.141 - ETA: 8s - loss: 3.3069 - accuracy: 0.14 - ETA: 8s - loss: 3.3068 - accuracy: 0.14 - ETA: 7s - loss: 3.3066 - accuracy: 0.14 - ETA: 6s - loss: 3.3061 - accuracy: 0.14 - ETA: 5s - loss: 3.3063 - accuracy: 0.14 - ETA: 5s - loss: 3.3065 - accuracy: 0.14 - ETA: 4s - loss: 3.3062 - accuracy: 0.14 - ETA: 3s - loss: 3.3063 - accuracy: 0.14 - ETA: 2s - loss: 3.3060 - accuracy: 0.14 - ETA: 2s - loss: 3.3053 - accuracy: 0.14 - ETA: 1s - loss: 3.3053 - accuracy: 0.14 - ETA: 0s - loss: 3.3051 - accuracy: 0.14 - ETA: 0s - loss: 3.3049 - accuracy: 0.14 - 260s 6ms/step - loss: 3.3046 - accuracy: 0.1413 - val_loss: 4.0436 - val_accuracy: 0.0312\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:06 - loss: 3.1608 - accuracy: 0.15 - ETA: 4:04 - loss: 3.1630 - accuracy: 0.14 - ETA: 4:11 - loss: 3.1735 - accuracy: 0.15 - ETA: 4:07 - loss: 3.2159 - accuracy: 0.14 - ETA: 4:04 - loss: 3.2511 - accuracy: 0.12 - ETA: 4:01 - loss: 3.2413 - accuracy: 0.13 - ETA: 4:02 - loss: 3.2391 - accuracy: 0.14 - ETA: 4:01 - loss: 3.3334 - accuracy: 0.13 - ETA: 3:58 - loss: 3.3017 - accuracy: 0.14 - ETA: 3:57 - loss: 3.3146 - accuracy: 0.14 - ETA: 3:56 - loss: 3.3082 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3136 - accuracy: 0.14 - ETA: 3:54 - loss: 3.3087 - accuracy: 0.14 - ETA: 3:53 - loss: 3.3099 - accuracy: 0.14 - ETA: 3:51 - loss: 3.3064 - accuracy: 0.14 - ETA: 3:53 - loss: 3.3120 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3204 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3203 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3088 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3114 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3060 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3079 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3186 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3227 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3243 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3217 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3149 - accuracy: 0.14 - ETA: 3:42 - loss: 3.3152 - accuracy: 0.14 - ETA: 3:41 - loss: 3.3131 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3068 - accuracy: 0.14 - ETA: 3:39 - loss: 3.3047 - accuracy: 0.14 - ETA: 3:38 - loss: 3.3045 - accuracy: 0.14 - ETA: 3:38 - loss: 3.3025 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3014 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3033 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3063 - accuracy: 0.14 - ETA: 3:35 - loss: 3.3036 - accuracy: 0.14 - ETA: 3:34 - loss: 3.3000 - accuracy: 0.14 - ETA: 3:34 - loss: 3.3028 - accuracy: 0.14 - ETA: 3:33 - loss: 3.3034 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3026 - accuracy: 0.14 - ETA: 3:31 - loss: 3.3031 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3004 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2980 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2983 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2938 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2963 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2977 - accuracy: 0.14 - ETA: 3:26 - loss: 3.3000 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2978 - accuracy: 0.14 - ETA: 3:25 - loss: 3.3023 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3010 - accuracy: 0.14 - ETA: 3:24 - loss: 3.3000 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2980 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2965 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2991 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2992 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2984 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2965 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2962 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2949 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2949 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2927 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2957 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2947 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2918 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2938 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2934 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2961 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2970 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2957 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2922 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2906 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2921 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2905 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2902 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2897 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2886 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2878 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2875 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2865 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2853 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2854 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2862 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2865 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2875 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2866 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2865 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2874 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2878 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2884 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2872 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2869 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2878 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2893 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2889 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2871 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2875 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2867 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2872 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2888 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2889 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2904 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2922 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2949 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2949 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2939 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2941 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2930 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2919 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2907 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2921 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2922 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2914 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2928 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2934 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2935 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2948 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2943 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2939 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2931 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2948 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2954 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2948 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2939 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2955 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2958 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2961 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2967 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2970 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2982 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2990 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2993 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2990 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2982 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2983 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2979 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2990 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2990 - accuracy: 0.14 - ETA: 2:19 - loss: 3.3003 - accuracy: 0.14 - ETA: 2:18 - loss: 3.3009 - accuracy: 0.14 - ETA: 2:18 - loss: 3.3015 - accuracy: 0.14 - ETA: 2:17 - loss: 3.3028 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3029 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3036 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3035 - accuracy: 0.14 - ETA: 2:14 - loss: 3.3035 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3026 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3032 - accuracy: 0.14 - ETA: 2:12 - loss: 3.3052 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3051 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3059 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3058 - accuracy: 0.14 - ETA: 2:09 - loss: 3.3045 - accuracy: 0.14 - ETA: 2:08 - loss: 3.3049 - accuracy: 0.14 - ETA: 2:08 - loss: 3.3052 - accuracy: 0.14 - ETA: 2:07 - loss: 3.3061 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3052 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3044 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3044 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3042 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3054 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3051 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3036 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3044 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3041 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3042 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3043 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3040 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3034 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3037 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3038 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3024 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3036 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3041 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3040 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3045 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3043 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3043 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3044 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3045 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3024 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3021 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3017 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3025 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3019 - accuracy: 0.1427"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3011 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3031 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3032 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3040 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3041 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3034 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3040 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3047 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3058 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3061 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3065 - accuracy: 0.14 - ETA: 1:37 - loss: 3.3059 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3064 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3058 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3059 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3052 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3054 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3059 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3059 - accuracy: 0.14 - ETA: 1:31 - loss: 3.3060 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3052 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3049 - accuracy: 0.14 - ETA: 1:29 - loss: 3.3051 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3047 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3039 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3040 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3034 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3035 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3045 - accuracy: 0.14 - ETA: 1:24 - loss: 3.3045 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3049 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3052 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3045 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3042 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3038 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3030 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3029 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3027 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3021 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3019 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3017 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3015 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3018 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3016 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3026 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3024 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3024 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3038 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3029 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3027 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3029 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3029 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3035 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3029 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3031 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3030 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3036 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3033 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3032 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3026 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3018 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3013 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3020 - accuracy: 0.14 - ETA: 59s - loss: 3.3020 - accuracy: 0.1417 - ETA: 58s - loss: 3.3021 - accuracy: 0.141 - ETA: 57s - loss: 3.3021 - accuracy: 0.141 - ETA: 57s - loss: 3.3020 - accuracy: 0.141 - ETA: 56s - loss: 3.3021 - accuracy: 0.141 - ETA: 55s - loss: 3.3023 - accuracy: 0.141 - ETA: 54s - loss: 3.3021 - accuracy: 0.141 - ETA: 54s - loss: 3.3021 - accuracy: 0.141 - ETA: 53s - loss: 3.3013 - accuracy: 0.142 - ETA: 52s - loss: 3.3010 - accuracy: 0.142 - ETA: 52s - loss: 3.3012 - accuracy: 0.142 - ETA: 51s - loss: 3.3017 - accuracy: 0.141 - ETA: 50s - loss: 3.3010 - accuracy: 0.142 - ETA: 49s - loss: 3.3009 - accuracy: 0.142 - ETA: 49s - loss: 3.3004 - accuracy: 0.142 - ETA: 48s - loss: 3.3009 - accuracy: 0.142 - ETA: 47s - loss: 3.3003 - accuracy: 0.142 - ETA: 46s - loss: 3.3008 - accuracy: 0.142 - ETA: 46s - loss: 3.3003 - accuracy: 0.142 - ETA: 45s - loss: 3.3001 - accuracy: 0.142 - ETA: 44s - loss: 3.3011 - accuracy: 0.142 - ETA: 43s - loss: 3.3008 - accuracy: 0.142 - ETA: 43s - loss: 3.3004 - accuracy: 0.142 - ETA: 42s - loss: 3.3003 - accuracy: 0.142 - ETA: 41s - loss: 3.2998 - accuracy: 0.142 - ETA: 40s - loss: 3.2999 - accuracy: 0.142 - ETA: 40s - loss: 3.2994 - accuracy: 0.142 - ETA: 39s - loss: 3.2994 - accuracy: 0.142 - ETA: 38s - loss: 3.2992 - accuracy: 0.142 - ETA: 38s - loss: 3.2989 - accuracy: 0.142 - ETA: 37s - loss: 3.2990 - accuracy: 0.142 - ETA: 36s - loss: 3.2989 - accuracy: 0.142 - ETA: 35s - loss: 3.2994 - accuracy: 0.142 - ETA: 35s - loss: 3.2995 - accuracy: 0.142 - ETA: 34s - loss: 3.2992 - accuracy: 0.142 - ETA: 33s - loss: 3.2997 - accuracy: 0.142 - ETA: 32s - loss: 3.3003 - accuracy: 0.142 - ETA: 32s - loss: 3.3003 - accuracy: 0.142 - ETA: 31s - loss: 3.3000 - accuracy: 0.142 - ETA: 30s - loss: 3.3001 - accuracy: 0.142 - ETA: 30s - loss: 3.2994 - accuracy: 0.142 - ETA: 29s - loss: 3.2998 - accuracy: 0.142 - ETA: 28s - loss: 3.2992 - accuracy: 0.142 - ETA: 27s - loss: 3.2994 - accuracy: 0.142 - ETA: 27s - loss: 3.2995 - accuracy: 0.142 - ETA: 26s - loss: 3.2996 - accuracy: 0.142 - ETA: 25s - loss: 3.2993 - accuracy: 0.142 - ETA: 24s - loss: 3.2988 - accuracy: 0.142 - ETA: 24s - loss: 3.2983 - accuracy: 0.142 - ETA: 23s - loss: 3.2982 - accuracy: 0.142 - ETA: 22s - loss: 3.2983 - accuracy: 0.142 - ETA: 21s - loss: 3.2983 - accuracy: 0.142 - ETA: 21s - loss: 3.2986 - accuracy: 0.142 - ETA: 20s - loss: 3.2988 - accuracy: 0.142 - ETA: 19s - loss: 3.2987 - accuracy: 0.142 - ETA: 19s - loss: 3.2989 - accuracy: 0.142 - ETA: 18s - loss: 3.2985 - accuracy: 0.142 - ETA: 17s - loss: 3.2983 - accuracy: 0.142 - ETA: 16s - loss: 3.2985 - accuracy: 0.142 - ETA: 16s - loss: 3.2989 - accuracy: 0.142 - ETA: 15s - loss: 3.2989 - accuracy: 0.142 - ETA: 14s - loss: 3.2994 - accuracy: 0.142 - ETA: 13s - loss: 3.2993 - accuracy: 0.142 - ETA: 13s - loss: 3.2993 - accuracy: 0.142 - ETA: 12s - loss: 3.3004 - accuracy: 0.142 - ETA: 11s - loss: 3.3005 - accuracy: 0.142 - ETA: 11s - loss: 3.3002 - accuracy: 0.142 - ETA: 10s - loss: 3.3006 - accuracy: 0.142 - ETA: 9s - loss: 3.3004 - accuracy: 0.141 - ETA: 8s - loss: 3.2998 - accuracy: 0.14 - ETA: 8s - loss: 3.2996 - accuracy: 0.14 - ETA: 7s - loss: 3.3000 - accuracy: 0.14 - ETA: 6s - loss: 3.3002 - accuracy: 0.14 - ETA: 5s - loss: 3.3004 - accuracy: 0.14 - ETA: 5s - loss: 3.2997 - accuracy: 0.14 - ETA: 4s - loss: 3.2994 - accuracy: 0.14 - ETA: 3s - loss: 3.2994 - accuracy: 0.14 - ETA: 2s - loss: 3.2994 - accuracy: 0.14 - ETA: 2s - loss: 3.2993 - accuracy: 0.14 - ETA: 1s - loss: 3.2991 - accuracy: 0.14 - ETA: 0s - loss: 3.2980 - accuracy: 0.14 - ETA: 0s - loss: 3.2978 - accuracy: 0.14 - 261s 6ms/step - loss: 3.2979 - accuracy: 0.1424 - val_loss: 4.1402 - val_accuracy: 0.0316\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:02 - loss: 3.4096 - accuracy: 0.10 - ETA: 3:55 - loss: 3.3337 - accuracy: 0.12 - ETA: 3:53 - loss: 3.2773 - accuracy: 0.14 - ETA: 3:56 - loss: 3.2562 - accuracy: 0.14 - ETA: 3:55 - loss: 3.2847 - accuracy: 0.14 - ETA: 3:55 - loss: 3.2831 - accuracy: 0.14 - ETA: 3:55 - loss: 3.2717 - accuracy: 0.15 - ETA: 3:55 - loss: 3.2565 - accuracy: 0.15 - ETA: 3:54 - loss: 3.2687 - accuracy: 0.14 - ETA: 3:54 - loss: 3.2715 - accuracy: 0.14 - ETA: 3:53 - loss: 3.2613 - accuracy: 0.14 - ETA: 3:53 - loss: 3.2563 - accuracy: 0.14 - ETA: 3:51 - loss: 3.2476 - accuracy: 0.14 - ETA: 3:51 - loss: 3.2513 - accuracy: 0.14 - ETA: 3:49 - loss: 3.2518 - accuracy: 0.14 - ETA: 3:50 - loss: 3.2579 - accuracy: 0.14 - ETA: 3:49 - loss: 3.2518 - accuracy: 0.14 - ETA: 3:48 - loss: 3.2555 - accuracy: 0.14 - ETA: 3:47 - loss: 3.2591 - accuracy: 0.14 - ETA: 3:47 - loss: 3.2588 - accuracy: 0.14 - ETA: 3:46 - loss: 3.2712 - accuracy: 0.14 - ETA: 3:44 - loss: 3.2726 - accuracy: 0.14 - ETA: 3:43 - loss: 3.2734 - accuracy: 0.14 - ETA: 3:43 - loss: 3.2771 - accuracy: 0.14 - ETA: 3:42 - loss: 3.2714 - accuracy: 0.14 - ETA: 3:41 - loss: 3.2683 - accuracy: 0.14 - ETA: 3:41 - loss: 3.2679 - accuracy: 0.14 - ETA: 3:40 - loss: 3.2675 - accuracy: 0.14 - ETA: 3:39 - loss: 3.2620 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2604 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2593 - accuracy: 0.14 - ETA: 3:37 - loss: 3.2690 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2701 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2734 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2709 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2660 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2649 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2682 - accuracy: 0.14 - ETA: 3:31 - loss: 3.2702 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2701 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2668 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2597 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2605 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2608 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2593 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2620 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2601 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2589 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2602 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2590 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2656 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2720 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2707 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2717 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2716 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2737 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2764 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2774 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2786 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2764 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2781 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2767 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2754 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2742 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2761 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2748 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2744 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2747 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2795 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2924 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2998 - accuracy: 0.14 - ETA: 3:09 - loss: 3.3005 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2976 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2975 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2988 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3000 - accuracy: 0.14 - ETA: 3:05 - loss: 3.3011 - accuracy: 0.14 - ETA: 3:04 - loss: 3.3048 - accuracy: 0.14 - ETA: 3:04 - loss: 3.3071 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3079 - accuracy: 0.14 - ETA: 3:02 - loss: 3.3084 - accuracy: 0.14 - ETA: 3:01 - loss: 3.3084 - accuracy: 0.14 - ETA: 3:01 - loss: 3.3091 - accuracy: 0.14 - ETA: 3:00 - loss: 3.3108 - accuracy: 0.14 - ETA: 2:59 - loss: 3.3114 - accuracy: 0.14 - ETA: 2:58 - loss: 3.3115 - accuracy: 0.14 - ETA: 2:58 - loss: 3.3116 - accuracy: 0.14 - ETA: 2:57 - loss: 3.3123 - accuracy: 0.14 - ETA: 2:56 - loss: 3.3138 - accuracy: 0.14 - ETA: 2:56 - loss: 3.3135 - accuracy: 0.14 - ETA: 2:55 - loss: 3.3154 - accuracy: 0.14 - ETA: 2:54 - loss: 3.3176 - accuracy: 0.14 - ETA: 2:54 - loss: 3.3180 - accuracy: 0.14 - ETA: 2:53 - loss: 3.3196 - accuracy: 0.14 - ETA: 2:52 - loss: 3.3179 - accuracy: 0.14 - ETA: 2:51 - loss: 3.3199 - accuracy: 0.14 - ETA: 2:51 - loss: 3.3203 - accuracy: 0.14 - ETA: 2:50 - loss: 3.3221 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3247 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3260 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3268 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3280 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3274 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3282 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3293 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3292 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3284 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3312 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3303 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3320 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3318 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3321 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3332 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3336 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3352 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3360 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3351 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3342 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3363 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3378 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3374 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3389 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3387 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3395 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3413 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3437 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3436 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3436 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3413 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3411 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3424 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3439 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3446 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3436 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3443 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3443 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3436 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3429 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3438 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3440 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3434 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3423 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3431 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3432 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3429 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3439 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3445 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3442 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3447 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3450 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3445 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3440 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3445 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3448 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3460 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3457 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3453 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3454 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3456 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3440 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3433 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3441 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3448 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3437 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3447 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3455 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3456 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3459 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3462 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3456 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3445 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3439 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3434 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3426 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3426 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3416 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3411 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3419 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3418 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3412 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3409 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3401 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3391 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3392 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3393 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3392 - accuracy: 0.1371"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.3383 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3385 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3389 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3387 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3375 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3375 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3378 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3377 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3384 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3379 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3371 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3361 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3361 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3354 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3359 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3360 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3360 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3358 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3352 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3351 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3346 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3353 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3355 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3352 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3341 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3336 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3342 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3337 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3322 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3305 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3302 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3304 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3298 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3293 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3290 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3291 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3292 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3294 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3297 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3291 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3284 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3274 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3268 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3265 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3264 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3265 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3261 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3263 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3263 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3256 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3254 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3261 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3260 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3258 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3256 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3255 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3251 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3249 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3252 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3254 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3254 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3247 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3247 - accuracy: 0.14 - ETA: 59s - loss: 3.3246 - accuracy: 0.1401 - ETA: 58s - loss: 3.3249 - accuracy: 0.140 - ETA: 58s - loss: 3.3249 - accuracy: 0.140 - ETA: 57s - loss: 3.3243 - accuracy: 0.140 - ETA: 56s - loss: 3.3238 - accuracy: 0.140 - ETA: 55s - loss: 3.3240 - accuracy: 0.140 - ETA: 55s - loss: 3.3240 - accuracy: 0.140 - ETA: 54s - loss: 3.3240 - accuracy: 0.140 - ETA: 53s - loss: 3.3245 - accuracy: 0.140 - ETA: 52s - loss: 3.3245 - accuracy: 0.140 - ETA: 52s - loss: 3.3240 - accuracy: 0.140 - ETA: 51s - loss: 3.3238 - accuracy: 0.140 - ETA: 50s - loss: 3.3247 - accuracy: 0.140 - ETA: 49s - loss: 3.3250 - accuracy: 0.139 - ETA: 49s - loss: 3.3246 - accuracy: 0.140 - ETA: 48s - loss: 3.3257 - accuracy: 0.139 - ETA: 47s - loss: 3.3262 - accuracy: 0.139 - ETA: 46s - loss: 3.3256 - accuracy: 0.139 - ETA: 46s - loss: 3.3256 - accuracy: 0.139 - ETA: 45s - loss: 3.3258 - accuracy: 0.139 - ETA: 44s - loss: 3.3257 - accuracy: 0.139 - ETA: 44s - loss: 3.3263 - accuracy: 0.139 - ETA: 43s - loss: 3.3266 - accuracy: 0.139 - ETA: 42s - loss: 3.3276 - accuracy: 0.139 - ETA: 41s - loss: 3.3282 - accuracy: 0.139 - ETA: 41s - loss: 3.3277 - accuracy: 0.139 - ETA: 40s - loss: 3.3278 - accuracy: 0.139 - ETA: 39s - loss: 3.3283 - accuracy: 0.139 - ETA: 38s - loss: 3.3285 - accuracy: 0.139 - ETA: 38s - loss: 3.3283 - accuracy: 0.139 - ETA: 37s - loss: 3.3282 - accuracy: 0.138 - ETA: 36s - loss: 3.3281 - accuracy: 0.139 - ETA: 35s - loss: 3.3284 - accuracy: 0.139 - ETA: 35s - loss: 3.3282 - accuracy: 0.139 - ETA: 34s - loss: 3.3282 - accuracy: 0.139 - ETA: 33s - loss: 3.3284 - accuracy: 0.138 - ETA: 33s - loss: 3.3283 - accuracy: 0.138 - ETA: 32s - loss: 3.3287 - accuracy: 0.138 - ETA: 31s - loss: 3.3286 - accuracy: 0.138 - ETA: 30s - loss: 3.3282 - accuracy: 0.139 - ETA: 30s - loss: 3.3282 - accuracy: 0.139 - ETA: 29s - loss: 3.3280 - accuracy: 0.139 - ETA: 28s - loss: 3.3282 - accuracy: 0.139 - ETA: 27s - loss: 3.3278 - accuracy: 0.139 - ETA: 27s - loss: 3.3276 - accuracy: 0.139 - ETA: 26s - loss: 3.3274 - accuracy: 0.139 - ETA: 25s - loss: 3.3268 - accuracy: 0.139 - ETA: 24s - loss: 3.3267 - accuracy: 0.139 - ETA: 24s - loss: 3.3264 - accuracy: 0.139 - ETA: 23s - loss: 3.3269 - accuracy: 0.139 - ETA: 22s - loss: 3.3270 - accuracy: 0.139 - ETA: 22s - loss: 3.3267 - accuracy: 0.139 - ETA: 21s - loss: 3.3270 - accuracy: 0.139 - ETA: 20s - loss: 3.3269 - accuracy: 0.139 - ETA: 19s - loss: 3.3264 - accuracy: 0.139 - ETA: 19s - loss: 3.3259 - accuracy: 0.139 - ETA: 18s - loss: 3.3259 - accuracy: 0.139 - ETA: 17s - loss: 3.3261 - accuracy: 0.139 - ETA: 16s - loss: 3.3262 - accuracy: 0.139 - ETA: 16s - loss: 3.3264 - accuracy: 0.138 - ETA: 15s - loss: 3.3262 - accuracy: 0.138 - ETA: 14s - loss: 3.3264 - accuracy: 0.138 - ETA: 13s - loss: 3.3265 - accuracy: 0.138 - ETA: 13s - loss: 3.3266 - accuracy: 0.138 - ETA: 12s - loss: 3.3262 - accuracy: 0.138 - ETA: 11s - loss: 3.3263 - accuracy: 0.138 - ETA: 11s - loss: 3.3267 - accuracy: 0.138 - ETA: 10s - loss: 3.3269 - accuracy: 0.138 - ETA: 9s - loss: 3.3267 - accuracy: 0.138 - ETA: 8s - loss: 3.3265 - accuracy: 0.13 - ETA: 8s - loss: 3.3265 - accuracy: 0.13 - ETA: 7s - loss: 3.3260 - accuracy: 0.13 - ETA: 6s - loss: 3.3257 - accuracy: 0.13 - ETA: 5s - loss: 3.3253 - accuracy: 0.13 - ETA: 5s - loss: 3.3247 - accuracy: 0.13 - ETA: 4s - loss: 3.3253 - accuracy: 0.13 - ETA: 3s - loss: 3.3247 - accuracy: 0.13 - ETA: 2s - loss: 3.3246 - accuracy: 0.13 - ETA: 2s - loss: 3.3239 - accuracy: 0.13 - ETA: 1s - loss: 3.3235 - accuracy: 0.13 - ETA: 0s - loss: 3.3233 - accuracy: 0.13 - ETA: 0s - loss: 3.3238 - accuracy: 0.13 - 261s 6ms/step - loss: 3.3238 - accuracy: 0.1391 - val_loss: 4.1293 - val_accuracy: 0.0317\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:57 - loss: 3.1833 - accuracy: 0.11 - ETA: 3:54 - loss: 3.2048 - accuracy: 0.12 - ETA: 3:54 - loss: 3.2206 - accuracy: 0.13 - ETA: 3:54 - loss: 3.2447 - accuracy: 0.13 - ETA: 3:53 - loss: 3.2464 - accuracy: 0.13 - ETA: 3:52 - loss: 3.2620 - accuracy: 0.13 - ETA: 3:55 - loss: 3.2107 - accuracy: 0.14 - ETA: 3:53 - loss: 3.2004 - accuracy: 0.15 - ETA: 3:53 - loss: 3.2114 - accuracy: 0.14 - ETA: 3:52 - loss: 3.2153 - accuracy: 0.14 - ETA: 3:52 - loss: 3.2185 - accuracy: 0.14 - ETA: 3:51 - loss: 3.2290 - accuracy: 0.14 - ETA: 3:50 - loss: 3.2421 - accuracy: 0.14 - ETA: 3:48 - loss: 3.2497 - accuracy: 0.14 - ETA: 3:48 - loss: 3.2609 - accuracy: 0.13 - ETA: 3:48 - loss: 3.2598 - accuracy: 0.13 - ETA: 3:47 - loss: 3.2588 - accuracy: 0.13 - ETA: 3:46 - loss: 3.2633 - accuracy: 0.13 - ETA: 3:46 - loss: 3.2576 - accuracy: 0.13 - ETA: 3:45 - loss: 3.2496 - accuracy: 0.14 - ETA: 3:44 - loss: 3.2569 - accuracy: 0.13 - ETA: 3:43 - loss: 3.2562 - accuracy: 0.13 - ETA: 3:42 - loss: 3.2646 - accuracy: 0.13 - ETA: 3:41 - loss: 3.2650 - accuracy: 0.13 - ETA: 3:40 - loss: 3.2668 - accuracy: 0.13 - ETA: 3:40 - loss: 3.2665 - accuracy: 0.13 - ETA: 3:39 - loss: 3.2644 - accuracy: 0.13 - ETA: 3:38 - loss: 3.2613 - accuracy: 0.13 - ETA: 3:37 - loss: 3.2584 - accuracy: 0.13 - ETA: 3:37 - loss: 3.2545 - accuracy: 0.13 - ETA: 3:36 - loss: 3.2563 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2528 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2510 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2555 - accuracy: 0.13 - ETA: 3:33 - loss: 3.2591 - accuracy: 0.13 - ETA: 3:32 - loss: 3.2617 - accuracy: 0.13 - ETA: 3:31 - loss: 3.2587 - accuracy: 0.14 - ETA: 3:31 - loss: 3.2593 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2623 - accuracy: 0.13 - ETA: 3:30 - loss: 3.2608 - accuracy: 0.13 - ETA: 3:29 - loss: 3.2610 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2585 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2580 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2583 - accuracy: 0.13 - ETA: 3:26 - loss: 3.2573 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2575 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2570 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2559 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2618 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2631 - accuracy: 0.13 - ETA: 3:22 - loss: 3.2639 - accuracy: 0.13 - ETA: 3:21 - loss: 3.2631 - accuracy: 0.13 - ETA: 3:21 - loss: 3.2630 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2645 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2687 - accuracy: 0.13 - ETA: 3:20 - loss: 3.2668 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2687 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2700 - accuracy: 0.13 - ETA: 3:18 - loss: 3.2703 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2704 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2729 - accuracy: 0.13 - ETA: 3:16 - loss: 3.2731 - accuracy: 0.13 - ETA: 3:15 - loss: 3.2722 - accuracy: 0.13 - ETA: 3:14 - loss: 3.2733 - accuracy: 0.13 - ETA: 3:14 - loss: 3.2732 - accuracy: 0.13 - ETA: 3:13 - loss: 3.2720 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2734 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2741 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2748 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2751 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2760 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2742 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2731 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2716 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2717 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2726 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2717 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2726 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2752 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2753 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2741 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2760 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2768 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2743 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2746 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2748 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2753 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2756 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2770 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2749 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2737 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2740 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2733 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2718 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2712 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2691 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2693 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2711 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2732 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2728 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2726 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2723 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2731 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2729 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2728 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2720 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2711 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2705 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2687 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2685 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2668 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2648 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2652 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2643 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2649 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2647 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2638 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2652 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2638 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2651 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2654 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2650 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2660 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2666 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2656 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2656 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2660 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2657 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2668 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2664 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2669 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2681 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2690 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2691 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2688 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2657 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2663 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2674 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2678 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2675 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2694 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2689 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2685 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2708 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2717 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2716 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2712 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2719 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2729 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2729 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2727 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2730 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2723 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2727 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2728 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2729 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2731 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2725 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2722 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2731 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2733 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2737 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2741 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2747 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2754 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2760 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2750 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2750 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2748 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2753 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2751 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2744 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2735 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2736 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2744 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2739 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2747 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2746 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2748 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2748 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2754 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2755 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2750 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2754 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2746 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2753 - accuracy: 0.1466"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.2762 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2758 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2767 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2760 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2762 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2758 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2762 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2779 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2781 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2789 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2787 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2795 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2796 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2807 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2804 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2814 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2814 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2818 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2817 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2815 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2805 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2813 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2813 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2823 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2829 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2828 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2830 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2827 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2824 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2816 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2827 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2833 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2831 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2833 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2835 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2832 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2834 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2844 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2835 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2830 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2822 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2820 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2825 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2819 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2815 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2813 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2821 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2816 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2819 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2835 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2839 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2842 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2846 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2842 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2847 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2849 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2839 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2840 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2840 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2840 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2839 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2840 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2845 - accuracy: 0.14 - ETA: 59s - loss: 3.2848 - accuracy: 0.1445 - ETA: 58s - loss: 3.2845 - accuracy: 0.144 - ETA: 57s - loss: 3.2853 - accuracy: 0.144 - ETA: 57s - loss: 3.2854 - accuracy: 0.144 - ETA: 56s - loss: 3.2850 - accuracy: 0.144 - ETA: 55s - loss: 3.2854 - accuracy: 0.144 - ETA: 55s - loss: 3.2855 - accuracy: 0.144 - ETA: 54s - loss: 3.2854 - accuracy: 0.144 - ETA: 53s - loss: 3.2854 - accuracy: 0.144 - ETA: 52s - loss: 3.2854 - accuracy: 0.144 - ETA: 52s - loss: 3.2851 - accuracy: 0.145 - ETA: 51s - loss: 3.2849 - accuracy: 0.144 - ETA: 50s - loss: 3.2847 - accuracy: 0.144 - ETA: 49s - loss: 3.2852 - accuracy: 0.144 - ETA: 49s - loss: 3.2846 - accuracy: 0.144 - ETA: 48s - loss: 3.2844 - accuracy: 0.144 - ETA: 47s - loss: 3.2846 - accuracy: 0.144 - ETA: 46s - loss: 3.2849 - accuracy: 0.144 - ETA: 46s - loss: 3.2845 - accuracy: 0.144 - ETA: 45s - loss: 3.2847 - accuracy: 0.144 - ETA: 44s - loss: 3.2855 - accuracy: 0.144 - ETA: 44s - loss: 3.2855 - accuracy: 0.144 - ETA: 43s - loss: 3.2856 - accuracy: 0.144 - ETA: 42s - loss: 3.2861 - accuracy: 0.144 - ETA: 41s - loss: 3.2859 - accuracy: 0.144 - ETA: 41s - loss: 3.2862 - accuracy: 0.144 - ETA: 40s - loss: 3.2865 - accuracy: 0.144 - ETA: 39s - loss: 3.2863 - accuracy: 0.144 - ETA: 38s - loss: 3.2863 - accuracy: 0.144 - ETA: 38s - loss: 3.2863 - accuracy: 0.144 - ETA: 37s - loss: 3.2862 - accuracy: 0.144 - ETA: 36s - loss: 3.2863 - accuracy: 0.144 - ETA: 36s - loss: 3.2865 - accuracy: 0.144 - ETA: 35s - loss: 3.2866 - accuracy: 0.144 - ETA: 34s - loss: 3.2861 - accuracy: 0.144 - ETA: 33s - loss: 3.2861 - accuracy: 0.144 - ETA: 33s - loss: 3.2862 - accuracy: 0.144 - ETA: 32s - loss: 3.2862 - accuracy: 0.144 - ETA: 31s - loss: 3.2860 - accuracy: 0.144 - ETA: 30s - loss: 3.2856 - accuracy: 0.144 - ETA: 30s - loss: 3.2859 - accuracy: 0.144 - ETA: 29s - loss: 3.2865 - accuracy: 0.144 - ETA: 28s - loss: 3.2865 - accuracy: 0.144 - ETA: 27s - loss: 3.2872 - accuracy: 0.144 - ETA: 27s - loss: 3.2877 - accuracy: 0.144 - ETA: 26s - loss: 3.2878 - accuracy: 0.144 - ETA: 25s - loss: 3.2876 - accuracy: 0.144 - ETA: 25s - loss: 3.2877 - accuracy: 0.144 - ETA: 24s - loss: 3.2876 - accuracy: 0.144 - ETA: 23s - loss: 3.2875 - accuracy: 0.144 - ETA: 22s - loss: 3.2878 - accuracy: 0.144 - ETA: 22s - loss: 3.2881 - accuracy: 0.144 - ETA: 21s - loss: 3.2892 - accuracy: 0.144 - ETA: 20s - loss: 3.2890 - accuracy: 0.144 - ETA: 19s - loss: 3.2884 - accuracy: 0.144 - ETA: 19s - loss: 3.2886 - accuracy: 0.144 - ETA: 18s - loss: 3.2888 - accuracy: 0.144 - ETA: 17s - loss: 3.2887 - accuracy: 0.144 - ETA: 16s - loss: 3.2890 - accuracy: 0.144 - ETA: 16s - loss: 3.2896 - accuracy: 0.144 - ETA: 15s - loss: 3.2893 - accuracy: 0.144 - ETA: 14s - loss: 3.2897 - accuracy: 0.144 - ETA: 14s - loss: 3.2893 - accuracy: 0.144 - ETA: 13s - loss: 3.2897 - accuracy: 0.144 - ETA: 12s - loss: 3.2901 - accuracy: 0.144 - ETA: 11s - loss: 3.2901 - accuracy: 0.144 - ETA: 11s - loss: 3.2905 - accuracy: 0.144 - ETA: 10s - loss: 3.2905 - accuracy: 0.144 - ETA: 9s - loss: 3.2908 - accuracy: 0.144 - ETA: 8s - loss: 3.2909 - accuracy: 0.14 - ETA: 8s - loss: 3.2909 - accuracy: 0.14 - ETA: 7s - loss: 3.2906 - accuracy: 0.14 - ETA: 6s - loss: 3.2902 - accuracy: 0.14 - ETA: 5s - loss: 3.2904 - accuracy: 0.14 - ETA: 5s - loss: 3.2901 - accuracy: 0.14 - ETA: 4s - loss: 3.2902 - accuracy: 0.14 - ETA: 3s - loss: 3.2904 - accuracy: 0.14 - ETA: 2s - loss: 3.2906 - accuracy: 0.14 - ETA: 2s - loss: 3.2909 - accuracy: 0.14 - ETA: 1s - loss: 3.2910 - accuracy: 0.14 - ETA: 0s - loss: 3.2908 - accuracy: 0.14 - ETA: 0s - loss: 3.2905 - accuracy: 0.14 - 261s 6ms/step - loss: 3.2905 - accuracy: 0.1447 - val_loss: 4.1672 - val_accuracy: 0.0290\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:00 - loss: 3.4451 - accuracy: 0.07 - ETA: 4:02 - loss: 3.3188 - accuracy: 0.11 - ETA: 3:57 - loss: 3.3062 - accuracy: 0.12 - ETA: 3:56 - loss: 3.3065 - accuracy: 0.12 - ETA: 3:56 - loss: 3.3039 - accuracy: 0.12 - ETA: 3:55 - loss: 3.2974 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3109 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3235 - accuracy: 0.12 - ETA: 3:54 - loss: 3.3030 - accuracy: 0.13 - ETA: 3:53 - loss: 3.3053 - accuracy: 0.13 - ETA: 3:52 - loss: 3.2986 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3060 - accuracy: 0.13 - ETA: 3:48 - loss: 3.2997 - accuracy: 0.13 - ETA: 3:48 - loss: 3.2922 - accuracy: 0.13 - ETA: 3:47 - loss: 3.2823 - accuracy: 0.13 - ETA: 3:46 - loss: 3.2855 - accuracy: 0.12 - ETA: 3:45 - loss: 3.2778 - accuracy: 0.13 - ETA: 3:45 - loss: 3.2775 - accuracy: 0.13 - ETA: 3:44 - loss: 3.2765 - accuracy: 0.13 - ETA: 3:44 - loss: 3.2854 - accuracy: 0.13 - ETA: 3:43 - loss: 3.2868 - accuracy: 0.13 - ETA: 3:43 - loss: 3.2873 - accuracy: 0.13 - ETA: 3:42 - loss: 3.2820 - accuracy: 0.13 - ETA: 3:41 - loss: 3.2681 - accuracy: 0.14 - ETA: 3:40 - loss: 3.2700 - accuracy: 0.14 - ETA: 3:40 - loss: 3.2663 - accuracy: 0.14 - ETA: 3:39 - loss: 3.2728 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2693 - accuracy: 0.14 - ETA: 3:37 - loss: 3.2672 - accuracy: 0.14 - ETA: 3:37 - loss: 3.2725 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2663 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2645 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2641 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2671 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2641 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2630 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2614 - accuracy: 0.14 - ETA: 3:31 - loss: 3.2627 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2635 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2640 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2642 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2618 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2607 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2630 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2605 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2662 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2662 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2688 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2679 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2689 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2662 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2690 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2696 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2719 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2762 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2767 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2783 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2799 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2806 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2809 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2813 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2794 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2768 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2772 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2799 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2809 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2803 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2794 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2805 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2834 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2863 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2854 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2861 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2894 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2903 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2907 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2899 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2898 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2934 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2935 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2940 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2924 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2913 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2922 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2927 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2938 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2948 - accuracy: 0.13 - ETA: 2:56 - loss: 3.2964 - accuracy: 0.13 - ETA: 2:55 - loss: 3.2973 - accuracy: 0.13 - ETA: 2:54 - loss: 3.2995 - accuracy: 0.13 - ETA: 2:53 - loss: 3.2992 - accuracy: 0.13 - ETA: 2:53 - loss: 3.2993 - accuracy: 0.13 - ETA: 2:52 - loss: 3.2988 - accuracy: 0.13 - ETA: 2:51 - loss: 3.2968 - accuracy: 0.13 - ETA: 2:51 - loss: 3.2958 - accuracy: 0.13 - ETA: 2:50 - loss: 3.2960 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2963 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2934 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2927 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2943 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2940 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2930 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2927 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2939 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2925 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2927 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2924 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2919 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2919 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2928 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2916 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2908 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2891 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2897 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2899 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2904 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2902 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2904 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2899 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2908 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2921 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2918 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2919 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2933 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2925 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2917 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2928 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2938 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2921 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2912 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2918 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2919 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2924 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2923 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2917 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2907 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2918 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2936 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2936 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2949 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2964 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2957 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2966 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2957 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2966 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2965 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2957 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2971 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2981 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2970 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2971 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2968 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2967 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2968 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2969 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2967 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2963 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2965 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2977 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2974 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2977 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2974 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2956 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2956 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2951 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2957 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2954 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2954 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2943 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2948 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2937 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2945 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2937 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2926 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2932 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2928 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2935 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2933 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2942 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2939 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2942 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2938 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2939 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2936 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2938 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2945 - accuracy: 0.1430"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.2941 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2934 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2937 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2935 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2938 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2936 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2934 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2941 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2947 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2941 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2946 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2947 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2952 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2960 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2961 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2963 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2955 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2956 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2952 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2948 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2954 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2949 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2938 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2933 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2926 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2932 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2935 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2932 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2928 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2926 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2921 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2935 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2925 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2922 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2918 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2923 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2924 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2928 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2921 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2921 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2920 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2923 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2923 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2915 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2911 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2916 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2912 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2910 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2907 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2898 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2896 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2895 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2890 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2893 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2885 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2883 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2877 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2881 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2874 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2864 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2878 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2869 - accuracy: 0.14 - ETA: 59s - loss: 3.2873 - accuracy: 0.1439 - ETA: 59s - loss: 3.2872 - accuracy: 0.143 - ETA: 58s - loss: 3.2871 - accuracy: 0.143 - ETA: 57s - loss: 3.2866 - accuracy: 0.143 - ETA: 57s - loss: 3.2871 - accuracy: 0.143 - ETA: 56s - loss: 3.2871 - accuracy: 0.143 - ETA: 55s - loss: 3.2862 - accuracy: 0.144 - ETA: 54s - loss: 3.2855 - accuracy: 0.144 - ETA: 54s - loss: 3.2858 - accuracy: 0.144 - ETA: 53s - loss: 3.2858 - accuracy: 0.144 - ETA: 52s - loss: 3.2858 - accuracy: 0.144 - ETA: 51s - loss: 3.2862 - accuracy: 0.144 - ETA: 51s - loss: 3.2855 - accuracy: 0.144 - ETA: 50s - loss: 3.2859 - accuracy: 0.144 - ETA: 49s - loss: 3.2859 - accuracy: 0.144 - ETA: 48s - loss: 3.2864 - accuracy: 0.144 - ETA: 48s - loss: 3.2864 - accuracy: 0.144 - ETA: 47s - loss: 3.2861 - accuracy: 0.144 - ETA: 46s - loss: 3.2860 - accuracy: 0.144 - ETA: 46s - loss: 3.2859 - accuracy: 0.144 - ETA: 45s - loss: 3.2860 - accuracy: 0.144 - ETA: 44s - loss: 3.2860 - accuracy: 0.144 - ETA: 43s - loss: 3.2856 - accuracy: 0.144 - ETA: 43s - loss: 3.2855 - accuracy: 0.144 - ETA: 42s - loss: 3.2854 - accuracy: 0.144 - ETA: 41s - loss: 3.2857 - accuracy: 0.144 - ETA: 40s - loss: 3.2863 - accuracy: 0.143 - ETA: 40s - loss: 3.2858 - accuracy: 0.144 - ETA: 39s - loss: 3.2856 - accuracy: 0.143 - ETA: 38s - loss: 3.2861 - accuracy: 0.143 - ETA: 38s - loss: 3.2865 - accuracy: 0.143 - ETA: 37s - loss: 3.2861 - accuracy: 0.143 - ETA: 36s - loss: 3.2860 - accuracy: 0.143 - ETA: 35s - loss: 3.2855 - accuracy: 0.143 - ETA: 35s - loss: 3.2857 - accuracy: 0.143 - ETA: 34s - loss: 3.2857 - accuracy: 0.144 - ETA: 33s - loss: 3.2856 - accuracy: 0.144 - ETA: 32s - loss: 3.2854 - accuracy: 0.144 - ETA: 32s - loss: 3.2857 - accuracy: 0.144 - ETA: 31s - loss: 3.2853 - accuracy: 0.144 - ETA: 30s - loss: 3.2850 - accuracy: 0.144 - ETA: 29s - loss: 3.2858 - accuracy: 0.144 - ETA: 29s - loss: 3.2862 - accuracy: 0.144 - ETA: 28s - loss: 3.2863 - accuracy: 0.144 - ETA: 27s - loss: 3.2859 - accuracy: 0.144 - ETA: 27s - loss: 3.2862 - accuracy: 0.144 - ETA: 26s - loss: 3.2855 - accuracy: 0.144 - ETA: 25s - loss: 3.2857 - accuracy: 0.144 - ETA: 24s - loss: 3.2859 - accuracy: 0.144 - ETA: 24s - loss: 3.2860 - accuracy: 0.144 - ETA: 23s - loss: 3.2859 - accuracy: 0.144 - ETA: 22s - loss: 3.2859 - accuracy: 0.144 - ETA: 21s - loss: 3.2858 - accuracy: 0.144 - ETA: 21s - loss: 3.2854 - accuracy: 0.144 - ETA: 20s - loss: 3.2849 - accuracy: 0.144 - ETA: 19s - loss: 3.2847 - accuracy: 0.144 - ETA: 19s - loss: 3.2847 - accuracy: 0.144 - ETA: 18s - loss: 3.2841 - accuracy: 0.144 - ETA: 17s - loss: 3.2847 - accuracy: 0.144 - ETA: 16s - loss: 3.2850 - accuracy: 0.144 - ETA: 16s - loss: 3.2850 - accuracy: 0.144 - ETA: 15s - loss: 3.2845 - accuracy: 0.144 - ETA: 14s - loss: 3.2845 - accuracy: 0.144 - ETA: 13s - loss: 3.2847 - accuracy: 0.144 - ETA: 13s - loss: 3.2851 - accuracy: 0.144 - ETA: 12s - loss: 3.2850 - accuracy: 0.144 - ETA: 11s - loss: 3.2851 - accuracy: 0.144 - ETA: 11s - loss: 3.2854 - accuracy: 0.144 - ETA: 10s - loss: 3.2851 - accuracy: 0.144 - ETA: 9s - loss: 3.2847 - accuracy: 0.144 - ETA: 8s - loss: 3.2851 - accuracy: 0.14 - ETA: 8s - loss: 3.2852 - accuracy: 0.14 - ETA: 7s - loss: 3.2850 - accuracy: 0.14 - ETA: 6s - loss: 3.2850 - accuracy: 0.14 - ETA: 5s - loss: 3.2851 - accuracy: 0.14 - ETA: 5s - loss: 3.2852 - accuracy: 0.14 - ETA: 4s - loss: 3.2858 - accuracy: 0.14 - ETA: 3s - loss: 3.2860 - accuracy: 0.14 - ETA: 2s - loss: 3.2865 - accuracy: 0.14 - ETA: 2s - loss: 3.2864 - accuracy: 0.14 - ETA: 1s - loss: 3.2866 - accuracy: 0.14 - ETA: 0s - loss: 3.2872 - accuracy: 0.14 - ETA: 0s - loss: 3.2873 - accuracy: 0.14 - 260s 6ms/step - loss: 3.2874 - accuracy: 0.1451 - val_loss: 3.8417 - val_accuracy: 0.0214\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:50 - loss: 3.3070 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3020 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3301 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3646 - accuracy: 0.12 - ETA: 3:50 - loss: 3.3250 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3547 - accuracy: 0.14 - ETA: 3:51 - loss: 3.3551 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3277 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3254 - accuracy: 0.14 - ETA: 3:51 - loss: 3.3461 - accuracy: 0.14 - ETA: 3:51 - loss: 3.3439 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3602 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3549 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3482 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3496 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3550 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3584 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3440 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3424 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3408 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3443 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3352 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3357 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3399 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3350 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3273 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3257 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3292 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3275 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3282 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3260 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3230 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3227 - accuracy: 0.13 - ETA: 3:33 - loss: 3.3193 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3178 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3208 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3170 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3119 - accuracy: 0.14 - ETA: 3:30 - loss: 3.3087 - accuracy: 0.14 - ETA: 3:29 - loss: 3.3061 - accuracy: 0.14 - ETA: 3:28 - loss: 3.3066 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3059 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3068 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3065 - accuracy: 0.14 - ETA: 3:26 - loss: 3.3051 - accuracy: 0.14 - ETA: 3:25 - loss: 3.3052 - accuracy: 0.14 - ETA: 3:25 - loss: 3.3035 - accuracy: 0.14 - ETA: 3:24 - loss: 3.3014 - accuracy: 0.14 - ETA: 3:23 - loss: 3.3022 - accuracy: 0.14 - ETA: 3:23 - loss: 3.3011 - accuracy: 0.14 - ETA: 3:22 - loss: 3.3000 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2948 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2958 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2960 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2983 - accuracy: 0.14 - ETA: 3:19 - loss: 3.3016 - accuracy: 0.14 - ETA: 3:18 - loss: 3.3017 - accuracy: 0.14 - ETA: 3:18 - loss: 3.3017 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3047 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3013 - accuracy: 0.14 - ETA: 3:16 - loss: 3.3025 - accuracy: 0.14 - ETA: 3:15 - loss: 3.3013 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3011 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3027 - accuracy: 0.14 - ETA: 3:13 - loss: 3.3040 - accuracy: 0.14 - ETA: 3:12 - loss: 3.3033 - accuracy: 0.14 - ETA: 3:12 - loss: 3.3019 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2985 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2979 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2986 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2987 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2976 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2926 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2916 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2902 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2923 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2906 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2910 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2893 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2873 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2870 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2885 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2875 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2853 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2870 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2860 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2847 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2841 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2847 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2849 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2862 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2865 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2860 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2875 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2880 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2871 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2876 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2867 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2879 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2865 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2843 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2847 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2858 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2856 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2869 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2873 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2863 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2858 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2868 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2856 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2845 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2839 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2845 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2844 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2829 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2824 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2809 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2816 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2814 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2809 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2805 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2810 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2806 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2806 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2827 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2818 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2806 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2801 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2791 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2796 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2796 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2784 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2769 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2768 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2775 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2790 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2802 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2803 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2799 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2805 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2800 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2804 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2807 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2806 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2799 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2804 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2802 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2789 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2784 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2779 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2784 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2775 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2764 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2763 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2760 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2768 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2769 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2769 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2763 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2769 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2766 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2765 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2770 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2768 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2775 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2787 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2778 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2778 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2780 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2790 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2795 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2801 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2798 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2791 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2783 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2779 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2776 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2767 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2764 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2762 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2751 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2751 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2753 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2749 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2753 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2748 - accuracy: 0.1441"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:42 - loss: 3.2751 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2748 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2745 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2757 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2754 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2745 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2741 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2737 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2732 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2736 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2738 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2739 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2731 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2722 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2719 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2722 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2715 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2717 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2721 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2721 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2729 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2726 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2728 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2711 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2718 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2725 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2734 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2738 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2732 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2722 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2717 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2710 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2718 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2710 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2716 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2719 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2719 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2722 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2717 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2707 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2702 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2702 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2699 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2692 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2688 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2684 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2679 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2671 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2671 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2665 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2667 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2659 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2658 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2654 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2652 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2653 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2646 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2647 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2649 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2651 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2648 - accuracy: 0.14 - ETA: 59s - loss: 3.2649 - accuracy: 0.1463 - ETA: 58s - loss: 3.2654 - accuracy: 0.146 - ETA: 58s - loss: 3.2653 - accuracy: 0.146 - ETA: 57s - loss: 3.2653 - accuracy: 0.146 - ETA: 56s - loss: 3.2644 - accuracy: 0.146 - ETA: 55s - loss: 3.2649 - accuracy: 0.146 - ETA: 55s - loss: 3.2655 - accuracy: 0.145 - ETA: 54s - loss: 3.2657 - accuracy: 0.145 - ETA: 53s - loss: 3.2652 - accuracy: 0.145 - ETA: 53s - loss: 3.2652 - accuracy: 0.145 - ETA: 52s - loss: 3.2653 - accuracy: 0.145 - ETA: 51s - loss: 3.2656 - accuracy: 0.145 - ETA: 50s - loss: 3.2659 - accuracy: 0.145 - ETA: 50s - loss: 3.2653 - accuracy: 0.145 - ETA: 49s - loss: 3.2653 - accuracy: 0.146 - ETA: 48s - loss: 3.2653 - accuracy: 0.146 - ETA: 48s - loss: 3.2655 - accuracy: 0.146 - ETA: 47s - loss: 3.2655 - accuracy: 0.146 - ETA: 46s - loss: 3.2653 - accuracy: 0.146 - ETA: 45s - loss: 3.2652 - accuracy: 0.146 - ETA: 45s - loss: 3.2651 - accuracy: 0.146 - ETA: 44s - loss: 3.2652 - accuracy: 0.146 - ETA: 43s - loss: 3.2652 - accuracy: 0.146 - ETA: 43s - loss: 3.2651 - accuracy: 0.146 - ETA: 42s - loss: 3.2650 - accuracy: 0.146 - ETA: 41s - loss: 3.2655 - accuracy: 0.145 - ETA: 40s - loss: 3.2656 - accuracy: 0.145 - ETA: 40s - loss: 3.2658 - accuracy: 0.145 - ETA: 39s - loss: 3.2654 - accuracy: 0.145 - ETA: 38s - loss: 3.2650 - accuracy: 0.145 - ETA: 38s - loss: 3.2645 - accuracy: 0.145 - ETA: 37s - loss: 3.2645 - accuracy: 0.146 - ETA: 36s - loss: 3.2650 - accuracy: 0.145 - ETA: 35s - loss: 3.2651 - accuracy: 0.145 - ETA: 35s - loss: 3.2650 - accuracy: 0.145 - ETA: 34s - loss: 3.2649 - accuracy: 0.145 - ETA: 33s - loss: 3.2653 - accuracy: 0.145 - ETA: 33s - loss: 3.2647 - accuracy: 0.145 - ETA: 32s - loss: 3.2645 - accuracy: 0.145 - ETA: 31s - loss: 3.2637 - accuracy: 0.145 - ETA: 30s - loss: 3.2638 - accuracy: 0.145 - ETA: 30s - loss: 3.2639 - accuracy: 0.145 - ETA: 29s - loss: 3.2647 - accuracy: 0.145 - ETA: 28s - loss: 3.2651 - accuracy: 0.145 - ETA: 28s - loss: 3.2653 - accuracy: 0.145 - ETA: 27s - loss: 3.2651 - accuracy: 0.145 - ETA: 26s - loss: 3.2651 - accuracy: 0.145 - ETA: 25s - loss: 3.2654 - accuracy: 0.145 - ETA: 25s - loss: 3.2648 - accuracy: 0.145 - ETA: 24s - loss: 3.2650 - accuracy: 0.145 - ETA: 23s - loss: 3.2652 - accuracy: 0.145 - ETA: 23s - loss: 3.2649 - accuracy: 0.145 - ETA: 22s - loss: 3.2645 - accuracy: 0.145 - ETA: 21s - loss: 3.2643 - accuracy: 0.145 - ETA: 20s - loss: 3.2640 - accuracy: 0.145 - ETA: 20s - loss: 3.2640 - accuracy: 0.145 - ETA: 19s - loss: 3.2639 - accuracy: 0.145 - ETA: 18s - loss: 3.2638 - accuracy: 0.145 - ETA: 18s - loss: 3.2629 - accuracy: 0.145 - ETA: 17s - loss: 3.2635 - accuracy: 0.145 - ETA: 16s - loss: 3.2635 - accuracy: 0.145 - ETA: 15s - loss: 3.2632 - accuracy: 0.145 - ETA: 15s - loss: 3.2634 - accuracy: 0.145 - ETA: 14s - loss: 3.2630 - accuracy: 0.145 - ETA: 13s - loss: 3.2627 - accuracy: 0.146 - ETA: 13s - loss: 3.2628 - accuracy: 0.145 - ETA: 12s - loss: 3.2632 - accuracy: 0.145 - ETA: 11s - loss: 3.2622 - accuracy: 0.145 - ETA: 10s - loss: 3.2622 - accuracy: 0.146 - ETA: 10s - loss: 3.2622 - accuracy: 0.146 - ETA: 9s - loss: 3.2621 - accuracy: 0.146 - ETA: 8s - loss: 3.2620 - accuracy: 0.14 - ETA: 7s - loss: 3.2621 - accuracy: 0.14 - ETA: 7s - loss: 3.2624 - accuracy: 0.14 - ETA: 6s - loss: 3.2627 - accuracy: 0.14 - ETA: 5s - loss: 3.2630 - accuracy: 0.14 - ETA: 5s - loss: 3.2635 - accuracy: 0.14 - ETA: 4s - loss: 3.2634 - accuracy: 0.14 - ETA: 3s - loss: 3.2629 - accuracy: 0.14 - ETA: 2s - loss: 3.2632 - accuracy: 0.14 - ETA: 2s - loss: 3.2625 - accuracy: 0.14 - ETA: 1s - loss: 3.2623 - accuracy: 0.14 - ETA: 0s - loss: 3.2624 - accuracy: 0.14 - ETA: 0s - loss: 3.2630 - accuracy: 0.14 - 257s 6ms/step - loss: 3.2630 - accuracy: 0.1464 - val_loss: 3.9002 - val_accuracy: 0.0259\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:57 - loss: 3.2701 - accuracy: 0.14 - ETA: 3:59 - loss: 3.2733 - accuracy: 0.15 - ETA: 4:00 - loss: 3.2752 - accuracy: 0.15 - ETA: 4:00 - loss: 3.3362 - accuracy: 0.14 - ETA: 3:59 - loss: 3.3339 - accuracy: 0.14 - ETA: 3:57 - loss: 3.3491 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3226 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3248 - accuracy: 0.14 - ETA: 3:54 - loss: 3.3107 - accuracy: 0.14 - ETA: 3:53 - loss: 3.2932 - accuracy: 0.15 - ETA: 3:53 - loss: 3.2843 - accuracy: 0.15 - ETA: 3:52 - loss: 3.2847 - accuracy: 0.15 - ETA: 3:51 - loss: 3.2902 - accuracy: 0.15 - ETA: 3:50 - loss: 3.2939 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3013 - accuracy: 0.14 - ETA: 3:49 - loss: 3.2926 - accuracy: 0.14 - ETA: 3:48 - loss: 3.2893 - accuracy: 0.14 - ETA: 3:48 - loss: 3.2947 - accuracy: 0.14 - ETA: 3:47 - loss: 3.2914 - accuracy: 0.14 - ETA: 3:46 - loss: 3.2867 - accuracy: 0.14 - ETA: 3:46 - loss: 3.2781 - accuracy: 0.14 - ETA: 3:45 - loss: 3.2680 - accuracy: 0.14 - ETA: 3:44 - loss: 3.2646 - accuracy: 0.15 - ETA: 3:44 - loss: 3.2685 - accuracy: 0.15 - ETA: 3:43 - loss: 3.2676 - accuracy: 0.15 - ETA: 3:42 - loss: 3.2676 - accuracy: 0.15 - ETA: 3:41 - loss: 3.2718 - accuracy: 0.15 - ETA: 3:40 - loss: 3.2695 - accuracy: 0.15 - ETA: 3:39 - loss: 3.2697 - accuracy: 0.15 - ETA: 3:39 - loss: 3.2756 - accuracy: 0.15 - ETA: 3:38 - loss: 3.2723 - accuracy: 0.15 - ETA: 3:37 - loss: 3.2678 - accuracy: 0.15 - ETA: 3:36 - loss: 3.2687 - accuracy: 0.15 - ETA: 3:36 - loss: 3.2744 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2735 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2753 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2713 - accuracy: 0.15 - ETA: 3:33 - loss: 3.2705 - accuracy: 0.15 - ETA: 3:32 - loss: 3.2736 - accuracy: 0.15 - ETA: 3:32 - loss: 3.2722 - accuracy: 0.15 - ETA: 3:31 - loss: 3.2703 - accuracy: 0.15 - ETA: 3:30 - loss: 3.2678 - accuracy: 0.15 - ETA: 3:30 - loss: 3.2670 - accuracy: 0.15 - ETA: 3:29 - loss: 3.2681 - accuracy: 0.15 - ETA: 3:28 - loss: 3.2702 - accuracy: 0.15 - ETA: 3:28 - loss: 3.2701 - accuracy: 0.15 - ETA: 3:27 - loss: 3.2658 - accuracy: 0.15 - ETA: 3:26 - loss: 3.2639 - accuracy: 0.15 - ETA: 3:25 - loss: 3.2629 - accuracy: 0.15 - ETA: 3:24 - loss: 3.2616 - accuracy: 0.15 - ETA: 3:24 - loss: 3.2608 - accuracy: 0.15 - ETA: 3:23 - loss: 3.2619 - accuracy: 0.15 - ETA: 3:22 - loss: 3.2611 - accuracy: 0.15 - ETA: 3:21 - loss: 3.2601 - accuracy: 0.15 - ETA: 3:21 - loss: 3.2611 - accuracy: 0.15 - ETA: 3:20 - loss: 3.2611 - accuracy: 0.15 - ETA: 3:19 - loss: 3.2586 - accuracy: 0.15 - ETA: 3:19 - loss: 3.2580 - accuracy: 0.15 - ETA: 3:18 - loss: 3.2573 - accuracy: 0.15 - ETA: 3:17 - loss: 3.2587 - accuracy: 0.15 - ETA: 3:17 - loss: 3.2580 - accuracy: 0.15 - ETA: 3:16 - loss: 3.2544 - accuracy: 0.15 - ETA: 3:15 - loss: 3.2560 - accuracy: 0.15 - ETA: 3:15 - loss: 3.2545 - accuracy: 0.15 - ETA: 3:14 - loss: 3.2506 - accuracy: 0.15 - ETA: 3:14 - loss: 3.2513 - accuracy: 0.15 - ETA: 3:13 - loss: 3.2511 - accuracy: 0.15 - ETA: 3:12 - loss: 3.2497 - accuracy: 0.15 - ETA: 3:12 - loss: 3.2519 - accuracy: 0.15 - ETA: 3:11 - loss: 3.2528 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2535 - accuracy: 0.15 - ETA: 3:09 - loss: 3.2527 - accuracy: 0.15 - ETA: 3:08 - loss: 3.2512 - accuracy: 0.15 - ETA: 3:08 - loss: 3.2513 - accuracy: 0.15 - ETA: 3:07 - loss: 3.2491 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2509 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2520 - accuracy: 0.15 - ETA: 3:05 - loss: 3.2516 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2524 - accuracy: 0.15 - ETA: 3:03 - loss: 3.2554 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2564 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2542 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2558 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2581 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2590 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2609 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2610 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2613 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2600 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2583 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2593 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2595 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2616 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2620 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2633 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2647 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2641 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2637 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2636 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2652 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2664 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2680 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2692 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2723 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2731 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2734 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2751 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2732 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2758 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2765 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2752 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2755 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2746 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2742 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2736 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2726 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2720 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2732 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2721 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2730 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2724 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2723 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2731 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2726 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2729 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2720 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2723 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2739 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2728 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2741 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2747 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2759 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2747 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2748 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2771 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2762 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2748 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2756 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2763 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2761 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2757 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2755 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2762 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2768 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2774 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2762 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2768 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2770 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2765 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2767 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2775 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2785 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2801 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2808 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2810 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2827 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2837 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2844 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2850 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2853 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2849 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2841 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2839 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2841 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2832 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2834 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2832 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2830 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2832 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2840 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2843 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2848 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2856 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2858 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2856 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2846 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2839 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2842 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2846 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2836 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2843 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2847 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2846 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2854 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2860 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2868 - accuracy: 0.1472"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.2870 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2863 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2859 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2852 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2848 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2842 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2847 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2851 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2853 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2854 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2844 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2846 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2851 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2858 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2858 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2846 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2854 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2864 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2863 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2862 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2864 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2861 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2854 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2852 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2854 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2855 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2851 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2851 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2856 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2858 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2856 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2853 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2854 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2848 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2851 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2853 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2841 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2836 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2838 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2841 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2838 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2845 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2838 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2841 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2835 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2837 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2838 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2844 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2843 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2843 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2843 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2846 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2848 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2851 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2847 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2847 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2853 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2864 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2865 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2866 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2865 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2861 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2859 - accuracy: 0.14 - ETA: 59s - loss: 3.2860 - accuracy: 0.1481 - ETA: 58s - loss: 3.2862 - accuracy: 0.148 - ETA: 57s - loss: 3.2858 - accuracy: 0.148 - ETA: 57s - loss: 3.2860 - accuracy: 0.148 - ETA: 56s - loss: 3.2861 - accuracy: 0.148 - ETA: 55s - loss: 3.2860 - accuracy: 0.148 - ETA: 54s - loss: 3.2856 - accuracy: 0.148 - ETA: 54s - loss: 3.2862 - accuracy: 0.148 - ETA: 53s - loss: 3.2865 - accuracy: 0.148 - ETA: 52s - loss: 3.2859 - accuracy: 0.148 - ETA: 51s - loss: 3.2858 - accuracy: 0.148 - ETA: 51s - loss: 3.2852 - accuracy: 0.148 - ETA: 50s - loss: 3.2850 - accuracy: 0.148 - ETA: 49s - loss: 3.2842 - accuracy: 0.148 - ETA: 49s - loss: 3.2845 - accuracy: 0.148 - ETA: 48s - loss: 3.2846 - accuracy: 0.148 - ETA: 47s - loss: 3.2855 - accuracy: 0.148 - ETA: 46s - loss: 3.2856 - accuracy: 0.148 - ETA: 46s - loss: 3.2855 - accuracy: 0.148 - ETA: 45s - loss: 3.2855 - accuracy: 0.148 - ETA: 44s - loss: 3.2847 - accuracy: 0.148 - ETA: 43s - loss: 3.2847 - accuracy: 0.148 - ETA: 43s - loss: 3.2840 - accuracy: 0.148 - ETA: 42s - loss: 3.2835 - accuracy: 0.148 - ETA: 41s - loss: 3.2836 - accuracy: 0.148 - ETA: 40s - loss: 3.2832 - accuracy: 0.148 - ETA: 40s - loss: 3.2829 - accuracy: 0.148 - ETA: 39s - loss: 3.2824 - accuracy: 0.148 - ETA: 38s - loss: 3.2825 - accuracy: 0.148 - ETA: 38s - loss: 3.2830 - accuracy: 0.148 - ETA: 37s - loss: 3.2829 - accuracy: 0.148 - ETA: 36s - loss: 3.2828 - accuracy: 0.148 - ETA: 35s - loss: 3.2820 - accuracy: 0.148 - ETA: 35s - loss: 3.2823 - accuracy: 0.148 - ETA: 34s - loss: 3.2822 - accuracy: 0.148 - ETA: 33s - loss: 3.2825 - accuracy: 0.148 - ETA: 32s - loss: 3.2816 - accuracy: 0.148 - ETA: 32s - loss: 3.2817 - accuracy: 0.148 - ETA: 31s - loss: 3.2819 - accuracy: 0.148 - ETA: 30s - loss: 3.2825 - accuracy: 0.148 - ETA: 30s - loss: 3.2824 - accuracy: 0.148 - ETA: 29s - loss: 3.2820 - accuracy: 0.148 - ETA: 28s - loss: 3.2821 - accuracy: 0.148 - ETA: 27s - loss: 3.2818 - accuracy: 0.148 - ETA: 27s - loss: 3.2812 - accuracy: 0.148 - ETA: 26s - loss: 3.2816 - accuracy: 0.148 - ETA: 25s - loss: 3.2819 - accuracy: 0.148 - ETA: 24s - loss: 3.2819 - accuracy: 0.148 - ETA: 24s - loss: 3.2820 - accuracy: 0.148 - ETA: 23s - loss: 3.2820 - accuracy: 0.148 - ETA: 22s - loss: 3.2817 - accuracy: 0.148 - ETA: 21s - loss: 3.2815 - accuracy: 0.148 - ETA: 21s - loss: 3.2815 - accuracy: 0.148 - ETA: 20s - loss: 3.2811 - accuracy: 0.148 - ETA: 19s - loss: 3.2816 - accuracy: 0.148 - ETA: 19s - loss: 3.2816 - accuracy: 0.148 - ETA: 18s - loss: 3.2818 - accuracy: 0.148 - ETA: 17s - loss: 3.2811 - accuracy: 0.148 - ETA: 16s - loss: 3.2804 - accuracy: 0.148 - ETA: 16s - loss: 3.2807 - accuracy: 0.148 - ETA: 15s - loss: 3.2806 - accuracy: 0.148 - ETA: 14s - loss: 3.2808 - accuracy: 0.148 - ETA: 13s - loss: 3.2809 - accuracy: 0.148 - ETA: 13s - loss: 3.2811 - accuracy: 0.148 - ETA: 12s - loss: 3.2810 - accuracy: 0.148 - ETA: 11s - loss: 3.2809 - accuracy: 0.148 - ETA: 11s - loss: 3.2809 - accuracy: 0.148 - ETA: 10s - loss: 3.2806 - accuracy: 0.148 - ETA: 9s - loss: 3.2804 - accuracy: 0.148 - ETA: 8s - loss: 3.2807 - accuracy: 0.14 - ETA: 8s - loss: 3.2808 - accuracy: 0.14 - ETA: 7s - loss: 3.2807 - accuracy: 0.14 - ETA: 6s - loss: 3.2797 - accuracy: 0.14 - ETA: 5s - loss: 3.2799 - accuracy: 0.14 - ETA: 5s - loss: 3.2795 - accuracy: 0.14 - ETA: 4s - loss: 3.2792 - accuracy: 0.14 - ETA: 3s - loss: 3.2793 - accuracy: 0.14 - ETA: 2s - loss: 3.2788 - accuracy: 0.14 - ETA: 2s - loss: 3.2783 - accuracy: 0.14 - ETA: 1s - loss: 3.2785 - accuracy: 0.14 - ETA: 0s - loss: 3.2789 - accuracy: 0.14 - ETA: 0s - loss: 3.2788 - accuracy: 0.14 - 260s 6ms/step - loss: 3.2788 - accuracy: 0.1488 - val_loss: 3.9844 - val_accuracy: 0.0343\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:02 - loss: 3.2250 - accuracy: 0.14 - ETA: 4:00 - loss: 3.2268 - accuracy: 0.16 - ETA: 3:57 - loss: 3.2027 - accuracy: 0.17 - ETA: 4:00 - loss: 3.2466 - accuracy: 0.16 - ETA: 3:58 - loss: 3.2367 - accuracy: 0.17 - ETA: 3:59 - loss: 3.2386 - accuracy: 0.16 - ETA: 3:57 - loss: 3.2533 - accuracy: 0.16 - ETA: 3:56 - loss: 3.2512 - accuracy: 0.15 - ETA: 3:55 - loss: 3.2458 - accuracy: 0.15 - ETA: 3:54 - loss: 3.2361 - accuracy: 0.16 - ETA: 3:53 - loss: 3.2300 - accuracy: 0.16 - ETA: 3:52 - loss: 3.2361 - accuracy: 0.16 - ETA: 3:52 - loss: 3.2369 - accuracy: 0.16 - ETA: 3:50 - loss: 3.2294 - accuracy: 0.16 - ETA: 3:50 - loss: 3.2304 - accuracy: 0.16 - ETA: 3:50 - loss: 3.2331 - accuracy: 0.16 - ETA: 3:50 - loss: 3.2313 - accuracy: 0.16 - ETA: 3:49 - loss: 3.2148 - accuracy: 0.16 - ETA: 3:48 - loss: 3.2146 - accuracy: 0.16 - ETA: 3:47 - loss: 3.2045 - accuracy: 0.16 - ETA: 3:46 - loss: 3.2087 - accuracy: 0.16 - ETA: 3:45 - loss: 3.2120 - accuracy: 0.16 - ETA: 3:44 - loss: 3.2142 - accuracy: 0.16 - ETA: 3:43 - loss: 3.2196 - accuracy: 0.16 - ETA: 3:42 - loss: 3.2252 - accuracy: 0.16 - ETA: 3:41 - loss: 3.2262 - accuracy: 0.16 - ETA: 3:40 - loss: 3.2258 - accuracy: 0.16 - ETA: 3:40 - loss: 3.2277 - accuracy: 0.16 - ETA: 3:39 - loss: 3.2283 - accuracy: 0.16 - ETA: 3:38 - loss: 3.2285 - accuracy: 0.16 - ETA: 3:37 - loss: 3.2328 - accuracy: 0.16 - ETA: 3:36 - loss: 3.2316 - accuracy: 0.16 - ETA: 3:36 - loss: 3.2316 - accuracy: 0.16 - ETA: 3:35 - loss: 3.2265 - accuracy: 0.16 - ETA: 3:34 - loss: 3.2218 - accuracy: 0.16 - ETA: 3:33 - loss: 3.2227 - accuracy: 0.16 - ETA: 3:32 - loss: 3.2254 - accuracy: 0.16 - ETA: 3:32 - loss: 3.2241 - accuracy: 0.16 - ETA: 3:31 - loss: 3.2234 - accuracy: 0.16 - ETA: 3:30 - loss: 3.2267 - accuracy: 0.16 - ETA: 3:29 - loss: 3.2262 - accuracy: 0.16 - ETA: 3:28 - loss: 3.2265 - accuracy: 0.16 - ETA: 3:27 - loss: 3.2293 - accuracy: 0.15 - ETA: 3:27 - loss: 3.2286 - accuracy: 0.15 - ETA: 3:26 - loss: 3.2356 - accuracy: 0.15 - ETA: 3:25 - loss: 3.2360 - accuracy: 0.16 - ETA: 3:25 - loss: 3.2377 - accuracy: 0.16 - ETA: 3:24 - loss: 3.2391 - accuracy: 0.16 - ETA: 3:23 - loss: 3.2349 - accuracy: 0.16 - ETA: 3:22 - loss: 3.2369 - accuracy: 0.16 - ETA: 3:22 - loss: 3.2362 - accuracy: 0.16 - ETA: 3:21 - loss: 3.2385 - accuracy: 0.16 - ETA: 3:20 - loss: 3.2415 - accuracy: 0.15 - ETA: 3:20 - loss: 3.2393 - accuracy: 0.16 - ETA: 3:19 - loss: 3.2407 - accuracy: 0.16 - ETA: 3:18 - loss: 3.2400 - accuracy: 0.16 - ETA: 3:18 - loss: 3.2422 - accuracy: 0.16 - ETA: 3:17 - loss: 3.2405 - accuracy: 0.16 - ETA: 3:16 - loss: 3.2437 - accuracy: 0.16 - ETA: 3:15 - loss: 3.2441 - accuracy: 0.15 - ETA: 3:15 - loss: 3.2455 - accuracy: 0.15 - ETA: 3:14 - loss: 3.2468 - accuracy: 0.15 - ETA: 3:13 - loss: 3.2458 - accuracy: 0.15 - ETA: 3:13 - loss: 3.2458 - accuracy: 0.15 - ETA: 3:12 - loss: 3.2450 - accuracy: 0.15 - ETA: 3:11 - loss: 3.2436 - accuracy: 0.15 - ETA: 3:11 - loss: 3.2455 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2437 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2438 - accuracy: 0.15 - ETA: 3:09 - loss: 3.2444 - accuracy: 0.15 - ETA: 3:08 - loss: 3.2437 - accuracy: 0.15 - ETA: 3:08 - loss: 3.2428 - accuracy: 0.15 - ETA: 3:07 - loss: 3.2430 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2445 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2485 - accuracy: 0.15 - ETA: 3:05 - loss: 3.2484 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2489 - accuracy: 0.15 - ETA: 3:03 - loss: 3.2483 - accuracy: 0.15 - ETA: 3:03 - loss: 3.2484 - accuracy: 0.15 - ETA: 3:02 - loss: 3.2487 - accuracy: 0.15 - ETA: 3:01 - loss: 3.2475 - accuracy: 0.15 - ETA: 3:00 - loss: 3.2439 - accuracy: 0.15 - ETA: 3:00 - loss: 3.2453 - accuracy: 0.15 - ETA: 2:59 - loss: 3.2462 - accuracy: 0.15 - ETA: 2:58 - loss: 3.2464 - accuracy: 0.15 - ETA: 2:58 - loss: 3.2457 - accuracy: 0.15 - ETA: 2:57 - loss: 3.2477 - accuracy: 0.15 - ETA: 2:56 - loss: 3.2480 - accuracy: 0.15 - ETA: 2:56 - loss: 3.2465 - accuracy: 0.15 - ETA: 2:55 - loss: 3.2456 - accuracy: 0.15 - ETA: 2:54 - loss: 3.2453 - accuracy: 0.15 - ETA: 2:53 - loss: 3.2479 - accuracy: 0.15 - ETA: 2:53 - loss: 3.2482 - accuracy: 0.15 - ETA: 2:52 - loss: 3.2455 - accuracy: 0.15 - ETA: 2:51 - loss: 3.2483 - accuracy: 0.15 - ETA: 2:51 - loss: 3.2515 - accuracy: 0.15 - ETA: 2:50 - loss: 3.2556 - accuracy: 0.15 - ETA: 2:49 - loss: 3.2571 - accuracy: 0.15 - ETA: 2:49 - loss: 3.2592 - accuracy: 0.15 - ETA: 2:48 - loss: 3.2590 - accuracy: 0.15 - ETA: 2:47 - loss: 3.2581 - accuracy: 0.15 - ETA: 2:47 - loss: 3.2610 - accuracy: 0.15 - ETA: 2:46 - loss: 3.2619 - accuracy: 0.15 - ETA: 2:45 - loss: 3.2613 - accuracy: 0.15 - ETA: 2:45 - loss: 3.2629 - accuracy: 0.15 - ETA: 2:44 - loss: 3.2620 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2635 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2632 - accuracy: 0.15 - ETA: 2:42 - loss: 3.2630 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2645 - accuracy: 0.15 - ETA: 2:40 - loss: 3.2650 - accuracy: 0.15 - ETA: 2:40 - loss: 3.2672 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2698 - accuracy: 0.15 - ETA: 2:38 - loss: 3.2707 - accuracy: 0.15 - ETA: 2:38 - loss: 3.2697 - accuracy: 0.15 - ETA: 2:37 - loss: 3.2714 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2719 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2722 - accuracy: 0.15 - ETA: 2:35 - loss: 3.2712 - accuracy: 0.15 - ETA: 2:34 - loss: 3.2714 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2722 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2719 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2723 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2709 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2722 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2735 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2731 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2725 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2741 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2750 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2739 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2743 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2748 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2763 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2773 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2777 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2773 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2789 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2793 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2792 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2789 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2792 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2788 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2787 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2795 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2802 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2797 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2791 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2791 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2777 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2763 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2769 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2770 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2765 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2758 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2757 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2766 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2765 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2759 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2762 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2767 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2770 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2763 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2763 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2767 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2766 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2769 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2764 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2763 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2760 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2767 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2773 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2773 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2771 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2773 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2772 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2776 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2774 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2767 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2767 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2774 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2765 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2770 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2774 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2773 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2776 - accuracy: 0.1493"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.2765 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2771 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2768 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2770 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2765 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2759 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2758 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2768 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2766 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2775 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2780 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2767 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2774 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2768 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2771 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2774 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2775 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2789 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2789 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2786 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2785 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2783 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2790 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2786 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2795 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2796 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2793 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2788 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2787 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2782 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2776 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2772 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2773 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2777 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2774 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2771 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2777 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2777 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2780 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2792 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2800 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2797 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2801 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2794 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2794 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2786 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2780 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2789 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2787 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2788 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2788 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2789 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2789 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2793 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2801 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2805 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2810 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2811 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2814 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2816 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2824 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2816 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2821 - accuracy: 0.14 - ETA: 59s - loss: 3.2820 - accuracy: 0.1478 - ETA: 58s - loss: 3.2824 - accuracy: 0.147 - ETA: 57s - loss: 3.2825 - accuracy: 0.147 - ETA: 57s - loss: 3.2828 - accuracy: 0.147 - ETA: 56s - loss: 3.2824 - accuracy: 0.147 - ETA: 55s - loss: 3.2825 - accuracy: 0.147 - ETA: 55s - loss: 3.2829 - accuracy: 0.147 - ETA: 54s - loss: 3.2831 - accuracy: 0.147 - ETA: 53s - loss: 3.2830 - accuracy: 0.147 - ETA: 52s - loss: 3.2838 - accuracy: 0.147 - ETA: 52s - loss: 3.2836 - accuracy: 0.147 - ETA: 51s - loss: 3.2838 - accuracy: 0.147 - ETA: 50s - loss: 3.2837 - accuracy: 0.147 - ETA: 49s - loss: 3.2837 - accuracy: 0.147 - ETA: 49s - loss: 3.2841 - accuracy: 0.147 - ETA: 48s - loss: 3.2848 - accuracy: 0.147 - ETA: 47s - loss: 3.2852 - accuracy: 0.147 - ETA: 46s - loss: 3.2860 - accuracy: 0.147 - ETA: 46s - loss: 3.2864 - accuracy: 0.146 - ETA: 45s - loss: 3.2860 - accuracy: 0.146 - ETA: 44s - loss: 3.2867 - accuracy: 0.146 - ETA: 44s - loss: 3.2865 - accuracy: 0.146 - ETA: 43s - loss: 3.2865 - accuracy: 0.146 - ETA: 42s - loss: 3.2868 - accuracy: 0.146 - ETA: 41s - loss: 3.2871 - accuracy: 0.146 - ETA: 41s - loss: 3.2871 - accuracy: 0.146 - ETA: 40s - loss: 3.2868 - accuracy: 0.146 - ETA: 39s - loss: 3.2864 - accuracy: 0.146 - ETA: 38s - loss: 3.2860 - accuracy: 0.146 - ETA: 38s - loss: 3.2860 - accuracy: 0.146 - ETA: 37s - loss: 3.2860 - accuracy: 0.146 - ETA: 36s - loss: 3.2859 - accuracy: 0.146 - ETA: 35s - loss: 3.2866 - accuracy: 0.146 - ETA: 35s - loss: 3.2865 - accuracy: 0.146 - ETA: 34s - loss: 3.2867 - accuracy: 0.146 - ETA: 33s - loss: 3.2865 - accuracy: 0.146 - ETA: 33s - loss: 3.2863 - accuracy: 0.147 - ETA: 32s - loss: 3.2866 - accuracy: 0.147 - ETA: 31s - loss: 3.2866 - accuracy: 0.147 - ETA: 30s - loss: 3.2867 - accuracy: 0.147 - ETA: 30s - loss: 3.2865 - accuracy: 0.147 - ETA: 29s - loss: 3.2859 - accuracy: 0.147 - ETA: 28s - loss: 3.2857 - accuracy: 0.147 - ETA: 27s - loss: 3.2859 - accuracy: 0.147 - ETA: 27s - loss: 3.2851 - accuracy: 0.147 - ETA: 26s - loss: 3.2854 - accuracy: 0.147 - ETA: 25s - loss: 3.2856 - accuracy: 0.147 - ETA: 24s - loss: 3.2862 - accuracy: 0.147 - ETA: 24s - loss: 3.2856 - accuracy: 0.147 - ETA: 23s - loss: 3.2860 - accuracy: 0.147 - ETA: 22s - loss: 3.2864 - accuracy: 0.146 - ETA: 22s - loss: 3.2863 - accuracy: 0.147 - ETA: 21s - loss: 3.2864 - accuracy: 0.146 - ETA: 20s - loss: 3.2862 - accuracy: 0.146 - ETA: 19s - loss: 3.2859 - accuracy: 0.147 - ETA: 19s - loss: 3.2862 - accuracy: 0.146 - ETA: 18s - loss: 3.2867 - accuracy: 0.146 - ETA: 17s - loss: 3.2865 - accuracy: 0.146 - ETA: 16s - loss: 3.2862 - accuracy: 0.146 - ETA: 16s - loss: 3.2859 - accuracy: 0.146 - ETA: 15s - loss: 3.2856 - accuracy: 0.146 - ETA: 14s - loss: 3.2857 - accuracy: 0.146 - ETA: 13s - loss: 3.2860 - accuracy: 0.146 - ETA: 13s - loss: 3.2860 - accuracy: 0.146 - ETA: 12s - loss: 3.2861 - accuracy: 0.146 - ETA: 11s - loss: 3.2854 - accuracy: 0.146 - ETA: 11s - loss: 3.2855 - accuracy: 0.146 - ETA: 10s - loss: 3.2854 - accuracy: 0.146 - ETA: 9s - loss: 3.2857 - accuracy: 0.146 - ETA: 8s - loss: 3.2857 - accuracy: 0.14 - ETA: 8s - loss: 3.2861 - accuracy: 0.14 - ETA: 7s - loss: 3.2860 - accuracy: 0.14 - ETA: 6s - loss: 3.2865 - accuracy: 0.14 - ETA: 5s - loss: 3.2865 - accuracy: 0.14 - ETA: 5s - loss: 3.2867 - accuracy: 0.14 - ETA: 4s - loss: 3.2865 - accuracy: 0.14 - ETA: 3s - loss: 3.2862 - accuracy: 0.14 - ETA: 2s - loss: 3.2865 - accuracy: 0.14 - ETA: 2s - loss: 3.2868 - accuracy: 0.14 - ETA: 1s - loss: 3.2870 - accuracy: 0.14 - ETA: 0s - loss: 3.2873 - accuracy: 0.14 - ETA: 0s - loss: 3.2870 - accuracy: 0.14 - 261s 6ms/step - loss: 3.2868 - accuracy: 0.1461 - val_loss: 3.9150 - val_accuracy: 0.0256\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:09 - loss: 3.4534 - accuracy: 0.09 - ETA: 4:04 - loss: 3.4411 - accuracy: 0.09 - ETA: 4:04 - loss: 3.3975 - accuracy: 0.11 - ETA: 4:02 - loss: 3.3402 - accuracy: 0.12 - ETA: 4:02 - loss: 3.3810 - accuracy: 0.12 - ETA: 4:01 - loss: 3.3421 - accuracy: 0.12 - ETA: 3:58 - loss: 3.3150 - accuracy: 0.13 - ETA: 3:58 - loss: 3.3145 - accuracy: 0.13 - ETA: 3:58 - loss: 3.3150 - accuracy: 0.13 - ETA: 3:58 - loss: 3.3109 - accuracy: 0.14 - ETA: 3:56 - loss: 3.3114 - accuracy: 0.14 - ETA: 3:56 - loss: 3.3017 - accuracy: 0.14 - ETA: 3:56 - loss: 3.2931 - accuracy: 0.14 - ETA: 3:55 - loss: 3.2919 - accuracy: 0.14 - ETA: 3:54 - loss: 3.2913 - accuracy: 0.14 - ETA: 3:54 - loss: 3.3029 - accuracy: 0.14 - ETA: 3:53 - loss: 3.3032 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3000 - accuracy: 0.14 - ETA: 3:51 - loss: 3.3020 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3030 - accuracy: 0.14 - ETA: 3:49 - loss: 3.2957 - accuracy: 0.14 - ETA: 3:48 - loss: 3.2917 - accuracy: 0.14 - ETA: 3:47 - loss: 3.2881 - accuracy: 0.14 - ETA: 3:47 - loss: 3.2850 - accuracy: 0.14 - ETA: 3:45 - loss: 3.2815 - accuracy: 0.14 - ETA: 3:44 - loss: 3.2806 - accuracy: 0.15 - ETA: 3:43 - loss: 3.2731 - accuracy: 0.15 - ETA: 3:42 - loss: 3.2749 - accuracy: 0.15 - ETA: 3:41 - loss: 3.2762 - accuracy: 0.15 - ETA: 3:40 - loss: 3.2715 - accuracy: 0.15 - ETA: 3:40 - loss: 3.2657 - accuracy: 0.15 - ETA: 3:39 - loss: 3.2657 - accuracy: 0.15 - ETA: 3:38 - loss: 3.2655 - accuracy: 0.15 - ETA: 3:37 - loss: 3.2703 - accuracy: 0.15 - ETA: 3:36 - loss: 3.2731 - accuracy: 0.15 - ETA: 3:35 - loss: 3.2775 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2740 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2760 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2784 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2743 - accuracy: 0.14 - ETA: 3:31 - loss: 3.2742 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2760 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2798 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2746 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2756 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2741 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2748 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2710 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2700 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2670 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2701 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2706 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2714 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2742 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2711 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2694 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2689 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2653 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2651 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2644 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2646 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2661 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2664 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2687 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2693 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2708 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2718 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2716 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2718 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2723 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2728 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2737 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2758 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2753 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2744 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2759 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2784 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2787 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2794 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2794 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2774 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2789 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2800 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2836 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2859 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2862 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2885 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2915 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2930 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2941 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2937 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2943 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2944 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2957 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2966 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2971 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2971 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2976 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2990 - accuracy: 0.14 - ETA: 2:49 - loss: 3.3006 - accuracy: 0.14 - ETA: 2:48 - loss: 3.3016 - accuracy: 0.14 - ETA: 2:47 - loss: 3.3021 - accuracy: 0.14 - ETA: 2:47 - loss: 3.3038 - accuracy: 0.14 - ETA: 2:46 - loss: 3.3034 - accuracy: 0.14 - ETA: 2:45 - loss: 3.3032 - accuracy: 0.14 - ETA: 2:44 - loss: 3.3036 - accuracy: 0.14 - ETA: 2:44 - loss: 3.3036 - accuracy: 0.14 - ETA: 2:43 - loss: 3.3030 - accuracy: 0.14 - ETA: 2:42 - loss: 3.3047 - accuracy: 0.14 - ETA: 2:41 - loss: 3.3055 - accuracy: 0.14 - ETA: 2:41 - loss: 3.3061 - accuracy: 0.14 - ETA: 2:40 - loss: 3.3075 - accuracy: 0.14 - ETA: 2:39 - loss: 3.3082 - accuracy: 0.14 - ETA: 2:39 - loss: 3.3086 - accuracy: 0.14 - ETA: 2:38 - loss: 3.3097 - accuracy: 0.14 - ETA: 2:37 - loss: 3.3090 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3078 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3064 - accuracy: 0.14 - ETA: 2:35 - loss: 3.3058 - accuracy: 0.14 - ETA: 2:34 - loss: 3.3047 - accuracy: 0.14 - ETA: 2:33 - loss: 3.3035 - accuracy: 0.14 - ETA: 2:32 - loss: 3.3030 - accuracy: 0.14 - ETA: 2:32 - loss: 3.3015 - accuracy: 0.14 - ETA: 2:31 - loss: 3.3017 - accuracy: 0.14 - ETA: 2:30 - loss: 3.3023 - accuracy: 0.14 - ETA: 2:29 - loss: 3.3028 - accuracy: 0.14 - ETA: 2:29 - loss: 3.3031 - accuracy: 0.14 - ETA: 2:28 - loss: 3.3018 - accuracy: 0.14 - ETA: 2:27 - loss: 3.3023 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3024 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3030 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3030 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3008 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3011 - accuracy: 0.14 - ETA: 2:23 - loss: 3.3017 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3011 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3007 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2998 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2983 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2997 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2990 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2990 - accuracy: 0.14 - ETA: 2:17 - loss: 3.3005 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2997 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3011 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3009 - accuracy: 0.14 - ETA: 2:14 - loss: 3.3010 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3008 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2990 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2995 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3011 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3012 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3001 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2999 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2993 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2985 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2989 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2994 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2992 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2991 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2995 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2995 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2996 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3000 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3004 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2995 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2994 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3002 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3003 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3003 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3003 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3007 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3007 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3001 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2996 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2995 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2988 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2982 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2969 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2962 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2956 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2958 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2963 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2962 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2955 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2952 - accuracy: 0.1434"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.2947 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2940 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2934 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2942 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2944 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2958 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2971 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2985 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2992 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2992 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2989 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2985 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2986 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2993 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3002 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3000 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3000 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3006 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3006 - accuracy: 0.14 - ETA: 1:31 - loss: 3.3006 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3004 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2996 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2993 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2987 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2982 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2975 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2963 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2961 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2964 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2975 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2973 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2976 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2977 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2980 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2980 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2985 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2982 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2980 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2978 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2974 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2966 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2970 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2959 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2957 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2956 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2945 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2945 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2950 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2948 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2950 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2946 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2938 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2939 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2933 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2928 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2931 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2930 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2930 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2931 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2932 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2930 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2929 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2933 - accuracy: 0.14 - ETA: 59s - loss: 3.2934 - accuracy: 0.1447 - ETA: 58s - loss: 3.2938 - accuracy: 0.144 - ETA: 57s - loss: 3.2944 - accuracy: 0.144 - ETA: 57s - loss: 3.2944 - accuracy: 0.144 - ETA: 56s - loss: 3.2942 - accuracy: 0.144 - ETA: 55s - loss: 3.2945 - accuracy: 0.144 - ETA: 54s - loss: 3.2948 - accuracy: 0.144 - ETA: 54s - loss: 3.2953 - accuracy: 0.144 - ETA: 53s - loss: 3.2953 - accuracy: 0.144 - ETA: 52s - loss: 3.2946 - accuracy: 0.144 - ETA: 52s - loss: 3.2948 - accuracy: 0.144 - ETA: 51s - loss: 3.2950 - accuracy: 0.144 - ETA: 50s - loss: 3.2948 - accuracy: 0.144 - ETA: 49s - loss: 3.2943 - accuracy: 0.144 - ETA: 49s - loss: 3.2943 - accuracy: 0.144 - ETA: 48s - loss: 3.2943 - accuracy: 0.144 - ETA: 47s - loss: 3.2941 - accuracy: 0.144 - ETA: 46s - loss: 3.2939 - accuracy: 0.144 - ETA: 46s - loss: 3.2941 - accuracy: 0.144 - ETA: 45s - loss: 3.2942 - accuracy: 0.144 - ETA: 44s - loss: 3.2937 - accuracy: 0.144 - ETA: 43s - loss: 3.2940 - accuracy: 0.144 - ETA: 43s - loss: 3.2938 - accuracy: 0.144 - ETA: 42s - loss: 3.2943 - accuracy: 0.144 - ETA: 41s - loss: 3.2937 - accuracy: 0.144 - ETA: 41s - loss: 3.2935 - accuracy: 0.144 - ETA: 40s - loss: 3.2935 - accuracy: 0.144 - ETA: 39s - loss: 3.2927 - accuracy: 0.144 - ETA: 38s - loss: 3.2924 - accuracy: 0.144 - ETA: 38s - loss: 3.2917 - accuracy: 0.144 - ETA: 37s - loss: 3.2920 - accuracy: 0.144 - ETA: 36s - loss: 3.2926 - accuracy: 0.144 - ETA: 35s - loss: 3.2927 - accuracy: 0.144 - ETA: 35s - loss: 3.2936 - accuracy: 0.144 - ETA: 34s - loss: 3.2932 - accuracy: 0.144 - ETA: 33s - loss: 3.2932 - accuracy: 0.144 - ETA: 32s - loss: 3.2929 - accuracy: 0.144 - ETA: 32s - loss: 3.2932 - accuracy: 0.144 - ETA: 31s - loss: 3.2934 - accuracy: 0.144 - ETA: 30s - loss: 3.2932 - accuracy: 0.144 - ETA: 30s - loss: 3.2938 - accuracy: 0.144 - ETA: 29s - loss: 3.2936 - accuracy: 0.144 - ETA: 28s - loss: 3.2934 - accuracy: 0.144 - ETA: 27s - loss: 3.2930 - accuracy: 0.144 - ETA: 27s - loss: 3.2933 - accuracy: 0.144 - ETA: 26s - loss: 3.2932 - accuracy: 0.144 - ETA: 25s - loss: 3.2938 - accuracy: 0.144 - ETA: 24s - loss: 3.2934 - accuracy: 0.144 - ETA: 24s - loss: 3.2928 - accuracy: 0.144 - ETA: 23s - loss: 3.2926 - accuracy: 0.144 - ETA: 22s - loss: 3.2918 - accuracy: 0.144 - ETA: 22s - loss: 3.2919 - accuracy: 0.144 - ETA: 21s - loss: 3.2918 - accuracy: 0.144 - ETA: 20s - loss: 3.2917 - accuracy: 0.144 - ETA: 19s - loss: 3.2921 - accuracy: 0.144 - ETA: 19s - loss: 3.2916 - accuracy: 0.144 - ETA: 18s - loss: 3.2915 - accuracy: 0.144 - ETA: 17s - loss: 3.2913 - accuracy: 0.145 - ETA: 16s - loss: 3.2909 - accuracy: 0.145 - ETA: 16s - loss: 3.2913 - accuracy: 0.145 - ETA: 15s - loss: 3.2916 - accuracy: 0.145 - ETA: 14s - loss: 3.2915 - accuracy: 0.145 - ETA: 13s - loss: 3.2918 - accuracy: 0.145 - ETA: 13s - loss: 3.2925 - accuracy: 0.144 - ETA: 12s - loss: 3.2930 - accuracy: 0.144 - ETA: 11s - loss: 3.2934 - accuracy: 0.144 - ETA: 11s - loss: 3.2938 - accuracy: 0.144 - ETA: 10s - loss: 3.2945 - accuracy: 0.144 - ETA: 9s - loss: 3.2951 - accuracy: 0.144 - ETA: 8s - loss: 3.2952 - accuracy: 0.14 - ETA: 8s - loss: 3.2955 - accuracy: 0.14 - ETA: 7s - loss: 3.2957 - accuracy: 0.14 - ETA: 6s - loss: 3.2956 - accuracy: 0.14 - ETA: 5s - loss: 3.2964 - accuracy: 0.14 - ETA: 5s - loss: 3.2967 - accuracy: 0.14 - ETA: 4s - loss: 3.2969 - accuracy: 0.14 - ETA: 3s - loss: 3.2975 - accuracy: 0.14 - ETA: 2s - loss: 3.2978 - accuracy: 0.14 - ETA: 2s - loss: 3.2981 - accuracy: 0.14 - ETA: 1s - loss: 3.2984 - accuracy: 0.14 - ETA: 0s - loss: 3.2985 - accuracy: 0.14 - ETA: 0s - loss: 3.2987 - accuracy: 0.14 - 260s 6ms/step - loss: 3.2988 - accuracy: 0.1433 - val_loss: 4.1619 - val_accuracy: 0.0175\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:10 - loss: 3.3888 - accuracy: 0.11 - ETA: 3:55 - loss: 3.2691 - accuracy: 0.13 - ETA: 3:57 - loss: 3.2903 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3301 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3531 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3838 - accuracy: 0.12 - ETA: 3:55 - loss: 3.3567 - accuracy: 0.14 - ETA: 3:54 - loss: 3.3788 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3745 - accuracy: 0.14 - ETA: 3:53 - loss: 3.3768 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3704 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3700 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3717 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3857 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3796 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3674 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3625 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3541 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3569 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3531 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3528 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3466 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3550 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3564 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3461 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3395 - accuracy: 0.14 - ETA: 3:42 - loss: 3.3403 - accuracy: 0.14 - ETA: 3:41 - loss: 3.3453 - accuracy: 0.14 - ETA: 3:41 - loss: 3.3479 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3525 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3532 - accuracy: 0.14 - ETA: 3:39 - loss: 3.3551 - accuracy: 0.14 - ETA: 3:38 - loss: 3.3557 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3609 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3643 - accuracy: 0.14 - ETA: 3:35 - loss: 3.3693 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3732 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3732 - accuracy: 0.13 - ETA: 3:33 - loss: 3.3766 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3782 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3787 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3798 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3775 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3750 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3761 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3740 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3728 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3720 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3714 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3741 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3699 - accuracy: 0.14 - ETA: 3:24 - loss: 3.3713 - accuracy: 0.14 - ETA: 3:24 - loss: 3.3715 - accuracy: 0.14 - ETA: 3:23 - loss: 3.3746 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3724 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3693 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3672 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3641 - accuracy: 0.14 - ETA: 3:19 - loss: 3.3636 - accuracy: 0.14 - ETA: 3:18 - loss: 3.3634 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3603 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3617 - accuracy: 0.14 - ETA: 3:16 - loss: 3.3620 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3619 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3605 - accuracy: 0.14 - ETA: 3:13 - loss: 3.3602 - accuracy: 0.14 - ETA: 3:13 - loss: 3.3561 - accuracy: 0.14 - ETA: 3:12 - loss: 3.3538 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3551 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3511 - accuracy: 0.14 - ETA: 3:10 - loss: 3.3498 - accuracy: 0.14 - ETA: 3:09 - loss: 3.3477 - accuracy: 0.14 - ETA: 3:09 - loss: 3.3478 - accuracy: 0.14 - ETA: 3:08 - loss: 3.3462 - accuracy: 0.14 - ETA: 3:07 - loss: 3.3449 - accuracy: 0.14 - ETA: 3:07 - loss: 3.3425 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3408 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3375 - accuracy: 0.14 - ETA: 3:05 - loss: 3.3376 - accuracy: 0.14 - ETA: 3:04 - loss: 3.3366 - accuracy: 0.14 - ETA: 3:04 - loss: 3.3325 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3315 - accuracy: 0.14 - ETA: 3:02 - loss: 3.3299 - accuracy: 0.14 - ETA: 3:01 - loss: 3.3274 - accuracy: 0.14 - ETA: 3:01 - loss: 3.3259 - accuracy: 0.14 - ETA: 3:00 - loss: 3.3265 - accuracy: 0.14 - ETA: 2:59 - loss: 3.3258 - accuracy: 0.14 - ETA: 2:58 - loss: 3.3237 - accuracy: 0.14 - ETA: 2:57 - loss: 3.3214 - accuracy: 0.14 - ETA: 2:57 - loss: 3.3210 - accuracy: 0.14 - ETA: 2:56 - loss: 3.3195 - accuracy: 0.14 - ETA: 2:55 - loss: 3.3193 - accuracy: 0.14 - ETA: 2:54 - loss: 3.3193 - accuracy: 0.14 - ETA: 2:53 - loss: 3.3186 - accuracy: 0.14 - ETA: 2:53 - loss: 3.3163 - accuracy: 0.14 - ETA: 2:52 - loss: 3.3153 - accuracy: 0.14 - ETA: 2:51 - loss: 3.3164 - accuracy: 0.14 - ETA: 2:50 - loss: 3.3158 - accuracy: 0.14 - ETA: 2:50 - loss: 3.3163 - accuracy: 0.14 - ETA: 2:49 - loss: 3.3152 - accuracy: 0.14 - ETA: 2:48 - loss: 3.3157 - accuracy: 0.14 - ETA: 2:47 - loss: 3.3147 - accuracy: 0.14 - ETA: 2:47 - loss: 3.3135 - accuracy: 0.14 - ETA: 2:46 - loss: 3.3137 - accuracy: 0.14 - ETA: 2:45 - loss: 3.3119 - accuracy: 0.14 - ETA: 2:44 - loss: 3.3118 - accuracy: 0.14 - ETA: 2:44 - loss: 3.3079 - accuracy: 0.14 - ETA: 2:43 - loss: 3.3072 - accuracy: 0.14 - ETA: 2:42 - loss: 3.3052 - accuracy: 0.14 - ETA: 2:42 - loss: 3.3044 - accuracy: 0.14 - ETA: 2:41 - loss: 3.3031 - accuracy: 0.14 - ETA: 2:40 - loss: 3.3017 - accuracy: 0.14 - ETA: 2:39 - loss: 3.3008 - accuracy: 0.14 - ETA: 2:39 - loss: 3.3021 - accuracy: 0.14 - ETA: 2:38 - loss: 3.3026 - accuracy: 0.14 - ETA: 2:37 - loss: 3.3033 - accuracy: 0.14 - ETA: 2:36 - loss: 3.3021 - accuracy: 0.14 - ETA: 2:36 - loss: 3.3025 - accuracy: 0.14 - ETA: 2:35 - loss: 3.3010 - accuracy: 0.14 - ETA: 2:34 - loss: 3.3008 - accuracy: 0.14 - ETA: 2:33 - loss: 3.3011 - accuracy: 0.14 - ETA: 2:33 - loss: 3.3012 - accuracy: 0.14 - ETA: 2:32 - loss: 3.3014 - accuracy: 0.14 - ETA: 2:31 - loss: 3.3014 - accuracy: 0.14 - ETA: 2:30 - loss: 3.3014 - accuracy: 0.14 - ETA: 2:30 - loss: 3.3017 - accuracy: 0.14 - ETA: 2:29 - loss: 3.3018 - accuracy: 0.14 - ETA: 2:28 - loss: 3.3015 - accuracy: 0.14 - ETA: 2:28 - loss: 3.3024 - accuracy: 0.14 - ETA: 2:27 - loss: 3.3008 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3009 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3006 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3019 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3015 - accuracy: 0.14 - ETA: 2:23 - loss: 3.3011 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3021 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3028 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3052 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3066 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3077 - accuracy: 0.14 - ETA: 2:19 - loss: 3.3076 - accuracy: 0.14 - ETA: 2:18 - loss: 3.3080 - accuracy: 0.14 - ETA: 2:17 - loss: 3.3090 - accuracy: 0.14 - ETA: 2:17 - loss: 3.3090 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3078 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3076 - accuracy: 0.14 - ETA: 2:14 - loss: 3.3067 - accuracy: 0.14 - ETA: 2:14 - loss: 3.3068 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3060 - accuracy: 0.14 - ETA: 2:12 - loss: 3.3065 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3063 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3062 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3060 - accuracy: 0.14 - ETA: 2:09 - loss: 3.3043 - accuracy: 0.14 - ETA: 2:09 - loss: 3.3032 - accuracy: 0.14 - ETA: 2:08 - loss: 3.3032 - accuracy: 0.14 - ETA: 2:07 - loss: 3.3013 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3010 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3018 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3019 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3022 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3009 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3009 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2998 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2995 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3000 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3009 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3013 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2996 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2990 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2989 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2994 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2992 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2989 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2985 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2980 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2973 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2960 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2959 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2965 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2965 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2967 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2970 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2973 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2966 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2958 - accuracy: 0.1479"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.2967 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2961 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2962 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2950 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2944 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2934 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2930 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2926 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2925 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2925 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2925 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2926 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2920 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2913 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2901 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2907 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2915 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2920 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2909 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2910 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2907 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2908 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2904 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2902 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2901 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2898 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2899 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2896 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2893 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2887 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2885 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2893 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2892 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2890 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2885 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2895 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2884 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2894 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2907 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2907 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2907 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2902 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2898 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2897 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2892 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2892 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2898 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2904 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2908 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2904 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2909 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2907 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2913 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2909 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2908 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2904 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2901 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2899 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2897 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2899 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2889 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2885 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2890 - accuracy: 0.14 - ETA: 59s - loss: 3.2889 - accuracy: 0.1479 - ETA: 58s - loss: 3.2891 - accuracy: 0.147 - ETA: 58s - loss: 3.2887 - accuracy: 0.147 - ETA: 57s - loss: 3.2886 - accuracy: 0.147 - ETA: 56s - loss: 3.2891 - accuracy: 0.147 - ETA: 55s - loss: 3.2895 - accuracy: 0.147 - ETA: 55s - loss: 3.2898 - accuracy: 0.147 - ETA: 54s - loss: 3.2899 - accuracy: 0.147 - ETA: 53s - loss: 3.2897 - accuracy: 0.147 - ETA: 53s - loss: 3.2896 - accuracy: 0.147 - ETA: 52s - loss: 3.2891 - accuracy: 0.147 - ETA: 51s - loss: 3.2894 - accuracy: 0.147 - ETA: 50s - loss: 3.2891 - accuracy: 0.147 - ETA: 50s - loss: 3.2888 - accuracy: 0.147 - ETA: 49s - loss: 3.2888 - accuracy: 0.147 - ETA: 48s - loss: 3.2891 - accuracy: 0.147 - ETA: 47s - loss: 3.2892 - accuracy: 0.147 - ETA: 47s - loss: 3.2892 - accuracy: 0.147 - ETA: 46s - loss: 3.2891 - accuracy: 0.147 - ETA: 45s - loss: 3.2891 - accuracy: 0.147 - ETA: 44s - loss: 3.2891 - accuracy: 0.147 - ETA: 44s - loss: 3.2888 - accuracy: 0.147 - ETA: 43s - loss: 3.2892 - accuracy: 0.147 - ETA: 42s - loss: 3.2887 - accuracy: 0.147 - ETA: 42s - loss: 3.2891 - accuracy: 0.147 - ETA: 41s - loss: 3.2899 - accuracy: 0.146 - ETA: 40s - loss: 3.2899 - accuracy: 0.146 - ETA: 39s - loss: 3.2900 - accuracy: 0.146 - ETA: 39s - loss: 3.2894 - accuracy: 0.147 - ETA: 38s - loss: 3.2887 - accuracy: 0.147 - ETA: 37s - loss: 3.2894 - accuracy: 0.147 - ETA: 36s - loss: 3.2895 - accuracy: 0.146 - ETA: 36s - loss: 3.2892 - accuracy: 0.146 - ETA: 35s - loss: 3.2896 - accuracy: 0.146 - ETA: 34s - loss: 3.2894 - accuracy: 0.146 - ETA: 33s - loss: 3.2893 - accuracy: 0.146 - ETA: 33s - loss: 3.2889 - accuracy: 0.146 - ETA: 32s - loss: 3.2892 - accuracy: 0.146 - ETA: 31s - loss: 3.2893 - accuracy: 0.146 - ETA: 30s - loss: 3.2895 - accuracy: 0.146 - ETA: 30s - loss: 3.2899 - accuracy: 0.146 - ETA: 29s - loss: 3.2897 - accuracy: 0.146 - ETA: 28s - loss: 3.2888 - accuracy: 0.146 - ETA: 28s - loss: 3.2883 - accuracy: 0.146 - ETA: 27s - loss: 3.2882 - accuracy: 0.146 - ETA: 26s - loss: 3.2882 - accuracy: 0.146 - ETA: 25s - loss: 3.2886 - accuracy: 0.146 - ETA: 25s - loss: 3.2879 - accuracy: 0.146 - ETA: 24s - loss: 3.2868 - accuracy: 0.146 - ETA: 23s - loss: 3.2871 - accuracy: 0.146 - ETA: 22s - loss: 3.2876 - accuracy: 0.146 - ETA: 22s - loss: 3.2870 - accuracy: 0.146 - ETA: 21s - loss: 3.2868 - accuracy: 0.146 - ETA: 20s - loss: 3.2862 - accuracy: 0.146 - ETA: 19s - loss: 3.2865 - accuracy: 0.146 - ETA: 19s - loss: 3.2859 - accuracy: 0.146 - ETA: 18s - loss: 3.2859 - accuracy: 0.146 - ETA: 17s - loss: 3.2860 - accuracy: 0.146 - ETA: 17s - loss: 3.2859 - accuracy: 0.146 - ETA: 16s - loss: 3.2864 - accuracy: 0.146 - ETA: 15s - loss: 3.2862 - accuracy: 0.146 - ETA: 14s - loss: 3.2859 - accuracy: 0.146 - ETA: 14s - loss: 3.2855 - accuracy: 0.146 - ETA: 13s - loss: 3.2854 - accuracy: 0.146 - ETA: 12s - loss: 3.2853 - accuracy: 0.146 - ETA: 11s - loss: 3.2850 - accuracy: 0.146 - ETA: 11s - loss: 3.2851 - accuracy: 0.146 - ETA: 10s - loss: 3.2848 - accuracy: 0.146 - ETA: 9s - loss: 3.2849 - accuracy: 0.146 - ETA: 8s - loss: 3.2848 - accuracy: 0.14 - ETA: 8s - loss: 3.2849 - accuracy: 0.14 - ETA: 7s - loss: 3.2851 - accuracy: 0.14 - ETA: 6s - loss: 3.2851 - accuracy: 0.14 - ETA: 5s - loss: 3.2855 - accuracy: 0.14 - ETA: 5s - loss: 3.2856 - accuracy: 0.14 - ETA: 4s - loss: 3.2856 - accuracy: 0.14 - ETA: 3s - loss: 3.2854 - accuracy: 0.14 - ETA: 3s - loss: 3.2855 - accuracy: 0.14 - ETA: 2s - loss: 3.2851 - accuracy: 0.14 - ETA: 1s - loss: 3.2848 - accuracy: 0.14 - ETA: 0s - loss: 3.2849 - accuracy: 0.14 - ETA: 0s - loss: 3.2851 - accuracy: 0.14 - 263s 6ms/step - loss: 3.2852 - accuracy: 0.1470 - val_loss: 4.1294 - val_accuracy: 0.0324\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:45 - loss: 3.4178 - accuracy: 0.12 - ETA: 3:49 - loss: 3.2803 - accuracy: 0.15 - ETA: 3:54 - loss: 3.2625 - accuracy: 0.15 - ETA: 3:54 - loss: 3.2595 - accuracy: 0.16 - ETA: 3:54 - loss: 3.2734 - accuracy: 0.15 - ETA: 3:55 - loss: 3.3217 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3146 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3086 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3068 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3172 - accuracy: 0.13 - ETA: 3:53 - loss: 3.3216 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3318 - accuracy: 0.13 - ETA: 3:53 - loss: 3.3349 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3352 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3301 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3200 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3267 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3125 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3023 - accuracy: 0.13 - ETA: 3:47 - loss: 3.2979 - accuracy: 0.14 - ETA: 3:46 - loss: 3.2955 - accuracy: 0.14 - ETA: 3:45 - loss: 3.2964 - accuracy: 0.14 - ETA: 3:45 - loss: 3.2928 - accuracy: 0.14 - ETA: 3:44 - loss: 3.2937 - accuracy: 0.14 - ETA: 3:43 - loss: 3.2909 - accuracy: 0.14 - ETA: 3:43 - loss: 3.2872 - accuracy: 0.14 - ETA: 3:42 - loss: 3.2867 - accuracy: 0.14 - ETA: 3:40 - loss: 3.2898 - accuracy: 0.14 - ETA: 3:40 - loss: 3.2912 - accuracy: 0.14 - ETA: 3:39 - loss: 3.2891 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2905 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2822 - accuracy: 0.14 - ETA: 3:37 - loss: 3.2839 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2789 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2769 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2720 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2690 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2678 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2700 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2690 - accuracy: 0.15 - ETA: 3:31 - loss: 3.2687 - accuracy: 0.15 - ETA: 3:30 - loss: 3.2700 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2720 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2692 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2725 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2720 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2755 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2757 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2776 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2801 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2807 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2821 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2837 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2840 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2848 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2839 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2824 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2823 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2819 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2787 - accuracy: 0.15 - ETA: 3:18 - loss: 3.2798 - accuracy: 0.15 - ETA: 3:17 - loss: 3.2810 - accuracy: 0.15 - ETA: 3:16 - loss: 3.2816 - accuracy: 0.15 - ETA: 3:15 - loss: 3.2823 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2813 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2823 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2820 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2841 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2819 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2812 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2803 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2802 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2800 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2784 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2789 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2801 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2785 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2781 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2795 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2773 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2774 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2789 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2790 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2785 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2796 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2788 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2789 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2789 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2795 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2807 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2803 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2806 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2822 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2822 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2817 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2841 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2855 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2844 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2841 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2843 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2851 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2846 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2849 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2842 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2832 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2812 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2816 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2803 - accuracy: 0.15 - ETA: 2:42 - loss: 3.2809 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2796 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2799 - accuracy: 0.15 - ETA: 2:40 - loss: 3.2797 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2792 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2779 - accuracy: 0.15 - ETA: 2:38 - loss: 3.2782 - accuracy: 0.15 - ETA: 2:37 - loss: 3.2769 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2773 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2780 - accuracy: 0.15 - ETA: 2:35 - loss: 3.2768 - accuracy: 0.15 - ETA: 2:34 - loss: 3.2777 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2783 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2784 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2778 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2776 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2784 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2779 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2767 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2753 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2737 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2739 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2739 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2745 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2743 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2746 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2744 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2741 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2741 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2731 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2738 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2735 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2731 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2731 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2720 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2707 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2722 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2711 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2709 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2714 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2714 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2714 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2720 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2711 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2713 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2708 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2709 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2702 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2682 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2677 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2673 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2667 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2666 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2674 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2663 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2654 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2650 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2657 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2652 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2640 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2657 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2645 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2627 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2626 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2630 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2629 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2633 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2629 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2628 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2620 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2619 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2616 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2619 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2614 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2621 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2617 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2620 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2617 - accuracy: 0.1514"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.2623 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2626 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2633 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2633 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2649 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2643 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2638 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2645 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2640 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2647 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2650 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2646 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2645 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2647 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2655 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2653 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2647 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2649 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2649 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2652 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2654 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2655 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2648 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2641 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2642 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2639 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2640 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2638 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2640 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2642 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2642 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2640 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2624 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2621 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2618 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2610 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2606 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2600 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2592 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2593 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2598 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2595 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2590 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2598 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2600 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2606 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2610 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2618 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2606 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2608 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2609 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2611 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2622 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2621 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2619 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2622 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2626 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2629 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2636 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2635 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2639 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2640 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2639 - accuracy: 0.14 - ETA: 59s - loss: 3.2635 - accuracy: 0.1488 - ETA: 58s - loss: 3.2640 - accuracy: 0.148 - ETA: 58s - loss: 3.2646 - accuracy: 0.148 - ETA: 57s - loss: 3.2647 - accuracy: 0.148 - ETA: 56s - loss: 3.2646 - accuracy: 0.148 - ETA: 55s - loss: 3.2646 - accuracy: 0.148 - ETA: 55s - loss: 3.2637 - accuracy: 0.148 - ETA: 54s - loss: 3.2641 - accuracy: 0.148 - ETA: 53s - loss: 3.2644 - accuracy: 0.148 - ETA: 52s - loss: 3.2635 - accuracy: 0.148 - ETA: 52s - loss: 3.2641 - accuracy: 0.148 - ETA: 51s - loss: 3.2643 - accuracy: 0.148 - ETA: 50s - loss: 3.2644 - accuracy: 0.148 - ETA: 49s - loss: 3.2640 - accuracy: 0.148 - ETA: 49s - loss: 3.2648 - accuracy: 0.148 - ETA: 48s - loss: 3.2651 - accuracy: 0.148 - ETA: 47s - loss: 3.2658 - accuracy: 0.148 - ETA: 47s - loss: 3.2648 - accuracy: 0.148 - ETA: 46s - loss: 3.2650 - accuracy: 0.148 - ETA: 45s - loss: 3.2660 - accuracy: 0.148 - ETA: 44s - loss: 3.2665 - accuracy: 0.148 - ETA: 44s - loss: 3.2664 - accuracy: 0.148 - ETA: 43s - loss: 3.2657 - accuracy: 0.148 - ETA: 42s - loss: 3.2657 - accuracy: 0.148 - ETA: 41s - loss: 3.2660 - accuracy: 0.148 - ETA: 41s - loss: 3.2657 - accuracy: 0.148 - ETA: 40s - loss: 3.2658 - accuracy: 0.148 - ETA: 39s - loss: 3.2661 - accuracy: 0.148 - ETA: 38s - loss: 3.2664 - accuracy: 0.148 - ETA: 38s - loss: 3.2660 - accuracy: 0.148 - ETA: 37s - loss: 3.2662 - accuracy: 0.148 - ETA: 36s - loss: 3.2659 - accuracy: 0.148 - ETA: 36s - loss: 3.2655 - accuracy: 0.148 - ETA: 35s - loss: 3.2662 - accuracy: 0.148 - ETA: 34s - loss: 3.2666 - accuracy: 0.148 - ETA: 33s - loss: 3.2668 - accuracy: 0.148 - ETA: 33s - loss: 3.2667 - accuracy: 0.148 - ETA: 32s - loss: 3.2666 - accuracy: 0.148 - ETA: 31s - loss: 3.2669 - accuracy: 0.148 - ETA: 30s - loss: 3.2666 - accuracy: 0.148 - ETA: 30s - loss: 3.2674 - accuracy: 0.148 - ETA: 29s - loss: 3.2681 - accuracy: 0.148 - ETA: 28s - loss: 3.2676 - accuracy: 0.148 - ETA: 28s - loss: 3.2667 - accuracy: 0.148 - ETA: 27s - loss: 3.2671 - accuracy: 0.148 - ETA: 26s - loss: 3.2670 - accuracy: 0.148 - ETA: 25s - loss: 3.2672 - accuracy: 0.148 - ETA: 25s - loss: 3.2668 - accuracy: 0.148 - ETA: 24s - loss: 3.2666 - accuracy: 0.148 - ETA: 23s - loss: 3.2669 - accuracy: 0.148 - ETA: 22s - loss: 3.2667 - accuracy: 0.148 - ETA: 22s - loss: 3.2662 - accuracy: 0.148 - ETA: 21s - loss: 3.2664 - accuracy: 0.148 - ETA: 20s - loss: 3.2664 - accuracy: 0.148 - ETA: 19s - loss: 3.2660 - accuracy: 0.148 - ETA: 19s - loss: 3.2651 - accuracy: 0.148 - ETA: 18s - loss: 3.2650 - accuracy: 0.148 - ETA: 17s - loss: 3.2643 - accuracy: 0.148 - ETA: 16s - loss: 3.2643 - accuracy: 0.148 - ETA: 16s - loss: 3.2639 - accuracy: 0.149 - ETA: 15s - loss: 3.2639 - accuracy: 0.149 - ETA: 14s - loss: 3.2637 - accuracy: 0.149 - ETA: 14s - loss: 3.2633 - accuracy: 0.149 - ETA: 13s - loss: 3.2634 - accuracy: 0.149 - ETA: 12s - loss: 3.2630 - accuracy: 0.149 - ETA: 11s - loss: 3.2622 - accuracy: 0.149 - ETA: 11s - loss: 3.2623 - accuracy: 0.149 - ETA: 10s - loss: 3.2620 - accuracy: 0.149 - ETA: 9s - loss: 3.2613 - accuracy: 0.149 - ETA: 8s - loss: 3.2613 - accuracy: 0.14 - ETA: 8s - loss: 3.2613 - accuracy: 0.14 - ETA: 7s - loss: 3.2606 - accuracy: 0.14 - ETA: 6s - loss: 3.2602 - accuracy: 0.14 - ETA: 5s - loss: 3.2599 - accuracy: 0.14 - ETA: 5s - loss: 3.2599 - accuracy: 0.14 - ETA: 4s - loss: 3.2601 - accuracy: 0.15 - ETA: 3s - loss: 3.2597 - accuracy: 0.15 - ETA: 3s - loss: 3.2596 - accuracy: 0.15 - ETA: 2s - loss: 3.2598 - accuracy: 0.15 - ETA: 1s - loss: 3.2599 - accuracy: 0.15 - ETA: 0s - loss: 3.2604 - accuracy: 0.14 - ETA: 0s - loss: 3.2608 - accuracy: 0.14 - 261s 6ms/step - loss: 3.2607 - accuracy: 0.1497 - val_loss: 4.1040 - val_accuracy: 0.0283\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:02 - loss: 3.1741 - accuracy: 0.16 - ETA: 3:56 - loss: 3.1948 - accuracy: 0.15 - ETA: 3:54 - loss: 3.1782 - accuracy: 0.16 - ETA: 3:54 - loss: 3.2005 - accuracy: 0.15 - ETA: 3:56 - loss: 3.1927 - accuracy: 0.16 - ETA: 4:00 - loss: 3.1906 - accuracy: 0.16 - ETA: 3:58 - loss: 3.1870 - accuracy: 0.15 - ETA: 3:57 - loss: 3.1962 - accuracy: 0.15 - ETA: 3:56 - loss: 3.2166 - accuracy: 0.14 - ETA: 3:56 - loss: 3.2263 - accuracy: 0.15 - ETA: 3:54 - loss: 3.2269 - accuracy: 0.14 - ETA: 3:54 - loss: 3.2332 - accuracy: 0.14 - ETA: 3:54 - loss: 3.2310 - accuracy: 0.14 - ETA: 3:52 - loss: 3.2394 - accuracy: 0.14 - ETA: 3:51 - loss: 3.2391 - accuracy: 0.14 - ETA: 3:50 - loss: 3.2500 - accuracy: 0.14 - ETA: 3:49 - loss: 3.2625 - accuracy: 0.14 - ETA: 3:48 - loss: 3.2488 - accuracy: 0.14 - ETA: 3:48 - loss: 3.2548 - accuracy: 0.14 - ETA: 3:47 - loss: 3.2580 - accuracy: 0.14 - ETA: 3:46 - loss: 3.2578 - accuracy: 0.14 - ETA: 3:45 - loss: 3.2603 - accuracy: 0.14 - ETA: 3:45 - loss: 3.2603 - accuracy: 0.14 - ETA: 3:44 - loss: 3.2546 - accuracy: 0.14 - ETA: 3:43 - loss: 3.2576 - accuracy: 0.14 - ETA: 3:43 - loss: 3.2544 - accuracy: 0.14 - ETA: 3:42 - loss: 3.2538 - accuracy: 0.14 - ETA: 3:41 - loss: 3.2534 - accuracy: 0.14 - ETA: 3:41 - loss: 3.2509 - accuracy: 0.14 - ETA: 3:40 - loss: 3.2452 - accuracy: 0.14 - ETA: 3:40 - loss: 3.2494 - accuracy: 0.14 - ETA: 3:39 - loss: 3.2484 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2498 - accuracy: 0.14 - ETA: 3:37 - loss: 3.2444 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2414 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2400 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2386 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2341 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2321 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2312 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2337 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2356 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2371 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2377 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2386 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2384 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2385 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2424 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2410 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2414 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2378 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2398 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2437 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2442 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2466 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2458 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2460 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2461 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2488 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2502 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2552 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2549 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2579 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2623 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2624 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2640 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2669 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2658 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2655 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2655 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2648 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2672 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2668 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2642 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2621 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2626 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2658 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2655 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2647 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2643 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2644 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2643 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2607 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2597 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2601 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2612 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2609 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2622 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2615 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2619 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2628 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2646 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2651 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2637 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2626 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2622 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2628 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2625 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2638 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2642 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2639 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2641 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2672 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2671 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2671 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2675 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2673 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2695 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2704 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2692 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2675 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2669 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2678 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2682 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2687 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2692 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2690 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2697 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2693 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2702 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2697 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2700 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2693 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2685 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2689 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2702 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2711 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2691 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2691 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2683 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2681 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2665 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2662 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2665 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2661 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2656 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2655 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2657 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2668 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2681 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2685 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2678 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2666 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2665 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2664 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2658 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2663 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2657 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2658 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2668 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2672 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2667 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2676 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2670 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2667 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2663 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2647 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2648 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2659 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2660 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2671 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2665 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2664 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2664 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2672 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2670 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2678 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2682 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2680 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2689 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2694 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2697 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2696 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2694 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2692 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2697 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2703 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2711 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2710 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2713 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2725 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2725 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2720 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2719 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2710 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2710 - accuracy: 0.1471"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.2710 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2708 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2705 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2706 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2695 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2695 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2697 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2686 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2690 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2684 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2679 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2677 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2671 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2670 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2669 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2666 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2669 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2655 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2656 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2660 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2659 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2655 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2657 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2652 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2650 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2648 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2653 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2656 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2664 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2667 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2665 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2669 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2664 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2656 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2652 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2653 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2655 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2657 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2656 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2649 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2657 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2656 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2655 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2660 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2663 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2661 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2664 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2664 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2657 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2655 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2655 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2655 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2650 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2641 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2640 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2642 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2637 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2637 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2642 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2644 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2640 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2647 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2649 - accuracy: 0.14 - ETA: 59s - loss: 3.2647 - accuracy: 0.1482 - ETA: 58s - loss: 3.2646 - accuracy: 0.148 - ETA: 58s - loss: 3.2644 - accuracy: 0.148 - ETA: 57s - loss: 3.2640 - accuracy: 0.148 - ETA: 56s - loss: 3.2635 - accuracy: 0.148 - ETA: 55s - loss: 3.2628 - accuracy: 0.148 - ETA: 55s - loss: 3.2622 - accuracy: 0.148 - ETA: 54s - loss: 3.2623 - accuracy: 0.149 - ETA: 53s - loss: 3.2619 - accuracy: 0.148 - ETA: 52s - loss: 3.2615 - accuracy: 0.149 - ETA: 52s - loss: 3.2622 - accuracy: 0.148 - ETA: 51s - loss: 3.2624 - accuracy: 0.148 - ETA: 50s - loss: 3.2635 - accuracy: 0.148 - ETA: 50s - loss: 3.2629 - accuracy: 0.148 - ETA: 49s - loss: 3.2616 - accuracy: 0.149 - ETA: 48s - loss: 3.2616 - accuracy: 0.149 - ETA: 47s - loss: 3.2613 - accuracy: 0.149 - ETA: 47s - loss: 3.2612 - accuracy: 0.149 - ETA: 46s - loss: 3.2610 - accuracy: 0.149 - ETA: 45s - loss: 3.2615 - accuracy: 0.149 - ETA: 44s - loss: 3.2616 - accuracy: 0.149 - ETA: 44s - loss: 3.2614 - accuracy: 0.149 - ETA: 43s - loss: 3.2615 - accuracy: 0.149 - ETA: 42s - loss: 3.2621 - accuracy: 0.149 - ETA: 41s - loss: 3.2624 - accuracy: 0.149 - ETA: 41s - loss: 3.2620 - accuracy: 0.149 - ETA: 40s - loss: 3.2616 - accuracy: 0.149 - ETA: 39s - loss: 3.2621 - accuracy: 0.149 - ETA: 39s - loss: 3.2621 - accuracy: 0.149 - ETA: 38s - loss: 3.2621 - accuracy: 0.149 - ETA: 37s - loss: 3.2614 - accuracy: 0.149 - ETA: 36s - loss: 3.2617 - accuracy: 0.149 - ETA: 36s - loss: 3.2618 - accuracy: 0.149 - ETA: 35s - loss: 3.2611 - accuracy: 0.149 - ETA: 34s - loss: 3.2618 - accuracy: 0.149 - ETA: 33s - loss: 3.2620 - accuracy: 0.149 - ETA: 33s - loss: 3.2618 - accuracy: 0.149 - ETA: 32s - loss: 3.2620 - accuracy: 0.149 - ETA: 31s - loss: 3.2624 - accuracy: 0.149 - ETA: 30s - loss: 3.2625 - accuracy: 0.149 - ETA: 30s - loss: 3.2624 - accuracy: 0.149 - ETA: 29s - loss: 3.2624 - accuracy: 0.149 - ETA: 28s - loss: 3.2620 - accuracy: 0.149 - ETA: 27s - loss: 3.2619 - accuracy: 0.149 - ETA: 27s - loss: 3.2621 - accuracy: 0.149 - ETA: 26s - loss: 3.2626 - accuracy: 0.149 - ETA: 25s - loss: 3.2625 - accuracy: 0.149 - ETA: 25s - loss: 3.2629 - accuracy: 0.149 - ETA: 24s - loss: 3.2627 - accuracy: 0.149 - ETA: 23s - loss: 3.2624 - accuracy: 0.149 - ETA: 22s - loss: 3.2631 - accuracy: 0.148 - ETA: 22s - loss: 3.2632 - accuracy: 0.148 - ETA: 21s - loss: 3.2633 - accuracy: 0.148 - ETA: 20s - loss: 3.2633 - accuracy: 0.148 - ETA: 19s - loss: 3.2638 - accuracy: 0.148 - ETA: 19s - loss: 3.2638 - accuracy: 0.148 - ETA: 18s - loss: 3.2637 - accuracy: 0.148 - ETA: 17s - loss: 3.2635 - accuracy: 0.149 - ETA: 16s - loss: 3.2644 - accuracy: 0.149 - ETA: 16s - loss: 3.2638 - accuracy: 0.148 - ETA: 15s - loss: 3.2639 - accuracy: 0.148 - ETA: 14s - loss: 3.2648 - accuracy: 0.148 - ETA: 14s - loss: 3.2650 - accuracy: 0.148 - ETA: 13s - loss: 3.2649 - accuracy: 0.148 - ETA: 12s - loss: 3.2651 - accuracy: 0.148 - ETA: 11s - loss: 3.2642 - accuracy: 0.148 - ETA: 11s - loss: 3.2646 - accuracy: 0.148 - ETA: 10s - loss: 3.2649 - accuracy: 0.148 - ETA: 9s - loss: 3.2655 - accuracy: 0.148 - ETA: 8s - loss: 3.2654 - accuracy: 0.14 - ETA: 8s - loss: 3.2647 - accuracy: 0.14 - ETA: 7s - loss: 3.2648 - accuracy: 0.14 - ETA: 6s - loss: 3.2646 - accuracy: 0.14 - ETA: 5s - loss: 3.2645 - accuracy: 0.14 - ETA: 5s - loss: 3.2644 - accuracy: 0.14 - ETA: 4s - loss: 3.2635 - accuracy: 0.14 - ETA: 3s - loss: 3.2640 - accuracy: 0.14 - ETA: 2s - loss: 3.2636 - accuracy: 0.14 - ETA: 2s - loss: 3.2636 - accuracy: 0.14 - ETA: 1s - loss: 3.2633 - accuracy: 0.14 - ETA: 0s - loss: 3.2635 - accuracy: 0.14 - ETA: 0s - loss: 3.2637 - accuracy: 0.14 - 262s 6ms/step - loss: 3.2638 - accuracy: 0.1490 - val_loss: 3.9929 - val_accuracy: 0.0235\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:58 - loss: 3.1715 - accuracy: 0.16 - ETA: 3:57 - loss: 3.1330 - accuracy: 0.18 - ETA: 3:55 - loss: 3.1722 - accuracy: 0.18 - ETA: 3:52 - loss: 3.1319 - accuracy: 0.18 - ETA: 3:52 - loss: 3.1640 - accuracy: 0.18 - ETA: 3:51 - loss: 3.1324 - accuracy: 0.18 - ETA: 3:51 - loss: 3.1634 - accuracy: 0.18 - ETA: 3:50 - loss: 3.2357 - accuracy: 0.17 - ETA: 3:50 - loss: 3.2528 - accuracy: 0.17 - ETA: 3:49 - loss: 3.2660 - accuracy: 0.17 - ETA: 3:51 - loss: 3.2688 - accuracy: 0.16 - ETA: 3:50 - loss: 3.2876 - accuracy: 0.16 - ETA: 3:49 - loss: 3.2886 - accuracy: 0.15 - ETA: 3:48 - loss: 3.2981 - accuracy: 0.15 - ETA: 3:47 - loss: 3.2943 - accuracy: 0.15 - ETA: 3:47 - loss: 3.3055 - accuracy: 0.15 - ETA: 3:47 - loss: 3.3052 - accuracy: 0.15 - ETA: 3:46 - loss: 3.2992 - accuracy: 0.15 - ETA: 3:45 - loss: 3.3141 - accuracy: 0.15 - ETA: 3:44 - loss: 3.3255 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3259 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3290 - accuracy: 0.14 - ETA: 3:43 - loss: 3.3296 - accuracy: 0.14 - ETA: 3:42 - loss: 3.3286 - accuracy: 0.14 - ETA: 3:42 - loss: 3.3348 - accuracy: 0.14 - ETA: 3:41 - loss: 3.3393 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3389 - accuracy: 0.14 - ETA: 3:39 - loss: 3.3316 - accuracy: 0.14 - ETA: 3:38 - loss: 3.3317 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3262 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3213 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3198 - accuracy: 0.14 - ETA: 3:35 - loss: 3.3142 - accuracy: 0.14 - ETA: 3:35 - loss: 3.3170 - accuracy: 0.14 - ETA: 3:34 - loss: 3.3194 - accuracy: 0.14 - ETA: 3:33 - loss: 3.3214 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3211 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3147 - accuracy: 0.14 - ETA: 3:31 - loss: 3.3184 - accuracy: 0.14 - ETA: 3:30 - loss: 3.3207 - accuracy: 0.14 - ETA: 3:30 - loss: 3.3220 - accuracy: 0.14 - ETA: 3:29 - loss: 3.3227 - accuracy: 0.14 - ETA: 3:28 - loss: 3.3188 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3196 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3223 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3203 - accuracy: 0.14 - ETA: 3:26 - loss: 3.3200 - accuracy: 0.14 - ETA: 3:25 - loss: 3.3202 - accuracy: 0.14 - ETA: 3:24 - loss: 3.3206 - accuracy: 0.14 - ETA: 3:24 - loss: 3.3198 - accuracy: 0.14 - ETA: 3:23 - loss: 3.3176 - accuracy: 0.14 - ETA: 3:22 - loss: 3.3140 - accuracy: 0.14 - ETA: 3:22 - loss: 3.3101 - accuracy: 0.14 - ETA: 3:21 - loss: 3.3125 - accuracy: 0.14 - ETA: 3:20 - loss: 3.3143 - accuracy: 0.14 - ETA: 3:19 - loss: 3.3142 - accuracy: 0.14 - ETA: 3:19 - loss: 3.3130 - accuracy: 0.14 - ETA: 3:18 - loss: 3.3116 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3141 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3149 - accuracy: 0.14 - ETA: 3:16 - loss: 3.3144 - accuracy: 0.14 - ETA: 3:15 - loss: 3.3157 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3135 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3137 - accuracy: 0.14 - ETA: 3:13 - loss: 3.3139 - accuracy: 0.14 - ETA: 3:12 - loss: 3.3129 - accuracy: 0.14 - ETA: 3:12 - loss: 3.3118 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3114 - accuracy: 0.14 - ETA: 3:10 - loss: 3.3121 - accuracy: 0.14 - ETA: 3:09 - loss: 3.3125 - accuracy: 0.14 - ETA: 3:09 - loss: 3.3131 - accuracy: 0.14 - ETA: 3:08 - loss: 3.3151 - accuracy: 0.14 - ETA: 3:08 - loss: 3.3147 - accuracy: 0.14 - ETA: 3:07 - loss: 3.3136 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3111 - accuracy: 0.14 - ETA: 3:05 - loss: 3.3111 - accuracy: 0.14 - ETA: 3:05 - loss: 3.3092 - accuracy: 0.14 - ETA: 3:04 - loss: 3.3086 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3074 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3073 - accuracy: 0.14 - ETA: 3:02 - loss: 3.3112 - accuracy: 0.14 - ETA: 3:01 - loss: 3.3129 - accuracy: 0.14 - ETA: 3:01 - loss: 3.3145 - accuracy: 0.14 - ETA: 3:00 - loss: 3.3142 - accuracy: 0.14 - ETA: 2:59 - loss: 3.3133 - accuracy: 0.14 - ETA: 2:59 - loss: 3.3145 - accuracy: 0.14 - ETA: 2:58 - loss: 3.3147 - accuracy: 0.14 - ETA: 2:57 - loss: 3.3146 - accuracy: 0.14 - ETA: 2:56 - loss: 3.3154 - accuracy: 0.14 - ETA: 2:56 - loss: 3.3142 - accuracy: 0.14 - ETA: 2:55 - loss: 3.3106 - accuracy: 0.14 - ETA: 2:54 - loss: 3.3091 - accuracy: 0.14 - ETA: 2:54 - loss: 3.3096 - accuracy: 0.14 - ETA: 2:53 - loss: 3.3081 - accuracy: 0.14 - ETA: 2:52 - loss: 3.3080 - accuracy: 0.14 - ETA: 2:51 - loss: 3.3083 - accuracy: 0.14 - ETA: 2:51 - loss: 3.3080 - accuracy: 0.14 - ETA: 2:50 - loss: 3.3088 - accuracy: 0.14 - ETA: 2:49 - loss: 3.3081 - accuracy: 0.14 - ETA: 2:49 - loss: 3.3092 - accuracy: 0.14 - ETA: 2:48 - loss: 3.3086 - accuracy: 0.14 - ETA: 2:47 - loss: 3.3087 - accuracy: 0.14 - ETA: 2:46 - loss: 3.3082 - accuracy: 0.14 - ETA: 2:46 - loss: 3.3074 - accuracy: 0.14 - ETA: 2:45 - loss: 3.3070 - accuracy: 0.14 - ETA: 2:44 - loss: 3.3073 - accuracy: 0.14 - ETA: 2:44 - loss: 3.3080 - accuracy: 0.14 - ETA: 2:43 - loss: 3.3072 - accuracy: 0.14 - ETA: 2:42 - loss: 3.3074 - accuracy: 0.14 - ETA: 2:41 - loss: 3.3074 - accuracy: 0.14 - ETA: 2:41 - loss: 3.3071 - accuracy: 0.14 - ETA: 2:40 - loss: 3.3067 - accuracy: 0.14 - ETA: 2:39 - loss: 3.3068 - accuracy: 0.14 - ETA: 2:38 - loss: 3.3079 - accuracy: 0.14 - ETA: 2:38 - loss: 3.3073 - accuracy: 0.14 - ETA: 2:37 - loss: 3.3091 - accuracy: 0.14 - ETA: 2:36 - loss: 3.3090 - accuracy: 0.14 - ETA: 2:35 - loss: 3.3087 - accuracy: 0.14 - ETA: 2:35 - loss: 3.3076 - accuracy: 0.14 - ETA: 2:34 - loss: 3.3065 - accuracy: 0.14 - ETA: 2:33 - loss: 3.3051 - accuracy: 0.14 - ETA: 2:33 - loss: 3.3052 - accuracy: 0.14 - ETA: 2:32 - loss: 3.3041 - accuracy: 0.14 - ETA: 2:31 - loss: 3.3056 - accuracy: 0.14 - ETA: 2:30 - loss: 3.3043 - accuracy: 0.14 - ETA: 2:30 - loss: 3.3037 - accuracy: 0.14 - ETA: 2:29 - loss: 3.3037 - accuracy: 0.14 - ETA: 2:28 - loss: 3.3029 - accuracy: 0.14 - ETA: 2:27 - loss: 3.3015 - accuracy: 0.14 - ETA: 2:27 - loss: 3.3016 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3007 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3005 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2997 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2987 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2973 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2957 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2949 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2958 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2954 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2943 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2935 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2919 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2904 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2901 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2886 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2891 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2886 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2889 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2891 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2891 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2886 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2882 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2889 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2898 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2900 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2894 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2886 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2877 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2873 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2876 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2881 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3051 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3052 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3051 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3044 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3041 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3041 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3023 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3013 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3013 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3023 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3017 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3013 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3017 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3017 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3008 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3000 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2993 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2996 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2990 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2971 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2955 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2957 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2952 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2946 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2947 - accuracy: 0.1486"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.2935 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2941 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2932 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2927 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2927 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2926 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2923 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2922 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2925 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2921 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2924 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2925 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2926 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2920 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2927 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2926 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2950 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2943 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2936 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2932 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2936 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2937 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2941 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2933 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2923 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2923 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2925 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2933 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2927 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2925 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2927 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2925 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2918 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2921 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2922 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2925 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2928 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2917 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2914 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2907 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2911 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2944 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2940 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2947 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2937 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2931 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2939 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2942 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2957 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2957 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2951 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2947 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2940 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2948 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2945 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2940 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2939 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2942 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2938 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2940 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2944 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2948 - accuracy: 0.15 - ETA: 59s - loss: 3.2949 - accuracy: 0.1505 - ETA: 59s - loss: 3.2946 - accuracy: 0.150 - ETA: 58s - loss: 3.2942 - accuracy: 0.150 - ETA: 57s - loss: 3.2944 - accuracy: 0.150 - ETA: 57s - loss: 3.2952 - accuracy: 0.150 - ETA: 56s - loss: 3.2958 - accuracy: 0.150 - ETA: 55s - loss: 3.2959 - accuracy: 0.150 - ETA: 54s - loss: 3.2957 - accuracy: 0.150 - ETA: 54s - loss: 3.2952 - accuracy: 0.150 - ETA: 53s - loss: 3.2950 - accuracy: 0.150 - ETA: 52s - loss: 3.2950 - accuracy: 0.150 - ETA: 52s - loss: 3.2943 - accuracy: 0.150 - ETA: 51s - loss: 3.2946 - accuracy: 0.149 - ETA: 50s - loss: 3.2943 - accuracy: 0.150 - ETA: 49s - loss: 3.2944 - accuracy: 0.149 - ETA: 49s - loss: 3.2941 - accuracy: 0.149 - ETA: 48s - loss: 3.2943 - accuracy: 0.149 - ETA: 47s - loss: 3.2942 - accuracy: 0.149 - ETA: 46s - loss: 3.2937 - accuracy: 0.149 - ETA: 46s - loss: 3.2940 - accuracy: 0.149 - ETA: 45s - loss: 3.2945 - accuracy: 0.149 - ETA: 44s - loss: 3.2943 - accuracy: 0.149 - ETA: 44s - loss: 3.2939 - accuracy: 0.149 - ETA: 43s - loss: 3.2943 - accuracy: 0.149 - ETA: 42s - loss: 3.2946 - accuracy: 0.149 - ETA: 41s - loss: 3.2942 - accuracy: 0.149 - ETA: 41s - loss: 3.2937 - accuracy: 0.149 - ETA: 40s - loss: 3.2941 - accuracy: 0.149 - ETA: 39s - loss: 3.2940 - accuracy: 0.149 - ETA: 38s - loss: 3.2941 - accuracy: 0.149 - ETA: 38s - loss: 3.2938 - accuracy: 0.149 - ETA: 37s - loss: 3.2941 - accuracy: 0.149 - ETA: 36s - loss: 3.2937 - accuracy: 0.149 - ETA: 35s - loss: 3.2934 - accuracy: 0.149 - ETA: 35s - loss: 3.2942 - accuracy: 0.149 - ETA: 34s - loss: 3.2941 - accuracy: 0.149 - ETA: 33s - loss: 3.2939 - accuracy: 0.149 - ETA: 33s - loss: 3.2944 - accuracy: 0.149 - ETA: 32s - loss: 3.2934 - accuracy: 0.149 - ETA: 31s - loss: 3.2937 - accuracy: 0.149 - ETA: 30s - loss: 3.2935 - accuracy: 0.149 - ETA: 30s - loss: 3.2925 - accuracy: 0.149 - ETA: 29s - loss: 3.2927 - accuracy: 0.149 - ETA: 28s - loss: 3.2931 - accuracy: 0.149 - ETA: 27s - loss: 3.2932 - accuracy: 0.149 - ETA: 27s - loss: 3.2933 - accuracy: 0.149 - ETA: 26s - loss: 3.2929 - accuracy: 0.149 - ETA: 25s - loss: 3.2936 - accuracy: 0.149 - ETA: 24s - loss: 3.2935 - accuracy: 0.149 - ETA: 24s - loss: 3.2944 - accuracy: 0.149 - ETA: 23s - loss: 3.2953 - accuracy: 0.149 - ETA: 22s - loss: 3.2952 - accuracy: 0.149 - ETA: 22s - loss: 3.2947 - accuracy: 0.149 - ETA: 21s - loss: 3.2953 - accuracy: 0.149 - ETA: 20s - loss: 3.2954 - accuracy: 0.149 - ETA: 19s - loss: 3.2955 - accuracy: 0.149 - ETA: 19s - loss: 3.2952 - accuracy: 0.149 - ETA: 18s - loss: 3.2951 - accuracy: 0.149 - ETA: 17s - loss: 3.2959 - accuracy: 0.149 - ETA: 16s - loss: 3.2960 - accuracy: 0.149 - ETA: 16s - loss: 3.2955 - accuracy: 0.149 - ETA: 15s - loss: 3.2964 - accuracy: 0.149 - ETA: 14s - loss: 3.2977 - accuracy: 0.149 - ETA: 13s - loss: 3.2987 - accuracy: 0.149 - ETA: 13s - loss: 3.2994 - accuracy: 0.149 - ETA: 12s - loss: 3.2993 - accuracy: 0.149 - ETA: 11s - loss: 3.2990 - accuracy: 0.149 - ETA: 11s - loss: 3.2993 - accuracy: 0.149 - ETA: 10s - loss: 3.2995 - accuracy: 0.149 - ETA: 9s - loss: 3.3000 - accuracy: 0.149 - ETA: 8s - loss: 3.3001 - accuracy: 0.14 - ETA: 8s - loss: 3.3002 - accuracy: 0.14 - ETA: 7s - loss: 3.3005 - accuracy: 0.14 - ETA: 6s - loss: 3.3003 - accuracy: 0.14 - ETA: 5s - loss: 3.3009 - accuracy: 0.14 - ETA: 5s - loss: 3.3012 - accuracy: 0.14 - ETA: 4s - loss: 3.3011 - accuracy: 0.14 - ETA: 3s - loss: 3.3014 - accuracy: 0.14 - ETA: 2s - loss: 3.3014 - accuracy: 0.14 - ETA: 2s - loss: 3.3013 - accuracy: 0.14 - ETA: 1s - loss: 3.3017 - accuracy: 0.14 - ETA: 0s - loss: 3.3020 - accuracy: 0.14 - ETA: 0s - loss: 3.3025 - accuracy: 0.14 - 261s 6ms/step - loss: 3.3025 - accuracy: 0.1481 - val_loss: 4.0016 - val_accuracy: 0.0174\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:05 - loss: 3.4101 - accuracy: 0.17 - ETA: 3:55 - loss: 3.3333 - accuracy: 0.15 - ETA: 3:58 - loss: 3.2977 - accuracy: 0.15 - ETA: 3:58 - loss: 3.2792 - accuracy: 0.16 - ETA: 3:56 - loss: 3.2567 - accuracy: 0.17 - ETA: 3:55 - loss: 3.2592 - accuracy: 0.16 - ETA: 3:54 - loss: 3.2856 - accuracy: 0.16 - ETA: 3:54 - loss: 3.2908 - accuracy: 0.15 - ETA: 3:52 - loss: 3.2886 - accuracy: 0.15 - ETA: 3:52 - loss: 3.2999 - accuracy: 0.15 - ETA: 3:51 - loss: 3.3083 - accuracy: 0.15 - ETA: 3:51 - loss: 3.2987 - accuracy: 0.15 - ETA: 3:51 - loss: 3.3074 - accuracy: 0.15 - ETA: 3:50 - loss: 3.3157 - accuracy: 0.15 - ETA: 3:49 - loss: 3.3197 - accuracy: 0.15 - ETA: 3:48 - loss: 3.3293 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3232 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3247 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3322 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3255 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3302 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3382 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3252 - accuracy: 0.14 - ETA: 3:45 - loss: 3.3303 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3255 - accuracy: 0.14 - ETA: 3:43 - loss: 3.3228 - accuracy: 0.14 - ETA: 3:42 - loss: 3.3245 - accuracy: 0.14 - ETA: 3:41 - loss: 3.3199 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3182 - accuracy: 0.15 - ETA: 3:40 - loss: 3.3261 - accuracy: 0.14 - ETA: 3:39 - loss: 3.3235 - accuracy: 0.14 - ETA: 3:38 - loss: 3.3228 - accuracy: 0.15 - ETA: 3:37 - loss: 3.3227 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3199 - accuracy: 0.15 - ETA: 3:36 - loss: 3.3173 - accuracy: 0.15 - ETA: 3:35 - loss: 3.3236 - accuracy: 0.15 - ETA: 3:35 - loss: 3.3132 - accuracy: 0.15 - ETA: 3:34 - loss: 3.3115 - accuracy: 0.15 - ETA: 3:33 - loss: 3.3140 - accuracy: 0.15 - ETA: 3:33 - loss: 3.3148 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3143 - accuracy: 0.14 - ETA: 3:31 - loss: 3.3141 - accuracy: 0.15 - ETA: 3:30 - loss: 3.3167 - accuracy: 0.14 - ETA: 3:29 - loss: 3.3161 - accuracy: 0.14 - ETA: 3:29 - loss: 3.3145 - accuracy: 0.14 - ETA: 3:28 - loss: 3.3116 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3160 - accuracy: 0.14 - ETA: 3:26 - loss: 3.3148 - accuracy: 0.14 - ETA: 3:25 - loss: 3.3130 - accuracy: 0.14 - ETA: 3:25 - loss: 3.3136 - accuracy: 0.14 - ETA: 3:24 - loss: 3.3150 - accuracy: 0.14 - ETA: 3:23 - loss: 3.3169 - accuracy: 0.14 - ETA: 3:22 - loss: 3.3204 - accuracy: 0.14 - ETA: 3:21 - loss: 3.3234 - accuracy: 0.14 - ETA: 3:21 - loss: 3.3273 - accuracy: 0.14 - ETA: 3:20 - loss: 3.3307 - accuracy: 0.14 - ETA: 3:19 - loss: 3.3333 - accuracy: 0.14 - ETA: 3:19 - loss: 3.3341 - accuracy: 0.14 - ETA: 3:18 - loss: 3.3322 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3304 - accuracy: 0.14 - ETA: 3:16 - loss: 3.3314 - accuracy: 0.14 - ETA: 3:16 - loss: 3.3348 - accuracy: 0.14 - ETA: 3:15 - loss: 3.3347 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3361 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3355 - accuracy: 0.14 - ETA: 3:13 - loss: 3.3346 - accuracy: 0.14 - ETA: 3:12 - loss: 3.3342 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3339 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3328 - accuracy: 0.14 - ETA: 3:10 - loss: 3.3330 - accuracy: 0.14 - ETA: 3:09 - loss: 3.3354 - accuracy: 0.14 - ETA: 3:08 - loss: 3.3358 - accuracy: 0.14 - ETA: 3:08 - loss: 3.3363 - accuracy: 0.14 - ETA: 3:07 - loss: 3.3346 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3360 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3342 - accuracy: 0.14 - ETA: 3:05 - loss: 3.3342 - accuracy: 0.14 - ETA: 3:04 - loss: 3.3317 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3290 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3295 - accuracy: 0.14 - ETA: 3:02 - loss: 3.3291 - accuracy: 0.14 - ETA: 3:01 - loss: 3.3268 - accuracy: 0.14 - ETA: 3:01 - loss: 3.3266 - accuracy: 0.14 - ETA: 3:00 - loss: 3.3242 - accuracy: 0.14 - ETA: 2:59 - loss: 3.3209 - accuracy: 0.14 - ETA: 2:59 - loss: 3.3222 - accuracy: 0.14 - ETA: 2:58 - loss: 3.3207 - accuracy: 0.14 - ETA: 2:57 - loss: 3.3207 - accuracy: 0.14 - ETA: 2:57 - loss: 3.3213 - accuracy: 0.14 - ETA: 2:56 - loss: 3.3200 - accuracy: 0.14 - ETA: 2:55 - loss: 3.3188 - accuracy: 0.14 - ETA: 2:54 - loss: 3.3193 - accuracy: 0.14 - ETA: 2:54 - loss: 3.3191 - accuracy: 0.14 - ETA: 2:53 - loss: 3.3182 - accuracy: 0.14 - ETA: 2:52 - loss: 3.3160 - accuracy: 0.14 - ETA: 2:51 - loss: 3.3138 - accuracy: 0.14 - ETA: 2:50 - loss: 3.3144 - accuracy: 0.14 - ETA: 2:50 - loss: 3.3141 - accuracy: 0.14 - ETA: 2:49 - loss: 3.3142 - accuracy: 0.14 - ETA: 2:48 - loss: 3.3137 - accuracy: 0.14 - ETA: 2:48 - loss: 3.3109 - accuracy: 0.14 - ETA: 2:47 - loss: 3.3116 - accuracy: 0.14 - ETA: 2:46 - loss: 3.3122 - accuracy: 0.14 - ETA: 2:45 - loss: 3.3099 - accuracy: 0.14 - ETA: 2:44 - loss: 3.3107 - accuracy: 0.14 - ETA: 2:44 - loss: 3.3103 - accuracy: 0.14 - ETA: 2:43 - loss: 3.3116 - accuracy: 0.14 - ETA: 2:42 - loss: 3.3092 - accuracy: 0.14 - ETA: 2:41 - loss: 3.3076 - accuracy: 0.14 - ETA: 2:41 - loss: 3.3081 - accuracy: 0.14 - ETA: 2:40 - loss: 3.3077 - accuracy: 0.14 - ETA: 2:39 - loss: 3.3064 - accuracy: 0.14 - ETA: 2:39 - loss: 3.3043 - accuracy: 0.14 - ETA: 2:38 - loss: 3.3036 - accuracy: 0.14 - ETA: 2:37 - loss: 3.3034 - accuracy: 0.14 - ETA: 2:36 - loss: 3.3027 - accuracy: 0.14 - ETA: 2:36 - loss: 3.3021 - accuracy: 0.14 - ETA: 2:35 - loss: 3.3019 - accuracy: 0.14 - ETA: 2:34 - loss: 3.3011 - accuracy: 0.14 - ETA: 2:34 - loss: 3.3014 - accuracy: 0.14 - ETA: 2:33 - loss: 3.3019 - accuracy: 0.14 - ETA: 2:32 - loss: 3.3021 - accuracy: 0.14 - ETA: 2:31 - loss: 3.3040 - accuracy: 0.14 - ETA: 2:31 - loss: 3.3043 - accuracy: 0.14 - ETA: 2:30 - loss: 3.3037 - accuracy: 0.14 - ETA: 2:29 - loss: 3.3038 - accuracy: 0.14 - ETA: 2:29 - loss: 3.3040 - accuracy: 0.14 - ETA: 2:28 - loss: 3.3046 - accuracy: 0.14 - ETA: 2:27 - loss: 3.3039 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3036 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3013 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3016 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3009 - accuracy: 0.14 - ETA: 2:23 - loss: 3.3004 - accuracy: 0.14 - ETA: 2:23 - loss: 3.3021 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3028 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3035 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3027 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3024 - accuracy: 0.14 - ETA: 2:19 - loss: 3.3029 - accuracy: 0.14 - ETA: 2:18 - loss: 3.3015 - accuracy: 0.14 - ETA: 2:18 - loss: 3.3008 - accuracy: 0.14 - ETA: 2:17 - loss: 3.3016 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3014 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3022 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3018 - accuracy: 0.14 - ETA: 2:14 - loss: 3.3015 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3012 - accuracy: 0.14 - ETA: 2:12 - loss: 3.3011 - accuracy: 0.14 - ETA: 2:12 - loss: 3.3008 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2999 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3001 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2998 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2992 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2983 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2986 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2984 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2979 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2977 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2974 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2970 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2962 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2956 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2951 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2945 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2941 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2946 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2952 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2950 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2943 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2945 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2944 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2940 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2938 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2936 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2924 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2913 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2911 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2919 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2911 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2918 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2922 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2939 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2933 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2935 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2934 - accuracy: 0.1478"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.2932 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2932 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2937 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2938 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2934 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2930 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2926 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2923 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2918 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2920 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2916 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2923 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2917 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2906 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2905 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2902 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2890 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2888 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2887 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2894 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2896 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2897 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2887 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2890 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2893 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2891 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2881 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2885 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2885 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2884 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2884 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2878 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2876 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2871 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2866 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2858 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2848 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2853 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2848 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2839 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2835 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2834 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2839 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2828 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2825 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2821 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2819 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2813 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2818 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2820 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2820 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2818 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2822 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2826 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2826 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2820 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2818 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2818 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2820 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2817 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2808 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2801 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2802 - accuracy: 0.14 - ETA: 59s - loss: 3.2807 - accuracy: 0.1497 - ETA: 58s - loss: 3.2805 - accuracy: 0.149 - ETA: 57s - loss: 3.2802 - accuracy: 0.149 - ETA: 57s - loss: 3.2805 - accuracy: 0.149 - ETA: 56s - loss: 3.2809 - accuracy: 0.149 - ETA: 55s - loss: 3.2803 - accuracy: 0.149 - ETA: 54s - loss: 3.2802 - accuracy: 0.149 - ETA: 54s - loss: 3.2807 - accuracy: 0.149 - ETA: 53s - loss: 3.2809 - accuracy: 0.149 - ETA: 52s - loss: 3.2811 - accuracy: 0.149 - ETA: 51s - loss: 3.2813 - accuracy: 0.149 - ETA: 51s - loss: 3.2812 - accuracy: 0.149 - ETA: 50s - loss: 3.2817 - accuracy: 0.149 - ETA: 49s - loss: 3.2819 - accuracy: 0.149 - ETA: 49s - loss: 3.2820 - accuracy: 0.149 - ETA: 48s - loss: 3.2817 - accuracy: 0.149 - ETA: 47s - loss: 3.2813 - accuracy: 0.149 - ETA: 46s - loss: 3.2814 - accuracy: 0.149 - ETA: 46s - loss: 3.2818 - accuracy: 0.149 - ETA: 45s - loss: 3.2822 - accuracy: 0.149 - ETA: 44s - loss: 3.2820 - accuracy: 0.149 - ETA: 43s - loss: 3.2819 - accuracy: 0.149 - ETA: 43s - loss: 3.2821 - accuracy: 0.149 - ETA: 42s - loss: 3.2825 - accuracy: 0.149 - ETA: 41s - loss: 3.2823 - accuracy: 0.149 - ETA: 41s - loss: 3.2818 - accuracy: 0.149 - ETA: 40s - loss: 3.2811 - accuracy: 0.149 - ETA: 39s - loss: 3.2806 - accuracy: 0.149 - ETA: 38s - loss: 3.2798 - accuracy: 0.150 - ETA: 38s - loss: 3.2802 - accuracy: 0.149 - ETA: 37s - loss: 3.2802 - accuracy: 0.149 - ETA: 36s - loss: 3.2808 - accuracy: 0.149 - ETA: 35s - loss: 3.2810 - accuracy: 0.149 - ETA: 35s - loss: 3.2808 - accuracy: 0.149 - ETA: 34s - loss: 3.2809 - accuracy: 0.149 - ETA: 33s - loss: 3.2810 - accuracy: 0.149 - ETA: 33s - loss: 3.2808 - accuracy: 0.149 - ETA: 32s - loss: 3.2805 - accuracy: 0.149 - ETA: 31s - loss: 3.2807 - accuracy: 0.149 - ETA: 30s - loss: 3.2806 - accuracy: 0.149 - ETA: 30s - loss: 3.2808 - accuracy: 0.149 - ETA: 29s - loss: 3.2811 - accuracy: 0.149 - ETA: 28s - loss: 3.2810 - accuracy: 0.149 - ETA: 27s - loss: 3.2816 - accuracy: 0.149 - ETA: 27s - loss: 3.2820 - accuracy: 0.149 - ETA: 26s - loss: 3.2825 - accuracy: 0.149 - ETA: 25s - loss: 3.2830 - accuracy: 0.149 - ETA: 24s - loss: 3.2827 - accuracy: 0.149 - ETA: 24s - loss: 3.2829 - accuracy: 0.149 - ETA: 23s - loss: 3.2824 - accuracy: 0.149 - ETA: 22s - loss: 3.2824 - accuracy: 0.149 - ETA: 22s - loss: 3.2819 - accuracy: 0.149 - ETA: 21s - loss: 3.2818 - accuracy: 0.149 - ETA: 20s - loss: 3.2819 - accuracy: 0.149 - ETA: 19s - loss: 3.2826 - accuracy: 0.148 - ETA: 19s - loss: 3.2821 - accuracy: 0.149 - ETA: 18s - loss: 3.2828 - accuracy: 0.149 - ETA: 17s - loss: 3.2823 - accuracy: 0.149 - ETA: 16s - loss: 3.2829 - accuracy: 0.149 - ETA: 16s - loss: 3.2830 - accuracy: 0.149 - ETA: 15s - loss: 3.2823 - accuracy: 0.149 - ETA: 14s - loss: 3.2823 - accuracy: 0.149 - ETA: 13s - loss: 3.2826 - accuracy: 0.149 - ETA: 13s - loss: 3.2822 - accuracy: 0.149 - ETA: 12s - loss: 3.2819 - accuracy: 0.149 - ETA: 11s - loss: 3.2814 - accuracy: 0.149 - ETA: 11s - loss: 3.2814 - accuracy: 0.149 - ETA: 10s - loss: 3.2820 - accuracy: 0.149 - ETA: 9s - loss: 3.2823 - accuracy: 0.149 - ETA: 8s - loss: 3.2828 - accuracy: 0.14 - ETA: 8s - loss: 3.2821 - accuracy: 0.14 - ETA: 7s - loss: 3.2822 - accuracy: 0.14 - ETA: 6s - loss: 3.2823 - accuracy: 0.14 - ETA: 5s - loss: 3.2822 - accuracy: 0.14 - ETA: 5s - loss: 3.2824 - accuracy: 0.14 - ETA: 4s - loss: 3.2826 - accuracy: 0.14 - ETA: 3s - loss: 3.2825 - accuracy: 0.14 - ETA: 2s - loss: 3.2826 - accuracy: 0.14 - ETA: 2s - loss: 3.2825 - accuracy: 0.14 - ETA: 1s - loss: 3.2815 - accuracy: 0.14 - ETA: 0s - loss: 3.2810 - accuracy: 0.14 - ETA: 0s - loss: 3.2807 - accuracy: 0.14 - 260s 6ms/step - loss: 3.2806 - accuracy: 0.1497 - val_loss: 3.9975 - val_accuracy: 0.0264\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:17 - loss: 3.3563 - accuracy: 0.13 - ETA: 4:10 - loss: 3.4769 - accuracy: 0.11 - ETA: 4:05 - loss: 3.3938 - accuracy: 0.11 - ETA: 4:01 - loss: 3.4052 - accuracy: 0.12 - ETA: 4:01 - loss: 3.3756 - accuracy: 0.12 - ETA: 4:00 - loss: 3.3462 - accuracy: 0.12 - ETA: 3:58 - loss: 3.3342 - accuracy: 0.13 - ETA: 3:58 - loss: 3.3121 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3113 - accuracy: 0.13 - ETA: 3:54 - loss: 3.2974 - accuracy: 0.14 - ETA: 3:54 - loss: 3.2891 - accuracy: 0.14 - ETA: 3:53 - loss: 3.2816 - accuracy: 0.14 - ETA: 3:51 - loss: 3.2870 - accuracy: 0.14 - ETA: 3:51 - loss: 3.2794 - accuracy: 0.14 - ETA: 3:51 - loss: 3.2785 - accuracy: 0.14 - ETA: 3:50 - loss: 3.2813 - accuracy: 0.15 - ETA: 3:49 - loss: 3.2838 - accuracy: 0.15 - ETA: 3:48 - loss: 3.2797 - accuracy: 0.14 - ETA: 3:48 - loss: 3.2814 - accuracy: 0.14 - ETA: 3:47 - loss: 3.2759 - accuracy: 0.15 - ETA: 3:47 - loss: 3.2713 - accuracy: 0.15 - ETA: 3:45 - loss: 3.2687 - accuracy: 0.15 - ETA: 3:46 - loss: 3.2646 - accuracy: 0.15 - ETA: 3:45 - loss: 3.2626 - accuracy: 0.15 - ETA: 3:44 - loss: 3.2647 - accuracy: 0.15 - ETA: 3:43 - loss: 3.2669 - accuracy: 0.14 - ETA: 3:43 - loss: 3.2719 - accuracy: 0.15 - ETA: 3:42 - loss: 3.2750 - accuracy: 0.15 - ETA: 3:41 - loss: 3.2759 - accuracy: 0.14 - ETA: 3:40 - loss: 3.2764 - accuracy: 0.14 - ETA: 3:39 - loss: 3.2725 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2688 - accuracy: 0.15 - ETA: 3:38 - loss: 3.2692 - accuracy: 0.15 - ETA: 3:37 - loss: 3.2731 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2779 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2758 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2760 - accuracy: 0.15 - ETA: 3:33 - loss: 3.2699 - accuracy: 0.15 - ETA: 3:32 - loss: 3.2654 - accuracy: 0.15 - ETA: 3:32 - loss: 3.2664 - accuracy: 0.15 - ETA: 3:31 - loss: 3.2651 - accuracy: 0.15 - ETA: 3:30 - loss: 3.2611 - accuracy: 0.15 - ETA: 3:30 - loss: 3.2606 - accuracy: 0.15 - ETA: 3:29 - loss: 3.2654 - accuracy: 0.15 - ETA: 3:28 - loss: 3.2618 - accuracy: 0.15 - ETA: 3:28 - loss: 3.2610 - accuracy: 0.15 - ETA: 3:27 - loss: 3.2600 - accuracy: 0.15 - ETA: 3:27 - loss: 3.2626 - accuracy: 0.15 - ETA: 3:26 - loss: 3.2647 - accuracy: 0.15 - ETA: 3:26 - loss: 3.2690 - accuracy: 0.15 - ETA: 3:25 - loss: 3.2687 - accuracy: 0.15 - ETA: 3:24 - loss: 3.2696 - accuracy: 0.15 - ETA: 3:23 - loss: 3.2707 - accuracy: 0.15 - ETA: 3:22 - loss: 3.2729 - accuracy: 0.15 - ETA: 3:22 - loss: 3.2705 - accuracy: 0.15 - ETA: 3:21 - loss: 3.2690 - accuracy: 0.15 - ETA: 3:20 - loss: 3.2686 - accuracy: 0.15 - ETA: 3:19 - loss: 3.2676 - accuracy: 0.15 - ETA: 3:19 - loss: 3.2691 - accuracy: 0.15 - ETA: 3:18 - loss: 3.2671 - accuracy: 0.15 - ETA: 3:17 - loss: 3.2652 - accuracy: 0.15 - ETA: 3:17 - loss: 3.2645 - accuracy: 0.15 - ETA: 3:16 - loss: 3.2642 - accuracy: 0.15 - ETA: 3:15 - loss: 3.2619 - accuracy: 0.15 - ETA: 3:14 - loss: 3.2645 - accuracy: 0.15 - ETA: 3:14 - loss: 3.2608 - accuracy: 0.15 - ETA: 3:13 - loss: 3.2614 - accuracy: 0.15 - ETA: 3:12 - loss: 3.2612 - accuracy: 0.15 - ETA: 3:11 - loss: 3.2623 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2594 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2579 - accuracy: 0.15 - ETA: 3:09 - loss: 3.2601 - accuracy: 0.15 - ETA: 3:08 - loss: 3.2597 - accuracy: 0.15 - ETA: 3:07 - loss: 3.2624 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2636 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2646 - accuracy: 0.15 - ETA: 3:05 - loss: 3.2642 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2625 - accuracy: 0.15 - ETA: 3:03 - loss: 3.2654 - accuracy: 0.15 - ETA: 3:03 - loss: 3.2658 - accuracy: 0.15 - ETA: 3:02 - loss: 3.2665 - accuracy: 0.15 - ETA: 3:01 - loss: 3.2677 - accuracy: 0.15 - ETA: 3:00 - loss: 3.2676 - accuracy: 0.15 - ETA: 3:00 - loss: 3.2702 - accuracy: 0.15 - ETA: 2:59 - loss: 3.2717 - accuracy: 0.15 - ETA: 2:58 - loss: 3.2736 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2724 - accuracy: 0.15 - ETA: 2:57 - loss: 3.2707 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2702 - accuracy: 0.15 - ETA: 2:56 - loss: 3.2722 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2751 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2761 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2771 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2779 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2786 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2774 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2769 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2784 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2796 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2788 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2806 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2815 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2819 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2826 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2824 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2822 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2823 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2816 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2813 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2815 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2817 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2821 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2836 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2820 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2819 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2818 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2817 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2809 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2819 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2816 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2842 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2850 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2838 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2829 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2824 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2821 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2816 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2818 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2815 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2805 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2818 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2824 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2810 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2818 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2821 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2816 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2799 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2806 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2807 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2802 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2805 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2805 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2808 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2810 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2806 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2806 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2792 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2790 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2787 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2786 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2782 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2784 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2783 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2795 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2795 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2801 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2796 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2792 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2782 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2789 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2800 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2802 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2798 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2797 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2788 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2798 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2798 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2805 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2804 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2804 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2803 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2806 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2793 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2801 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2803 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2795 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2794 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2796 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2806 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2803 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2804 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2803 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2806 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2799 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2791 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2792 - accuracy: 0.1507"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.2796 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2787 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2793 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2786 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2790 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2784 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2782 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2786 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2777 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2791 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2785 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2788 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2782 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2776 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2778 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2778 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2772 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2766 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2760 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2759 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2754 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2747 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2745 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2746 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2743 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2741 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2748 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2750 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2739 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2737 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2739 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2735 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2738 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2738 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2728 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2724 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2722 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2732 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2722 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2717 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2714 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2714 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2713 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2710 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2712 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2705 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2714 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2714 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2709 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2713 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2707 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2705 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2702 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2699 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2697 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2695 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2698 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2699 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2691 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2688 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2686 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2694 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2696 - accuracy: 0.15 - ETA: 59s - loss: 3.2698 - accuracy: 0.1509 - ETA: 58s - loss: 3.2694 - accuracy: 0.151 - ETA: 58s - loss: 3.2690 - accuracy: 0.151 - ETA: 57s - loss: 3.2680 - accuracy: 0.151 - ETA: 56s - loss: 3.2674 - accuracy: 0.151 - ETA: 55s - loss: 3.2676 - accuracy: 0.151 - ETA: 55s - loss: 3.2669 - accuracy: 0.151 - ETA: 54s - loss: 3.2666 - accuracy: 0.151 - ETA: 53s - loss: 3.2665 - accuracy: 0.151 - ETA: 52s - loss: 3.2660 - accuracy: 0.151 - ETA: 52s - loss: 3.2652 - accuracy: 0.151 - ETA: 51s - loss: 3.2656 - accuracy: 0.151 - ETA: 50s - loss: 3.2655 - accuracy: 0.151 - ETA: 49s - loss: 3.2650 - accuracy: 0.151 - ETA: 49s - loss: 3.2649 - accuracy: 0.151 - ETA: 48s - loss: 3.2652 - accuracy: 0.151 - ETA: 47s - loss: 3.2645 - accuracy: 0.151 - ETA: 47s - loss: 3.2645 - accuracy: 0.151 - ETA: 46s - loss: 3.2644 - accuracy: 0.151 - ETA: 45s - loss: 3.2646 - accuracy: 0.151 - ETA: 44s - loss: 3.2648 - accuracy: 0.151 - ETA: 44s - loss: 3.2647 - accuracy: 0.151 - ETA: 43s - loss: 3.2647 - accuracy: 0.150 - ETA: 42s - loss: 3.2650 - accuracy: 0.151 - ETA: 41s - loss: 3.2653 - accuracy: 0.150 - ETA: 41s - loss: 3.2653 - accuracy: 0.150 - ETA: 40s - loss: 3.2646 - accuracy: 0.151 - ETA: 39s - loss: 3.2646 - accuracy: 0.151 - ETA: 38s - loss: 3.2647 - accuracy: 0.151 - ETA: 38s - loss: 3.2641 - accuracy: 0.151 - ETA: 37s - loss: 3.2640 - accuracy: 0.151 - ETA: 36s - loss: 3.2634 - accuracy: 0.151 - ETA: 36s - loss: 3.2632 - accuracy: 0.151 - ETA: 35s - loss: 3.2631 - accuracy: 0.151 - ETA: 34s - loss: 3.2631 - accuracy: 0.151 - ETA: 33s - loss: 3.2629 - accuracy: 0.150 - ETA: 33s - loss: 3.2623 - accuracy: 0.151 - ETA: 32s - loss: 3.2625 - accuracy: 0.151 - ETA: 31s - loss: 3.2621 - accuracy: 0.151 - ETA: 30s - loss: 3.2619 - accuracy: 0.151 - ETA: 30s - loss: 3.2616 - accuracy: 0.151 - ETA: 29s - loss: 3.2616 - accuracy: 0.151 - ETA: 28s - loss: 3.2617 - accuracy: 0.151 - ETA: 27s - loss: 3.2624 - accuracy: 0.150 - ETA: 27s - loss: 3.2621 - accuracy: 0.151 - ETA: 26s - loss: 3.2623 - accuracy: 0.151 - ETA: 25s - loss: 3.2617 - accuracy: 0.151 - ETA: 25s - loss: 3.2621 - accuracy: 0.151 - ETA: 24s - loss: 3.2623 - accuracy: 0.150 - ETA: 23s - loss: 3.2618 - accuracy: 0.151 - ETA: 22s - loss: 3.2614 - accuracy: 0.151 - ETA: 22s - loss: 3.2617 - accuracy: 0.151 - ETA: 21s - loss: 3.2613 - accuracy: 0.151 - ETA: 20s - loss: 3.2616 - accuracy: 0.151 - ETA: 19s - loss: 3.2614 - accuracy: 0.151 - ETA: 19s - loss: 3.2616 - accuracy: 0.151 - ETA: 18s - loss: 3.2615 - accuracy: 0.151 - ETA: 17s - loss: 3.2617 - accuracy: 0.151 - ETA: 16s - loss: 3.2614 - accuracy: 0.151 - ETA: 16s - loss: 3.2613 - accuracy: 0.151 - ETA: 15s - loss: 3.2615 - accuracy: 0.151 - ETA: 14s - loss: 3.2615 - accuracy: 0.151 - ETA: 13s - loss: 3.2616 - accuracy: 0.151 - ETA: 13s - loss: 3.2615 - accuracy: 0.151 - ETA: 12s - loss: 3.2616 - accuracy: 0.151 - ETA: 11s - loss: 3.2611 - accuracy: 0.151 - ETA: 11s - loss: 3.2610 - accuracy: 0.151 - ETA: 10s - loss: 3.2613 - accuracy: 0.151 - ETA: 9s - loss: 3.2609 - accuracy: 0.151 - ETA: 8s - loss: 3.2612 - accuracy: 0.15 - ETA: 8s - loss: 3.2608 - accuracy: 0.15 - ETA: 7s - loss: 3.2605 - accuracy: 0.15 - ETA: 6s - loss: 3.2604 - accuracy: 0.15 - ETA: 5s - loss: 3.2610 - accuracy: 0.15 - ETA: 5s - loss: 3.2606 - accuracy: 0.15 - ETA: 4s - loss: 3.2607 - accuracy: 0.15 - ETA: 3s - loss: 3.2609 - accuracy: 0.15 - ETA: 2s - loss: 3.2610 - accuracy: 0.15 - ETA: 2s - loss: 3.2617 - accuracy: 0.15 - ETA: 1s - loss: 3.2617 - accuracy: 0.15 - ETA: 0s - loss: 3.2616 - accuracy: 0.15 - ETA: 0s - loss: 3.2621 - accuracy: 0.15 - 260s 6ms/step - loss: 3.2621 - accuracy: 0.1514 - val_loss: 4.0043 - val_accuracy: 0.0184\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:15 - loss: 3.4208 - accuracy: 0.07 - ETA: 4:11 - loss: 3.3286 - accuracy: 0.12 - ETA: 4:10 - loss: 3.3419 - accuracy: 0.11 - ETA: 4:06 - loss: 3.3369 - accuracy: 0.12 - ETA: 4:05 - loss: 3.3182 - accuracy: 0.13 - ETA: 4:04 - loss: 3.3002 - accuracy: 0.14 - ETA: 4:01 - loss: 3.3016 - accuracy: 0.14 - ETA: 4:00 - loss: 3.3195 - accuracy: 0.14 - ETA: 3:58 - loss: 3.3063 - accuracy: 0.14 - ETA: 3:57 - loss: 3.3010 - accuracy: 0.15 - ETA: 3:55 - loss: 3.3038 - accuracy: 0.15 - ETA: 3:54 - loss: 3.2996 - accuracy: 0.16 - ETA: 3:53 - loss: 3.3046 - accuracy: 0.15 - ETA: 3:52 - loss: 3.3043 - accuracy: 0.15 - ETA: 3:53 - loss: 3.3012 - accuracy: 0.15 - ETA: 3:51 - loss: 3.3088 - accuracy: 0.15 - ETA: 3:50 - loss: 3.3092 - accuracy: 0.15 - ETA: 3:51 - loss: 3.3144 - accuracy: 0.15 - ETA: 3:49 - loss: 3.3124 - accuracy: 0.15 - ETA: 3:48 - loss: 3.3104 - accuracy: 0.15 - ETA: 3:47 - loss: 3.3131 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3002 - accuracy: 0.14 - ETA: 3:46 - loss: 3.2973 - accuracy: 0.15 - ETA: 3:45 - loss: 3.2930 - accuracy: 0.15 - ETA: 3:44 - loss: 3.2888 - accuracy: 0.15 - ETA: 3:43 - loss: 3.2942 - accuracy: 0.15 - ETA: 3:43 - loss: 3.2891 - accuracy: 0.15 - ETA: 3:42 - loss: 3.2833 - accuracy: 0.15 - ETA: 3:41 - loss: 3.2925 - accuracy: 0.15 - ETA: 3:40 - loss: 3.2967 - accuracy: 0.15 - ETA: 3:40 - loss: 3.2959 - accuracy: 0.15 - ETA: 3:39 - loss: 3.2941 - accuracy: 0.15 - ETA: 3:38 - loss: 3.2887 - accuracy: 0.15 - ETA: 3:37 - loss: 3.2842 - accuracy: 0.15 - ETA: 3:36 - loss: 3.2880 - accuracy: 0.15 - ETA: 3:36 - loss: 3.2916 - accuracy: 0.15 - ETA: 3:35 - loss: 3.2915 - accuracy: 0.15 - ETA: 3:34 - loss: 3.2963 - accuracy: 0.15 - ETA: 3:33 - loss: 3.2922 - accuracy: 0.15 - ETA: 3:32 - loss: 3.2881 - accuracy: 0.15 - ETA: 3:31 - loss: 3.2859 - accuracy: 0.15 - ETA: 3:30 - loss: 3.2883 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2865 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2846 - accuracy: 0.15 - ETA: 3:29 - loss: 3.2839 - accuracy: 0.15 - ETA: 3:28 - loss: 3.2869 - accuracy: 0.15 - ETA: 3:27 - loss: 3.2869 - accuracy: 0.15 - ETA: 3:27 - loss: 3.2878 - accuracy: 0.15 - ETA: 3:26 - loss: 3.2854 - accuracy: 0.15 - ETA: 3:25 - loss: 3.2843 - accuracy: 0.15 - ETA: 3:24 - loss: 3.2843 - accuracy: 0.15 - ETA: 3:23 - loss: 3.2842 - accuracy: 0.15 - ETA: 3:23 - loss: 3.2835 - accuracy: 0.15 - ETA: 3:22 - loss: 3.2852 - accuracy: 0.15 - ETA: 3:21 - loss: 3.2874 - accuracy: 0.15 - ETA: 3:20 - loss: 3.2864 - accuracy: 0.15 - ETA: 3:20 - loss: 3.2859 - accuracy: 0.15 - ETA: 3:19 - loss: 3.2853 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2840 - accuracy: 0.15 - ETA: 3:17 - loss: 3.2830 - accuracy: 0.15 - ETA: 3:17 - loss: 3.2835 - accuracy: 0.15 - ETA: 3:16 - loss: 3.2839 - accuracy: 0.15 - ETA: 3:15 - loss: 3.2840 - accuracy: 0.15 - ETA: 3:14 - loss: 3.2862 - accuracy: 0.15 - ETA: 3:13 - loss: 3.2830 - accuracy: 0.15 - ETA: 3:13 - loss: 3.2825 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2829 - accuracy: 0.15 - ETA: 3:11 - loss: 3.2817 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2841 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2795 - accuracy: 0.15 - ETA: 3:09 - loss: 3.2798 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2772 - accuracy: 0.15 - ETA: 3:08 - loss: 3.2751 - accuracy: 0.15 - ETA: 3:07 - loss: 3.2750 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2761 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2778 - accuracy: 0.15 - ETA: 3:05 - loss: 3.2771 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2740 - accuracy: 0.15 - ETA: 3:03 - loss: 3.2753 - accuracy: 0.15 - ETA: 3:02 - loss: 3.2759 - accuracy: 0.15 - ETA: 3:02 - loss: 3.2749 - accuracy: 0.15 - ETA: 3:01 - loss: 3.2747 - accuracy: 0.15 - ETA: 3:00 - loss: 3.2762 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2776 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2795 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2782 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2787 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2760 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2739 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2720 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2720 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2719 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2713 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2707 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2716 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2740 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2738 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2724 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2719 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2722 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2708 - accuracy: 0.15 - ETA: 2:47 - loss: 3.2698 - accuracy: 0.15 - ETA: 2:47 - loss: 3.2707 - accuracy: 0.15 - ETA: 2:46 - loss: 3.2692 - accuracy: 0.15 - ETA: 2:45 - loss: 3.2686 - accuracy: 0.15 - ETA: 2:45 - loss: 3.2687 - accuracy: 0.15 - ETA: 2:44 - loss: 3.2667 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2666 - accuracy: 0.15 - ETA: 2:42 - loss: 3.2664 - accuracy: 0.15 - ETA: 2:42 - loss: 3.2662 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2662 - accuracy: 0.15 - ETA: 2:40 - loss: 3.2635 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2621 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2618 - accuracy: 0.15 - ETA: 2:38 - loss: 3.2600 - accuracy: 0.15 - ETA: 2:37 - loss: 3.2594 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2593 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2612 - accuracy: 0.15 - ETA: 2:35 - loss: 3.2625 - accuracy: 0.15 - ETA: 2:34 - loss: 3.2624 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2625 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2616 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2626 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2611 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2612 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2607 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2608 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2615 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2616 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2611 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2616 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2611 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2608 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2609 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2596 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2604 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2615 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2613 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2612 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2606 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2604 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2592 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2580 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2576 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2579 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2583 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2589 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2577 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2572 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2571 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2570 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2576 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2551 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2545 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2542 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2533 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2531 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2525 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2530 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2531 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2539 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2530 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2527 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2520 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2510 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2511 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2515 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2521 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2511 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2503 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2498 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2504 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2500 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2504 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2499 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2508 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2497 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2497 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2493 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2486 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2498 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2492 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2495 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2491 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2494 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2493 - accuracy: 0.1548"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.2502 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2499 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2503 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2509 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2511 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2518 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2526 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2529 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2536 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2544 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2542 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2535 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2536 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2531 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2529 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2531 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2535 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2542 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2550 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2553 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2559 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2560 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2560 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2558 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2566 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2568 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2575 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2575 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2574 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2586 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2589 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2607 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2610 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2613 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2612 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2620 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2621 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2626 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2628 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2635 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2636 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2643 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2646 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2645 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2648 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2640 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2635 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2635 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2636 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2640 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2643 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2649 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2653 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2651 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2660 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2666 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2659 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2657 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2656 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2654 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2651 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2644 - accuracy: 0.15 - ETA: 59s - loss: 3.2642 - accuracy: 0.1504 - ETA: 59s - loss: 3.2642 - accuracy: 0.150 - ETA: 58s - loss: 3.2646 - accuracy: 0.150 - ETA: 57s - loss: 3.2653 - accuracy: 0.150 - ETA: 57s - loss: 3.2651 - accuracy: 0.150 - ETA: 56s - loss: 3.2658 - accuracy: 0.150 - ETA: 55s - loss: 3.2661 - accuracy: 0.150 - ETA: 54s - loss: 3.2668 - accuracy: 0.150 - ETA: 54s - loss: 3.2673 - accuracy: 0.150 - ETA: 53s - loss: 3.2668 - accuracy: 0.150 - ETA: 52s - loss: 3.2672 - accuracy: 0.150 - ETA: 51s - loss: 3.2675 - accuracy: 0.150 - ETA: 51s - loss: 3.2684 - accuracy: 0.149 - ETA: 50s - loss: 3.2684 - accuracy: 0.149 - ETA: 49s - loss: 3.2685 - accuracy: 0.149 - ETA: 49s - loss: 3.2690 - accuracy: 0.149 - ETA: 48s - loss: 3.2694 - accuracy: 0.149 - ETA: 47s - loss: 3.2700 - accuracy: 0.149 - ETA: 46s - loss: 3.2709 - accuracy: 0.149 - ETA: 46s - loss: 3.2714 - accuracy: 0.149 - ETA: 45s - loss: 3.2719 - accuracy: 0.149 - ETA: 44s - loss: 3.2708 - accuracy: 0.149 - ETA: 43s - loss: 3.2711 - accuracy: 0.149 - ETA: 43s - loss: 3.2711 - accuracy: 0.149 - ETA: 42s - loss: 3.2705 - accuracy: 0.149 - ETA: 41s - loss: 3.2713 - accuracy: 0.149 - ETA: 41s - loss: 3.2718 - accuracy: 0.149 - ETA: 40s - loss: 3.2729 - accuracy: 0.149 - ETA: 39s - loss: 3.2734 - accuracy: 0.149 - ETA: 38s - loss: 3.2732 - accuracy: 0.149 - ETA: 38s - loss: 3.2736 - accuracy: 0.149 - ETA: 37s - loss: 3.2742 - accuracy: 0.149 - ETA: 36s - loss: 3.2743 - accuracy: 0.149 - ETA: 35s - loss: 3.2744 - accuracy: 0.149 - ETA: 35s - loss: 3.2747 - accuracy: 0.149 - ETA: 34s - loss: 3.2751 - accuracy: 0.148 - ETA: 33s - loss: 3.2752 - accuracy: 0.148 - ETA: 32s - loss: 3.2753 - accuracy: 0.148 - ETA: 32s - loss: 3.2753 - accuracy: 0.148 - ETA: 31s - loss: 3.2755 - accuracy: 0.148 - ETA: 30s - loss: 3.2762 - accuracy: 0.148 - ETA: 30s - loss: 3.2765 - accuracy: 0.148 - ETA: 29s - loss: 3.2767 - accuracy: 0.148 - ETA: 28s - loss: 3.2775 - accuracy: 0.148 - ETA: 27s - loss: 3.2772 - accuracy: 0.148 - ETA: 27s - loss: 3.2774 - accuracy: 0.148 - ETA: 26s - loss: 3.2776 - accuracy: 0.148 - ETA: 25s - loss: 3.2779 - accuracy: 0.148 - ETA: 24s - loss: 3.2776 - accuracy: 0.148 - ETA: 24s - loss: 3.2780 - accuracy: 0.148 - ETA: 23s - loss: 3.2783 - accuracy: 0.148 - ETA: 22s - loss: 3.2784 - accuracy: 0.148 - ETA: 21s - loss: 3.2786 - accuracy: 0.148 - ETA: 21s - loss: 3.2789 - accuracy: 0.148 - ETA: 20s - loss: 3.2787 - accuracy: 0.148 - ETA: 19s - loss: 3.2784 - accuracy: 0.148 - ETA: 19s - loss: 3.2784 - accuracy: 0.148 - ETA: 18s - loss: 3.2783 - accuracy: 0.148 - ETA: 17s - loss: 3.2789 - accuracy: 0.148 - ETA: 16s - loss: 3.2793 - accuracy: 0.148 - ETA: 16s - loss: 3.2795 - accuracy: 0.148 - ETA: 15s - loss: 3.2799 - accuracy: 0.147 - ETA: 14s - loss: 3.2797 - accuracy: 0.147 - ETA: 13s - loss: 3.2795 - accuracy: 0.147 - ETA: 13s - loss: 3.2792 - accuracy: 0.148 - ETA: 12s - loss: 3.2798 - accuracy: 0.147 - ETA: 11s - loss: 3.2800 - accuracy: 0.147 - ETA: 11s - loss: 3.2803 - accuracy: 0.147 - ETA: 10s - loss: 3.2807 - accuracy: 0.147 - ETA: 9s - loss: 3.2813 - accuracy: 0.147 - ETA: 8s - loss: 3.2815 - accuracy: 0.14 - ETA: 8s - loss: 3.2809 - accuracy: 0.14 - ETA: 7s - loss: 3.2807 - accuracy: 0.14 - ETA: 6s - loss: 3.2809 - accuracy: 0.14 - ETA: 5s - loss: 3.2811 - accuracy: 0.14 - ETA: 5s - loss: 3.2811 - accuracy: 0.14 - ETA: 4s - loss: 3.2816 - accuracy: 0.14 - ETA: 3s - loss: 3.2813 - accuracy: 0.14 - ETA: 2s - loss: 3.2824 - accuracy: 0.14 - ETA: 2s - loss: 3.2824 - accuracy: 0.14 - ETA: 1s - loss: 3.2830 - accuracy: 0.14 - ETA: 0s - loss: 3.2831 - accuracy: 0.14 - ETA: 0s - loss: 3.2832 - accuracy: 0.14 - 259s 6ms/step - loss: 3.2832 - accuracy: 0.1474 - val_loss: 3.9342 - val_accuracy: 0.0184\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:55 - loss: 3.3176 - accuracy: 0.13 - ETA: 3:46 - loss: 3.2893 - accuracy: 0.16 - ETA: 3:50 - loss: 3.2706 - accuracy: 0.16 - ETA: 3:55 - loss: 3.3027 - accuracy: 0.15 - ETA: 3:57 - loss: 3.3025 - accuracy: 0.15 - ETA: 3:57 - loss: 3.2909 - accuracy: 0.15 - ETA: 3:57 - loss: 3.3225 - accuracy: 0.15 - ETA: 3:58 - loss: 3.3465 - accuracy: 0.15 - ETA: 3:57 - loss: 3.3410 - accuracy: 0.15 - ETA: 3:56 - loss: 3.3448 - accuracy: 0.14 - ETA: 3:54 - loss: 3.3363 - accuracy: 0.15 - ETA: 3:53 - loss: 3.3382 - accuracy: 0.14 - ETA: 3:51 - loss: 3.3346 - accuracy: 0.15 - ETA: 3:50 - loss: 3.3279 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3350 - accuracy: 0.14 - ETA: 3:51 - loss: 3.3385 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3382 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3365 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3333 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3354 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3395 - accuracy: 0.14 - ETA: 3:45 - loss: 3.3356 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3296 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3320 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3387 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3364 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3326 - accuracy: 0.14 - ETA: 3:45 - loss: 3.3306 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3208 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3230 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3157 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3163 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3058 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3015 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3011 - accuracy: 0.14 - ETA: 3:50 - loss: 3.2963 - accuracy: 0.15 - ETA: 3:51 - loss: 3.2919 - accuracy: 0.15 - ETA: 3:52 - loss: 3.2891 - accuracy: 0.15 - ETA: 3:53 - loss: 3.2896 - accuracy: 0.15 - ETA: 3:52 - loss: 3.2936 - accuracy: 0.15 - ETA: 3:52 - loss: 3.2902 - accuracy: 0.15 - ETA: 3:51 - loss: 3.2909 - accuracy: 0.14 - ETA: 3:50 - loss: 3.2948 - accuracy: 0.14 - ETA: 3:49 - loss: 3.2946 - accuracy: 0.14 - ETA: 3:49 - loss: 3.2949 - accuracy: 0.14 - ETA: 3:48 - loss: 3.2911 - accuracy: 0.14 - ETA: 3:47 - loss: 3.2925 - accuracy: 0.14 - ETA: 3:46 - loss: 3.2906 - accuracy: 0.14 - ETA: 3:45 - loss: 3.2924 - accuracy: 0.14 - ETA: 3:44 - loss: 3.2919 - accuracy: 0.14 - ETA: 3:43 - loss: 3.2942 - accuracy: 0.14 - ETA: 3:42 - loss: 3.2926 - accuracy: 0.14 - ETA: 3:41 - loss: 3.2910 - accuracy: 0.14 - ETA: 3:40 - loss: 3.2908 - accuracy: 0.14 - ETA: 3:39 - loss: 3.2925 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2903 - accuracy: 0.14 - ETA: 3:37 - loss: 3.2905 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2911 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2861 - accuracy: 0.15 - ETA: 3:34 - loss: 3.2868 - accuracy: 0.15 - ETA: 3:33 - loss: 3.2873 - accuracy: 0.15 - ETA: 3:32 - loss: 3.2899 - accuracy: 0.14 - ETA: 3:31 - loss: 3.2925 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2936 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2942 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2949 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2948 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2940 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2948 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2948 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2953 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2970 - accuracy: 0.14 - ETA: 3:22 - loss: 3.3011 - accuracy: 0.14 - ETA: 3:21 - loss: 3.3002 - accuracy: 0.15 - ETA: 3:20 - loss: 3.3007 - accuracy: 0.15 - ETA: 3:19 - loss: 3.3020 - accuracy: 0.14 - ETA: 3:18 - loss: 3.3018 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3004 - accuracy: 0.15 - ETA: 3:16 - loss: 3.2984 - accuracy: 0.15 - ETA: 3:15 - loss: 3.2982 - accuracy: 0.15 - ETA: 3:14 - loss: 3.2977 - accuracy: 0.15 - ETA: 3:13 - loss: 3.2971 - accuracy: 0.15 - ETA: 3:12 - loss: 3.2961 - accuracy: 0.15 - ETA: 3:11 - loss: 3.2965 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2949 - accuracy: 0.15 - ETA: 3:09 - loss: 3.2952 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2948 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2945 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2945 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2928 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2915 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2919 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2901 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2902 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2917 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2908 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2918 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2944 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2956 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2947 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2942 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2965 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2948 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2964 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2975 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2961 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2945 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2951 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2954 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2960 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2954 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2945 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2958 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2950 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2949 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2937 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2928 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2920 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2929 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2908 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2917 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2934 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2946 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2953 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2949 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2950 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2957 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2967 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2965 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2975 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2993 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2985 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2974 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2982 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2971 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2976 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2983 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2995 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3000 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3007 - accuracy: 0.14 - ETA: 2:23 - loss: 3.3010 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3015 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3015 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3016 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3006 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2998 - accuracy: 0.14 - ETA: 2:18 - loss: 3.3001 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2998 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2996 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2992 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2995 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2997 - accuracy: 0.14 - ETA: 2:14 - loss: 3.3000 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3005 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2999 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3004 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3002 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2999 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2993 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2986 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2992 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2992 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2993 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2989 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2975 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2979 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2974 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2977 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2975 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2972 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2970 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2974 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2964 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2963 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2959 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2953 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2951 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2959 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2949 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2941 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2942 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2934 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2928 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2934 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2944 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2933 - accuracy: 0.1466"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:48 - loss: 3.2929 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2922 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2918 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2920 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2909 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2911 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2905 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2903 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2906 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2908 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2904 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2910 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2905 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2911 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2913 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2909 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2906 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2908 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2915 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2905 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2910 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2922 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2921 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2920 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2921 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2914 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2924 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2925 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2925 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2929 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2926 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2933 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2936 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2942 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2940 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2951 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2960 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2959 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2956 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2956 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2960 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2966 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2959 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2960 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2957 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2955 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2958 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2953 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2956 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2952 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2952 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2949 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2944 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2936 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2939 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2937 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2934 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2937 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2928 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2929 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2930 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2932 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2932 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2940 - accuracy: 0.14 - ETA: 59s - loss: 3.2939 - accuracy: 0.1468 - ETA: 58s - loss: 3.2940 - accuracy: 0.146 - ETA: 58s - loss: 3.2942 - accuracy: 0.146 - ETA: 57s - loss: 3.2940 - accuracy: 0.146 - ETA: 56s - loss: 3.2938 - accuracy: 0.146 - ETA: 55s - loss: 3.2935 - accuracy: 0.146 - ETA: 55s - loss: 3.2935 - accuracy: 0.146 - ETA: 54s - loss: 3.2929 - accuracy: 0.147 - ETA: 53s - loss: 3.2924 - accuracy: 0.147 - ETA: 52s - loss: 3.2916 - accuracy: 0.147 - ETA: 52s - loss: 3.2912 - accuracy: 0.147 - ETA: 51s - loss: 3.2908 - accuracy: 0.147 - ETA: 50s - loss: 3.2907 - accuracy: 0.147 - ETA: 49s - loss: 3.2907 - accuracy: 0.147 - ETA: 49s - loss: 3.2910 - accuracy: 0.147 - ETA: 48s - loss: 3.2910 - accuracy: 0.147 - ETA: 47s - loss: 3.2909 - accuracy: 0.147 - ETA: 46s - loss: 3.2913 - accuracy: 0.147 - ETA: 46s - loss: 3.2899 - accuracy: 0.147 - ETA: 45s - loss: 3.2892 - accuracy: 0.147 - ETA: 44s - loss: 3.2887 - accuracy: 0.147 - ETA: 43s - loss: 3.2885 - accuracy: 0.147 - ETA: 43s - loss: 3.2880 - accuracy: 0.148 - ETA: 42s - loss: 3.2883 - accuracy: 0.147 - ETA: 41s - loss: 3.2885 - accuracy: 0.147 - ETA: 40s - loss: 3.2876 - accuracy: 0.148 - ETA: 40s - loss: 3.2876 - accuracy: 0.148 - ETA: 39s - loss: 3.2865 - accuracy: 0.148 - ETA: 38s - loss: 3.2856 - accuracy: 0.148 - ETA: 37s - loss: 3.2851 - accuracy: 0.148 - ETA: 37s - loss: 3.2850 - accuracy: 0.148 - ETA: 36s - loss: 3.2845 - accuracy: 0.148 - ETA: 35s - loss: 3.2840 - accuracy: 0.148 - ETA: 34s - loss: 3.2845 - accuracy: 0.148 - ETA: 34s - loss: 3.2846 - accuracy: 0.148 - ETA: 33s - loss: 3.2849 - accuracy: 0.148 - ETA: 32s - loss: 3.2841 - accuracy: 0.148 - ETA: 32s - loss: 3.2836 - accuracy: 0.148 - ETA: 31s - loss: 3.2836 - accuracy: 0.148 - ETA: 30s - loss: 3.2832 - accuracy: 0.148 - ETA: 29s - loss: 3.2825 - accuracy: 0.148 - ETA: 29s - loss: 3.2821 - accuracy: 0.148 - ETA: 28s - loss: 3.2829 - accuracy: 0.148 - ETA: 27s - loss: 3.2833 - accuracy: 0.148 - ETA: 26s - loss: 3.2836 - accuracy: 0.148 - ETA: 26s - loss: 3.2834 - accuracy: 0.148 - ETA: 25s - loss: 3.2829 - accuracy: 0.148 - ETA: 24s - loss: 3.2831 - accuracy: 0.148 - ETA: 23s - loss: 3.2835 - accuracy: 0.148 - ETA: 23s - loss: 3.2831 - accuracy: 0.148 - ETA: 22s - loss: 3.2831 - accuracy: 0.148 - ETA: 21s - loss: 3.2827 - accuracy: 0.148 - ETA: 20s - loss: 3.2828 - accuracy: 0.148 - ETA: 20s - loss: 3.2829 - accuracy: 0.148 - ETA: 19s - loss: 3.2829 - accuracy: 0.148 - ETA: 18s - loss: 3.2823 - accuracy: 0.148 - ETA: 17s - loss: 3.2826 - accuracy: 0.148 - ETA: 17s - loss: 3.2830 - accuracy: 0.148 - ETA: 16s - loss: 3.2827 - accuracy: 0.148 - ETA: 15s - loss: 3.2832 - accuracy: 0.147 - ETA: 14s - loss: 3.2830 - accuracy: 0.147 - ETA: 14s - loss: 3.2828 - accuracy: 0.147 - ETA: 13s - loss: 3.2825 - accuracy: 0.147 - ETA: 12s - loss: 3.2822 - accuracy: 0.148 - ETA: 11s - loss: 3.2819 - accuracy: 0.148 - ETA: 11s - loss: 3.2816 - accuracy: 0.148 - ETA: 10s - loss: 3.2811 - accuracy: 0.148 - ETA: 9s - loss: 3.2805 - accuracy: 0.148 - ETA: 8s - loss: 3.2804 - accuracy: 0.14 - ETA: 8s - loss: 3.2803 - accuracy: 0.14 - ETA: 7s - loss: 3.2800 - accuracy: 0.14 - ETA: 6s - loss: 3.2799 - accuracy: 0.14 - ETA: 5s - loss: 3.2801 - accuracy: 0.14 - ETA: 5s - loss: 3.2803 - accuracy: 0.14 - ETA: 4s - loss: 3.2801 - accuracy: 0.14 - ETA: 3s - loss: 3.2802 - accuracy: 0.14 - ETA: 3s - loss: 3.2799 - accuracy: 0.14 - ETA: 2s - loss: 3.2799 - accuracy: 0.14 - ETA: 1s - loss: 3.2795 - accuracy: 0.14 - ETA: 0s - loss: 3.2799 - accuracy: 0.14 - ETA: 0s - loss: 3.2797 - accuracy: 0.14 - 263s 6ms/step - loss: 3.2798 - accuracy: 0.1486 - val_loss: 4.1574 - val_accuracy: 0.0329\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:55 - loss: 3.2179 - accuracy: 0.16 - ETA: 3:52 - loss: 3.2341 - accuracy: 0.17 - ETA: 3:55 - loss: 3.1913 - accuracy: 0.17 - ETA: 3:58 - loss: 3.2434 - accuracy: 0.15 - ETA: 4:00 - loss: 3.2542 - accuracy: 0.14 - ETA: 4:04 - loss: 3.2416 - accuracy: 0.15 - ETA: 4:04 - loss: 3.2351 - accuracy: 0.14 - ETA: 4:03 - loss: 3.2422 - accuracy: 0.14 - ETA: 4:01 - loss: 3.2643 - accuracy: 0.13 - ETA: 4:00 - loss: 3.2586 - accuracy: 0.14 - ETA: 3:59 - loss: 3.2519 - accuracy: 0.14 - ETA: 3:57 - loss: 3.2563 - accuracy: 0.14 - ETA: 3:57 - loss: 3.2528 - accuracy: 0.14 - ETA: 3:55 - loss: 3.2298 - accuracy: 0.14 - ETA: 3:54 - loss: 3.2421 - accuracy: 0.14 - ETA: 3:53 - loss: 3.2497 - accuracy: 0.14 - ETA: 3:52 - loss: 3.2485 - accuracy: 0.14 - ETA: 3:51 - loss: 3.2561 - accuracy: 0.14 - ETA: 3:50 - loss: 3.2610 - accuracy: 0.13 - ETA: 3:49 - loss: 3.2715 - accuracy: 0.13 - ETA: 3:48 - loss: 3.2670 - accuracy: 0.13 - ETA: 3:48 - loss: 3.2656 - accuracy: 0.14 - ETA: 3:47 - loss: 3.2645 - accuracy: 0.14 - ETA: 3:46 - loss: 3.2753 - accuracy: 0.14 - ETA: 3:45 - loss: 3.2712 - accuracy: 0.14 - ETA: 3:45 - loss: 3.2738 - accuracy: 0.14 - ETA: 3:44 - loss: 3.2730 - accuracy: 0.14 - ETA: 3:43 - loss: 3.2716 - accuracy: 0.14 - ETA: 3:42 - loss: 3.2768 - accuracy: 0.13 - ETA: 3:41 - loss: 3.2876 - accuracy: 0.13 - ETA: 3:40 - loss: 3.2836 - accuracy: 0.14 - ETA: 3:39 - loss: 3.2820 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2787 - accuracy: 0.14 - ETA: 3:37 - loss: 3.2769 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2757 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2757 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2776 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2777 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2781 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2814 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2814 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2825 - accuracy: 0.14 - ETA: 3:31 - loss: 3.2781 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2790 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2762 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2801 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2797 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2771 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2719 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2667 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2690 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2716 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2752 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2726 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2721 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2735 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2712 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2727 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2728 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2749 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2737 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2738 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2722 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2725 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2755 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2747 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2731 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2751 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2747 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2747 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2727 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2729 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2721 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2713 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2708 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2680 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2668 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2660 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2685 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2663 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2655 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2660 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2629 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2630 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2626 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2631 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2627 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2616 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2617 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2626 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2624 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2598 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2615 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2603 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2615 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2646 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2621 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2613 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2611 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2613 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2613 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2594 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2579 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2579 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2581 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2584 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2574 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2581 - accuracy: 0.15 - ETA: 2:42 - loss: 3.2581 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2568 - accuracy: 0.15 - ETA: 2:40 - loss: 3.2575 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2575 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2584 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2588 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2592 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2597 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2595 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2597 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2602 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2605 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2599 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2602 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2614 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2619 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2616 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2629 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2633 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2629 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2623 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2628 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2637 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2645 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2647 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2661 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2648 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2642 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2631 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2641 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2627 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2625 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2625 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2626 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2622 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2631 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2627 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2627 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2619 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2612 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2623 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2633 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2626 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2621 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2612 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2608 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2621 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2622 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2622 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2621 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2616 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2616 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2615 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2623 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2616 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2610 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2609 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2602 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2604 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2604 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2600 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2608 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2601 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2580 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2579 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2587 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2596 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2599 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2593 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2594 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2599 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2586 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2579 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2578 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2569 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2565 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2575 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2580 - accuracy: 0.1495"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.2572 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2570 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2569 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2566 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2558 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2571 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2571 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2561 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2568 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2574 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2580 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2569 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2557 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2555 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2547 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2537 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2538 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2532 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2531 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2523 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2516 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2524 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2511 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2503 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2513 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2515 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2517 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2516 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2511 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2513 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2502 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2505 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2496 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2498 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2499 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2494 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2486 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2483 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2480 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2477 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2479 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2479 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2471 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2478 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2477 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2478 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2481 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2475 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2466 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2464 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2464 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2460 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2453 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2448 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2452 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2445 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2441 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2442 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2445 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2441 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2432 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2431 - accuracy: 0.15 - ETA: 59s - loss: 3.2428 - accuracy: 0.1542 - ETA: 59s - loss: 3.2432 - accuracy: 0.154 - ETA: 58s - loss: 3.2432 - accuracy: 0.154 - ETA: 57s - loss: 3.2427 - accuracy: 0.154 - ETA: 57s - loss: 3.2421 - accuracy: 0.154 - ETA: 56s - loss: 3.2423 - accuracy: 0.154 - ETA: 55s - loss: 3.2429 - accuracy: 0.154 - ETA: 54s - loss: 3.2425 - accuracy: 0.154 - ETA: 54s - loss: 3.2433 - accuracy: 0.154 - ETA: 53s - loss: 3.2431 - accuracy: 0.154 - ETA: 52s - loss: 3.2429 - accuracy: 0.154 - ETA: 51s - loss: 3.2424 - accuracy: 0.154 - ETA: 51s - loss: 3.2422 - accuracy: 0.154 - ETA: 50s - loss: 3.2426 - accuracy: 0.154 - ETA: 49s - loss: 3.2428 - accuracy: 0.155 - ETA: 49s - loss: 3.2435 - accuracy: 0.154 - ETA: 48s - loss: 3.2437 - accuracy: 0.154 - ETA: 47s - loss: 3.2433 - accuracy: 0.154 - ETA: 46s - loss: 3.2438 - accuracy: 0.154 - ETA: 46s - loss: 3.2443 - accuracy: 0.154 - ETA: 45s - loss: 3.2445 - accuracy: 0.154 - ETA: 44s - loss: 3.2445 - accuracy: 0.154 - ETA: 43s - loss: 3.2440 - accuracy: 0.154 - ETA: 43s - loss: 3.2440 - accuracy: 0.154 - ETA: 42s - loss: 3.2436 - accuracy: 0.154 - ETA: 41s - loss: 3.2438 - accuracy: 0.154 - ETA: 41s - loss: 3.2441 - accuracy: 0.154 - ETA: 40s - loss: 3.2443 - accuracy: 0.154 - ETA: 39s - loss: 3.2446 - accuracy: 0.154 - ETA: 38s - loss: 3.2451 - accuracy: 0.154 - ETA: 38s - loss: 3.2448 - accuracy: 0.154 - ETA: 37s - loss: 3.2456 - accuracy: 0.154 - ETA: 36s - loss: 3.2459 - accuracy: 0.154 - ETA: 35s - loss: 3.2459 - accuracy: 0.154 - ETA: 35s - loss: 3.2463 - accuracy: 0.154 - ETA: 34s - loss: 3.2459 - accuracy: 0.154 - ETA: 33s - loss: 3.2463 - accuracy: 0.154 - ETA: 33s - loss: 3.2465 - accuracy: 0.154 - ETA: 32s - loss: 3.2468 - accuracy: 0.154 - ETA: 31s - loss: 3.2470 - accuracy: 0.154 - ETA: 30s - loss: 3.2472 - accuracy: 0.154 - ETA: 30s - loss: 3.2475 - accuracy: 0.153 - ETA: 29s - loss: 3.2476 - accuracy: 0.153 - ETA: 28s - loss: 3.2479 - accuracy: 0.153 - ETA: 27s - loss: 3.2479 - accuracy: 0.153 - ETA: 27s - loss: 3.2482 - accuracy: 0.153 - ETA: 26s - loss: 3.2477 - accuracy: 0.153 - ETA: 25s - loss: 3.2478 - accuracy: 0.153 - ETA: 25s - loss: 3.2476 - accuracy: 0.153 - ETA: 24s - loss: 3.2475 - accuracy: 0.153 - ETA: 23s - loss: 3.2472 - accuracy: 0.154 - ETA: 22s - loss: 3.2477 - accuracy: 0.153 - ETA: 22s - loss: 3.2478 - accuracy: 0.153 - ETA: 21s - loss: 3.2478 - accuracy: 0.153 - ETA: 20s - loss: 3.2477 - accuracy: 0.153 - ETA: 19s - loss: 3.2476 - accuracy: 0.153 - ETA: 19s - loss: 3.2475 - accuracy: 0.153 - ETA: 18s - loss: 3.2478 - accuracy: 0.153 - ETA: 17s - loss: 3.2481 - accuracy: 0.153 - ETA: 16s - loss: 3.2480 - accuracy: 0.153 - ETA: 16s - loss: 3.2483 - accuracy: 0.153 - ETA: 15s - loss: 3.2487 - accuracy: 0.153 - ETA: 14s - loss: 3.2496 - accuracy: 0.152 - ETA: 13s - loss: 3.2503 - accuracy: 0.152 - ETA: 13s - loss: 3.2503 - accuracy: 0.152 - ETA: 12s - loss: 3.2498 - accuracy: 0.152 - ETA: 11s - loss: 3.2495 - accuracy: 0.153 - ETA: 11s - loss: 3.2500 - accuracy: 0.152 - ETA: 10s - loss: 3.2498 - accuracy: 0.153 - ETA: 9s - loss: 3.2505 - accuracy: 0.152 - ETA: 8s - loss: 3.2502 - accuracy: 0.15 - ETA: 8s - loss: 3.2499 - accuracy: 0.15 - ETA: 7s - loss: 3.2499 - accuracy: 0.15 - ETA: 6s - loss: 3.2499 - accuracy: 0.15 - ETA: 5s - loss: 3.2503 - accuracy: 0.15 - ETA: 5s - loss: 3.2499 - accuracy: 0.15 - ETA: 4s - loss: 3.2494 - accuracy: 0.15 - ETA: 3s - loss: 3.2495 - accuracy: 0.15 - ETA: 2s - loss: 3.2499 - accuracy: 0.15 - ETA: 2s - loss: 3.2503 - accuracy: 0.15 - ETA: 1s - loss: 3.2505 - accuracy: 0.15 - ETA: 0s - loss: 3.2504 - accuracy: 0.15 - ETA: 0s - loss: 3.2504 - accuracy: 0.15 - 261s 6ms/step - loss: 3.2504 - accuracy: 0.1528 - val_loss: 4.0293 - val_accuracy: 0.0250\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:00 - loss: 3.1645 - accuracy: 0.16 - ETA: 3:58 - loss: 3.1479 - accuracy: 0.17 - ETA: 3:56 - loss: 3.2591 - accuracy: 0.15 - ETA: 4:03 - loss: 3.2384 - accuracy: 0.16 - ETA: 4:01 - loss: 3.2480 - accuracy: 0.15 - ETA: 4:04 - loss: 3.2395 - accuracy: 0.15 - ETA: 4:05 - loss: 3.2494 - accuracy: 0.15 - ETA: 4:05 - loss: 3.2667 - accuracy: 0.15 - ETA: 4:03 - loss: 3.2549 - accuracy: 0.15 - ETA: 4:04 - loss: 3.2515 - accuracy: 0.15 - ETA: 4:02 - loss: 3.2551 - accuracy: 0.15 - ETA: 4:00 - loss: 3.2688 - accuracy: 0.14 - ETA: 3:59 - loss: 3.2684 - accuracy: 0.14 - ETA: 3:57 - loss: 3.2601 - accuracy: 0.14 - ETA: 3:56 - loss: 3.2652 - accuracy: 0.14 - ETA: 3:55 - loss: 3.2656 - accuracy: 0.14 - ETA: 3:54 - loss: 3.2731 - accuracy: 0.14 - ETA: 3:52 - loss: 3.2697 - accuracy: 0.14 - ETA: 3:51 - loss: 3.2741 - accuracy: 0.14 - ETA: 3:50 - loss: 3.2704 - accuracy: 0.14 - ETA: 3:49 - loss: 3.2716 - accuracy: 0.14 - ETA: 3:48 - loss: 3.2775 - accuracy: 0.14 - ETA: 3:48 - loss: 3.2820 - accuracy: 0.14 - ETA: 3:48 - loss: 3.2846 - accuracy: 0.14 - ETA: 3:46 - loss: 3.2812 - accuracy: 0.14 - ETA: 3:46 - loss: 3.2805 - accuracy: 0.14 - ETA: 3:45 - loss: 3.2770 - accuracy: 0.14 - ETA: 3:44 - loss: 3.2811 - accuracy: 0.14 - ETA: 3:43 - loss: 3.2892 - accuracy: 0.13 - ETA: 3:42 - loss: 3.2802 - accuracy: 0.14 - ETA: 3:41 - loss: 3.2783 - accuracy: 0.14 - ETA: 3:40 - loss: 3.2828 - accuracy: 0.14 - ETA: 3:39 - loss: 3.2808 - accuracy: 0.14 - ETA: 3:39 - loss: 3.2827 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2784 - accuracy: 0.14 - ETA: 3:37 - loss: 3.2783 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2757 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2797 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2764 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2682 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2648 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2639 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2644 - accuracy: 0.14 - ETA: 3:31 - loss: 3.2661 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2657 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2615 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2631 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2636 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2613 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2634 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2633 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2632 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2598 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2609 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2627 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2605 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2591 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2612 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2576 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2536 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2540 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2528 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2524 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2542 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2547 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2521 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2507 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2484 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2476 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2494 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2515 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2514 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2530 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2518 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2504 - accuracy: 0.15 - ETA: 3:08 - loss: 3.2519 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2545 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2543 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2527 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2541 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2525 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2510 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2514 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2507 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2519 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2509 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2524 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2527 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2527 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2510 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2508 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2502 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2503 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2484 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2485 - accuracy: 0.15 - ETA: 2:54 - loss: 3.2474 - accuracy: 0.15 - ETA: 2:54 - loss: 3.2459 - accuracy: 0.15 - ETA: 2:53 - loss: 3.2454 - accuracy: 0.15 - ETA: 2:52 - loss: 3.2439 - accuracy: 0.15 - ETA: 2:52 - loss: 3.2430 - accuracy: 0.15 - ETA: 2:51 - loss: 3.2452 - accuracy: 0.15 - ETA: 2:50 - loss: 3.2448 - accuracy: 0.15 - ETA: 2:49 - loss: 3.2448 - accuracy: 0.15 - ETA: 2:48 - loss: 3.2453 - accuracy: 0.15 - ETA: 2:48 - loss: 3.2473 - accuracy: 0.15 - ETA: 2:47 - loss: 3.2457 - accuracy: 0.15 - ETA: 2:46 - loss: 3.2456 - accuracy: 0.15 - ETA: 2:45 - loss: 3.2443 - accuracy: 0.15 - ETA: 2:44 - loss: 3.2436 - accuracy: 0.15 - ETA: 2:44 - loss: 3.2434 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2432 - accuracy: 0.15 - ETA: 2:42 - loss: 3.2422 - accuracy: 0.15 - ETA: 2:42 - loss: 3.2416 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2399 - accuracy: 0.15 - ETA: 2:40 - loss: 3.2400 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2408 - accuracy: 0.15 - ETA: 2:38 - loss: 3.2399 - accuracy: 0.15 - ETA: 2:38 - loss: 3.2394 - accuracy: 0.15 - ETA: 2:37 - loss: 3.2390 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2391 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2389 - accuracy: 0.15 - ETA: 2:35 - loss: 3.2371 - accuracy: 0.15 - ETA: 2:34 - loss: 3.2348 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2346 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2340 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2346 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2345 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2360 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2360 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2362 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2346 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2343 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2336 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2340 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2351 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2351 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2356 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2361 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2373 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2371 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2382 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2382 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2376 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2365 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2367 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2357 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2359 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2362 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2368 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2357 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2341 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2349 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2340 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2342 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2349 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2361 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2357 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2349 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2353 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2354 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2356 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2355 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2359 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2375 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2378 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2369 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2376 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2380 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2379 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2374 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2368 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2377 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2389 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2387 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2385 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2390 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2387 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2394 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2385 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2380 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2370 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2380 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2374 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2372 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2368 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2367 - accuracy: 0.1535"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.2362 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2364 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2358 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2354 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2397 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2408 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2420 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2427 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2428 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2422 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2416 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2421 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2425 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2425 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2421 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2427 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2425 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2427 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2431 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2442 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2452 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2455 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2462 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2468 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2476 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2486 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2483 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2496 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2499 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2505 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2511 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2518 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2523 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2524 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2530 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2535 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2539 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2543 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2547 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2543 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2541 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2541 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2541 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2538 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2542 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2535 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2535 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2537 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2530 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2534 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2534 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2530 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2531 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2529 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2527 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2525 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2519 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2527 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2525 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2536 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2537 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2540 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2537 - accuracy: 0.15 - ETA: 59s - loss: 3.2538 - accuracy: 0.1515 - ETA: 58s - loss: 3.2540 - accuracy: 0.151 - ETA: 58s - loss: 3.2534 - accuracy: 0.151 - ETA: 57s - loss: 3.2533 - accuracy: 0.151 - ETA: 56s - loss: 3.2537 - accuracy: 0.151 - ETA: 55s - loss: 3.2536 - accuracy: 0.151 - ETA: 55s - loss: 3.2538 - accuracy: 0.151 - ETA: 54s - loss: 3.2532 - accuracy: 0.151 - ETA: 53s - loss: 3.2530 - accuracy: 0.151 - ETA: 52s - loss: 3.2526 - accuracy: 0.151 - ETA: 52s - loss: 3.2535 - accuracy: 0.151 - ETA: 51s - loss: 3.2539 - accuracy: 0.151 - ETA: 50s - loss: 3.2544 - accuracy: 0.151 - ETA: 50s - loss: 3.2544 - accuracy: 0.151 - ETA: 49s - loss: 3.2542 - accuracy: 0.151 - ETA: 48s - loss: 3.2534 - accuracy: 0.151 - ETA: 47s - loss: 3.2530 - accuracy: 0.151 - ETA: 47s - loss: 3.2528 - accuracy: 0.151 - ETA: 46s - loss: 3.2533 - accuracy: 0.151 - ETA: 45s - loss: 3.2528 - accuracy: 0.152 - ETA: 44s - loss: 3.2527 - accuracy: 0.152 - ETA: 44s - loss: 3.2532 - accuracy: 0.151 - ETA: 43s - loss: 3.2532 - accuracy: 0.152 - ETA: 42s - loss: 3.2530 - accuracy: 0.152 - ETA: 41s - loss: 3.2528 - accuracy: 0.152 - ETA: 41s - loss: 3.2529 - accuracy: 0.151 - ETA: 40s - loss: 3.2530 - accuracy: 0.151 - ETA: 39s - loss: 3.2524 - accuracy: 0.151 - ETA: 39s - loss: 3.2522 - accuracy: 0.152 - ETA: 38s - loss: 3.2524 - accuracy: 0.151 - ETA: 37s - loss: 3.2534 - accuracy: 0.151 - ETA: 36s - loss: 3.2564 - accuracy: 0.151 - ETA: 36s - loss: 3.2576 - accuracy: 0.151 - ETA: 35s - loss: 3.2580 - accuracy: 0.151 - ETA: 34s - loss: 3.2583 - accuracy: 0.151 - ETA: 33s - loss: 3.2587 - accuracy: 0.151 - ETA: 33s - loss: 3.2579 - accuracy: 0.151 - ETA: 32s - loss: 3.2579 - accuracy: 0.151 - ETA: 31s - loss: 3.2575 - accuracy: 0.151 - ETA: 30s - loss: 3.2579 - accuracy: 0.151 - ETA: 30s - loss: 3.2580 - accuracy: 0.151 - ETA: 29s - loss: 3.2578 - accuracy: 0.151 - ETA: 28s - loss: 3.2574 - accuracy: 0.151 - ETA: 27s - loss: 3.2573 - accuracy: 0.151 - ETA: 27s - loss: 3.2571 - accuracy: 0.151 - ETA: 26s - loss: 3.2575 - accuracy: 0.151 - ETA: 25s - loss: 3.2580 - accuracy: 0.151 - ETA: 25s - loss: 3.2583 - accuracy: 0.151 - ETA: 24s - loss: 3.2585 - accuracy: 0.151 - ETA: 23s - loss: 3.2587 - accuracy: 0.151 - ETA: 22s - loss: 3.2589 - accuracy: 0.151 - ETA: 22s - loss: 3.2591 - accuracy: 0.151 - ETA: 21s - loss: 3.2590 - accuracy: 0.151 - ETA: 20s - loss: 3.2598 - accuracy: 0.151 - ETA: 19s - loss: 3.2598 - accuracy: 0.151 - ETA: 19s - loss: 3.2601 - accuracy: 0.151 - ETA: 18s - loss: 3.2600 - accuracy: 0.151 - ETA: 17s - loss: 3.2603 - accuracy: 0.151 - ETA: 16s - loss: 3.2610 - accuracy: 0.150 - ETA: 16s - loss: 3.2612 - accuracy: 0.150 - ETA: 15s - loss: 3.2614 - accuracy: 0.150 - ETA: 14s - loss: 3.2615 - accuracy: 0.150 - ETA: 14s - loss: 3.2618 - accuracy: 0.150 - ETA: 13s - loss: 3.2620 - accuracy: 0.150 - ETA: 12s - loss: 3.2617 - accuracy: 0.150 - ETA: 11s - loss: 3.2624 - accuracy: 0.150 - ETA: 11s - loss: 3.2629 - accuracy: 0.150 - ETA: 10s - loss: 3.2639 - accuracy: 0.150 - ETA: 9s - loss: 3.2643 - accuracy: 0.150 - ETA: 8s - loss: 3.2651 - accuracy: 0.15 - ETA: 8s - loss: 3.2650 - accuracy: 0.15 - ETA: 7s - loss: 3.2652 - accuracy: 0.15 - ETA: 6s - loss: 3.2670 - accuracy: 0.15 - ETA: 5s - loss: 3.2681 - accuracy: 0.15 - ETA: 5s - loss: 3.2687 - accuracy: 0.15 - ETA: 4s - loss: 3.2691 - accuracy: 0.15 - ETA: 3s - loss: 3.2696 - accuracy: 0.15 - ETA: 2s - loss: 3.2698 - accuracy: 0.15 - ETA: 2s - loss: 3.2697 - accuracy: 0.15 - ETA: 1s - loss: 3.2698 - accuracy: 0.15 - ETA: 0s - loss: 3.2706 - accuracy: 0.15 - ETA: 0s - loss: 3.2705 - accuracy: 0.15 - 260s 6ms/step - loss: 3.2707 - accuracy: 0.1503 - val_loss: 3.8784 - val_accuracy: 0.0198\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:55 - loss: 3.5104 - accuracy: 0.12 - ETA: 3:59 - loss: 3.4581 - accuracy: 0.13 - ETA: 4:00 - loss: 3.3786 - accuracy: 0.14 - ETA: 3:56 - loss: 3.3933 - accuracy: 0.13 - ETA: 3:55 - loss: 3.4107 - accuracy: 0.13 - ETA: 3:53 - loss: 3.4035 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3989 - accuracy: 0.14 - ETA: 3:51 - loss: 3.3767 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3840 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3729 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3702 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3641 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3724 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3657 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3596 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3494 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3533 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3494 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3468 - accuracy: 0.14 - ETA: 3:45 - loss: 3.3441 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3562 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3615 - accuracy: 0.14 - ETA: 3:43 - loss: 3.3657 - accuracy: 0.14 - ETA: 3:42 - loss: 3.3673 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3655 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3667 - accuracy: 0.14 - ETA: 3:39 - loss: 3.3678 - accuracy: 0.14 - ETA: 3:38 - loss: 3.3622 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3623 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3657 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3612 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3549 - accuracy: 0.14 - ETA: 3:35 - loss: 3.3558 - accuracy: 0.14 - ETA: 3:34 - loss: 3.3544 - accuracy: 0.14 - ETA: 3:33 - loss: 3.3576 - accuracy: 0.14 - ETA: 3:33 - loss: 3.3582 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3569 - accuracy: 0.14 - ETA: 3:31 - loss: 3.3572 - accuracy: 0.14 - ETA: 3:31 - loss: 3.3588 - accuracy: 0.14 - ETA: 3:30 - loss: 3.3589 - accuracy: 0.14 - ETA: 3:29 - loss: 3.3586 - accuracy: 0.14 - ETA: 3:28 - loss: 3.3589 - accuracy: 0.14 - ETA: 3:28 - loss: 3.3607 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3590 - accuracy: 0.14 - ETA: 3:26 - loss: 3.3587 - accuracy: 0.14 - ETA: 3:25 - loss: 3.3603 - accuracy: 0.14 - ETA: 3:25 - loss: 3.3600 - accuracy: 0.14 - ETA: 3:24 - loss: 3.3613 - accuracy: 0.14 - ETA: 3:23 - loss: 3.3588 - accuracy: 0.14 - ETA: 3:22 - loss: 3.3588 - accuracy: 0.14 - ETA: 3:22 - loss: 3.3599 - accuracy: 0.14 - ETA: 3:21 - loss: 3.3578 - accuracy: 0.14 - ETA: 3:20 - loss: 3.3565 - accuracy: 0.14 - ETA: 3:20 - loss: 3.3535 - accuracy: 0.14 - ETA: 3:19 - loss: 3.3517 - accuracy: 0.14 - ETA: 3:18 - loss: 3.3511 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3521 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3520 - accuracy: 0.14 - ETA: 3:16 - loss: 3.3496 - accuracy: 0.14 - ETA: 3:16 - loss: 3.3462 - accuracy: 0.14 - ETA: 3:15 - loss: 3.3467 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3468 - accuracy: 0.14 - ETA: 3:13 - loss: 3.3467 - accuracy: 0.14 - ETA: 3:12 - loss: 3.3483 - accuracy: 0.14 - ETA: 3:12 - loss: 3.3496 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3494 - accuracy: 0.14 - ETA: 3:10 - loss: 3.3484 - accuracy: 0.14 - ETA: 3:10 - loss: 3.3486 - accuracy: 0.14 - ETA: 3:09 - loss: 3.3485 - accuracy: 0.14 - ETA: 3:08 - loss: 3.3459 - accuracy: 0.14 - ETA: 3:07 - loss: 3.3445 - accuracy: 0.14 - ETA: 3:07 - loss: 3.3431 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3427 - accuracy: 0.14 - ETA: 3:05 - loss: 3.3417 - accuracy: 0.14 - ETA: 3:04 - loss: 3.3423 - accuracy: 0.14 - ETA: 3:04 - loss: 3.3411 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3423 - accuracy: 0.14 - ETA: 3:02 - loss: 3.3427 - accuracy: 0.14 - ETA: 3:01 - loss: 3.3408 - accuracy: 0.14 - ETA: 3:01 - loss: 3.3402 - accuracy: 0.14 - ETA: 3:00 - loss: 3.3411 - accuracy: 0.14 - ETA: 2:59 - loss: 3.3414 - accuracy: 0.14 - ETA: 2:58 - loss: 3.3414 - accuracy: 0.14 - ETA: 2:56 - loss: 3.3404 - accuracy: 0.14 - ETA: 2:55 - loss: 3.3366 - accuracy: 0.14 - ETA: 2:54 - loss: 3.3369 - accuracy: 0.14 - ETA: 2:53 - loss: 3.3364 - accuracy: 0.14 - ETA: 2:51 - loss: 3.3378 - accuracy: 0.14 - ETA: 2:50 - loss: 3.3387 - accuracy: 0.14 - ETA: 2:49 - loss: 3.3361 - accuracy: 0.14 - ETA: 2:48 - loss: 3.3372 - accuracy: 0.14 - ETA: 2:46 - loss: 3.3385 - accuracy: 0.14 - ETA: 2:46 - loss: 3.3372 - accuracy: 0.14 - ETA: 2:45 - loss: 3.3375 - accuracy: 0.14 - ETA: 2:44 - loss: 3.3358 - accuracy: 0.14 - ETA: 2:44 - loss: 3.3347 - accuracy: 0.14 - ETA: 2:43 - loss: 3.3347 - accuracy: 0.14 - ETA: 2:43 - loss: 3.3342 - accuracy: 0.14 - ETA: 2:42 - loss: 3.3341 - accuracy: 0.14 - ETA: 2:42 - loss: 3.3344 - accuracy: 0.14 - ETA: 2:41 - loss: 3.3346 - accuracy: 0.14 - ETA: 2:40 - loss: 3.3323 - accuracy: 0.14 - ETA: 2:40 - loss: 3.3306 - accuracy: 0.14 - ETA: 2:39 - loss: 3.3302 - accuracy: 0.14 - ETA: 2:38 - loss: 3.3283 - accuracy: 0.14 - ETA: 2:38 - loss: 3.3284 - accuracy: 0.14 - ETA: 2:37 - loss: 3.3284 - accuracy: 0.14 - ETA: 2:37 - loss: 3.3288 - accuracy: 0.14 - ETA: 2:36 - loss: 3.3285 - accuracy: 0.14 - ETA: 2:36 - loss: 3.3288 - accuracy: 0.14 - ETA: 2:35 - loss: 3.3279 - accuracy: 0.14 - ETA: 2:34 - loss: 3.3268 - accuracy: 0.14 - ETA: 2:34 - loss: 3.3254 - accuracy: 0.14 - ETA: 2:33 - loss: 3.3258 - accuracy: 0.14 - ETA: 2:32 - loss: 3.3273 - accuracy: 0.14 - ETA: 2:32 - loss: 3.3286 - accuracy: 0.14 - ETA: 2:31 - loss: 3.3285 - accuracy: 0.14 - ETA: 2:30 - loss: 3.3289 - accuracy: 0.14 - ETA: 2:29 - loss: 3.3279 - accuracy: 0.14 - ETA: 2:29 - loss: 3.3273 - accuracy: 0.14 - ETA: 2:28 - loss: 3.3274 - accuracy: 0.14 - ETA: 2:27 - loss: 3.3284 - accuracy: 0.14 - ETA: 2:27 - loss: 3.3284 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3281 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3283 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3282 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3278 - accuracy: 0.14 - ETA: 2:23 - loss: 3.3265 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3259 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3258 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3250 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3256 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3249 - accuracy: 0.14 - ETA: 2:19 - loss: 3.3254 - accuracy: 0.14 - ETA: 2:18 - loss: 3.3256 - accuracy: 0.14 - ETA: 2:18 - loss: 3.3254 - accuracy: 0.14 - ETA: 2:17 - loss: 3.3259 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3254 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3252 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3259 - accuracy: 0.14 - ETA: 2:14 - loss: 3.3249 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3262 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3254 - accuracy: 0.14 - ETA: 2:12 - loss: 3.3243 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3248 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3255 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3250 - accuracy: 0.14 - ETA: 2:09 - loss: 3.3244 - accuracy: 0.14 - ETA: 2:09 - loss: 3.3242 - accuracy: 0.14 - ETA: 2:08 - loss: 3.3239 - accuracy: 0.14 - ETA: 2:07 - loss: 3.3241 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3254 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3257 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3253 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3255 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3253 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3260 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3268 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3264 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3265 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3273 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3278 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3276 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3277 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3275 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3268 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3273 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3273 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3261 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3271 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3266 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3278 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3266 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3262 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3261 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3258 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3259 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3259 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3253 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3255 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3246 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3242 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3243 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3234 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3234 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3229 - accuracy: 0.1432"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:42 - loss: 3.3242 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3243 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3235 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3230 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3228 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3230 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3230 - accuracy: 0.14 - ETA: 1:37 - loss: 3.3229 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3220 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3226 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3232 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3230 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3221 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3216 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3219 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3223 - accuracy: 0.14 - ETA: 1:31 - loss: 3.3220 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3215 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3210 - accuracy: 0.14 - ETA: 1:29 - loss: 3.3201 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3199 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3195 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3187 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3180 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3173 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3166 - accuracy: 0.14 - ETA: 1:24 - loss: 3.3164 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3160 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3158 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3163 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3167 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3161 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3153 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3152 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3144 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3137 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3138 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3140 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3143 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3126 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3134 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3137 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3127 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3129 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3124 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3118 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3123 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3119 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3114 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3104 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3109 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3103 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3102 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3103 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3104 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3108 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3103 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3104 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3095 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3089 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3082 - accuracy: 0.14 - ETA: 59s - loss: 3.3087 - accuracy: 0.1453 - ETA: 58s - loss: 3.3081 - accuracy: 0.145 - ETA: 57s - loss: 3.3079 - accuracy: 0.145 - ETA: 57s - loss: 3.3081 - accuracy: 0.145 - ETA: 56s - loss: 3.3083 - accuracy: 0.145 - ETA: 55s - loss: 3.3079 - accuracy: 0.145 - ETA: 55s - loss: 3.3068 - accuracy: 0.145 - ETA: 54s - loss: 3.3066 - accuracy: 0.145 - ETA: 53s - loss: 3.3064 - accuracy: 0.145 - ETA: 52s - loss: 3.3061 - accuracy: 0.145 - ETA: 52s - loss: 3.3056 - accuracy: 0.145 - ETA: 51s - loss: 3.3060 - accuracy: 0.145 - ETA: 50s - loss: 3.3053 - accuracy: 0.145 - ETA: 50s - loss: 3.3049 - accuracy: 0.145 - ETA: 49s - loss: 3.3047 - accuracy: 0.145 - ETA: 48s - loss: 3.3047 - accuracy: 0.145 - ETA: 47s - loss: 3.3052 - accuracy: 0.145 - ETA: 47s - loss: 3.3044 - accuracy: 0.145 - ETA: 46s - loss: 3.3042 - accuracy: 0.145 - ETA: 45s - loss: 3.3043 - accuracy: 0.145 - ETA: 45s - loss: 3.3038 - accuracy: 0.145 - ETA: 44s - loss: 3.3036 - accuracy: 0.145 - ETA: 43s - loss: 3.3029 - accuracy: 0.145 - ETA: 42s - loss: 3.3026 - accuracy: 0.145 - ETA: 42s - loss: 3.3020 - accuracy: 0.145 - ETA: 41s - loss: 3.3017 - accuracy: 0.146 - ETA: 40s - loss: 3.3015 - accuracy: 0.146 - ETA: 40s - loss: 3.3012 - accuracy: 0.146 - ETA: 39s - loss: 3.3016 - accuracy: 0.145 - ETA: 38s - loss: 3.3014 - accuracy: 0.145 - ETA: 37s - loss: 3.3019 - accuracy: 0.145 - ETA: 37s - loss: 3.3020 - accuracy: 0.145 - ETA: 36s - loss: 3.3021 - accuracy: 0.145 - ETA: 35s - loss: 3.3019 - accuracy: 0.145 - ETA: 35s - loss: 3.3021 - accuracy: 0.145 - ETA: 34s - loss: 3.3014 - accuracy: 0.145 - ETA: 33s - loss: 3.3008 - accuracy: 0.145 - ETA: 32s - loss: 3.3006 - accuracy: 0.145 - ETA: 32s - loss: 3.3002 - accuracy: 0.146 - ETA: 31s - loss: 3.3006 - accuracy: 0.145 - ETA: 30s - loss: 3.3006 - accuracy: 0.145 - ETA: 30s - loss: 3.3001 - accuracy: 0.146 - ETA: 29s - loss: 3.3003 - accuracy: 0.146 - ETA: 28s - loss: 3.3001 - accuracy: 0.146 - ETA: 27s - loss: 3.2999 - accuracy: 0.146 - ETA: 27s - loss: 3.2992 - accuracy: 0.146 - ETA: 26s - loss: 3.2985 - accuracy: 0.146 - ETA: 25s - loss: 3.2986 - accuracy: 0.146 - ETA: 25s - loss: 3.2982 - accuracy: 0.146 - ETA: 24s - loss: 3.2978 - accuracy: 0.146 - ETA: 23s - loss: 3.2976 - accuracy: 0.146 - ETA: 22s - loss: 3.2975 - accuracy: 0.146 - ETA: 22s - loss: 3.2971 - accuracy: 0.146 - ETA: 21s - loss: 3.2972 - accuracy: 0.146 - ETA: 20s - loss: 3.2972 - accuracy: 0.146 - ETA: 20s - loss: 3.2968 - accuracy: 0.146 - ETA: 19s - loss: 3.2976 - accuracy: 0.146 - ETA: 18s - loss: 3.2973 - accuracy: 0.146 - ETA: 17s - loss: 3.2972 - accuracy: 0.146 - ETA: 17s - loss: 3.2960 - accuracy: 0.147 - ETA: 16s - loss: 3.2964 - accuracy: 0.147 - ETA: 15s - loss: 3.2963 - accuracy: 0.147 - ETA: 15s - loss: 3.2961 - accuracy: 0.147 - ETA: 14s - loss: 3.2961 - accuracy: 0.147 - ETA: 13s - loss: 3.2957 - accuracy: 0.147 - ETA: 12s - loss: 3.2954 - accuracy: 0.147 - ETA: 12s - loss: 3.2954 - accuracy: 0.147 - ETA: 11s - loss: 3.2952 - accuracy: 0.147 - ETA: 10s - loss: 3.2950 - accuracy: 0.147 - ETA: 10s - loss: 3.2952 - accuracy: 0.147 - ETA: 9s - loss: 3.2948 - accuracy: 0.147 - ETA: 8s - loss: 3.2946 - accuracy: 0.14 - ETA: 7s - loss: 3.2946 - accuracy: 0.14 - ETA: 7s - loss: 3.2941 - accuracy: 0.14 - ETA: 6s - loss: 3.2940 - accuracy: 0.14 - ETA: 5s - loss: 3.2940 - accuracy: 0.14 - ETA: 5s - loss: 3.2947 - accuracy: 0.14 - ETA: 4s - loss: 3.2944 - accuracy: 0.14 - ETA: 3s - loss: 3.2946 - accuracy: 0.14 - ETA: 2s - loss: 3.2941 - accuracy: 0.14 - ETA: 2s - loss: 3.2938 - accuracy: 0.14 - ETA: 1s - loss: 3.2940 - accuracy: 0.14 - ETA: 0s - loss: 3.2940 - accuracy: 0.14 - ETA: 0s - loss: 3.2942 - accuracy: 0.14 - 254s 6ms/step - loss: 3.2940 - accuracy: 0.1475 - val_loss: 3.9809 - val_accuracy: 0.0279\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:00 - loss: 3.1967 - accuracy: 0.17 - ETA: 3:54 - loss: 3.2295 - accuracy: 0.16 - ETA: 3:51 - loss: 3.2055 - accuracy: 0.16 - ETA: 3:53 - loss: 3.2306 - accuracy: 0.16 - ETA: 3:54 - loss: 3.2484 - accuracy: 0.14 - ETA: 3:52 - loss: 3.2470 - accuracy: 0.14 - ETA: 3:52 - loss: 3.2467 - accuracy: 0.15 - ETA: 3:52 - loss: 3.2416 - accuracy: 0.15 - ETA: 3:51 - loss: 3.2578 - accuracy: 0.15 - ETA: 3:50 - loss: 3.2575 - accuracy: 0.14 - ETA: 3:50 - loss: 3.2596 - accuracy: 0.14 - ETA: 3:49 - loss: 3.2713 - accuracy: 0.14 - ETA: 3:49 - loss: 3.2727 - accuracy: 0.14 - ETA: 3:49 - loss: 3.2707 - accuracy: 0.14 - ETA: 3:48 - loss: 3.2748 - accuracy: 0.14 - ETA: 3:47 - loss: 3.2623 - accuracy: 0.15 - ETA: 3:46 - loss: 3.2636 - accuracy: 0.15 - ETA: 3:46 - loss: 3.2517 - accuracy: 0.15 - ETA: 3:47 - loss: 3.2542 - accuracy: 0.15 - ETA: 3:46 - loss: 3.2573 - accuracy: 0.15 - ETA: 3:45 - loss: 3.2614 - accuracy: 0.15 - ETA: 3:45 - loss: 3.2573 - accuracy: 0.15 - ETA: 3:45 - loss: 3.2493 - accuracy: 0.15 - ETA: 3:45 - loss: 3.2460 - accuracy: 0.15 - ETA: 3:45 - loss: 3.2491 - accuracy: 0.15 - ETA: 3:43 - loss: 3.2524 - accuracy: 0.15 - ETA: 3:42 - loss: 3.2473 - accuracy: 0.15 - ETA: 3:42 - loss: 3.2477 - accuracy: 0.15 - ETA: 3:41 - loss: 3.2470 - accuracy: 0.15 - ETA: 3:40 - loss: 3.2481 - accuracy: 0.15 - ETA: 3:39 - loss: 3.2525 - accuracy: 0.15 - ETA: 3:39 - loss: 3.2576 - accuracy: 0.15 - ETA: 3:38 - loss: 3.2588 - accuracy: 0.15 - ETA: 3:37 - loss: 3.2629 - accuracy: 0.15 - ETA: 3:36 - loss: 3.2618 - accuracy: 0.15 - ETA: 3:35 - loss: 3.2641 - accuracy: 0.15 - ETA: 3:34 - loss: 3.2645 - accuracy: 0.15 - ETA: 3:34 - loss: 3.2652 - accuracy: 0.15 - ETA: 3:33 - loss: 3.2654 - accuracy: 0.15 - ETA: 3:32 - loss: 3.2639 - accuracy: 0.15 - ETA: 3:31 - loss: 3.2671 - accuracy: 0.15 - ETA: 3:30 - loss: 3.2624 - accuracy: 0.15 - ETA: 3:29 - loss: 3.2602 - accuracy: 0.15 - ETA: 3:29 - loss: 3.2596 - accuracy: 0.15 - ETA: 3:28 - loss: 3.2572 - accuracy: 0.15 - ETA: 3:27 - loss: 3.2561 - accuracy: 0.15 - ETA: 3:26 - loss: 3.2733 - accuracy: 0.15 - ETA: 3:25 - loss: 3.2754 - accuracy: 0.15 - ETA: 3:24 - loss: 3.2735 - accuracy: 0.15 - ETA: 3:24 - loss: 3.2727 - accuracy: 0.15 - ETA: 3:23 - loss: 3.2712 - accuracy: 0.15 - ETA: 3:22 - loss: 3.2668 - accuracy: 0.15 - ETA: 3:22 - loss: 3.2682 - accuracy: 0.15 - ETA: 3:21 - loss: 3.2705 - accuracy: 0.15 - ETA: 3:20 - loss: 3.2710 - accuracy: 0.15 - ETA: 3:19 - loss: 3.2675 - accuracy: 0.15 - ETA: 3:18 - loss: 3.2655 - accuracy: 0.15 - ETA: 3:18 - loss: 3.2637 - accuracy: 0.15 - ETA: 3:17 - loss: 3.2662 - accuracy: 0.15 - ETA: 3:16 - loss: 3.2643 - accuracy: 0.15 - ETA: 3:15 - loss: 3.2654 - accuracy: 0.15 - ETA: 3:15 - loss: 3.2706 - accuracy: 0.15 - ETA: 3:14 - loss: 3.2733 - accuracy: 0.15 - ETA: 3:13 - loss: 3.2744 - accuracy: 0.15 - ETA: 3:12 - loss: 3.2731 - accuracy: 0.15 - ETA: 3:12 - loss: 3.2746 - accuracy: 0.15 - ETA: 3:11 - loss: 3.2732 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2736 - accuracy: 0.15 - ETA: 3:09 - loss: 3.2698 - accuracy: 0.15 - ETA: 3:09 - loss: 3.2710 - accuracy: 0.15 - ETA: 3:08 - loss: 3.2700 - accuracy: 0.15 - ETA: 3:07 - loss: 3.2707 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2697 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2725 - accuracy: 0.15 - ETA: 3:05 - loss: 3.2734 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2767 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2760 - accuracy: 0.15 - ETA: 3:02 - loss: 3.2766 - accuracy: 0.15 - ETA: 3:01 - loss: 3.2764 - accuracy: 0.15 - ETA: 3:00 - loss: 3.2789 - accuracy: 0.15 - ETA: 2:58 - loss: 3.2782 - accuracy: 0.15 - ETA: 2:57 - loss: 3.2784 - accuracy: 0.15 - ETA: 2:56 - loss: 3.2798 - accuracy: 0.15 - ETA: 2:54 - loss: 3.2807 - accuracy: 0.15 - ETA: 2:53 - loss: 3.2820 - accuracy: 0.15 - ETA: 2:52 - loss: 3.2820 - accuracy: 0.15 - ETA: 2:50 - loss: 3.2833 - accuracy: 0.15 - ETA: 2:49 - loss: 3.2836 - accuracy: 0.15 - ETA: 2:49 - loss: 3.2831 - accuracy: 0.15 - ETA: 2:48 - loss: 3.2826 - accuracy: 0.15 - ETA: 2:47 - loss: 3.2828 - accuracy: 0.15 - ETA: 2:47 - loss: 3.2824 - accuracy: 0.15 - ETA: 2:46 - loss: 3.2818 - accuracy: 0.15 - ETA: 2:45 - loss: 3.2804 - accuracy: 0.15 - ETA: 2:45 - loss: 3.2791 - accuracy: 0.15 - ETA: 2:44 - loss: 3.2792 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2785 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2792 - accuracy: 0.15 - ETA: 2:42 - loss: 3.2797 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2796 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2797 - accuracy: 0.15 - ETA: 2:40 - loss: 3.2795 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2790 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2782 - accuracy: 0.15 - ETA: 2:38 - loss: 3.2793 - accuracy: 0.15 - ETA: 2:37 - loss: 3.2777 - accuracy: 0.15 - ETA: 2:37 - loss: 3.2765 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2775 - accuracy: 0.15 - ETA: 2:35 - loss: 3.2797 - accuracy: 0.15 - ETA: 2:35 - loss: 3.2811 - accuracy: 0.15 - ETA: 2:34 - loss: 3.2802 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2806 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2806 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2798 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2809 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2808 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2816 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2814 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2823 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2823 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2823 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2833 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2844 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2832 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2824 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2817 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2802 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2803 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2810 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2815 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2828 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2824 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2810 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2817 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2815 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2813 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2801 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2801 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2795 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2790 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2788 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2798 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2792 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2782 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2791 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2801 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2788 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2786 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2776 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2773 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2767 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2770 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2758 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2752 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2743 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2747 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2733 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2731 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2730 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2731 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2730 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2733 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2729 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2733 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2732 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2732 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2731 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2732 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2740 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2732 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2736 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2734 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2731 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2731 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2742 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2740 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2749 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2753 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2753 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2761 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2760 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2770 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2770 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2779 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2785 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2785 - accuracy: 0.1499"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:42 - loss: 3.2796 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2793 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2792 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2796 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2795 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2803 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2802 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2803 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2799 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2803 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2805 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2807 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2804 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2810 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2816 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2823 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2830 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2833 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2843 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2847 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2848 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2851 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2851 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2855 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2865 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2880 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2880 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2898 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2907 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2910 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2917 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2919 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2942 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2935 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2933 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2935 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2944 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2948 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2946 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2947 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2953 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2959 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2966 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2976 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2980 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2985 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2993 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2997 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2988 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2986 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2990 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2989 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2985 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2987 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2995 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3005 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3002 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3005 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3018 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3034 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3045 - accuracy: 0.14 - ETA: 59s - loss: 3.3045 - accuracy: 0.1459 - ETA: 58s - loss: 3.3055 - accuracy: 0.145 - ETA: 58s - loss: 3.3061 - accuracy: 0.145 - ETA: 57s - loss: 3.3069 - accuracy: 0.145 - ETA: 56s - loss: 3.3076 - accuracy: 0.145 - ETA: 55s - loss: 3.3084 - accuracy: 0.145 - ETA: 55s - loss: 3.3084 - accuracy: 0.144 - ETA: 54s - loss: 3.3090 - accuracy: 0.144 - ETA: 53s - loss: 3.3090 - accuracy: 0.144 - ETA: 53s - loss: 3.3090 - accuracy: 0.144 - ETA: 52s - loss: 3.3095 - accuracy: 0.144 - ETA: 51s - loss: 3.3095 - accuracy: 0.144 - ETA: 50s - loss: 3.3098 - accuracy: 0.144 - ETA: 50s - loss: 3.3097 - accuracy: 0.144 - ETA: 49s - loss: 3.3093 - accuracy: 0.144 - ETA: 48s - loss: 3.3098 - accuracy: 0.144 - ETA: 48s - loss: 3.3099 - accuracy: 0.144 - ETA: 47s - loss: 3.3104 - accuracy: 0.144 - ETA: 46s - loss: 3.3107 - accuracy: 0.144 - ETA: 45s - loss: 3.3109 - accuracy: 0.144 - ETA: 45s - loss: 3.3110 - accuracy: 0.144 - ETA: 44s - loss: 3.3108 - accuracy: 0.144 - ETA: 43s - loss: 3.3113 - accuracy: 0.144 - ETA: 43s - loss: 3.3117 - accuracy: 0.144 - ETA: 42s - loss: 3.3117 - accuracy: 0.144 - ETA: 41s - loss: 3.3118 - accuracy: 0.144 - ETA: 40s - loss: 3.3118 - accuracy: 0.144 - ETA: 40s - loss: 3.3117 - accuracy: 0.144 - ETA: 39s - loss: 3.3120 - accuracy: 0.144 - ETA: 38s - loss: 3.3118 - accuracy: 0.144 - ETA: 38s - loss: 3.3111 - accuracy: 0.144 - ETA: 37s - loss: 3.3108 - accuracy: 0.144 - ETA: 36s - loss: 3.3104 - accuracy: 0.144 - ETA: 35s - loss: 3.3104 - accuracy: 0.144 - ETA: 35s - loss: 3.3104 - accuracy: 0.144 - ETA: 34s - loss: 3.3110 - accuracy: 0.144 - ETA: 33s - loss: 3.3114 - accuracy: 0.144 - ETA: 33s - loss: 3.3108 - accuracy: 0.144 - ETA: 32s - loss: 3.3116 - accuracy: 0.144 - ETA: 31s - loss: 3.3110 - accuracy: 0.144 - ETA: 30s - loss: 3.3111 - accuracy: 0.144 - ETA: 30s - loss: 3.3109 - accuracy: 0.144 - ETA: 29s - loss: 3.3111 - accuracy: 0.144 - ETA: 28s - loss: 3.3106 - accuracy: 0.144 - ETA: 28s - loss: 3.3112 - accuracy: 0.143 - ETA: 27s - loss: 3.3109 - accuracy: 0.144 - ETA: 26s - loss: 3.3101 - accuracy: 0.144 - ETA: 25s - loss: 3.3099 - accuracy: 0.144 - ETA: 25s - loss: 3.3093 - accuracy: 0.144 - ETA: 24s - loss: 3.3085 - accuracy: 0.144 - ETA: 23s - loss: 3.3083 - accuracy: 0.144 - ETA: 23s - loss: 3.3083 - accuracy: 0.144 - ETA: 22s - loss: 3.3085 - accuracy: 0.144 - ETA: 21s - loss: 3.3091 - accuracy: 0.144 - ETA: 20s - loss: 3.3089 - accuracy: 0.144 - ETA: 20s - loss: 3.3094 - accuracy: 0.144 - ETA: 19s - loss: 3.3094 - accuracy: 0.143 - ETA: 18s - loss: 3.3097 - accuracy: 0.143 - ETA: 17s - loss: 3.3099 - accuracy: 0.143 - ETA: 17s - loss: 3.3101 - accuracy: 0.144 - ETA: 16s - loss: 3.3099 - accuracy: 0.144 - ETA: 15s - loss: 3.3102 - accuracy: 0.144 - ETA: 14s - loss: 3.3103 - accuracy: 0.144 - ETA: 14s - loss: 3.3109 - accuracy: 0.143 - ETA: 13s - loss: 3.3108 - accuracy: 0.143 - ETA: 12s - loss: 3.3108 - accuracy: 0.143 - ETA: 12s - loss: 3.3116 - accuracy: 0.143 - ETA: 11s - loss: 3.3118 - accuracy: 0.143 - ETA: 10s - loss: 3.3119 - accuracy: 0.143 - ETA: 10s - loss: 3.3117 - accuracy: 0.143 - ETA: 9s - loss: 3.3118 - accuracy: 0.143 - ETA: 8s - loss: 3.3122 - accuracy: 0.14 - ETA: 7s - loss: 3.3122 - accuracy: 0.14 - ETA: 7s - loss: 3.3128 - accuracy: 0.14 - ETA: 6s - loss: 3.3129 - accuracy: 0.14 - ETA: 5s - loss: 3.3131 - accuracy: 0.14 - ETA: 5s - loss: 3.3133 - accuracy: 0.14 - ETA: 4s - loss: 3.3133 - accuracy: 0.14 - ETA: 3s - loss: 3.3135 - accuracy: 0.14 - ETA: 2s - loss: 3.3136 - accuracy: 0.14 - ETA: 2s - loss: 3.3136 - accuracy: 0.14 - ETA: 1s - loss: 3.3142 - accuracy: 0.14 - ETA: 0s - loss: 3.3140 - accuracy: 0.14 - ETA: 0s - loss: 3.3146 - accuracy: 0.14 - 254s 6ms/step - loss: 3.3145 - accuracy: 0.1433 - val_loss: 4.0034 - val_accuracy: 0.0211\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:58 - loss: 3.4532 - accuracy: 0.11 - ETA: 3:56 - loss: 3.4079 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3344 - accuracy: 0.15 - ETA: 3:52 - loss: 3.2936 - accuracy: 0.15 - ETA: 3:52 - loss: 3.2783 - accuracy: 0.15 - ETA: 3:50 - loss: 3.2622 - accuracy: 0.15 - ETA: 3:53 - loss: 3.2793 - accuracy: 0.15 - ETA: 3:53 - loss: 3.2785 - accuracy: 0.15 - ETA: 3:52 - loss: 3.2908 - accuracy: 0.15 - ETA: 3:52 - loss: 3.2986 - accuracy: 0.14 - ETA: 3:51 - loss: 3.2962 - accuracy: 0.15 - ETA: 3:51 - loss: 3.2893 - accuracy: 0.15 - ETA: 3:50 - loss: 3.2956 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3009 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3067 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3069 - accuracy: 0.14 - ETA: 3:46 - loss: 3.2996 - accuracy: 0.14 - ETA: 3:45 - loss: 3.2965 - accuracy: 0.14 - ETA: 3:45 - loss: 3.2909 - accuracy: 0.14 - ETA: 3:44 - loss: 3.2873 - accuracy: 0.15 - ETA: 3:43 - loss: 3.2941 - accuracy: 0.14 - ETA: 3:42 - loss: 3.2955 - accuracy: 0.14 - ETA: 3:41 - loss: 3.2884 - accuracy: 0.14 - ETA: 3:40 - loss: 3.2861 - accuracy: 0.15 - ETA: 3:40 - loss: 3.2940 - accuracy: 0.14 - ETA: 3:39 - loss: 3.2842 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2837 - accuracy: 0.15 - ETA: 3:37 - loss: 3.2912 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2914 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2913 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2965 - accuracy: 0.14 - ETA: 3:34 - loss: 3.2970 - accuracy: 0.14 - ETA: 3:34 - loss: 3.3016 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2981 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2997 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3004 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2983 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2976 - accuracy: 0.14 - ETA: 3:31 - loss: 3.2959 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2938 - accuracy: 0.14 - ETA: 3:30 - loss: 3.2896 - accuracy: 0.14 - ETA: 3:29 - loss: 3.2871 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2857 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2841 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2820 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2802 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2784 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2789 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2808 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2844 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2903 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2923 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2933 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2962 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2974 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2956 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2982 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2969 - accuracy: 0.14 - ETA: 3:17 - loss: 3.2980 - accuracy: 0.14 - ETA: 3:16 - loss: 3.2932 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2911 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2876 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2868 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2872 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2883 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2876 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2878 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2872 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2859 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2856 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2872 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2877 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2870 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2869 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2869 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2848 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2833 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2813 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2808 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2803 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2811 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2811 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2800 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2798 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2799 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2804 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2812 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2806 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2778 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2778 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2789 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2776 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2779 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2800 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2816 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2831 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2843 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2852 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2857 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2855 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2865 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2880 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2894 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2905 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2910 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2925 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2929 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2934 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2948 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2942 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2945 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2964 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2957 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2969 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2979 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2996 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2995 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2990 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2989 - accuracy: 0.14 - ETA: 2:32 - loss: 3.3007 - accuracy: 0.14 - ETA: 2:31 - loss: 3.3002 - accuracy: 0.14 - ETA: 2:30 - loss: 3.3007 - accuracy: 0.14 - ETA: 2:29 - loss: 3.3019 - accuracy: 0.14 - ETA: 2:29 - loss: 3.3023 - accuracy: 0.14 - ETA: 2:28 - loss: 3.3029 - accuracy: 0.14 - ETA: 2:28 - loss: 3.3023 - accuracy: 0.14 - ETA: 2:27 - loss: 3.3016 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3026 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3046 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3049 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3058 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3061 - accuracy: 0.14 - ETA: 2:23 - loss: 3.3057 - accuracy: 0.14 - ETA: 2:23 - loss: 3.3065 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3075 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3082 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3083 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3107 - accuracy: 0.14 - ETA: 2:19 - loss: 3.3095 - accuracy: 0.14 - ETA: 2:18 - loss: 3.3095 - accuracy: 0.14 - ETA: 2:18 - loss: 3.3108 - accuracy: 0.14 - ETA: 2:17 - loss: 3.3105 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3113 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3104 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3109 - accuracy: 0.14 - ETA: 2:14 - loss: 3.3114 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3109 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3118 - accuracy: 0.14 - ETA: 2:12 - loss: 3.3110 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3106 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3113 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3123 - accuracy: 0.14 - ETA: 2:09 - loss: 3.3126 - accuracy: 0.14 - ETA: 2:08 - loss: 3.3137 - accuracy: 0.14 - ETA: 2:08 - loss: 3.3136 - accuracy: 0.14 - ETA: 2:07 - loss: 3.3133 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3129 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3131 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3152 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3154 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3155 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3158 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3165 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3160 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3168 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3173 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3176 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3173 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3172 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3171 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3171 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3165 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3164 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3160 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3155 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3165 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3170 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3165 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3167 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3164 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3171 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3179 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3191 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3199 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3199 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3204 - accuracy: 0.1460"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:44 - loss: 3.3201 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3200 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3201 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3203 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3203 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3201 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3202 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3202 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3209 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3213 - accuracy: 0.14 - ETA: 1:37 - loss: 3.3206 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3213 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3217 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3211 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3218 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3222 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3215 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3213 - accuracy: 0.14 - ETA: 1:31 - loss: 3.3218 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3214 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3221 - accuracy: 0.14 - ETA: 1:29 - loss: 3.3237 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3242 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3283 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3287 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3283 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3279 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3280 - accuracy: 0.14 - ETA: 1:24 - loss: 3.3285 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3291 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3289 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3291 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3291 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3292 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3290 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3296 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3295 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3296 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3296 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3298 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3309 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3310 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3311 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3308 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3314 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3308 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3307 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3310 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3311 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3323 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3310 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3307 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3311 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3317 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3310 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3309 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3307 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3306 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3306 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3300 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3301 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3308 - accuracy: 0.14 - ETA: 59s - loss: 3.3307 - accuracy: 0.1439 - ETA: 58s - loss: 3.3307 - accuracy: 0.143 - ETA: 58s - loss: 3.3308 - accuracy: 0.143 - ETA: 57s - loss: 3.3310 - accuracy: 0.143 - ETA: 56s - loss: 3.3303 - accuracy: 0.143 - ETA: 56s - loss: 3.3304 - accuracy: 0.143 - ETA: 55s - loss: 3.3301 - accuracy: 0.143 - ETA: 54s - loss: 3.3305 - accuracy: 0.143 - ETA: 53s - loss: 3.3305 - accuracy: 0.143 - ETA: 53s - loss: 3.3304 - accuracy: 0.143 - ETA: 52s - loss: 3.3307 - accuracy: 0.143 - ETA: 51s - loss: 3.3309 - accuracy: 0.143 - ETA: 50s - loss: 3.3302 - accuracy: 0.143 - ETA: 50s - loss: 3.3301 - accuracy: 0.143 - ETA: 49s - loss: 3.3298 - accuracy: 0.144 - ETA: 48s - loss: 3.3293 - accuracy: 0.144 - ETA: 48s - loss: 3.3292 - accuracy: 0.144 - ETA: 47s - loss: 3.3286 - accuracy: 0.144 - ETA: 46s - loss: 3.3289 - accuracy: 0.144 - ETA: 45s - loss: 3.3286 - accuracy: 0.144 - ETA: 45s - loss: 3.3295 - accuracy: 0.144 - ETA: 44s - loss: 3.3281 - accuracy: 0.144 - ETA: 43s - loss: 3.3280 - accuracy: 0.144 - ETA: 42s - loss: 3.3273 - accuracy: 0.144 - ETA: 42s - loss: 3.3269 - accuracy: 0.144 - ETA: 41s - loss: 3.3271 - accuracy: 0.144 - ETA: 40s - loss: 3.3262 - accuracy: 0.145 - ETA: 40s - loss: 3.3260 - accuracy: 0.145 - ETA: 39s - loss: 3.3250 - accuracy: 0.145 - ETA: 38s - loss: 3.3252 - accuracy: 0.145 - ETA: 37s - loss: 3.3247 - accuracy: 0.145 - ETA: 37s - loss: 3.3250 - accuracy: 0.145 - ETA: 36s - loss: 3.3250 - accuracy: 0.145 - ETA: 35s - loss: 3.3247 - accuracy: 0.145 - ETA: 34s - loss: 3.3244 - accuracy: 0.144 - ETA: 34s - loss: 3.3244 - accuracy: 0.144 - ETA: 33s - loss: 3.3242 - accuracy: 0.144 - ETA: 32s - loss: 3.3237 - accuracy: 0.144 - ETA: 32s - loss: 3.3231 - accuracy: 0.145 - ETA: 31s - loss: 3.3229 - accuracy: 0.145 - ETA: 30s - loss: 3.3222 - accuracy: 0.145 - ETA: 29s - loss: 3.3224 - accuracy: 0.145 - ETA: 29s - loss: 3.3222 - accuracy: 0.145 - ETA: 28s - loss: 3.3223 - accuracy: 0.145 - ETA: 27s - loss: 3.3226 - accuracy: 0.144 - ETA: 26s - loss: 3.3216 - accuracy: 0.145 - ETA: 26s - loss: 3.3214 - accuracy: 0.145 - ETA: 25s - loss: 3.3209 - accuracy: 0.145 - ETA: 24s - loss: 3.3202 - accuracy: 0.145 - ETA: 24s - loss: 3.3199 - accuracy: 0.145 - ETA: 23s - loss: 3.3193 - accuracy: 0.145 - ETA: 22s - loss: 3.3192 - accuracy: 0.145 - ETA: 21s - loss: 3.3194 - accuracy: 0.145 - ETA: 21s - loss: 3.3193 - accuracy: 0.145 - ETA: 20s - loss: 3.3192 - accuracy: 0.145 - ETA: 19s - loss: 3.3187 - accuracy: 0.145 - ETA: 18s - loss: 3.3190 - accuracy: 0.145 - ETA: 18s - loss: 3.3185 - accuracy: 0.145 - ETA: 17s - loss: 3.3186 - accuracy: 0.145 - ETA: 16s - loss: 3.3183 - accuracy: 0.145 - ETA: 16s - loss: 3.3185 - accuracy: 0.145 - ETA: 15s - loss: 3.3187 - accuracy: 0.145 - ETA: 14s - loss: 3.3189 - accuracy: 0.145 - ETA: 13s - loss: 3.3190 - accuracy: 0.145 - ETA: 13s - loss: 3.3196 - accuracy: 0.145 - ETA: 12s - loss: 3.3194 - accuracy: 0.145 - ETA: 11s - loss: 3.3191 - accuracy: 0.145 - ETA: 10s - loss: 3.3186 - accuracy: 0.145 - ETA: 10s - loss: 3.3187 - accuracy: 0.145 - ETA: 9s - loss: 3.3178 - accuracy: 0.145 - ETA: 8s - loss: 3.3180 - accuracy: 0.14 - ETA: 8s - loss: 3.3182 - accuracy: 0.14 - ETA: 7s - loss: 3.3181 - accuracy: 0.14 - ETA: 6s - loss: 3.3184 - accuracy: 0.14 - ETA: 5s - loss: 3.3187 - accuracy: 0.14 - ETA: 5s - loss: 3.3186 - accuracy: 0.14 - ETA: 4s - loss: 3.3183 - accuracy: 0.14 - ETA: 3s - loss: 3.3181 - accuracy: 0.14 - ETA: 2s - loss: 3.3185 - accuracy: 0.14 - ETA: 2s - loss: 3.3187 - accuracy: 0.14 - ETA: 1s - loss: 3.3187 - accuracy: 0.14 - ETA: 0s - loss: 3.3186 - accuracy: 0.14 - ETA: 0s - loss: 3.3182 - accuracy: 0.14 - 258s 6ms/step - loss: 3.3182 - accuracy: 0.1453 - val_loss: 4.0674 - val_accuracy: 0.0298\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:50 - loss: 3.3963 - accuracy: 0.16 - ETA: 4:00 - loss: 3.3458 - accuracy: 0.14 - ETA: 3:58 - loss: 3.3765 - accuracy: 0.13 - ETA: 4:00 - loss: 3.3705 - accuracy: 0.14 - ETA: 4:02 - loss: 3.3669 - accuracy: 0.13 - ETA: 4:00 - loss: 3.3435 - accuracy: 0.14 - ETA: 3:59 - loss: 3.3481 - accuracy: 0.13 - ETA: 3:58 - loss: 3.3458 - accuracy: 0.13 - ETA: 3:57 - loss: 3.3348 - accuracy: 0.14 - ETA: 3:56 - loss: 3.3257 - accuracy: 0.14 - ETA: 3:54 - loss: 3.3322 - accuracy: 0.14 - ETA: 3:53 - loss: 3.3418 - accuracy: 0.14 - ETA: 3:52 - loss: 3.3397 - accuracy: 0.14 - ETA: 3:51 - loss: 3.3311 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3312 - accuracy: 0.14 - ETA: 3:51 - loss: 3.3272 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3229 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3276 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3313 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3188 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3216 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3228 - accuracy: 0.14 - ETA: 3:45 - loss: 3.3158 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3102 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3005 - accuracy: 0.15 - ETA: 3:43 - loss: 3.2991 - accuracy: 0.14 - ETA: 3:43 - loss: 3.2977 - accuracy: 0.14 - ETA: 3:42 - loss: 3.2959 - accuracy: 0.14 - ETA: 3:41 - loss: 3.2924 - accuracy: 0.14 - ETA: 3:39 - loss: 3.2914 - accuracy: 0.14 - ETA: 3:39 - loss: 3.2907 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2920 - accuracy: 0.14 - ETA: 3:37 - loss: 3.2906 - accuracy: 0.14 - ETA: 3:36 - loss: 3.2934 - accuracy: 0.14 - ETA: 3:35 - loss: 3.2988 - accuracy: 0.14 - ETA: 3:34 - loss: 3.3036 - accuracy: 0.14 - ETA: 3:33 - loss: 3.2997 - accuracy: 0.14 - ETA: 3:32 - loss: 3.2981 - accuracy: 0.14 - ETA: 3:31 - loss: 3.3038 - accuracy: 0.14 - ETA: 3:30 - loss: 3.3109 - accuracy: 0.14 - ETA: 3:30 - loss: 3.3129 - accuracy: 0.14 - ETA: 3:29 - loss: 3.3091 - accuracy: 0.14 - ETA: 3:29 - loss: 3.3054 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2992 - accuracy: 0.14 - ETA: 3:28 - loss: 3.2970 - accuracy: 0.14 - ETA: 3:27 - loss: 3.2953 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2939 - accuracy: 0.14 - ETA: 3:26 - loss: 3.2920 - accuracy: 0.14 - ETA: 3:25 - loss: 3.2875 - accuracy: 0.14 - ETA: 3:24 - loss: 3.2912 - accuracy: 0.14 - ETA: 3:23 - loss: 3.2874 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2865 - accuracy: 0.14 - ETA: 3:22 - loss: 3.2843 - accuracy: 0.14 - ETA: 3:21 - loss: 3.2811 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2789 - accuracy: 0.14 - ETA: 3:20 - loss: 3.2796 - accuracy: 0.14 - ETA: 3:19 - loss: 3.2776 - accuracy: 0.14 - ETA: 3:18 - loss: 3.2767 - accuracy: 0.15 - ETA: 3:18 - loss: 3.2761 - accuracy: 0.15 - ETA: 3:17 - loss: 3.2761 - accuracy: 0.15 - ETA: 3:16 - loss: 3.2749 - accuracy: 0.15 - ETA: 3:16 - loss: 3.2726 - accuracy: 0.15 - ETA: 3:15 - loss: 3.2723 - accuracy: 0.15 - ETA: 3:14 - loss: 3.2723 - accuracy: 0.15 - ETA: 3:13 - loss: 3.2688 - accuracy: 0.15 - ETA: 3:12 - loss: 3.2656 - accuracy: 0.15 - ETA: 3:12 - loss: 3.2650 - accuracy: 0.15 - ETA: 3:11 - loss: 3.2640 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2643 - accuracy: 0.15 - ETA: 3:10 - loss: 3.2647 - accuracy: 0.15 - ETA: 3:09 - loss: 3.2655 - accuracy: 0.15 - ETA: 3:08 - loss: 3.2633 - accuracy: 0.15 - ETA: 3:07 - loss: 3.2623 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2615 - accuracy: 0.15 - ETA: 3:06 - loss: 3.2620 - accuracy: 0.15 - ETA: 3:05 - loss: 3.2633 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2643 - accuracy: 0.15 - ETA: 3:04 - loss: 3.2643 - accuracy: 0.15 - ETA: 3:03 - loss: 3.2657 - accuracy: 0.15 - ETA: 3:02 - loss: 3.2651 - accuracy: 0.15 - ETA: 3:02 - loss: 3.2639 - accuracy: 0.15 - ETA: 3:01 - loss: 3.2621 - accuracy: 0.15 - ETA: 3:00 - loss: 3.2608 - accuracy: 0.15 - ETA: 2:59 - loss: 3.2629 - accuracy: 0.15 - ETA: 2:59 - loss: 3.2639 - accuracy: 0.15 - ETA: 2:58 - loss: 3.2657 - accuracy: 0.15 - ETA: 2:57 - loss: 3.2656 - accuracy: 0.15 - ETA: 2:57 - loss: 3.2664 - accuracy: 0.15 - ETA: 2:56 - loss: 3.2648 - accuracy: 0.15 - ETA: 2:55 - loss: 3.2640 - accuracy: 0.15 - ETA: 2:54 - loss: 3.2640 - accuracy: 0.15 - ETA: 2:54 - loss: 3.2647 - accuracy: 0.15 - ETA: 2:53 - loss: 3.2639 - accuracy: 0.15 - ETA: 2:52 - loss: 3.2638 - accuracy: 0.15 - ETA: 2:51 - loss: 3.2630 - accuracy: 0.15 - ETA: 2:51 - loss: 3.2635 - accuracy: 0.15 - ETA: 2:50 - loss: 3.2642 - accuracy: 0.15 - ETA: 2:49 - loss: 3.2647 - accuracy: 0.15 - ETA: 2:49 - loss: 3.2655 - accuracy: 0.15 - ETA: 2:48 - loss: 3.2648 - accuracy: 0.15 - ETA: 2:47 - loss: 3.2636 - accuracy: 0.15 - ETA: 2:46 - loss: 3.2644 - accuracy: 0.15 - ETA: 2:46 - loss: 3.2650 - accuracy: 0.15 - ETA: 2:45 - loss: 3.2633 - accuracy: 0.15 - ETA: 2:44 - loss: 3.2631 - accuracy: 0.15 - ETA: 2:44 - loss: 3.2618 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2609 - accuracy: 0.15 - ETA: 2:42 - loss: 3.2604 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2593 - accuracy: 0.15 - ETA: 2:41 - loss: 3.2604 - accuracy: 0.15 - ETA: 2:40 - loss: 3.2603 - accuracy: 0.15 - ETA: 2:39 - loss: 3.2615 - accuracy: 0.15 - ETA: 2:38 - loss: 3.2605 - accuracy: 0.15 - ETA: 2:38 - loss: 3.2607 - accuracy: 0.15 - ETA: 2:37 - loss: 3.2597 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2593 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2595 - accuracy: 0.15 - ETA: 2:35 - loss: 3.2584 - accuracy: 0.15 - ETA: 2:34 - loss: 3.2573 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2564 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2559 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2567 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2587 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2580 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2567 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2567 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2577 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2570 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2571 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2564 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2568 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2574 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2595 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2605 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2601 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2603 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2603 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2600 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2594 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2600 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2584 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2577 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2568 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2566 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2570 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2580 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2577 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2573 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2569 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2574 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2569 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2567 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2588 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2590 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2580 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2582 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2586 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2600 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2609 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2620 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2618 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2627 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2637 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2635 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2638 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2646 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2657 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2658 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2663 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2668 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2676 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2689 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2695 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2707 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2702 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2690 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2687 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2687 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2683 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2689 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2697 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2704 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2706 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2704 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2714 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2726 - accuracy: 0.1475"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:45 - loss: 3.2739 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2744 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2750 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2755 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2761 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2763 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2767 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2780 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2791 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2796 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2808 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2819 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2836 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2843 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2850 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2869 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2879 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2893 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2901 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2911 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2924 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2938 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2949 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2957 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2961 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2970 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2980 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2985 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2995 - accuracy: 0.14 - ETA: 1:24 - loss: 3.3010 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3015 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3029 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3041 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3049 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3052 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3061 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3071 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3080 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3096 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3114 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3125 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3136 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3145 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3159 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3176 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3181 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3187 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3196 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3199 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3211 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3214 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3224 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3233 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3239 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3247 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3257 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3260 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3268 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3274 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3282 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3290 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3294 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3303 - accuracy: 0.13 - ETA: 59s - loss: 3.3310 - accuracy: 0.1393 - ETA: 58s - loss: 3.3313 - accuracy: 0.139 - ETA: 57s - loss: 3.3315 - accuracy: 0.139 - ETA: 57s - loss: 3.3320 - accuracy: 0.139 - ETA: 56s - loss: 3.3323 - accuracy: 0.139 - ETA: 55s - loss: 3.3334 - accuracy: 0.139 - ETA: 54s - loss: 3.3336 - accuracy: 0.139 - ETA: 54s - loss: 3.3346 - accuracy: 0.138 - ETA: 53s - loss: 3.3349 - accuracy: 0.138 - ETA: 52s - loss: 3.3351 - accuracy: 0.138 - ETA: 51s - loss: 3.3355 - accuracy: 0.138 - ETA: 51s - loss: 3.3359 - accuracy: 0.138 - ETA: 50s - loss: 3.3366 - accuracy: 0.138 - ETA: 49s - loss: 3.3375 - accuracy: 0.138 - ETA: 49s - loss: 3.3387 - accuracy: 0.138 - ETA: 48s - loss: 3.3391 - accuracy: 0.138 - ETA: 47s - loss: 3.3393 - accuracy: 0.138 - ETA: 46s - loss: 3.3400 - accuracy: 0.138 - ETA: 46s - loss: 3.3407 - accuracy: 0.138 - ETA: 45s - loss: 3.3408 - accuracy: 0.138 - ETA: 44s - loss: 3.3420 - accuracy: 0.138 - ETA: 43s - loss: 3.3429 - accuracy: 0.138 - ETA: 43s - loss: 3.3437 - accuracy: 0.137 - ETA: 42s - loss: 3.3443 - accuracy: 0.137 - ETA: 41s - loss: 3.3453 - accuracy: 0.137 - ETA: 40s - loss: 3.3456 - accuracy: 0.137 - ETA: 40s - loss: 3.3456 - accuracy: 0.137 - ETA: 39s - loss: 3.3456 - accuracy: 0.137 - ETA: 38s - loss: 3.3453 - accuracy: 0.137 - ETA: 38s - loss: 3.3457 - accuracy: 0.137 - ETA: 37s - loss: 3.3462 - accuracy: 0.137 - ETA: 36s - loss: 3.3464 - accuracy: 0.137 - ETA: 35s - loss: 3.3468 - accuracy: 0.137 - ETA: 35s - loss: 3.3475 - accuracy: 0.137 - ETA: 34s - loss: 3.3478 - accuracy: 0.137 - ETA: 33s - loss: 3.3481 - accuracy: 0.137 - ETA: 32s - loss: 3.3485 - accuracy: 0.137 - ETA: 32s - loss: 3.3488 - accuracy: 0.137 - ETA: 31s - loss: 3.3489 - accuracy: 0.137 - ETA: 30s - loss: 3.3494 - accuracy: 0.137 - ETA: 29s - loss: 3.3494 - accuracy: 0.137 - ETA: 29s - loss: 3.3498 - accuracy: 0.137 - ETA: 28s - loss: 3.3500 - accuracy: 0.137 - ETA: 27s - loss: 3.3507 - accuracy: 0.136 - ETA: 27s - loss: 3.3514 - accuracy: 0.136 - ETA: 26s - loss: 3.3520 - accuracy: 0.136 - ETA: 25s - loss: 3.3527 - accuracy: 0.136 - ETA: 24s - loss: 3.3526 - accuracy: 0.136 - ETA: 24s - loss: 3.3534 - accuracy: 0.136 - ETA: 23s - loss: 3.3539 - accuracy: 0.136 - ETA: 22s - loss: 3.3542 - accuracy: 0.136 - ETA: 21s - loss: 3.3543 - accuracy: 0.136 - ETA: 21s - loss: 3.3547 - accuracy: 0.136 - ETA: 20s - loss: 3.3560 - accuracy: 0.136 - ETA: 19s - loss: 3.3565 - accuracy: 0.136 - ETA: 19s - loss: 3.3568 - accuracy: 0.136 - ETA: 18s - loss: 3.3571 - accuracy: 0.136 - ETA: 17s - loss: 3.3571 - accuracy: 0.136 - ETA: 16s - loss: 3.3576 - accuracy: 0.136 - ETA: 16s - loss: 3.3582 - accuracy: 0.136 - ETA: 15s - loss: 3.3585 - accuracy: 0.136 - ETA: 14s - loss: 3.3589 - accuracy: 0.136 - ETA: 13s - loss: 3.3592 - accuracy: 0.136 - ETA: 13s - loss: 3.3595 - accuracy: 0.136 - ETA: 12s - loss: 3.3598 - accuracy: 0.136 - ETA: 11s - loss: 3.3602 - accuracy: 0.136 - ETA: 10s - loss: 3.3607 - accuracy: 0.136 - ETA: 10s - loss: 3.3611 - accuracy: 0.136 - ETA: 9s - loss: 3.3623 - accuracy: 0.135 - ETA: 8s - loss: 3.3625 - accuracy: 0.13 - ETA: 8s - loss: 3.3633 - accuracy: 0.13 - ETA: 7s - loss: 3.3655 - accuracy: 0.13 - ETA: 6s - loss: 3.3672 - accuracy: 0.13 - ETA: 5s - loss: 3.3683 - accuracy: 0.13 - ETA: 5s - loss: 3.3695 - accuracy: 0.13 - ETA: 4s - loss: 3.3699 - accuracy: 0.13 - ETA: 3s - loss: 3.3702 - accuracy: 0.13 - ETA: 2s - loss: 3.3708 - accuracy: 0.13 - ETA: 2s - loss: 3.3713 - accuracy: 0.13 - ETA: 1s - loss: 3.3719 - accuracy: 0.13 - ETA: 0s - loss: 3.3724 - accuracy: 0.13 - ETA: 0s - loss: 3.3728 - accuracy: 0.13 - 259s 6ms/step - loss: 3.3728 - accuracy: 0.1344 - val_loss: 3.9785 - val_accuracy: 0.0148\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:43 - loss: 3.5622 - accuracy: 0.10 - ETA: 3:50 - loss: 3.5552 - accuracy: 0.10 - ETA: 3:53 - loss: 3.5186 - accuracy: 0.11 - ETA: 3:52 - loss: 3.5054 - accuracy: 0.11 - ETA: 3:51 - loss: 3.5256 - accuracy: 0.11 - ETA: 3:51 - loss: 3.5242 - accuracy: 0.11 - ETA: 3:50 - loss: 3.5200 - accuracy: 0.11 - ETA: 3:49 - loss: 3.5142 - accuracy: 0.11 - ETA: 3:48 - loss: 3.5087 - accuracy: 0.11 - ETA: 3:48 - loss: 3.5167 - accuracy: 0.10 - ETA: 3:46 - loss: 3.5122 - accuracy: 0.10 - ETA: 3:46 - loss: 3.5091 - accuracy: 0.11 - ETA: 3:45 - loss: 3.4998 - accuracy: 0.11 - ETA: 3:44 - loss: 3.5025 - accuracy: 0.11 - ETA: 3:44 - loss: 3.4989 - accuracy: 0.11 - ETA: 3:43 - loss: 3.5088 - accuracy: 0.11 - ETA: 3:43 - loss: 3.5067 - accuracy: 0.11 - ETA: 3:42 - loss: 3.5074 - accuracy: 0.11 - ETA: 3:41 - loss: 3.5065 - accuracy: 0.11 - ETA: 3:41 - loss: 3.5127 - accuracy: 0.11 - ETA: 3:41 - loss: 3.5119 - accuracy: 0.11 - ETA: 3:40 - loss: 3.5158 - accuracy: 0.11 - ETA: 3:40 - loss: 3.5149 - accuracy: 0.11 - ETA: 3:40 - loss: 3.5174 - accuracy: 0.11 - ETA: 3:39 - loss: 3.5104 - accuracy: 0.11 - ETA: 3:38 - loss: 3.5148 - accuracy: 0.11 - ETA: 3:38 - loss: 3.5136 - accuracy: 0.11 - ETA: 3:37 - loss: 3.5168 - accuracy: 0.11 - ETA: 3:36 - loss: 3.5134 - accuracy: 0.11 - ETA: 3:35 - loss: 3.5084 - accuracy: 0.11 - ETA: 3:34 - loss: 3.5093 - accuracy: 0.11 - ETA: 3:33 - loss: 3.5071 - accuracy: 0.11 - ETA: 3:33 - loss: 3.5072 - accuracy: 0.11 - ETA: 3:32 - loss: 3.4986 - accuracy: 0.11 - ETA: 3:32 - loss: 3.4991 - accuracy: 0.11 - ETA: 3:31 - loss: 3.4983 - accuracy: 0.11 - ETA: 3:31 - loss: 3.4990 - accuracy: 0.11 - ETA: 3:30 - loss: 3.4982 - accuracy: 0.11 - ETA: 3:29 - loss: 3.5002 - accuracy: 0.11 - ETA: 3:28 - loss: 3.4981 - accuracy: 0.11 - ETA: 3:28 - loss: 3.4990 - accuracy: 0.11 - ETA: 3:27 - loss: 3.5005 - accuracy: 0.11 - ETA: 3:26 - loss: 3.5027 - accuracy: 0.11 - ETA: 3:25 - loss: 3.5011 - accuracy: 0.11 - ETA: 3:24 - loss: 3.5023 - accuracy: 0.11 - ETA: 3:24 - loss: 3.5019 - accuracy: 0.11 - ETA: 3:24 - loss: 3.5022 - accuracy: 0.11 - ETA: 3:24 - loss: 3.5048 - accuracy: 0.11 - ETA: 3:24 - loss: 3.5153 - accuracy: 0.11 - ETA: 3:23 - loss: 3.5137 - accuracy: 0.11 - ETA: 3:23 - loss: 3.5128 - accuracy: 0.11 - ETA: 3:22 - loss: 3.5233 - accuracy: 0.11 - ETA: 3:21 - loss: 3.5199 - accuracy: 0.11 - ETA: 3:20 - loss: 3.5188 - accuracy: 0.11 - ETA: 3:20 - loss: 3.5175 - accuracy: 0.11 - ETA: 3:19 - loss: 3.5169 - accuracy: 0.11 - ETA: 3:18 - loss: 3.5162 - accuracy: 0.11 - ETA: 3:17 - loss: 3.5134 - accuracy: 0.11 - ETA: 3:17 - loss: 3.5119 - accuracy: 0.11 - ETA: 3:16 - loss: 3.5136 - accuracy: 0.11 - ETA: 3:15 - loss: 3.5131 - accuracy: 0.11 - ETA: 3:15 - loss: 3.5126 - accuracy: 0.11 - ETA: 3:14 - loss: 3.5132 - accuracy: 0.11 - ETA: 3:13 - loss: 3.5123 - accuracy: 0.11 - ETA: 3:12 - loss: 3.5137 - accuracy: 0.11 - ETA: 3:11 - loss: 3.5123 - accuracy: 0.11 - ETA: 3:11 - loss: 3.5108 - accuracy: 0.11 - ETA: 3:10 - loss: 3.5112 - accuracy: 0.11 - ETA: 3:09 - loss: 3.5102 - accuracy: 0.11 - ETA: 3:08 - loss: 3.5113 - accuracy: 0.11 - ETA: 3:08 - loss: 3.5105 - accuracy: 0.11 - ETA: 3:07 - loss: 3.5072 - accuracy: 0.11 - ETA: 3:06 - loss: 3.5062 - accuracy: 0.11 - ETA: 3:06 - loss: 3.5043 - accuracy: 0.11 - ETA: 3:05 - loss: 3.5038 - accuracy: 0.11 - ETA: 3:04 - loss: 3.5025 - accuracy: 0.11 - ETA: 3:03 - loss: 3.5014 - accuracy: 0.11 - ETA: 3:03 - loss: 3.5010 - accuracy: 0.11 - ETA: 3:02 - loss: 3.5001 - accuracy: 0.11 - ETA: 3:01 - loss: 3.5023 - accuracy: 0.11 - ETA: 3:00 - loss: 3.5030 - accuracy: 0.11 - ETA: 3:00 - loss: 3.5038 - accuracy: 0.11 - ETA: 2:59 - loss: 3.5023 - accuracy: 0.11 - ETA: 2:58 - loss: 3.5039 - accuracy: 0.11 - ETA: 2:57 - loss: 3.5022 - accuracy: 0.11 - ETA: 2:57 - loss: 3.5034 - accuracy: 0.11 - ETA: 2:56 - loss: 3.5012 - accuracy: 0.11 - ETA: 2:55 - loss: 3.5025 - accuracy: 0.11 - ETA: 2:54 - loss: 3.5011 - accuracy: 0.11 - ETA: 2:54 - loss: 3.4996 - accuracy: 0.11 - ETA: 2:53 - loss: 3.4987 - accuracy: 0.11 - ETA: 2:52 - loss: 3.4984 - accuracy: 0.11 - ETA: 2:51 - loss: 3.4985 - accuracy: 0.11 - ETA: 2:51 - loss: 3.4993 - accuracy: 0.11 - ETA: 2:50 - loss: 3.4993 - accuracy: 0.11 - ETA: 2:49 - loss: 3.4981 - accuracy: 0.11 - ETA: 2:48 - loss: 3.4981 - accuracy: 0.11 - ETA: 2:48 - loss: 3.4965 - accuracy: 0.11 - ETA: 2:47 - loss: 3.4957 - accuracy: 0.11 - ETA: 2:46 - loss: 3.4958 - accuracy: 0.11 - ETA: 2:45 - loss: 3.4963 - accuracy: 0.11 - ETA: 2:45 - loss: 3.4951 - accuracy: 0.11 - ETA: 2:44 - loss: 3.4945 - accuracy: 0.11 - ETA: 2:43 - loss: 3.4935 - accuracy: 0.11 - ETA: 2:43 - loss: 3.4937 - accuracy: 0.11 - ETA: 2:42 - loss: 3.4929 - accuracy: 0.11 - ETA: 2:41 - loss: 3.4909 - accuracy: 0.11 - ETA: 2:40 - loss: 3.4907 - accuracy: 0.11 - ETA: 2:40 - loss: 3.4903 - accuracy: 0.11 - ETA: 2:39 - loss: 3.4917 - accuracy: 0.11 - ETA: 2:38 - loss: 3.4907 - accuracy: 0.11 - ETA: 2:38 - loss: 3.4895 - accuracy: 0.11 - ETA: 2:37 - loss: 3.4895 - accuracy: 0.11 - ETA: 2:36 - loss: 3.4895 - accuracy: 0.11 - ETA: 2:36 - loss: 3.4888 - accuracy: 0.11 - ETA: 2:35 - loss: 3.4889 - accuracy: 0.11 - ETA: 2:34 - loss: 3.4889 - accuracy: 0.11 - ETA: 2:33 - loss: 3.4882 - accuracy: 0.11 - ETA: 2:33 - loss: 3.4894 - accuracy: 0.11 - ETA: 2:32 - loss: 3.4894 - accuracy: 0.11 - ETA: 2:31 - loss: 3.4891 - accuracy: 0.11 - ETA: 2:30 - loss: 3.4882 - accuracy: 0.11 - ETA: 2:30 - loss: 3.4887 - accuracy: 0.11 - ETA: 2:29 - loss: 3.4893 - accuracy: 0.11 - ETA: 2:28 - loss: 3.4896 - accuracy: 0.11 - ETA: 2:27 - loss: 3.4888 - accuracy: 0.11 - ETA: 2:27 - loss: 3.4867 - accuracy: 0.11 - ETA: 2:26 - loss: 3.4867 - accuracy: 0.11 - ETA: 2:25 - loss: 3.4854 - accuracy: 0.11 - ETA: 2:25 - loss: 3.4849 - accuracy: 0.11 - ETA: 2:24 - loss: 3.4848 - accuracy: 0.11 - ETA: 2:23 - loss: 3.4854 - accuracy: 0.11 - ETA: 2:23 - loss: 3.4859 - accuracy: 0.11 - ETA: 2:22 - loss: 3.4865 - accuracy: 0.11 - ETA: 2:21 - loss: 3.4864 - accuracy: 0.11 - ETA: 2:20 - loss: 3.4863 - accuracy: 0.11 - ETA: 2:20 - loss: 3.4853 - accuracy: 0.11 - ETA: 2:19 - loss: 3.4863 - accuracy: 0.11 - ETA: 2:18 - loss: 3.4861 - accuracy: 0.11 - ETA: 2:18 - loss: 3.4862 - accuracy: 0.11 - ETA: 2:17 - loss: 3.4851 - accuracy: 0.11 - ETA: 2:16 - loss: 3.4840 - accuracy: 0.11 - ETA: 2:16 - loss: 3.4842 - accuracy: 0.11 - ETA: 2:15 - loss: 3.4845 - accuracy: 0.11 - ETA: 2:14 - loss: 3.4829 - accuracy: 0.11 - ETA: 2:14 - loss: 3.4827 - accuracy: 0.11 - ETA: 2:13 - loss: 3.4829 - accuracy: 0.11 - ETA: 2:12 - loss: 3.4827 - accuracy: 0.11 - ETA: 2:11 - loss: 3.4810 - accuracy: 0.11 - ETA: 2:11 - loss: 3.4802 - accuracy: 0.11 - ETA: 2:10 - loss: 3.4802 - accuracy: 0.11 - ETA: 2:09 - loss: 3.4791 - accuracy: 0.11 - ETA: 2:08 - loss: 3.4796 - accuracy: 0.11 - ETA: 2:08 - loss: 3.4799 - accuracy: 0.11 - ETA: 2:07 - loss: 3.4792 - accuracy: 0.11 - ETA: 2:06 - loss: 3.4785 - accuracy: 0.11 - ETA: 2:05 - loss: 3.4793 - accuracy: 0.11 - ETA: 2:05 - loss: 3.4785 - accuracy: 0.11 - ETA: 2:04 - loss: 3.4784 - accuracy: 0.11 - ETA: 2:03 - loss: 3.4794 - accuracy: 0.11 - ETA: 2:02 - loss: 3.4789 - accuracy: 0.11 - ETA: 2:02 - loss: 3.4772 - accuracy: 0.11 - ETA: 2:01 - loss: 3.4770 - accuracy: 0.11 - ETA: 2:00 - loss: 3.4765 - accuracy: 0.11 - ETA: 2:00 - loss: 3.4766 - accuracy: 0.11 - ETA: 1:59 - loss: 3.4766 - accuracy: 0.11 - ETA: 1:58 - loss: 3.4764 - accuracy: 0.11 - ETA: 1:57 - loss: 3.4761 - accuracy: 0.11 - ETA: 1:57 - loss: 3.4760 - accuracy: 0.11 - ETA: 1:56 - loss: 3.4762 - accuracy: 0.11 - ETA: 1:55 - loss: 3.4765 - accuracy: 0.11 - ETA: 1:54 - loss: 3.4758 - accuracy: 0.11 - ETA: 1:54 - loss: 3.4749 - accuracy: 0.11 - ETA: 1:53 - loss: 3.4743 - accuracy: 0.11 - ETA: 1:52 - loss: 3.4749 - accuracy: 0.11 - ETA: 1:52 - loss: 3.4748 - accuracy: 0.11 - ETA: 1:51 - loss: 3.4741 - accuracy: 0.11 - ETA: 1:50 - loss: 3.4745 - accuracy: 0.11 - ETA: 1:49 - loss: 3.4750 - accuracy: 0.11 - ETA: 1:49 - loss: 3.4753 - accuracy: 0.11 - ETA: 1:48 - loss: 3.4757 - accuracy: 0.11 - ETA: 1:47 - loss: 3.4756 - accuracy: 0.11 - ETA: 1:46 - loss: 3.4757 - accuracy: 0.11 - ETA: 1:46 - loss: 3.4745 - accuracy: 0.11 - ETA: 1:45 - loss: 3.4743 - accuracy: 0.11 - ETA: 1:44 - loss: 3.4723 - accuracy: 0.1199"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:44 - loss: 3.4724 - accuracy: 0.11 - ETA: 1:43 - loss: 3.4725 - accuracy: 0.11 - ETA: 1:42 - loss: 3.4734 - accuracy: 0.11 - ETA: 1:41 - loss: 3.4729 - accuracy: 0.11 - ETA: 1:41 - loss: 3.4728 - accuracy: 0.11 - ETA: 1:40 - loss: 3.4725 - accuracy: 0.11 - ETA: 1:39 - loss: 3.4711 - accuracy: 0.11 - ETA: 1:38 - loss: 3.4701 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4700 - accuracy: 0.12 - ETA: 1:37 - loss: 3.4683 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4673 - accuracy: 0.12 - ETA: 1:36 - loss: 3.4673 - accuracy: 0.12 - ETA: 1:35 - loss: 3.4668 - accuracy: 0.12 - ETA: 1:34 - loss: 3.4662 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4656 - accuracy: 0.12 - ETA: 1:33 - loss: 3.4666 - accuracy: 0.12 - ETA: 1:32 - loss: 3.4658 - accuracy: 0.12 - ETA: 1:31 - loss: 3.4649 - accuracy: 0.12 - ETA: 1:31 - loss: 3.4652 - accuracy: 0.12 - ETA: 1:30 - loss: 3.4641 - accuracy: 0.12 - ETA: 1:29 - loss: 3.4641 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4636 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4630 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4636 - accuracy: 0.12 - ETA: 1:26 - loss: 3.4640 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4639 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4636 - accuracy: 0.12 - ETA: 1:24 - loss: 3.4641 - accuracy: 0.12 - ETA: 1:23 - loss: 3.4642 - accuracy: 0.12 - ETA: 1:23 - loss: 3.4635 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4617 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4617 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4611 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4610 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4612 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4604 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4600 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4596 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4593 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4583 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4577 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4569 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4562 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4554 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4556 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4549 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4545 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4541 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4537 - accuracy: 0.12 - ETA: 1:08 - loss: 3.4522 - accuracy: 0.12 - ETA: 1:08 - loss: 3.4517 - accuracy: 0.12 - ETA: 1:07 - loss: 3.4515 - accuracy: 0.12 - ETA: 1:06 - loss: 3.4507 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4503 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4502 - accuracy: 0.12 - ETA: 1:04 - loss: 3.4491 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4485 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4481 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4479 - accuracy: 0.12 - ETA: 1:01 - loss: 3.4471 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4469 - accuracy: 0.12 - ETA: 1:00 - loss: 3.4466 - accuracy: 0.12 - ETA: 59s - loss: 3.4461 - accuracy: 0.1243 - ETA: 58s - loss: 3.4469 - accuracy: 0.124 - ETA: 57s - loss: 3.4461 - accuracy: 0.124 - ETA: 57s - loss: 3.4461 - accuracy: 0.124 - ETA: 56s - loss: 3.4443 - accuracy: 0.124 - ETA: 55s - loss: 3.4435 - accuracy: 0.125 - ETA: 55s - loss: 3.4427 - accuracy: 0.125 - ETA: 54s - loss: 3.4420 - accuracy: 0.125 - ETA: 53s - loss: 3.4419 - accuracy: 0.125 - ETA: 52s - loss: 3.4411 - accuracy: 0.125 - ETA: 52s - loss: 3.4409 - accuracy: 0.125 - ETA: 51s - loss: 3.4403 - accuracy: 0.125 - ETA: 50s - loss: 3.4401 - accuracy: 0.125 - ETA: 49s - loss: 3.4396 - accuracy: 0.125 - ETA: 49s - loss: 3.4388 - accuracy: 0.125 - ETA: 48s - loss: 3.4387 - accuracy: 0.125 - ETA: 47s - loss: 3.4383 - accuracy: 0.126 - ETA: 47s - loss: 3.4375 - accuracy: 0.126 - ETA: 46s - loss: 3.4367 - accuracy: 0.126 - ETA: 45s - loss: 3.4355 - accuracy: 0.126 - ETA: 44s - loss: 3.4348 - accuracy: 0.126 - ETA: 44s - loss: 3.4345 - accuracy: 0.126 - ETA: 43s - loss: 3.4342 - accuracy: 0.126 - ETA: 42s - loss: 3.4337 - accuracy: 0.126 - ETA: 41s - loss: 3.4335 - accuracy: 0.126 - ETA: 41s - loss: 3.4334 - accuracy: 0.126 - ETA: 40s - loss: 3.4333 - accuracy: 0.126 - ETA: 39s - loss: 3.4333 - accuracy: 0.126 - ETA: 39s - loss: 3.4334 - accuracy: 0.126 - ETA: 38s - loss: 3.4328 - accuracy: 0.126 - ETA: 37s - loss: 3.4321 - accuracy: 0.127 - ETA: 36s - loss: 3.4315 - accuracy: 0.126 - ETA: 36s - loss: 3.4308 - accuracy: 0.127 - ETA: 35s - loss: 3.4306 - accuracy: 0.127 - ETA: 34s - loss: 3.4303 - accuracy: 0.127 - ETA: 33s - loss: 3.4295 - accuracy: 0.127 - ETA: 33s - loss: 3.4297 - accuracy: 0.127 - ETA: 32s - loss: 3.4290 - accuracy: 0.127 - ETA: 31s - loss: 3.4286 - accuracy: 0.127 - ETA: 31s - loss: 3.4283 - accuracy: 0.127 - ETA: 30s - loss: 3.4276 - accuracy: 0.127 - ETA: 29s - loss: 3.4275 - accuracy: 0.127 - ETA: 28s - loss: 3.4268 - accuracy: 0.127 - ETA: 28s - loss: 3.4265 - accuracy: 0.127 - ETA: 27s - loss: 3.4254 - accuracy: 0.128 - ETA: 26s - loss: 3.4253 - accuracy: 0.128 - ETA: 26s - loss: 3.4248 - accuracy: 0.128 - ETA: 25s - loss: 3.4240 - accuracy: 0.128 - ETA: 24s - loss: 3.4235 - accuracy: 0.128 - ETA: 23s - loss: 3.4226 - accuracy: 0.128 - ETA: 23s - loss: 3.4222 - accuracy: 0.128 - ETA: 22s - loss: 3.4213 - accuracy: 0.128 - ETA: 21s - loss: 3.4211 - accuracy: 0.128 - ETA: 20s - loss: 3.4206 - accuracy: 0.128 - ETA: 20s - loss: 3.4198 - accuracy: 0.128 - ETA: 19s - loss: 3.4195 - accuracy: 0.128 - ETA: 18s - loss: 3.4191 - accuracy: 0.128 - ETA: 18s - loss: 3.4189 - accuracy: 0.128 - ETA: 17s - loss: 3.4184 - accuracy: 0.129 - ETA: 16s - loss: 3.4177 - accuracy: 0.129 - ETA: 15s - loss: 3.4171 - accuracy: 0.129 - ETA: 15s - loss: 3.4169 - accuracy: 0.129 - ETA: 14s - loss: 3.4160 - accuracy: 0.129 - ETA: 13s - loss: 3.4158 - accuracy: 0.129 - ETA: 13s - loss: 3.4148 - accuracy: 0.129 - ETA: 12s - loss: 3.4142 - accuracy: 0.129 - ETA: 11s - loss: 3.4134 - accuracy: 0.129 - ETA: 10s - loss: 3.4123 - accuracy: 0.129 - ETA: 10s - loss: 3.4132 - accuracy: 0.129 - ETA: 9s - loss: 3.4131 - accuracy: 0.129 - ETA: 8s - loss: 3.4125 - accuracy: 0.12 - ETA: 7s - loss: 3.4121 - accuracy: 0.12 - ETA: 7s - loss: 3.4113 - accuracy: 0.13 - ETA: 6s - loss: 3.4110 - accuracy: 0.13 - ETA: 5s - loss: 3.4105 - accuracy: 0.13 - ETA: 5s - loss: 3.4105 - accuracy: 0.13 - ETA: 4s - loss: 3.4099 - accuracy: 0.13 - ETA: 3s - loss: 3.4102 - accuracy: 0.13 - ETA: 2s - loss: 3.4101 - accuracy: 0.13 - ETA: 2s - loss: 3.4100 - accuracy: 0.13 - ETA: 1s - loss: 3.4099 - accuracy: 0.13 - ETA: 0s - loss: 3.4096 - accuracy: 0.13 - ETA: 0s - loss: 3.4094 - accuracy: 0.13 - 256s 6ms/step - loss: 3.4093 - accuracy: 0.1304 - val_loss: 4.0165 - val_accuracy: 0.0256\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:00 - loss: 3.1862 - accuracy: 0.17 - ETA: 3:54 - loss: 3.2586 - accuracy: 0.16 - ETA: 3:53 - loss: 3.2451 - accuracy: 0.15 - ETA: 3:54 - loss: 3.2477 - accuracy: 0.15 - ETA: 3:53 - loss: 3.2721 - accuracy: 0.15 - ETA: 3:52 - loss: 3.2678 - accuracy: 0.15 - ETA: 3:51 - loss: 3.2535 - accuracy: 0.15 - ETA: 3:50 - loss: 3.2251 - accuracy: 0.15 - ETA: 3:48 - loss: 3.1983 - accuracy: 0.16 - ETA: 3:48 - loss: 3.2100 - accuracy: 0.16 - ETA: 3:47 - loss: 3.2146 - accuracy: 0.15 - ETA: 3:46 - loss: 3.2239 - accuracy: 0.15 - ETA: 3:45 - loss: 3.2086 - accuracy: 0.15 - ETA: 3:44 - loss: 3.2188 - accuracy: 0.15 - ETA: 3:44 - loss: 3.2322 - accuracy: 0.15 - ETA: 3:45 - loss: 3.2219 - accuracy: 0.15 - ETA: 3:44 - loss: 3.2235 - accuracy: 0.15 - ETA: 3:44 - loss: 3.2295 - accuracy: 0.15 - ETA: 3:43 - loss: 3.2259 - accuracy: 0.15 - ETA: 3:42 - loss: 3.2374 - accuracy: 0.15 - ETA: 3:41 - loss: 3.2404 - accuracy: 0.15 - ETA: 3:41 - loss: 3.2423 - accuracy: 0.15 - ETA: 3:40 - loss: 3.2514 - accuracy: 0.15 - ETA: 3:39 - loss: 3.2586 - accuracy: 0.15 - ETA: 3:39 - loss: 3.2546 - accuracy: 0.15 - ETA: 3:38 - loss: 3.2469 - accuracy: 0.15 - ETA: 3:37 - loss: 3.2409 - accuracy: 0.15 - ETA: 3:37 - loss: 3.2367 - accuracy: 0.15 - ETA: 3:36 - loss: 3.2360 - accuracy: 0.15 - ETA: 3:36 - loss: 3.2451 - accuracy: 0.15 - ETA: 3:35 - loss: 3.2493 - accuracy: 0.15 - ETA: 3:34 - loss: 3.2543 - accuracy: 0.15 - ETA: 3:33 - loss: 3.2545 - accuracy: 0.15 - ETA: 3:33 - loss: 3.2518 - accuracy: 0.15 - ETA: 3:32 - loss: 3.2493 - accuracy: 0.15 - ETA: 3:31 - loss: 3.2512 - accuracy: 0.15 - ETA: 3:31 - loss: 3.2476 - accuracy: 0.15 - ETA: 3:30 - loss: 3.2487 - accuracy: 0.15 - ETA: 3:29 - loss: 3.2496 - accuracy: 0.15 - ETA: 3:29 - loss: 3.2494 - accuracy: 0.15 - ETA: 3:28 - loss: 3.2515 - accuracy: 0.15 - ETA: 3:27 - loss: 3.2516 - accuracy: 0.15 - ETA: 3:26 - loss: 3.2569 - accuracy: 0.15 - ETA: 3:26 - loss: 3.2647 - accuracy: 0.15 - ETA: 3:25 - loss: 3.2646 - accuracy: 0.15 - ETA: 3:24 - loss: 3.2672 - accuracy: 0.15 - ETA: 3:24 - loss: 3.2648 - accuracy: 0.15 - ETA: 3:23 - loss: 3.2662 - accuracy: 0.15 - ETA: 3:22 - loss: 3.2643 - accuracy: 0.15 - ETA: 3:21 - loss: 3.2650 - accuracy: 0.15 - ETA: 3:21 - loss: 3.2640 - accuracy: 0.15 - ETA: 3:20 - loss: 3.2665 - accuracy: 0.15 - ETA: 3:19 - loss: 3.2673 - accuracy: 0.15 - ETA: 3:18 - loss: 3.2679 - accuracy: 0.15 - ETA: 3:18 - loss: 3.2674 - accuracy: 0.15 - ETA: 3:17 - loss: 3.2682 - accuracy: 0.15 - ETA: 3:17 - loss: 3.2716 - accuracy: 0.15 - ETA: 3:16 - loss: 3.2748 - accuracy: 0.15 - ETA: 3:16 - loss: 3.2775 - accuracy: 0.14 - ETA: 3:15 - loss: 3.2785 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2760 - accuracy: 0.14 - ETA: 3:14 - loss: 3.2774 - accuracy: 0.14 - ETA: 3:13 - loss: 3.2774 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2779 - accuracy: 0.14 - ETA: 3:12 - loss: 3.2792 - accuracy: 0.14 - ETA: 3:11 - loss: 3.2787 - accuracy: 0.14 - ETA: 3:10 - loss: 3.2799 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2833 - accuracy: 0.14 - ETA: 3:09 - loss: 3.2837 - accuracy: 0.14 - ETA: 3:08 - loss: 3.2826 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2811 - accuracy: 0.14 - ETA: 3:07 - loss: 3.2809 - accuracy: 0.14 - ETA: 3:06 - loss: 3.2807 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2821 - accuracy: 0.14 - ETA: 3:05 - loss: 3.2793 - accuracy: 0.14 - ETA: 3:04 - loss: 3.2795 - accuracy: 0.14 - ETA: 3:03 - loss: 3.2790 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2804 - accuracy: 0.14 - ETA: 3:02 - loss: 3.2790 - accuracy: 0.14 - ETA: 3:01 - loss: 3.2806 - accuracy: 0.14 - ETA: 3:00 - loss: 3.2808 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2801 - accuracy: 0.14 - ETA: 2:59 - loss: 3.2808 - accuracy: 0.14 - ETA: 2:58 - loss: 3.2800 - accuracy: 0.14 - ETA: 2:57 - loss: 3.2790 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2795 - accuracy: 0.14 - ETA: 2:56 - loss: 3.2784 - accuracy: 0.14 - ETA: 2:55 - loss: 3.2774 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2775 - accuracy: 0.14 - ETA: 2:54 - loss: 3.2776 - accuracy: 0.14 - ETA: 2:53 - loss: 3.2779 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2767 - accuracy: 0.14 - ETA: 2:52 - loss: 3.2768 - accuracy: 0.14 - ETA: 2:51 - loss: 3.2782 - accuracy: 0.14 - ETA: 2:50 - loss: 3.2800 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2792 - accuracy: 0.14 - ETA: 2:49 - loss: 3.2792 - accuracy: 0.14 - ETA: 2:48 - loss: 3.2796 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2793 - accuracy: 0.14 - ETA: 2:47 - loss: 3.2807 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2801 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2804 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2806 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2798 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2791 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2794 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2802 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2782 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2775 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2763 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2750 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2750 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2743 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2749 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2754 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2757 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2773 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2786 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2774 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2754 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2761 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2757 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2758 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2752 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2749 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2745 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2741 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2734 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2728 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2723 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2728 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2736 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2731 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2740 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2743 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2751 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2749 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2741 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2737 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2748 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2743 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2741 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2728 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2738 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2734 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2742 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2744 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2748 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2745 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2761 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2757 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2762 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2774 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2766 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2772 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2776 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2778 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2786 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2782 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2782 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2779 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2792 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2796 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2808 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2801 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2800 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2803 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2805 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2804 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2811 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2801 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2790 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2789 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2793 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2793 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2795 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2794 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2792 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2776 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2764 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2766 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2765 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2759 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2758 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2759 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2757 - accuracy: 0.1471"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:44 - loss: 3.2754 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2746 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2743 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2740 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2736 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2743 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2742 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2740 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2737 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2735 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2733 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2729 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2734 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2736 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2736 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2729 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2728 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2727 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2740 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2741 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2736 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2737 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2732 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2733 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2734 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2727 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2733 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2720 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2723 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2723 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2724 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2726 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2726 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2723 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2725 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2720 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2720 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2722 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2718 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2721 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2721 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2718 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2718 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2719 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2709 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2708 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2712 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2719 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2716 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2718 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2715 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2715 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2708 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2703 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2705 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2703 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2705 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2703 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2700 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2697 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2695 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2694 - accuracy: 0.14 - ETA: 59s - loss: 3.2695 - accuracy: 0.1492 - ETA: 59s - loss: 3.2702 - accuracy: 0.149 - ETA: 58s - loss: 3.2704 - accuracy: 0.149 - ETA: 57s - loss: 3.2704 - accuracy: 0.149 - ETA: 56s - loss: 3.2701 - accuracy: 0.149 - ETA: 56s - loss: 3.2699 - accuracy: 0.149 - ETA: 55s - loss: 3.2693 - accuracy: 0.149 - ETA: 54s - loss: 3.2696 - accuracy: 0.149 - ETA: 54s - loss: 3.2700 - accuracy: 0.149 - ETA: 53s - loss: 3.2699 - accuracy: 0.149 - ETA: 52s - loss: 3.2703 - accuracy: 0.149 - ETA: 51s - loss: 3.2700 - accuracy: 0.149 - ETA: 51s - loss: 3.2702 - accuracy: 0.149 - ETA: 50s - loss: 3.2706 - accuracy: 0.149 - ETA: 49s - loss: 3.2706 - accuracy: 0.149 - ETA: 48s - loss: 3.2700 - accuracy: 0.149 - ETA: 48s - loss: 3.2699 - accuracy: 0.149 - ETA: 47s - loss: 3.2693 - accuracy: 0.150 - ETA: 46s - loss: 3.2693 - accuracy: 0.149 - ETA: 46s - loss: 3.2687 - accuracy: 0.150 - ETA: 45s - loss: 3.2690 - accuracy: 0.149 - ETA: 44s - loss: 3.2696 - accuracy: 0.149 - ETA: 43s - loss: 3.2691 - accuracy: 0.149 - ETA: 43s - loss: 3.2692 - accuracy: 0.149 - ETA: 42s - loss: 3.2685 - accuracy: 0.149 - ETA: 41s - loss: 3.2685 - accuracy: 0.149 - ETA: 40s - loss: 3.2685 - accuracy: 0.149 - ETA: 40s - loss: 3.2688 - accuracy: 0.149 - ETA: 39s - loss: 3.2693 - accuracy: 0.149 - ETA: 38s - loss: 3.2699 - accuracy: 0.149 - ETA: 38s - loss: 3.2699 - accuracy: 0.149 - ETA: 37s - loss: 3.2702 - accuracy: 0.149 - ETA: 36s - loss: 3.2705 - accuracy: 0.149 - ETA: 35s - loss: 3.2698 - accuracy: 0.149 - ETA: 35s - loss: 3.2700 - accuracy: 0.149 - ETA: 34s - loss: 3.2702 - accuracy: 0.149 - ETA: 33s - loss: 3.2707 - accuracy: 0.149 - ETA: 33s - loss: 3.2713 - accuracy: 0.149 - ETA: 32s - loss: 3.2714 - accuracy: 0.149 - ETA: 31s - loss: 3.2720 - accuracy: 0.149 - ETA: 30s - loss: 3.2721 - accuracy: 0.149 - ETA: 30s - loss: 3.2721 - accuracy: 0.149 - ETA: 29s - loss: 3.2719 - accuracy: 0.149 - ETA: 28s - loss: 3.2724 - accuracy: 0.149 - ETA: 27s - loss: 3.2724 - accuracy: 0.149 - ETA: 27s - loss: 3.2737 - accuracy: 0.148 - ETA: 26s - loss: 3.2742 - accuracy: 0.148 - ETA: 25s - loss: 3.2744 - accuracy: 0.148 - ETA: 24s - loss: 3.2749 - accuracy: 0.148 - ETA: 24s - loss: 3.2751 - accuracy: 0.148 - ETA: 23s - loss: 3.2756 - accuracy: 0.148 - ETA: 22s - loss: 3.2762 - accuracy: 0.148 - ETA: 22s - loss: 3.2769 - accuracy: 0.148 - ETA: 21s - loss: 3.2771 - accuracy: 0.148 - ETA: 20s - loss: 3.2772 - accuracy: 0.148 - ETA: 19s - loss: 3.2774 - accuracy: 0.148 - ETA: 19s - loss: 3.2775 - accuracy: 0.148 - ETA: 18s - loss: 3.2778 - accuracy: 0.148 - ETA: 17s - loss: 3.2783 - accuracy: 0.148 - ETA: 16s - loss: 3.2788 - accuracy: 0.148 - ETA: 16s - loss: 3.2785 - accuracy: 0.148 - ETA: 15s - loss: 3.2781 - accuracy: 0.148 - ETA: 14s - loss: 3.2788 - accuracy: 0.148 - ETA: 13s - loss: 3.2797 - accuracy: 0.147 - ETA: 13s - loss: 3.2797 - accuracy: 0.147 - ETA: 12s - loss: 3.2796 - accuracy: 0.148 - ETA: 11s - loss: 3.2802 - accuracy: 0.148 - ETA: 11s - loss: 3.2805 - accuracy: 0.148 - ETA: 10s - loss: 3.2811 - accuracy: 0.148 - ETA: 9s - loss: 3.2820 - accuracy: 0.147 - ETA: 8s - loss: 3.2822 - accuracy: 0.14 - ETA: 8s - loss: 3.2821 - accuracy: 0.14 - ETA: 7s - loss: 3.2822 - accuracy: 0.14 - ETA: 6s - loss: 3.2819 - accuracy: 0.14 - ETA: 5s - loss: 3.2827 - accuracy: 0.14 - ETA: 5s - loss: 3.2825 - accuracy: 0.14 - ETA: 4s - loss: 3.2832 - accuracy: 0.14 - ETA: 3s - loss: 3.2830 - accuracy: 0.14 - ETA: 2s - loss: 3.2834 - accuracy: 0.14 - ETA: 2s - loss: 3.2836 - accuracy: 0.14 - ETA: 1s - loss: 3.2837 - accuracy: 0.14 - ETA: 0s - loss: 3.2844 - accuracy: 0.14 - ETA: 0s - loss: 3.2851 - accuracy: 0.14 - 262s 6ms/step - loss: 3.2852 - accuracy: 0.1473 - val_loss: 4.0524 - val_accuracy: 0.0208\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:20 - loss: 3.4264 - accuracy: 0.14 - ETA: 4:17 - loss: 3.4405 - accuracy: 0.14 - ETA: 4:10 - loss: 3.3994 - accuracy: 0.14 - ETA: 4:06 - loss: 3.4177 - accuracy: 0.13 - ETA: 4:04 - loss: 3.4322 - accuracy: 0.12 - ETA: 4:02 - loss: 3.4246 - accuracy: 0.12 - ETA: 4:03 - loss: 3.3943 - accuracy: 0.13 - ETA: 4:03 - loss: 3.3985 - accuracy: 0.12 - ETA: 4:01 - loss: 3.3800 - accuracy: 0.13 - ETA: 4:00 - loss: 3.3762 - accuracy: 0.13 - ETA: 3:59 - loss: 3.3664 - accuracy: 0.13 - ETA: 3:57 - loss: 3.3785 - accuracy: 0.13 - ETA: 3:56 - loss: 3.3682 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3681 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3614 - accuracy: 0.13 - ETA: 3:53 - loss: 3.3707 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3730 - accuracy: 0.13 - ETA: 3:53 - loss: 3.3744 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3672 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3631 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3402 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3372 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3407 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3408 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3397 - accuracy: 0.13 - ETA: 3:47 - loss: 3.3404 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3444 - accuracy: 0.13 - ETA: 3:46 - loss: 3.3396 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3417 - accuracy: 0.13 - ETA: 3:44 - loss: 3.3392 - accuracy: 0.13 - ETA: 3:43 - loss: 3.3379 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3357 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3390 - accuracy: 0.13 - ETA: 3:41 - loss: 3.3384 - accuracy: 0.13 - ETA: 3:40 - loss: 3.3381 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3380 - accuracy: 0.13 - ETA: 3:39 - loss: 3.3361 - accuracy: 0.13 - ETA: 3:38 - loss: 3.3343 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3321 - accuracy: 0.13 - ETA: 3:37 - loss: 3.3344 - accuracy: 0.13 - ETA: 3:36 - loss: 3.3325 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3329 - accuracy: 0.13 - ETA: 3:35 - loss: 3.3338 - accuracy: 0.13 - ETA: 3:34 - loss: 3.3356 - accuracy: 0.13 - ETA: 3:33 - loss: 3.3358 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3335 - accuracy: 0.13 - ETA: 3:32 - loss: 3.3328 - accuracy: 0.13 - ETA: 3:31 - loss: 3.3321 - accuracy: 0.13 - ETA: 3:30 - loss: 3.3367 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3328 - accuracy: 0.13 - ETA: 3:29 - loss: 3.3362 - accuracy: 0.13 - ETA: 3:28 - loss: 3.3342 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3457 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3437 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3414 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3407 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3426 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3407 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3420 - accuracy: 0.13 - ETA: 3:23 - loss: 3.3437 - accuracy: 0.13 - ETA: 3:22 - loss: 3.3431 - accuracy: 0.13 - ETA: 3:21 - loss: 3.3436 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3437 - accuracy: 0.13 - ETA: 3:20 - loss: 3.3469 - accuracy: 0.13 - ETA: 3:19 - loss: 3.3469 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3482 - accuracy: 0.13 - ETA: 3:18 - loss: 3.3498 - accuracy: 0.13 - ETA: 3:17 - loss: 3.3503 - accuracy: 0.13 - ETA: 3:16 - loss: 3.3503 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3534 - accuracy: 0.13 - ETA: 3:15 - loss: 3.3541 - accuracy: 0.13 - ETA: 3:14 - loss: 3.3553 - accuracy: 0.13 - ETA: 3:13 - loss: 3.3566 - accuracy: 0.13 - ETA: 3:12 - loss: 3.3571 - accuracy: 0.13 - ETA: 3:12 - loss: 3.3594 - accuracy: 0.13 - ETA: 3:11 - loss: 3.3614 - accuracy: 0.13 - ETA: 3:10 - loss: 3.3577 - accuracy: 0.13 - ETA: 3:09 - loss: 3.3572 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3570 - accuracy: 0.13 - ETA: 3:08 - loss: 3.3557 - accuracy: 0.13 - ETA: 3:07 - loss: 3.3572 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3586 - accuracy: 0.13 - ETA: 3:06 - loss: 3.3579 - accuracy: 0.13 - ETA: 3:05 - loss: 3.3600 - accuracy: 0.13 - ETA: 3:04 - loss: 3.3602 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3601 - accuracy: 0.13 - ETA: 3:03 - loss: 3.3608 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3610 - accuracy: 0.13 - ETA: 3:02 - loss: 3.3606 - accuracy: 0.13 - ETA: 3:01 - loss: 3.3583 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3579 - accuracy: 0.13 - ETA: 3:00 - loss: 3.3576 - accuracy: 0.13 - ETA: 2:59 - loss: 3.3573 - accuracy: 0.13 - ETA: 2:58 - loss: 3.3562 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3561 - accuracy: 0.13 - ETA: 2:57 - loss: 3.3553 - accuracy: 0.13 - ETA: 2:56 - loss: 3.3568 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3574 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3569 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3575 - accuracy: 0.13 - ETA: 2:53 - loss: 3.3570 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3561 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3554 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3543 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3543 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3556 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3571 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3559 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3552 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3564 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3575 - accuracy: 0.13 - ETA: 2:44 - loss: 3.3581 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3577 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3570 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3564 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3568 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3574 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3573 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3560 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3552 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3550 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3546 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3554 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3544 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3536 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3531 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3528 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3532 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3532 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3532 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3524 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3523 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3527 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3523 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3516 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3520 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3517 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3501 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3498 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3502 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3484 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3489 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3493 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3497 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3502 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3510 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3521 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3518 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3507 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3511 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3504 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3518 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3524 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3518 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3521 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3518 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3519 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3534 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3542 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3539 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3536 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3533 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3536 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3535 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3534 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3523 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3528 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3528 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3518 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3514 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3518 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3528 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3518 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3522 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3530 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3532 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3539 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3534 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3540 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3533 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3536 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3537 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3543 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3549 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3543 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3539 - accuracy: 0.1343"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:48 - loss: 3.3536 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3546 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3537 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3536 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3544 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3550 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3563 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3560 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3557 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3555 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3558 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3558 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3556 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3551 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3557 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3551 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3556 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3557 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3559 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3556 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3559 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3559 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3565 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3559 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3550 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3544 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3545 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3538 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3528 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3534 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3536 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3545 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3544 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3553 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3552 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3549 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3548 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3549 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3545 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3540 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3541 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3548 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3545 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3542 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3539 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3543 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3538 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3545 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3540 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3545 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3540 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3550 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3551 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3543 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3546 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3542 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3543 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3539 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3532 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3519 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3514 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3512 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3509 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3512 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3516 - accuracy: 0.13 - ETA: 59s - loss: 3.3523 - accuracy: 0.1364 - ETA: 58s - loss: 3.3525 - accuracy: 0.136 - ETA: 58s - loss: 3.3525 - accuracy: 0.136 - ETA: 57s - loss: 3.3533 - accuracy: 0.136 - ETA: 56s - loss: 3.3530 - accuracy: 0.136 - ETA: 55s - loss: 3.3539 - accuracy: 0.136 - ETA: 55s - loss: 3.3532 - accuracy: 0.136 - ETA: 54s - loss: 3.3526 - accuracy: 0.136 - ETA: 53s - loss: 3.3525 - accuracy: 0.136 - ETA: 52s - loss: 3.3526 - accuracy: 0.136 - ETA: 52s - loss: 3.3521 - accuracy: 0.136 - ETA: 51s - loss: 3.3521 - accuracy: 0.136 - ETA: 50s - loss: 3.3521 - accuracy: 0.136 - ETA: 49s - loss: 3.3525 - accuracy: 0.136 - ETA: 49s - loss: 3.3520 - accuracy: 0.137 - ETA: 48s - loss: 3.3524 - accuracy: 0.136 - ETA: 47s - loss: 3.3516 - accuracy: 0.137 - ETA: 46s - loss: 3.3506 - accuracy: 0.137 - ETA: 46s - loss: 3.3506 - accuracy: 0.137 - ETA: 45s - loss: 3.3504 - accuracy: 0.137 - ETA: 44s - loss: 3.3499 - accuracy: 0.137 - ETA: 43s - loss: 3.3497 - accuracy: 0.137 - ETA: 43s - loss: 3.3494 - accuracy: 0.137 - ETA: 42s - loss: 3.3493 - accuracy: 0.137 - ETA: 41s - loss: 3.3490 - accuracy: 0.137 - ETA: 40s - loss: 3.3491 - accuracy: 0.137 - ETA: 40s - loss: 3.3485 - accuracy: 0.137 - ETA: 39s - loss: 3.3487 - accuracy: 0.137 - ETA: 38s - loss: 3.3474 - accuracy: 0.138 - ETA: 37s - loss: 3.3474 - accuracy: 0.138 - ETA: 37s - loss: 3.3476 - accuracy: 0.138 - ETA: 36s - loss: 3.3481 - accuracy: 0.138 - ETA: 35s - loss: 3.3478 - accuracy: 0.138 - ETA: 34s - loss: 3.3481 - accuracy: 0.138 - ETA: 34s - loss: 3.3478 - accuracy: 0.138 - ETA: 33s - loss: 3.3477 - accuracy: 0.138 - ETA: 32s - loss: 3.3473 - accuracy: 0.138 - ETA: 31s - loss: 3.3472 - accuracy: 0.138 - ETA: 31s - loss: 3.3467 - accuracy: 0.138 - ETA: 30s - loss: 3.3462 - accuracy: 0.138 - ETA: 29s - loss: 3.3457 - accuracy: 0.138 - ETA: 28s - loss: 3.3455 - accuracy: 0.138 - ETA: 28s - loss: 3.3444 - accuracy: 0.138 - ETA: 27s - loss: 3.3448 - accuracy: 0.138 - ETA: 26s - loss: 3.3447 - accuracy: 0.138 - ETA: 25s - loss: 3.3443 - accuracy: 0.138 - ETA: 24s - loss: 3.3440 - accuracy: 0.138 - ETA: 24s - loss: 3.3444 - accuracy: 0.138 - ETA: 23s - loss: 3.3439 - accuracy: 0.138 - ETA: 22s - loss: 3.3446 - accuracy: 0.138 - ETA: 21s - loss: 3.3439 - accuracy: 0.138 - ETA: 21s - loss: 3.3441 - accuracy: 0.138 - ETA: 20s - loss: 3.3442 - accuracy: 0.138 - ETA: 19s - loss: 3.3439 - accuracy: 0.138 - ETA: 18s - loss: 3.3431 - accuracy: 0.138 - ETA: 18s - loss: 3.3426 - accuracy: 0.138 - ETA: 17s - loss: 3.3427 - accuracy: 0.138 - ETA: 16s - loss: 3.3420 - accuracy: 0.138 - ETA: 15s - loss: 3.3421 - accuracy: 0.138 - ETA: 15s - loss: 3.3415 - accuracy: 0.138 - ETA: 14s - loss: 3.3415 - accuracy: 0.138 - ETA: 13s - loss: 3.3409 - accuracy: 0.139 - ETA: 12s - loss: 3.3406 - accuracy: 0.139 - ETA: 12s - loss: 3.3413 - accuracy: 0.138 - ETA: 11s - loss: 3.3415 - accuracy: 0.138 - ETA: 10s - loss: 3.3407 - accuracy: 0.139 - ETA: 9s - loss: 3.3398 - accuracy: 0.139 - ETA: 9s - loss: 3.3394 - accuracy: 0.13 - ETA: 8s - loss: 3.3390 - accuracy: 0.13 - ETA: 7s - loss: 3.3380 - accuracy: 0.13 - ETA: 6s - loss: 3.3378 - accuracy: 0.13 - ETA: 6s - loss: 3.3374 - accuracy: 0.13 - ETA: 5s - loss: 3.3373 - accuracy: 0.13 - ETA: 4s - loss: 3.3367 - accuracy: 0.13 - ETA: 3s - loss: 3.3366 - accuracy: 0.13 - ETA: 3s - loss: 3.3363 - accuracy: 0.14 - ETA: 2s - loss: 3.3363 - accuracy: 0.14 - ETA: 1s - loss: 3.3365 - accuracy: 0.13 - ETA: 0s - loss: 3.3367 - accuracy: 0.13 - ETA: 0s - loss: 3.3368 - accuracy: 0.13 - 269s 6ms/step - loss: 3.3367 - accuracy: 0.1399 - val_loss: 4.0035 - val_accuracy: 0.0296\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:18 - loss: 3.2929 - accuracy: 0.17 - ETA: 4:13 - loss: 3.2703 - accuracy: 0.16 - ETA: 4:07 - loss: 3.2849 - accuracy: 0.16 - ETA: 4:04 - loss: 3.2743 - accuracy: 0.16 - ETA: 4:04 - loss: 3.2931 - accuracy: 0.16 - ETA: 4:03 - loss: 3.2762 - accuracy: 0.17 - ETA: 4:00 - loss: 3.2968 - accuracy: 0.16 - ETA: 4:01 - loss: 3.3037 - accuracy: 0.16 - ETA: 4:01 - loss: 3.3104 - accuracy: 0.15 - ETA: 4:00 - loss: 3.3024 - accuracy: 0.16 - ETA: 4:00 - loss: 3.2997 - accuracy: 0.15 - ETA: 4:01 - loss: 3.3088 - accuracy: 0.15 - ETA: 4:01 - loss: 3.3177 - accuracy: 0.15 - ETA: 4:00 - loss: 3.3216 - accuracy: 0.15 - ETA: 4:00 - loss: 3.3241 - accuracy: 0.14 - ETA: 3:59 - loss: 3.3264 - accuracy: 0.14 - ETA: 3:58 - loss: 3.3319 - accuracy: 0.14 - ETA: 3:57 - loss: 3.3246 - accuracy: 0.14 - ETA: 3:56 - loss: 3.3193 - accuracy: 0.15 - ETA: 3:55 - loss: 3.3231 - accuracy: 0.15 - ETA: 3:54 - loss: 3.3217 - accuracy: 0.15 - ETA: 3:53 - loss: 3.3226 - accuracy: 0.15 - ETA: 3:53 - loss: 3.3191 - accuracy: 0.15 - ETA: 3:52 - loss: 3.3135 - accuracy: 0.15 - ETA: 3:52 - loss: 3.3133 - accuracy: 0.15 - ETA: 3:51 - loss: 3.3114 - accuracy: 0.15 - ETA: 3:50 - loss: 3.3113 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3088 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3157 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3145 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3130 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3127 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3175 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3143 - accuracy: 0.14 - ETA: 3:45 - loss: 3.3137 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3086 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3109 - accuracy: 0.14 - ETA: 3:43 - loss: 3.3124 - accuracy: 0.14 - ETA: 3:42 - loss: 3.3151 - accuracy: 0.14 - ETA: 3:41 - loss: 3.3161 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3191 - accuracy: 0.14 - ETA: 3:39 - loss: 3.3197 - accuracy: 0.14 - ETA: 3:39 - loss: 3.3226 - accuracy: 0.14 - ETA: 3:38 - loss: 3.3226 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3247 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3257 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3218 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3230 - accuracy: 0.14 - ETA: 3:35 - loss: 3.3234 - accuracy: 0.14 - ETA: 3:35 - loss: 3.3230 - accuracy: 0.14 - ETA: 3:34 - loss: 3.3242 - accuracy: 0.14 - ETA: 3:33 - loss: 3.3236 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3235 - accuracy: 0.14 - ETA: 3:31 - loss: 3.3239 - accuracy: 0.14 - ETA: 3:31 - loss: 3.3237 - accuracy: 0.14 - ETA: 3:30 - loss: 3.3256 - accuracy: 0.14 - ETA: 3:29 - loss: 3.3260 - accuracy: 0.14 - ETA: 3:28 - loss: 3.3236 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3220 - accuracy: 0.14 - ETA: 3:26 - loss: 3.3206 - accuracy: 0.14 - ETA: 3:25 - loss: 3.3196 - accuracy: 0.14 - ETA: 3:24 - loss: 3.3191 - accuracy: 0.14 - ETA: 3:24 - loss: 3.3189 - accuracy: 0.14 - ETA: 3:23 - loss: 3.3205 - accuracy: 0.14 - ETA: 3:22 - loss: 3.3181 - accuracy: 0.14 - ETA: 3:21 - loss: 3.3196 - accuracy: 0.14 - ETA: 3:20 - loss: 3.3218 - accuracy: 0.14 - ETA: 3:20 - loss: 3.3227 - accuracy: 0.14 - ETA: 3:19 - loss: 3.3203 - accuracy: 0.14 - ETA: 3:18 - loss: 3.3206 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3211 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3198 - accuracy: 0.14 - ETA: 3:16 - loss: 3.3195 - accuracy: 0.14 - ETA: 3:15 - loss: 3.3203 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3189 - accuracy: 0.14 - ETA: 3:13 - loss: 3.3180 - accuracy: 0.14 - ETA: 3:13 - loss: 3.3174 - accuracy: 0.14 - ETA: 3:12 - loss: 3.3176 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3172 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3165 - accuracy: 0.14 - ETA: 3:10 - loss: 3.3189 - accuracy: 0.14 - ETA: 3:09 - loss: 3.3181 - accuracy: 0.14 - ETA: 3:08 - loss: 3.3195 - accuracy: 0.14 - ETA: 3:07 - loss: 3.3188 - accuracy: 0.14 - ETA: 3:07 - loss: 3.3174 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3181 - accuracy: 0.14 - ETA: 3:05 - loss: 3.3173 - accuracy: 0.14 - ETA: 3:04 - loss: 3.3166 - accuracy: 0.14 - ETA: 3:04 - loss: 3.3152 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3143 - accuracy: 0.14 - ETA: 3:02 - loss: 3.3152 - accuracy: 0.14 - ETA: 3:02 - loss: 3.3161 - accuracy: 0.14 - ETA: 3:01 - loss: 3.3171 - accuracy: 0.14 - ETA: 3:00 - loss: 3.3169 - accuracy: 0.14 - ETA: 2:59 - loss: 3.3165 - accuracy: 0.14 - ETA: 2:58 - loss: 3.3162 - accuracy: 0.14 - ETA: 2:58 - loss: 3.3162 - accuracy: 0.14 - ETA: 2:57 - loss: 3.3129 - accuracy: 0.14 - ETA: 2:56 - loss: 3.3114 - accuracy: 0.14 - ETA: 2:55 - loss: 3.3090 - accuracy: 0.14 - ETA: 2:54 - loss: 3.3095 - accuracy: 0.14 - ETA: 2:54 - loss: 3.3098 - accuracy: 0.14 - ETA: 2:53 - loss: 3.3086 - accuracy: 0.14 - ETA: 2:52 - loss: 3.3057 - accuracy: 0.14 - ETA: 2:51 - loss: 3.3056 - accuracy: 0.14 - ETA: 2:50 - loss: 3.3050 - accuracy: 0.14 - ETA: 2:50 - loss: 3.3043 - accuracy: 0.14 - ETA: 2:49 - loss: 3.3037 - accuracy: 0.14 - ETA: 2:48 - loss: 3.3044 - accuracy: 0.14 - ETA: 2:47 - loss: 3.3022 - accuracy: 0.14 - ETA: 2:47 - loss: 3.3001 - accuracy: 0.14 - ETA: 2:46 - loss: 3.2983 - accuracy: 0.14 - ETA: 2:45 - loss: 3.2979 - accuracy: 0.14 - ETA: 2:44 - loss: 3.2973 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2953 - accuracy: 0.14 - ETA: 2:43 - loss: 3.2919 - accuracy: 0.14 - ETA: 2:42 - loss: 3.2893 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2906 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2897 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2883 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2917 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2928 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2941 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2952 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2959 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2967 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2952 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2954 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2934 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2928 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2937 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2943 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2936 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2930 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2929 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2940 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2940 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2942 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2933 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2921 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2931 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2941 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2933 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2939 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2924 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2929 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2924 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2921 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2924 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2927 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2936 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2932 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2945 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2941 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2936 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2931 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2923 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2922 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2918 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2926 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2933 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2928 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2923 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2923 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2911 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2910 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2916 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2906 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2900 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2890 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2890 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2899 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2882 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2872 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2877 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2877 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2868 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2863 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2864 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2860 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2862 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2874 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2871 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2873 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2879 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2875 - accuracy: 0.1465"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:49 - loss: 3.2862 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2868 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2863 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2850 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2867 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2857 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2855 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2858 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2863 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2850 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2856 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2858 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2866 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2866 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2874 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2874 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2875 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2871 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2868 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2864 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2859 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2862 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2855 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2855 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2855 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2849 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2843 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2848 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2851 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2853 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2841 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2834 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2834 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2829 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2824 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2813 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2811 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2809 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2799 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2796 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2799 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2792 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2792 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2789 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2786 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2789 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2786 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2786 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2790 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2795 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2801 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2791 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2792 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2790 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2797 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2794 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2787 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2793 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2796 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2788 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2792 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2786 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2796 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2795 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2800 - accuracy: 0.14 - ETA: 59s - loss: 3.2805 - accuracy: 0.1472 - ETA: 59s - loss: 3.2809 - accuracy: 0.147 - ETA: 58s - loss: 3.2809 - accuracy: 0.147 - ETA: 57s - loss: 3.2809 - accuracy: 0.147 - ETA: 56s - loss: 3.2806 - accuracy: 0.147 - ETA: 56s - loss: 3.2801 - accuracy: 0.147 - ETA: 55s - loss: 3.2799 - accuracy: 0.147 - ETA: 54s - loss: 3.2805 - accuracy: 0.147 - ETA: 53s - loss: 3.2819 - accuracy: 0.147 - ETA: 53s - loss: 3.2817 - accuracy: 0.147 - ETA: 52s - loss: 3.2825 - accuracy: 0.146 - ETA: 51s - loss: 3.2823 - accuracy: 0.146 - ETA: 50s - loss: 3.2824 - accuracy: 0.146 - ETA: 50s - loss: 3.2822 - accuracy: 0.146 - ETA: 49s - loss: 3.2827 - accuracy: 0.146 - ETA: 48s - loss: 3.2830 - accuracy: 0.146 - ETA: 47s - loss: 3.2833 - accuracy: 0.146 - ETA: 47s - loss: 3.2827 - accuracy: 0.146 - ETA: 46s - loss: 3.2822 - accuracy: 0.147 - ETA: 45s - loss: 3.2818 - accuracy: 0.147 - ETA: 44s - loss: 3.2821 - accuracy: 0.147 - ETA: 44s - loss: 3.2821 - accuracy: 0.147 - ETA: 43s - loss: 3.2818 - accuracy: 0.147 - ETA: 42s - loss: 3.2820 - accuracy: 0.147 - ETA: 41s - loss: 3.2821 - accuracy: 0.147 - ETA: 40s - loss: 3.2819 - accuracy: 0.147 - ETA: 40s - loss: 3.2820 - accuracy: 0.146 - ETA: 39s - loss: 3.2817 - accuracy: 0.147 - ETA: 38s - loss: 3.2816 - accuracy: 0.147 - ETA: 37s - loss: 3.2818 - accuracy: 0.146 - ETA: 37s - loss: 3.2814 - accuracy: 0.147 - ETA: 36s - loss: 3.2817 - accuracy: 0.147 - ETA: 35s - loss: 3.2822 - accuracy: 0.147 - ETA: 34s - loss: 3.2823 - accuracy: 0.147 - ETA: 34s - loss: 3.2831 - accuracy: 0.146 - ETA: 33s - loss: 3.2826 - accuracy: 0.147 - ETA: 32s - loss: 3.2825 - accuracy: 0.147 - ETA: 31s - loss: 3.2815 - accuracy: 0.147 - ETA: 31s - loss: 3.2814 - accuracy: 0.147 - ETA: 30s - loss: 3.2816 - accuracy: 0.147 - ETA: 29s - loss: 3.2816 - accuracy: 0.147 - ETA: 28s - loss: 3.2815 - accuracy: 0.147 - ETA: 28s - loss: 3.2806 - accuracy: 0.147 - ETA: 27s - loss: 3.2808 - accuracy: 0.147 - ETA: 26s - loss: 3.2807 - accuracy: 0.147 - ETA: 25s - loss: 3.2810 - accuracy: 0.147 - ETA: 25s - loss: 3.2812 - accuracy: 0.147 - ETA: 24s - loss: 3.2815 - accuracy: 0.147 - ETA: 23s - loss: 3.2812 - accuracy: 0.147 - ETA: 22s - loss: 3.2813 - accuracy: 0.147 - ETA: 22s - loss: 3.2817 - accuracy: 0.147 - ETA: 21s - loss: 3.2812 - accuracy: 0.147 - ETA: 20s - loss: 3.2811 - accuracy: 0.147 - ETA: 19s - loss: 3.2809 - accuracy: 0.147 - ETA: 19s - loss: 3.2804 - accuracy: 0.147 - ETA: 18s - loss: 3.2802 - accuracy: 0.147 - ETA: 17s - loss: 3.2800 - accuracy: 0.147 - ETA: 16s - loss: 3.2797 - accuracy: 0.148 - ETA: 15s - loss: 3.2787 - accuracy: 0.148 - ETA: 15s - loss: 3.2785 - accuracy: 0.148 - ETA: 14s - loss: 3.2780 - accuracy: 0.148 - ETA: 13s - loss: 3.2782 - accuracy: 0.148 - ETA: 12s - loss: 3.2785 - accuracy: 0.148 - ETA: 12s - loss: 3.2786 - accuracy: 0.148 - ETA: 11s - loss: 3.2789 - accuracy: 0.147 - ETA: 10s - loss: 3.2786 - accuracy: 0.147 - ETA: 9s - loss: 3.2786 - accuracy: 0.147 - ETA: 9s - loss: 3.2789 - accuracy: 0.14 - ETA: 8s - loss: 3.2792 - accuracy: 0.14 - ETA: 7s - loss: 3.2791 - accuracy: 0.14 - ETA: 6s - loss: 3.2798 - accuracy: 0.14 - ETA: 6s - loss: 3.2796 - accuracy: 0.14 - ETA: 5s - loss: 3.2799 - accuracy: 0.14 - ETA: 4s - loss: 3.2799 - accuracy: 0.14 - ETA: 3s - loss: 3.2797 - accuracy: 0.14 - ETA: 3s - loss: 3.2795 - accuracy: 0.14 - ETA: 2s - loss: 3.2795 - accuracy: 0.14 - ETA: 1s - loss: 3.2795 - accuracy: 0.14 - ETA: 0s - loss: 3.2793 - accuracy: 0.14 - ETA: 0s - loss: 3.2791 - accuracy: 0.14 - 270s 6ms/step - loss: 3.2790 - accuracy: 0.1481 - val_loss: 4.3037 - val_accuracy: 0.0275\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:48 - loss: 3.4094 - accuracy: 0.12 - ETA: 4:01 - loss: 3.3627 - accuracy: 0.11 - ETA: 4:03 - loss: 3.2931 - accuracy: 0.13 - ETA: 4:04 - loss: 3.2721 - accuracy: 0.14 - ETA: 4:04 - loss: 3.2864 - accuracy: 0.13 - ETA: 4:03 - loss: 3.2506 - accuracy: 0.14 - ETA: 4:02 - loss: 3.2509 - accuracy: 0.15 - ETA: 4:03 - loss: 3.5973 - accuracy: 0.14 - ETA: 4:03 - loss: 3.5495 - accuracy: 0.14 - ETA: 4:05 - loss: 3.5377 - accuracy: 0.14 - ETA: 4:04 - loss: 3.5249 - accuracy: 0.13 - ETA: 4:05 - loss: 3.4937 - accuracy: 0.14 - ETA: 4:04 - loss: 3.4904 - accuracy: 0.13 - ETA: 4:03 - loss: 3.4877 - accuracy: 0.13 - ETA: 4:03 - loss: 3.4667 - accuracy: 0.13 - ETA: 4:02 - loss: 3.4523 - accuracy: 0.13 - ETA: 4:01 - loss: 3.4441 - accuracy: 0.13 - ETA: 4:00 - loss: 3.4490 - accuracy: 0.13 - ETA: 3:59 - loss: 3.4487 - accuracy: 0.13 - ETA: 3:57 - loss: 3.4448 - accuracy: 0.13 - ETA: 3:56 - loss: 3.4410 - accuracy: 0.13 - ETA: 3:55 - loss: 3.4281 - accuracy: 0.13 - ETA: 3:54 - loss: 3.4255 - accuracy: 0.13 - ETA: 3:53 - loss: 3.4197 - accuracy: 0.14 - ETA: 3:52 - loss: 3.4152 - accuracy: 0.14 - ETA: 3:51 - loss: 3.4059 - accuracy: 0.14 - ETA: 3:50 - loss: 3.4019 - accuracy: 0.14 - ETA: 3:50 - loss: 3.3930 - accuracy: 0.14 - ETA: 3:49 - loss: 3.3974 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3991 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3956 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3935 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3891 - accuracy: 0.14 - ETA: 3:45 - loss: 3.3900 - accuracy: 0.14 - ETA: 3:45 - loss: 3.3876 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3875 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3810 - accuracy: 0.14 - ETA: 3:43 - loss: 3.3739 - accuracy: 0.14 - ETA: 3:43 - loss: 3.3722 - accuracy: 0.14 - ETA: 3:42 - loss: 3.3703 - accuracy: 0.14 - ETA: 3:41 - loss: 3.3685 - accuracy: 0.14 - ETA: 3:41 - loss: 3.3681 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3673 - accuracy: 0.14 - ETA: 3:39 - loss: 3.3646 - accuracy: 0.14 - ETA: 3:38 - loss: 3.3704 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3681 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3690 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3673 - accuracy: 0.14 - ETA: 3:35 - loss: 3.3634 - accuracy: 0.14 - ETA: 3:34 - loss: 3.3624 - accuracy: 0.14 - ETA: 3:33 - loss: 3.3605 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3596 - accuracy: 0.14 - ETA: 3:31 - loss: 3.3575 - accuracy: 0.14 - ETA: 3:30 - loss: 3.3528 - accuracy: 0.14 - ETA: 3:30 - loss: 3.3537 - accuracy: 0.14 - ETA: 3:29 - loss: 3.3508 - accuracy: 0.14 - ETA: 3:28 - loss: 3.3481 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3493 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3474 - accuracy: 0.14 - ETA: 3:26 - loss: 3.3502 - accuracy: 0.14 - ETA: 3:25 - loss: 3.3493 - accuracy: 0.14 - ETA: 3:24 - loss: 3.3469 - accuracy: 0.14 - ETA: 3:24 - loss: 3.3476 - accuracy: 0.14 - ETA: 3:23 - loss: 3.3489 - accuracy: 0.14 - ETA: 3:22 - loss: 3.3470 - accuracy: 0.14 - ETA: 3:21 - loss: 3.3468 - accuracy: 0.14 - ETA: 3:20 - loss: 3.3442 - accuracy: 0.14 - ETA: 3:20 - loss: 3.3427 - accuracy: 0.14 - ETA: 3:19 - loss: 3.3394 - accuracy: 0.14 - ETA: 3:18 - loss: 3.3334 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3327 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3314 - accuracy: 0.14 - ETA: 3:16 - loss: 3.3311 - accuracy: 0.14 - ETA: 3:15 - loss: 3.3307 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3292 - accuracy: 0.14 - ETA: 3:13 - loss: 3.3251 - accuracy: 0.14 - ETA: 3:13 - loss: 3.3239 - accuracy: 0.14 - ETA: 3:12 - loss: 3.3224 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3210 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3192 - accuracy: 0.14 - ETA: 3:10 - loss: 3.3192 - accuracy: 0.14 - ETA: 3:09 - loss: 3.3197 - accuracy: 0.14 - ETA: 3:08 - loss: 3.3160 - accuracy: 0.14 - ETA: 3:07 - loss: 3.3166 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3157 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3158 - accuracy: 0.14 - ETA: 3:05 - loss: 3.3159 - accuracy: 0.14 - ETA: 3:04 - loss: 3.3150 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3144 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3137 - accuracy: 0.14 - ETA: 3:02 - loss: 3.3142 - accuracy: 0.14 - ETA: 3:01 - loss: 3.3134 - accuracy: 0.14 - ETA: 3:00 - loss: 3.3122 - accuracy: 0.14 - ETA: 2:59 - loss: 3.3138 - accuracy: 0.14 - ETA: 2:59 - loss: 3.3147 - accuracy: 0.14 - ETA: 2:58 - loss: 3.3139 - accuracy: 0.14 - ETA: 2:57 - loss: 3.3124 - accuracy: 0.14 - ETA: 2:56 - loss: 3.3104 - accuracy: 0.14 - ETA: 2:56 - loss: 3.3099 - accuracy: 0.14 - ETA: 2:55 - loss: 3.3107 - accuracy: 0.14 - ETA: 2:54 - loss: 3.3114 - accuracy: 0.14 - ETA: 2:53 - loss: 3.3104 - accuracy: 0.14 - ETA: 2:53 - loss: 3.3104 - accuracy: 0.14 - ETA: 2:52 - loss: 3.3105 - accuracy: 0.14 - ETA: 2:51 - loss: 3.3105 - accuracy: 0.14 - ETA: 2:50 - loss: 3.3091 - accuracy: 0.14 - ETA: 2:49 - loss: 3.3069 - accuracy: 0.14 - ETA: 2:48 - loss: 3.3080 - accuracy: 0.14 - ETA: 2:48 - loss: 3.3072 - accuracy: 0.14 - ETA: 2:47 - loss: 3.3067 - accuracy: 0.14 - ETA: 2:46 - loss: 3.3052 - accuracy: 0.14 - ETA: 2:45 - loss: 3.3037 - accuracy: 0.14 - ETA: 2:45 - loss: 3.3031 - accuracy: 0.14 - ETA: 2:44 - loss: 3.3025 - accuracy: 0.14 - ETA: 2:43 - loss: 3.3023 - accuracy: 0.14 - ETA: 2:42 - loss: 3.3018 - accuracy: 0.14 - ETA: 2:42 - loss: 3.3008 - accuracy: 0.14 - ETA: 2:41 - loss: 3.2983 - accuracy: 0.14 - ETA: 2:40 - loss: 3.2981 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2980 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2960 - accuracy: 0.14 - ETA: 2:38 - loss: 3.2941 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2942 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2949 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2954 - accuracy: 0.14 - ETA: 2:35 - loss: 3.2933 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2928 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2930 - accuracy: 0.14 - ETA: 2:33 - loss: 3.2931 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2934 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2912 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2906 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2901 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2904 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2911 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2896 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2891 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2890 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2881 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2882 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2880 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2866 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2865 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2856 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2851 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2845 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2832 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2834 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2842 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2838 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2833 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2815 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2811 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2800 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2792 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2796 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2791 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2796 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2786 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2782 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2775 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2764 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2753 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2732 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2738 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2732 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2734 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2758 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2783 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2814 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2807 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2802 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2800 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2808 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2812 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2819 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2814 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2817 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2815 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2818 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2817 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2811 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2804 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2802 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2803 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2797 - accuracy: 0.1515"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:49 - loss: 3.2792 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2792 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2787 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2783 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2781 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2784 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2786 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2785 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2787 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2798 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2806 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2801 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2798 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2797 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2796 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2790 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2787 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2796 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2789 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2792 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2789 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2785 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2784 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2791 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2783 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2777 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2769 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2760 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2755 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2758 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2755 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2754 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2759 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2755 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2754 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2749 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2751 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2745 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2737 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2732 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2730 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2738 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2737 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2742 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2747 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2753 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2755 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2769 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2763 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2764 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2767 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2776 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2777 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2775 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2782 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2784 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2785 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2789 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2793 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2798 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2808 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2809 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2810 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2815 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2814 - accuracy: 0.14 - ETA: 59s - loss: 3.2820 - accuracy: 0.1498 - ETA: 59s - loss: 3.2815 - accuracy: 0.149 - ETA: 58s - loss: 3.2815 - accuracy: 0.149 - ETA: 57s - loss: 3.2818 - accuracy: 0.149 - ETA: 56s - loss: 3.2817 - accuracy: 0.149 - ETA: 56s - loss: 3.2819 - accuracy: 0.150 - ETA: 55s - loss: 3.2817 - accuracy: 0.150 - ETA: 54s - loss: 3.2814 - accuracy: 0.150 - ETA: 53s - loss: 3.2814 - accuracy: 0.150 - ETA: 53s - loss: 3.2816 - accuracy: 0.150 - ETA: 52s - loss: 3.2821 - accuracy: 0.149 - ETA: 51s - loss: 3.2820 - accuracy: 0.149 - ETA: 50s - loss: 3.2827 - accuracy: 0.149 - ETA: 49s - loss: 3.2828 - accuracy: 0.149 - ETA: 49s - loss: 3.2828 - accuracy: 0.149 - ETA: 48s - loss: 3.2831 - accuracy: 0.149 - ETA: 47s - loss: 3.2834 - accuracy: 0.149 - ETA: 46s - loss: 3.2836 - accuracy: 0.149 - ETA: 46s - loss: 3.2838 - accuracy: 0.149 - ETA: 45s - loss: 3.2842 - accuracy: 0.149 - ETA: 44s - loss: 3.2848 - accuracy: 0.149 - ETA: 43s - loss: 3.2849 - accuracy: 0.148 - ETA: 43s - loss: 3.2857 - accuracy: 0.148 - ETA: 42s - loss: 3.2860 - accuracy: 0.148 - ETA: 41s - loss: 3.2864 - accuracy: 0.148 - ETA: 40s - loss: 3.2866 - accuracy: 0.148 - ETA: 40s - loss: 3.2868 - accuracy: 0.148 - ETA: 39s - loss: 3.2868 - accuracy: 0.148 - ETA: 38s - loss: 3.2872 - accuracy: 0.148 - ETA: 37s - loss: 3.2875 - accuracy: 0.148 - ETA: 37s - loss: 3.2879 - accuracy: 0.148 - ETA: 36s - loss: 3.2893 - accuracy: 0.148 - ETA: 35s - loss: 3.2897 - accuracy: 0.148 - ETA: 34s - loss: 3.2889 - accuracy: 0.148 - ETA: 34s - loss: 3.2890 - accuracy: 0.148 - ETA: 33s - loss: 3.2890 - accuracy: 0.148 - ETA: 32s - loss: 3.2895 - accuracy: 0.148 - ETA: 31s - loss: 3.2897 - accuracy: 0.148 - ETA: 31s - loss: 3.2902 - accuracy: 0.148 - ETA: 30s - loss: 3.2900 - accuracy: 0.148 - ETA: 29s - loss: 3.2904 - accuracy: 0.148 - ETA: 28s - loss: 3.2910 - accuracy: 0.148 - ETA: 28s - loss: 3.2911 - accuracy: 0.148 - ETA: 27s - loss: 3.2908 - accuracy: 0.148 - ETA: 26s - loss: 3.2910 - accuracy: 0.148 - ETA: 25s - loss: 3.2915 - accuracy: 0.148 - ETA: 25s - loss: 3.2917 - accuracy: 0.148 - ETA: 24s - loss: 3.2914 - accuracy: 0.148 - ETA: 23s - loss: 3.2916 - accuracy: 0.148 - ETA: 22s - loss: 3.2925 - accuracy: 0.148 - ETA: 21s - loss: 3.2926 - accuracy: 0.148 - ETA: 21s - loss: 3.2927 - accuracy: 0.148 - ETA: 20s - loss: 3.2924 - accuracy: 0.148 - ETA: 19s - loss: 3.2924 - accuracy: 0.148 - ETA: 18s - loss: 3.2923 - accuracy: 0.148 - ETA: 18s - loss: 3.2920 - accuracy: 0.148 - ETA: 17s - loss: 3.2923 - accuracy: 0.148 - ETA: 16s - loss: 3.2922 - accuracy: 0.148 - ETA: 15s - loss: 3.2922 - accuracy: 0.147 - ETA: 15s - loss: 3.2925 - accuracy: 0.148 - ETA: 14s - loss: 3.2926 - accuracy: 0.148 - ETA: 13s - loss: 3.2925 - accuracy: 0.148 - ETA: 12s - loss: 3.2928 - accuracy: 0.148 - ETA: 12s - loss: 3.2932 - accuracy: 0.148 - ETA: 11s - loss: 3.2932 - accuracy: 0.147 - ETA: 10s - loss: 3.2937 - accuracy: 0.147 - ETA: 9s - loss: 3.2939 - accuracy: 0.147 - ETA: 9s - loss: 3.2940 - accuracy: 0.14 - ETA: 8s - loss: 3.2940 - accuracy: 0.14 - ETA: 7s - loss: 3.2941 - accuracy: 0.14 - ETA: 6s - loss: 3.2944 - accuracy: 0.14 - ETA: 6s - loss: 3.2950 - accuracy: 0.14 - ETA: 54s - loss: 3.2958 - accuracy: 0.147 - ETA: 46s - loss: 3.2963 - accuracy: 0.147 - ETA: 39s - loss: 3.2967 - accuracy: 0.147 - ETA: 31s - loss: 3.2968 - accuracy: 0.147 - ETA: 23s - loss: 3.2968 - accuracy: 0.147 - ETA: 15s - loss: 3.2976 - accuracy: 0.147 - ETA: 8s - loss: 3.2980 - accuracy: 0.147 - ETA: 0s - loss: 3.2989 - accuracy: 0.14 - 2527s 60ms/step - loss: 3.2989 - accuracy: 0.1468 - val_loss: 4.0363 - val_accuracy: 0.0198\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 3:00 - loss: 3.3958 - accuracy: 0.09 - ETA: 2:58 - loss: 3.3846 - accuracy: 0.09 - ETA: 3:01 - loss: 3.3053 - accuracy: 0.12 - ETA: 2:57 - loss: 3.3351 - accuracy: 0.11 - ETA: 2:56 - loss: 3.3086 - accuracy: 0.12 - ETA: 2:54 - loss: 3.3215 - accuracy: 0.12 - ETA: 2:55 - loss: 3.3149 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3258 - accuracy: 0.12 - ETA: 2:54 - loss: 3.3311 - accuracy: 0.13 - ETA: 2:55 - loss: 3.3305 - accuracy: 0.13 - ETA: 2:54 - loss: 3.3376 - accuracy: 0.13 - ETA: 2:52 - loss: 3.3338 - accuracy: 0.13 - ETA: 2:51 - loss: 3.3389 - accuracy: 0.13 - ETA: 2:50 - loss: 3.3421 - accuracy: 0.13 - ETA: 2:49 - loss: 3.3434 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3550 - accuracy: 0.13 - ETA: 2:48 - loss: 3.3340 - accuracy: 0.13 - ETA: 2:47 - loss: 3.3381 - accuracy: 0.13 - ETA: 2:46 - loss: 3.3506 - accuracy: 0.13 - ETA: 2:45 - loss: 3.3521 - accuracy: 0.12 - ETA: 2:44 - loss: 3.3432 - accuracy: 0.13 - ETA: 2:43 - loss: 3.3465 - accuracy: 0.13 - ETA: 2:42 - loss: 3.3510 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3528 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3551 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3549 - accuracy: 0.12 - ETA: 2:40 - loss: 3.3460 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3444 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3446 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3461 - accuracy: 0.12 - ETA: 2:38 - loss: 3.3443 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3494 - accuracy: 0.12 - ETA: 2:36 - loss: 3.3524 - accuracy: 0.12 - ETA: 2:35 - loss: 3.3477 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3503 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3512 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3539 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3502 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3523 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3509 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3468 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3457 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3481 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3473 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3458 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3466 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3490 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3487 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3466 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3451 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3397 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3417 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3432 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3430 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3450 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3465 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3438 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3436 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3414 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3437 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3428 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3455 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3444 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3450 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3461 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3432 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3456 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3453 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3442 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3414 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3421 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3412 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3389 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3383 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3368 - accuracy: 0.14 - ETA: 2:08 - loss: 3.3362 - accuracy: 0.14 - ETA: 2:07 - loss: 3.3373 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3390 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3405 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3469 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3488 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3533 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3566 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3585 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3623 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3634 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3644 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3654 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3665 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3650 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3658 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3664 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3660 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3647 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3663 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3671 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3662 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3646 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3671 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3682 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3695 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3710 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3708 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3714 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3717 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3702 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3711 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3708 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3719 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3704 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3707 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3711 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3719 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3743 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3749 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3760 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3756 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3766 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3765 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3778 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3789 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3794 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3798 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3801 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3806 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3810 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3823 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3840 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3849 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3844 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3837 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3835 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3815 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3811 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3799 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3796 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3796 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3805 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3807 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3803 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3793 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3783 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3772 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3770 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3766 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3760 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3762 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3771 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3763 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3752 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3749 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3753 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3745 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3738 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3736 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3726 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3725 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3723 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3726 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3718 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3720 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3708 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3702 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3700 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3701 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3705 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3699 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3711 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3707 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3705 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3703 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3696 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3696 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3701 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3712 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3714 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3721 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3724 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3721 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3724 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3710 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3702 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3696 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3694 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3693 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3690 - accuracy: 0.1379"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:09 - loss: 3.3676 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3680 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3680 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3680 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3681 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3675 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3677 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3681 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3676 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3677 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3673 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3668 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3665 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3664 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3660 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3649 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3640 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3636 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3636 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3634 - accuracy: 0.13 - ETA: 59s - loss: 3.3636 - accuracy: 0.1375 - ETA: 59s - loss: 3.3626 - accuracy: 0.137 - ETA: 58s - loss: 3.3624 - accuracy: 0.137 - ETA: 58s - loss: 3.3616 - accuracy: 0.137 - ETA: 57s - loss: 3.3617 - accuracy: 0.137 - ETA: 57s - loss: 3.3613 - accuracy: 0.137 - ETA: 56s - loss: 3.3612 - accuracy: 0.137 - ETA: 56s - loss: 3.3602 - accuracy: 0.137 - ETA: 55s - loss: 3.3592 - accuracy: 0.137 - ETA: 55s - loss: 3.3588 - accuracy: 0.137 - ETA: 54s - loss: 3.3578 - accuracy: 0.138 - ETA: 54s - loss: 3.3573 - accuracy: 0.138 - ETA: 53s - loss: 3.3585 - accuracy: 0.138 - ETA: 53s - loss: 3.3582 - accuracy: 0.138 - ETA: 52s - loss: 3.3569 - accuracy: 0.138 - ETA: 52s - loss: 3.3579 - accuracy: 0.138 - ETA: 51s - loss: 3.3581 - accuracy: 0.138 - ETA: 51s - loss: 3.3585 - accuracy: 0.137 - ETA: 50s - loss: 3.3581 - accuracy: 0.137 - ETA: 50s - loss: 3.3580 - accuracy: 0.138 - ETA: 49s - loss: 3.3571 - accuracy: 0.138 - ETA: 49s - loss: 3.3568 - accuracy: 0.138 - ETA: 48s - loss: 3.3562 - accuracy: 0.138 - ETA: 48s - loss: 3.3557 - accuracy: 0.138 - ETA: 47s - loss: 3.3557 - accuracy: 0.138 - ETA: 47s - loss: 3.3556 - accuracy: 0.138 - ETA: 46s - loss: 3.3556 - accuracy: 0.138 - ETA: 46s - loss: 3.3551 - accuracy: 0.138 - ETA: 45s - loss: 3.3554 - accuracy: 0.138 - ETA: 45s - loss: 3.3553 - accuracy: 0.138 - ETA: 44s - loss: 3.3553 - accuracy: 0.138 - ETA: 44s - loss: 3.3550 - accuracy: 0.138 - ETA: 43s - loss: 3.3551 - accuracy: 0.138 - ETA: 43s - loss: 3.3546 - accuracy: 0.138 - ETA: 42s - loss: 3.3541 - accuracy: 0.139 - ETA: 42s - loss: 3.3542 - accuracy: 0.139 - ETA: 41s - loss: 3.3554 - accuracy: 0.139 - ETA: 41s - loss: 3.3552 - accuracy: 0.139 - ETA: 40s - loss: 3.3554 - accuracy: 0.139 - ETA: 40s - loss: 3.3558 - accuracy: 0.139 - ETA: 39s - loss: 3.3562 - accuracy: 0.139 - ETA: 39s - loss: 3.3563 - accuracy: 0.139 - ETA: 38s - loss: 3.3560 - accuracy: 0.139 - ETA: 38s - loss: 3.3563 - accuracy: 0.139 - ETA: 37s - loss: 3.3556 - accuracy: 0.139 - ETA: 37s - loss: 3.3554 - accuracy: 0.139 - ETA: 37s - loss: 3.3555 - accuracy: 0.139 - ETA: 36s - loss: 3.3553 - accuracy: 0.139 - ETA: 36s - loss: 3.3551 - accuracy: 0.139 - ETA: 35s - loss: 3.3548 - accuracy: 0.139 - ETA: 35s - loss: 3.3543 - accuracy: 0.139 - ETA: 34s - loss: 3.3540 - accuracy: 0.139 - ETA: 34s - loss: 3.3544 - accuracy: 0.139 - ETA: 33s - loss: 3.3535 - accuracy: 0.139 - ETA: 33s - loss: 3.3525 - accuracy: 0.139 - ETA: 32s - loss: 3.3518 - accuracy: 0.140 - ETA: 32s - loss: 3.3517 - accuracy: 0.140 - ETA: 31s - loss: 3.3515 - accuracy: 0.140 - ETA: 31s - loss: 3.3517 - accuracy: 0.140 - ETA: 30s - loss: 3.3514 - accuracy: 0.140 - ETA: 30s - loss: 3.3508 - accuracy: 0.140 - ETA: 29s - loss: 3.3505 - accuracy: 0.140 - ETA: 29s - loss: 3.3498 - accuracy: 0.140 - ETA: 28s - loss: 3.3497 - accuracy: 0.140 - ETA: 28s - loss: 3.3500 - accuracy: 0.140 - ETA: 27s - loss: 3.3505 - accuracy: 0.140 - ETA: 27s - loss: 3.3502 - accuracy: 0.140 - ETA: 26s - loss: 3.3505 - accuracy: 0.140 - ETA: 26s - loss: 3.3500 - accuracy: 0.140 - ETA: 26s - loss: 3.3499 - accuracy: 0.140 - ETA: 25s - loss: 3.3499 - accuracy: 0.140 - ETA: 25s - loss: 3.3495 - accuracy: 0.140 - ETA: 24s - loss: 3.3499 - accuracy: 0.140 - ETA: 24s - loss: 3.3495 - accuracy: 0.140 - ETA: 23s - loss: 3.3492 - accuracy: 0.140 - ETA: 23s - loss: 3.3488 - accuracy: 0.140 - ETA: 22s - loss: 3.3490 - accuracy: 0.140 - ETA: 22s - loss: 3.3494 - accuracy: 0.140 - ETA: 21s - loss: 3.3491 - accuracy: 0.140 - ETA: 21s - loss: 3.3484 - accuracy: 0.140 - ETA: 20s - loss: 3.3482 - accuracy: 0.140 - ETA: 20s - loss: 3.3483 - accuracy: 0.140 - ETA: 19s - loss: 3.3478 - accuracy: 0.140 - ETA: 19s - loss: 3.3477 - accuracy: 0.140 - ETA: 18s - loss: 3.3468 - accuracy: 0.140 - ETA: 18s - loss: 3.3468 - accuracy: 0.140 - ETA: 17s - loss: 3.3464 - accuracy: 0.140 - ETA: 17s - loss: 3.3454 - accuracy: 0.141 - ETA: 16s - loss: 3.3458 - accuracy: 0.141 - ETA: 16s - loss: 3.3453 - accuracy: 0.140 - ETA: 16s - loss: 3.3452 - accuracy: 0.141 - ETA: 15s - loss: 3.3443 - accuracy: 0.141 - ETA: 15s - loss: 3.3443 - accuracy: 0.141 - ETA: 14s - loss: 3.3442 - accuracy: 0.141 - ETA: 14s - loss: 3.3448 - accuracy: 0.141 - ETA: 13s - loss: 3.3450 - accuracy: 0.141 - ETA: 13s - loss: 3.3445 - accuracy: 0.141 - ETA: 12s - loss: 3.3442 - accuracy: 0.141 - ETA: 12s - loss: 3.3443 - accuracy: 0.141 - ETA: 11s - loss: 3.3436 - accuracy: 0.141 - ETA: 11s - loss: 3.3431 - accuracy: 0.142 - ETA: 10s - loss: 3.3425 - accuracy: 0.142 - ETA: 10s - loss: 3.3424 - accuracy: 0.142 - ETA: 9s - loss: 3.3419 - accuracy: 0.142 - ETA: 9s - loss: 3.3423 - accuracy: 0.14 - ETA: 8s - loss: 3.3419 - accuracy: 0.14 - ETA: 8s - loss: 3.3419 - accuracy: 0.14 - ETA: 8s - loss: 3.3415 - accuracy: 0.14 - ETA: 7s - loss: 3.3419 - accuracy: 0.14 - ETA: 7s - loss: 3.3425 - accuracy: 0.14 - ETA: 6s - loss: 3.3427 - accuracy: 0.14 - ETA: 6s - loss: 3.3431 - accuracy: 0.14 - ETA: 5s - loss: 3.3427 - accuracy: 0.14 - ETA: 5s - loss: 3.3426 - accuracy: 0.14 - ETA: 4s - loss: 3.3426 - accuracy: 0.14 - ETA: 4s - loss: 3.3427 - accuracy: 0.14 - ETA: 3s - loss: 3.3422 - accuracy: 0.14 - ETA: 3s - loss: 3.3416 - accuracy: 0.14 - ETA: 2s - loss: 3.3413 - accuracy: 0.14 - ETA: 2s - loss: 3.3417 - accuracy: 0.14 - ETA: 1s - loss: 3.3409 - accuracy: 0.14 - ETA: 1s - loss: 3.3404 - accuracy: 0.14 - ETA: 0s - loss: 3.3404 - accuracy: 0.14 - ETA: 0s - loss: 3.3400 - accuracy: 0.14 - ETA: 0s - loss: 3.3402 - accuracy: 0.14 - 165s 4ms/step - loss: 3.3402 - accuracy: 0.1434 - val_loss: 3.9912 - val_accuracy: 0.0315\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:42 - loss: 3.3374 - accuracy: 0.17 - ETA: 2:33 - loss: 3.3578 - accuracy: 0.16 - ETA: 2:30 - loss: 3.2959 - accuracy: 0.15 - ETA: 2:28 - loss: 3.3213 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3280 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3312 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3819 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3876 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3989 - accuracy: 0.12 - ETA: 2:23 - loss: 3.3970 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4035 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4013 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4036 - accuracy: 0.12 - ETA: 2:21 - loss: 3.3976 - accuracy: 0.12 - ETA: 2:20 - loss: 3.3792 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3821 - accuracy: 0.12 - ETA: 2:19 - loss: 3.3812 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3778 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3704 - accuracy: 0.12 - ETA: 2:17 - loss: 3.3667 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3619 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3633 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3651 - accuracy: 0.12 - ETA: 2:17 - loss: 3.3639 - accuracy: 0.12 - ETA: 2:17 - loss: 3.3546 - accuracy: 0.12 - ETA: 2:17 - loss: 3.3588 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3515 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3496 - accuracy: 0.12 - ETA: 2:15 - loss: 3.3509 - accuracy: 0.12 - ETA: 2:14 - loss: 3.3521 - accuracy: 0.12 - ETA: 2:14 - loss: 3.3525 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3472 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3485 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3478 - accuracy: 0.12 - ETA: 2:12 - loss: 3.3446 - accuracy: 0.12 - ETA: 2:12 - loss: 3.3383 - accuracy: 0.12 - ETA: 2:12 - loss: 3.3349 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3321 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3337 - accuracy: 0.12 - ETA: 2:10 - loss: 3.3251 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3201 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3202 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3216 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3212 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3216 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3214 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3241 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3221 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3216 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3215 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3274 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3256 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3259 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3239 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3218 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3236 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3214 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3182 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3188 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3174 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3165 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3170 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3168 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3137 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3141 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3129 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3114 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3104 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3111 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3097 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3088 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3095 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3101 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3104 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3099 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3105 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3104 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3096 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3073 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3059 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3044 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3060 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3054 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3030 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3023 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3020 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3023 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3021 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3017 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3005 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2994 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2990 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3014 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3022 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3014 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3017 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3006 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2991 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2971 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2968 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2977 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2970 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2960 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2965 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2964 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2971 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2970 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3002 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3004 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3006 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2998 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2997 - accuracy: 0.14 - ETA: 1:37 - loss: 3.3004 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2998 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2982 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2968 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2962 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2953 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2937 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2953 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2935 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2921 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2913 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2921 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2913 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2906 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2907 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2910 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2910 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2903 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2901 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2906 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2888 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2895 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2899 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2903 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2907 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2911 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2898 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2886 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2874 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2868 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2867 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2863 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2864 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2872 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2877 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2867 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2864 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2864 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2856 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2849 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2851 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2858 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2862 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2861 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2871 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2884 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2903 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2903 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2902 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2901 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2886 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2891 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2899 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2900 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2898 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2900 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2903 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2907 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2909 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2915 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2917 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2926 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2925 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2925 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2938 - accuracy: 0.14 - ETA: 1:08 - loss: 3.2940 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2936 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2935 - accuracy: 0.14 - ETA: 1:07 - loss: 3.2937 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2941 - accuracy: 0.14 - ETA: 1:06 - loss: 3.2953 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2966 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2963 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2960 - accuracy: 0.1415"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:04 - loss: 3.2956 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2948 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2958 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2951 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2952 - accuracy: 0.14 - ETA: 1:02 - loss: 3.2948 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2947 - accuracy: 0.14 - ETA: 1:01 - loss: 3.2942 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2945 - accuracy: 0.14 - ETA: 1:00 - loss: 3.2941 - accuracy: 0.14 - ETA: 59s - loss: 3.2947 - accuracy: 0.1419 - ETA: 59s - loss: 3.2941 - accuracy: 0.142 - ETA: 59s - loss: 3.2943 - accuracy: 0.141 - ETA: 58s - loss: 3.2942 - accuracy: 0.141 - ETA: 58s - loss: 3.2943 - accuracy: 0.141 - ETA: 57s - loss: 3.2939 - accuracy: 0.141 - ETA: 57s - loss: 3.2940 - accuracy: 0.141 - ETA: 56s - loss: 3.2937 - accuracy: 0.141 - ETA: 56s - loss: 3.2942 - accuracy: 0.141 - ETA: 55s - loss: 3.2934 - accuracy: 0.141 - ETA: 55s - loss: 3.2930 - accuracy: 0.141 - ETA: 55s - loss: 3.2932 - accuracy: 0.141 - ETA: 54s - loss: 3.2930 - accuracy: 0.141 - ETA: 54s - loss: 3.2922 - accuracy: 0.141 - ETA: 53s - loss: 3.2922 - accuracy: 0.141 - ETA: 53s - loss: 3.2927 - accuracy: 0.141 - ETA: 52s - loss: 3.2926 - accuracy: 0.142 - ETA: 52s - loss: 3.2923 - accuracy: 0.142 - ETA: 51s - loss: 3.2914 - accuracy: 0.142 - ETA: 51s - loss: 3.2916 - accuracy: 0.142 - ETA: 51s - loss: 3.2920 - accuracy: 0.142 - ETA: 50s - loss: 3.2915 - accuracy: 0.142 - ETA: 50s - loss: 3.2917 - accuracy: 0.142 - ETA: 49s - loss: 3.2914 - accuracy: 0.142 - ETA: 49s - loss: 3.2914 - accuracy: 0.142 - ETA: 48s - loss: 3.2905 - accuracy: 0.142 - ETA: 48s - loss: 3.2907 - accuracy: 0.142 - ETA: 47s - loss: 3.2898 - accuracy: 0.142 - ETA: 47s - loss: 3.2902 - accuracy: 0.142 - ETA: 46s - loss: 3.2897 - accuracy: 0.142 - ETA: 46s - loss: 3.2895 - accuracy: 0.142 - ETA: 46s - loss: 3.2888 - accuracy: 0.142 - ETA: 45s - loss: 3.2887 - accuracy: 0.142 - ETA: 45s - loss: 3.2885 - accuracy: 0.142 - ETA: 44s - loss: 3.2874 - accuracy: 0.143 - ETA: 44s - loss: 3.2874 - accuracy: 0.143 - ETA: 43s - loss: 3.2869 - accuracy: 0.143 - ETA: 43s - loss: 3.2873 - accuracy: 0.143 - ETA: 42s - loss: 3.2871 - accuracy: 0.143 - ETA: 42s - loss: 3.2876 - accuracy: 0.142 - ETA: 42s - loss: 3.2869 - accuracy: 0.143 - ETA: 41s - loss: 3.2863 - accuracy: 0.143 - ETA: 41s - loss: 3.2862 - accuracy: 0.143 - ETA: 40s - loss: 3.2859 - accuracy: 0.143 - ETA: 40s - loss: 3.2865 - accuracy: 0.143 - ETA: 39s - loss: 3.2858 - accuracy: 0.143 - ETA: 39s - loss: 3.2852 - accuracy: 0.143 - ETA: 38s - loss: 3.2847 - accuracy: 0.143 - ETA: 38s - loss: 3.2856 - accuracy: 0.143 - ETA: 38s - loss: 3.2848 - accuracy: 0.143 - ETA: 37s - loss: 3.2848 - accuracy: 0.143 - ETA: 37s - loss: 3.2836 - accuracy: 0.144 - ETA: 36s - loss: 3.2825 - accuracy: 0.144 - ETA: 36s - loss: 3.2830 - accuracy: 0.144 - ETA: 35s - loss: 3.2831 - accuracy: 0.144 - ETA: 35s - loss: 3.2826 - accuracy: 0.144 - ETA: 34s - loss: 3.2824 - accuracy: 0.144 - ETA: 34s - loss: 3.2831 - accuracy: 0.144 - ETA: 33s - loss: 3.2831 - accuracy: 0.144 - ETA: 33s - loss: 3.2833 - accuracy: 0.144 - ETA: 33s - loss: 3.2832 - accuracy: 0.144 - ETA: 32s - loss: 3.2834 - accuracy: 0.144 - ETA: 32s - loss: 3.2836 - accuracy: 0.144 - ETA: 31s - loss: 3.2838 - accuracy: 0.144 - ETA: 31s - loss: 3.2835 - accuracy: 0.144 - ETA: 30s - loss: 3.2826 - accuracy: 0.144 - ETA: 30s - loss: 3.2828 - accuracy: 0.144 - ETA: 29s - loss: 3.2828 - accuracy: 0.144 - ETA: 29s - loss: 3.2826 - accuracy: 0.144 - ETA: 29s - loss: 3.2823 - accuracy: 0.144 - ETA: 28s - loss: 3.2822 - accuracy: 0.144 - ETA: 28s - loss: 3.2808 - accuracy: 0.144 - ETA: 27s - loss: 3.2806 - accuracy: 0.144 - ETA: 27s - loss: 3.2802 - accuracy: 0.144 - ETA: 26s - loss: 3.2795 - accuracy: 0.144 - ETA: 26s - loss: 3.2789 - accuracy: 0.144 - ETA: 26s - loss: 3.2792 - accuracy: 0.144 - ETA: 25s - loss: 3.2788 - accuracy: 0.144 - ETA: 25s - loss: 3.2789 - accuracy: 0.144 - ETA: 24s - loss: 3.2786 - accuracy: 0.145 - ETA: 24s - loss: 3.2777 - accuracy: 0.145 - ETA: 23s - loss: 3.2774 - accuracy: 0.145 - ETA: 23s - loss: 3.2782 - accuracy: 0.145 - ETA: 22s - loss: 3.2776 - accuracy: 0.145 - ETA: 22s - loss: 3.2772 - accuracy: 0.145 - ETA: 22s - loss: 3.2778 - accuracy: 0.145 - ETA: 21s - loss: 3.2774 - accuracy: 0.145 - ETA: 21s - loss: 3.2774 - accuracy: 0.145 - ETA: 20s - loss: 3.2768 - accuracy: 0.145 - ETA: 20s - loss: 3.2764 - accuracy: 0.145 - ETA: 19s - loss: 3.2756 - accuracy: 0.145 - ETA: 19s - loss: 3.2753 - accuracy: 0.145 - ETA: 19s - loss: 3.2748 - accuracy: 0.145 - ETA: 18s - loss: 3.2749 - accuracy: 0.145 - ETA: 18s - loss: 3.2750 - accuracy: 0.146 - ETA: 17s - loss: 3.2741 - accuracy: 0.146 - ETA: 17s - loss: 3.2739 - accuracy: 0.146 - ETA: 16s - loss: 3.2740 - accuracy: 0.146 - ETA: 16s - loss: 3.2744 - accuracy: 0.146 - ETA: 15s - loss: 3.2749 - accuracy: 0.146 - ETA: 15s - loss: 3.2752 - accuracy: 0.146 - ETA: 14s - loss: 3.2752 - accuracy: 0.146 - ETA: 14s - loss: 3.2752 - accuracy: 0.146 - ETA: 14s - loss: 3.2754 - accuracy: 0.146 - ETA: 13s - loss: 3.2754 - accuracy: 0.146 - ETA: 13s - loss: 3.2754 - accuracy: 0.146 - ETA: 12s - loss: 3.2756 - accuracy: 0.146 - ETA: 12s - loss: 3.2759 - accuracy: 0.146 - ETA: 11s - loss: 3.2758 - accuracy: 0.146 - ETA: 11s - loss: 3.2760 - accuracy: 0.146 - ETA: 10s - loss: 3.2759 - accuracy: 0.145 - ETA: 10s - loss: 3.2766 - accuracy: 0.146 - ETA: 10s - loss: 3.2766 - accuracy: 0.145 - ETA: 9s - loss: 3.2769 - accuracy: 0.145 - ETA: 9s - loss: 3.2766 - accuracy: 0.14 - ETA: 8s - loss: 3.2765 - accuracy: 0.14 - ETA: 8s - loss: 3.2764 - accuracy: 0.14 - ETA: 7s - loss: 3.2767 - accuracy: 0.14 - ETA: 7s - loss: 3.2769 - accuracy: 0.14 - ETA: 6s - loss: 3.2766 - accuracy: 0.14 - ETA: 6s - loss: 3.2765 - accuracy: 0.14 - ETA: 5s - loss: 3.2779 - accuracy: 0.14 - ETA: 5s - loss: 3.2777 - accuracy: 0.14 - ETA: 5s - loss: 3.2785 - accuracy: 0.14 - ETA: 4s - loss: 3.2787 - accuracy: 0.14 - ETA: 4s - loss: 3.2787 - accuracy: 0.14 - ETA: 3s - loss: 3.2788 - accuracy: 0.14 - ETA: 3s - loss: 3.2790 - accuracy: 0.14 - ETA: 2s - loss: 3.2792 - accuracy: 0.14 - ETA: 2s - loss: 3.2795 - accuracy: 0.14 - ETA: 1s - loss: 3.2800 - accuracy: 0.14 - ETA: 1s - loss: 3.2802 - accuracy: 0.14 - ETA: 0s - loss: 3.2806 - accuracy: 0.14 - ETA: 0s - loss: 3.2809 - accuracy: 0.14 - ETA: 0s - loss: 3.2809 - accuracy: 0.14 - 161s 4ms/step - loss: 3.2810 - accuracy: 0.1461 - val_loss: 3.9842 - val_accuracy: 0.0277\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:42 - loss: 3.3978 - accuracy: 0.14 - ETA: 2:32 - loss: 3.3570 - accuracy: 0.16 - ETA: 2:28 - loss: 3.3370 - accuracy: 0.16 - ETA: 2:25 - loss: 3.3275 - accuracy: 0.16 - ETA: 2:24 - loss: 3.3392 - accuracy: 0.16 - ETA: 2:25 - loss: 3.3312 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2909 - accuracy: 0.16 - ETA: 2:26 - loss: 3.2965 - accuracy: 0.16 - ETA: 2:26 - loss: 3.3128 - accuracy: 0.15 - ETA: 2:27 - loss: 3.3124 - accuracy: 0.15 - ETA: 2:26 - loss: 3.3004 - accuracy: 0.16 - ETA: 2:26 - loss: 3.3011 - accuracy: 0.15 - ETA: 2:26 - loss: 3.3076 - accuracy: 0.15 - ETA: 2:26 - loss: 3.3102 - accuracy: 0.15 - ETA: 2:26 - loss: 3.3097 - accuracy: 0.15 - ETA: 2:25 - loss: 3.3062 - accuracy: 0.15 - ETA: 2:25 - loss: 3.3080 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2936 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2918 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2919 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2863 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2898 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2912 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2921 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2905 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2832 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2767 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2776 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2809 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2798 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2831 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2867 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2854 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2851 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2837 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2831 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2802 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2785 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2728 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2715 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2674 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2672 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2647 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2629 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2633 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2680 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2660 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2604 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2603 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2561 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2537 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2512 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2520 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2523 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2511 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2498 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2503 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2535 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2572 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2589 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2566 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2570 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2552 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2496 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2476 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2500 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2485 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2506 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2503 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2509 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2518 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2508 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2504 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2489 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2494 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2494 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2493 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2466 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2452 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2450 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2437 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2452 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2460 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2473 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2491 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2485 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2481 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2473 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2484 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2489 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2470 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2461 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2465 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2463 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2483 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2471 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2474 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2468 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2484 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2472 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2468 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2473 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2475 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2482 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2486 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2479 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2481 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2497 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2490 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2494 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2498 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2497 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2497 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2495 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2485 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2495 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2507 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2498 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2495 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2497 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2490 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2477 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2460 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2453 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2455 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2456 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2446 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2464 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2447 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2442 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2431 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2433 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2425 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2442 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2443 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2443 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2455 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2466 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2479 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2486 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2486 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2474 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2469 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2472 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2469 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2472 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2465 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2463 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2457 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2461 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2463 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2467 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2463 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2460 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2445 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2445 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2453 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2451 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2445 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2449 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2454 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2462 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2473 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2475 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2473 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2461 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2457 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2459 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2457 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2468 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2468 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2465 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2486 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2478 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2475 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2478 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2478 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2468 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2463 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2464 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2465 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2457 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2457 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2466 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2466 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2471 - accuracy: 0.1522"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:04 - loss: 3.2466 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2460 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2458 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2459 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2459 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2466 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2466 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2449 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2446 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2455 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2450 - accuracy: 0.15 - ETA: 59s - loss: 3.2446 - accuracy: 0.1532 - ETA: 59s - loss: 3.2441 - accuracy: 0.153 - ETA: 58s - loss: 3.2440 - accuracy: 0.153 - ETA: 58s - loss: 3.2444 - accuracy: 0.153 - ETA: 57s - loss: 3.2441 - accuracy: 0.153 - ETA: 57s - loss: 3.2432 - accuracy: 0.153 - ETA: 57s - loss: 3.2426 - accuracy: 0.153 - ETA: 56s - loss: 3.2427 - accuracy: 0.153 - ETA: 56s - loss: 3.2422 - accuracy: 0.153 - ETA: 55s - loss: 3.2419 - accuracy: 0.153 - ETA: 55s - loss: 3.2420 - accuracy: 0.153 - ETA: 54s - loss: 3.2426 - accuracy: 0.153 - ETA: 54s - loss: 3.2424 - accuracy: 0.153 - ETA: 53s - loss: 3.2418 - accuracy: 0.153 - ETA: 53s - loss: 3.2410 - accuracy: 0.153 - ETA: 52s - loss: 3.2407 - accuracy: 0.153 - ETA: 52s - loss: 3.2400 - accuracy: 0.154 - ETA: 52s - loss: 3.2392 - accuracy: 0.154 - ETA: 51s - loss: 3.2396 - accuracy: 0.154 - ETA: 51s - loss: 3.2391 - accuracy: 0.154 - ETA: 50s - loss: 3.2391 - accuracy: 0.154 - ETA: 50s - loss: 3.2390 - accuracy: 0.154 - ETA: 49s - loss: 3.2386 - accuracy: 0.154 - ETA: 49s - loss: 3.2386 - accuracy: 0.154 - ETA: 48s - loss: 3.2380 - accuracy: 0.154 - ETA: 48s - loss: 3.2379 - accuracy: 0.153 - ETA: 47s - loss: 3.2378 - accuracy: 0.153 - ETA: 47s - loss: 3.2371 - accuracy: 0.154 - ETA: 47s - loss: 3.2373 - accuracy: 0.154 - ETA: 46s - loss: 3.2363 - accuracy: 0.154 - ETA: 46s - loss: 3.2356 - accuracy: 0.154 - ETA: 45s - loss: 3.2357 - accuracy: 0.154 - ETA: 45s - loss: 3.2362 - accuracy: 0.154 - ETA: 44s - loss: 3.2354 - accuracy: 0.154 - ETA: 44s - loss: 3.2358 - accuracy: 0.154 - ETA: 43s - loss: 3.2358 - accuracy: 0.154 - ETA: 43s - loss: 3.2358 - accuracy: 0.154 - ETA: 43s - loss: 3.2360 - accuracy: 0.154 - ETA: 42s - loss: 3.2367 - accuracy: 0.154 - ETA: 42s - loss: 3.2363 - accuracy: 0.154 - ETA: 41s - loss: 3.2371 - accuracy: 0.154 - ETA: 41s - loss: 3.2373 - accuracy: 0.154 - ETA: 40s - loss: 3.2380 - accuracy: 0.154 - ETA: 40s - loss: 3.2382 - accuracy: 0.154 - ETA: 39s - loss: 3.2384 - accuracy: 0.154 - ETA: 39s - loss: 3.2383 - accuracy: 0.154 - ETA: 39s - loss: 3.2377 - accuracy: 0.154 - ETA: 38s - loss: 3.2370 - accuracy: 0.154 - ETA: 38s - loss: 3.2369 - accuracy: 0.154 - ETA: 37s - loss: 3.2373 - accuracy: 0.154 - ETA: 37s - loss: 3.2378 - accuracy: 0.154 - ETA: 36s - loss: 3.2381 - accuracy: 0.154 - ETA: 36s - loss: 3.2380 - accuracy: 0.154 - ETA: 35s - loss: 3.2380 - accuracy: 0.154 - ETA: 35s - loss: 3.2377 - accuracy: 0.154 - ETA: 34s - loss: 3.2374 - accuracy: 0.154 - ETA: 34s - loss: 3.2371 - accuracy: 0.155 - ETA: 34s - loss: 3.2369 - accuracy: 0.155 - ETA: 33s - loss: 3.2366 - accuracy: 0.155 - ETA: 33s - loss: 3.2366 - accuracy: 0.154 - ETA: 32s - loss: 3.2366 - accuracy: 0.155 - ETA: 32s - loss: 3.2365 - accuracy: 0.155 - ETA: 31s - loss: 3.2365 - accuracy: 0.155 - ETA: 31s - loss: 3.2361 - accuracy: 0.154 - ETA: 30s - loss: 3.2356 - accuracy: 0.155 - ETA: 30s - loss: 3.2350 - accuracy: 0.155 - ETA: 30s - loss: 3.2351 - accuracy: 0.155 - ETA: 29s - loss: 3.2344 - accuracy: 0.155 - ETA: 29s - loss: 3.2335 - accuracy: 0.155 - ETA: 28s - loss: 3.2338 - accuracy: 0.155 - ETA: 28s - loss: 3.2337 - accuracy: 0.155 - ETA: 27s - loss: 3.2341 - accuracy: 0.155 - ETA: 27s - loss: 3.2343 - accuracy: 0.155 - ETA: 26s - loss: 3.2345 - accuracy: 0.155 - ETA: 26s - loss: 3.2340 - accuracy: 0.155 - ETA: 26s - loss: 3.2338 - accuracy: 0.155 - ETA: 25s - loss: 3.2334 - accuracy: 0.155 - ETA: 25s - loss: 3.2333 - accuracy: 0.155 - ETA: 24s - loss: 3.2339 - accuracy: 0.155 - ETA: 24s - loss: 3.2341 - accuracy: 0.155 - ETA: 23s - loss: 3.2344 - accuracy: 0.155 - ETA: 23s - loss: 3.2344 - accuracy: 0.155 - ETA: 22s - loss: 3.2342 - accuracy: 0.155 - ETA: 22s - loss: 3.2346 - accuracy: 0.155 - ETA: 21s - loss: 3.2344 - accuracy: 0.155 - ETA: 21s - loss: 3.2350 - accuracy: 0.155 - ETA: 21s - loss: 3.2353 - accuracy: 0.155 - ETA: 20s - loss: 3.2355 - accuracy: 0.154 - ETA: 20s - loss: 3.2357 - accuracy: 0.155 - ETA: 19s - loss: 3.2355 - accuracy: 0.155 - ETA: 19s - loss: 3.2369 - accuracy: 0.154 - ETA: 18s - loss: 3.2377 - accuracy: 0.155 - ETA: 18s - loss: 3.2394 - accuracy: 0.155 - ETA: 17s - loss: 3.2392 - accuracy: 0.155 - ETA: 17s - loss: 3.2391 - accuracy: 0.155 - ETA: 17s - loss: 3.2389 - accuracy: 0.155 - ETA: 16s - loss: 3.2386 - accuracy: 0.155 - ETA: 16s - loss: 3.2381 - accuracy: 0.155 - ETA: 15s - loss: 3.2377 - accuracy: 0.155 - ETA: 15s - loss: 3.2383 - accuracy: 0.155 - ETA: 14s - loss: 3.2381 - accuracy: 0.155 - ETA: 14s - loss: 3.2376 - accuracy: 0.155 - ETA: 13s - loss: 3.2373 - accuracy: 0.155 - ETA: 13s - loss: 3.2371 - accuracy: 0.155 - ETA: 13s - loss: 3.2367 - accuracy: 0.155 - ETA: 12s - loss: 3.2371 - accuracy: 0.155 - ETA: 12s - loss: 3.2375 - accuracy: 0.155 - ETA: 11s - loss: 3.2371 - accuracy: 0.155 - ETA: 11s - loss: 3.2367 - accuracy: 0.156 - ETA: 10s - loss: 3.2373 - accuracy: 0.155 - ETA: 10s - loss: 3.2374 - accuracy: 0.155 - ETA: 9s - loss: 3.2373 - accuracy: 0.155 - ETA: 9s - loss: 3.2374 - accuracy: 0.15 - ETA: 8s - loss: 3.2374 - accuracy: 0.15 - ETA: 8s - loss: 3.2374 - accuracy: 0.15 - ETA: 8s - loss: 3.2372 - accuracy: 0.15 - ETA: 7s - loss: 3.2377 - accuracy: 0.15 - ETA: 7s - loss: 3.2378 - accuracy: 0.15 - ETA: 6s - loss: 3.2381 - accuracy: 0.15 - ETA: 6s - loss: 3.2385 - accuracy: 0.15 - ETA: 5s - loss: 3.2388 - accuracy: 0.15 - ETA: 5s - loss: 3.2389 - accuracy: 0.15 - ETA: 4s - loss: 3.2393 - accuracy: 0.15 - ETA: 4s - loss: 3.2399 - accuracy: 0.15 - ETA: 4s - loss: 3.2401 - accuracy: 0.15 - ETA: 3s - loss: 3.2402 - accuracy: 0.15 - ETA: 3s - loss: 3.2399 - accuracy: 0.15 - ETA: 2s - loss: 3.2396 - accuracy: 0.15 - ETA: 2s - loss: 3.2392 - accuracy: 0.15 - ETA: 1s - loss: 3.2396 - accuracy: 0.15 - ETA: 1s - loss: 3.2395 - accuracy: 0.15 - ETA: 0s - loss: 3.2401 - accuracy: 0.15 - ETA: 0s - loss: 3.2403 - accuracy: 0.15 - ETA: 0s - loss: 3.2403 - accuracy: 0.15 - 158s 4ms/step - loss: 3.2404 - accuracy: 0.1558 - val_loss: 4.1204 - val_accuracy: 0.0271\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:51 - loss: 3.3171 - accuracy: 0.13 - ETA: 2:41 - loss: 3.2225 - accuracy: 0.17 - ETA: 2:34 - loss: 3.1942 - accuracy: 0.16 - ETA: 2:30 - loss: 3.2311 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2440 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2521 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3091 - accuracy: 0.14 - ETA: 2:23 - loss: 3.3049 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2981 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2931 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2819 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2891 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2956 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2932 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2983 - accuracy: 0.14 - ETA: 2:19 - loss: 3.3004 - accuracy: 0.14 - ETA: 2:18 - loss: 3.3026 - accuracy: 0.14 - ETA: 2:18 - loss: 3.3096 - accuracy: 0.14 - ETA: 2:17 - loss: 3.3081 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3043 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3076 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3046 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3051 - accuracy: 0.13 - ETA: 2:16 - loss: 3.2932 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2913 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2883 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2961 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2940 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2979 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2945 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2923 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2913 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2924 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2914 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2911 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2884 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2847 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2819 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2805 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2764 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2702 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2747 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2735 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2731 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2714 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2730 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2712 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2663 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2629 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2581 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2565 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2574 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2583 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2608 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2613 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2593 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2578 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2575 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2555 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2529 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2520 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2509 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2471 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2473 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2464 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2426 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2431 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2447 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2461 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2437 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2413 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2407 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2396 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2390 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2367 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2355 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2360 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2335 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2310 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2262 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2258 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2271 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2263 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2267 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2273 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2263 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2269 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2261 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2298 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2339 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2385 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2408 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2461 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2473 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2564 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2548 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2564 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2552 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2535 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2532 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2529 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2533 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2542 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2534 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2542 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2564 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2598 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2608 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2604 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2610 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2618 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2624 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2624 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2630 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2639 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2636 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2631 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2642 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2630 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2633 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2614 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2624 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2626 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2609 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2601 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2598 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2607 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2609 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2609 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2615 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2603 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2602 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2600 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2588 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2591 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2592 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2597 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2602 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2604 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2605 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2610 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2610 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2607 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2602 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2588 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2583 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2583 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2597 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2600 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2614 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2622 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2622 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2622 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2623 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2631 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2630 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2625 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2625 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2626 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2622 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2630 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2624 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2625 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2620 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2625 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2620 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2613 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2607 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2615 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2607 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2606 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2607 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2601 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2601 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2610 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2612 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2612 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2609 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2603 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2603 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2610 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2618 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2611 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2608 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2599 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2602 - accuracy: 0.1549"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:04 - loss: 3.2596 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2592 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2584 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2584 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2577 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2577 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2578 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2575 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2578 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2581 - accuracy: 0.15 - ETA: 59s - loss: 3.2584 - accuracy: 0.1552 - ETA: 59s - loss: 3.2584 - accuracy: 0.155 - ETA: 58s - loss: 3.2592 - accuracy: 0.154 - ETA: 58s - loss: 3.2596 - accuracy: 0.154 - ETA: 57s - loss: 3.2591 - accuracy: 0.154 - ETA: 57s - loss: 3.2591 - accuracy: 0.154 - ETA: 57s - loss: 3.2590 - accuracy: 0.154 - ETA: 56s - loss: 3.2597 - accuracy: 0.154 - ETA: 56s - loss: 3.2598 - accuracy: 0.154 - ETA: 55s - loss: 3.2600 - accuracy: 0.154 - ETA: 55s - loss: 3.2604 - accuracy: 0.153 - ETA: 54s - loss: 3.2607 - accuracy: 0.153 - ETA: 54s - loss: 3.2605 - accuracy: 0.153 - ETA: 54s - loss: 3.2598 - accuracy: 0.154 - ETA: 53s - loss: 3.2599 - accuracy: 0.154 - ETA: 53s - loss: 3.2602 - accuracy: 0.154 - ETA: 52s - loss: 3.2601 - accuracy: 0.154 - ETA: 52s - loss: 3.2607 - accuracy: 0.154 - ETA: 51s - loss: 3.2609 - accuracy: 0.154 - ETA: 51s - loss: 3.2611 - accuracy: 0.154 - ETA: 50s - loss: 3.2606 - accuracy: 0.154 - ETA: 50s - loss: 3.2608 - accuracy: 0.154 - ETA: 50s - loss: 3.2613 - accuracy: 0.154 - ETA: 49s - loss: 3.2608 - accuracy: 0.154 - ETA: 49s - loss: 3.2608 - accuracy: 0.154 - ETA: 48s - loss: 3.2616 - accuracy: 0.153 - ETA: 48s - loss: 3.2609 - accuracy: 0.153 - ETA: 47s - loss: 3.2594 - accuracy: 0.154 - ETA: 47s - loss: 3.2594 - accuracy: 0.154 - ETA: 46s - loss: 3.2590 - accuracy: 0.154 - ETA: 46s - loss: 3.2597 - accuracy: 0.154 - ETA: 46s - loss: 3.2591 - accuracy: 0.154 - ETA: 45s - loss: 3.2589 - accuracy: 0.154 - ETA: 45s - loss: 3.2584 - accuracy: 0.154 - ETA: 44s - loss: 3.2579 - accuracy: 0.154 - ETA: 44s - loss: 3.2577 - accuracy: 0.154 - ETA: 43s - loss: 3.2580 - accuracy: 0.154 - ETA: 43s - loss: 3.2582 - accuracy: 0.154 - ETA: 42s - loss: 3.2583 - accuracy: 0.154 - ETA: 42s - loss: 3.2583 - accuracy: 0.154 - ETA: 42s - loss: 3.2574 - accuracy: 0.154 - ETA: 41s - loss: 3.2569 - accuracy: 0.154 - ETA: 41s - loss: 3.2567 - accuracy: 0.154 - ETA: 40s - loss: 3.2571 - accuracy: 0.154 - ETA: 40s - loss: 3.2564 - accuracy: 0.155 - ETA: 39s - loss: 3.2565 - accuracy: 0.155 - ETA: 39s - loss: 3.2568 - accuracy: 0.155 - ETA: 38s - loss: 3.2560 - accuracy: 0.155 - ETA: 38s - loss: 3.2559 - accuracy: 0.155 - ETA: 38s - loss: 3.2560 - accuracy: 0.155 - ETA: 37s - loss: 3.2554 - accuracy: 0.155 - ETA: 37s - loss: 3.2557 - accuracy: 0.155 - ETA: 36s - loss: 3.2552 - accuracy: 0.155 - ETA: 36s - loss: 3.2554 - accuracy: 0.155 - ETA: 35s - loss: 3.2557 - accuracy: 0.155 - ETA: 35s - loss: 3.2561 - accuracy: 0.154 - ETA: 34s - loss: 3.2566 - accuracy: 0.154 - ETA: 34s - loss: 3.2572 - accuracy: 0.154 - ETA: 34s - loss: 3.2569 - accuracy: 0.154 - ETA: 33s - loss: 3.2571 - accuracy: 0.154 - ETA: 33s - loss: 3.2568 - accuracy: 0.154 - ETA: 32s - loss: 3.2565 - accuracy: 0.154 - ETA: 32s - loss: 3.2562 - accuracy: 0.155 - ETA: 31s - loss: 3.2566 - accuracy: 0.154 - ETA: 31s - loss: 3.2562 - accuracy: 0.155 - ETA: 30s - loss: 3.2561 - accuracy: 0.155 - ETA: 30s - loss: 3.2571 - accuracy: 0.155 - ETA: 30s - loss: 3.2591 - accuracy: 0.155 - ETA: 29s - loss: 3.2589 - accuracy: 0.155 - ETA: 29s - loss: 3.2590 - accuracy: 0.155 - ETA: 28s - loss: 3.2589 - accuracy: 0.154 - ETA: 28s - loss: 3.2589 - accuracy: 0.154 - ETA: 27s - loss: 3.2591 - accuracy: 0.154 - ETA: 27s - loss: 3.2593 - accuracy: 0.154 - ETA: 26s - loss: 3.2593 - accuracy: 0.154 - ETA: 26s - loss: 3.2596 - accuracy: 0.154 - ETA: 25s - loss: 3.2594 - accuracy: 0.154 - ETA: 25s - loss: 3.2603 - accuracy: 0.154 - ETA: 25s - loss: 3.2606 - accuracy: 0.154 - ETA: 24s - loss: 3.2611 - accuracy: 0.154 - ETA: 24s - loss: 3.2620 - accuracy: 0.153 - ETA: 23s - loss: 3.2627 - accuracy: 0.153 - ETA: 23s - loss: 3.2630 - accuracy: 0.153 - ETA: 22s - loss: 3.2638 - accuracy: 0.153 - ETA: 22s - loss: 3.2641 - accuracy: 0.153 - ETA: 21s - loss: 3.2651 - accuracy: 0.153 - ETA: 21s - loss: 3.2654 - accuracy: 0.153 - ETA: 21s - loss: 3.2656 - accuracy: 0.153 - ETA: 20s - loss: 3.2650 - accuracy: 0.153 - ETA: 20s - loss: 3.2653 - accuracy: 0.153 - ETA: 19s - loss: 3.2655 - accuracy: 0.153 - ETA: 19s - loss: 3.2658 - accuracy: 0.153 - ETA: 18s - loss: 3.2659 - accuracy: 0.153 - ETA: 18s - loss: 3.2658 - accuracy: 0.153 - ETA: 17s - loss: 3.2661 - accuracy: 0.153 - ETA: 17s - loss: 3.2660 - accuracy: 0.153 - ETA: 17s - loss: 3.2659 - accuracy: 0.153 - ETA: 16s - loss: 3.2659 - accuracy: 0.153 - ETA: 16s - loss: 3.2663 - accuracy: 0.153 - ETA: 15s - loss: 3.2659 - accuracy: 0.153 - ETA: 15s - loss: 3.2664 - accuracy: 0.153 - ETA: 14s - loss: 3.2671 - accuracy: 0.153 - ETA: 14s - loss: 3.2674 - accuracy: 0.153 - ETA: 13s - loss: 3.2672 - accuracy: 0.153 - ETA: 13s - loss: 3.2675 - accuracy: 0.153 - ETA: 12s - loss: 3.2681 - accuracy: 0.153 - ETA: 12s - loss: 3.2685 - accuracy: 0.152 - ETA: 12s - loss: 3.2684 - accuracy: 0.153 - ETA: 11s - loss: 3.2686 - accuracy: 0.153 - ETA: 11s - loss: 3.2683 - accuracy: 0.153 - ETA: 10s - loss: 3.2681 - accuracy: 0.153 - ETA: 10s - loss: 3.2685 - accuracy: 0.153 - ETA: 9s - loss: 3.2691 - accuracy: 0.153 - ETA: 9s - loss: 3.2687 - accuracy: 0.15 - ETA: 8s - loss: 3.2688 - accuracy: 0.15 - ETA: 8s - loss: 3.2688 - accuracy: 0.15 - ETA: 8s - loss: 3.2686 - accuracy: 0.15 - ETA: 7s - loss: 3.2694 - accuracy: 0.15 - ETA: 7s - loss: 3.2695 - accuracy: 0.15 - ETA: 6s - loss: 3.2697 - accuracy: 0.15 - ETA: 6s - loss: 3.2697 - accuracy: 0.15 - ETA: 5s - loss: 3.2693 - accuracy: 0.15 - ETA: 5s - loss: 3.2696 - accuracy: 0.15 - ETA: 4s - loss: 3.2701 - accuracy: 0.15 - ETA: 4s - loss: 3.2693 - accuracy: 0.15 - ETA: 4s - loss: 3.2705 - accuracy: 0.15 - ETA: 3s - loss: 3.2704 - accuracy: 0.15 - ETA: 3s - loss: 3.2702 - accuracy: 0.15 - ETA: 2s - loss: 3.2703 - accuracy: 0.15 - ETA: 2s - loss: 3.2706 - accuracy: 0.15 - ETA: 1s - loss: 3.2705 - accuracy: 0.15 - ETA: 1s - loss: 3.2707 - accuracy: 0.15 - ETA: 0s - loss: 3.2708 - accuracy: 0.15 - ETA: 0s - loss: 3.2705 - accuracy: 0.15 - ETA: 0s - loss: 3.2704 - accuracy: 0.15 - 159s 4ms/step - loss: 3.2705 - accuracy: 0.1533 - val_loss: 3.9981 - val_accuracy: 0.0190\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:46 - loss: 3.2935 - accuracy: 0.14 - ETA: 2:39 - loss: 3.1778 - accuracy: 0.15 - ETA: 2:38 - loss: 3.2335 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2565 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2852 - accuracy: 0.13 - ETA: 2:36 - loss: 3.2973 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3145 - accuracy: 0.12 - ETA: 2:31 - loss: 3.3196 - accuracy: 0.12 - ETA: 2:30 - loss: 3.3092 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3221 - accuracy: 0.12 - ETA: 2:27 - loss: 3.3197 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3125 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3165 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3075 - accuracy: 0.13 - ETA: 2:24 - loss: 3.2906 - accuracy: 0.13 - ETA: 2:24 - loss: 3.2969 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3031 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3048 - accuracy: 0.13 - ETA: 2:21 - loss: 3.2961 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3033 - accuracy: 0.13 - ETA: 2:20 - loss: 3.2952 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3087 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3090 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3043 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3045 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3093 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3078 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3117 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3071 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3127 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3113 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3140 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3089 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3084 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3043 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3026 - accuracy: 0.14 - ETA: 2:12 - loss: 3.3053 - accuracy: 0.14 - ETA: 2:12 - loss: 3.3082 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3100 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3074 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3059 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3051 - accuracy: 0.14 - ETA: 2:09 - loss: 3.3064 - accuracy: 0.14 - ETA: 2:09 - loss: 3.3055 - accuracy: 0.14 - ETA: 2:08 - loss: 3.3070 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3054 - accuracy: 0.14 - ETA: 2:07 - loss: 3.3051 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3018 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3059 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3072 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3064 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3051 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3045 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3033 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3084 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3078 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3068 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3069 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3056 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3082 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3068 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3046 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3033 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3022 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3039 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3014 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3023 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3041 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2997 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3016 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3004 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2988 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2971 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2970 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2985 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2993 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2955 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2950 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2953 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2957 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2963 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2952 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2961 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2965 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2944 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2965 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2954 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2956 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2942 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2970 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2975 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2973 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2973 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2974 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2972 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2969 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2948 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2963 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2955 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2970 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2976 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2966 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2980 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2972 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2957 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2967 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2971 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2953 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2941 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2934 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2933 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2933 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2941 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2935 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2924 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2931 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2919 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2914 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2904 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2880 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2883 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2890 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2885 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2884 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2867 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2868 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2877 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2880 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2874 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2875 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2877 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2866 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2892 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2896 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2903 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2927 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2930 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2936 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2959 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2968 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2960 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2952 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2956 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2941 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2937 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2941 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2938 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2941 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2939 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2931 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2934 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2927 - accuracy: 0.14 - ETA: 1:20 - loss: 3.2930 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2923 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2924 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2928 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2933 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2927 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2929 - accuracy: 0.14 - ETA: 1:17 - loss: 3.2929 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2922 - accuracy: 0.14 - ETA: 1:16 - loss: 3.2915 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2918 - accuracy: 0.14 - ETA: 1:15 - loss: 3.2914 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2934 - accuracy: 0.14 - ETA: 1:14 - loss: 3.2956 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2987 - accuracy: 0.14 - ETA: 1:13 - loss: 3.2993 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2989 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2986 - accuracy: 0.14 - ETA: 1:12 - loss: 3.2986 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2985 - accuracy: 0.14 - ETA: 1:11 - loss: 3.2987 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2994 - accuracy: 0.14 - ETA: 1:10 - loss: 3.2996 - accuracy: 0.14 - ETA: 1:09 - loss: 3.2997 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3007 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3016 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3020 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3026 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3032 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3033 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3052 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3060 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3062 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3067 - accuracy: 0.1471"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:04 - loss: 3.3076 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3082 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3083 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3082 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3084 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3093 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3098 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3100 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3109 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3110 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3114 - accuracy: 0.14 - ETA: 59s - loss: 3.3117 - accuracy: 0.1458 - ETA: 59s - loss: 3.3123 - accuracy: 0.145 - ETA: 58s - loss: 3.3132 - accuracy: 0.145 - ETA: 58s - loss: 3.3132 - accuracy: 0.145 - ETA: 57s - loss: 3.3133 - accuracy: 0.145 - ETA: 57s - loss: 3.3128 - accuracy: 0.145 - ETA: 57s - loss: 3.3130 - accuracy: 0.145 - ETA: 56s - loss: 3.3130 - accuracy: 0.145 - ETA: 56s - loss: 3.3137 - accuracy: 0.145 - ETA: 55s - loss: 3.3135 - accuracy: 0.145 - ETA: 55s - loss: 3.3134 - accuracy: 0.145 - ETA: 54s - loss: 3.3140 - accuracy: 0.145 - ETA: 54s - loss: 3.3149 - accuracy: 0.145 - ETA: 53s - loss: 3.3145 - accuracy: 0.145 - ETA: 53s - loss: 3.3143 - accuracy: 0.145 - ETA: 53s - loss: 3.3148 - accuracy: 0.145 - ETA: 52s - loss: 3.3143 - accuracy: 0.145 - ETA: 52s - loss: 3.3135 - accuracy: 0.145 - ETA: 51s - loss: 3.3128 - accuracy: 0.145 - ETA: 51s - loss: 3.3130 - accuracy: 0.145 - ETA: 50s - loss: 3.3133 - accuracy: 0.145 - ETA: 50s - loss: 3.3126 - accuracy: 0.146 - ETA: 49s - loss: 3.3127 - accuracy: 0.145 - ETA: 49s - loss: 3.3126 - accuracy: 0.145 - ETA: 48s - loss: 3.3121 - accuracy: 0.146 - ETA: 48s - loss: 3.3120 - accuracy: 0.146 - ETA: 48s - loss: 3.3123 - accuracy: 0.146 - ETA: 47s - loss: 3.3115 - accuracy: 0.146 - ETA: 47s - loss: 3.3116 - accuracy: 0.146 - ETA: 46s - loss: 3.3119 - accuracy: 0.146 - ETA: 46s - loss: 3.3129 - accuracy: 0.146 - ETA: 45s - loss: 3.3126 - accuracy: 0.146 - ETA: 45s - loss: 3.3127 - accuracy: 0.146 - ETA: 44s - loss: 3.3120 - accuracy: 0.146 - ETA: 44s - loss: 3.3119 - accuracy: 0.146 - ETA: 44s - loss: 3.3121 - accuracy: 0.146 - ETA: 43s - loss: 3.3122 - accuracy: 0.146 - ETA: 43s - loss: 3.3124 - accuracy: 0.146 - ETA: 42s - loss: 3.3116 - accuracy: 0.146 - ETA: 42s - loss: 3.3109 - accuracy: 0.146 - ETA: 41s - loss: 3.3116 - accuracy: 0.145 - ETA: 41s - loss: 3.3112 - accuracy: 0.145 - ETA: 40s - loss: 3.3117 - accuracy: 0.145 - ETA: 40s - loss: 3.3122 - accuracy: 0.145 - ETA: 39s - loss: 3.3120 - accuracy: 0.145 - ETA: 39s - loss: 3.3121 - accuracy: 0.145 - ETA: 39s - loss: 3.3120 - accuracy: 0.145 - ETA: 38s - loss: 3.3118 - accuracy: 0.145 - ETA: 38s - loss: 3.3115 - accuracy: 0.145 - ETA: 37s - loss: 3.3113 - accuracy: 0.145 - ETA: 37s - loss: 3.3109 - accuracy: 0.146 - ETA: 36s - loss: 3.3107 - accuracy: 0.146 - ETA: 36s - loss: 3.3104 - accuracy: 0.146 - ETA: 35s - loss: 3.3100 - accuracy: 0.146 - ETA: 35s - loss: 3.3102 - accuracy: 0.146 - ETA: 35s - loss: 3.3099 - accuracy: 0.146 - ETA: 34s - loss: 3.3105 - accuracy: 0.146 - ETA: 34s - loss: 3.3105 - accuracy: 0.146 - ETA: 33s - loss: 3.3105 - accuracy: 0.146 - ETA: 33s - loss: 3.3104 - accuracy: 0.146 - ETA: 32s - loss: 3.3101 - accuracy: 0.146 - ETA: 32s - loss: 3.3096 - accuracy: 0.146 - ETA: 31s - loss: 3.3091 - accuracy: 0.146 - ETA: 31s - loss: 3.3101 - accuracy: 0.146 - ETA: 30s - loss: 3.3100 - accuracy: 0.146 - ETA: 30s - loss: 3.3100 - accuracy: 0.146 - ETA: 30s - loss: 3.3098 - accuracy: 0.146 - ETA: 29s - loss: 3.3094 - accuracy: 0.146 - ETA: 29s - loss: 3.3094 - accuracy: 0.146 - ETA: 28s - loss: 3.3092 - accuracy: 0.146 - ETA: 28s - loss: 3.3090 - accuracy: 0.146 - ETA: 27s - loss: 3.3094 - accuracy: 0.146 - ETA: 27s - loss: 3.3090 - accuracy: 0.146 - ETA: 26s - loss: 3.3086 - accuracy: 0.146 - ETA: 26s - loss: 3.3089 - accuracy: 0.146 - ETA: 26s - loss: 3.3087 - accuracy: 0.146 - ETA: 25s - loss: 3.3087 - accuracy: 0.146 - ETA: 25s - loss: 3.3083 - accuracy: 0.146 - ETA: 24s - loss: 3.3084 - accuracy: 0.146 - ETA: 24s - loss: 3.3084 - accuracy: 0.146 - ETA: 23s - loss: 3.3080 - accuracy: 0.146 - ETA: 23s - loss: 3.3080 - accuracy: 0.146 - ETA: 22s - loss: 3.3076 - accuracy: 0.147 - ETA: 22s - loss: 3.3076 - accuracy: 0.147 - ETA: 22s - loss: 3.3079 - accuracy: 0.146 - ETA: 21s - loss: 3.3080 - accuracy: 0.146 - ETA: 21s - loss: 3.3081 - accuracy: 0.146 - ETA: 20s - loss: 3.3084 - accuracy: 0.146 - ETA: 20s - loss: 3.3087 - accuracy: 0.146 - ETA: 19s - loss: 3.3090 - accuracy: 0.146 - ETA: 19s - loss: 3.3090 - accuracy: 0.146 - ETA: 18s - loss: 3.3094 - accuracy: 0.146 - ETA: 18s - loss: 3.3091 - accuracy: 0.146 - ETA: 17s - loss: 3.3090 - accuracy: 0.146 - ETA: 17s - loss: 3.3088 - accuracy: 0.146 - ETA: 17s - loss: 3.3081 - accuracy: 0.146 - ETA: 16s - loss: 3.3082 - accuracy: 0.146 - ETA: 16s - loss: 3.3077 - accuracy: 0.146 - ETA: 15s - loss: 3.3075 - accuracy: 0.146 - ETA: 15s - loss: 3.3076 - accuracy: 0.146 - ETA: 14s - loss: 3.3083 - accuracy: 0.146 - ETA: 14s - loss: 3.3081 - accuracy: 0.146 - ETA: 13s - loss: 3.3092 - accuracy: 0.146 - ETA: 13s - loss: 3.3089 - accuracy: 0.146 - ETA: 13s - loss: 3.3075 - accuracy: 0.147 - ETA: 12s - loss: 3.3073 - accuracy: 0.147 - ETA: 12s - loss: 3.3078 - accuracy: 0.147 - ETA: 11s - loss: 3.3084 - accuracy: 0.147 - ETA: 11s - loss: 3.3089 - accuracy: 0.147 - ETA: 10s - loss: 3.3093 - accuracy: 0.147 - ETA: 10s - loss: 3.3087 - accuracy: 0.147 - ETA: 9s - loss: 3.3084 - accuracy: 0.147 - ETA: 9s - loss: 3.3083 - accuracy: 0.14 - ETA: 9s - loss: 3.3082 - accuracy: 0.14 - ETA: 8s - loss: 3.3083 - accuracy: 0.14 - ETA: 8s - loss: 3.3083 - accuracy: 0.14 - ETA: 7s - loss: 3.3082 - accuracy: 0.14 - ETA: 7s - loss: 3.3081 - accuracy: 0.14 - ETA: 6s - loss: 3.3082 - accuracy: 0.14 - ETA: 6s - loss: 3.3084 - accuracy: 0.14 - ETA: 5s - loss: 3.3086 - accuracy: 0.14 - ETA: 5s - loss: 3.3085 - accuracy: 0.14 - ETA: 4s - loss: 3.3083 - accuracy: 0.14 - ETA: 4s - loss: 3.3086 - accuracy: 0.14 - ETA: 4s - loss: 3.3088 - accuracy: 0.14 - ETA: 3s - loss: 3.3084 - accuracy: 0.14 - ETA: 3s - loss: 3.3080 - accuracy: 0.14 - ETA: 2s - loss: 3.3080 - accuracy: 0.14 - ETA: 2s - loss: 3.3074 - accuracy: 0.14 - ETA: 1s - loss: 3.3071 - accuracy: 0.14 - ETA: 1s - loss: 3.3067 - accuracy: 0.14 - ETA: 0s - loss: 3.3062 - accuracy: 0.14 - ETA: 0s - loss: 3.3059 - accuracy: 0.14 - ETA: 0s - loss: 3.3061 - accuracy: 0.14 - 159s 4ms/step - loss: 3.3062 - accuracy: 0.1469 - val_loss: 4.2669 - val_accuracy: 0.0291\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:49 - loss: 3.5354 - accuracy: 0.11 - ETA: 2:43 - loss: 3.4686 - accuracy: 0.11 - ETA: 2:42 - loss: 3.3985 - accuracy: 0.12 - ETA: 2:39 - loss: 3.3515 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3454 - accuracy: 0.14 - ETA: 2:39 - loss: 3.3210 - accuracy: 0.14 - ETA: 2:39 - loss: 3.3047 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2919 - accuracy: 0.15 - ETA: 2:36 - loss: 3.2954 - accuracy: 0.15 - ETA: 2:35 - loss: 3.3017 - accuracy: 0.15 - ETA: 2:34 - loss: 3.2854 - accuracy: 0.15 - ETA: 2:34 - loss: 3.2781 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2693 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2775 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2787 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2940 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2989 - accuracy: 0.15 - ETA: 2:32 - loss: 3.2969 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2891 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2916 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2892 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2964 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2899 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2900 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2811 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2800 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2814 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2815 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2835 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2794 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2776 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2709 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2701 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2665 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2727 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2774 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2716 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2665 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2655 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2662 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2692 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2732 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2758 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2752 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2766 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2762 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2750 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2714 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2749 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2768 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2794 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2817 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2793 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2806 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2809 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2816 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2811 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2822 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2795 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2820 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2808 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2816 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2807 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2816 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2832 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2841 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2837 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2863 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2877 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2870 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2877 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2904 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2911 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2936 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2939 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2930 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2903 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2904 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2926 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2917 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2905 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2913 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2903 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2900 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2889 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2909 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2911 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2919 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2917 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2921 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2899 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2909 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2905 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2888 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2893 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2877 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2865 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2849 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2842 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2834 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2840 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2840 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2814 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2817 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2814 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2819 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2836 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2838 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2811 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2810 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2824 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2826 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2840 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2851 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2832 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2819 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2819 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2819 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2827 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2825 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2822 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2822 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2821 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2805 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2808 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2814 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2820 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2828 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2810 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2803 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2811 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2802 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2777 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2786 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2793 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2800 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2811 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2804 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2800 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2788 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2774 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2759 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2755 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2753 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2757 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2751 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2747 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2747 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2745 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2738 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2738 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2741 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2744 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2727 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2719 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2709 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2702 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2703 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2697 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2695 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2683 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2676 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2670 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2665 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2669 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2671 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2681 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2683 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2685 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2680 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2680 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2679 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2684 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2679 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2670 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2675 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2657 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2660 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2658 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2651 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2653 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2640 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2636 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2633 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2634 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2632 - accuracy: 0.1560"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:06 - loss: 3.2629 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2625 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2618 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2621 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2619 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2626 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2622 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2618 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2614 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2613 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2622 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2616 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2622 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2627 - accuracy: 0.15 - ETA: 59s - loss: 3.2629 - accuracy: 0.1555 - ETA: 59s - loss: 3.2635 - accuracy: 0.155 - ETA: 59s - loss: 3.2627 - accuracy: 0.155 - ETA: 58s - loss: 3.2637 - accuracy: 0.155 - ETA: 58s - loss: 3.2639 - accuracy: 0.155 - ETA: 57s - loss: 3.2641 - accuracy: 0.155 - ETA: 57s - loss: 3.2633 - accuracy: 0.155 - ETA: 56s - loss: 3.2641 - accuracy: 0.155 - ETA: 56s - loss: 3.2644 - accuracy: 0.154 - ETA: 55s - loss: 3.2646 - accuracy: 0.154 - ETA: 55s - loss: 3.2651 - accuracy: 0.154 - ETA: 54s - loss: 3.2645 - accuracy: 0.155 - ETA: 54s - loss: 3.2640 - accuracy: 0.155 - ETA: 53s - loss: 3.2638 - accuracy: 0.155 - ETA: 53s - loss: 3.2642 - accuracy: 0.155 - ETA: 52s - loss: 3.2634 - accuracy: 0.155 - ETA: 52s - loss: 3.2627 - accuracy: 0.155 - ETA: 52s - loss: 3.2623 - accuracy: 0.155 - ETA: 51s - loss: 3.2620 - accuracy: 0.155 - ETA: 51s - loss: 3.2614 - accuracy: 0.155 - ETA: 50s - loss: 3.2671 - accuracy: 0.155 - ETA: 50s - loss: 3.2675 - accuracy: 0.155 - ETA: 49s - loss: 3.2676 - accuracy: 0.155 - ETA: 49s - loss: 3.2685 - accuracy: 0.155 - ETA: 48s - loss: 3.2679 - accuracy: 0.155 - ETA: 48s - loss: 3.2682 - accuracy: 0.155 - ETA: 47s - loss: 3.2685 - accuracy: 0.155 - ETA: 47s - loss: 3.2684 - accuracy: 0.155 - ETA: 46s - loss: 3.2687 - accuracy: 0.154 - ETA: 46s - loss: 3.2689 - accuracy: 0.154 - ETA: 46s - loss: 3.2695 - accuracy: 0.154 - ETA: 45s - loss: 3.2694 - accuracy: 0.154 - ETA: 45s - loss: 3.2691 - accuracy: 0.154 - ETA: 44s - loss: 3.2698 - accuracy: 0.154 - ETA: 44s - loss: 3.2696 - accuracy: 0.154 - ETA: 43s - loss: 3.2701 - accuracy: 0.154 - ETA: 43s - loss: 3.2704 - accuracy: 0.154 - ETA: 42s - loss: 3.2709 - accuracy: 0.154 - ETA: 42s - loss: 3.2714 - accuracy: 0.154 - ETA: 41s - loss: 3.2713 - accuracy: 0.154 - ETA: 41s - loss: 3.2719 - accuracy: 0.154 - ETA: 40s - loss: 3.2722 - accuracy: 0.153 - ETA: 40s - loss: 3.2725 - accuracy: 0.153 - ETA: 40s - loss: 3.2717 - accuracy: 0.153 - ETA: 39s - loss: 3.2716 - accuracy: 0.154 - ETA: 39s - loss: 3.2721 - accuracy: 0.153 - ETA: 38s - loss: 3.2726 - accuracy: 0.153 - ETA: 38s - loss: 3.2733 - accuracy: 0.153 - ETA: 37s - loss: 3.2737 - accuracy: 0.153 - ETA: 37s - loss: 3.2734 - accuracy: 0.153 - ETA: 36s - loss: 3.2734 - accuracy: 0.153 - ETA: 36s - loss: 3.2730 - accuracy: 0.153 - ETA: 35s - loss: 3.2733 - accuracy: 0.153 - ETA: 35s - loss: 3.2735 - accuracy: 0.153 - ETA: 34s - loss: 3.2735 - accuracy: 0.153 - ETA: 34s - loss: 3.2739 - accuracy: 0.153 - ETA: 33s - loss: 3.2742 - accuracy: 0.153 - ETA: 33s - loss: 3.2735 - accuracy: 0.153 - ETA: 33s - loss: 3.2729 - accuracy: 0.153 - ETA: 32s - loss: 3.2728 - accuracy: 0.153 - ETA: 32s - loss: 3.2723 - accuracy: 0.153 - ETA: 31s - loss: 3.2728 - accuracy: 0.153 - ETA: 31s - loss: 3.2734 - accuracy: 0.153 - ETA: 30s - loss: 3.2738 - accuracy: 0.153 - ETA: 30s - loss: 3.2737 - accuracy: 0.153 - ETA: 29s - loss: 3.2735 - accuracy: 0.153 - ETA: 29s - loss: 3.2737 - accuracy: 0.153 - ETA: 28s - loss: 3.2739 - accuracy: 0.153 - ETA: 28s - loss: 3.2742 - accuracy: 0.153 - ETA: 27s - loss: 3.2745 - accuracy: 0.153 - ETA: 27s - loss: 3.2747 - accuracy: 0.152 - ETA: 27s - loss: 3.2756 - accuracy: 0.152 - ETA: 26s - loss: 3.2758 - accuracy: 0.152 - ETA: 26s - loss: 3.2758 - accuracy: 0.152 - ETA: 25s - loss: 3.2754 - accuracy: 0.152 - ETA: 25s - loss: 3.2750 - accuracy: 0.152 - ETA: 24s - loss: 3.2756 - accuracy: 0.152 - ETA: 24s - loss: 3.2760 - accuracy: 0.152 - ETA: 23s - loss: 3.2761 - accuracy: 0.152 - ETA: 23s - loss: 3.2764 - accuracy: 0.152 - ETA: 22s - loss: 3.2762 - accuracy: 0.152 - ETA: 22s - loss: 3.2767 - accuracy: 0.152 - ETA: 21s - loss: 3.2769 - accuracy: 0.152 - ETA: 21s - loss: 3.2771 - accuracy: 0.152 - ETA: 21s - loss: 3.2774 - accuracy: 0.152 - ETA: 20s - loss: 3.2773 - accuracy: 0.151 - ETA: 20s - loss: 3.2768 - accuracy: 0.152 - ETA: 19s - loss: 3.2764 - accuracy: 0.152 - ETA: 19s - loss: 3.2761 - accuracy: 0.152 - ETA: 18s - loss: 3.2756 - accuracy: 0.152 - ETA: 18s - loss: 3.2762 - accuracy: 0.152 - ETA: 17s - loss: 3.2761 - accuracy: 0.152 - ETA: 17s - loss: 3.2763 - accuracy: 0.152 - ETA: 16s - loss: 3.2767 - accuracy: 0.151 - ETA: 16s - loss: 3.2772 - accuracy: 0.151 - ETA: 16s - loss: 3.2779 - accuracy: 0.151 - ETA: 15s - loss: 3.2776 - accuracy: 0.151 - ETA: 15s - loss: 3.2777 - accuracy: 0.151 - ETA: 14s - loss: 3.2776 - accuracy: 0.151 - ETA: 14s - loss: 3.2777 - accuracy: 0.151 - ETA: 13s - loss: 3.2778 - accuracy: 0.151 - ETA: 13s - loss: 3.2781 - accuracy: 0.151 - ETA: 12s - loss: 3.2784 - accuracy: 0.151 - ETA: 12s - loss: 3.2782 - accuracy: 0.151 - ETA: 11s - loss: 3.2779 - accuracy: 0.151 - ETA: 11s - loss: 3.2780 - accuracy: 0.151 - ETA: 10s - loss: 3.2777 - accuracy: 0.151 - ETA: 10s - loss: 3.2774 - accuracy: 0.151 - ETA: 10s - loss: 3.2775 - accuracy: 0.151 - ETA: 9s - loss: 3.2772 - accuracy: 0.151 - ETA: 9s - loss: 3.2767 - accuracy: 0.15 - ETA: 8s - loss: 3.2768 - accuracy: 0.15 - ETA: 8s - loss: 3.2767 - accuracy: 0.15 - ETA: 7s - loss: 3.2767 - accuracy: 0.15 - ETA: 7s - loss: 3.2767 - accuracy: 0.15 - ETA: 6s - loss: 3.2770 - accuracy: 0.15 - ETA: 6s - loss: 3.2766 - accuracy: 0.15 - ETA: 5s - loss: 3.2766 - accuracy: 0.15 - ETA: 5s - loss: 3.2766 - accuracy: 0.15 - ETA: 5s - loss: 3.2771 - accuracy: 0.15 - ETA: 4s - loss: 3.2778 - accuracy: 0.15 - ETA: 4s - loss: 3.2773 - accuracy: 0.15 - ETA: 3s - loss: 3.2781 - accuracy: 0.15 - ETA: 3s - loss: 3.2775 - accuracy: 0.15 - ETA: 2s - loss: 3.2778 - accuracy: 0.15 - ETA: 2s - loss: 3.2784 - accuracy: 0.15 - ETA: 1s - loss: 3.2787 - accuracy: 0.15 - ETA: 1s - loss: 3.2789 - accuracy: 0.15 - ETA: 0s - loss: 3.2788 - accuracy: 0.15 - ETA: 0s - loss: 3.2788 - accuracy: 0.15 - ETA: 0s - loss: 3.2791 - accuracy: 0.15 - 161s 4ms/step - loss: 3.2790 - accuracy: 0.1511 - val_loss: 4.4227 - val_accuracy: 0.0195\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:39 - loss: 3.2870 - accuracy: 0.16 - ETA: 2:30 - loss: 3.3508 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2919 - accuracy: 0.15 - ETA: 2:33 - loss: 3.3251 - accuracy: 0.15 - ETA: 2:31 - loss: 3.3220 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2962 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2853 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2599 - accuracy: 0.16 - ETA: 2:25 - loss: 3.2456 - accuracy: 0.16 - ETA: 2:24 - loss: 3.2450 - accuracy: 0.16 - ETA: 2:23 - loss: 3.2376 - accuracy: 0.16 - ETA: 2:22 - loss: 3.2552 - accuracy: 0.16 - ETA: 2:21 - loss: 3.2498 - accuracy: 0.16 - ETA: 2:21 - loss: 3.2466 - accuracy: 0.16 - ETA: 2:22 - loss: 3.2438 - accuracy: 0.16 - ETA: 2:22 - loss: 3.2674 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2861 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2842 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2781 - accuracy: 0.16 - ETA: 2:21 - loss: 3.2819 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2814 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2845 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2915 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2867 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2825 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2759 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2766 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2769 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2724 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2684 - accuracy: 0.16 - ETA: 2:16 - loss: 3.2657 - accuracy: 0.16 - ETA: 2:16 - loss: 3.2631 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2670 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2657 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2648 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2646 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2648 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2686 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2693 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2693 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2682 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2698 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2703 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2728 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2734 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2724 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2689 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2692 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2710 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2717 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2691 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2704 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2715 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2744 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2686 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2666 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2658 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2670 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2680 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2684 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2689 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2708 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2724 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2712 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2711 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2727 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2740 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2737 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2719 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2737 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2728 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2680 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2664 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2653 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2637 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2601 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2598 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2600 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2599 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2587 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2576 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2559 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2555 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2549 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2568 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2549 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2543 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2543 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2542 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2551 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2533 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2549 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2525 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2514 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2484 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2477 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2489 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2490 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2486 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2493 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2487 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2483 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2503 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2507 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2497 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2489 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2491 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2473 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2462 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2466 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2471 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2461 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2459 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2476 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2468 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2453 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2457 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2454 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2451 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2447 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2449 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2450 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2438 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2430 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2442 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2441 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2433 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2428 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2436 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2424 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2427 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2430 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2429 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2438 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2442 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2441 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2461 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2457 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2457 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2453 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2457 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2469 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2470 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2478 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2480 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2489 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2501 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2509 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2515 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2532 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2535 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2550 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2556 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2557 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2571 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2565 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2571 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2573 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2572 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2582 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2585 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2609 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2627 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2627 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2633 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2636 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2638 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2640 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2643 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2650 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2647 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2658 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2669 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2671 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2678 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2680 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2676 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2674 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2681 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2682 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2699 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2710 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2723 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2739 - accuracy: 0.14 - ETA: 1:05 - loss: 3.2748 - accuracy: 0.14 - ETA: 1:04 - loss: 3.2743 - accuracy: 0.1499"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:04 - loss: 3.2749 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2747 - accuracy: 0.14 - ETA: 1:03 - loss: 3.2739 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2739 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2736 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2737 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2735 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2739 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2726 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2731 - accuracy: 0.15 - ETA: 59s - loss: 3.2725 - accuracy: 0.1505 - ETA: 59s - loss: 3.2730 - accuracy: 0.150 - ETA: 58s - loss: 3.2737 - accuracy: 0.150 - ETA: 58s - loss: 3.2743 - accuracy: 0.150 - ETA: 58s - loss: 3.2744 - accuracy: 0.150 - ETA: 57s - loss: 3.2748 - accuracy: 0.150 - ETA: 57s - loss: 3.2740 - accuracy: 0.150 - ETA: 56s - loss: 3.2740 - accuracy: 0.150 - ETA: 56s - loss: 3.2742 - accuracy: 0.150 - ETA: 55s - loss: 3.2745 - accuracy: 0.150 - ETA: 55s - loss: 3.2756 - accuracy: 0.149 - ETA: 55s - loss: 3.2754 - accuracy: 0.149 - ETA: 54s - loss: 3.2755 - accuracy: 0.149 - ETA: 54s - loss: 3.2752 - accuracy: 0.150 - ETA: 53s - loss: 3.2789 - accuracy: 0.150 - ETA: 53s - loss: 3.2786 - accuracy: 0.150 - ETA: 52s - loss: 3.2786 - accuracy: 0.150 - ETA: 52s - loss: 3.2788 - accuracy: 0.150 - ETA: 51s - loss: 3.2809 - accuracy: 0.150 - ETA: 51s - loss: 3.2818 - accuracy: 0.150 - ETA: 50s - loss: 3.2853 - accuracy: 0.150 - ETA: 50s - loss: 3.2883 - accuracy: 0.150 - ETA: 50s - loss: 3.2889 - accuracy: 0.150 - ETA: 49s - loss: 3.2884 - accuracy: 0.150 - ETA: 49s - loss: 3.2883 - accuracy: 0.150 - ETA: 48s - loss: 3.2884 - accuracy: 0.150 - ETA: 48s - loss: 3.2895 - accuracy: 0.150 - ETA: 47s - loss: 3.2899 - accuracy: 0.150 - ETA: 47s - loss: 3.2887 - accuracy: 0.150 - ETA: 46s - loss: 3.2885 - accuracy: 0.150 - ETA: 46s - loss: 3.2883 - accuracy: 0.150 - ETA: 45s - loss: 3.2883 - accuracy: 0.150 - ETA: 45s - loss: 3.2875 - accuracy: 0.150 - ETA: 45s - loss: 3.2889 - accuracy: 0.150 - ETA: 44s - loss: 3.2891 - accuracy: 0.150 - ETA: 44s - loss: 3.2899 - accuracy: 0.150 - ETA: 43s - loss: 3.2894 - accuracy: 0.150 - ETA: 43s - loss: 3.2891 - accuracy: 0.150 - ETA: 42s - loss: 3.2894 - accuracy: 0.150 - ETA: 42s - loss: 3.2897 - accuracy: 0.150 - ETA: 41s - loss: 3.2894 - accuracy: 0.150 - ETA: 41s - loss: 3.2901 - accuracy: 0.150 - ETA: 41s - loss: 3.2901 - accuracy: 0.150 - ETA: 40s - loss: 3.2899 - accuracy: 0.150 - ETA: 40s - loss: 3.2908 - accuracy: 0.149 - ETA: 39s - loss: 3.2904 - accuracy: 0.149 - ETA: 39s - loss: 3.2892 - accuracy: 0.150 - ETA: 38s - loss: 3.2899 - accuracy: 0.150 - ETA: 38s - loss: 3.2905 - accuracy: 0.149 - ETA: 37s - loss: 3.2900 - accuracy: 0.150 - ETA: 37s - loss: 3.2902 - accuracy: 0.150 - ETA: 37s - loss: 3.2895 - accuracy: 0.150 - ETA: 36s - loss: 3.2902 - accuracy: 0.150 - ETA: 36s - loss: 3.2902 - accuracy: 0.150 - ETA: 35s - loss: 3.2904 - accuracy: 0.150 - ETA: 35s - loss: 3.2910 - accuracy: 0.150 - ETA: 34s - loss: 3.2913 - accuracy: 0.150 - ETA: 34s - loss: 3.2919 - accuracy: 0.150 - ETA: 33s - loss: 3.2915 - accuracy: 0.150 - ETA: 33s - loss: 3.2907 - accuracy: 0.150 - ETA: 33s - loss: 3.2914 - accuracy: 0.150 - ETA: 32s - loss: 3.2920 - accuracy: 0.149 - ETA: 32s - loss: 3.2921 - accuracy: 0.149 - ETA: 31s - loss: 3.2926 - accuracy: 0.149 - ETA: 31s - loss: 3.2927 - accuracy: 0.149 - ETA: 30s - loss: 3.2928 - accuracy: 0.149 - ETA: 30s - loss: 3.2927 - accuracy: 0.149 - ETA: 29s - loss: 3.2930 - accuracy: 0.149 - ETA: 29s - loss: 3.2931 - accuracy: 0.149 - ETA: 29s - loss: 3.2936 - accuracy: 0.149 - ETA: 28s - loss: 3.2938 - accuracy: 0.149 - ETA: 28s - loss: 3.2945 - accuracy: 0.148 - ETA: 27s - loss: 3.2940 - accuracy: 0.148 - ETA: 27s - loss: 3.2936 - accuracy: 0.148 - ETA: 26s - loss: 3.2942 - accuracy: 0.148 - ETA: 26s - loss: 3.2943 - accuracy: 0.148 - ETA: 25s - loss: 3.2946 - accuracy: 0.148 - ETA: 25s - loss: 3.2947 - accuracy: 0.148 - ETA: 24s - loss: 3.2948 - accuracy: 0.148 - ETA: 24s - loss: 3.2952 - accuracy: 0.148 - ETA: 24s - loss: 3.2962 - accuracy: 0.148 - ETA: 23s - loss: 3.2963 - accuracy: 0.148 - ETA: 23s - loss: 3.2963 - accuracy: 0.148 - ETA: 22s - loss: 3.2958 - accuracy: 0.148 - ETA: 22s - loss: 3.2959 - accuracy: 0.148 - ETA: 21s - loss: 3.2955 - accuracy: 0.148 - ETA: 21s - loss: 3.2961 - accuracy: 0.148 - ETA: 21s - loss: 3.2956 - accuracy: 0.148 - ETA: 20s - loss: 3.2955 - accuracy: 0.148 - ETA: 20s - loss: 3.2956 - accuracy: 0.148 - ETA: 19s - loss: 3.2964 - accuracy: 0.148 - ETA: 19s - loss: 3.2971 - accuracy: 0.148 - ETA: 18s - loss: 3.2973 - accuracy: 0.148 - ETA: 18s - loss: 3.2975 - accuracy: 0.148 - ETA: 17s - loss: 3.2976 - accuracy: 0.147 - ETA: 17s - loss: 3.2984 - accuracy: 0.147 - ETA: 16s - loss: 3.2983 - accuracy: 0.147 - ETA: 16s - loss: 3.2991 - accuracy: 0.147 - ETA: 16s - loss: 3.2998 - accuracy: 0.147 - ETA: 15s - loss: 3.2997 - accuracy: 0.147 - ETA: 15s - loss: 3.3000 - accuracy: 0.147 - ETA: 14s - loss: 3.3002 - accuracy: 0.147 - ETA: 14s - loss: 3.2995 - accuracy: 0.147 - ETA: 13s - loss: 3.2996 - accuracy: 0.147 - ETA: 13s - loss: 3.2998 - accuracy: 0.147 - ETA: 12s - loss: 3.2996 - accuracy: 0.147 - ETA: 12s - loss: 3.2992 - accuracy: 0.148 - ETA: 12s - loss: 3.2994 - accuracy: 0.148 - ETA: 11s - loss: 3.2999 - accuracy: 0.147 - ETA: 11s - loss: 3.3006 - accuracy: 0.147 - ETA: 10s - loss: 3.3008 - accuracy: 0.147 - ETA: 10s - loss: 3.3005 - accuracy: 0.147 - ETA: 9s - loss: 3.3008 - accuracy: 0.147 - ETA: 9s - loss: 3.3007 - accuracy: 0.14 - ETA: 8s - loss: 3.3010 - accuracy: 0.14 - ETA: 8s - loss: 3.3009 - accuracy: 0.14 - ETA: 8s - loss: 3.3013 - accuracy: 0.14 - ETA: 7s - loss: 3.3015 - accuracy: 0.14 - ETA: 7s - loss: 3.3019 - accuracy: 0.14 - ETA: 6s - loss: 3.3020 - accuracy: 0.14 - ETA: 6s - loss: 3.3023 - accuracy: 0.14 - ETA: 5s - loss: 3.3021 - accuracy: 0.14 - ETA: 5s - loss: 3.3026 - accuracy: 0.14 - ETA: 4s - loss: 3.3030 - accuracy: 0.14 - ETA: 4s - loss: 3.3030 - accuracy: 0.14 - ETA: 4s - loss: 3.3030 - accuracy: 0.14 - ETA: 3s - loss: 3.3030 - accuracy: 0.14 - ETA: 3s - loss: 3.3032 - accuracy: 0.14 - ETA: 2s - loss: 3.3030 - accuracy: 0.14 - ETA: 2s - loss: 3.3035 - accuracy: 0.14 - ETA: 1s - loss: 3.3037 - accuracy: 0.14 - ETA: 1s - loss: 3.3038 - accuracy: 0.14 - ETA: 0s - loss: 3.3041 - accuracy: 0.14 - ETA: 0s - loss: 3.3039 - accuracy: 0.14 - ETA: 0s - loss: 3.3041 - accuracy: 0.14 - 158s 4ms/step - loss: 3.3040 - accuracy: 0.1468 - val_loss: 4.3414 - val_accuracy: 0.0175\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:43 - loss: 3.4009 - accuracy: 0.12 - ETA: 2:34 - loss: 3.3883 - accuracy: 0.14 - ETA: 2:29 - loss: 3.3852 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3472 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3227 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3381 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3292 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3253 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3471 - accuracy: 0.12 - ETA: 2:23 - loss: 3.3464 - accuracy: 0.12 - ETA: 2:23 - loss: 3.3339 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3429 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3367 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3496 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3541 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3752 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3735 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3787 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3777 - accuracy: 0.12 - ETA: 2:18 - loss: 3.3792 - accuracy: 0.12 - ETA: 2:18 - loss: 3.3890 - accuracy: 0.12 - ETA: 2:17 - loss: 3.3886 - accuracy: 0.12 - ETA: 2:16 - loss: 3.3942 - accuracy: 0.12 - ETA: 2:15 - loss: 3.3954 - accuracy: 0.12 - ETA: 2:15 - loss: 3.3919 - accuracy: 0.12 - ETA: 2:14 - loss: 3.3934 - accuracy: 0.12 - ETA: 2:14 - loss: 3.3893 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3892 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3900 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3878 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3839 - accuracy: 0.12 - ETA: 2:13 - loss: 3.3783 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3789 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3810 - accuracy: 0.12 - ETA: 2:12 - loss: 3.3814 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3812 - accuracy: 0.12 - ETA: 2:11 - loss: 3.3803 - accuracy: 0.12 - ETA: 2:10 - loss: 3.3802 - accuracy: 0.12 - ETA: 2:10 - loss: 3.3810 - accuracy: 0.12 - ETA: 2:09 - loss: 3.3826 - accuracy: 0.12 - ETA: 2:09 - loss: 3.3773 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3790 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3722 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3699 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3675 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3686 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3723 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3776 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3747 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3741 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3724 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3723 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3690 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3696 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3707 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3707 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3712 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3720 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3697 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3692 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3675 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3671 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3676 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3666 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3679 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3692 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3656 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3646 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3635 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3638 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3634 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3658 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3653 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3663 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3657 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3656 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3630 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3644 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3653 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3648 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3648 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3633 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3630 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3621 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3642 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3601 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3586 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3580 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3573 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3585 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3591 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3592 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3600 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3607 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3594 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3573 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3581 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3580 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3567 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3556 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3568 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3566 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3559 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3553 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3549 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3554 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3552 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3542 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3553 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3559 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3553 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3554 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3545 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3535 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3531 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3520 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3506 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3507 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3502 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3505 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3520 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3508 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3502 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3507 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3512 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3530 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3527 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3534 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3546 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3568 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3554 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3561 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3552 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3542 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3540 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3532 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3523 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3518 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3515 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3513 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3512 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3508 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3511 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3512 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3518 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3516 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3518 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3525 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3526 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3516 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3508 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3510 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3518 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3517 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3506 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3501 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3497 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3482 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3483 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3506 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3516 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3511 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3507 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3502 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3490 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3493 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3495 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3490 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3480 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3485 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3475 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3478 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3479 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3475 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3496 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3499 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3509 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3510 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3502 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3510 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3501 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3505 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3503 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3500 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3496 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3496 - accuracy: 0.1366"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:04 - loss: 3.3494 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3500 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3498 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3503 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3503 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3505 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3506 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3515 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3517 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3524 - accuracy: 0.13 - ETA: 59s - loss: 3.3524 - accuracy: 0.1365 - ETA: 59s - loss: 3.3528 - accuracy: 0.136 - ETA: 58s - loss: 3.3534 - accuracy: 0.136 - ETA: 58s - loss: 3.3548 - accuracy: 0.136 - ETA: 58s - loss: 3.3551 - accuracy: 0.136 - ETA: 57s - loss: 3.3553 - accuracy: 0.136 - ETA: 57s - loss: 3.3552 - accuracy: 0.136 - ETA: 56s - loss: 3.3555 - accuracy: 0.136 - ETA: 56s - loss: 3.3561 - accuracy: 0.136 - ETA: 55s - loss: 3.3569 - accuracy: 0.136 - ETA: 55s - loss: 3.3579 - accuracy: 0.135 - ETA: 54s - loss: 3.3584 - accuracy: 0.135 - ETA: 54s - loss: 3.3589 - accuracy: 0.135 - ETA: 54s - loss: 3.3587 - accuracy: 0.135 - ETA: 53s - loss: 3.3589 - accuracy: 0.135 - ETA: 53s - loss: 3.3595 - accuracy: 0.135 - ETA: 52s - loss: 3.3594 - accuracy: 0.135 - ETA: 52s - loss: 3.3587 - accuracy: 0.136 - ETA: 51s - loss: 3.3583 - accuracy: 0.136 - ETA: 51s - loss: 3.3583 - accuracy: 0.136 - ETA: 50s - loss: 3.3570 - accuracy: 0.136 - ETA: 50s - loss: 3.3571 - accuracy: 0.136 - ETA: 50s - loss: 3.3565 - accuracy: 0.136 - ETA: 49s - loss: 3.3568 - accuracy: 0.137 - ETA: 49s - loss: 3.3566 - accuracy: 0.137 - ETA: 48s - loss: 3.3566 - accuracy: 0.137 - ETA: 48s - loss: 3.3566 - accuracy: 0.136 - ETA: 47s - loss: 3.3559 - accuracy: 0.137 - ETA: 47s - loss: 3.3554 - accuracy: 0.137 - ETA: 46s - loss: 3.3558 - accuracy: 0.137 - ETA: 46s - loss: 3.3561 - accuracy: 0.137 - ETA: 45s - loss: 3.3555 - accuracy: 0.137 - ETA: 45s - loss: 3.3557 - accuracy: 0.137 - ETA: 45s - loss: 3.3556 - accuracy: 0.137 - ETA: 44s - loss: 3.3561 - accuracy: 0.137 - ETA: 44s - loss: 3.3559 - accuracy: 0.137 - ETA: 43s - loss: 3.3560 - accuracy: 0.137 - ETA: 43s - loss: 3.3560 - accuracy: 0.137 - ETA: 42s - loss: 3.3555 - accuracy: 0.137 - ETA: 42s - loss: 3.3557 - accuracy: 0.137 - ETA: 41s - loss: 3.3558 - accuracy: 0.137 - ETA: 41s - loss: 3.3558 - accuracy: 0.137 - ETA: 41s - loss: 3.3559 - accuracy: 0.137 - ETA: 40s - loss: 3.3557 - accuracy: 0.137 - ETA: 40s - loss: 3.3550 - accuracy: 0.137 - ETA: 39s - loss: 3.3552 - accuracy: 0.137 - ETA: 39s - loss: 3.3548 - accuracy: 0.137 - ETA: 38s - loss: 3.3549 - accuracy: 0.137 - ETA: 38s - loss: 3.3555 - accuracy: 0.137 - ETA: 37s - loss: 3.3559 - accuracy: 0.137 - ETA: 37s - loss: 3.3559 - accuracy: 0.137 - ETA: 37s - loss: 3.3558 - accuracy: 0.137 - ETA: 36s - loss: 3.3556 - accuracy: 0.137 - ETA: 36s - loss: 3.3551 - accuracy: 0.137 - ETA: 35s - loss: 3.3549 - accuracy: 0.137 - ETA: 35s - loss: 3.3545 - accuracy: 0.137 - ETA: 34s - loss: 3.3542 - accuracy: 0.137 - ETA: 34s - loss: 3.3540 - accuracy: 0.137 - ETA: 33s - loss: 3.3536 - accuracy: 0.137 - ETA: 33s - loss: 3.3535 - accuracy: 0.137 - ETA: 33s - loss: 3.3534 - accuracy: 0.137 - ETA: 32s - loss: 3.3527 - accuracy: 0.137 - ETA: 32s - loss: 3.3534 - accuracy: 0.137 - ETA: 31s - loss: 3.3530 - accuracy: 0.137 - ETA: 31s - loss: 3.3531 - accuracy: 0.137 - ETA: 30s - loss: 3.3530 - accuracy: 0.137 - ETA: 30s - loss: 3.3525 - accuracy: 0.137 - ETA: 29s - loss: 3.3527 - accuracy: 0.137 - ETA: 29s - loss: 3.3522 - accuracy: 0.137 - ETA: 29s - loss: 3.3519 - accuracy: 0.137 - ETA: 28s - loss: 3.3517 - accuracy: 0.137 - ETA: 28s - loss: 3.3517 - accuracy: 0.137 - ETA: 27s - loss: 3.3519 - accuracy: 0.137 - ETA: 27s - loss: 3.3522 - accuracy: 0.137 - ETA: 26s - loss: 3.3518 - accuracy: 0.137 - ETA: 26s - loss: 3.3517 - accuracy: 0.137 - ETA: 25s - loss: 3.3515 - accuracy: 0.137 - ETA: 25s - loss: 3.3516 - accuracy: 0.137 - ETA: 25s - loss: 3.3516 - accuracy: 0.137 - ETA: 24s - loss: 3.3521 - accuracy: 0.137 - ETA: 24s - loss: 3.3521 - accuracy: 0.137 - ETA: 23s - loss: 3.3524 - accuracy: 0.137 - ETA: 23s - loss: 3.3524 - accuracy: 0.137 - ETA: 22s - loss: 3.3527 - accuracy: 0.137 - ETA: 22s - loss: 3.3535 - accuracy: 0.137 - ETA: 21s - loss: 3.3538 - accuracy: 0.136 - ETA: 21s - loss: 3.3540 - accuracy: 0.136 - ETA: 21s - loss: 3.3537 - accuracy: 0.136 - ETA: 20s - loss: 3.3528 - accuracy: 0.136 - ETA: 20s - loss: 3.3524 - accuracy: 0.136 - ETA: 19s - loss: 3.3525 - accuracy: 0.136 - ETA: 19s - loss: 3.3530 - accuracy: 0.136 - ETA: 18s - loss: 3.3530 - accuracy: 0.136 - ETA: 18s - loss: 3.3528 - accuracy: 0.136 - ETA: 17s - loss: 3.3528 - accuracy: 0.136 - ETA: 17s - loss: 3.3528 - accuracy: 0.136 - ETA: 17s - loss: 3.3529 - accuracy: 0.136 - ETA: 16s - loss: 3.3533 - accuracy: 0.136 - ETA: 16s - loss: 3.3534 - accuracy: 0.136 - ETA: 15s - loss: 3.3535 - accuracy: 0.136 - ETA: 15s - loss: 3.3537 - accuracy: 0.136 - ETA: 14s - loss: 3.3539 - accuracy: 0.136 - ETA: 14s - loss: 3.3542 - accuracy: 0.136 - ETA: 13s - loss: 3.3542 - accuracy: 0.136 - ETA: 13s - loss: 3.3552 - accuracy: 0.136 - ETA: 13s - loss: 3.3560 - accuracy: 0.136 - ETA: 12s - loss: 3.3577 - accuracy: 0.136 - ETA: 12s - loss: 3.3583 - accuracy: 0.136 - ETA: 11s - loss: 3.3596 - accuracy: 0.135 - ETA: 11s - loss: 3.3598 - accuracy: 0.135 - ETA: 10s - loss: 3.3618 - accuracy: 0.135 - ETA: 10s - loss: 3.3618 - accuracy: 0.135 - ETA: 9s - loss: 3.3619 - accuracy: 0.135 - ETA: 9s - loss: 3.3621 - accuracy: 0.13 - ETA: 8s - loss: 3.3626 - accuracy: 0.13 - ETA: 8s - loss: 3.3628 - accuracy: 0.13 - ETA: 8s - loss: 3.3629 - accuracy: 0.13 - ETA: 7s - loss: 3.3633 - accuracy: 0.13 - ETA: 7s - loss: 3.3634 - accuracy: 0.13 - ETA: 6s - loss: 3.3632 - accuracy: 0.13 - ETA: 6s - loss: 3.3633 - accuracy: 0.13 - ETA: 5s - loss: 3.3636 - accuracy: 0.13 - ETA: 5s - loss: 3.3638 - accuracy: 0.13 - ETA: 4s - loss: 3.3640 - accuracy: 0.13 - ETA: 4s - loss: 3.3637 - accuracy: 0.13 - ETA: 4s - loss: 3.3639 - accuracy: 0.13 - ETA: 3s - loss: 3.3644 - accuracy: 0.13 - ETA: 3s - loss: 3.3647 - accuracy: 0.13 - ETA: 2s - loss: 3.3648 - accuracy: 0.13 - ETA: 2s - loss: 3.3645 - accuracy: 0.13 - ETA: 1s - loss: 3.3645 - accuracy: 0.13 - ETA: 1s - loss: 3.3649 - accuracy: 0.13 - ETA: 0s - loss: 3.3653 - accuracy: 0.13 - ETA: 0s - loss: 3.3654 - accuracy: 0.13 - ETA: 0s - loss: 3.3655 - accuracy: 0.13 - 159s 4ms/step - loss: 3.3654 - accuracy: 0.1341 - val_loss: 4.2324 - val_accuracy: 0.0243\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:43 - loss: 3.4043 - accuracy: 0.13 - ETA: 2:30 - loss: 3.4416 - accuracy: 0.15 - ETA: 2:27 - loss: 3.4732 - accuracy: 0.13 - ETA: 2:25 - loss: 3.4726 - accuracy: 0.13 - ETA: 2:24 - loss: 3.4502 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4468 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4347 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4258 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4295 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4380 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4333 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4302 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4315 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4321 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4327 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4375 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4529 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4456 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4426 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4418 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4508 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4514 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4508 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4461 - accuracy: 0.11 - ETA: 2:14 - loss: 3.4421 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4350 - accuracy: 0.11 - ETA: 2:13 - loss: 3.4333 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4320 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4320 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4294 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4306 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4325 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4278 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4307 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4313 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4327 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4352 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4359 - accuracy: 0.11 - ETA: 2:09 - loss: 3.4355 - accuracy: 0.11 - ETA: 2:09 - loss: 3.4341 - accuracy: 0.11 - ETA: 2:08 - loss: 3.4357 - accuracy: 0.11 - ETA: 2:08 - loss: 3.4344 - accuracy: 0.11 - ETA: 2:07 - loss: 3.4325 - accuracy: 0.11 - ETA: 2:07 - loss: 3.4349 - accuracy: 0.11 - ETA: 2:07 - loss: 3.4298 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4325 - accuracy: 0.11 - ETA: 2:06 - loss: 3.4346 - accuracy: 0.11 - ETA: 2:06 - loss: 3.4373 - accuracy: 0.11 - ETA: 2:05 - loss: 3.4373 - accuracy: 0.11 - ETA: 2:05 - loss: 3.4350 - accuracy: 0.11 - ETA: 2:05 - loss: 3.4323 - accuracy: 0.11 - ETA: 2:04 - loss: 3.4333 - accuracy: 0.11 - ETA: 2:04 - loss: 3.4321 - accuracy: 0.11 - ETA: 2:04 - loss: 3.4340 - accuracy: 0.11 - ETA: 2:03 - loss: 3.4354 - accuracy: 0.11 - ETA: 2:03 - loss: 3.4356 - accuracy: 0.11 - ETA: 2:02 - loss: 3.4363 - accuracy: 0.11 - ETA: 2:02 - loss: 3.4356 - accuracy: 0.11 - ETA: 2:01 - loss: 3.4358 - accuracy: 0.11 - ETA: 2:01 - loss: 3.4360 - accuracy: 0.11 - ETA: 2:00 - loss: 3.4363 - accuracy: 0.11 - ETA: 2:00 - loss: 3.4333 - accuracy: 0.12 - ETA: 2:00 - loss: 3.4364 - accuracy: 0.11 - ETA: 1:59 - loss: 3.4360 - accuracy: 0.11 - ETA: 1:59 - loss: 3.4367 - accuracy: 0.11 - ETA: 1:58 - loss: 3.4368 - accuracy: 0.11 - ETA: 1:58 - loss: 3.4359 - accuracy: 0.11 - ETA: 1:57 - loss: 3.4344 - accuracy: 0.11 - ETA: 1:57 - loss: 3.4337 - accuracy: 0.11 - ETA: 1:57 - loss: 3.4330 - accuracy: 0.11 - ETA: 1:56 - loss: 3.4335 - accuracy: 0.11 - ETA: 1:56 - loss: 3.4327 - accuracy: 0.11 - ETA: 1:55 - loss: 3.4312 - accuracy: 0.11 - ETA: 1:55 - loss: 3.4300 - accuracy: 0.11 - ETA: 1:55 - loss: 3.4292 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4294 - accuracy: 0.11 - ETA: 1:54 - loss: 3.4287 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4286 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4286 - accuracy: 0.11 - ETA: 1:52 - loss: 3.4296 - accuracy: 0.11 - ETA: 1:52 - loss: 3.4285 - accuracy: 0.11 - ETA: 1:51 - loss: 3.4287 - accuracy: 0.11 - ETA: 1:51 - loss: 3.4263 - accuracy: 0.11 - ETA: 1:50 - loss: 3.4261 - accuracy: 0.11 - ETA: 1:50 - loss: 3.4265 - accuracy: 0.11 - ETA: 1:49 - loss: 3.4260 - accuracy: 0.11 - ETA: 1:49 - loss: 3.4250 - accuracy: 0.11 - ETA: 1:48 - loss: 3.4249 - accuracy: 0.11 - ETA: 1:48 - loss: 3.4252 - accuracy: 0.11 - ETA: 1:47 - loss: 3.4255 - accuracy: 0.11 - ETA: 1:47 - loss: 3.4240 - accuracy: 0.11 - ETA: 1:46 - loss: 3.4224 - accuracy: 0.11 - ETA: 1:46 - loss: 3.4228 - accuracy: 0.11 - ETA: 1:45 - loss: 3.4216 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4211 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4228 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4226 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4227 - accuracy: 0.12 - ETA: 1:43 - loss: 3.4239 - accuracy: 0.11 - ETA: 1:43 - loss: 3.4223 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4210 - accuracy: 0.12 - ETA: 1:42 - loss: 3.4199 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4189 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4193 - accuracy: 0.12 - ETA: 1:41 - loss: 3.4190 - accuracy: 0.12 - ETA: 1:40 - loss: 3.4184 - accuracy: 0.12 - ETA: 1:40 - loss: 3.4186 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4178 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4171 - accuracy: 0.12 - ETA: 1:39 - loss: 3.4163 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4171 - accuracy: 0.12 - ETA: 1:38 - loss: 3.4168 - accuracy: 0.12 - ETA: 1:37 - loss: 3.4171 - accuracy: 0.12 - ETA: 1:37 - loss: 3.4182 - accuracy: 0.11 - ETA: 1:36 - loss: 3.4182 - accuracy: 0.11 - ETA: 1:36 - loss: 3.4184 - accuracy: 0.11 - ETA: 1:35 - loss: 3.4168 - accuracy: 0.11 - ETA: 1:35 - loss: 3.4162 - accuracy: 0.11 - ETA: 1:34 - loss: 3.4158 - accuracy: 0.11 - ETA: 1:34 - loss: 3.4146 - accuracy: 0.11 - ETA: 1:33 - loss: 3.4128 - accuracy: 0.11 - ETA: 1:33 - loss: 3.4141 - accuracy: 0.11 - ETA: 1:33 - loss: 3.4136 - accuracy: 0.11 - ETA: 1:32 - loss: 3.4120 - accuracy: 0.11 - ETA: 1:32 - loss: 3.4120 - accuracy: 0.11 - ETA: 1:31 - loss: 3.4118 - accuracy: 0.11 - ETA: 1:31 - loss: 3.4112 - accuracy: 0.11 - ETA: 1:30 - loss: 3.4114 - accuracy: 0.11 - ETA: 1:30 - loss: 3.4128 - accuracy: 0.11 - ETA: 1:29 - loss: 3.4128 - accuracy: 0.11 - ETA: 1:29 - loss: 3.4115 - accuracy: 0.11 - ETA: 1:28 - loss: 3.4106 - accuracy: 0.12 - ETA: 1:28 - loss: 3.4103 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4103 - accuracy: 0.11 - ETA: 1:27 - loss: 3.4095 - accuracy: 0.12 - ETA: 1:27 - loss: 3.4091 - accuracy: 0.12 - ETA: 1:26 - loss: 3.4098 - accuracy: 0.12 - ETA: 1:26 - loss: 3.4097 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4096 - accuracy: 0.12 - ETA: 1:25 - loss: 3.4105 - accuracy: 0.12 - ETA: 1:24 - loss: 3.4097 - accuracy: 0.12 - ETA: 1:24 - loss: 3.4101 - accuracy: 0.12 - ETA: 1:23 - loss: 3.4096 - accuracy: 0.12 - ETA: 1:23 - loss: 3.4114 - accuracy: 0.12 - ETA: 1:23 - loss: 3.4107 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4104 - accuracy: 0.12 - ETA: 1:22 - loss: 3.4101 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4094 - accuracy: 0.12 - ETA: 1:21 - loss: 3.4101 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4110 - accuracy: 0.12 - ETA: 1:20 - loss: 3.4109 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4110 - accuracy: 0.12 - ETA: 1:19 - loss: 3.4109 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4106 - accuracy: 0.12 - ETA: 1:18 - loss: 3.4117 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4121 - accuracy: 0.11 - ETA: 1:17 - loss: 3.4111 - accuracy: 0.12 - ETA: 1:17 - loss: 3.4110 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4100 - accuracy: 0.12 - ETA: 1:16 - loss: 3.4100 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4097 - accuracy: 0.12 - ETA: 1:15 - loss: 3.4100 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4110 - accuracy: 0.12 - ETA: 1:14 - loss: 3.4124 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4120 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4118 - accuracy: 0.12 - ETA: 1:13 - loss: 3.4105 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4103 - accuracy: 0.12 - ETA: 1:12 - loss: 3.4099 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4093 - accuracy: 0.12 - ETA: 1:11 - loss: 3.4081 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4094 - accuracy: 0.12 - ETA: 1:10 - loss: 3.4117 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4147 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4164 - accuracy: 0.12 - ETA: 1:09 - loss: 3.4178 - accuracy: 0.12 - ETA: 1:08 - loss: 3.4189 - accuracy: 0.12 - ETA: 1:08 - loss: 3.4198 - accuracy: 0.12 - ETA: 1:07 - loss: 3.4211 - accuracy: 0.12 - ETA: 1:07 - loss: 3.4210 - accuracy: 0.12 - ETA: 1:06 - loss: 3.4209 - accuracy: 0.12 - ETA: 1:06 - loss: 3.4216 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4217 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4218 - accuracy: 0.12 - ETA: 1:05 - loss: 3.4220 - accuracy: 0.12 - ETA: 1:04 - loss: 3.4218 - accuracy: 0.1205"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:04 - loss: 3.4223 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4227 - accuracy: 0.12 - ETA: 1:03 - loss: 3.4232 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4234 - accuracy: 0.12 - ETA: 1:02 - loss: 3.4236 - accuracy: 0.11 - ETA: 1:01 - loss: 3.4241 - accuracy: 0.11 - ETA: 1:01 - loss: 3.4241 - accuracy: 0.11 - ETA: 1:01 - loss: 3.4237 - accuracy: 0.11 - ETA: 1:00 - loss: 3.4255 - accuracy: 0.11 - ETA: 1:00 - loss: 3.4253 - accuracy: 0.11 - ETA: 59s - loss: 3.4257 - accuracy: 0.1196 - ETA: 59s - loss: 3.4259 - accuracy: 0.119 - ETA: 58s - loss: 3.4268 - accuracy: 0.119 - ETA: 58s - loss: 3.4271 - accuracy: 0.119 - ETA: 57s - loss: 3.4275 - accuracy: 0.119 - ETA: 57s - loss: 3.4280 - accuracy: 0.119 - ETA: 57s - loss: 3.4287 - accuracy: 0.119 - ETA: 56s - loss: 3.4296 - accuracy: 0.118 - ETA: 56s - loss: 3.4302 - accuracy: 0.118 - ETA: 55s - loss: 3.4308 - accuracy: 0.118 - ETA: 55s - loss: 3.4309 - accuracy: 0.118 - ETA: 54s - loss: 3.4308 - accuracy: 0.118 - ETA: 54s - loss: 3.4311 - accuracy: 0.118 - ETA: 53s - loss: 3.4309 - accuracy: 0.118 - ETA: 53s - loss: 3.4313 - accuracy: 0.118 - ETA: 53s - loss: 3.4308 - accuracy: 0.118 - ETA: 52s - loss: 3.4306 - accuracy: 0.118 - ETA: 52s - loss: 3.4309 - accuracy: 0.119 - ETA: 51s - loss: 3.4309 - accuracy: 0.119 - ETA: 51s - loss: 3.4307 - accuracy: 0.119 - ETA: 50s - loss: 3.4309 - accuracy: 0.118 - ETA: 50s - loss: 3.4308 - accuracy: 0.118 - ETA: 49s - loss: 3.4302 - accuracy: 0.118 - ETA: 49s - loss: 3.4299 - accuracy: 0.119 - ETA: 49s - loss: 3.4300 - accuracy: 0.119 - ETA: 48s - loss: 3.4294 - accuracy: 0.119 - ETA: 48s - loss: 3.4289 - accuracy: 0.119 - ETA: 47s - loss: 3.4287 - accuracy: 0.119 - ETA: 47s - loss: 3.4293 - accuracy: 0.119 - ETA: 46s - loss: 3.4296 - accuracy: 0.119 - ETA: 46s - loss: 3.4288 - accuracy: 0.119 - ETA: 45s - loss: 3.4291 - accuracy: 0.119 - ETA: 45s - loss: 3.4280 - accuracy: 0.119 - ETA: 45s - loss: 3.4274 - accuracy: 0.119 - ETA: 44s - loss: 3.4279 - accuracy: 0.119 - ETA: 44s - loss: 3.4277 - accuracy: 0.119 - ETA: 43s - loss: 3.4279 - accuracy: 0.119 - ETA: 43s - loss: 3.4274 - accuracy: 0.119 - ETA: 42s - loss: 3.4269 - accuracy: 0.119 - ETA: 42s - loss: 3.4268 - accuracy: 0.119 - ETA: 41s - loss: 3.4265 - accuracy: 0.119 - ETA: 41s - loss: 3.4267 - accuracy: 0.119 - ETA: 41s - loss: 3.4273 - accuracy: 0.119 - ETA: 40s - loss: 3.4271 - accuracy: 0.119 - ETA: 40s - loss: 3.4272 - accuracy: 0.119 - ETA: 39s - loss: 3.4273 - accuracy: 0.119 - ETA: 39s - loss: 3.4269 - accuracy: 0.119 - ETA: 38s - loss: 3.4265 - accuracy: 0.120 - ETA: 38s - loss: 3.4266 - accuracy: 0.120 - ETA: 37s - loss: 3.4262 - accuracy: 0.120 - ETA: 37s - loss: 3.4256 - accuracy: 0.120 - ETA: 36s - loss: 3.4251 - accuracy: 0.120 - ETA: 36s - loss: 3.4255 - accuracy: 0.120 - ETA: 36s - loss: 3.4252 - accuracy: 0.120 - ETA: 35s - loss: 3.4246 - accuracy: 0.120 - ETA: 35s - loss: 3.4248 - accuracy: 0.120 - ETA: 34s - loss: 3.4251 - accuracy: 0.120 - ETA: 34s - loss: 3.4248 - accuracy: 0.120 - ETA: 33s - loss: 3.4250 - accuracy: 0.120 - ETA: 33s - loss: 3.4249 - accuracy: 0.120 - ETA: 33s - loss: 3.4255 - accuracy: 0.120 - ETA: 32s - loss: 3.4257 - accuracy: 0.120 - ETA: 32s - loss: 3.4252 - accuracy: 0.120 - ETA: 31s - loss: 3.4246 - accuracy: 0.120 - ETA: 31s - loss: 3.4245 - accuracy: 0.120 - ETA: 30s - loss: 3.4245 - accuracy: 0.120 - ETA: 30s - loss: 3.4238 - accuracy: 0.121 - ETA: 29s - loss: 3.4232 - accuracy: 0.121 - ETA: 29s - loss: 3.4231 - accuracy: 0.121 - ETA: 28s - loss: 3.4223 - accuracy: 0.121 - ETA: 28s - loss: 3.4219 - accuracy: 0.121 - ETA: 28s - loss: 3.4219 - accuracy: 0.121 - ETA: 27s - loss: 3.4217 - accuracy: 0.121 - ETA: 27s - loss: 3.4215 - accuracy: 0.121 - ETA: 26s - loss: 3.4210 - accuracy: 0.121 - ETA: 26s - loss: 3.4208 - accuracy: 0.121 - ETA: 25s - loss: 3.4205 - accuracy: 0.121 - ETA: 25s - loss: 3.4204 - accuracy: 0.121 - ETA: 24s - loss: 3.4202 - accuracy: 0.121 - ETA: 24s - loss: 3.4206 - accuracy: 0.121 - ETA: 24s - loss: 3.4200 - accuracy: 0.122 - ETA: 23s - loss: 3.4197 - accuracy: 0.122 - ETA: 23s - loss: 3.4199 - accuracy: 0.121 - ETA: 22s - loss: 3.4195 - accuracy: 0.122 - ETA: 22s - loss: 3.4189 - accuracy: 0.122 - ETA: 21s - loss: 3.4183 - accuracy: 0.122 - ETA: 21s - loss: 3.4185 - accuracy: 0.122 - ETA: 20s - loss: 3.4182 - accuracy: 0.122 - ETA: 20s - loss: 3.4181 - accuracy: 0.122 - ETA: 20s - loss: 3.4174 - accuracy: 0.122 - ETA: 19s - loss: 3.4170 - accuracy: 0.122 - ETA: 19s - loss: 3.4174 - accuracy: 0.122 - ETA: 18s - loss: 3.4168 - accuracy: 0.123 - ETA: 18s - loss: 3.4170 - accuracy: 0.122 - ETA: 17s - loss: 3.4171 - accuracy: 0.122 - ETA: 17s - loss: 3.4169 - accuracy: 0.122 - ETA: 16s - loss: 3.4164 - accuracy: 0.123 - ETA: 16s - loss: 3.4160 - accuracy: 0.123 - ETA: 16s - loss: 3.4153 - accuracy: 0.123 - ETA: 15s - loss: 3.4151 - accuracy: 0.123 - ETA: 15s - loss: 3.4153 - accuracy: 0.123 - ETA: 14s - loss: 3.4152 - accuracy: 0.123 - ETA: 14s - loss: 3.4150 - accuracy: 0.123 - ETA: 13s - loss: 3.4148 - accuracy: 0.123 - ETA: 13s - loss: 3.4147 - accuracy: 0.123 - ETA: 12s - loss: 3.4148 - accuracy: 0.123 - ETA: 12s - loss: 3.4146 - accuracy: 0.123 - ETA: 12s - loss: 3.4144 - accuracy: 0.123 - ETA: 11s - loss: 3.4140 - accuracy: 0.123 - ETA: 11s - loss: 3.4130 - accuracy: 0.123 - ETA: 10s - loss: 3.4132 - accuracy: 0.123 - ETA: 10s - loss: 3.4126 - accuracy: 0.123 - ETA: 9s - loss: 3.4125 - accuracy: 0.123 - ETA: 9s - loss: 3.4119 - accuracy: 0.12 - ETA: 8s - loss: 3.4118 - accuracy: 0.12 - ETA: 8s - loss: 3.4112 - accuracy: 0.12 - ETA: 8s - loss: 3.4110 - accuracy: 0.12 - ETA: 7s - loss: 3.4102 - accuracy: 0.12 - ETA: 7s - loss: 3.4099 - accuracy: 0.12 - ETA: 6s - loss: 3.4098 - accuracy: 0.12 - ETA: 6s - loss: 3.4098 - accuracy: 0.12 - ETA: 5s - loss: 3.4091 - accuracy: 0.12 - ETA: 5s - loss: 3.4085 - accuracy: 0.12 - ETA: 4s - loss: 3.4087 - accuracy: 0.12 - ETA: 4s - loss: 3.4082 - accuracy: 0.12 - ETA: 4s - loss: 3.4077 - accuracy: 0.12 - ETA: 3s - loss: 3.4070 - accuracy: 0.12 - ETA: 3s - loss: 3.4063 - accuracy: 0.12 - ETA: 2s - loss: 3.4058 - accuracy: 0.12 - ETA: 2s - loss: 3.4059 - accuracy: 0.12 - ETA: 1s - loss: 3.4054 - accuracy: 0.12 - ETA: 1s - loss: 3.4054 - accuracy: 0.12 - ETA: 0s - loss: 3.4053 - accuracy: 0.12 - ETA: 0s - loss: 3.4051 - accuracy: 0.12 - ETA: 0s - loss: 3.4045 - accuracy: 0.12 - 158s 4ms/step - loss: 3.4045 - accuracy: 0.1253 - val_loss: 4.3451 - val_accuracy: 0.0251\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:45 - loss: 3.2826 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3058 - accuracy: 0.12 - ETA: 2:45 - loss: 3.2092 - accuracy: 0.15 - ETA: 2:49 - loss: 3.1887 - accuracy: 0.15 - ETA: 2:47 - loss: 3.1764 - accuracy: 0.16 - ETA: 2:45 - loss: 3.2080 - accuracy: 0.15 - ETA: 2:43 - loss: 3.2299 - accuracy: 0.15 - ETA: 2:42 - loss: 3.2597 - accuracy: 0.15 - ETA: 2:40 - loss: 3.2902 - accuracy: 0.14 - ETA: 2:39 - loss: 3.2958 - accuracy: 0.14 - ETA: 2:37 - loss: 3.2884 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2850 - accuracy: 0.14 - ETA: 2:36 - loss: 3.2868 - accuracy: 0.15 - ETA: 2:35 - loss: 3.2900 - accuracy: 0.15 - ETA: 2:34 - loss: 3.2909 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2894 - accuracy: 0.16 - ETA: 2:31 - loss: 3.2825 - accuracy: 0.16 - ETA: 2:29 - loss: 3.2828 - accuracy: 0.16 - ETA: 2:28 - loss: 3.2955 - accuracy: 0.16 - ETA: 2:27 - loss: 3.3042 - accuracy: 0.15 - ETA: 2:26 - loss: 3.3044 - accuracy: 0.15 - ETA: 2:25 - loss: 3.3110 - accuracy: 0.15 - ETA: 2:24 - loss: 3.3146 - accuracy: 0.15 - ETA: 2:24 - loss: 3.3123 - accuracy: 0.15 - ETA: 2:24 - loss: 3.3083 - accuracy: 0.15 - ETA: 2:24 - loss: 3.3034 - accuracy: 0.15 - ETA: 2:23 - loss: 3.3001 - accuracy: 0.15 - ETA: 2:22 - loss: 3.3040 - accuracy: 0.15 - ETA: 2:21 - loss: 3.3036 - accuracy: 0.15 - ETA: 2:20 - loss: 3.3039 - accuracy: 0.15 - ETA: 2:19 - loss: 3.3070 - accuracy: 0.15 - ETA: 2:19 - loss: 3.3096 - accuracy: 0.15 - ETA: 2:18 - loss: 3.3039 - accuracy: 0.15 - ETA: 2:17 - loss: 3.3012 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2987 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2973 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2955 - accuracy: 0.15 - ETA: 2:16 - loss: 3.3025 - accuracy: 0.15 - ETA: 2:16 - loss: 3.3012 - accuracy: 0.15 - ETA: 2:16 - loss: 3.3026 - accuracy: 0.15 - ETA: 2:15 - loss: 3.3030 - accuracy: 0.15 - ETA: 2:15 - loss: 3.3020 - accuracy: 0.15 - ETA: 2:14 - loss: 3.3034 - accuracy: 0.15 - ETA: 2:14 - loss: 3.3063 - accuracy: 0.15 - ETA: 2:14 - loss: 3.3213 - accuracy: 0.15 - ETA: 2:13 - loss: 3.3231 - accuracy: 0.15 - ETA: 2:13 - loss: 3.3252 - accuracy: 0.15 - ETA: 2:12 - loss: 3.3226 - accuracy: 0.15 - ETA: 2:12 - loss: 3.3229 - accuracy: 0.15 - ETA: 2:11 - loss: 3.3242 - accuracy: 0.15 - ETA: 2:11 - loss: 3.3253 - accuracy: 0.15 - ETA: 2:10 - loss: 3.3237 - accuracy: 0.15 - ETA: 2:10 - loss: 3.3231 - accuracy: 0.15 - ETA: 2:10 - loss: 3.3224 - accuracy: 0.15 - ETA: 2:09 - loss: 3.3228 - accuracy: 0.15 - ETA: 2:09 - loss: 3.3203 - accuracy: 0.15 - ETA: 2:08 - loss: 3.3205 - accuracy: 0.15 - ETA: 2:08 - loss: 3.3192 - accuracy: 0.15 - ETA: 2:08 - loss: 3.3190 - accuracy: 0.15 - ETA: 2:07 - loss: 3.3186 - accuracy: 0.15 - ETA: 2:07 - loss: 3.3168 - accuracy: 0.15 - ETA: 2:06 - loss: 3.3188 - accuracy: 0.15 - ETA: 2:06 - loss: 3.3192 - accuracy: 0.15 - ETA: 2:05 - loss: 3.3177 - accuracy: 0.15 - ETA: 2:05 - loss: 3.3173 - accuracy: 0.15 - ETA: 2:04 - loss: 3.3184 - accuracy: 0.15 - ETA: 2:04 - loss: 3.3200 - accuracy: 0.15 - ETA: 2:04 - loss: 3.3199 - accuracy: 0.15 - ETA: 2:03 - loss: 3.3211 - accuracy: 0.15 - ETA: 2:03 - loss: 3.3199 - accuracy: 0.15 - ETA: 2:02 - loss: 3.3193 - accuracy: 0.15 - ETA: 2:02 - loss: 3.3193 - accuracy: 0.15 - ETA: 2:02 - loss: 3.3190 - accuracy: 0.15 - ETA: 2:01 - loss: 3.3187 - accuracy: 0.15 - ETA: 2:01 - loss: 3.3177 - accuracy: 0.15 - ETA: 2:00 - loss: 3.3173 - accuracy: 0.15 - ETA: 2:00 - loss: 3.3152 - accuracy: 0.15 - ETA: 1:59 - loss: 3.3136 - accuracy: 0.15 - ETA: 1:58 - loss: 3.3159 - accuracy: 0.15 - ETA: 1:58 - loss: 3.3184 - accuracy: 0.15 - ETA: 1:57 - loss: 3.3165 - accuracy: 0.15 - ETA: 1:57 - loss: 3.3157 - accuracy: 0.15 - ETA: 1:56 - loss: 3.3154 - accuracy: 0.15 - ETA: 1:55 - loss: 3.3152 - accuracy: 0.15 - ETA: 1:55 - loss: 3.3161 - accuracy: 0.15 - ETA: 1:54 - loss: 3.3169 - accuracy: 0.15 - ETA: 1:54 - loss: 3.3148 - accuracy: 0.15 - ETA: 1:53 - loss: 3.3124 - accuracy: 0.15 - ETA: 1:53 - loss: 3.3128 - accuracy: 0.15 - ETA: 1:52 - loss: 3.3115 - accuracy: 0.15 - ETA: 1:52 - loss: 3.3113 - accuracy: 0.15 - ETA: 1:51 - loss: 3.3114 - accuracy: 0.15 - ETA: 1:51 - loss: 3.3101 - accuracy: 0.15 - ETA: 1:50 - loss: 3.3122 - accuracy: 0.15 - ETA: 1:49 - loss: 3.3115 - accuracy: 0.15 - ETA: 1:49 - loss: 3.3107 - accuracy: 0.15 - ETA: 1:48 - loss: 3.3104 - accuracy: 0.15 - ETA: 1:48 - loss: 3.3099 - accuracy: 0.15 - ETA: 1:47 - loss: 3.3100 - accuracy: 0.15 - ETA: 1:47 - loss: 3.3099 - accuracy: 0.15 - ETA: 1:46 - loss: 3.3097 - accuracy: 0.15 - ETA: 1:46 - loss: 3.3106 - accuracy: 0.15 - ETA: 1:45 - loss: 3.3097 - accuracy: 0.15 - ETA: 1:45 - loss: 3.3085 - accuracy: 0.15 - ETA: 1:44 - loss: 3.3067 - accuracy: 0.15 - ETA: 1:44 - loss: 3.3055 - accuracy: 0.15 - ETA: 1:44 - loss: 3.3058 - accuracy: 0.15 - ETA: 1:43 - loss: 3.3041 - accuracy: 0.15 - ETA: 1:43 - loss: 3.3036 - accuracy: 0.15 - ETA: 1:42 - loss: 3.3028 - accuracy: 0.15 - ETA: 1:42 - loss: 3.3016 - accuracy: 0.15 - ETA: 1:41 - loss: 3.3016 - accuracy: 0.15 - ETA: 1:41 - loss: 3.3016 - accuracy: 0.15 - ETA: 1:40 - loss: 3.3016 - accuracy: 0.15 - ETA: 1:40 - loss: 3.3002 - accuracy: 0.15 - ETA: 1:39 - loss: 3.3007 - accuracy: 0.15 - ETA: 1:39 - loss: 3.3007 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2994 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2997 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2984 - accuracy: 0.15 - ETA: 1:37 - loss: 3.3001 - accuracy: 0.15 - ETA: 1:37 - loss: 3.3014 - accuracy: 0.15 - ETA: 1:36 - loss: 3.3004 - accuracy: 0.15 - ETA: 1:36 - loss: 3.3007 - accuracy: 0.15 - ETA: 1:35 - loss: 3.3002 - accuracy: 0.15 - ETA: 1:35 - loss: 3.3002 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2992 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2981 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2987 - accuracy: 0.15 - ETA: 1:33 - loss: 3.3003 - accuracy: 0.15 - ETA: 1:33 - loss: 3.3004 - accuracy: 0.15 - ETA: 1:32 - loss: 3.3002 - accuracy: 0.15 - ETA: 1:32 - loss: 3.3004 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2987 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2986 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2985 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2985 - accuracy: 0.15 - ETA: 1:30 - loss: 3.3003 - accuracy: 0.15 - ETA: 1:29 - loss: 3.3013 - accuracy: 0.15 - ETA: 1:29 - loss: 3.3004 - accuracy: 0.15 - ETA: 1:28 - loss: 3.3009 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2997 - accuracy: 0.15 - ETA: 1:27 - loss: 3.3001 - accuracy: 0.15 - ETA: 1:27 - loss: 3.3005 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2999 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2996 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2998 - accuracy: 0.15 - ETA: 1:25 - loss: 3.3007 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2999 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2996 - accuracy: 0.15 - ETA: 1:23 - loss: 3.3002 - accuracy: 0.15 - ETA: 1:23 - loss: 3.3002 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2998 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2980 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2974 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2966 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2944 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2939 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2945 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2942 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2938 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2936 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2932 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2935 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2922 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2917 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2911 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2915 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2919 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2917 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2913 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2910 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2920 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2917 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2922 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2924 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2927 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2926 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2924 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2927 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2920 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2924 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2919 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2919 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2925 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2920 - accuracy: 0.1522"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:06 - loss: 3.2910 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2899 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2897 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2907 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2910 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2908 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2909 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2911 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2907 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2913 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2916 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2915 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2924 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2924 - accuracy: 0.15 - ETA: 59s - loss: 3.2921 - accuracy: 0.1521 - ETA: 59s - loss: 3.2924 - accuracy: 0.152 - ETA: 58s - loss: 3.2914 - accuracy: 0.152 - ETA: 58s - loss: 3.2924 - accuracy: 0.151 - ETA: 57s - loss: 3.2919 - accuracy: 0.152 - ETA: 57s - loss: 3.2927 - accuracy: 0.151 - ETA: 56s - loss: 3.2924 - accuracy: 0.152 - ETA: 56s - loss: 3.2912 - accuracy: 0.152 - ETA: 56s - loss: 3.2905 - accuracy: 0.152 - ETA: 55s - loss: 3.2907 - accuracy: 0.152 - ETA: 55s - loss: 3.2903 - accuracy: 0.152 - ETA: 54s - loss: 3.2898 - accuracy: 0.152 - ETA: 54s - loss: 3.2893 - accuracy: 0.152 - ETA: 53s - loss: 3.2892 - accuracy: 0.152 - ETA: 53s - loss: 3.2884 - accuracy: 0.152 - ETA: 52s - loss: 3.2884 - accuracy: 0.152 - ETA: 52s - loss: 3.2872 - accuracy: 0.152 - ETA: 51s - loss: 3.2872 - accuracy: 0.152 - ETA: 51s - loss: 3.2878 - accuracy: 0.152 - ETA: 50s - loss: 3.2870 - accuracy: 0.152 - ETA: 50s - loss: 3.2882 - accuracy: 0.152 - ETA: 50s - loss: 3.2887 - accuracy: 0.152 - ETA: 49s - loss: 3.2900 - accuracy: 0.152 - ETA: 49s - loss: 3.2912 - accuracy: 0.152 - ETA: 48s - loss: 3.2904 - accuracy: 0.152 - ETA: 48s - loss: 3.2915 - accuracy: 0.152 - ETA: 47s - loss: 3.2920 - accuracy: 0.152 - ETA: 47s - loss: 3.2922 - accuracy: 0.152 - ETA: 46s - loss: 3.2921 - accuracy: 0.152 - ETA: 46s - loss: 3.2922 - accuracy: 0.152 - ETA: 45s - loss: 3.2923 - accuracy: 0.152 - ETA: 45s - loss: 3.2927 - accuracy: 0.152 - ETA: 44s - loss: 3.2925 - accuracy: 0.152 - ETA: 44s - loss: 3.2929 - accuracy: 0.152 - ETA: 43s - loss: 3.2923 - accuracy: 0.152 - ETA: 43s - loss: 3.2925 - accuracy: 0.152 - ETA: 43s - loss: 3.2924 - accuracy: 0.152 - ETA: 42s - loss: 3.2920 - accuracy: 0.152 - ETA: 42s - loss: 3.2926 - accuracy: 0.152 - ETA: 41s - loss: 3.2924 - accuracy: 0.152 - ETA: 41s - loss: 3.2918 - accuracy: 0.152 - ETA: 40s - loss: 3.2925 - accuracy: 0.152 - ETA: 40s - loss: 3.2929 - accuracy: 0.152 - ETA: 39s - loss: 3.2941 - accuracy: 0.151 - ETA: 39s - loss: 3.2937 - accuracy: 0.152 - ETA: 38s - loss: 3.2946 - accuracy: 0.151 - ETA: 38s - loss: 3.2941 - accuracy: 0.151 - ETA: 37s - loss: 3.2947 - accuracy: 0.151 - ETA: 37s - loss: 3.2946 - accuracy: 0.151 - ETA: 37s - loss: 3.2946 - accuracy: 0.151 - ETA: 36s - loss: 3.2952 - accuracy: 0.151 - ETA: 36s - loss: 3.2952 - accuracy: 0.151 - ETA: 35s - loss: 3.2960 - accuracy: 0.151 - ETA: 35s - loss: 3.2961 - accuracy: 0.151 - ETA: 34s - loss: 3.2956 - accuracy: 0.151 - ETA: 34s - loss: 3.2957 - accuracy: 0.151 - ETA: 33s - loss: 3.2966 - accuracy: 0.151 - ETA: 33s - loss: 3.2962 - accuracy: 0.151 - ETA: 32s - loss: 3.2965 - accuracy: 0.151 - ETA: 32s - loss: 3.2964 - accuracy: 0.151 - ETA: 31s - loss: 3.2969 - accuracy: 0.151 - ETA: 31s - loss: 3.2961 - accuracy: 0.151 - ETA: 31s - loss: 3.2962 - accuracy: 0.151 - ETA: 30s - loss: 3.2960 - accuracy: 0.151 - ETA: 30s - loss: 3.2965 - accuracy: 0.151 - ETA: 29s - loss: 3.2964 - accuracy: 0.151 - ETA: 29s - loss: 3.2965 - accuracy: 0.151 - ETA: 28s - loss: 3.2961 - accuracy: 0.151 - ETA: 28s - loss: 3.2957 - accuracy: 0.151 - ETA: 27s - loss: 3.2958 - accuracy: 0.151 - ETA: 27s - loss: 3.2953 - accuracy: 0.151 - ETA: 26s - loss: 3.2952 - accuracy: 0.151 - ETA: 26s - loss: 3.2949 - accuracy: 0.151 - ETA: 26s - loss: 3.2948 - accuracy: 0.151 - ETA: 25s - loss: 3.2941 - accuracy: 0.152 - ETA: 25s - loss: 3.2937 - accuracy: 0.152 - ETA: 24s - loss: 3.2935 - accuracy: 0.152 - ETA: 24s - loss: 3.2930 - accuracy: 0.152 - ETA: 23s - loss: 3.2929 - accuracy: 0.152 - ETA: 23s - loss: 3.2931 - accuracy: 0.152 - ETA: 22s - loss: 3.2929 - accuracy: 0.152 - ETA: 22s - loss: 3.2942 - accuracy: 0.152 - ETA: 21s - loss: 3.2941 - accuracy: 0.152 - ETA: 21s - loss: 3.2940 - accuracy: 0.152 - ETA: 21s - loss: 3.2939 - accuracy: 0.152 - ETA: 20s - loss: 3.2942 - accuracy: 0.152 - ETA: 20s - loss: 3.2942 - accuracy: 0.152 - ETA: 19s - loss: 3.2939 - accuracy: 0.152 - ETA: 19s - loss: 3.2944 - accuracy: 0.152 - ETA: 18s - loss: 3.2956 - accuracy: 0.151 - ETA: 18s - loss: 3.2961 - accuracy: 0.151 - ETA: 17s - loss: 3.2969 - accuracy: 0.151 - ETA: 17s - loss: 3.2977 - accuracy: 0.151 - ETA: 16s - loss: 3.2972 - accuracy: 0.152 - ETA: 16s - loss: 3.2987 - accuracy: 0.151 - ETA: 15s - loss: 3.2989 - accuracy: 0.151 - ETA: 15s - loss: 3.2990 - accuracy: 0.151 - ETA: 15s - loss: 3.2996 - accuracy: 0.151 - ETA: 14s - loss: 3.2989 - accuracy: 0.151 - ETA: 14s - loss: 3.2985 - accuracy: 0.151 - ETA: 13s - loss: 3.2976 - accuracy: 0.152 - ETA: 13s - loss: 3.2976 - accuracy: 0.151 - ETA: 12s - loss: 3.2975 - accuracy: 0.151 - ETA: 12s - loss: 3.2972 - accuracy: 0.151 - ETA: 11s - loss: 3.2964 - accuracy: 0.152 - ETA: 11s - loss: 3.2964 - accuracy: 0.152 - ETA: 10s - loss: 3.2964 - accuracy: 0.152 - ETA: 10s - loss: 3.2962 - accuracy: 0.152 - ETA: 10s - loss: 3.2966 - accuracy: 0.152 - ETA: 9s - loss: 3.2960 - accuracy: 0.152 - ETA: 9s - loss: 3.2969 - accuracy: 0.15 - ETA: 8s - loss: 3.2975 - accuracy: 0.15 - ETA: 8s - loss: 3.2973 - accuracy: 0.15 - ETA: 7s - loss: 3.2974 - accuracy: 0.15 - ETA: 7s - loss: 3.2975 - accuracy: 0.15 - ETA: 6s - loss: 3.2973 - accuracy: 0.15 - ETA: 6s - loss: 3.2975 - accuracy: 0.15 - ETA: 5s - loss: 3.2976 - accuracy: 0.15 - ETA: 5s - loss: 3.2974 - accuracy: 0.15 - ETA: 5s - loss: 3.2975 - accuracy: 0.15 - ETA: 4s - loss: 3.2984 - accuracy: 0.15 - ETA: 4s - loss: 3.2979 - accuracy: 0.15 - ETA: 3s - loss: 3.2982 - accuracy: 0.15 - ETA: 3s - loss: 3.2983 - accuracy: 0.15 - ETA: 2s - loss: 3.2984 - accuracy: 0.15 - ETA: 2s - loss: 3.2984 - accuracy: 0.15 - ETA: 1s - loss: 3.2982 - accuracy: 0.15 - ETA: 1s - loss: 3.2978 - accuracy: 0.15 - ETA: 0s - loss: 3.2973 - accuracy: 0.15 - ETA: 0s - loss: 3.2970 - accuracy: 0.15 - ETA: 0s - loss: 3.2972 - accuracy: 0.15 - 161s 4ms/step - loss: 3.2971 - accuracy: 0.1519 - val_loss: 4.1974 - val_accuracy: 0.0233\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:50 - loss: 3.2327 - accuracy: 0.17 - ETA: 2:44 - loss: 3.2910 - accuracy: 0.16 - ETA: 2:42 - loss: 3.3253 - accuracy: 0.15 - ETA: 2:37 - loss: 3.3271 - accuracy: 0.14 - ETA: 2:32 - loss: 3.3238 - accuracy: 0.15 - ETA: 2:29 - loss: 3.3319 - accuracy: 0.14 - ETA: 2:27 - loss: 3.3063 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3012 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2857 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2909 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2940 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2952 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2940 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2939 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2903 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2930 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2895 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2864 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2865 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2899 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2791 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2783 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2772 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2757 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2758 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2771 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2726 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2652 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2719 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2642 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2668 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2619 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2652 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2649 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2582 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2528 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2540 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2530 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2526 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2481 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2496 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2518 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2486 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2503 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2488 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2465 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2457 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2468 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2454 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2460 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2451 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2448 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2453 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2440 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2437 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2427 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2433 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2430 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2458 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2477 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2479 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2495 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2513 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2489 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2485 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2496 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2497 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2524 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2516 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2510 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2475 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2513 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2508 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2493 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2503 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2508 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2499 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2517 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2523 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2544 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2549 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2552 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2556 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2553 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2573 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2579 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2582 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2589 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2576 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2593 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2590 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2590 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2605 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2605 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2623 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2641 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2627 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2639 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2630 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2647 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2659 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2666 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2682 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2690 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2697 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2683 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2685 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2694 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2709 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2714 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2733 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2731 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2720 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2725 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2729 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2731 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2738 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2750 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2748 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2750 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2763 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2778 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2793 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2775 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2764 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2770 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2772 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2773 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2759 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2747 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2755 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2750 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2748 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2746 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2743 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2733 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2741 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2739 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2743 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2739 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2741 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2732 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2737 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2733 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2713 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2705 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2715 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2718 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2707 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2714 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2702 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2692 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2690 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2691 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2689 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2681 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2672 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2669 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2666 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2669 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2663 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2653 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2653 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2652 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2647 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2637 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2634 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2639 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2626 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2632 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2634 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2627 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2639 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2646 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2644 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2637 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2631 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2629 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2625 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2609 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2603 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2603 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2610 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2603 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2604 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2601 - accuracy: 0.1535"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:04 - loss: 3.2598 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2592 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2591 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2589 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2582 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2584 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2585 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2588 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2582 - accuracy: 0.15 - ETA: 59s - loss: 3.2586 - accuracy: 0.1538 - ETA: 59s - loss: 3.2584 - accuracy: 0.154 - ETA: 59s - loss: 3.2575 - accuracy: 0.154 - ETA: 58s - loss: 3.2576 - accuracy: 0.154 - ETA: 58s - loss: 3.2577 - accuracy: 0.153 - ETA: 57s - loss: 3.2579 - accuracy: 0.154 - ETA: 57s - loss: 3.2572 - accuracy: 0.154 - ETA: 56s - loss: 3.2582 - accuracy: 0.153 - ETA: 56s - loss: 3.2584 - accuracy: 0.153 - ETA: 55s - loss: 3.2580 - accuracy: 0.153 - ETA: 55s - loss: 3.2578 - accuracy: 0.153 - ETA: 55s - loss: 3.2580 - accuracy: 0.153 - ETA: 54s - loss: 3.2581 - accuracy: 0.153 - ETA: 54s - loss: 3.2582 - accuracy: 0.153 - ETA: 53s - loss: 3.2590 - accuracy: 0.153 - ETA: 53s - loss: 3.2597 - accuracy: 0.153 - ETA: 52s - loss: 3.2599 - accuracy: 0.153 - ETA: 52s - loss: 3.2601 - accuracy: 0.153 - ETA: 51s - loss: 3.2608 - accuracy: 0.152 - ETA: 51s - loss: 3.2606 - accuracy: 0.152 - ETA: 51s - loss: 3.2615 - accuracy: 0.152 - ETA: 50s - loss: 3.2618 - accuracy: 0.152 - ETA: 50s - loss: 3.2621 - accuracy: 0.152 - ETA: 49s - loss: 3.2617 - accuracy: 0.152 - ETA: 49s - loss: 3.2620 - accuracy: 0.152 - ETA: 48s - loss: 3.2616 - accuracy: 0.153 - ETA: 48s - loss: 3.2614 - accuracy: 0.153 - ETA: 48s - loss: 3.2615 - accuracy: 0.153 - ETA: 47s - loss: 3.2604 - accuracy: 0.153 - ETA: 47s - loss: 3.2610 - accuracy: 0.153 - ETA: 46s - loss: 3.2613 - accuracy: 0.153 - ETA: 46s - loss: 3.2616 - accuracy: 0.153 - ETA: 45s - loss: 3.2609 - accuracy: 0.153 - ETA: 45s - loss: 3.2605 - accuracy: 0.153 - ETA: 44s - loss: 3.2609 - accuracy: 0.153 - ETA: 44s - loss: 3.2607 - accuracy: 0.153 - ETA: 44s - loss: 3.2609 - accuracy: 0.153 - ETA: 43s - loss: 3.2609 - accuracy: 0.153 - ETA: 43s - loss: 3.2609 - accuracy: 0.153 - ETA: 42s - loss: 3.2610 - accuracy: 0.153 - ETA: 42s - loss: 3.2613 - accuracy: 0.153 - ETA: 41s - loss: 3.2613 - accuracy: 0.153 - ETA: 41s - loss: 3.2611 - accuracy: 0.152 - ETA: 40s - loss: 3.2609 - accuracy: 0.152 - ETA: 40s - loss: 3.2614 - accuracy: 0.152 - ETA: 40s - loss: 3.2610 - accuracy: 0.152 - ETA: 39s - loss: 3.2605 - accuracy: 0.152 - ETA: 39s - loss: 3.2608 - accuracy: 0.152 - ETA: 38s - loss: 3.2606 - accuracy: 0.152 - ETA: 38s - loss: 3.2612 - accuracy: 0.152 - ETA: 37s - loss: 3.2611 - accuracy: 0.152 - ETA: 37s - loss: 3.2604 - accuracy: 0.152 - ETA: 36s - loss: 3.2598 - accuracy: 0.152 - ETA: 36s - loss: 3.2589 - accuracy: 0.153 - ETA: 36s - loss: 3.2592 - accuracy: 0.152 - ETA: 35s - loss: 3.2591 - accuracy: 0.152 - ETA: 35s - loss: 3.2586 - accuracy: 0.152 - ETA: 34s - loss: 3.2590 - accuracy: 0.152 - ETA: 34s - loss: 3.2588 - accuracy: 0.152 - ETA: 33s - loss: 3.2594 - accuracy: 0.152 - ETA: 33s - loss: 3.2587 - accuracy: 0.152 - ETA: 32s - loss: 3.2588 - accuracy: 0.152 - ETA: 32s - loss: 3.2584 - accuracy: 0.153 - ETA: 32s - loss: 3.2585 - accuracy: 0.153 - ETA: 31s - loss: 3.2583 - accuracy: 0.153 - ETA: 31s - loss: 3.2576 - accuracy: 0.153 - ETA: 30s - loss: 3.2576 - accuracy: 0.153 - ETA: 30s - loss: 3.2573 - accuracy: 0.153 - ETA: 29s - loss: 3.2575 - accuracy: 0.153 - ETA: 29s - loss: 3.2569 - accuracy: 0.153 - ETA: 28s - loss: 3.2569 - accuracy: 0.153 - ETA: 28s - loss: 3.2566 - accuracy: 0.153 - ETA: 28s - loss: 3.2568 - accuracy: 0.153 - ETA: 27s - loss: 3.2565 - accuracy: 0.153 - ETA: 27s - loss: 3.2567 - accuracy: 0.153 - ETA: 26s - loss: 3.2565 - accuracy: 0.153 - ETA: 26s - loss: 3.2562 - accuracy: 0.153 - ETA: 25s - loss: 3.2562 - accuracy: 0.153 - ETA: 25s - loss: 3.2558 - accuracy: 0.154 - ETA: 24s - loss: 3.2567 - accuracy: 0.153 - ETA: 24s - loss: 3.2573 - accuracy: 0.153 - ETA: 24s - loss: 3.2572 - accuracy: 0.153 - ETA: 23s - loss: 3.2573 - accuracy: 0.153 - ETA: 23s - loss: 3.2575 - accuracy: 0.153 - ETA: 22s - loss: 3.2568 - accuracy: 0.153 - ETA: 22s - loss: 3.2570 - accuracy: 0.153 - ETA: 21s - loss: 3.2572 - accuracy: 0.154 - ETA: 21s - loss: 3.2572 - accuracy: 0.154 - ETA: 20s - loss: 3.2568 - accuracy: 0.154 - ETA: 20s - loss: 3.2561 - accuracy: 0.154 - ETA: 20s - loss: 3.2563 - accuracy: 0.154 - ETA: 19s - loss: 3.2562 - accuracy: 0.154 - ETA: 19s - loss: 3.2564 - accuracy: 0.154 - ETA: 18s - loss: 3.2569 - accuracy: 0.154 - ETA: 18s - loss: 3.2570 - accuracy: 0.154 - ETA: 17s - loss: 3.2572 - accuracy: 0.153 - ETA: 17s - loss: 3.2570 - accuracy: 0.153 - ETA: 16s - loss: 3.2576 - accuracy: 0.153 - ETA: 16s - loss: 3.2578 - accuracy: 0.153 - ETA: 16s - loss: 3.2583 - accuracy: 0.153 - ETA: 15s - loss: 3.2585 - accuracy: 0.153 - ETA: 15s - loss: 3.2585 - accuracy: 0.153 - ETA: 14s - loss: 3.2585 - accuracy: 0.153 - ETA: 14s - loss: 3.2586 - accuracy: 0.153 - ETA: 13s - loss: 3.2584 - accuracy: 0.153 - ETA: 13s - loss: 3.2578 - accuracy: 0.153 - ETA: 12s - loss: 3.2580 - accuracy: 0.153 - ETA: 12s - loss: 3.2579 - accuracy: 0.153 - ETA: 12s - loss: 3.2576 - accuracy: 0.153 - ETA: 11s - loss: 3.2571 - accuracy: 0.153 - ETA: 11s - loss: 3.2561 - accuracy: 0.154 - ETA: 10s - loss: 3.2561 - accuracy: 0.154 - ETA: 10s - loss: 3.2563 - accuracy: 0.153 - ETA: 9s - loss: 3.2563 - accuracy: 0.153 - ETA: 9s - loss: 3.2563 - accuracy: 0.15 - ETA: 8s - loss: 3.2564 - accuracy: 0.15 - ETA: 8s - loss: 3.2555 - accuracy: 0.15 - ETA: 8s - loss: 3.2557 - accuracy: 0.15 - ETA: 7s - loss: 3.2557 - accuracy: 0.15 - ETA: 7s - loss: 3.2558 - accuracy: 0.15 - ETA: 6s - loss: 3.2556 - accuracy: 0.15 - ETA: 6s - loss: 3.2555 - accuracy: 0.15 - ETA: 5s - loss: 3.2561 - accuracy: 0.15 - ETA: 5s - loss: 3.2559 - accuracy: 0.15 - ETA: 4s - loss: 3.2554 - accuracy: 0.15 - ETA: 4s - loss: 3.2556 - accuracy: 0.15 - ETA: 4s - loss: 3.2555 - accuracy: 0.15 - ETA: 3s - loss: 3.2560 - accuracy: 0.15 - ETA: 3s - loss: 3.2557 - accuracy: 0.15 - ETA: 2s - loss: 3.2558 - accuracy: 0.15 - ETA: 2s - loss: 3.2563 - accuracy: 0.15 - ETA: 1s - loss: 3.2569 - accuracy: 0.15 - ETA: 1s - loss: 3.2568 - accuracy: 0.15 - ETA: 0s - loss: 3.2562 - accuracy: 0.15 - ETA: 0s - loss: 3.2557 - accuracy: 0.15 - ETA: 0s - loss: 3.2553 - accuracy: 0.15 - 158s 4ms/step - loss: 3.2553 - accuracy: 0.1544 - val_loss: 4.3810 - val_accuracy: 0.0307\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:42 - loss: 3.2989 - accuracy: 0.17 - ETA: 2:32 - loss: 3.2891 - accuracy: 0.17 - ETA: 2:30 - loss: 3.2736 - accuracy: 0.16 - ETA: 2:28 - loss: 3.2131 - accuracy: 0.17 - ETA: 2:27 - loss: 3.2334 - accuracy: 0.17 - ETA: 2:29 - loss: 3.2306 - accuracy: 0.17 - ETA: 2:32 - loss: 3.2129 - accuracy: 0.16 - ETA: 2:34 - loss: 3.2245 - accuracy: 0.16 - ETA: 2:33 - loss: 3.2255 - accuracy: 0.16 - ETA: 2:31 - loss: 3.2300 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2314 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2208 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2276 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2340 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2240 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2325 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2354 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2416 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2428 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2477 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2486 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2539 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2514 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2552 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2510 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2517 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2561 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2488 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2556 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2440 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2469 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2463 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2483 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2487 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2496 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2505 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2547 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2543 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2546 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2549 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2534 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2541 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2527 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2542 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2548 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2577 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2571 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2587 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2593 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2633 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2617 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2647 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2634 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2602 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2612 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2605 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2626 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2636 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2623 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2641 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2652 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2669 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2691 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2684 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2731 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2762 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2753 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2774 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2778 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2794 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2783 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2769 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2745 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2770 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2746 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2741 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2765 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2771 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2777 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2772 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2788 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2766 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2763 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2750 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2753 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2756 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2814 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2784 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2780 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2770 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2765 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2762 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2763 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2751 - accuracy: 0.14 - ETA: 1:50 - loss: 3.2739 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2732 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2736 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2717 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2714 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2709 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2706 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2677 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2677 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2663 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2643 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2653 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2655 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2642 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2636 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2635 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2620 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2624 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2632 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2633 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2639 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2629 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2633 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2625 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2614 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2593 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2589 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2589 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2609 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2627 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2649 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2678 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2688 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2689 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2673 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2674 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2662 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2651 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2651 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2637 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2635 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2624 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2622 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2624 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2617 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2622 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2616 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2620 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2628 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2634 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2634 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2630 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2627 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2626 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2621 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2633 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2627 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2636 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2631 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2632 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2622 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2633 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2640 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2633 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2637 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2641 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2645 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2653 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2644 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2641 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2641 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2641 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2629 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2638 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2630 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2635 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2630 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2630 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2627 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2634 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2624 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2618 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2621 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2626 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2638 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2645 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2650 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2658 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2656 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2653 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2653 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2647 - accuracy: 0.1539"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:06 - loss: 3.2644 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2654 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2661 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2656 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2651 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2665 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2666 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2657 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2664 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2675 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2679 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2670 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2660 - accuracy: 0.15 - ETA: 59s - loss: 3.2661 - accuracy: 0.1531 - ETA: 59s - loss: 3.2657 - accuracy: 0.153 - ETA: 58s - loss: 3.2651 - accuracy: 0.153 - ETA: 58s - loss: 3.2650 - accuracy: 0.153 - ETA: 58s - loss: 3.2650 - accuracy: 0.153 - ETA: 57s - loss: 3.2660 - accuracy: 0.152 - ETA: 57s - loss: 3.2660 - accuracy: 0.153 - ETA: 56s - loss: 3.2655 - accuracy: 0.153 - ETA: 56s - loss: 3.2655 - accuracy: 0.153 - ETA: 55s - loss: 3.2649 - accuracy: 0.153 - ETA: 55s - loss: 3.2638 - accuracy: 0.153 - ETA: 54s - loss: 3.2639 - accuracy: 0.153 - ETA: 54s - loss: 3.2630 - accuracy: 0.153 - ETA: 53s - loss: 3.2624 - accuracy: 0.154 - ETA: 53s - loss: 3.2621 - accuracy: 0.154 - ETA: 52s - loss: 3.2615 - accuracy: 0.154 - ETA: 52s - loss: 3.2618 - accuracy: 0.154 - ETA: 52s - loss: 3.2620 - accuracy: 0.154 - ETA: 51s - loss: 3.2618 - accuracy: 0.154 - ETA: 51s - loss: 3.2621 - accuracy: 0.154 - ETA: 50s - loss: 3.2630 - accuracy: 0.154 - ETA: 50s - loss: 3.2625 - accuracy: 0.154 - ETA: 49s - loss: 3.2620 - accuracy: 0.154 - ETA: 49s - loss: 3.2624 - accuracy: 0.154 - ETA: 48s - loss: 3.2631 - accuracy: 0.154 - ETA: 48s - loss: 3.2630 - accuracy: 0.154 - ETA: 47s - loss: 3.2637 - accuracy: 0.154 - ETA: 47s - loss: 3.2627 - accuracy: 0.154 - ETA: 46s - loss: 3.2634 - accuracy: 0.154 - ETA: 46s - loss: 3.2635 - accuracy: 0.154 - ETA: 46s - loss: 3.2638 - accuracy: 0.154 - ETA: 45s - loss: 3.2639 - accuracy: 0.154 - ETA: 45s - loss: 3.2642 - accuracy: 0.153 - ETA: 44s - loss: 3.2646 - accuracy: 0.153 - ETA: 44s - loss: 3.2648 - accuracy: 0.153 - ETA: 43s - loss: 3.2655 - accuracy: 0.153 - ETA: 43s - loss: 3.2655 - accuracy: 0.153 - ETA: 42s - loss: 3.2663 - accuracy: 0.153 - ETA: 42s - loss: 3.2663 - accuracy: 0.153 - ETA: 41s - loss: 3.2662 - accuracy: 0.153 - ETA: 41s - loss: 3.2667 - accuracy: 0.153 - ETA: 40s - loss: 3.2669 - accuracy: 0.153 - ETA: 40s - loss: 3.2663 - accuracy: 0.153 - ETA: 40s - loss: 3.2673 - accuracy: 0.153 - ETA: 39s - loss: 3.2672 - accuracy: 0.153 - ETA: 39s - loss: 3.2684 - accuracy: 0.153 - ETA: 38s - loss: 3.2690 - accuracy: 0.152 - ETA: 38s - loss: 3.2694 - accuracy: 0.152 - ETA: 37s - loss: 3.2696 - accuracy: 0.152 - ETA: 37s - loss: 3.2699 - accuracy: 0.152 - ETA: 36s - loss: 3.2698 - accuracy: 0.152 - ETA: 36s - loss: 3.2703 - accuracy: 0.152 - ETA: 35s - loss: 3.2707 - accuracy: 0.152 - ETA: 35s - loss: 3.2708 - accuracy: 0.152 - ETA: 35s - loss: 3.2710 - accuracy: 0.152 - ETA: 34s - loss: 3.2713 - accuracy: 0.152 - ETA: 34s - loss: 3.2729 - accuracy: 0.152 - ETA: 33s - loss: 3.2734 - accuracy: 0.152 - ETA: 33s - loss: 3.2740 - accuracy: 0.152 - ETA: 32s - loss: 3.2745 - accuracy: 0.151 - ETA: 32s - loss: 3.2744 - accuracy: 0.151 - ETA: 31s - loss: 3.2745 - accuracy: 0.151 - ETA: 31s - loss: 3.2747 - accuracy: 0.151 - ETA: 30s - loss: 3.2755 - accuracy: 0.151 - ETA: 30s - loss: 3.2753 - accuracy: 0.151 - ETA: 29s - loss: 3.2755 - accuracy: 0.151 - ETA: 29s - loss: 3.2759 - accuracy: 0.151 - ETA: 29s - loss: 3.2759 - accuracy: 0.151 - ETA: 28s - loss: 3.2755 - accuracy: 0.151 - ETA: 28s - loss: 3.2758 - accuracy: 0.151 - ETA: 27s - loss: 3.2754 - accuracy: 0.151 - ETA: 27s - loss: 3.2759 - accuracy: 0.151 - ETA: 26s - loss: 3.2758 - accuracy: 0.151 - ETA: 26s - loss: 3.2764 - accuracy: 0.151 - ETA: 25s - loss: 3.2769 - accuracy: 0.151 - ETA: 25s - loss: 3.2768 - accuracy: 0.151 - ETA: 24s - loss: 3.2768 - accuracy: 0.151 - ETA: 24s - loss: 3.2773 - accuracy: 0.151 - ETA: 24s - loss: 3.2776 - accuracy: 0.151 - ETA: 23s - loss: 3.2775 - accuracy: 0.151 - ETA: 23s - loss: 3.2777 - accuracy: 0.151 - ETA: 22s - loss: 3.2776 - accuracy: 0.151 - ETA: 22s - loss: 3.2767 - accuracy: 0.151 - ETA: 21s - loss: 3.2768 - accuracy: 0.151 - ETA: 21s - loss: 3.2766 - accuracy: 0.151 - ETA: 20s - loss: 3.2761 - accuracy: 0.151 - ETA: 20s - loss: 3.2764 - accuracy: 0.151 - ETA: 19s - loss: 3.2763 - accuracy: 0.151 - ETA: 19s - loss: 3.2759 - accuracy: 0.151 - ETA: 19s - loss: 3.2758 - accuracy: 0.151 - ETA: 18s - loss: 3.2753 - accuracy: 0.151 - ETA: 18s - loss: 3.2746 - accuracy: 0.151 - ETA: 17s - loss: 3.2750 - accuracy: 0.151 - ETA: 17s - loss: 3.2751 - accuracy: 0.151 - ETA: 16s - loss: 3.2749 - accuracy: 0.151 - ETA: 16s - loss: 3.2748 - accuracy: 0.151 - ETA: 15s - loss: 3.2749 - accuracy: 0.151 - ETA: 15s - loss: 3.2749 - accuracy: 0.151 - ETA: 14s - loss: 3.2748 - accuracy: 0.151 - ETA: 14s - loss: 3.2748 - accuracy: 0.151 - ETA: 14s - loss: 3.2750 - accuracy: 0.151 - ETA: 13s - loss: 3.2747 - accuracy: 0.151 - ETA: 13s - loss: 3.2748 - accuracy: 0.151 - ETA: 12s - loss: 3.2742 - accuracy: 0.151 - ETA: 12s - loss: 3.2739 - accuracy: 0.151 - ETA: 11s - loss: 3.2740 - accuracy: 0.151 - ETA: 11s - loss: 3.2741 - accuracy: 0.151 - ETA: 10s - loss: 3.2736 - accuracy: 0.151 - ETA: 10s - loss: 3.2733 - accuracy: 0.151 - ETA: 9s - loss: 3.2728 - accuracy: 0.151 - ETA: 9s - loss: 3.2731 - accuracy: 0.15 - ETA: 9s - loss: 3.2726 - accuracy: 0.15 - ETA: 8s - loss: 3.2723 - accuracy: 0.15 - ETA: 8s - loss: 3.2721 - accuracy: 0.15 - ETA: 7s - loss: 3.2724 - accuracy: 0.15 - ETA: 7s - loss: 3.2723 - accuracy: 0.15 - ETA: 6s - loss: 3.2718 - accuracy: 0.15 - ETA: 6s - loss: 3.2724 - accuracy: 0.15 - ETA: 5s - loss: 3.2725 - accuracy: 0.15 - ETA: 5s - loss: 3.2723 - accuracy: 0.15 - ETA: 5s - loss: 3.2730 - accuracy: 0.15 - ETA: 4s - loss: 3.2725 - accuracy: 0.15 - ETA: 4s - loss: 3.2729 - accuracy: 0.15 - ETA: 3s - loss: 3.2729 - accuracy: 0.15 - ETA: 3s - loss: 3.2729 - accuracy: 0.15 - ETA: 2s - loss: 3.2730 - accuracy: 0.15 - ETA: 2s - loss: 3.2729 - accuracy: 0.15 - ETA: 1s - loss: 3.2727 - accuracy: 0.15 - ETA: 1s - loss: 3.2724 - accuracy: 0.15 - ETA: 0s - loss: 3.2724 - accuracy: 0.15 - ETA: 0s - loss: 3.2720 - accuracy: 0.15 - ETA: 0s - loss: 3.2717 - accuracy: 0.15 - 160s 4ms/step - loss: 3.2717 - accuracy: 0.1515 - val_loss: 4.4962 - val_accuracy: 0.0323\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:41 - loss: 3.2155 - accuracy: 0.17 - ETA: 2:31 - loss: 3.1556 - accuracy: 0.16 - ETA: 2:28 - loss: 3.1983 - accuracy: 0.16 - ETA: 2:26 - loss: 3.2428 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2798 - accuracy: 0.13 - ETA: 2:29 - loss: 3.2553 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2588 - accuracy: 0.14 - ETA: 2:31 - loss: 3.2810 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2673 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2876 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2766 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2677 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2849 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2814 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2844 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2756 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2750 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2833 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2830 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2855 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2905 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2784 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2777 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2814 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2787 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2682 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2714 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2762 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2719 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2716 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2748 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2794 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2779 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2771 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2770 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2804 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2729 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2719 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2713 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2665 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2617 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2601 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2624 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2645 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2600 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2599 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2617 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2621 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2606 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2604 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2573 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2598 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2608 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2577 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2571 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2557 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2590 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2568 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2583 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2586 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2578 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2581 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2567 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2546 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2556 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2546 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2548 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2577 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2568 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2560 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2578 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2558 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2535 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2547 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2533 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2530 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2523 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2540 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2509 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2511 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2509 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2498 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2502 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2485 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2473 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2482 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2504 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2473 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2469 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2457 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2427 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2419 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2397 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2411 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2424 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2431 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2424 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2416 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2417 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2409 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2417 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2418 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2402 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2413 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2424 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2404 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2393 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2403 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2406 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2387 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2376 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2376 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2366 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2374 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2372 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2364 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2366 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2365 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2376 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2365 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2366 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2379 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2353 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2361 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2354 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2336 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2317 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2307 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2305 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2303 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2302 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2304 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2301 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2295 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2295 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2295 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2295 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2302 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2309 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2314 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2315 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2312 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2311 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2315 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2355 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2362 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2363 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2349 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2348 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2344 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2329 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2323 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2325 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2331 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2335 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2334 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2323 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2321 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2327 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2342 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2346 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2341 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2344 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2337 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2329 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2325 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2324 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2327 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2329 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2320 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2324 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2312 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2313 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2317 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2323 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2335 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2330 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2324 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2322 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2329 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2327 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2327 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2325 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2326 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2318 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2310 - accuracy: 0.1579"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:04 - loss: 3.2310 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2306 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2304 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2305 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2305 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2304 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2309 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2311 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2312 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2299 - accuracy: 0.15 - ETA: 59s - loss: 3.2300 - accuracy: 0.1588 - ETA: 59s - loss: 3.2299 - accuracy: 0.158 - ETA: 58s - loss: 3.2294 - accuracy: 0.158 - ETA: 58s - loss: 3.2285 - accuracy: 0.159 - ETA: 57s - loss: 3.2286 - accuracy: 0.159 - ETA: 57s - loss: 3.2292 - accuracy: 0.158 - ETA: 56s - loss: 3.2295 - accuracy: 0.158 - ETA: 56s - loss: 3.2282 - accuracy: 0.158 - ETA: 56s - loss: 3.2279 - accuracy: 0.158 - ETA: 55s - loss: 3.2280 - accuracy: 0.158 - ETA: 55s - loss: 3.2277 - accuracy: 0.159 - ETA: 54s - loss: 3.2275 - accuracy: 0.158 - ETA: 54s - loss: 3.2275 - accuracy: 0.158 - ETA: 53s - loss: 3.2275 - accuracy: 0.158 - ETA: 53s - loss: 3.2279 - accuracy: 0.158 - ETA: 52s - loss: 3.2275 - accuracy: 0.158 - ETA: 52s - loss: 3.2277 - accuracy: 0.158 - ETA: 52s - loss: 3.2265 - accuracy: 0.159 - ETA: 51s - loss: 3.2261 - accuracy: 0.158 - ETA: 51s - loss: 3.2257 - accuracy: 0.159 - ETA: 50s - loss: 3.2258 - accuracy: 0.159 - ETA: 50s - loss: 3.2263 - accuracy: 0.158 - ETA: 49s - loss: 3.2262 - accuracy: 0.159 - ETA: 49s - loss: 3.2262 - accuracy: 0.158 - ETA: 48s - loss: 3.2282 - accuracy: 0.158 - ETA: 48s - loss: 3.2293 - accuracy: 0.158 - ETA: 48s - loss: 3.2291 - accuracy: 0.158 - ETA: 47s - loss: 3.2291 - accuracy: 0.158 - ETA: 47s - loss: 3.2290 - accuracy: 0.158 - ETA: 46s - loss: 3.2290 - accuracy: 0.158 - ETA: 46s - loss: 3.2291 - accuracy: 0.158 - ETA: 45s - loss: 3.2288 - accuracy: 0.158 - ETA: 45s - loss: 3.2284 - accuracy: 0.158 - ETA: 44s - loss: 3.2291 - accuracy: 0.157 - ETA: 44s - loss: 3.2291 - accuracy: 0.157 - ETA: 44s - loss: 3.2292 - accuracy: 0.157 - ETA: 43s - loss: 3.2286 - accuracy: 0.157 - ETA: 43s - loss: 3.2287 - accuracy: 0.157 - ETA: 42s - loss: 3.2290 - accuracy: 0.157 - ETA: 42s - loss: 3.2293 - accuracy: 0.157 - ETA: 41s - loss: 3.2287 - accuracy: 0.157 - ETA: 41s - loss: 3.2293 - accuracy: 0.157 - ETA: 40s - loss: 3.2290 - accuracy: 0.157 - ETA: 40s - loss: 3.2294 - accuracy: 0.157 - ETA: 40s - loss: 3.2292 - accuracy: 0.157 - ETA: 39s - loss: 3.2299 - accuracy: 0.157 - ETA: 39s - loss: 3.2291 - accuracy: 0.157 - ETA: 38s - loss: 3.2291 - accuracy: 0.157 - ETA: 38s - loss: 3.2281 - accuracy: 0.157 - ETA: 37s - loss: 3.2280 - accuracy: 0.157 - ETA: 37s - loss: 3.2283 - accuracy: 0.157 - ETA: 36s - loss: 3.2280 - accuracy: 0.157 - ETA: 36s - loss: 3.2287 - accuracy: 0.157 - ETA: 36s - loss: 3.2288 - accuracy: 0.157 - ETA: 35s - loss: 3.2296 - accuracy: 0.157 - ETA: 35s - loss: 3.2297 - accuracy: 0.157 - ETA: 34s - loss: 3.2297 - accuracy: 0.157 - ETA: 34s - loss: 3.2294 - accuracy: 0.157 - ETA: 33s - loss: 3.2299 - accuracy: 0.157 - ETA: 33s - loss: 3.2303 - accuracy: 0.157 - ETA: 33s - loss: 3.2304 - accuracy: 0.157 - ETA: 32s - loss: 3.2309 - accuracy: 0.157 - ETA: 32s - loss: 3.2312 - accuracy: 0.157 - ETA: 31s - loss: 3.2307 - accuracy: 0.157 - ETA: 31s - loss: 3.2305 - accuracy: 0.157 - ETA: 30s - loss: 3.2308 - accuracy: 0.157 - ETA: 30s - loss: 3.2305 - accuracy: 0.157 - ETA: 29s - loss: 3.2307 - accuracy: 0.157 - ETA: 29s - loss: 3.2307 - accuracy: 0.157 - ETA: 29s - loss: 3.2307 - accuracy: 0.157 - ETA: 28s - loss: 3.2309 - accuracy: 0.157 - ETA: 28s - loss: 3.2309 - accuracy: 0.157 - ETA: 27s - loss: 3.2309 - accuracy: 0.157 - ETA: 27s - loss: 3.2304 - accuracy: 0.157 - ETA: 26s - loss: 3.2300 - accuracy: 0.157 - ETA: 26s - loss: 3.2301 - accuracy: 0.157 - ETA: 25s - loss: 3.2300 - accuracy: 0.157 - ETA: 25s - loss: 3.2291 - accuracy: 0.157 - ETA: 24s - loss: 3.2300 - accuracy: 0.157 - ETA: 24s - loss: 3.2298 - accuracy: 0.157 - ETA: 24s - loss: 3.2299 - accuracy: 0.157 - ETA: 23s - loss: 3.2296 - accuracy: 0.157 - ETA: 23s - loss: 3.2297 - accuracy: 0.157 - ETA: 22s - loss: 3.2300 - accuracy: 0.157 - ETA: 22s - loss: 3.2295 - accuracy: 0.157 - ETA: 21s - loss: 3.2293 - accuracy: 0.157 - ETA: 21s - loss: 3.2292 - accuracy: 0.157 - ETA: 20s - loss: 3.2284 - accuracy: 0.157 - ETA: 20s - loss: 3.2284 - accuracy: 0.158 - ETA: 20s - loss: 3.2284 - accuracy: 0.158 - ETA: 19s - loss: 3.2287 - accuracy: 0.158 - ETA: 19s - loss: 3.2284 - accuracy: 0.158 - ETA: 18s - loss: 3.2289 - accuracy: 0.157 - ETA: 18s - loss: 3.2289 - accuracy: 0.157 - ETA: 17s - loss: 3.2286 - accuracy: 0.157 - ETA: 17s - loss: 3.2290 - accuracy: 0.157 - ETA: 16s - loss: 3.2282 - accuracy: 0.157 - ETA: 16s - loss: 3.2282 - accuracy: 0.157 - ETA: 16s - loss: 3.2278 - accuracy: 0.157 - ETA: 15s - loss: 3.2276 - accuracy: 0.158 - ETA: 15s - loss: 3.2275 - accuracy: 0.158 - ETA: 14s - loss: 3.2284 - accuracy: 0.158 - ETA: 14s - loss: 3.2289 - accuracy: 0.158 - ETA: 13s - loss: 3.2313 - accuracy: 0.158 - ETA: 13s - loss: 3.2333 - accuracy: 0.157 - ETA: 12s - loss: 3.2336 - accuracy: 0.157 - ETA: 12s - loss: 3.2344 - accuracy: 0.157 - ETA: 12s - loss: 3.2346 - accuracy: 0.157 - ETA: 11s - loss: 3.2348 - accuracy: 0.157 - ETA: 11s - loss: 3.2349 - accuracy: 0.157 - ETA: 10s - loss: 3.2345 - accuracy: 0.157 - ETA: 10s - loss: 3.2344 - accuracy: 0.157 - ETA: 9s - loss: 3.2344 - accuracy: 0.157 - ETA: 9s - loss: 3.2343 - accuracy: 0.15 - ETA: 8s - loss: 3.2336 - accuracy: 0.15 - ETA: 8s - loss: 3.2340 - accuracy: 0.15 - ETA: 8s - loss: 3.2345 - accuracy: 0.15 - ETA: 7s - loss: 3.2348 - accuracy: 0.15 - ETA: 7s - loss: 3.2353 - accuracy: 0.15 - ETA: 6s - loss: 3.2363 - accuracy: 0.15 - ETA: 6s - loss: 3.2373 - accuracy: 0.15 - ETA: 5s - loss: 3.2375 - accuracy: 0.15 - ETA: 5s - loss: 3.2374 - accuracy: 0.15 - ETA: 4s - loss: 3.2378 - accuracy: 0.15 - ETA: 4s - loss: 3.2380 - accuracy: 0.15 - ETA: 4s - loss: 3.2380 - accuracy: 0.15 - ETA: 3s - loss: 3.2377 - accuracy: 0.15 - ETA: 3s - loss: 3.2376 - accuracy: 0.15 - ETA: 2s - loss: 3.2377 - accuracy: 0.15 - ETA: 2s - loss: 3.2377 - accuracy: 0.15 - ETA: 1s - loss: 3.2373 - accuracy: 0.15 - ETA: 1s - loss: 3.2378 - accuracy: 0.15 - ETA: 0s - loss: 3.2379 - accuracy: 0.15 - ETA: 0s - loss: 3.2380 - accuracy: 0.15 - ETA: 0s - loss: 3.2377 - accuracy: 0.15 - 158s 4ms/step - loss: 3.2378 - accuracy: 0.1568 - val_loss: 4.6004 - val_accuracy: 0.0224\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:56 - loss: 3.2840 - accuracy: 0.17 - ETA: 2:37 - loss: 3.3487 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2979 - accuracy: 0.15 - ETA: 2:29 - loss: 3.3432 - accuracy: 0.14 - ETA: 2:27 - loss: 3.3481 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3519 - accuracy: 0.15 - ETA: 2:25 - loss: 3.3544 - accuracy: 0.15 - ETA: 2:24 - loss: 3.3541 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3629 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3466 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3555 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3264 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3216 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3090 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3064 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2995 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3004 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3088 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3057 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3035 - accuracy: 0.14 - ETA: 2:19 - loss: 3.3053 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2952 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2941 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2828 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2813 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2866 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2921 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2869 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2896 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2930 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2895 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2903 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2837 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2879 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2879 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2894 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2859 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2823 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2819 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2820 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2755 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2763 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2817 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2738 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2714 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2722 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2743 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2747 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2758 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2768 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2748 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2762 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2753 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2758 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2737 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2731 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2744 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2768 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2731 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2744 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2751 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2740 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2746 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2743 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2734 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2726 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2736 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2734 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2738 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2768 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2759 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2722 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2704 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2703 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2699 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2685 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2702 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2689 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2681 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2688 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2719 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2733 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2722 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2749 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2740 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2744 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2722 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2755 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2742 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2751 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2743 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2723 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2712 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2702 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2684 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2701 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2706 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2708 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2696 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2676 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2686 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2689 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2688 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2692 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2678 - accuracy: 0.15 - ETA: 1:41 - loss: 3.2674 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2683 - accuracy: 0.15 - ETA: 1:40 - loss: 3.2681 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2683 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2681 - accuracy: 0.15 - ETA: 1:39 - loss: 3.2670 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2658 - accuracy: 0.15 - ETA: 1:38 - loss: 3.2646 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2640 - accuracy: 0.15 - ETA: 1:37 - loss: 3.2629 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2628 - accuracy: 0.15 - ETA: 1:36 - loss: 3.2613 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2594 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2567 - accuracy: 0.15 - ETA: 1:35 - loss: 3.2557 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2540 - accuracy: 0.15 - ETA: 1:34 - loss: 3.2528 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2529 - accuracy: 0.15 - ETA: 1:33 - loss: 3.2527 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2528 - accuracy: 0.15 - ETA: 1:32 - loss: 3.2521 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2510 - accuracy: 0.15 - ETA: 1:31 - loss: 3.2503 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2490 - accuracy: 0.15 - ETA: 1:30 - loss: 3.2496 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2494 - accuracy: 0.15 - ETA: 1:29 - loss: 3.2492 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2505 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2512 - accuracy: 0.15 - ETA: 1:28 - loss: 3.2510 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2510 - accuracy: 0.15 - ETA: 1:27 - loss: 3.2518 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2523 - accuracy: 0.15 - ETA: 1:26 - loss: 3.2526 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2526 - accuracy: 0.15 - ETA: 1:25 - loss: 3.2525 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2526 - accuracy: 0.15 - ETA: 1:24 - loss: 3.2532 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2532 - accuracy: 0.15 - ETA: 1:23 - loss: 3.2536 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2533 - accuracy: 0.15 - ETA: 1:22 - loss: 3.2546 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2538 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2535 - accuracy: 0.15 - ETA: 1:21 - loss: 3.2533 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2553 - accuracy: 0.15 - ETA: 1:20 - loss: 3.2562 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2570 - accuracy: 0.15 - ETA: 1:19 - loss: 3.2572 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2555 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2551 - accuracy: 0.15 - ETA: 1:18 - loss: 3.2551 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2549 - accuracy: 0.15 - ETA: 1:17 - loss: 3.2544 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2546 - accuracy: 0.15 - ETA: 1:16 - loss: 3.2520 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2529 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2538 - accuracy: 0.15 - ETA: 1:15 - loss: 3.2551 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2555 - accuracy: 0.15 - ETA: 1:14 - loss: 3.2565 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2560 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2555 - accuracy: 0.15 - ETA: 1:13 - loss: 3.2552 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2558 - accuracy: 0.15 - ETA: 1:12 - loss: 3.2562 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2563 - accuracy: 0.15 - ETA: 1:11 - loss: 3.2550 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2543 - accuracy: 0.15 - ETA: 1:10 - loss: 3.2538 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2527 - accuracy: 0.15 - ETA: 1:09 - loss: 3.2532 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2529 - accuracy: 0.15 - ETA: 1:08 - loss: 3.2541 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2547 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2548 - accuracy: 0.15 - ETA: 1:07 - loss: 3.2549 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2544 - accuracy: 0.15 - ETA: 1:06 - loss: 3.2556 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2558 - accuracy: 0.15 - ETA: 1:05 - loss: 3.2556 - accuracy: 0.1553"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:04 - loss: 3.2555 - accuracy: 0.15 - ETA: 1:04 - loss: 3.2553 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2541 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2541 - accuracy: 0.15 - ETA: 1:03 - loss: 3.2537 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2542 - accuracy: 0.15 - ETA: 1:02 - loss: 3.2545 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2540 - accuracy: 0.15 - ETA: 1:01 - loss: 3.2540 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2534 - accuracy: 0.15 - ETA: 1:00 - loss: 3.2527 - accuracy: 0.15 - ETA: 59s - loss: 3.2519 - accuracy: 0.1559 - ETA: 59s - loss: 3.2523 - accuracy: 0.155 - ETA: 59s - loss: 3.2521 - accuracy: 0.155 - ETA: 58s - loss: 3.2523 - accuracy: 0.155 - ETA: 58s - loss: 3.2533 - accuracy: 0.155 - ETA: 57s - loss: 3.2532 - accuracy: 0.155 - ETA: 57s - loss: 3.2541 - accuracy: 0.155 - ETA: 56s - loss: 3.2545 - accuracy: 0.155 - ETA: 56s - loss: 3.2542 - accuracy: 0.155 - ETA: 55s - loss: 3.2549 - accuracy: 0.155 - ETA: 55s - loss: 3.2543 - accuracy: 0.155 - ETA: 54s - loss: 3.2543 - accuracy: 0.155 - ETA: 54s - loss: 3.2547 - accuracy: 0.155 - ETA: 53s - loss: 3.2543 - accuracy: 0.155 - ETA: 53s - loss: 3.2547 - accuracy: 0.155 - ETA: 53s - loss: 3.2547 - accuracy: 0.155 - ETA: 52s - loss: 3.2554 - accuracy: 0.155 - ETA: 52s - loss: 3.2551 - accuracy: 0.155 - ETA: 51s - loss: 3.2547 - accuracy: 0.155 - ETA: 51s - loss: 3.2544 - accuracy: 0.155 - ETA: 50s - loss: 3.2543 - accuracy: 0.155 - ETA: 50s - loss: 3.2544 - accuracy: 0.155 - ETA: 49s - loss: 3.2549 - accuracy: 0.155 - ETA: 49s - loss: 3.2543 - accuracy: 0.155 - ETA: 48s - loss: 3.2542 - accuracy: 0.155 - ETA: 48s - loss: 3.2543 - accuracy: 0.155 - ETA: 48s - loss: 3.2539 - accuracy: 0.155 - ETA: 47s - loss: 3.2548 - accuracy: 0.155 - ETA: 47s - loss: 3.2544 - accuracy: 0.156 - ETA: 46s - loss: 3.2541 - accuracy: 0.156 - ETA: 46s - loss: 3.2546 - accuracy: 0.156 - ETA: 45s - loss: 3.2549 - accuracy: 0.155 - ETA: 45s - loss: 3.2562 - accuracy: 0.155 - ETA: 44s - loss: 3.2568 - accuracy: 0.155 - ETA: 44s - loss: 3.2566 - accuracy: 0.155 - ETA: 44s - loss: 3.2575 - accuracy: 0.155 - ETA: 43s - loss: 3.2567 - accuracy: 0.155 - ETA: 43s - loss: 3.2567 - accuracy: 0.155 - ETA: 42s - loss: 3.2567 - accuracy: 0.155 - ETA: 42s - loss: 3.2574 - accuracy: 0.155 - ETA: 41s - loss: 3.2567 - accuracy: 0.155 - ETA: 41s - loss: 3.2567 - accuracy: 0.155 - ETA: 40s - loss: 3.2564 - accuracy: 0.155 - ETA: 40s - loss: 3.2567 - accuracy: 0.155 - ETA: 39s - loss: 3.2568 - accuracy: 0.155 - ETA: 39s - loss: 3.2565 - accuracy: 0.155 - ETA: 39s - loss: 3.2562 - accuracy: 0.155 - ETA: 38s - loss: 3.2568 - accuracy: 0.155 - ETA: 38s - loss: 3.2571 - accuracy: 0.155 - ETA: 37s - loss: 3.2574 - accuracy: 0.155 - ETA: 37s - loss: 3.2574 - accuracy: 0.155 - ETA: 36s - loss: 3.2574 - accuracy: 0.155 - ETA: 36s - loss: 3.2575 - accuracy: 0.155 - ETA: 35s - loss: 3.2573 - accuracy: 0.155 - ETA: 35s - loss: 3.2573 - accuracy: 0.155 - ETA: 34s - loss: 3.2575 - accuracy: 0.155 - ETA: 34s - loss: 3.2576 - accuracy: 0.155 - ETA: 34s - loss: 3.2569 - accuracy: 0.155 - ETA: 33s - loss: 3.2574 - accuracy: 0.155 - ETA: 33s - loss: 3.2574 - accuracy: 0.155 - ETA: 32s - loss: 3.2577 - accuracy: 0.154 - ETA: 32s - loss: 3.2572 - accuracy: 0.155 - ETA: 31s - loss: 3.2577 - accuracy: 0.155 - ETA: 31s - loss: 3.2581 - accuracy: 0.155 - ETA: 30s - loss: 3.2583 - accuracy: 0.155 - ETA: 30s - loss: 3.2586 - accuracy: 0.155 - ETA: 30s - loss: 3.2583 - accuracy: 0.155 - ETA: 29s - loss: 3.2584 - accuracy: 0.155 - ETA: 29s - loss: 3.2582 - accuracy: 0.155 - ETA: 28s - loss: 3.2580 - accuracy: 0.155 - ETA: 28s - loss: 3.2579 - accuracy: 0.155 - ETA: 27s - loss: 3.2570 - accuracy: 0.155 - ETA: 27s - loss: 3.2571 - accuracy: 0.155 - ETA: 26s - loss: 3.2571 - accuracy: 0.155 - ETA: 26s - loss: 3.2564 - accuracy: 0.155 - ETA: 26s - loss: 3.2564 - accuracy: 0.155 - ETA: 25s - loss: 3.2562 - accuracy: 0.155 - ETA: 25s - loss: 3.2562 - accuracy: 0.155 - ETA: 24s - loss: 3.2560 - accuracy: 0.155 - ETA: 24s - loss: 3.2555 - accuracy: 0.155 - ETA: 23s - loss: 3.2551 - accuracy: 0.155 - ETA: 23s - loss: 3.2555 - accuracy: 0.155 - ETA: 22s - loss: 3.2550 - accuracy: 0.155 - ETA: 22s - loss: 3.2540 - accuracy: 0.155 - ETA: 21s - loss: 3.2539 - accuracy: 0.155 - ETA: 21s - loss: 3.2536 - accuracy: 0.155 - ETA: 21s - loss: 3.2537 - accuracy: 0.155 - ETA: 20s - loss: 3.2533 - accuracy: 0.155 - ETA: 20s - loss: 3.2535 - accuracy: 0.155 - ETA: 19s - loss: 3.2535 - accuracy: 0.155 - ETA: 19s - loss: 3.2532 - accuracy: 0.155 - ETA: 18s - loss: 3.2532 - accuracy: 0.155 - ETA: 18s - loss: 3.2536 - accuracy: 0.155 - ETA: 17s - loss: 3.2534 - accuracy: 0.155 - ETA: 17s - loss: 3.2535 - accuracy: 0.155 - ETA: 17s - loss: 3.2538 - accuracy: 0.155 - ETA: 16s - loss: 3.2536 - accuracy: 0.155 - ETA: 16s - loss: 3.2535 - accuracy: 0.155 - ETA: 15s - loss: 3.2532 - accuracy: 0.155 - ETA: 15s - loss: 3.2530 - accuracy: 0.155 - ETA: 14s - loss: 3.2534 - accuracy: 0.155 - ETA: 14s - loss: 3.2533 - accuracy: 0.155 - ETA: 13s - loss: 3.2532 - accuracy: 0.155 - ETA: 13s - loss: 3.2536 - accuracy: 0.155 - ETA: 13s - loss: 3.2534 - accuracy: 0.155 - ETA: 12s - loss: 3.2528 - accuracy: 0.155 - ETA: 12s - loss: 3.2527 - accuracy: 0.155 - ETA: 11s - loss: 3.2524 - accuracy: 0.155 - ETA: 11s - loss: 3.2524 - accuracy: 0.155 - ETA: 10s - loss: 3.2522 - accuracy: 0.155 - ETA: 10s - loss: 3.2521 - accuracy: 0.155 - ETA: 9s - loss: 3.2525 - accuracy: 0.155 - ETA: 9s - loss: 3.2525 - accuracy: 0.15 - ETA: 8s - loss: 3.2523 - accuracy: 0.15 - ETA: 8s - loss: 3.2526 - accuracy: 0.15 - ETA: 8s - loss: 3.2522 - accuracy: 0.15 - ETA: 7s - loss: 3.2519 - accuracy: 0.15 - ETA: 7s - loss: 3.2516 - accuracy: 0.15 - ETA: 6s - loss: 3.2515 - accuracy: 0.15 - ETA: 6s - loss: 3.2516 - accuracy: 0.15 - ETA: 5s - loss: 3.2517 - accuracy: 0.15 - ETA: 5s - loss: 3.2510 - accuracy: 0.15 - ETA: 4s - loss: 3.2506 - accuracy: 0.15 - ETA: 4s - loss: 3.2503 - accuracy: 0.15 - ETA: 4s - loss: 3.2501 - accuracy: 0.15 - ETA: 3s - loss: 3.2500 - accuracy: 0.15 - ETA: 3s - loss: 3.2501 - accuracy: 0.15 - ETA: 2s - loss: 3.2500 - accuracy: 0.15 - ETA: 2s - loss: 3.2499 - accuracy: 0.15 - ETA: 1s - loss: 3.2505 - accuracy: 0.15 - ETA: 1s - loss: 3.2505 - accuracy: 0.15 - ETA: 0s - loss: 3.2507 - accuracy: 0.15 - ETA: 0s - loss: 3.2508 - accuracy: 0.15 - ETA: 0s - loss: 3.2510 - accuracy: 0.15 - 159s 4ms/step - loss: 3.2510 - accuracy: 0.1558 - val_loss: 4.0476 - val_accuracy: 0.0276\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:42 - loss: 3.1250 - accuracy: 0.16 - ETA: 2:30 - loss: 3.1980 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2371 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2048 - accuracy: 0.14 - ETA: 2:28 - loss: 3.2001 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2147 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2342 - accuracy: 0.13 - ETA: 2:23 - loss: 3.2364 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2557 - accuracy: 0.13 - ETA: 2:22 - loss: 3.2620 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2547 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2493 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2443 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2388 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2447 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2406 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2455 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2589 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2652 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2736 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2744 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2781 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2794 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2895 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2964 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3056 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3149 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3183 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3201 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3124 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3153 - accuracy: 0.14 - ETA: 2:12 - loss: 3.3097 - accuracy: 0.14 - ETA: 2:12 - loss: 3.3107 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3095 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3087 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3148 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3145 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3137 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3142 - accuracy: 0.14 - ETA: 2:09 - loss: 3.3178 - accuracy: 0.14 - ETA: 2:08 - loss: 3.3198 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3176 - accuracy: 0.14 - ETA: 2:07 - loss: 3.3134 - accuracy: 0.14 - ETA: 2:07 - loss: 3.3124 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3143 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3185 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3197 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3215 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3236 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3249 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3324 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3335 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3390 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3380 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3384 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3365 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3369 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3378 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3384 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3341 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3367 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3358 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3371 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3384 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3412 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3416 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3407 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3377 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3346 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3360 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3352 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3371 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3355 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3327 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3319 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3337 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3327 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3333 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3335 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3350 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3359 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3348 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3352 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3389 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3391 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3494 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3505 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3512 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3507 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3507 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3495 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3498 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3505 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3522 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3522 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3521 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3538 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3540 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3537 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3511 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3516 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3514 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3522 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3521 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3515 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3515 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3520 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3526 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3543 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3539 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3535 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3525 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3513 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3525 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3511 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3498 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3502 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3507 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3521 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3513 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3527 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3522 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3520 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3525 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3524 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3531 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3523 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3533 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3532 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3532 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3533 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3532 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3526 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3526 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3536 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3537 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3535 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3534 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3538 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3531 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3538 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3526 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3526 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3528 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3524 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3523 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3533 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3529 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3534 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3543 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3551 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3559 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3554 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3549 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3552 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3546 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3547 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3569 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3568 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3570 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3569 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3561 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3576 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3602 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3607 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3605 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3599 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3609 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3606 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3614 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3611 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3602 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3600 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3604 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3599 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3601 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3597 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3592 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3592 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3593 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3593 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3586 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3582 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3586 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3587 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3580 - accuracy: 0.1354"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:04 - loss: 3.3568 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3563 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3560 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3566 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3561 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3559 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3572 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3567 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3571 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3573 - accuracy: 0.13 - ETA: 59s - loss: 3.3569 - accuracy: 0.1361 - ETA: 59s - loss: 3.3568 - accuracy: 0.136 - ETA: 58s - loss: 3.3561 - accuracy: 0.136 - ETA: 58s - loss: 3.3559 - accuracy: 0.136 - ETA: 58s - loss: 3.3556 - accuracy: 0.136 - ETA: 57s - loss: 3.3550 - accuracy: 0.136 - ETA: 57s - loss: 3.3551 - accuracy: 0.136 - ETA: 56s - loss: 3.3554 - accuracy: 0.136 - ETA: 56s - loss: 3.3557 - accuracy: 0.135 - ETA: 55s - loss: 3.3555 - accuracy: 0.135 - ETA: 55s - loss: 3.3556 - accuracy: 0.135 - ETA: 54s - loss: 3.3559 - accuracy: 0.135 - ETA: 54s - loss: 3.3556 - accuracy: 0.135 - ETA: 54s - loss: 3.3548 - accuracy: 0.135 - ETA: 53s - loss: 3.3558 - accuracy: 0.135 - ETA: 53s - loss: 3.3557 - accuracy: 0.135 - ETA: 52s - loss: 3.3553 - accuracy: 0.135 - ETA: 52s - loss: 3.3557 - accuracy: 0.135 - ETA: 51s - loss: 3.3566 - accuracy: 0.135 - ETA: 51s - loss: 3.3567 - accuracy: 0.135 - ETA: 50s - loss: 3.3573 - accuracy: 0.135 - ETA: 50s - loss: 3.3576 - accuracy: 0.135 - ETA: 50s - loss: 3.3573 - accuracy: 0.135 - ETA: 49s - loss: 3.3573 - accuracy: 0.135 - ETA: 49s - loss: 3.3576 - accuracy: 0.135 - ETA: 48s - loss: 3.3577 - accuracy: 0.135 - ETA: 48s - loss: 3.3568 - accuracy: 0.135 - ETA: 47s - loss: 3.3565 - accuracy: 0.135 - ETA: 47s - loss: 3.3558 - accuracy: 0.135 - ETA: 46s - loss: 3.3558 - accuracy: 0.135 - ETA: 46s - loss: 3.3564 - accuracy: 0.135 - ETA: 45s - loss: 3.3565 - accuracy: 0.135 - ETA: 45s - loss: 3.3563 - accuracy: 0.136 - ETA: 45s - loss: 3.3559 - accuracy: 0.136 - ETA: 44s - loss: 3.3554 - accuracy: 0.136 - ETA: 44s - loss: 3.3556 - accuracy: 0.136 - ETA: 43s - loss: 3.3557 - accuracy: 0.136 - ETA: 43s - loss: 3.3550 - accuracy: 0.136 - ETA: 42s - loss: 3.3550 - accuracy: 0.136 - ETA: 42s - loss: 3.3545 - accuracy: 0.136 - ETA: 42s - loss: 3.3543 - accuracy: 0.136 - ETA: 41s - loss: 3.3539 - accuracy: 0.136 - ETA: 41s - loss: 3.3542 - accuracy: 0.136 - ETA: 40s - loss: 3.3539 - accuracy: 0.136 - ETA: 40s - loss: 3.3538 - accuracy: 0.136 - ETA: 39s - loss: 3.3534 - accuracy: 0.136 - ETA: 39s - loss: 3.3527 - accuracy: 0.137 - ETA: 38s - loss: 3.3525 - accuracy: 0.136 - ETA: 38s - loss: 3.3525 - accuracy: 0.136 - ETA: 37s - loss: 3.3527 - accuracy: 0.136 - ETA: 37s - loss: 3.3518 - accuracy: 0.137 - ETA: 37s - loss: 3.3524 - accuracy: 0.137 - ETA: 36s - loss: 3.3521 - accuracy: 0.137 - ETA: 36s - loss: 3.3510 - accuracy: 0.137 - ETA: 35s - loss: 3.3512 - accuracy: 0.137 - ETA: 35s - loss: 3.3509 - accuracy: 0.137 - ETA: 34s - loss: 3.3504 - accuracy: 0.137 - ETA: 34s - loss: 3.3504 - accuracy: 0.137 - ETA: 33s - loss: 3.3504 - accuracy: 0.137 - ETA: 33s - loss: 3.3505 - accuracy: 0.137 - ETA: 33s - loss: 3.3503 - accuracy: 0.137 - ETA: 32s - loss: 3.3503 - accuracy: 0.137 - ETA: 32s - loss: 3.3499 - accuracy: 0.137 - ETA: 31s - loss: 3.3500 - accuracy: 0.137 - ETA: 31s - loss: 3.3498 - accuracy: 0.137 - ETA: 30s - loss: 3.3490 - accuracy: 0.137 - ETA: 30s - loss: 3.3486 - accuracy: 0.137 - ETA: 29s - loss: 3.3484 - accuracy: 0.137 - ETA: 29s - loss: 3.3480 - accuracy: 0.137 - ETA: 29s - loss: 3.3481 - accuracy: 0.137 - ETA: 28s - loss: 3.3478 - accuracy: 0.137 - ETA: 28s - loss: 3.3474 - accuracy: 0.137 - ETA: 27s - loss: 3.3468 - accuracy: 0.138 - ETA: 27s - loss: 3.3467 - accuracy: 0.138 - ETA: 26s - loss: 3.3466 - accuracy: 0.138 - ETA: 26s - loss: 3.3467 - accuracy: 0.138 - ETA: 25s - loss: 3.3462 - accuracy: 0.138 - ETA: 25s - loss: 3.3449 - accuracy: 0.138 - ETA: 25s - loss: 3.3448 - accuracy: 0.138 - ETA: 24s - loss: 3.3449 - accuracy: 0.138 - ETA: 24s - loss: 3.3448 - accuracy: 0.138 - ETA: 23s - loss: 3.3444 - accuracy: 0.138 - ETA: 23s - loss: 3.3442 - accuracy: 0.138 - ETA: 22s - loss: 3.3432 - accuracy: 0.138 - ETA: 22s - loss: 3.3430 - accuracy: 0.138 - ETA: 21s - loss: 3.3424 - accuracy: 0.138 - ETA: 21s - loss: 3.3424 - accuracy: 0.138 - ETA: 21s - loss: 3.3422 - accuracy: 0.138 - ETA: 20s - loss: 3.3418 - accuracy: 0.138 - ETA: 20s - loss: 3.3410 - accuracy: 0.138 - ETA: 19s - loss: 3.3411 - accuracy: 0.138 - ETA: 19s - loss: 3.3410 - accuracy: 0.138 - ETA: 18s - loss: 3.3409 - accuracy: 0.138 - ETA: 18s - loss: 3.3414 - accuracy: 0.138 - ETA: 17s - loss: 3.3416 - accuracy: 0.138 - ETA: 17s - loss: 3.3413 - accuracy: 0.138 - ETA: 17s - loss: 3.3409 - accuracy: 0.138 - ETA: 16s - loss: 3.3411 - accuracy: 0.138 - ETA: 16s - loss: 3.3410 - accuracy: 0.138 - ETA: 15s - loss: 3.3410 - accuracy: 0.138 - ETA: 15s - loss: 3.3402 - accuracy: 0.138 - ETA: 14s - loss: 3.3405 - accuracy: 0.138 - ETA: 14s - loss: 3.3404 - accuracy: 0.138 - ETA: 13s - loss: 3.3403 - accuracy: 0.138 - ETA: 13s - loss: 3.3404 - accuracy: 0.138 - ETA: 12s - loss: 3.3404 - accuracy: 0.138 - ETA: 12s - loss: 3.3404 - accuracy: 0.138 - ETA: 12s - loss: 3.3407 - accuracy: 0.138 - ETA: 11s - loss: 3.3408 - accuracy: 0.138 - ETA: 11s - loss: 3.3403 - accuracy: 0.138 - ETA: 10s - loss: 3.3396 - accuracy: 0.138 - ETA: 10s - loss: 3.3388 - accuracy: 0.139 - ETA: 9s - loss: 3.3384 - accuracy: 0.139 - ETA: 9s - loss: 3.3381 - accuracy: 0.13 - ETA: 8s - loss: 3.3377 - accuracy: 0.13 - ETA: 8s - loss: 3.3367 - accuracy: 0.13 - ETA: 8s - loss: 3.3371 - accuracy: 0.13 - ETA: 7s - loss: 3.3370 - accuracy: 0.13 - ETA: 7s - loss: 3.3375 - accuracy: 0.13 - ETA: 6s - loss: 3.3369 - accuracy: 0.13 - ETA: 6s - loss: 3.3361 - accuracy: 0.13 - ETA: 5s - loss: 3.3360 - accuracy: 0.13 - ETA: 5s - loss: 3.3355 - accuracy: 0.13 - ETA: 4s - loss: 3.3356 - accuracy: 0.13 - ETA: 4s - loss: 3.3354 - accuracy: 0.13 - ETA: 4s - loss: 3.3352 - accuracy: 0.14 - ETA: 3s - loss: 3.3355 - accuracy: 0.13 - ETA: 3s - loss: 3.3360 - accuracy: 0.13 - ETA: 2s - loss: 3.3358 - accuracy: 0.13 - ETA: 2s - loss: 3.3361 - accuracy: 0.13 - ETA: 1s - loss: 3.3359 - accuracy: 0.13 - ETA: 1s - loss: 3.3358 - accuracy: 0.13 - ETA: 0s - loss: 3.3363 - accuracy: 0.14 - ETA: 0s - loss: 3.3360 - accuracy: 0.14 - ETA: 0s - loss: 3.3357 - accuracy: 0.14 - 158s 4ms/step - loss: 3.3354 - accuracy: 0.1401 - val_loss: 4.2441 - val_accuracy: 0.0256\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:41 - loss: 3.2628 - accuracy: 0.17 - ETA: 2:31 - loss: 3.3143 - accuracy: 0.16 - ETA: 2:28 - loss: 3.2245 - accuracy: 0.16 - ETA: 2:26 - loss: 3.2572 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2509 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2651 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2710 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2624 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2612 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2570 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2619 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2579 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2408 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2367 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2354 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2439 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2442 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2443 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2454 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2496 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2457 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2417 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2491 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2473 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2523 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2487 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2500 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2483 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2558 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2549 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2539 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2618 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2660 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2686 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2668 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2687 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2707 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2745 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2733 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2748 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2695 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2704 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2671 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2693 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2670 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2716 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2720 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2721 - accuracy: 0.14 - ETA: 2:05 - loss: 3.2701 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2701 - accuracy: 0.14 - ETA: 2:04 - loss: 3.2718 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2720 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2697 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2688 - accuracy: 0.14 - ETA: 2:03 - loss: 3.2696 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2696 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2688 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2726 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2757 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2751 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2790 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2796 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2791 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2783 - accuracy: 0.14 - ETA: 2:02 - loss: 3.2780 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2758 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2767 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2786 - accuracy: 0.14 - ETA: 2:01 - loss: 3.2799 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2826 - accuracy: 0.14 - ETA: 2:00 - loss: 3.2823 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2835 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2817 - accuracy: 0.14 - ETA: 1:59 - loss: 3.2848 - accuracy: 0.14 - ETA: 1:58 - loss: 3.2842 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2834 - accuracy: 0.14 - ETA: 1:57 - loss: 3.2835 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2858 - accuracy: 0.14 - ETA: 1:56 - loss: 3.2882 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2872 - accuracy: 0.14 - ETA: 1:55 - loss: 3.2860 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2858 - accuracy: 0.14 - ETA: 1:54 - loss: 3.2823 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2811 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2813 - accuracy: 0.14 - ETA: 1:53 - loss: 3.2812 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2797 - accuracy: 0.14 - ETA: 1:52 - loss: 3.2802 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2791 - accuracy: 0.14 - ETA: 1:51 - loss: 3.2779 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2764 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2777 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2788 - accuracy: 0.14 - ETA: 1:49 - loss: 3.2809 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2819 - accuracy: 0.14 - ETA: 1:48 - loss: 3.2810 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2833 - accuracy: 0.14 - ETA: 1:47 - loss: 3.2819 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2825 - accuracy: 0.14 - ETA: 1:46 - loss: 3.2816 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2831 - accuracy: 0.14 - ETA: 1:45 - loss: 3.2845 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2818 - accuracy: 0.14 - ETA: 1:44 - loss: 3.2828 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2818 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2831 - accuracy: 0.14 - ETA: 1:43 - loss: 3.2851 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2870 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2862 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2901 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2901 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2899 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2898 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2916 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2929 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2937 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2954 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2965 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2959 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2974 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2970 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2981 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2996 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3021 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3024 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3030 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3040 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3062 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3066 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3078 - accuracy: 0.14 - ETA: 1:31 - loss: 3.3084 - accuracy: 0.14 - ETA: 1:31 - loss: 3.3095 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3119 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3118 - accuracy: 0.14 - ETA: 1:29 - loss: 3.3120 - accuracy: 0.14 - ETA: 1:29 - loss: 3.3132 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3132 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3122 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3121 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3123 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3114 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3128 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3128 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3135 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3134 - accuracy: 0.14 - ETA: 1:24 - loss: 3.3124 - accuracy: 0.14 - ETA: 1:24 - loss: 3.3129 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3132 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3144 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3139 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3149 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3164 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3164 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3162 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3167 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3172 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3173 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3174 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3176 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3185 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3184 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3192 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3194 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3201 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3202 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3218 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3223 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3237 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3231 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3228 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3227 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3238 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3237 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3225 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3224 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3224 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3226 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3218 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3210 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3212 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3200 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3196 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3187 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3188 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3189 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3188 - accuracy: 0.1432"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:05 - loss: 3.3182 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3172 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3171 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3174 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3178 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3170 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3165 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3162 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3160 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3166 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3153 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3152 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3155 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3156 - accuracy: 0.14 - ETA: 59s - loss: 3.3159 - accuracy: 0.1435 - ETA: 59s - loss: 3.3155 - accuracy: 0.143 - ETA: 59s - loss: 3.3160 - accuracy: 0.143 - ETA: 58s - loss: 3.3172 - accuracy: 0.143 - ETA: 58s - loss: 3.3184 - accuracy: 0.143 - ETA: 57s - loss: 3.3194 - accuracy: 0.143 - ETA: 57s - loss: 3.3190 - accuracy: 0.143 - ETA: 56s - loss: 3.3191 - accuracy: 0.143 - ETA: 56s - loss: 3.3193 - accuracy: 0.143 - ETA: 55s - loss: 3.3197 - accuracy: 0.143 - ETA: 55s - loss: 3.3191 - accuracy: 0.143 - ETA: 54s - loss: 3.3189 - accuracy: 0.144 - ETA: 54s - loss: 3.3185 - accuracy: 0.144 - ETA: 53s - loss: 3.3193 - accuracy: 0.144 - ETA: 53s - loss: 3.3183 - accuracy: 0.144 - ETA: 52s - loss: 3.3188 - accuracy: 0.144 - ETA: 52s - loss: 3.3189 - accuracy: 0.144 - ETA: 52s - loss: 3.3190 - accuracy: 0.144 - ETA: 51s - loss: 3.3191 - accuracy: 0.144 - ETA: 51s - loss: 3.3194 - accuracy: 0.144 - ETA: 50s - loss: 3.3193 - accuracy: 0.144 - ETA: 50s - loss: 3.3199 - accuracy: 0.144 - ETA: 49s - loss: 3.3197 - accuracy: 0.144 - ETA: 49s - loss: 3.3196 - accuracy: 0.144 - ETA: 48s - loss: 3.3193 - accuracy: 0.144 - ETA: 48s - loss: 3.3198 - accuracy: 0.143 - ETA: 47s - loss: 3.3198 - accuracy: 0.143 - ETA: 47s - loss: 3.3198 - accuracy: 0.143 - ETA: 46s - loss: 3.3200 - accuracy: 0.144 - ETA: 46s - loss: 3.3197 - accuracy: 0.144 - ETA: 46s - loss: 3.3202 - accuracy: 0.143 - ETA: 45s - loss: 3.3198 - accuracy: 0.143 - ETA: 45s - loss: 3.3181 - accuracy: 0.144 - ETA: 44s - loss: 3.3175 - accuracy: 0.144 - ETA: 44s - loss: 3.3168 - accuracy: 0.144 - ETA: 43s - loss: 3.3167 - accuracy: 0.144 - ETA: 43s - loss: 3.3170 - accuracy: 0.144 - ETA: 42s - loss: 3.3155 - accuracy: 0.145 - ETA: 42s - loss: 3.3159 - accuracy: 0.145 - ETA: 41s - loss: 3.3154 - accuracy: 0.145 - ETA: 41s - loss: 3.3160 - accuracy: 0.145 - ETA: 40s - loss: 3.3157 - accuracy: 0.145 - ETA: 40s - loss: 3.3157 - accuracy: 0.145 - ETA: 39s - loss: 3.3149 - accuracy: 0.145 - ETA: 39s - loss: 3.3157 - accuracy: 0.145 - ETA: 39s - loss: 3.3160 - accuracy: 0.145 - ETA: 38s - loss: 3.3160 - accuracy: 0.145 - ETA: 38s - loss: 3.3162 - accuracy: 0.145 - ETA: 37s - loss: 3.3161 - accuracy: 0.145 - ETA: 37s - loss: 3.3163 - accuracy: 0.145 - ETA: 36s - loss: 3.3161 - accuracy: 0.145 - ETA: 36s - loss: 3.3163 - accuracy: 0.145 - ETA: 35s - loss: 3.3160 - accuracy: 0.145 - ETA: 35s - loss: 3.3159 - accuracy: 0.145 - ETA: 34s - loss: 3.3159 - accuracy: 0.145 - ETA: 34s - loss: 3.3159 - accuracy: 0.145 - ETA: 34s - loss: 3.3160 - accuracy: 0.145 - ETA: 33s - loss: 3.3156 - accuracy: 0.145 - ETA: 33s - loss: 3.3156 - accuracy: 0.145 - ETA: 32s - loss: 3.3159 - accuracy: 0.145 - ETA: 32s - loss: 3.3159 - accuracy: 0.145 - ETA: 31s - loss: 3.3161 - accuracy: 0.145 - ETA: 31s - loss: 3.3155 - accuracy: 0.145 - ETA: 30s - loss: 3.3152 - accuracy: 0.145 - ETA: 30s - loss: 3.3152 - accuracy: 0.145 - ETA: 29s - loss: 3.3147 - accuracy: 0.145 - ETA: 29s - loss: 3.3146 - accuracy: 0.145 - ETA: 28s - loss: 3.3148 - accuracy: 0.145 - ETA: 28s - loss: 3.3153 - accuracy: 0.145 - ETA: 28s - loss: 3.3152 - accuracy: 0.145 - ETA: 27s - loss: 3.3147 - accuracy: 0.145 - ETA: 27s - loss: 3.3151 - accuracy: 0.145 - ETA: 26s - loss: 3.3151 - accuracy: 0.144 - ETA: 26s - loss: 3.3148 - accuracy: 0.144 - ETA: 25s - loss: 3.3145 - accuracy: 0.145 - ETA: 25s - loss: 3.3143 - accuracy: 0.145 - ETA: 24s - loss: 3.3141 - accuracy: 0.145 - ETA: 24s - loss: 3.3143 - accuracy: 0.145 - ETA: 23s - loss: 3.3140 - accuracy: 0.145 - ETA: 23s - loss: 3.3139 - accuracy: 0.145 - ETA: 22s - loss: 3.3145 - accuracy: 0.144 - ETA: 22s - loss: 3.3167 - accuracy: 0.144 - ETA: 22s - loss: 3.3166 - accuracy: 0.144 - ETA: 21s - loss: 3.3160 - accuracy: 0.145 - ETA: 21s - loss: 3.3165 - accuracy: 0.144 - ETA: 20s - loss: 3.3167 - accuracy: 0.144 - ETA: 20s - loss: 3.3168 - accuracy: 0.144 - ETA: 19s - loss: 3.3170 - accuracy: 0.144 - ETA: 19s - loss: 3.3163 - accuracy: 0.144 - ETA: 18s - loss: 3.3153 - accuracy: 0.145 - ETA: 18s - loss: 3.3153 - accuracy: 0.145 - ETA: 17s - loss: 3.3151 - accuracy: 0.145 - ETA: 17s - loss: 3.3146 - accuracy: 0.145 - ETA: 16s - loss: 3.3138 - accuracy: 0.145 - ETA: 16s - loss: 3.3134 - accuracy: 0.145 - ETA: 16s - loss: 3.3132 - accuracy: 0.145 - ETA: 15s - loss: 3.3127 - accuracy: 0.145 - ETA: 15s - loss: 3.3127 - accuracy: 0.145 - ETA: 14s - loss: 3.3130 - accuracy: 0.145 - ETA: 14s - loss: 3.3132 - accuracy: 0.145 - ETA: 13s - loss: 3.3132 - accuracy: 0.145 - ETA: 13s - loss: 3.3132 - accuracy: 0.145 - ETA: 12s - loss: 3.3136 - accuracy: 0.145 - ETA: 12s - loss: 3.3135 - accuracy: 0.145 - ETA: 11s - loss: 3.3140 - accuracy: 0.145 - ETA: 11s - loss: 3.3140 - accuracy: 0.145 - ETA: 11s - loss: 3.3140 - accuracy: 0.145 - ETA: 10s - loss: 3.3145 - accuracy: 0.145 - ETA: 10s - loss: 3.3144 - accuracy: 0.145 - ETA: 9s - loss: 3.3144 - accuracy: 0.145 - ETA: 9s - loss: 3.3136 - accuracy: 0.14 - ETA: 8s - loss: 3.3140 - accuracy: 0.14 - ETA: 8s - loss: 3.3146 - accuracy: 0.14 - ETA: 7s - loss: 3.3143 - accuracy: 0.14 - ETA: 7s - loss: 3.3145 - accuracy: 0.14 - ETA: 6s - loss: 3.3146 - accuracy: 0.14 - ETA: 6s - loss: 3.3141 - accuracy: 0.14 - ETA: 5s - loss: 3.3140 - accuracy: 0.14 - ETA: 5s - loss: 3.3134 - accuracy: 0.14 - ETA: 5s - loss: 3.3136 - accuracy: 0.14 - ETA: 4s - loss: 3.3137 - accuracy: 0.14 - ETA: 4s - loss: 3.3139 - accuracy: 0.14 - ETA: 3s - loss: 3.3144 - accuracy: 0.14 - ETA: 3s - loss: 3.3145 - accuracy: 0.14 - ETA: 2s - loss: 3.3146 - accuracy: 0.14 - ETA: 2s - loss: 3.3148 - accuracy: 0.14 - ETA: 1s - loss: 3.3149 - accuracy: 0.14 - ETA: 1s - loss: 3.3149 - accuracy: 0.14 - ETA: 0s - loss: 3.3148 - accuracy: 0.14 - ETA: 0s - loss: 3.3150 - accuracy: 0.14 - ETA: 0s - loss: 3.3155 - accuracy: 0.14 - 162s 4ms/step - loss: 3.3155 - accuracy: 0.1451 - val_loss: 4.1395 - val_accuracy: 0.0200\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:37 - loss: 3.3285 - accuracy: 0.11 - ETA: 2:31 - loss: 3.3742 - accuracy: 0.10 - ETA: 2:28 - loss: 3.3553 - accuracy: 0.12 - ETA: 2:29 - loss: 3.3856 - accuracy: 0.12 - ETA: 2:28 - loss: 3.3687 - accuracy: 0.12 - ETA: 2:27 - loss: 3.3824 - accuracy: 0.12 - ETA: 2:26 - loss: 3.3780 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3612 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3686 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3616 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3646 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3673 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3734 - accuracy: 0.12 - ETA: 2:24 - loss: 3.3705 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3606 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3612 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3705 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3733 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3770 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3708 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3694 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3705 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3706 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3710 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3694 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3662 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3633 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3688 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3680 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3636 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3681 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3704 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3735 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3767 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3754 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3743 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3788 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3787 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3794 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3790 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3739 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3764 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3771 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3796 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3770 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3797 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3778 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3755 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3725 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3720 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3772 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3794 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3769 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3748 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3789 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3794 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3788 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3811 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3842 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3843 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3848 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3866 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3881 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3903 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3903 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3900 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3892 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3897 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3947 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3951 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3953 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3949 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3960 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3987 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3974 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3976 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3972 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3972 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3985 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3978 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3977 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3963 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3974 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3973 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3976 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3976 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3980 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3980 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3979 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3976 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3983 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3996 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3981 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3989 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3996 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3989 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3983 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3994 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3977 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3972 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3961 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3953 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3962 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3940 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3937 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3918 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3913 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3900 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3906 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3903 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3882 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3882 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3875 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3876 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3867 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3864 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3847 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3835 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3837 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3820 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3814 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3804 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3784 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3759 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3748 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3733 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3745 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3744 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3750 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3757 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3766 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3756 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3757 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3762 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3759 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3764 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3764 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3758 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3759 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3755 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3747 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3753 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3755 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3766 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3768 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3752 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3743 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3727 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3723 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3716 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3698 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3699 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3697 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3692 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3697 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3693 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3686 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3694 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3701 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3714 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3712 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3716 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3723 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3721 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3719 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3714 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3716 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3722 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3722 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3719 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3729 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3708 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3701 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3705 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3710 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3707 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3708 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3712 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3717 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3718 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3724 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3716 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3728 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3733 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3731 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3732 - accuracy: 0.1362"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:05 - loss: 3.3732 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3724 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3726 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3730 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3731 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3722 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3720 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3714 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3704 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3697 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3694 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3688 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3691 - accuracy: 0.13 - ETA: 59s - loss: 3.3684 - accuracy: 0.1369 - ETA: 59s - loss: 3.3684 - accuracy: 0.137 - ETA: 58s - loss: 3.3679 - accuracy: 0.137 - ETA: 58s - loss: 3.3672 - accuracy: 0.137 - ETA: 57s - loss: 3.3675 - accuracy: 0.137 - ETA: 57s - loss: 3.3689 - accuracy: 0.136 - ETA: 57s - loss: 3.3692 - accuracy: 0.136 - ETA: 56s - loss: 3.3682 - accuracy: 0.136 - ETA: 56s - loss: 3.3674 - accuracy: 0.137 - ETA: 55s - loss: 3.3680 - accuracy: 0.137 - ETA: 55s - loss: 3.3682 - accuracy: 0.136 - ETA: 54s - loss: 3.3673 - accuracy: 0.137 - ETA: 54s - loss: 3.3674 - accuracy: 0.136 - ETA: 53s - loss: 3.3665 - accuracy: 0.137 - ETA: 53s - loss: 3.3665 - accuracy: 0.137 - ETA: 52s - loss: 3.3660 - accuracy: 0.137 - ETA: 52s - loss: 3.3663 - accuracy: 0.137 - ETA: 51s - loss: 3.3656 - accuracy: 0.137 - ETA: 51s - loss: 3.3694 - accuracy: 0.137 - ETA: 51s - loss: 3.3693 - accuracy: 0.137 - ETA: 50s - loss: 3.3692 - accuracy: 0.137 - ETA: 50s - loss: 3.3687 - accuracy: 0.137 - ETA: 49s - loss: 3.3684 - accuracy: 0.138 - ETA: 49s - loss: 3.3676 - accuracy: 0.138 - ETA: 48s - loss: 3.3670 - accuracy: 0.138 - ETA: 48s - loss: 3.3672 - accuracy: 0.138 - ETA: 47s - loss: 3.3670 - accuracy: 0.138 - ETA: 47s - loss: 3.3671 - accuracy: 0.138 - ETA: 46s - loss: 3.3670 - accuracy: 0.138 - ETA: 46s - loss: 3.3674 - accuracy: 0.138 - ETA: 46s - loss: 3.3662 - accuracy: 0.138 - ETA: 45s - loss: 3.3672 - accuracy: 0.138 - ETA: 45s - loss: 3.3670 - accuracy: 0.138 - ETA: 44s - loss: 3.3673 - accuracy: 0.138 - ETA: 44s - loss: 3.3667 - accuracy: 0.138 - ETA: 43s - loss: 3.3666 - accuracy: 0.138 - ETA: 43s - loss: 3.3658 - accuracy: 0.138 - ETA: 42s - loss: 3.3656 - accuracy: 0.138 - ETA: 42s - loss: 3.3656 - accuracy: 0.138 - ETA: 41s - loss: 3.3657 - accuracy: 0.138 - ETA: 41s - loss: 3.3649 - accuracy: 0.138 - ETA: 41s - loss: 3.3651 - accuracy: 0.138 - ETA: 40s - loss: 3.3657 - accuracy: 0.138 - ETA: 40s - loss: 3.3659 - accuracy: 0.138 - ETA: 39s - loss: 3.3660 - accuracy: 0.138 - ETA: 39s - loss: 3.3653 - accuracy: 0.138 - ETA: 38s - loss: 3.3648 - accuracy: 0.138 - ETA: 38s - loss: 3.3655 - accuracy: 0.138 - ETA: 37s - loss: 3.3651 - accuracy: 0.138 - ETA: 37s - loss: 3.3654 - accuracy: 0.138 - ETA: 36s - loss: 3.3662 - accuracy: 0.138 - ETA: 36s - loss: 3.3659 - accuracy: 0.138 - ETA: 36s - loss: 3.3657 - accuracy: 0.138 - ETA: 35s - loss: 3.3656 - accuracy: 0.138 - ETA: 35s - loss: 3.3652 - accuracy: 0.138 - ETA: 34s - loss: 3.3656 - accuracy: 0.138 - ETA: 34s - loss: 3.3661 - accuracy: 0.138 - ETA: 33s - loss: 3.3659 - accuracy: 0.138 - ETA: 33s - loss: 3.3660 - accuracy: 0.138 - ETA: 32s - loss: 3.3661 - accuracy: 0.138 - ETA: 32s - loss: 3.3664 - accuracy: 0.138 - ETA: 31s - loss: 3.3658 - accuracy: 0.138 - ETA: 31s - loss: 3.3656 - accuracy: 0.138 - ETA: 31s - loss: 3.3644 - accuracy: 0.138 - ETA: 30s - loss: 3.3647 - accuracy: 0.138 - ETA: 30s - loss: 3.3640 - accuracy: 0.138 - ETA: 29s - loss: 3.3638 - accuracy: 0.138 - ETA: 29s - loss: 3.3635 - accuracy: 0.138 - ETA: 28s - loss: 3.3634 - accuracy: 0.138 - ETA: 28s - loss: 3.3637 - accuracy: 0.138 - ETA: 27s - loss: 3.3635 - accuracy: 0.138 - ETA: 27s - loss: 3.3637 - accuracy: 0.138 - ETA: 26s - loss: 3.3631 - accuracy: 0.139 - ETA: 26s - loss: 3.3620 - accuracy: 0.139 - ETA: 25s - loss: 3.3621 - accuracy: 0.139 - ETA: 25s - loss: 3.3623 - accuracy: 0.138 - ETA: 25s - loss: 3.3618 - accuracy: 0.139 - ETA: 24s - loss: 3.3617 - accuracy: 0.139 - ETA: 24s - loss: 3.3615 - accuracy: 0.139 - ETA: 23s - loss: 3.3616 - accuracy: 0.139 - ETA: 23s - loss: 3.3615 - accuracy: 0.139 - ETA: 22s - loss: 3.3609 - accuracy: 0.139 - ETA: 22s - loss: 3.3613 - accuracy: 0.139 - ETA: 21s - loss: 3.3609 - accuracy: 0.139 - ETA: 21s - loss: 3.3603 - accuracy: 0.139 - ETA: 20s - loss: 3.3602 - accuracy: 0.139 - ETA: 20s - loss: 3.3599 - accuracy: 0.139 - ETA: 20s - loss: 3.3597 - accuracy: 0.139 - ETA: 19s - loss: 3.3597 - accuracy: 0.139 - ETA: 19s - loss: 3.3597 - accuracy: 0.139 - ETA: 18s - loss: 3.3594 - accuracy: 0.139 - ETA: 18s - loss: 3.3594 - accuracy: 0.139 - ETA: 17s - loss: 3.3596 - accuracy: 0.139 - ETA: 17s - loss: 3.3599 - accuracy: 0.139 - ETA: 16s - loss: 3.3600 - accuracy: 0.139 - ETA: 16s - loss: 3.3596 - accuracy: 0.139 - ETA: 15s - loss: 3.3595 - accuracy: 0.139 - ETA: 15s - loss: 3.3594 - accuracy: 0.139 - ETA: 15s - loss: 3.3595 - accuracy: 0.139 - ETA: 14s - loss: 3.3594 - accuracy: 0.139 - ETA: 14s - loss: 3.3600 - accuracy: 0.139 - ETA: 13s - loss: 3.3599 - accuracy: 0.139 - ETA: 13s - loss: 3.3601 - accuracy: 0.139 - ETA: 12s - loss: 3.3597 - accuracy: 0.139 - ETA: 12s - loss: 3.3604 - accuracy: 0.139 - ETA: 11s - loss: 3.3603 - accuracy: 0.139 - ETA: 11s - loss: 3.3603 - accuracy: 0.139 - ETA: 10s - loss: 3.3598 - accuracy: 0.139 - ETA: 10s - loss: 3.3597 - accuracy: 0.139 - ETA: 10s - loss: 3.3594 - accuracy: 0.139 - ETA: 9s - loss: 3.3593 - accuracy: 0.139 - ETA: 9s - loss: 3.3588 - accuracy: 0.13 - ETA: 8s - loss: 3.3589 - accuracy: 0.13 - ETA: 8s - loss: 3.3590 - accuracy: 0.13 - ETA: 7s - loss: 3.3586 - accuracy: 0.13 - ETA: 7s - loss: 3.3583 - accuracy: 0.13 - ETA: 6s - loss: 3.3581 - accuracy: 0.13 - ETA: 6s - loss: 3.3582 - accuracy: 0.13 - ETA: 5s - loss: 3.3581 - accuracy: 0.13 - ETA: 5s - loss: 3.3583 - accuracy: 0.13 - ETA: 5s - loss: 3.3582 - accuracy: 0.13 - ETA: 4s - loss: 3.3583 - accuracy: 0.13 - ETA: 4s - loss: 3.3586 - accuracy: 0.13 - ETA: 3s - loss: 3.3579 - accuracy: 0.13 - ETA: 3s - loss: 3.3581 - accuracy: 0.13 - ETA: 2s - loss: 3.3581 - accuracy: 0.13 - ETA: 2s - loss: 3.3578 - accuracy: 0.13 - ETA: 1s - loss: 3.3576 - accuracy: 0.13 - ETA: 1s - loss: 3.3574 - accuracy: 0.13 - ETA: 0s - loss: 3.3576 - accuracy: 0.13 - ETA: 0s - loss: 3.3577 - accuracy: 0.13 - ETA: 0s - loss: 3.3577 - accuracy: 0.13 - 161s 4ms/step - loss: 3.3577 - accuracy: 0.1392 - val_loss: 4.3005 - val_accuracy: 0.0197\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:47 - loss: 3.3796 - accuracy: 0.14 - ETA: 2:39 - loss: 3.4430 - accuracy: 0.11 - ETA: 2:34 - loss: 3.3635 - accuracy: 0.11 - ETA: 2:31 - loss: 3.3156 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2794 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2959 - accuracy: 0.15 - ETA: 2:27 - loss: 3.3007 - accuracy: 0.15 - ETA: 2:26 - loss: 3.3031 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2819 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2866 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2872 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2819 - accuracy: 0.16 - ETA: 2:28 - loss: 3.2870 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2922 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2985 - accuracy: 0.15 - ETA: 2:28 - loss: 3.3021 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2987 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2883 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2900 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2950 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2942 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2952 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2971 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2996 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2952 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2965 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2965 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3003 - accuracy: 0.14 - ETA: 2:19 - loss: 3.3052 - accuracy: 0.14 - ETA: 2:18 - loss: 3.3052 - accuracy: 0.14 - ETA: 2:18 - loss: 3.3114 - accuracy: 0.14 - ETA: 2:17 - loss: 3.3089 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3063 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3072 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3123 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3100 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3101 - accuracy: 0.14 - ETA: 2:14 - loss: 3.3089 - accuracy: 0.14 - ETA: 2:14 - loss: 3.3098 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3020 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2993 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2987 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2985 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2944 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2935 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2892 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2910 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2892 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2906 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2933 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2952 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2939 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2998 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3012 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3040 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3069 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3046 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3026 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3043 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3040 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3116 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3133 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3170 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3132 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3107 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3140 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3161 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3165 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3164 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3150 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3170 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3181 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3187 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3192 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3218 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3218 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3206 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3224 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3259 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3256 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3233 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3234 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3224 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3235 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3236 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3239 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3251 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3255 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3238 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3242 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3261 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3226 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3230 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3239 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3243 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3239 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3254 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3263 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3256 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3240 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3230 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3218 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3201 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3210 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3209 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3208 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3242 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3252 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3256 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3267 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3267 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3262 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3258 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3241 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3230 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3226 - accuracy: 0.14 - ETA: 1:37 - loss: 3.3223 - accuracy: 0.14 - ETA: 1:37 - loss: 3.3224 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3238 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3243 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3236 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3249 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3232 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3234 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3237 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3247 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3231 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3252 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3251 - accuracy: 0.14 - ETA: 1:31 - loss: 3.3251 - accuracy: 0.14 - ETA: 1:31 - loss: 3.3257 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3265 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3273 - accuracy: 0.14 - ETA: 1:29 - loss: 3.3273 - accuracy: 0.14 - ETA: 1:29 - loss: 3.3267 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3269 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3266 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3275 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3282 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3291 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3289 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3281 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3278 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3281 - accuracy: 0.14 - ETA: 1:24 - loss: 3.3291 - accuracy: 0.14 - ETA: 1:24 - loss: 3.3295 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3284 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3290 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3300 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3305 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3310 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3317 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3314 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3310 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3310 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3314 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3304 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3303 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3312 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3310 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3303 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3293 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3283 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3286 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3290 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3294 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3297 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3302 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3303 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3291 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3285 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3277 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3269 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3268 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3269 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3268 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3272 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3286 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3280 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3278 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3280 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3277 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3288 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3280 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3281 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3289 - accuracy: 0.1438"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:05 - loss: 3.3293 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3285 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3279 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3285 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3283 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3278 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3277 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3283 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3283 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3286 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3283 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3282 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3285 - accuracy: 0.14 - ETA: 59s - loss: 3.3274 - accuracy: 0.1448 - ETA: 59s - loss: 3.3271 - accuracy: 0.144 - ETA: 58s - loss: 3.3270 - accuracy: 0.144 - ETA: 58s - loss: 3.3267 - accuracy: 0.145 - ETA: 57s - loss: 3.3266 - accuracy: 0.145 - ETA: 57s - loss: 3.3263 - accuracy: 0.145 - ETA: 56s - loss: 3.3263 - accuracy: 0.145 - ETA: 56s - loss: 3.3262 - accuracy: 0.145 - ETA: 55s - loss: 3.3263 - accuracy: 0.145 - ETA: 55s - loss: 3.3263 - accuracy: 0.145 - ETA: 54s - loss: 3.3263 - accuracy: 0.145 - ETA: 54s - loss: 3.3256 - accuracy: 0.145 - ETA: 54s - loss: 3.3267 - accuracy: 0.145 - ETA: 53s - loss: 3.3268 - accuracy: 0.145 - ETA: 53s - loss: 3.3274 - accuracy: 0.145 - ETA: 52s - loss: 3.3283 - accuracy: 0.145 - ETA: 52s - loss: 3.3287 - accuracy: 0.145 - ETA: 51s - loss: 3.3298 - accuracy: 0.144 - ETA: 51s - loss: 3.3296 - accuracy: 0.145 - ETA: 50s - loss: 3.3301 - accuracy: 0.144 - ETA: 50s - loss: 3.3306 - accuracy: 0.144 - ETA: 50s - loss: 3.3303 - accuracy: 0.144 - ETA: 49s - loss: 3.3305 - accuracy: 0.144 - ETA: 49s - loss: 3.3305 - accuracy: 0.144 - ETA: 48s - loss: 3.3303 - accuracy: 0.144 - ETA: 48s - loss: 3.3305 - accuracy: 0.144 - ETA: 47s - loss: 3.3307 - accuracy: 0.144 - ETA: 47s - loss: 3.3293 - accuracy: 0.145 - ETA: 46s - loss: 3.3293 - accuracy: 0.145 - ETA: 46s - loss: 3.3296 - accuracy: 0.145 - ETA: 45s - loss: 3.3301 - accuracy: 0.144 - ETA: 45s - loss: 3.3310 - accuracy: 0.144 - ETA: 45s - loss: 3.3306 - accuracy: 0.144 - ETA: 44s - loss: 3.3304 - accuracy: 0.144 - ETA: 44s - loss: 3.3301 - accuracy: 0.144 - ETA: 43s - loss: 3.3303 - accuracy: 0.144 - ETA: 43s - loss: 3.3309 - accuracy: 0.144 - ETA: 42s - loss: 3.3303 - accuracy: 0.144 - ETA: 42s - loss: 3.3298 - accuracy: 0.144 - ETA: 41s - loss: 3.3302 - accuracy: 0.144 - ETA: 41s - loss: 3.3295 - accuracy: 0.144 - ETA: 40s - loss: 3.3294 - accuracy: 0.144 - ETA: 40s - loss: 3.3288 - accuracy: 0.144 - ETA: 40s - loss: 3.3292 - accuracy: 0.144 - ETA: 39s - loss: 3.3296 - accuracy: 0.144 - ETA: 39s - loss: 3.3294 - accuracy: 0.144 - ETA: 38s - loss: 3.3282 - accuracy: 0.145 - ETA: 38s - loss: 3.3285 - accuracy: 0.145 - ETA: 37s - loss: 3.3280 - accuracy: 0.145 - ETA: 37s - loss: 3.3280 - accuracy: 0.145 - ETA: 36s - loss: 3.3276 - accuracy: 0.145 - ETA: 36s - loss: 3.3277 - accuracy: 0.145 - ETA: 35s - loss: 3.3270 - accuracy: 0.145 - ETA: 35s - loss: 3.3273 - accuracy: 0.144 - ETA: 35s - loss: 3.3261 - accuracy: 0.145 - ETA: 34s - loss: 3.3257 - accuracy: 0.145 - ETA: 34s - loss: 3.3245 - accuracy: 0.145 - ETA: 33s - loss: 3.3242 - accuracy: 0.145 - ETA: 33s - loss: 3.3251 - accuracy: 0.145 - ETA: 32s - loss: 3.3250 - accuracy: 0.145 - ETA: 32s - loss: 3.3250 - accuracy: 0.145 - ETA: 31s - loss: 3.3252 - accuracy: 0.145 - ETA: 31s - loss: 3.3250 - accuracy: 0.145 - ETA: 30s - loss: 3.3246 - accuracy: 0.145 - ETA: 30s - loss: 3.3244 - accuracy: 0.145 - ETA: 30s - loss: 3.3240 - accuracy: 0.145 - ETA: 29s - loss: 3.3240 - accuracy: 0.145 - ETA: 29s - loss: 3.3240 - accuracy: 0.145 - ETA: 28s - loss: 3.3239 - accuracy: 0.145 - ETA: 28s - loss: 3.3244 - accuracy: 0.145 - ETA: 27s - loss: 3.3244 - accuracy: 0.145 - ETA: 27s - loss: 3.3248 - accuracy: 0.145 - ETA: 26s - loss: 3.3250 - accuracy: 0.145 - ETA: 26s - loss: 3.3252 - accuracy: 0.144 - ETA: 25s - loss: 3.3261 - accuracy: 0.144 - ETA: 25s - loss: 3.3254 - accuracy: 0.144 - ETA: 25s - loss: 3.3257 - accuracy: 0.144 - ETA: 24s - loss: 3.3253 - accuracy: 0.144 - ETA: 24s - loss: 3.3252 - accuracy: 0.144 - ETA: 23s - loss: 3.3256 - accuracy: 0.144 - ETA: 23s - loss: 3.3255 - accuracy: 0.144 - ETA: 22s - loss: 3.3247 - accuracy: 0.144 - ETA: 22s - loss: 3.3240 - accuracy: 0.144 - ETA: 21s - loss: 3.3236 - accuracy: 0.144 - ETA: 21s - loss: 3.3238 - accuracy: 0.144 - ETA: 20s - loss: 3.3240 - accuracy: 0.144 - ETA: 20s - loss: 3.3242 - accuracy: 0.144 - ETA: 20s - loss: 3.3245 - accuracy: 0.144 - ETA: 19s - loss: 3.3242 - accuracy: 0.144 - ETA: 19s - loss: 3.3240 - accuracy: 0.144 - ETA: 18s - loss: 3.3236 - accuracy: 0.144 - ETA: 18s - loss: 3.3244 - accuracy: 0.144 - ETA: 17s - loss: 3.3248 - accuracy: 0.144 - ETA: 17s - loss: 3.3250 - accuracy: 0.144 - ETA: 16s - loss: 3.3245 - accuracy: 0.144 - ETA: 16s - loss: 3.3241 - accuracy: 0.144 - ETA: 15s - loss: 3.3239 - accuracy: 0.144 - ETA: 15s - loss: 3.3239 - accuracy: 0.144 - ETA: 15s - loss: 3.3242 - accuracy: 0.144 - ETA: 14s - loss: 3.3236 - accuracy: 0.144 - ETA: 14s - loss: 3.3228 - accuracy: 0.144 - ETA: 13s - loss: 3.3230 - accuracy: 0.144 - ETA: 13s - loss: 3.3230 - accuracy: 0.144 - ETA: 12s - loss: 3.3235 - accuracy: 0.144 - ETA: 12s - loss: 3.3237 - accuracy: 0.144 - ETA: 11s - loss: 3.3242 - accuracy: 0.144 - ETA: 11s - loss: 3.3246 - accuracy: 0.144 - ETA: 10s - loss: 3.3246 - accuracy: 0.144 - ETA: 10s - loss: 3.3254 - accuracy: 0.143 - ETA: 10s - loss: 3.3257 - accuracy: 0.143 - ETA: 9s - loss: 3.3256 - accuracy: 0.143 - ETA: 9s - loss: 3.3261 - accuracy: 0.14 - ETA: 8s - loss: 3.3257 - accuracy: 0.14 - ETA: 8s - loss: 3.3257 - accuracy: 0.14 - ETA: 7s - loss: 3.3259 - accuracy: 0.14 - ETA: 7s - loss: 3.3257 - accuracy: 0.14 - ETA: 6s - loss: 3.3253 - accuracy: 0.14 - ETA: 6s - loss: 3.3252 - accuracy: 0.14 - ETA: 5s - loss: 3.3254 - accuracy: 0.14 - ETA: 5s - loss: 3.3257 - accuracy: 0.14 - ETA: 5s - loss: 3.3266 - accuracy: 0.14 - ETA: 4s - loss: 3.3269 - accuracy: 0.14 - ETA: 4s - loss: 3.3274 - accuracy: 0.14 - ETA: 3s - loss: 3.3276 - accuracy: 0.14 - ETA: 3s - loss: 3.3278 - accuracy: 0.14 - ETA: 2s - loss: 3.3278 - accuracy: 0.14 - ETA: 2s - loss: 3.3279 - accuracy: 0.14 - ETA: 1s - loss: 3.3281 - accuracy: 0.14 - ETA: 1s - loss: 3.3281 - accuracy: 0.14 - ETA: 0s - loss: 3.3275 - accuracy: 0.14 - ETA: 0s - loss: 3.3280 - accuracy: 0.14 - ETA: 0s - loss: 3.3280 - accuracy: 0.14 - 161s 4ms/step - loss: 3.3280 - accuracy: 0.1442 - val_loss: 4.4060 - val_accuracy: 0.0207\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:39 - loss: 3.2516 - accuracy: 0.17 - ETA: 2:31 - loss: 3.2742 - accuracy: 0.16 - ETA: 2:29 - loss: 3.2960 - accuracy: 0.15 - ETA: 2:33 - loss: 3.3184 - accuracy: 0.15 - ETA: 2:32 - loss: 3.3114 - accuracy: 0.14 - ETA: 2:31 - loss: 3.3749 - accuracy: 0.14 - ETA: 2:30 - loss: 3.3630 - accuracy: 0.15 - ETA: 2:32 - loss: 3.3579 - accuracy: 0.14 - ETA: 2:33 - loss: 3.3606 - accuracy: 0.14 - ETA: 2:33 - loss: 3.3606 - accuracy: 0.14 - ETA: 2:33 - loss: 3.3562 - accuracy: 0.14 - ETA: 2:32 - loss: 3.3711 - accuracy: 0.14 - ETA: 2:31 - loss: 3.3716 - accuracy: 0.14 - ETA: 2:30 - loss: 3.3892 - accuracy: 0.14 - ETA: 2:29 - loss: 3.3938 - accuracy: 0.14 - ETA: 2:28 - loss: 3.4037 - accuracy: 0.13 - ETA: 2:27 - loss: 3.4003 - accuracy: 0.13 - ETA: 2:26 - loss: 3.4036 - accuracy: 0.13 - ETA: 2:25 - loss: 3.4028 - accuracy: 0.13 - ETA: 2:24 - loss: 3.4068 - accuracy: 0.13 - ETA: 2:24 - loss: 3.4094 - accuracy: 0.13 - ETA: 2:23 - loss: 3.4069 - accuracy: 0.13 - ETA: 2:22 - loss: 3.4018 - accuracy: 0.13 - ETA: 2:22 - loss: 3.4012 - accuracy: 0.13 - ETA: 2:21 - loss: 3.4043 - accuracy: 0.13 - ETA: 2:20 - loss: 3.4041 - accuracy: 0.13 - ETA: 2:20 - loss: 3.4032 - accuracy: 0.13 - ETA: 2:19 - loss: 3.4040 - accuracy: 0.13 - ETA: 2:18 - loss: 3.4126 - accuracy: 0.13 - ETA: 2:18 - loss: 3.4152 - accuracy: 0.13 - ETA: 2:17 - loss: 3.4167 - accuracy: 0.13 - ETA: 2:17 - loss: 3.4122 - accuracy: 0.13 - ETA: 2:16 - loss: 3.4183 - accuracy: 0.13 - ETA: 2:16 - loss: 3.4192 - accuracy: 0.13 - ETA: 2:15 - loss: 3.4231 - accuracy: 0.13 - ETA: 2:15 - loss: 3.4217 - accuracy: 0.13 - ETA: 2:14 - loss: 3.4211 - accuracy: 0.13 - ETA: 2:13 - loss: 3.4183 - accuracy: 0.13 - ETA: 2:13 - loss: 3.4156 - accuracy: 0.13 - ETA: 2:12 - loss: 3.4095 - accuracy: 0.13 - ETA: 2:12 - loss: 3.4101 - accuracy: 0.13 - ETA: 2:11 - loss: 3.4076 - accuracy: 0.13 - ETA: 2:11 - loss: 3.4073 - accuracy: 0.13 - ETA: 2:11 - loss: 3.4042 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3982 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3990 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3979 - accuracy: 0.14 - ETA: 2:09 - loss: 3.3966 - accuracy: 0.14 - ETA: 2:09 - loss: 3.3953 - accuracy: 0.14 - ETA: 2:08 - loss: 3.3962 - accuracy: 0.14 - ETA: 2:07 - loss: 3.3974 - accuracy: 0.14 - ETA: 2:07 - loss: 3.3999 - accuracy: 0.14 - ETA: 2:06 - loss: 3.4024 - accuracy: 0.14 - ETA: 2:06 - loss: 3.4022 - accuracy: 0.14 - ETA: 2:06 - loss: 3.4046 - accuracy: 0.14 - ETA: 2:05 - loss: 3.4018 - accuracy: 0.14 - ETA: 2:05 - loss: 3.4000 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3987 - accuracy: 0.14 - ETA: 2:04 - loss: 3.4000 - accuracy: 0.14 - ETA: 2:03 - loss: 3.4007 - accuracy: 0.13 - ETA: 2:03 - loss: 3.4010 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3979 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3995 - accuracy: 0.13 - ETA: 2:01 - loss: 3.4022 - accuracy: 0.13 - ETA: 2:01 - loss: 3.4027 - accuracy: 0.13 - ETA: 2:00 - loss: 3.4020 - accuracy: 0.13 - ETA: 2:00 - loss: 3.4022 - accuracy: 0.13 - ETA: 1:59 - loss: 3.4012 - accuracy: 0.13 - ETA: 1:59 - loss: 3.4012 - accuracy: 0.13 - ETA: 1:58 - loss: 3.4003 - accuracy: 0.13 - ETA: 1:58 - loss: 3.4020 - accuracy: 0.13 - ETA: 1:57 - loss: 3.4016 - accuracy: 0.13 - ETA: 1:57 - loss: 3.4025 - accuracy: 0.13 - ETA: 1:56 - loss: 3.4011 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3991 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3987 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3988 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3972 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3949 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3958 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3920 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3917 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3902 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3912 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3908 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3896 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3906 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3880 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3871 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3845 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3846 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3845 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3832 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3833 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3845 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3847 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3844 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3851 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3864 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3868 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3872 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3872 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3892 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3876 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3875 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3875 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3859 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3866 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3874 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3878 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3876 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3887 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3887 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3879 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3873 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3887 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3906 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3909 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3895 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3882 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3892 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3887 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3895 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3903 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3896 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3905 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3906 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3899 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3890 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3886 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3879 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3869 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3872 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3868 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3855 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3849 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3848 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3850 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3835 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3834 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3830 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3827 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3812 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3807 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3811 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3806 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3795 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3801 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3798 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3801 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3809 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3811 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3807 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3808 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3812 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3810 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3811 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3813 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3810 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3813 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3816 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3817 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3806 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3831 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3831 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3839 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3827 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3831 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3824 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3821 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3829 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3824 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3842 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3844 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3840 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3840 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3835 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3836 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3828 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3835 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3826 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3810 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3805 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3793 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3784 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3779 - accuracy: 0.1409"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:05 - loss: 3.3782 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3785 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3772 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3759 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3750 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3751 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3749 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3739 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3753 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3770 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3765 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3757 - accuracy: 0.14 - ETA: 59s - loss: 3.3749 - accuracy: 0.1419 - ETA: 59s - loss: 3.3750 - accuracy: 0.141 - ETA: 58s - loss: 3.3749 - accuracy: 0.141 - ETA: 58s - loss: 3.3750 - accuracy: 0.141 - ETA: 58s - loss: 3.3743 - accuracy: 0.141 - ETA: 57s - loss: 3.3750 - accuracy: 0.141 - ETA: 57s - loss: 3.3750 - accuracy: 0.141 - ETA: 56s - loss: 3.3750 - accuracy: 0.141 - ETA: 56s - loss: 3.3742 - accuracy: 0.141 - ETA: 55s - loss: 3.3742 - accuracy: 0.141 - ETA: 55s - loss: 3.3740 - accuracy: 0.141 - ETA: 54s - loss: 3.3734 - accuracy: 0.141 - ETA: 54s - loss: 3.3727 - accuracy: 0.141 - ETA: 53s - loss: 3.3727 - accuracy: 0.141 - ETA: 53s - loss: 3.3716 - accuracy: 0.142 - ETA: 52s - loss: 3.3725 - accuracy: 0.142 - ETA: 52s - loss: 3.3720 - accuracy: 0.142 - ETA: 52s - loss: 3.3724 - accuracy: 0.142 - ETA: 51s - loss: 3.3717 - accuracy: 0.142 - ETA: 51s - loss: 3.3709 - accuracy: 0.142 - ETA: 50s - loss: 3.3706 - accuracy: 0.142 - ETA: 50s - loss: 3.3705 - accuracy: 0.142 - ETA: 49s - loss: 3.3695 - accuracy: 0.142 - ETA: 49s - loss: 3.3689 - accuracy: 0.142 - ETA: 48s - loss: 3.3692 - accuracy: 0.142 - ETA: 48s - loss: 3.3689 - accuracy: 0.142 - ETA: 48s - loss: 3.3690 - accuracy: 0.142 - ETA: 47s - loss: 3.3688 - accuracy: 0.142 - ETA: 47s - loss: 3.3696 - accuracy: 0.142 - ETA: 46s - loss: 3.3697 - accuracy: 0.141 - ETA: 46s - loss: 3.3695 - accuracy: 0.141 - ETA: 45s - loss: 3.3693 - accuracy: 0.141 - ETA: 45s - loss: 3.3694 - accuracy: 0.141 - ETA: 44s - loss: 3.3695 - accuracy: 0.141 - ETA: 44s - loss: 3.3696 - accuracy: 0.141 - ETA: 43s - loss: 3.3699 - accuracy: 0.141 - ETA: 43s - loss: 3.3692 - accuracy: 0.141 - ETA: 43s - loss: 3.3696 - accuracy: 0.141 - ETA: 42s - loss: 3.3704 - accuracy: 0.141 - ETA: 42s - loss: 3.3697 - accuracy: 0.141 - ETA: 41s - loss: 3.3706 - accuracy: 0.140 - ETA: 41s - loss: 3.3703 - accuracy: 0.141 - ETA: 40s - loss: 3.3706 - accuracy: 0.140 - ETA: 40s - loss: 3.3700 - accuracy: 0.140 - ETA: 39s - loss: 3.3697 - accuracy: 0.140 - ETA: 39s - loss: 3.3704 - accuracy: 0.140 - ETA: 38s - loss: 3.3702 - accuracy: 0.140 - ETA: 38s - loss: 3.3698 - accuracy: 0.140 - ETA: 38s - loss: 3.3691 - accuracy: 0.141 - ETA: 37s - loss: 3.3684 - accuracy: 0.141 - ETA: 37s - loss: 3.3683 - accuracy: 0.141 - ETA: 36s - loss: 3.3677 - accuracy: 0.141 - ETA: 36s - loss: 3.3680 - accuracy: 0.141 - ETA: 35s - loss: 3.3683 - accuracy: 0.141 - ETA: 35s - loss: 3.3681 - accuracy: 0.141 - ETA: 34s - loss: 3.3679 - accuracy: 0.141 - ETA: 34s - loss: 3.3679 - accuracy: 0.141 - ETA: 34s - loss: 3.3671 - accuracy: 0.141 - ETA: 33s - loss: 3.3658 - accuracy: 0.142 - ETA: 33s - loss: 3.3656 - accuracy: 0.142 - ETA: 32s - loss: 3.3650 - accuracy: 0.142 - ETA: 32s - loss: 3.3646 - accuracy: 0.142 - ETA: 31s - loss: 3.3644 - accuracy: 0.142 - ETA: 31s - loss: 3.3645 - accuracy: 0.142 - ETA: 30s - loss: 3.3644 - accuracy: 0.142 - ETA: 30s - loss: 3.3641 - accuracy: 0.142 - ETA: 29s - loss: 3.3639 - accuracy: 0.142 - ETA: 29s - loss: 3.3632 - accuracy: 0.142 - ETA: 29s - loss: 3.3626 - accuracy: 0.142 - ETA: 28s - loss: 3.3624 - accuracy: 0.142 - ETA: 28s - loss: 3.3614 - accuracy: 0.142 - ETA: 27s - loss: 3.3616 - accuracy: 0.142 - ETA: 27s - loss: 3.3613 - accuracy: 0.142 - ETA: 26s - loss: 3.3608 - accuracy: 0.142 - ETA: 26s - loss: 3.3600 - accuracy: 0.142 - ETA: 25s - loss: 3.3601 - accuracy: 0.142 - ETA: 25s - loss: 3.3598 - accuracy: 0.142 - ETA: 24s - loss: 3.3601 - accuracy: 0.142 - ETA: 24s - loss: 3.3598 - accuracy: 0.142 - ETA: 24s - loss: 3.3589 - accuracy: 0.142 - ETA: 23s - loss: 3.3591 - accuracy: 0.143 - ETA: 23s - loss: 3.3586 - accuracy: 0.143 - ETA: 22s - loss: 3.3587 - accuracy: 0.143 - ETA: 22s - loss: 3.3586 - accuracy: 0.142 - ETA: 21s - loss: 3.3583 - accuracy: 0.143 - ETA: 21s - loss: 3.3587 - accuracy: 0.142 - ETA: 20s - loss: 3.3586 - accuracy: 0.142 - ETA: 20s - loss: 3.3580 - accuracy: 0.143 - ETA: 19s - loss: 3.3578 - accuracy: 0.143 - ETA: 19s - loss: 3.3578 - accuracy: 0.143 - ETA: 19s - loss: 3.3577 - accuracy: 0.142 - ETA: 18s - loss: 3.3569 - accuracy: 0.143 - ETA: 18s - loss: 3.3569 - accuracy: 0.143 - ETA: 17s - loss: 3.3563 - accuracy: 0.143 - ETA: 17s - loss: 3.3566 - accuracy: 0.143 - ETA: 16s - loss: 3.3565 - accuracy: 0.143 - ETA: 16s - loss: 3.3565 - accuracy: 0.143 - ETA: 15s - loss: 3.3559 - accuracy: 0.143 - ETA: 15s - loss: 3.3559 - accuracy: 0.143 - ETA: 14s - loss: 3.3557 - accuracy: 0.143 - ETA: 14s - loss: 3.3557 - accuracy: 0.143 - ETA: 14s - loss: 3.3547 - accuracy: 0.143 - ETA: 13s - loss: 3.3545 - accuracy: 0.143 - ETA: 13s - loss: 3.3540 - accuracy: 0.143 - ETA: 12s - loss: 3.3538 - accuracy: 0.143 - ETA: 12s - loss: 3.3541 - accuracy: 0.143 - ETA: 11s - loss: 3.3538 - accuracy: 0.143 - ETA: 11s - loss: 3.3540 - accuracy: 0.143 - ETA: 10s - loss: 3.3543 - accuracy: 0.143 - ETA: 10s - loss: 3.3539 - accuracy: 0.143 - ETA: 10s - loss: 3.3540 - accuracy: 0.143 - ETA: 9s - loss: 3.3535 - accuracy: 0.143 - ETA: 9s - loss: 3.3537 - accuracy: 0.14 - ETA: 8s - loss: 3.3536 - accuracy: 0.14 - ETA: 8s - loss: 3.3536 - accuracy: 0.14 - ETA: 7s - loss: 3.3538 - accuracy: 0.14 - ETA: 7s - loss: 3.3541 - accuracy: 0.14 - ETA: 6s - loss: 3.3544 - accuracy: 0.14 - ETA: 6s - loss: 3.3542 - accuracy: 0.14 - ETA: 5s - loss: 3.3542 - accuracy: 0.14 - ETA: 5s - loss: 3.3537 - accuracy: 0.14 - ETA: 5s - loss: 3.3534 - accuracy: 0.14 - ETA: 4s - loss: 3.3536 - accuracy: 0.14 - ETA: 4s - loss: 3.3536 - accuracy: 0.14 - ETA: 3s - loss: 3.3542 - accuracy: 0.14 - ETA: 3s - loss: 3.3538 - accuracy: 0.14 - ETA: 2s - loss: 3.3545 - accuracy: 0.14 - ETA: 2s - loss: 3.3545 - accuracy: 0.14 - ETA: 1s - loss: 3.3546 - accuracy: 0.14 - ETA: 1s - loss: 3.3548 - accuracy: 0.14 - ETA: 0s - loss: 3.3544 - accuracy: 0.14 - ETA: 0s - loss: 3.3543 - accuracy: 0.14 - ETA: 0s - loss: 3.3548 - accuracy: 0.14 - 161s 4ms/step - loss: 3.3549 - accuracy: 0.1437 - val_loss: 4.0920 - val_accuracy: 0.0399\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:52 - loss: 3.5672 - accuracy: 0.07 - ETA: 2:39 - loss: 3.4381 - accuracy: 0.09 - ETA: 2:34 - loss: 3.4522 - accuracy: 0.10 - ETA: 2:31 - loss: 3.4089 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4015 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4225 - accuracy: 0.12 - ETA: 2:28 - loss: 3.4226 - accuracy: 0.11 - ETA: 2:30 - loss: 3.4380 - accuracy: 0.12 - ETA: 2:31 - loss: 3.4622 - accuracy: 0.11 - ETA: 2:30 - loss: 3.4778 - accuracy: 0.11 - ETA: 2:29 - loss: 3.4865 - accuracy: 0.11 - ETA: 2:28 - loss: 3.4939 - accuracy: 0.11 - ETA: 2:27 - loss: 3.5014 - accuracy: 0.11 - ETA: 2:26 - loss: 3.4991 - accuracy: 0.11 - ETA: 2:25 - loss: 3.5013 - accuracy: 0.11 - ETA: 2:24 - loss: 3.4991 - accuracy: 0.11 - ETA: 2:23 - loss: 3.4933 - accuracy: 0.12 - ETA: 2:23 - loss: 3.4928 - accuracy: 0.11 - ETA: 2:22 - loss: 3.4845 - accuracy: 0.12 - ETA: 2:22 - loss: 3.4762 - accuracy: 0.12 - ETA: 2:21 - loss: 3.4719 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4774 - accuracy: 0.12 - ETA: 2:20 - loss: 3.4790 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4793 - accuracy: 0.12 - ETA: 2:19 - loss: 3.4695 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4670 - accuracy: 0.12 - ETA: 2:18 - loss: 3.4698 - accuracy: 0.12 - ETA: 2:17 - loss: 3.4724 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4799 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4765 - accuracy: 0.12 - ETA: 2:16 - loss: 3.4761 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4739 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4765 - accuracy: 0.12 - ETA: 2:15 - loss: 3.4720 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4714 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4703 - accuracy: 0.12 - ETA: 2:14 - loss: 3.4645 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4608 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4600 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4608 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4627 - accuracy: 0.12 - ETA: 2:13 - loss: 3.4585 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4591 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4533 - accuracy: 0.12 - ETA: 2:12 - loss: 3.4494 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4469 - accuracy: 0.12 - ETA: 2:11 - loss: 3.4459 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4425 - accuracy: 0.12 - ETA: 2:10 - loss: 3.4395 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4390 - accuracy: 0.12 - ETA: 2:09 - loss: 3.4435 - accuracy: 0.12 - ETA: 2:08 - loss: 3.4422 - accuracy: 0.12 - ETA: 2:08 - loss: 3.4475 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4436 - accuracy: 0.12 - ETA: 2:07 - loss: 3.4429 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4440 - accuracy: 0.12 - ETA: 2:06 - loss: 3.4437 - accuracy: 0.12 - ETA: 2:05 - loss: 3.4436 - accuracy: 0.12 - ETA: 2:05 - loss: 3.4459 - accuracy: 0.12 - ETA: 2:04 - loss: 3.4443 - accuracy: 0.12 - ETA: 2:04 - loss: 3.4450 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4421 - accuracy: 0.12 - ETA: 2:03 - loss: 3.4418 - accuracy: 0.12 - ETA: 2:02 - loss: 3.4423 - accuracy: 0.12 - ETA: 2:02 - loss: 3.4420 - accuracy: 0.12 - ETA: 2:01 - loss: 3.4406 - accuracy: 0.12 - ETA: 2:00 - loss: 3.4391 - accuracy: 0.12 - ETA: 2:00 - loss: 3.4373 - accuracy: 0.12 - ETA: 1:59 - loss: 3.4366 - accuracy: 0.12 - ETA: 1:59 - loss: 3.4362 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4332 - accuracy: 0.12 - ETA: 1:58 - loss: 3.4337 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4333 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4313 - accuracy: 0.12 - ETA: 1:57 - loss: 3.4321 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4308 - accuracy: 0.12 - ETA: 1:56 - loss: 3.4297 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4275 - accuracy: 0.12 - ETA: 1:55 - loss: 3.4266 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4249 - accuracy: 0.12 - ETA: 1:54 - loss: 3.4226 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4211 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4195 - accuracy: 0.12 - ETA: 1:53 - loss: 3.4178 - accuracy: 0.13 - ETA: 1:52 - loss: 3.4182 - accuracy: 0.13 - ETA: 1:52 - loss: 3.4175 - accuracy: 0.12 - ETA: 1:51 - loss: 3.4166 - accuracy: 0.12 - ETA: 1:51 - loss: 3.4159 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4158 - accuracy: 0.12 - ETA: 1:50 - loss: 3.4143 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4141 - accuracy: 0.12 - ETA: 1:49 - loss: 3.4142 - accuracy: 0.12 - ETA: 1:48 - loss: 3.4135 - accuracy: 0.12 - ETA: 1:48 - loss: 3.4144 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4143 - accuracy: 0.12 - ETA: 1:47 - loss: 3.4120 - accuracy: 0.13 - ETA: 1:46 - loss: 3.4120 - accuracy: 0.13 - ETA: 1:46 - loss: 3.4112 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4100 - accuracy: 0.12 - ETA: 1:45 - loss: 3.4105 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4114 - accuracy: 0.12 - ETA: 1:44 - loss: 3.4093 - accuracy: 0.12 - ETA: 1:43 - loss: 3.4089 - accuracy: 0.13 - ETA: 1:43 - loss: 3.4063 - accuracy: 0.13 - ETA: 1:42 - loss: 3.4062 - accuracy: 0.13 - ETA: 1:42 - loss: 3.4050 - accuracy: 0.13 - ETA: 1:42 - loss: 3.4056 - accuracy: 0.13 - ETA: 1:41 - loss: 3.4056 - accuracy: 0.13 - ETA: 1:41 - loss: 3.4060 - accuracy: 0.13 - ETA: 1:40 - loss: 3.4065 - accuracy: 0.13 - ETA: 1:40 - loss: 3.4043 - accuracy: 0.13 - ETA: 1:39 - loss: 3.4034 - accuracy: 0.13 - ETA: 1:39 - loss: 3.4024 - accuracy: 0.13 - ETA: 1:38 - loss: 3.4032 - accuracy: 0.13 - ETA: 1:38 - loss: 3.4012 - accuracy: 0.13 - ETA: 1:38 - loss: 3.4008 - accuracy: 0.13 - ETA: 1:37 - loss: 3.4012 - accuracy: 0.13 - ETA: 1:37 - loss: 3.4016 - accuracy: 0.13 - ETA: 1:36 - loss: 3.4010 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3997 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3992 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3987 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3960 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3953 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3940 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3937 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3933 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3922 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3909 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3909 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3893 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3903 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3894 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3885 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3864 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3863 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3869 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3876 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3864 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3853 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3850 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3827 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3818 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3805 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3783 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3763 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3747 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3750 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3739 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3728 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3723 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3715 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3699 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3694 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3687 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3685 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3681 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3669 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3675 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3665 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3663 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3658 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3650 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3658 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3649 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3648 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3648 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3636 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3632 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3630 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3624 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3616 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3614 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3620 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3618 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3608 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3602 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3597 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3589 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3587 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3583 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3586 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3581 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3579 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3583 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3582 - accuracy: 0.1386"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:05 - loss: 3.3588 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3588 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3587 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3590 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3576 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3575 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3578 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3577 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3574 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3572 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3562 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3561 - accuracy: 0.13 - ETA: 59s - loss: 3.3551 - accuracy: 0.1394 - ETA: 59s - loss: 3.3538 - accuracy: 0.139 - ETA: 59s - loss: 3.3529 - accuracy: 0.139 - ETA: 58s - loss: 3.3526 - accuracy: 0.139 - ETA: 58s - loss: 3.3530 - accuracy: 0.139 - ETA: 57s - loss: 3.3531 - accuracy: 0.139 - ETA: 57s - loss: 3.3526 - accuracy: 0.139 - ETA: 56s - loss: 3.3529 - accuracy: 0.139 - ETA: 56s - loss: 3.3526 - accuracy: 0.139 - ETA: 55s - loss: 3.3532 - accuracy: 0.139 - ETA: 55s - loss: 3.3529 - accuracy: 0.139 - ETA: 54s - loss: 3.3533 - accuracy: 0.139 - ETA: 54s - loss: 3.3523 - accuracy: 0.139 - ETA: 53s - loss: 3.3521 - accuracy: 0.139 - ETA: 53s - loss: 3.3521 - accuracy: 0.139 - ETA: 53s - loss: 3.3512 - accuracy: 0.139 - ETA: 52s - loss: 3.3504 - accuracy: 0.140 - ETA: 52s - loss: 3.3506 - accuracy: 0.139 - ETA: 51s - loss: 3.3498 - accuracy: 0.140 - ETA: 51s - loss: 3.3489 - accuracy: 0.140 - ETA: 50s - loss: 3.3489 - accuracy: 0.140 - ETA: 50s - loss: 3.3492 - accuracy: 0.140 - ETA: 49s - loss: 3.3490 - accuracy: 0.140 - ETA: 49s - loss: 3.3485 - accuracy: 0.140 - ETA: 48s - loss: 3.3480 - accuracy: 0.140 - ETA: 48s - loss: 3.3473 - accuracy: 0.140 - ETA: 48s - loss: 3.3466 - accuracy: 0.140 - ETA: 47s - loss: 3.3460 - accuracy: 0.140 - ETA: 47s - loss: 3.3464 - accuracy: 0.140 - ETA: 46s - loss: 3.3451 - accuracy: 0.141 - ETA: 46s - loss: 3.3449 - accuracy: 0.141 - ETA: 45s - loss: 3.3443 - accuracy: 0.141 - ETA: 45s - loss: 3.3442 - accuracy: 0.140 - ETA: 44s - loss: 3.3444 - accuracy: 0.140 - ETA: 44s - loss: 3.3440 - accuracy: 0.140 - ETA: 44s - loss: 3.3435 - accuracy: 0.140 - ETA: 43s - loss: 3.3437 - accuracy: 0.140 - ETA: 43s - loss: 3.3437 - accuracy: 0.140 - ETA: 42s - loss: 3.3436 - accuracy: 0.140 - ETA: 42s - loss: 3.3426 - accuracy: 0.141 - ETA: 41s - loss: 3.3422 - accuracy: 0.141 - ETA: 41s - loss: 3.3412 - accuracy: 0.141 - ETA: 40s - loss: 3.3416 - accuracy: 0.141 - ETA: 40s - loss: 3.3411 - accuracy: 0.141 - ETA: 39s - loss: 3.3414 - accuracy: 0.140 - ETA: 39s - loss: 3.3412 - accuracy: 0.140 - ETA: 39s - loss: 3.3408 - accuracy: 0.140 - ETA: 38s - loss: 3.3398 - accuracy: 0.140 - ETA: 38s - loss: 3.3403 - accuracy: 0.140 - ETA: 37s - loss: 3.3401 - accuracy: 0.140 - ETA: 37s - loss: 3.3401 - accuracy: 0.140 - ETA: 36s - loss: 3.3403 - accuracy: 0.140 - ETA: 36s - loss: 3.3405 - accuracy: 0.140 - ETA: 35s - loss: 3.3405 - accuracy: 0.140 - ETA: 35s - loss: 3.3407 - accuracy: 0.140 - ETA: 34s - loss: 3.3403 - accuracy: 0.140 - ETA: 34s - loss: 3.3397 - accuracy: 0.140 - ETA: 34s - loss: 3.3394 - accuracy: 0.140 - ETA: 33s - loss: 3.3393 - accuracy: 0.140 - ETA: 33s - loss: 3.3393 - accuracy: 0.141 - ETA: 32s - loss: 3.3391 - accuracy: 0.141 - ETA: 32s - loss: 3.3385 - accuracy: 0.141 - ETA: 31s - loss: 3.3383 - accuracy: 0.141 - ETA: 31s - loss: 3.3384 - accuracy: 0.141 - ETA: 30s - loss: 3.3382 - accuracy: 0.141 - ETA: 30s - loss: 3.3375 - accuracy: 0.141 - ETA: 29s - loss: 3.3368 - accuracy: 0.141 - ETA: 29s - loss: 3.3372 - accuracy: 0.141 - ETA: 29s - loss: 3.3366 - accuracy: 0.141 - ETA: 28s - loss: 3.3364 - accuracy: 0.141 - ETA: 28s - loss: 3.3367 - accuracy: 0.141 - ETA: 27s - loss: 3.3365 - accuracy: 0.141 - ETA: 27s - loss: 3.3365 - accuracy: 0.141 - ETA: 26s - loss: 3.3371 - accuracy: 0.141 - ETA: 26s - loss: 3.3364 - accuracy: 0.141 - ETA: 25s - loss: 3.3357 - accuracy: 0.141 - ETA: 25s - loss: 3.3352 - accuracy: 0.141 - ETA: 24s - loss: 3.3351 - accuracy: 0.141 - ETA: 24s - loss: 3.3358 - accuracy: 0.141 - ETA: 24s - loss: 3.3389 - accuracy: 0.141 - ETA: 23s - loss: 3.3380 - accuracy: 0.141 - ETA: 23s - loss: 3.3382 - accuracy: 0.141 - ETA: 22s - loss: 3.3374 - accuracy: 0.141 - ETA: 22s - loss: 3.3370 - accuracy: 0.141 - ETA: 21s - loss: 3.3365 - accuracy: 0.141 - ETA: 21s - loss: 3.3360 - accuracy: 0.141 - ETA: 20s - loss: 3.3363 - accuracy: 0.141 - ETA: 20s - loss: 3.3360 - accuracy: 0.141 - ETA: 19s - loss: 3.3360 - accuracy: 0.141 - ETA: 19s - loss: 3.3358 - accuracy: 0.141 - ETA: 19s - loss: 3.3361 - accuracy: 0.141 - ETA: 18s - loss: 3.3356 - accuracy: 0.141 - ETA: 18s - loss: 3.3352 - accuracy: 0.141 - ETA: 17s - loss: 3.3348 - accuracy: 0.142 - ETA: 17s - loss: 3.3348 - accuracy: 0.142 - ETA: 16s - loss: 3.3345 - accuracy: 0.141 - ETA: 16s - loss: 3.3347 - accuracy: 0.141 - ETA: 15s - loss: 3.3345 - accuracy: 0.141 - ETA: 15s - loss: 3.3346 - accuracy: 0.141 - ETA: 14s - loss: 3.3341 - accuracy: 0.141 - ETA: 14s - loss: 3.3343 - accuracy: 0.141 - ETA: 14s - loss: 3.3340 - accuracy: 0.141 - ETA: 13s - loss: 3.3336 - accuracy: 0.141 - ETA: 13s - loss: 3.3340 - accuracy: 0.141 - ETA: 12s - loss: 3.3345 - accuracy: 0.141 - ETA: 12s - loss: 3.3345 - accuracy: 0.141 - ETA: 11s - loss: 3.3351 - accuracy: 0.141 - ETA: 11s - loss: 3.3356 - accuracy: 0.141 - ETA: 10s - loss: 3.3348 - accuracy: 0.141 - ETA: 10s - loss: 3.3344 - accuracy: 0.141 - ETA: 10s - loss: 3.3346 - accuracy: 0.141 - ETA: 9s - loss: 3.3343 - accuracy: 0.141 - ETA: 9s - loss: 3.3342 - accuracy: 0.14 - ETA: 8s - loss: 3.3344 - accuracy: 0.14 - ETA: 8s - loss: 3.3337 - accuracy: 0.14 - ETA: 7s - loss: 3.3336 - accuracy: 0.14 - ETA: 7s - loss: 3.3332 - accuracy: 0.14 - ETA: 6s - loss: 3.3327 - accuracy: 0.14 - ETA: 6s - loss: 3.3324 - accuracy: 0.14 - ETA: 5s - loss: 3.3321 - accuracy: 0.14 - ETA: 5s - loss: 3.3319 - accuracy: 0.14 - ETA: 5s - loss: 3.3316 - accuracy: 0.14 - ETA: 4s - loss: 3.3313 - accuracy: 0.14 - ETA: 4s - loss: 3.3310 - accuracy: 0.14 - ETA: 3s - loss: 3.3309 - accuracy: 0.14 - ETA: 3s - loss: 3.3310 - accuracy: 0.14 - ETA: 2s - loss: 3.3315 - accuracy: 0.14 - ETA: 2s - loss: 3.3312 - accuracy: 0.14 - ETA: 1s - loss: 3.3313 - accuracy: 0.14 - ETA: 1s - loss: 3.3315 - accuracy: 0.14 - ETA: 0s - loss: 3.3314 - accuracy: 0.14 - ETA: 0s - loss: 3.3310 - accuracy: 0.14 - ETA: 0s - loss: 3.3309 - accuracy: 0.14 - 160s 4ms/step - loss: 3.3309 - accuracy: 0.1419 - val_loss: 4.0247 - val_accuracy: 0.0266\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:53 - loss: 3.3779 - accuracy: 0.14 - ETA: 2:38 - loss: 3.3242 - accuracy: 0.15 - ETA: 2:32 - loss: 3.3041 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2943 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2974 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2946 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2982 - accuracy: 0.14 - ETA: 2:32 - loss: 3.2916 - accuracy: 0.14 - ETA: 2:31 - loss: 3.3017 - accuracy: 0.14 - ETA: 2:30 - loss: 3.2870 - accuracy: 0.15 - ETA: 2:29 - loss: 3.2875 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2989 - accuracy: 0.14 - ETA: 2:27 - loss: 3.2989 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2937 - accuracy: 0.14 - ETA: 2:26 - loss: 3.2850 - accuracy: 0.14 - ETA: 2:25 - loss: 3.2895 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2887 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2920 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2977 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3078 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3191 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3333 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3477 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3536 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3640 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3731 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3734 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3743 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3767 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3772 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3786 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3783 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3790 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3841 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3851 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3835 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3792 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3796 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3818 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3813 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3836 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3866 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3856 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3899 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3851 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3828 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3860 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3888 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3917 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3891 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3911 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3875 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3896 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3932 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3957 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3912 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3923 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3926 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3913 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3919 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3915 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3910 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3924 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3963 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3981 - accuracy: 0.13 - ETA: 2:00 - loss: 3.4001 - accuracy: 0.13 - ETA: 1:59 - loss: 3.4002 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3997 - accuracy: 0.12 - ETA: 1:59 - loss: 3.3983 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3965 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3946 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3958 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3938 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3927 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3958 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3960 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3948 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3960 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3971 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3987 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3987 - accuracy: 0.13 - ETA: 1:53 - loss: 3.4001 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3988 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3987 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3981 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3986 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3986 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3983 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3980 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3975 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3958 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3941 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3934 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3956 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3951 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3952 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3933 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3931 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3923 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3906 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3890 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3886 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3863 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3854 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3847 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3835 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3838 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3853 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3841 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3839 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3857 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3865 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3868 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3883 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3885 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3879 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3902 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3895 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3896 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3899 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3907 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3919 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3899 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3893 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3901 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3892 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3913 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3904 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3909 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3920 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3936 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3949 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3947 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3951 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3943 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3940 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3933 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3913 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3893 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3880 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3877 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3870 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3869 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3854 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3851 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3851 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3847 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3840 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3846 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3842 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3826 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3820 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3819 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3821 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3817 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3812 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3809 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3805 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3805 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3804 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3793 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3779 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3772 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3769 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3767 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3758 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3762 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3768 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3768 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3773 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3777 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3773 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3780 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3764 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3764 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3765 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3759 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3753 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3757 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3756 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3745 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3755 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3758 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3758 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3760 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3753 - accuracy: 0.1382"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:05 - loss: 3.3743 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3739 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3732 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3736 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3741 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3736 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3729 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3729 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3720 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3721 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3726 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3717 - accuracy: 0.13 - ETA: 59s - loss: 3.3701 - accuracy: 0.1395 - ETA: 59s - loss: 3.3705 - accuracy: 0.139 - ETA: 58s - loss: 3.3707 - accuracy: 0.139 - ETA: 58s - loss: 3.3700 - accuracy: 0.139 - ETA: 57s - loss: 3.3694 - accuracy: 0.139 - ETA: 57s - loss: 3.3686 - accuracy: 0.139 - ETA: 57s - loss: 3.3682 - accuracy: 0.139 - ETA: 56s - loss: 3.3683 - accuracy: 0.139 - ETA: 56s - loss: 3.3676 - accuracy: 0.139 - ETA: 55s - loss: 3.3678 - accuracy: 0.139 - ETA: 55s - loss: 3.3670 - accuracy: 0.139 - ETA: 54s - loss: 3.3666 - accuracy: 0.140 - ETA: 54s - loss: 3.3663 - accuracy: 0.140 - ETA: 53s - loss: 3.3654 - accuracy: 0.140 - ETA: 53s - loss: 3.3652 - accuracy: 0.140 - ETA: 52s - loss: 3.3645 - accuracy: 0.140 - ETA: 52s - loss: 3.3642 - accuracy: 0.140 - ETA: 52s - loss: 3.3642 - accuracy: 0.140 - ETA: 51s - loss: 3.3649 - accuracy: 0.140 - ETA: 51s - loss: 3.3643 - accuracy: 0.140 - ETA: 50s - loss: 3.3647 - accuracy: 0.140 - ETA: 50s - loss: 3.3641 - accuracy: 0.140 - ETA: 49s - loss: 3.3638 - accuracy: 0.140 - ETA: 49s - loss: 3.3638 - accuracy: 0.140 - ETA: 48s - loss: 3.3634 - accuracy: 0.140 - ETA: 48s - loss: 3.3635 - accuracy: 0.140 - ETA: 48s - loss: 3.3634 - accuracy: 0.140 - ETA: 47s - loss: 3.3634 - accuracy: 0.140 - ETA: 47s - loss: 3.3641 - accuracy: 0.140 - ETA: 46s - loss: 3.3632 - accuracy: 0.140 - ETA: 46s - loss: 3.3630 - accuracy: 0.140 - ETA: 45s - loss: 3.3631 - accuracy: 0.140 - ETA: 45s - loss: 3.3635 - accuracy: 0.140 - ETA: 44s - loss: 3.3633 - accuracy: 0.140 - ETA: 44s - loss: 3.3636 - accuracy: 0.140 - ETA: 43s - loss: 3.3636 - accuracy: 0.140 - ETA: 43s - loss: 3.3637 - accuracy: 0.140 - ETA: 42s - loss: 3.3635 - accuracy: 0.140 - ETA: 42s - loss: 3.3634 - accuracy: 0.140 - ETA: 42s - loss: 3.3638 - accuracy: 0.140 - ETA: 41s - loss: 3.3637 - accuracy: 0.140 - ETA: 41s - loss: 3.3633 - accuracy: 0.140 - ETA: 40s - loss: 3.3631 - accuracy: 0.140 - ETA: 40s - loss: 3.3627 - accuracy: 0.140 - ETA: 39s - loss: 3.3623 - accuracy: 0.140 - ETA: 39s - loss: 3.3627 - accuracy: 0.140 - ETA: 38s - loss: 3.3625 - accuracy: 0.140 - ETA: 38s - loss: 3.3628 - accuracy: 0.140 - ETA: 37s - loss: 3.3629 - accuracy: 0.140 - ETA: 37s - loss: 3.3628 - accuracy: 0.140 - ETA: 37s - loss: 3.3624 - accuracy: 0.140 - ETA: 36s - loss: 3.3619 - accuracy: 0.140 - ETA: 36s - loss: 3.3614 - accuracy: 0.140 - ETA: 35s - loss: 3.3611 - accuracy: 0.140 - ETA: 35s - loss: 3.3613 - accuracy: 0.140 - ETA: 34s - loss: 3.3614 - accuracy: 0.140 - ETA: 34s - loss: 3.3617 - accuracy: 0.140 - ETA: 33s - loss: 3.3614 - accuracy: 0.140 - ETA: 33s - loss: 3.3609 - accuracy: 0.140 - ETA: 33s - loss: 3.3606 - accuracy: 0.140 - ETA: 32s - loss: 3.3609 - accuracy: 0.140 - ETA: 32s - loss: 3.3600 - accuracy: 0.140 - ETA: 31s - loss: 3.3594 - accuracy: 0.140 - ETA: 31s - loss: 3.3590 - accuracy: 0.140 - ETA: 30s - loss: 3.3588 - accuracy: 0.140 - ETA: 30s - loss: 3.3581 - accuracy: 0.140 - ETA: 29s - loss: 3.3579 - accuracy: 0.140 - ETA: 29s - loss: 3.3578 - accuracy: 0.140 - ETA: 28s - loss: 3.3574 - accuracy: 0.140 - ETA: 28s - loss: 3.3570 - accuracy: 0.141 - ETA: 28s - loss: 3.3567 - accuracy: 0.141 - ETA: 27s - loss: 3.3567 - accuracy: 0.141 - ETA: 27s - loss: 3.3562 - accuracy: 0.141 - ETA: 26s - loss: 3.3560 - accuracy: 0.141 - ETA: 26s - loss: 3.3554 - accuracy: 0.141 - ETA: 25s - loss: 3.3551 - accuracy: 0.141 - ETA: 25s - loss: 3.3545 - accuracy: 0.141 - ETA: 24s - loss: 3.3546 - accuracy: 0.141 - ETA: 24s - loss: 3.3543 - accuracy: 0.141 - ETA: 23s - loss: 3.3552 - accuracy: 0.141 - ETA: 23s - loss: 3.3557 - accuracy: 0.141 - ETA: 23s - loss: 3.3555 - accuracy: 0.141 - ETA: 22s - loss: 3.3550 - accuracy: 0.141 - ETA: 22s - loss: 3.3549 - accuracy: 0.141 - ETA: 21s - loss: 3.3544 - accuracy: 0.141 - ETA: 21s - loss: 3.3537 - accuracy: 0.141 - ETA: 20s - loss: 3.3537 - accuracy: 0.141 - ETA: 20s - loss: 3.3532 - accuracy: 0.141 - ETA: 19s - loss: 3.3535 - accuracy: 0.141 - ETA: 19s - loss: 3.3537 - accuracy: 0.141 - ETA: 18s - loss: 3.3542 - accuracy: 0.141 - ETA: 18s - loss: 3.3539 - accuracy: 0.141 - ETA: 18s - loss: 3.3539 - accuracy: 0.141 - ETA: 17s - loss: 3.3536 - accuracy: 0.141 - ETA: 17s - loss: 3.3529 - accuracy: 0.141 - ETA: 16s - loss: 3.3537 - accuracy: 0.141 - ETA: 16s - loss: 3.3535 - accuracy: 0.141 - ETA: 15s - loss: 3.3534 - accuracy: 0.141 - ETA: 15s - loss: 3.3531 - accuracy: 0.141 - ETA: 14s - loss: 3.3529 - accuracy: 0.141 - ETA: 14s - loss: 3.3520 - accuracy: 0.141 - ETA: 14s - loss: 3.3518 - accuracy: 0.141 - ETA: 13s - loss: 3.3515 - accuracy: 0.141 - ETA: 13s - loss: 3.3514 - accuracy: 0.141 - ETA: 12s - loss: 3.3514 - accuracy: 0.141 - ETA: 12s - loss: 3.3510 - accuracy: 0.141 - ETA: 11s - loss: 3.3498 - accuracy: 0.141 - ETA: 11s - loss: 3.3501 - accuracy: 0.141 - ETA: 10s - loss: 3.3508 - accuracy: 0.141 - ETA: 10s - loss: 3.3509 - accuracy: 0.141 - ETA: 9s - loss: 3.3508 - accuracy: 0.141 - ETA: 9s - loss: 3.3513 - accuracy: 0.14 - ETA: 9s - loss: 3.3512 - accuracy: 0.14 - ETA: 8s - loss: 3.3516 - accuracy: 0.14 - ETA: 8s - loss: 3.3515 - accuracy: 0.14 - ETA: 7s - loss: 3.3514 - accuracy: 0.14 - ETA: 7s - loss: 3.3517 - accuracy: 0.14 - ETA: 6s - loss: 3.3507 - accuracy: 0.14 - ETA: 6s - loss: 3.3498 - accuracy: 0.14 - ETA: 5s - loss: 3.3493 - accuracy: 0.14 - ETA: 5s - loss: 3.3492 - accuracy: 0.14 - ETA: 5s - loss: 3.3493 - accuracy: 0.14 - ETA: 4s - loss: 3.3493 - accuracy: 0.14 - ETA: 4s - loss: 3.3492 - accuracy: 0.14 - ETA: 3s - loss: 3.3491 - accuracy: 0.14 - ETA: 3s - loss: 3.3487 - accuracy: 0.14 - ETA: 2s - loss: 3.3484 - accuracy: 0.14 - ETA: 2s - loss: 3.3489 - accuracy: 0.14 - ETA: 1s - loss: 3.3488 - accuracy: 0.14 - ETA: 1s - loss: 3.3486 - accuracy: 0.14 - ETA: 0s - loss: 3.3484 - accuracy: 0.14 - ETA: 0s - loss: 3.3485 - accuracy: 0.14 - ETA: 0s - loss: 3.3484 - accuracy: 0.14 - 160s 4ms/step - loss: 3.3483 - accuracy: 0.1419 - val_loss: 4.0033 - val_accuracy: 0.0200\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:53 - loss: 3.3319 - accuracy: 0.11 - ETA: 2:39 - loss: 3.2448 - accuracy: 0.14 - ETA: 2:34 - loss: 3.2211 - accuracy: 0.16 - ETA: 2:31 - loss: 3.2701 - accuracy: 0.16 - ETA: 2:32 - loss: 3.2683 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2568 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2552 - accuracy: 0.15 - ETA: 2:33 - loss: 3.2594 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2817 - accuracy: 0.15 - ETA: 2:30 - loss: 3.2852 - accuracy: 0.15 - ETA: 2:29 - loss: 3.3004 - accuracy: 0.14 - ETA: 2:28 - loss: 3.3108 - accuracy: 0.14 - ETA: 2:27 - loss: 3.3261 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3274 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3265 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3166 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3006 - accuracy: 0.14 - ETA: 2:24 - loss: 3.2947 - accuracy: 0.14 - ETA: 2:23 - loss: 3.2969 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2934 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2892 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2919 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2917 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2965 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2937 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2951 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2966 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2930 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2965 - accuracy: 0.14 - ETA: 2:17 - loss: 3.3023 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2956 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2973 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2977 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2976 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2955 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2986 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3008 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2954 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2979 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3007 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3031 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3043 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3024 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3046 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3044 - accuracy: 0.14 - ETA: 2:09 - loss: 3.3054 - accuracy: 0.14 - ETA: 2:09 - loss: 3.3051 - accuracy: 0.14 - ETA: 2:08 - loss: 3.3057 - accuracy: 0.14 - ETA: 2:08 - loss: 3.3031 - accuracy: 0.14 - ETA: 2:07 - loss: 3.3065 - accuracy: 0.14 - ETA: 2:07 - loss: 3.3049 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3039 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3063 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3095 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3078 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3106 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3099 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3082 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3104 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3098 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3147 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3146 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3146 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3156 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3173 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3217 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3222 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3220 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3200 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3228 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3207 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3191 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3189 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3201 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3200 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3219 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3210 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3236 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3221 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3223 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3228 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3221 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3217 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3210 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3215 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3226 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3227 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3230 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3245 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3235 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3229 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3226 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3209 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3213 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3202 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3207 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3208 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3205 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3189 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3178 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3191 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3200 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3198 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3181 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3183 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3179 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3175 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3183 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3169 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3148 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3149 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3151 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3149 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3151 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3149 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3162 - accuracy: 0.14 - ETA: 1:37 - loss: 3.3153 - accuracy: 0.14 - ETA: 1:37 - loss: 3.3149 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3152 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3160 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3168 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3158 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3169 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3157 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3172 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3169 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3160 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3166 - accuracy: 0.14 - ETA: 1:31 - loss: 3.3170 - accuracy: 0.14 - ETA: 1:31 - loss: 3.3167 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3162 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3160 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3153 - accuracy: 0.14 - ETA: 1:29 - loss: 3.3155 - accuracy: 0.14 - ETA: 1:29 - loss: 3.3148 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3144 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3147 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3145 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3144 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3140 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3132 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3136 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3129 - accuracy: 0.14 - ETA: 1:24 - loss: 3.3129 - accuracy: 0.14 - ETA: 1:24 - loss: 3.3134 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3135 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3146 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3155 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3156 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3160 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3163 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3157 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3146 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3149 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3150 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3160 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3164 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3161 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3164 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3161 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3153 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3152 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3153 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3146 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3145 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3153 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3154 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3156 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3158 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3155 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3145 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3141 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3147 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3133 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3123 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3113 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3110 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3107 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3098 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3090 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3087 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3089 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3096 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3096 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3092 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3094 - accuracy: 0.1444"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:05 - loss: 3.3096 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3087 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3088 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3101 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3093 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3097 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3097 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3091 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3087 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3081 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3076 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3069 - accuracy: 0.14 - ETA: 59s - loss: 3.3076 - accuracy: 0.1440 - ETA: 59s - loss: 3.3080 - accuracy: 0.144 - ETA: 58s - loss: 3.3083 - accuracy: 0.144 - ETA: 58s - loss: 3.3088 - accuracy: 0.143 - ETA: 57s - loss: 3.3092 - accuracy: 0.143 - ETA: 57s - loss: 3.3092 - accuracy: 0.143 - ETA: 57s - loss: 3.3085 - accuracy: 0.143 - ETA: 56s - loss: 3.3087 - accuracy: 0.143 - ETA: 56s - loss: 3.3084 - accuracy: 0.143 - ETA: 55s - loss: 3.3093 - accuracy: 0.143 - ETA: 55s - loss: 3.3095 - accuracy: 0.143 - ETA: 54s - loss: 3.3084 - accuracy: 0.144 - ETA: 54s - loss: 3.3093 - accuracy: 0.143 - ETA: 53s - loss: 3.3095 - accuracy: 0.143 - ETA: 53s - loss: 3.3110 - accuracy: 0.143 - ETA: 52s - loss: 3.3111 - accuracy: 0.143 - ETA: 52s - loss: 3.3104 - accuracy: 0.143 - ETA: 52s - loss: 3.3096 - accuracy: 0.144 - ETA: 51s - loss: 3.3085 - accuracy: 0.144 - ETA: 51s - loss: 3.3089 - accuracy: 0.144 - ETA: 50s - loss: 3.3085 - accuracy: 0.144 - ETA: 50s - loss: 3.3082 - accuracy: 0.144 - ETA: 49s - loss: 3.3073 - accuracy: 0.144 - ETA: 49s - loss: 3.3072 - accuracy: 0.144 - ETA: 48s - loss: 3.3064 - accuracy: 0.144 - ETA: 48s - loss: 3.3067 - accuracy: 0.144 - ETA: 47s - loss: 3.3066 - accuracy: 0.144 - ETA: 47s - loss: 3.3062 - accuracy: 0.145 - ETA: 47s - loss: 3.3066 - accuracy: 0.145 - ETA: 46s - loss: 3.3068 - accuracy: 0.145 - ETA: 46s - loss: 3.3073 - accuracy: 0.145 - ETA: 45s - loss: 3.3071 - accuracy: 0.145 - ETA: 45s - loss: 3.3078 - accuracy: 0.145 - ETA: 44s - loss: 3.3074 - accuracy: 0.145 - ETA: 44s - loss: 3.3072 - accuracy: 0.145 - ETA: 43s - loss: 3.3068 - accuracy: 0.145 - ETA: 43s - loss: 3.3075 - accuracy: 0.145 - ETA: 42s - loss: 3.3072 - accuracy: 0.145 - ETA: 42s - loss: 3.3073 - accuracy: 0.146 - ETA: 42s - loss: 3.3071 - accuracy: 0.146 - ETA: 41s - loss: 3.3071 - accuracy: 0.146 - ETA: 41s - loss: 3.3071 - accuracy: 0.146 - ETA: 40s - loss: 3.3070 - accuracy: 0.146 - ETA: 40s - loss: 3.3073 - accuracy: 0.146 - ETA: 39s - loss: 3.3068 - accuracy: 0.146 - ETA: 39s - loss: 3.3066 - accuracy: 0.146 - ETA: 38s - loss: 3.3064 - accuracy: 0.146 - ETA: 38s - loss: 3.3059 - accuracy: 0.146 - ETA: 38s - loss: 3.3059 - accuracy: 0.146 - ETA: 37s - loss: 3.3063 - accuracy: 0.146 - ETA: 37s - loss: 3.3060 - accuracy: 0.146 - ETA: 36s - loss: 3.3063 - accuracy: 0.146 - ETA: 36s - loss: 3.3065 - accuracy: 0.146 - ETA: 35s - loss: 3.3066 - accuracy: 0.146 - ETA: 35s - loss: 3.3067 - accuracy: 0.145 - ETA: 34s - loss: 3.3057 - accuracy: 0.146 - ETA: 34s - loss: 3.3057 - accuracy: 0.146 - ETA: 34s - loss: 3.3049 - accuracy: 0.146 - ETA: 33s - loss: 3.3044 - accuracy: 0.145 - ETA: 33s - loss: 3.3040 - accuracy: 0.146 - ETA: 32s - loss: 3.3043 - accuracy: 0.145 - ETA: 32s - loss: 3.3038 - accuracy: 0.145 - ETA: 31s - loss: 3.3036 - accuracy: 0.146 - ETA: 31s - loss: 3.3029 - accuracy: 0.146 - ETA: 30s - loss: 3.3029 - accuracy: 0.145 - ETA: 30s - loss: 3.3023 - accuracy: 0.146 - ETA: 29s - loss: 3.3021 - accuracy: 0.146 - ETA: 29s - loss: 3.3015 - accuracy: 0.146 - ETA: 29s - loss: 3.3012 - accuracy: 0.146 - ETA: 28s - loss: 3.3008 - accuracy: 0.146 - ETA: 28s - loss: 3.3013 - accuracy: 0.146 - ETA: 27s - loss: 3.3007 - accuracy: 0.146 - ETA: 27s - loss: 3.2996 - accuracy: 0.146 - ETA: 26s - loss: 3.3003 - accuracy: 0.146 - ETA: 26s - loss: 3.2999 - accuracy: 0.146 - ETA: 25s - loss: 3.3000 - accuracy: 0.146 - ETA: 25s - loss: 3.2995 - accuracy: 0.146 - ETA: 24s - loss: 3.2995 - accuracy: 0.146 - ETA: 24s - loss: 3.2991 - accuracy: 0.146 - ETA: 24s - loss: 3.2986 - accuracy: 0.146 - ETA: 23s - loss: 3.2986 - accuracy: 0.146 - ETA: 23s - loss: 3.2980 - accuracy: 0.146 - ETA: 22s - loss: 3.2974 - accuracy: 0.146 - ETA: 22s - loss: 3.2973 - accuracy: 0.146 - ETA: 21s - loss: 3.2979 - accuracy: 0.146 - ETA: 21s - loss: 3.2979 - accuracy: 0.146 - ETA: 20s - loss: 3.2986 - accuracy: 0.146 - ETA: 20s - loss: 3.2980 - accuracy: 0.146 - ETA: 19s - loss: 3.2980 - accuracy: 0.146 - ETA: 19s - loss: 3.2980 - accuracy: 0.146 - ETA: 19s - loss: 3.2979 - accuracy: 0.146 - ETA: 18s - loss: 3.2975 - accuracy: 0.146 - ETA: 18s - loss: 3.2977 - accuracy: 0.146 - ETA: 17s - loss: 3.2969 - accuracy: 0.146 - ETA: 17s - loss: 3.2966 - accuracy: 0.147 - ETA: 16s - loss: 3.2961 - accuracy: 0.147 - ETA: 16s - loss: 3.2961 - accuracy: 0.147 - ETA: 15s - loss: 3.2959 - accuracy: 0.147 - ETA: 15s - loss: 3.2956 - accuracy: 0.147 - ETA: 14s - loss: 3.2957 - accuracy: 0.147 - ETA: 14s - loss: 3.2950 - accuracy: 0.147 - ETA: 14s - loss: 3.2950 - accuracy: 0.147 - ETA: 13s - loss: 3.2945 - accuracy: 0.147 - ETA: 13s - loss: 3.2942 - accuracy: 0.147 - ETA: 12s - loss: 3.2940 - accuracy: 0.147 - ETA: 12s - loss: 3.2940 - accuracy: 0.147 - ETA: 11s - loss: 3.2937 - accuracy: 0.147 - ETA: 11s - loss: 3.2935 - accuracy: 0.147 - ETA: 10s - loss: 3.2939 - accuracy: 0.147 - ETA: 10s - loss: 3.2941 - accuracy: 0.147 - ETA: 9s - loss: 3.2938 - accuracy: 0.147 - ETA: 9s - loss: 3.2942 - accuracy: 0.14 - ETA: 9s - loss: 3.2945 - accuracy: 0.14 - ETA: 8s - loss: 3.2947 - accuracy: 0.14 - ETA: 8s - loss: 3.2952 - accuracy: 0.14 - ETA: 7s - loss: 3.2949 - accuracy: 0.14 - ETA: 7s - loss: 3.2950 - accuracy: 0.14 - ETA: 6s - loss: 3.2949 - accuracy: 0.14 - ETA: 6s - loss: 3.2951 - accuracy: 0.14 - ETA: 5s - loss: 3.2947 - accuracy: 0.14 - ETA: 5s - loss: 3.2945 - accuracy: 0.14 - ETA: 5s - loss: 3.2943 - accuracy: 0.14 - ETA: 4s - loss: 3.2941 - accuracy: 0.14 - ETA: 4s - loss: 3.2942 - accuracy: 0.14 - ETA: 3s - loss: 3.2934 - accuracy: 0.14 - ETA: 3s - loss: 3.2929 - accuracy: 0.14 - ETA: 2s - loss: 3.2928 - accuracy: 0.14 - ETA: 2s - loss: 3.2926 - accuracy: 0.14 - ETA: 1s - loss: 3.2925 - accuracy: 0.14 - ETA: 1s - loss: 3.2921 - accuracy: 0.14 - ETA: 0s - loss: 3.2920 - accuracy: 0.14 - ETA: 0s - loss: 3.2921 - accuracy: 0.14 - ETA: 0s - loss: 3.2913 - accuracy: 0.14 - 160s 4ms/step - loss: 3.2912 - accuracy: 0.1467 - val_loss: 4.0024 - val_accuracy: 0.0299\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:57 - loss: 3.2405 - accuracy: 0.15 - ETA: 2:40 - loss: 3.1554 - accuracy: 0.17 - ETA: 2:35 - loss: 3.1629 - accuracy: 0.17 - ETA: 2:33 - loss: 3.2055 - accuracy: 0.16 - ETA: 2:33 - loss: 3.2051 - accuracy: 0.16 - ETA: 2:33 - loss: 3.2036 - accuracy: 0.15 - ETA: 2:33 - loss: 3.1980 - accuracy: 0.16 - ETA: 2:33 - loss: 3.2235 - accuracy: 0.15 - ETA: 2:31 - loss: 3.2351 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2332 - accuracy: 0.15 - ETA: 2:28 - loss: 3.2287 - accuracy: 0.15 - ETA: 2:27 - loss: 3.2303 - accuracy: 0.15 - ETA: 2:26 - loss: 3.2207 - accuracy: 0.15 - ETA: 2:25 - loss: 3.2229 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2238 - accuracy: 0.15 - ETA: 2:24 - loss: 3.2258 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2258 - accuracy: 0.15 - ETA: 2:23 - loss: 3.2402 - accuracy: 0.15 - ETA: 2:22 - loss: 3.2358 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2383 - accuracy: 0.15 - ETA: 2:21 - loss: 3.2256 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2195 - accuracy: 0.15 - ETA: 2:20 - loss: 3.2292 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2288 - accuracy: 0.15 - ETA: 2:19 - loss: 3.2276 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2329 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2362 - accuracy: 0.15 - ETA: 2:18 - loss: 3.2374 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2322 - accuracy: 0.15 - ETA: 2:17 - loss: 3.2313 - accuracy: 0.15 - ETA: 2:16 - loss: 3.2331 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2381 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2330 - accuracy: 0.15 - ETA: 2:15 - loss: 3.2354 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2299 - accuracy: 0.15 - ETA: 2:14 - loss: 3.2354 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2332 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2347 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2298 - accuracy: 0.15 - ETA: 2:13 - loss: 3.2316 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2281 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2282 - accuracy: 0.15 - ETA: 2:12 - loss: 3.2273 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2272 - accuracy: 0.15 - ETA: 2:11 - loss: 3.2313 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2334 - accuracy: 0.15 - ETA: 2:10 - loss: 3.2356 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2390 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2366 - accuracy: 0.15 - ETA: 2:09 - loss: 3.2385 - accuracy: 0.15 - ETA: 2:08 - loss: 3.2399 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2411 - accuracy: 0.15 - ETA: 2:07 - loss: 3.2427 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2430 - accuracy: 0.15 - ETA: 2:06 - loss: 3.2404 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2403 - accuracy: 0.15 - ETA: 2:05 - loss: 3.2387 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2401 - accuracy: 0.15 - ETA: 2:04 - loss: 3.2426 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2435 - accuracy: 0.15 - ETA: 2:03 - loss: 3.2443 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2446 - accuracy: 0.15 - ETA: 2:02 - loss: 3.2436 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2443 - accuracy: 0.15 - ETA: 2:01 - loss: 3.2442 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2438 - accuracy: 0.15 - ETA: 2:00 - loss: 3.2435 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2464 - accuracy: 0.15 - ETA: 1:59 - loss: 3.2488 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2469 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2500 - accuracy: 0.15 - ETA: 1:58 - loss: 3.2486 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2444 - accuracy: 0.15 - ETA: 1:57 - loss: 3.2446 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2439 - accuracy: 0.15 - ETA: 1:56 - loss: 3.2429 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2422 - accuracy: 0.15 - ETA: 1:55 - loss: 3.2448 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2432 - accuracy: 0.15 - ETA: 1:54 - loss: 3.2429 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2445 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2434 - accuracy: 0.15 - ETA: 1:53 - loss: 3.2461 - accuracy: 0.15 - ETA: 1:52 - loss: 3.2466 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2487 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2520 - accuracy: 0.15 - ETA: 1:51 - loss: 3.2525 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2530 - accuracy: 0.15 - ETA: 1:50 - loss: 3.2525 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2535 - accuracy: 0.15 - ETA: 1:49 - loss: 3.2552 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2561 - accuracy: 0.15 - ETA: 1:48 - loss: 3.2565 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2571 - accuracy: 0.15 - ETA: 1:47 - loss: 3.2568 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2578 - accuracy: 0.15 - ETA: 1:46 - loss: 3.2589 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2609 - accuracy: 0.15 - ETA: 1:45 - loss: 3.2615 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2640 - accuracy: 0.15 - ETA: 1:44 - loss: 3.2643 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2663 - accuracy: 0.15 - ETA: 1:43 - loss: 3.2681 - accuracy: 0.15 - ETA: 1:42 - loss: 3.2724 - accuracy: 0.14 - ETA: 1:42 - loss: 3.2721 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2732 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2739 - accuracy: 0.14 - ETA: 1:41 - loss: 3.2746 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2751 - accuracy: 0.14 - ETA: 1:40 - loss: 3.2750 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2751 - accuracy: 0.14 - ETA: 1:39 - loss: 3.2752 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2766 - accuracy: 0.14 - ETA: 1:38 - loss: 3.2772 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2792 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2794 - accuracy: 0.14 - ETA: 1:37 - loss: 3.2795 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2809 - accuracy: 0.14 - ETA: 1:36 - loss: 3.2823 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2819 - accuracy: 0.14 - ETA: 1:35 - loss: 3.2838 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2846 - accuracy: 0.14 - ETA: 1:34 - loss: 3.2846 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2859 - accuracy: 0.14 - ETA: 1:33 - loss: 3.2860 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2872 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2891 - accuracy: 0.14 - ETA: 1:32 - loss: 3.2895 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2919 - accuracy: 0.14 - ETA: 1:31 - loss: 3.2924 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2939 - accuracy: 0.14 - ETA: 1:30 - loss: 3.2947 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2950 - accuracy: 0.14 - ETA: 1:29 - loss: 3.2951 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2956 - accuracy: 0.14 - ETA: 1:28 - loss: 3.2959 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2971 - accuracy: 0.14 - ETA: 1:27 - loss: 3.2978 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2983 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2982 - accuracy: 0.14 - ETA: 1:26 - loss: 3.2989 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2976 - accuracy: 0.14 - ETA: 1:25 - loss: 3.2970 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2977 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2976 - accuracy: 0.14 - ETA: 1:24 - loss: 3.2980 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2977 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2983 - accuracy: 0.14 - ETA: 1:23 - loss: 3.2981 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2982 - accuracy: 0.14 - ETA: 1:22 - loss: 3.2980 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2989 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2993 - accuracy: 0.14 - ETA: 1:21 - loss: 3.2999 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3000 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3002 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2992 - accuracy: 0.14 - ETA: 1:19 - loss: 3.2995 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3001 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3002 - accuracy: 0.14 - ETA: 1:18 - loss: 3.2996 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3013 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3023 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3027 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3042 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3052 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3049 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3057 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3060 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3063 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3052 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3051 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3057 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3056 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3057 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3060 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3066 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3065 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3063 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3077 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3083 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3084 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3095 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3094 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3097 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3110 - accuracy: 0.1412"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:06 - loss: 3.3105 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3109 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3101 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3098 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3108 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3105 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3113 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3117 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3121 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3117 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3122 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3128 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3131 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3143 - accuracy: 0.14 - ETA: 59s - loss: 3.3141 - accuracy: 0.1412 - ETA: 59s - loss: 3.3147 - accuracy: 0.141 - ETA: 58s - loss: 3.3152 - accuracy: 0.141 - ETA: 58s - loss: 3.3157 - accuracy: 0.140 - ETA: 57s - loss: 3.3159 - accuracy: 0.141 - ETA: 57s - loss: 3.3151 - accuracy: 0.141 - ETA: 56s - loss: 3.3153 - accuracy: 0.141 - ETA: 56s - loss: 3.3157 - accuracy: 0.141 - ETA: 55s - loss: 3.3160 - accuracy: 0.141 - ETA: 55s - loss: 3.3166 - accuracy: 0.140 - ETA: 54s - loss: 3.3171 - accuracy: 0.140 - ETA: 54s - loss: 3.3187 - accuracy: 0.140 - ETA: 54s - loss: 3.3197 - accuracy: 0.140 - ETA: 53s - loss: 3.3215 - accuracy: 0.140 - ETA: 53s - loss: 3.3213 - accuracy: 0.140 - ETA: 52s - loss: 3.3221 - accuracy: 0.140 - ETA: 52s - loss: 3.3225 - accuracy: 0.140 - ETA: 51s - loss: 3.3222 - accuracy: 0.140 - ETA: 51s - loss: 3.3226 - accuracy: 0.140 - ETA: 50s - loss: 3.3224 - accuracy: 0.140 - ETA: 50s - loss: 3.3231 - accuracy: 0.140 - ETA: 49s - loss: 3.3231 - accuracy: 0.140 - ETA: 49s - loss: 3.3241 - accuracy: 0.140 - ETA: 48s - loss: 3.3242 - accuracy: 0.140 - ETA: 48s - loss: 3.3242 - accuracy: 0.140 - ETA: 48s - loss: 3.3247 - accuracy: 0.140 - ETA: 47s - loss: 3.3250 - accuracy: 0.140 - ETA: 47s - loss: 3.3253 - accuracy: 0.140 - ETA: 46s - loss: 3.3254 - accuracy: 0.140 - ETA: 46s - loss: 3.3263 - accuracy: 0.140 - ETA: 45s - loss: 3.3266 - accuracy: 0.140 - ETA: 45s - loss: 3.3268 - accuracy: 0.140 - ETA: 44s - loss: 3.3265 - accuracy: 0.140 - ETA: 44s - loss: 3.3265 - accuracy: 0.140 - ETA: 43s - loss: 3.3260 - accuracy: 0.140 - ETA: 43s - loss: 3.3260 - accuracy: 0.140 - ETA: 42s - loss: 3.3261 - accuracy: 0.140 - ETA: 42s - loss: 3.3256 - accuracy: 0.140 - ETA: 42s - loss: 3.3260 - accuracy: 0.140 - ETA: 41s - loss: 3.3263 - accuracy: 0.140 - ETA: 41s - loss: 3.3263 - accuracy: 0.140 - ETA: 40s - loss: 3.3267 - accuracy: 0.140 - ETA: 40s - loss: 3.3270 - accuracy: 0.140 - ETA: 39s - loss: 3.3266 - accuracy: 0.140 - ETA: 39s - loss: 3.3270 - accuracy: 0.140 - ETA: 38s - loss: 3.3273 - accuracy: 0.140 - ETA: 38s - loss: 3.3271 - accuracy: 0.140 - ETA: 37s - loss: 3.3269 - accuracy: 0.140 - ETA: 37s - loss: 3.3263 - accuracy: 0.140 - ETA: 36s - loss: 3.3265 - accuracy: 0.140 - ETA: 36s - loss: 3.3264 - accuracy: 0.140 - ETA: 36s - loss: 3.3268 - accuracy: 0.140 - ETA: 35s - loss: 3.3266 - accuracy: 0.140 - ETA: 35s - loss: 3.3264 - accuracy: 0.140 - ETA: 34s - loss: 3.3271 - accuracy: 0.140 - ETA: 34s - loss: 3.3264 - accuracy: 0.140 - ETA: 33s - loss: 3.3267 - accuracy: 0.140 - ETA: 33s - loss: 3.3271 - accuracy: 0.140 - ETA: 32s - loss: 3.3273 - accuracy: 0.140 - ETA: 32s - loss: 3.3274 - accuracy: 0.140 - ETA: 31s - loss: 3.3272 - accuracy: 0.140 - ETA: 31s - loss: 3.3274 - accuracy: 0.140 - ETA: 31s - loss: 3.3269 - accuracy: 0.140 - ETA: 30s - loss: 3.3271 - accuracy: 0.140 - ETA: 30s - loss: 3.3271 - accuracy: 0.140 - ETA: 29s - loss: 3.3270 - accuracy: 0.140 - ETA: 29s - loss: 3.3269 - accuracy: 0.140 - ETA: 28s - loss: 3.3265 - accuracy: 0.140 - ETA: 28s - loss: 3.3265 - accuracy: 0.140 - ETA: 27s - loss: 3.3263 - accuracy: 0.140 - ETA: 27s - loss: 3.3263 - accuracy: 0.140 - ETA: 26s - loss: 3.3260 - accuracy: 0.140 - ETA: 26s - loss: 3.3253 - accuracy: 0.140 - ETA: 26s - loss: 3.3254 - accuracy: 0.140 - ETA: 25s - loss: 3.3253 - accuracy: 0.140 - ETA: 25s - loss: 3.3249 - accuracy: 0.140 - ETA: 24s - loss: 3.3245 - accuracy: 0.140 - ETA: 24s - loss: 3.3246 - accuracy: 0.140 - ETA: 23s - loss: 3.3251 - accuracy: 0.140 - ETA: 23s - loss: 3.3253 - accuracy: 0.140 - ETA: 22s - loss: 3.3255 - accuracy: 0.140 - ETA: 22s - loss: 3.3259 - accuracy: 0.140 - ETA: 21s - loss: 3.3264 - accuracy: 0.140 - ETA: 21s - loss: 3.3261 - accuracy: 0.140 - ETA: 21s - loss: 3.3261 - accuracy: 0.140 - ETA: 20s - loss: 3.3253 - accuracy: 0.140 - ETA: 20s - loss: 3.3252 - accuracy: 0.140 - ETA: 19s - loss: 3.3258 - accuracy: 0.140 - ETA: 19s - loss: 3.3254 - accuracy: 0.140 - ETA: 18s - loss: 3.3251 - accuracy: 0.140 - ETA: 18s - loss: 3.3253 - accuracy: 0.140 - ETA: 17s - loss: 3.3255 - accuracy: 0.140 - ETA: 17s - loss: 3.3256 - accuracy: 0.140 - ETA: 17s - loss: 3.3256 - accuracy: 0.140 - ETA: 16s - loss: 3.3256 - accuracy: 0.140 - ETA: 16s - loss: 3.3290 - accuracy: 0.140 - ETA: 15s - loss: 3.3293 - accuracy: 0.140 - ETA: 15s - loss: 3.3292 - accuracy: 0.140 - ETA: 14s - loss: 3.3289 - accuracy: 0.140 - ETA: 14s - loss: 3.3291 - accuracy: 0.140 - ETA: 13s - loss: 3.3282 - accuracy: 0.140 - ETA: 13s - loss: 3.3283 - accuracy: 0.140 - ETA: 12s - loss: 3.3284 - accuracy: 0.140 - ETA: 12s - loss: 3.3282 - accuracy: 0.140 - ETA: 11s - loss: 3.3286 - accuracy: 0.140 - ETA: 11s - loss: 3.3283 - accuracy: 0.140 - ETA: 11s - loss: 3.3283 - accuracy: 0.140 - ETA: 10s - loss: 3.3275 - accuracy: 0.140 - ETA: 10s - loss: 3.3272 - accuracy: 0.140 - ETA: 9s - loss: 3.3268 - accuracy: 0.140 - ETA: 9s - loss: 3.3270 - accuracy: 0.14 - ETA: 8s - loss: 3.3276 - accuracy: 0.14 - ETA: 8s - loss: 3.3276 - accuracy: 0.14 - ETA: 7s - loss: 3.3278 - accuracy: 0.14 - ETA: 7s - loss: 3.3280 - accuracy: 0.14 - ETA: 6s - loss: 3.3276 - accuracy: 0.14 - ETA: 6s - loss: 3.3279 - accuracy: 0.14 - ETA: 5s - loss: 3.3275 - accuracy: 0.14 - ETA: 5s - loss: 3.3266 - accuracy: 0.14 - ETA: 5s - loss: 3.3264 - accuracy: 0.14 - ETA: 4s - loss: 3.3263 - accuracy: 0.14 - ETA: 4s - loss: 3.3261 - accuracy: 0.14 - ETA: 3s - loss: 3.3262 - accuracy: 0.14 - ETA: 3s - loss: 3.3255 - accuracy: 0.14 - ETA: 2s - loss: 3.3251 - accuracy: 0.14 - ETA: 2s - loss: 3.3249 - accuracy: 0.14 - ETA: 1s - loss: 3.3260 - accuracy: 0.14 - ETA: 1s - loss: 3.3272 - accuracy: 0.14 - ETA: 0s - loss: 3.3278 - accuracy: 0.14 - ETA: 0s - loss: 3.3282 - accuracy: 0.14 - ETA: 0s - loss: 3.3279 - accuracy: 0.14 - 162s 4ms/step - loss: 3.3278 - accuracy: 0.1412 - val_loss: 4.0694 - val_accuracy: 0.0294\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:47 - loss: 3.3381 - accuracy: 0.11 - ETA: 2:35 - loss: 3.3534 - accuracy: 0.12 - ETA: 2:32 - loss: 3.2859 - accuracy: 0.14 - ETA: 2:29 - loss: 3.2907 - accuracy: 0.15 - ETA: 2:28 - loss: 3.3150 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3200 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3261 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3342 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3333 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3275 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3151 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3147 - accuracy: 0.14 - ETA: 2:23 - loss: 3.3088 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3013 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3018 - accuracy: 0.14 - ETA: 2:22 - loss: 3.2906 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2919 - accuracy: 0.14 - ETA: 2:21 - loss: 3.2910 - accuracy: 0.14 - ETA: 2:20 - loss: 3.2952 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2930 - accuracy: 0.14 - ETA: 2:19 - loss: 3.2871 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2901 - accuracy: 0.14 - ETA: 2:18 - loss: 3.2872 - accuracy: 0.13 - ETA: 2:17 - loss: 3.2878 - accuracy: 0.14 - ETA: 2:17 - loss: 3.2902 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2875 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2939 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2955 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2976 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2968 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2942 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2946 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2913 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2934 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2931 - accuracy: 0.14 - ETA: 2:16 - loss: 3.2930 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2880 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2895 - accuracy: 0.14 - ETA: 2:15 - loss: 3.2877 - accuracy: 0.14 - ETA: 2:14 - loss: 3.2862 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2879 - accuracy: 0.14 - ETA: 2:13 - loss: 3.2894 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2897 - accuracy: 0.14 - ETA: 2:12 - loss: 3.2913 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2849 - accuracy: 0.14 - ETA: 2:11 - loss: 3.2839 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2844 - accuracy: 0.14 - ETA: 2:10 - loss: 3.2865 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2875 - accuracy: 0.14 - ETA: 2:09 - loss: 3.2910 - accuracy: 0.14 - ETA: 2:08 - loss: 3.2908 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2936 - accuracy: 0.14 - ETA: 2:07 - loss: 3.2960 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2956 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2947 - accuracy: 0.14 - ETA: 2:06 - loss: 3.2982 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3011 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3007 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3005 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3010 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3018 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3059 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3050 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3043 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3076 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3101 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3110 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3113 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3130 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3125 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3144 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3165 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3189 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3207 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3246 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3238 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3258 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3246 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3268 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3274 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3280 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3286 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3293 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3279 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3277 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3267 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3253 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3256 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3248 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3232 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3244 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3253 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3244 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3228 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3231 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3237 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3240 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3251 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3250 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3252 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3248 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3259 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3249 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3251 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3256 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3236 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3240 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3235 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3226 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3214 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3204 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3206 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3206 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3214 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3235 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3232 - accuracy: 0.14 - ETA: 1:37 - loss: 3.3222 - accuracy: 0.14 - ETA: 1:37 - loss: 3.3232 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3239 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3237 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3238 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3236 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3244 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3242 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3243 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3242 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3252 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3259 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3259 - accuracy: 0.14 - ETA: 1:31 - loss: 3.3265 - accuracy: 0.14 - ETA: 1:31 - loss: 3.3253 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3260 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3260 - accuracy: 0.14 - ETA: 1:29 - loss: 3.3244 - accuracy: 0.14 - ETA: 1:29 - loss: 3.3240 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3235 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3238 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3251 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3241 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3251 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3261 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3258 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3269 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3277 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3284 - accuracy: 0.14 - ETA: 1:24 - loss: 3.3270 - accuracy: 0.14 - ETA: 1:24 - loss: 3.3263 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3259 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3256 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3241 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3252 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3255 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3250 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3269 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3260 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3263 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3268 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3272 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3283 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3290 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3290 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3291 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3292 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3285 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3287 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3285 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3287 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3281 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3273 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3281 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3280 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3279 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3270 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3269 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3267 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3270 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3262 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3265 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3267 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3272 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3271 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3266 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3276 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3288 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3286 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3283 - accuracy: 0.1444"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:05 - loss: 3.3277 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3275 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3278 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3283 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3287 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3292 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3286 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3279 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3280 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3293 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3299 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3299 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3296 - accuracy: 0.14 - ETA: 59s - loss: 3.3312 - accuracy: 0.1448 - ETA: 59s - loss: 3.3322 - accuracy: 0.144 - ETA: 58s - loss: 3.3314 - accuracy: 0.144 - ETA: 58s - loss: 3.3298 - accuracy: 0.145 - ETA: 57s - loss: 3.3287 - accuracy: 0.145 - ETA: 57s - loss: 3.3290 - accuracy: 0.145 - ETA: 57s - loss: 3.3284 - accuracy: 0.145 - ETA: 56s - loss: 3.3282 - accuracy: 0.145 - ETA: 56s - loss: 3.3280 - accuracy: 0.145 - ETA: 55s - loss: 3.3283 - accuracy: 0.145 - ETA: 55s - loss: 3.3277 - accuracy: 0.145 - ETA: 54s - loss: 3.3269 - accuracy: 0.146 - ETA: 54s - loss: 3.3265 - accuracy: 0.145 - ETA: 53s - loss: 3.3262 - accuracy: 0.145 - ETA: 53s - loss: 3.3263 - accuracy: 0.146 - ETA: 52s - loss: 3.3256 - accuracy: 0.146 - ETA: 52s - loss: 3.3254 - accuracy: 0.146 - ETA: 52s - loss: 3.3245 - accuracy: 0.146 - ETA: 51s - loss: 3.3247 - accuracy: 0.146 - ETA: 51s - loss: 3.3248 - accuracy: 0.146 - ETA: 50s - loss: 3.3244 - accuracy: 0.146 - ETA: 50s - loss: 3.3249 - accuracy: 0.146 - ETA: 49s - loss: 3.3255 - accuracy: 0.146 - ETA: 49s - loss: 3.3248 - accuracy: 0.146 - ETA: 48s - loss: 3.3250 - accuracy: 0.146 - ETA: 48s - loss: 3.3247 - accuracy: 0.146 - ETA: 48s - loss: 3.3257 - accuracy: 0.146 - ETA: 47s - loss: 3.3248 - accuracy: 0.146 - ETA: 47s - loss: 3.3247 - accuracy: 0.146 - ETA: 46s - loss: 3.3238 - accuracy: 0.146 - ETA: 46s - loss: 3.3239 - accuracy: 0.146 - ETA: 45s - loss: 3.3246 - accuracy: 0.146 - ETA: 45s - loss: 3.3250 - accuracy: 0.146 - ETA: 44s - loss: 3.3248 - accuracy: 0.146 - ETA: 44s - loss: 3.3254 - accuracy: 0.145 - ETA: 43s - loss: 3.3249 - accuracy: 0.146 - ETA: 43s - loss: 3.3248 - accuracy: 0.146 - ETA: 42s - loss: 3.3247 - accuracy: 0.146 - ETA: 42s - loss: 3.3249 - accuracy: 0.145 - ETA: 42s - loss: 3.3251 - accuracy: 0.145 - ETA: 41s - loss: 3.3247 - accuracy: 0.145 - ETA: 41s - loss: 3.3251 - accuracy: 0.145 - ETA: 40s - loss: 3.3248 - accuracy: 0.145 - ETA: 40s - loss: 3.3257 - accuracy: 0.145 - ETA: 39s - loss: 3.3262 - accuracy: 0.145 - ETA: 39s - loss: 3.3260 - accuracy: 0.145 - ETA: 38s - loss: 3.3257 - accuracy: 0.145 - ETA: 38s - loss: 3.3260 - accuracy: 0.145 - ETA: 37s - loss: 3.3263 - accuracy: 0.145 - ETA: 37s - loss: 3.3267 - accuracy: 0.145 - ETA: 37s - loss: 3.3264 - accuracy: 0.145 - ETA: 36s - loss: 3.3266 - accuracy: 0.145 - ETA: 36s - loss: 3.3262 - accuracy: 0.145 - ETA: 35s - loss: 3.3262 - accuracy: 0.145 - ETA: 35s - loss: 3.3261 - accuracy: 0.145 - ETA: 34s - loss: 3.3254 - accuracy: 0.145 - ETA: 34s - loss: 3.3253 - accuracy: 0.145 - ETA: 33s - loss: 3.3255 - accuracy: 0.145 - ETA: 33s - loss: 3.3256 - accuracy: 0.145 - ETA: 32s - loss: 3.3257 - accuracy: 0.145 - ETA: 32s - loss: 3.3258 - accuracy: 0.145 - ETA: 32s - loss: 3.3255 - accuracy: 0.145 - ETA: 31s - loss: 3.3255 - accuracy: 0.145 - ETA: 31s - loss: 3.3255 - accuracy: 0.145 - ETA: 30s - loss: 3.3263 - accuracy: 0.145 - ETA: 30s - loss: 3.3263 - accuracy: 0.145 - ETA: 29s - loss: 3.3261 - accuracy: 0.145 - ETA: 29s - loss: 3.3260 - accuracy: 0.145 - ETA: 28s - loss: 3.3262 - accuracy: 0.145 - ETA: 28s - loss: 3.3262 - accuracy: 0.145 - ETA: 27s - loss: 3.3265 - accuracy: 0.145 - ETA: 27s - loss: 3.3261 - accuracy: 0.145 - ETA: 26s - loss: 3.3257 - accuracy: 0.145 - ETA: 26s - loss: 3.3259 - accuracy: 0.145 - ETA: 26s - loss: 3.3263 - accuracy: 0.145 - ETA: 25s - loss: 3.3265 - accuracy: 0.145 - ETA: 25s - loss: 3.3264 - accuracy: 0.145 - ETA: 24s - loss: 3.3261 - accuracy: 0.145 - ETA: 24s - loss: 3.3265 - accuracy: 0.145 - ETA: 23s - loss: 3.3261 - accuracy: 0.145 - ETA: 23s - loss: 3.3262 - accuracy: 0.145 - ETA: 22s - loss: 3.3260 - accuracy: 0.145 - ETA: 22s - loss: 3.3261 - accuracy: 0.145 - ETA: 21s - loss: 3.3251 - accuracy: 0.145 - ETA: 21s - loss: 3.3258 - accuracy: 0.145 - ETA: 21s - loss: 3.3256 - accuracy: 0.145 - ETA: 20s - loss: 3.3259 - accuracy: 0.145 - ETA: 20s - loss: 3.3252 - accuracy: 0.145 - ETA: 19s - loss: 3.3255 - accuracy: 0.145 - ETA: 19s - loss: 3.3258 - accuracy: 0.145 - ETA: 18s - loss: 3.3262 - accuracy: 0.145 - ETA: 18s - loss: 3.3259 - accuracy: 0.145 - ETA: 17s - loss: 3.3262 - accuracy: 0.145 - ETA: 17s - loss: 3.3262 - accuracy: 0.145 - ETA: 16s - loss: 3.3263 - accuracy: 0.145 - ETA: 16s - loss: 3.3261 - accuracy: 0.145 - ETA: 16s - loss: 3.3263 - accuracy: 0.144 - ETA: 15s - loss: 3.3265 - accuracy: 0.145 - ETA: 15s - loss: 3.3267 - accuracy: 0.145 - ETA: 14s - loss: 3.3269 - accuracy: 0.144 - ETA: 14s - loss: 3.3265 - accuracy: 0.144 - ETA: 13s - loss: 3.3272 - accuracy: 0.144 - ETA: 13s - loss: 3.3276 - accuracy: 0.144 - ETA: 12s - loss: 3.3273 - accuracy: 0.145 - ETA: 12s - loss: 3.3274 - accuracy: 0.144 - ETA: 11s - loss: 3.3281 - accuracy: 0.144 - ETA: 11s - loss: 3.3279 - accuracy: 0.144 - ETA: 10s - loss: 3.3278 - accuracy: 0.144 - ETA: 10s - loss: 3.3278 - accuracy: 0.144 - ETA: 10s - loss: 3.3277 - accuracy: 0.144 - ETA: 9s - loss: 3.3271 - accuracy: 0.144 - ETA: 9s - loss: 3.3270 - accuracy: 0.14 - ETA: 8s - loss: 3.3270 - accuracy: 0.14 - ETA: 8s - loss: 3.3267 - accuracy: 0.14 - ETA: 7s - loss: 3.3259 - accuracy: 0.14 - ETA: 7s - loss: 3.3255 - accuracy: 0.14 - ETA: 6s - loss: 3.3247 - accuracy: 0.14 - ETA: 6s - loss: 3.3245 - accuracy: 0.14 - ETA: 5s - loss: 3.3247 - accuracy: 0.14 - ETA: 5s - loss: 3.3252 - accuracy: 0.14 - ETA: 5s - loss: 3.3255 - accuracy: 0.14 - ETA: 4s - loss: 3.3255 - accuracy: 0.14 - ETA: 4s - loss: 3.3258 - accuracy: 0.14 - ETA: 3s - loss: 3.3259 - accuracy: 0.14 - ETA: 3s - loss: 3.3262 - accuracy: 0.14 - ETA: 2s - loss: 3.3259 - accuracy: 0.14 - ETA: 2s - loss: 3.3260 - accuracy: 0.14 - ETA: 1s - loss: 3.3255 - accuracy: 0.14 - ETA: 1s - loss: 3.3259 - accuracy: 0.14 - ETA: 0s - loss: 3.3260 - accuracy: 0.14 - ETA: 0s - loss: 3.3264 - accuracy: 0.14 - ETA: 0s - loss: 3.3262 - accuracy: 0.14 - 162s 4ms/step - loss: 3.3262 - accuracy: 0.1449 - val_loss: 4.0455 - val_accuracy: 0.0267\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 2:40 - loss: 3.4885 - accuracy: 0.12 - ETA: 2:32 - loss: 3.4214 - accuracy: 0.13 - ETA: 2:28 - loss: 3.4079 - accuracy: 0.12 - ETA: 2:27 - loss: 3.4288 - accuracy: 0.11 - ETA: 2:27 - loss: 3.4209 - accuracy: 0.11 - ETA: 2:26 - loss: 3.3958 - accuracy: 0.12 - ETA: 2:25 - loss: 3.3928 - accuracy: 0.12 - ETA: 2:26 - loss: 3.4058 - accuracy: 0.12 - ETA: 2:25 - loss: 3.3818 - accuracy: 0.12 - ETA: 2:25 - loss: 3.3789 - accuracy: 0.12 - ETA: 2:24 - loss: 3.3551 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3470 - accuracy: 0.14 - ETA: 2:23 - loss: 3.3381 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3415 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3407 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3358 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3320 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3366 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3387 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3360 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3439 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3477 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3446 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3424 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3416 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3340 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3368 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3412 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3451 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3518 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3504 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3547 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3529 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3534 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3523 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3500 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3584 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3568 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3564 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3508 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3532 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3543 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3556 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3587 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3558 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3570 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3609 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3635 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3586 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3577 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3539 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3510 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3483 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3440 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3413 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3390 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3402 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3390 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3402 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3446 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3449 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3500 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3497 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3498 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3475 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3473 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3490 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3466 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3436 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3437 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3445 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3447 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3441 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3440 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3426 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3432 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3419 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3437 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3458 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3460 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3466 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3457 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3445 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3439 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3439 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3441 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3431 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3413 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3426 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3426 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3439 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3454 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3439 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3416 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3420 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3407 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3399 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3387 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3407 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3413 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3381 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3370 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3381 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3381 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3392 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3380 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3378 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3375 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3343 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3342 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3343 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3341 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3343 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3311 - accuracy: 0.14 - ETA: 1:37 - loss: 3.3308 - accuracy: 0.14 - ETA: 1:37 - loss: 3.3307 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3318 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3305 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3307 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3312 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3324 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3318 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3307 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3318 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3312 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3322 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3317 - accuracy: 0.14 - ETA: 1:31 - loss: 3.3311 - accuracy: 0.14 - ETA: 1:31 - loss: 3.3303 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3289 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3283 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3282 - accuracy: 0.14 - ETA: 1:29 - loss: 3.3280 - accuracy: 0.14 - ETA: 1:29 - loss: 3.3270 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3272 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3275 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3293 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3303 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3312 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3311 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3303 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3308 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3300 - accuracy: 0.14 - ETA: 1:24 - loss: 3.3300 - accuracy: 0.14 - ETA: 1:24 - loss: 3.3303 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3291 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3279 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3280 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3281 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3283 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3279 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3271 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3261 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3250 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3252 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3241 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3246 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3246 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3241 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3242 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3245 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3251 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3250 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3256 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3265 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3261 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3252 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3248 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3247 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3244 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3247 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3248 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3241 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3250 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3249 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3253 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3258 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3263 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3271 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3278 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3280 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3286 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3289 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3301 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3304 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3312 - accuracy: 0.1431"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:09 - loss: 3.3326 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3332 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3342 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3339 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3352 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3358 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3367 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3378 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3380 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3383 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3384 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3378 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3375 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3384 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3380 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3390 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3392 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3400 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3392 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3393 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3397 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3395 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3398 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3400 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3391 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3394 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3403 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3403 - accuracy: 0.14 - ETA: 59s - loss: 3.3408 - accuracy: 0.1400 - ETA: 59s - loss: 3.3403 - accuracy: 0.140 - ETA: 59s - loss: 3.3407 - accuracy: 0.139 - ETA: 58s - loss: 3.3411 - accuracy: 0.139 - ETA: 58s - loss: 3.3414 - accuracy: 0.139 - ETA: 57s - loss: 3.3425 - accuracy: 0.139 - ETA: 57s - loss: 3.3422 - accuracy: 0.139 - ETA: 56s - loss: 3.3418 - accuracy: 0.139 - ETA: 56s - loss: 3.3420 - accuracy: 0.139 - ETA: 56s - loss: 3.3410 - accuracy: 0.139 - ETA: 55s - loss: 3.3403 - accuracy: 0.139 - ETA: 55s - loss: 3.3400 - accuracy: 0.140 - ETA: 54s - loss: 3.3399 - accuracy: 0.140 - ETA: 54s - loss: 3.3398 - accuracy: 0.140 - ETA: 54s - loss: 3.3404 - accuracy: 0.140 - ETA: 53s - loss: 3.3405 - accuracy: 0.140 - ETA: 53s - loss: 3.3405 - accuracy: 0.140 - ETA: 52s - loss: 3.3400 - accuracy: 0.140 - ETA: 52s - loss: 3.3405 - accuracy: 0.139 - ETA: 51s - loss: 3.3405 - accuracy: 0.139 - ETA: 51s - loss: 3.3404 - accuracy: 0.140 - ETA: 50s - loss: 3.3412 - accuracy: 0.139 - ETA: 50s - loss: 3.3418 - accuracy: 0.139 - ETA: 50s - loss: 3.3415 - accuracy: 0.139 - ETA: 49s - loss: 3.3410 - accuracy: 0.139 - ETA: 49s - loss: 3.3408 - accuracy: 0.139 - ETA: 48s - loss: 3.3403 - accuracy: 0.139 - ETA: 48s - loss: 3.3414 - accuracy: 0.140 - ETA: 47s - loss: 3.3412 - accuracy: 0.140 - ETA: 47s - loss: 3.3400 - accuracy: 0.140 - ETA: 46s - loss: 3.3391 - accuracy: 0.140 - ETA: 46s - loss: 3.3392 - accuracy: 0.140 - ETA: 45s - loss: 3.3390 - accuracy: 0.140 - ETA: 45s - loss: 3.3396 - accuracy: 0.140 - ETA: 44s - loss: 3.3392 - accuracy: 0.140 - ETA: 44s - loss: 3.3393 - accuracy: 0.140 - ETA: 43s - loss: 3.3396 - accuracy: 0.139 - ETA: 43s - loss: 3.3402 - accuracy: 0.139 - ETA: 42s - loss: 3.3402 - accuracy: 0.139 - ETA: 42s - loss: 3.3402 - accuracy: 0.139 - ETA: 41s - loss: 3.3403 - accuracy: 0.139 - ETA: 41s - loss: 3.3404 - accuracy: 0.139 - ETA: 40s - loss: 3.3401 - accuracy: 0.139 - ETA: 40s - loss: 3.3402 - accuracy: 0.139 - ETA: 39s - loss: 3.3409 - accuracy: 0.139 - ETA: 39s - loss: 3.3416 - accuracy: 0.139 - ETA: 38s - loss: 3.3424 - accuracy: 0.139 - ETA: 38s - loss: 3.3434 - accuracy: 0.139 - ETA: 37s - loss: 3.3447 - accuracy: 0.139 - ETA: 37s - loss: 3.3451 - accuracy: 0.139 - ETA: 36s - loss: 3.3449 - accuracy: 0.139 - ETA: 36s - loss: 3.3447 - accuracy: 0.139 - ETA: 35s - loss: 3.3450 - accuracy: 0.139 - ETA: 35s - loss: 3.3453 - accuracy: 0.139 - ETA: 34s - loss: 3.3450 - accuracy: 0.139 - ETA: 34s - loss: 3.3451 - accuracy: 0.139 - ETA: 33s - loss: 3.3442 - accuracy: 0.139 - ETA: 33s - loss: 3.3438 - accuracy: 0.139 - ETA: 32s - loss: 3.3440 - accuracy: 0.139 - ETA: 32s - loss: 3.3435 - accuracy: 0.139 - ETA: 31s - loss: 3.3436 - accuracy: 0.139 - ETA: 31s - loss: 3.3437 - accuracy: 0.139 - ETA: 30s - loss: 3.3436 - accuracy: 0.139 - ETA: 30s - loss: 3.3437 - accuracy: 0.139 - ETA: 29s - loss: 3.3439 - accuracy: 0.139 - ETA: 29s - loss: 3.3440 - accuracy: 0.139 - ETA: 28s - loss: 3.3436 - accuracy: 0.139 - ETA: 27s - loss: 3.3444 - accuracy: 0.139 - ETA: 27s - loss: 3.3446 - accuracy: 0.139 - ETA: 26s - loss: 3.3451 - accuracy: 0.139 - ETA: 26s - loss: 3.3447 - accuracy: 0.139 - ETA: 25s - loss: 3.3451 - accuracy: 0.139 - ETA: 25s - loss: 3.3456 - accuracy: 0.139 - ETA: 24s - loss: 3.3459 - accuracy: 0.139 - ETA: 24s - loss: 3.3460 - accuracy: 0.139 - ETA: 23s - loss: 3.3461 - accuracy: 0.139 - ETA: 23s - loss: 3.3455 - accuracy: 0.139 - ETA: 22s - loss: 3.3452 - accuracy: 0.139 - ETA: 21s - loss: 3.3457 - accuracy: 0.139 - ETA: 21s - loss: 3.3455 - accuracy: 0.139 - ETA: 20s - loss: 3.3452 - accuracy: 0.139 - ETA: 20s - loss: 3.3458 - accuracy: 0.139 - ETA: 19s - loss: 3.3454 - accuracy: 0.139 - ETA: 19s - loss: 3.3454 - accuracy: 0.139 - ETA: 18s - loss: 3.3450 - accuracy: 0.139 - ETA: 18s - loss: 3.3449 - accuracy: 0.139 - ETA: 17s - loss: 3.3448 - accuracy: 0.139 - ETA: 16s - loss: 3.3449 - accuracy: 0.139 - ETA: 16s - loss: 3.3453 - accuracy: 0.139 - ETA: 15s - loss: 3.3447 - accuracy: 0.139 - ETA: 15s - loss: 3.3445 - accuracy: 0.139 - ETA: 14s - loss: 3.3445 - accuracy: 0.139 - ETA: 14s - loss: 3.3450 - accuracy: 0.139 - ETA: 13s - loss: 3.3450 - accuracy: 0.139 - ETA: 12s - loss: 3.3451 - accuracy: 0.139 - ETA: 12s - loss: 3.3450 - accuracy: 0.139 - ETA: 11s - loss: 3.3444 - accuracy: 0.139 - ETA: 11s - loss: 3.3441 - accuracy: 0.139 - ETA: 10s - loss: 3.3437 - accuracy: 0.139 - ETA: 10s - loss: 3.3437 - accuracy: 0.139 - ETA: 9s - loss: 3.3437 - accuracy: 0.139 - ETA: 8s - loss: 3.3429 - accuracy: 0.13 - ETA: 8s - loss: 3.3422 - accuracy: 0.13 - ETA: 7s - loss: 3.3422 - accuracy: 0.13 - ETA: 7s - loss: 3.3424 - accuracy: 0.13 - ETA: 6s - loss: 3.3428 - accuracy: 0.13 - ETA: 5s - loss: 3.3426 - accuracy: 0.13 - ETA: 5s - loss: 3.3426 - accuracy: 0.13 - ETA: 4s - loss: 3.3422 - accuracy: 0.13 - ETA: 4s - loss: 3.3425 - accuracy: 0.13 - ETA: 3s - loss: 3.3428 - accuracy: 0.13 - ETA: 3s - loss: 3.3433 - accuracy: 0.13 - ETA: 2s - loss: 3.3436 - accuracy: 0.13 - ETA: 1s - loss: 3.3432 - accuracy: 0.13 - ETA: 1s - loss: 3.3433 - accuracy: 0.13 - ETA: 0s - loss: 3.3434 - accuracy: 0.13 - ETA: 0s - loss: 3.3434 - accuracy: 0.13 - 214s 5ms/step - loss: 3.3434 - accuracy: 0.1397 - val_loss: 4.1032 - val_accuracy: 0.0242\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:00 - loss: 3.3749 - accuracy: 0.18 - ETA: 3:58 - loss: 3.3417 - accuracy: 0.17 - ETA: 3:57 - loss: 3.2979 - accuracy: 0.17 - ETA: 3:56 - loss: 3.2875 - accuracy: 0.17 - ETA: 3:57 - loss: 3.3059 - accuracy: 0.17 - ETA: 3:55 - loss: 3.3282 - accuracy: 0.15 - ETA: 3:57 - loss: 3.3338 - accuracy: 0.15 - ETA: 3:58 - loss: 3.3335 - accuracy: 0.15 - ETA: 3:58 - loss: 3.3316 - accuracy: 0.15 - ETA: 3:56 - loss: 3.3465 - accuracy: 0.14 - ETA: 3:57 - loss: 3.3576 - accuracy: 0.14 - ETA: 3:57 - loss: 3.3657 - accuracy: 0.14 - ETA: 3:56 - loss: 3.3781 - accuracy: 0.13 - ETA: 3:55 - loss: 3.3842 - accuracy: 0.13 - ETA: 3:54 - loss: 3.3903 - accuracy: 0.13 - ETA: 3:53 - loss: 3.3891 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3819 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3846 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3812 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3752 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3704 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3605 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3529 - accuracy: 0.13 - ETA: 3:48 - loss: 3.3465 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3496 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3533 - accuracy: 0.13 - ETA: 3:45 - loss: 3.3513 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3536 - accuracy: 0.14 - ETA: 3:43 - loss: 3.3511 - accuracy: 0.13 - ETA: 3:42 - loss: 3.3492 - accuracy: 0.14 - ETA: 3:41 - loss: 3.3488 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3472 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3473 - accuracy: 0.14 - ETA: 3:39 - loss: 3.3479 - accuracy: 0.14 - ETA: 3:38 - loss: 3.3438 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3423 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3372 - accuracy: 0.14 - ETA: 3:35 - loss: 3.3378 - accuracy: 0.14 - ETA: 3:34 - loss: 3.3363 - accuracy: 0.14 - ETA: 3:34 - loss: 3.3360 - accuracy: 0.14 - ETA: 3:33 - loss: 3.3400 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3406 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3403 - accuracy: 0.14 - ETA: 3:31 - loss: 3.3411 - accuracy: 0.14 - ETA: 3:30 - loss: 3.3407 - accuracy: 0.14 - ETA: 3:29 - loss: 3.3438 - accuracy: 0.14 - ETA: 3:29 - loss: 3.3428 - accuracy: 0.14 - ETA: 3:28 - loss: 3.3465 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3487 - accuracy: 0.13 - ETA: 3:27 - loss: 3.3468 - accuracy: 0.13 - ETA: 3:26 - loss: 3.3480 - accuracy: 0.13 - ETA: 3:25 - loss: 3.3458 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3447 - accuracy: 0.13 - ETA: 3:24 - loss: 3.3423 - accuracy: 0.14 - ETA: 3:23 - loss: 3.3438 - accuracy: 0.14 - ETA: 3:22 - loss: 3.3455 - accuracy: 0.14 - ETA: 3:21 - loss: 3.3455 - accuracy: 0.14 - ETA: 3:21 - loss: 3.3431 - accuracy: 0.14 - ETA: 3:20 - loss: 3.3428 - accuracy: 0.14 - ETA: 3:19 - loss: 3.3423 - accuracy: 0.14 - ETA: 3:18 - loss: 3.3407 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3405 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3397 - accuracy: 0.14 - ETA: 3:16 - loss: 3.3389 - accuracy: 0.14 - ETA: 3:15 - loss: 3.3407 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3391 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3405 - accuracy: 0.14 - ETA: 3:13 - loss: 3.3414 - accuracy: 0.14 - ETA: 3:12 - loss: 3.3411 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3399 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3416 - accuracy: 0.14 - ETA: 3:10 - loss: 3.3434 - accuracy: 0.14 - ETA: 3:09 - loss: 3.3419 - accuracy: 0.14 - ETA: 3:08 - loss: 3.3426 - accuracy: 0.14 - ETA: 3:08 - loss: 3.3437 - accuracy: 0.14 - ETA: 3:07 - loss: 3.3410 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3397 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3376 - accuracy: 0.14 - ETA: 3:05 - loss: 3.3384 - accuracy: 0.14 - ETA: 3:04 - loss: 3.3378 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3369 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3400 - accuracy: 0.14 - ETA: 3:02 - loss: 3.3400 - accuracy: 0.14 - ETA: 3:01 - loss: 3.3425 - accuracy: 0.14 - ETA: 3:00 - loss: 3.3469 - accuracy: 0.14 - ETA: 2:59 - loss: 3.3531 - accuracy: 0.14 - ETA: 2:59 - loss: 3.3567 - accuracy: 0.14 - ETA: 2:58 - loss: 3.3610 - accuracy: 0.14 - ETA: 2:57 - loss: 3.3622 - accuracy: 0.14 - ETA: 2:57 - loss: 3.3615 - accuracy: 0.14 - ETA: 2:56 - loss: 3.3609 - accuracy: 0.14 - ETA: 2:55 - loss: 3.3618 - accuracy: 0.14 - ETA: 2:54 - loss: 3.3632 - accuracy: 0.14 - ETA: 2:54 - loss: 3.3624 - accuracy: 0.14 - ETA: 2:53 - loss: 3.3627 - accuracy: 0.14 - ETA: 2:52 - loss: 3.3620 - accuracy: 0.14 - ETA: 2:51 - loss: 3.3635 - accuracy: 0.14 - ETA: 2:51 - loss: 3.3637 - accuracy: 0.14 - ETA: 2:50 - loss: 3.3634 - accuracy: 0.14 - ETA: 2:49 - loss: 3.3625 - accuracy: 0.14 - ETA: 2:49 - loss: 3.3633 - accuracy: 0.14 - ETA: 2:48 - loss: 3.3637 - accuracy: 0.14 - ETA: 2:47 - loss: 3.3628 - accuracy: 0.14 - ETA: 2:47 - loss: 3.3618 - accuracy: 0.14 - ETA: 2:46 - loss: 3.3611 - accuracy: 0.14 - ETA: 2:45 - loss: 3.3600 - accuracy: 0.14 - ETA: 2:45 - loss: 3.3599 - accuracy: 0.14 - ETA: 2:44 - loss: 3.3600 - accuracy: 0.14 - ETA: 2:43 - loss: 3.3593 - accuracy: 0.14 - ETA: 2:42 - loss: 3.3585 - accuracy: 0.14 - ETA: 2:41 - loss: 3.3591 - accuracy: 0.13 - ETA: 2:41 - loss: 3.3588 - accuracy: 0.13 - ETA: 2:40 - loss: 3.3587 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3585 - accuracy: 0.13 - ETA: 2:39 - loss: 3.3579 - accuracy: 0.13 - ETA: 2:38 - loss: 3.3591 - accuracy: 0.13 - ETA: 2:37 - loss: 3.3607 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3608 - accuracy: 0.13 - ETA: 2:36 - loss: 3.3610 - accuracy: 0.13 - ETA: 2:35 - loss: 3.3625 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3632 - accuracy: 0.13 - ETA: 2:34 - loss: 3.3626 - accuracy: 0.13 - ETA: 2:33 - loss: 3.3635 - accuracy: 0.13 - ETA: 2:32 - loss: 3.3641 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3635 - accuracy: 0.13 - ETA: 2:31 - loss: 3.3637 - accuracy: 0.13 - ETA: 2:30 - loss: 3.3632 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3643 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3644 - accuracy: 0.13 - ETA: 2:29 - loss: 3.3639 - accuracy: 0.13 - ETA: 2:28 - loss: 3.3629 - accuracy: 0.13 - ETA: 2:27 - loss: 3.3625 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3632 - accuracy: 0.13 - ETA: 2:26 - loss: 3.3637 - accuracy: 0.13 - ETA: 2:25 - loss: 3.3758 - accuracy: 0.13 - ETA: 2:24 - loss: 3.3762 - accuracy: 0.13 - ETA: 2:23 - loss: 3.3764 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3771 - accuracy: 0.13 - ETA: 2:22 - loss: 3.3766 - accuracy: 0.13 - ETA: 2:21 - loss: 3.3751 - accuracy: 0.13 - ETA: 2:20 - loss: 3.3751 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3756 - accuracy: 0.13 - ETA: 2:19 - loss: 3.3760 - accuracy: 0.13 - ETA: 2:18 - loss: 3.3758 - accuracy: 0.13 - ETA: 2:17 - loss: 3.3749 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3759 - accuracy: 0.13 - ETA: 2:16 - loss: 3.3763 - accuracy: 0.13 - ETA: 2:15 - loss: 3.3747 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3753 - accuracy: 0.13 - ETA: 2:14 - loss: 3.3747 - accuracy: 0.13 - ETA: 2:13 - loss: 3.3747 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3741 - accuracy: 0.13 - ETA: 2:12 - loss: 3.3730 - accuracy: 0.13 - ETA: 2:11 - loss: 3.3727 - accuracy: 0.13 - ETA: 2:10 - loss: 3.3727 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3715 - accuracy: 0.13 - ETA: 2:09 - loss: 3.3713 - accuracy: 0.13 - ETA: 2:08 - loss: 3.3718 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3719 - accuracy: 0.13 - ETA: 2:07 - loss: 3.3732 - accuracy: 0.13 - ETA: 2:06 - loss: 3.3735 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3742 - accuracy: 0.13 - ETA: 2:05 - loss: 3.3729 - accuracy: 0.13 - ETA: 2:04 - loss: 3.3732 - accuracy: 0.13 - ETA: 2:03 - loss: 3.3729 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3713 - accuracy: 0.13 - ETA: 2:02 - loss: 3.3696 - accuracy: 0.13 - ETA: 2:01 - loss: 3.3695 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3692 - accuracy: 0.13 - ETA: 2:00 - loss: 3.3687 - accuracy: 0.13 - ETA: 1:59 - loss: 3.3733 - accuracy: 0.13 - ETA: 1:58 - loss: 3.3729 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3728 - accuracy: 0.13 - ETA: 1:57 - loss: 3.3722 - accuracy: 0.13 - ETA: 1:56 - loss: 3.3728 - accuracy: 0.13 - ETA: 1:55 - loss: 3.3734 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3732 - accuracy: 0.13 - ETA: 1:54 - loss: 3.3728 - accuracy: 0.13 - ETA: 1:53 - loss: 3.3734 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3759 - accuracy: 0.13 - ETA: 1:52 - loss: 3.3758 - accuracy: 0.13 - ETA: 1:51 - loss: 3.3757 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3766 - accuracy: 0.13 - ETA: 1:50 - loss: 3.3764 - accuracy: 0.13 - ETA: 1:49 - loss: 3.3757 - accuracy: 0.13 - ETA: 1:48 - loss: 3.3754 - accuracy: 0.1387"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:47 - loss: 3.3755 - accuracy: 0.13 - ETA: 1:47 - loss: 3.3755 - accuracy: 0.13 - ETA: 1:46 - loss: 3.3760 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3763 - accuracy: 0.13 - ETA: 1:45 - loss: 3.3771 - accuracy: 0.13 - ETA: 1:44 - loss: 3.3767 - accuracy: 0.13 - ETA: 1:43 - loss: 3.3769 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3775 - accuracy: 0.13 - ETA: 1:42 - loss: 3.3772 - accuracy: 0.13 - ETA: 1:41 - loss: 3.3773 - accuracy: 0.13 - ETA: 1:40 - loss: 3.3780 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3786 - accuracy: 0.13 - ETA: 1:39 - loss: 3.3779 - accuracy: 0.13 - ETA: 1:38 - loss: 3.3779 - accuracy: 0.13 - ETA: 1:37 - loss: 3.3778 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3769 - accuracy: 0.13 - ETA: 1:36 - loss: 3.3760 - accuracy: 0.13 - ETA: 1:35 - loss: 3.3748 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3750 - accuracy: 0.13 - ETA: 1:34 - loss: 3.3747 - accuracy: 0.13 - ETA: 1:33 - loss: 3.3749 - accuracy: 0.13 - ETA: 1:32 - loss: 3.3753 - accuracy: 0.13 - ETA: 1:31 - loss: 3.3748 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3754 - accuracy: 0.13 - ETA: 1:30 - loss: 3.3752 - accuracy: 0.13 - ETA: 1:29 - loss: 3.3756 - accuracy: 0.13 - ETA: 1:28 - loss: 3.3761 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3757 - accuracy: 0.13 - ETA: 1:27 - loss: 3.3752 - accuracy: 0.13 - ETA: 1:26 - loss: 3.3758 - accuracy: 0.13 - ETA: 1:25 - loss: 3.3752 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3753 - accuracy: 0.13 - ETA: 1:24 - loss: 3.3750 - accuracy: 0.13 - ETA: 1:23 - loss: 3.3744 - accuracy: 0.13 - ETA: 1:22 - loss: 3.3742 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3742 - accuracy: 0.13 - ETA: 1:21 - loss: 3.3742 - accuracy: 0.13 - ETA: 1:20 - loss: 3.3742 - accuracy: 0.13 - ETA: 1:19 - loss: 3.3746 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3749 - accuracy: 0.13 - ETA: 1:18 - loss: 3.3747 - accuracy: 0.13 - ETA: 1:17 - loss: 3.3745 - accuracy: 0.13 - ETA: 1:16 - loss: 3.3746 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3745 - accuracy: 0.13 - ETA: 1:15 - loss: 3.3745 - accuracy: 0.13 - ETA: 1:14 - loss: 3.3743 - accuracy: 0.13 - ETA: 1:13 - loss: 3.3743 - accuracy: 0.13 - ETA: 1:12 - loss: 3.3738 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3735 - accuracy: 0.13 - ETA: 1:11 - loss: 3.3718 - accuracy: 0.13 - ETA: 1:10 - loss: 3.3715 - accuracy: 0.13 - ETA: 1:09 - loss: 3.3725 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3727 - accuracy: 0.13 - ETA: 1:08 - loss: 3.3728 - accuracy: 0.13 - ETA: 1:07 - loss: 3.3718 - accuracy: 0.13 - ETA: 1:06 - loss: 3.3717 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3719 - accuracy: 0.13 - ETA: 1:05 - loss: 3.3713 - accuracy: 0.13 - ETA: 1:04 - loss: 3.3712 - accuracy: 0.13 - ETA: 1:03 - loss: 3.3709 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3715 - accuracy: 0.13 - ETA: 1:02 - loss: 3.3711 - accuracy: 0.13 - ETA: 1:01 - loss: 3.3718 - accuracy: 0.13 - ETA: 1:00 - loss: 3.3703 - accuracy: 0.13 - ETA: 59s - loss: 3.3703 - accuracy: 0.1382 - ETA: 59s - loss: 3.3708 - accuracy: 0.138 - ETA: 58s - loss: 3.3706 - accuracy: 0.138 - ETA: 57s - loss: 3.3702 - accuracy: 0.138 - ETA: 56s - loss: 3.3704 - accuracy: 0.138 - ETA: 56s - loss: 3.3703 - accuracy: 0.138 - ETA: 55s - loss: 3.3708 - accuracy: 0.138 - ETA: 54s - loss: 3.3705 - accuracy: 0.138 - ETA: 53s - loss: 3.3708 - accuracy: 0.138 - ETA: 53s - loss: 3.3712 - accuracy: 0.138 - ETA: 52s - loss: 3.3707 - accuracy: 0.138 - ETA: 51s - loss: 3.3710 - accuracy: 0.137 - ETA: 50s - loss: 3.3703 - accuracy: 0.138 - ETA: 50s - loss: 3.3703 - accuracy: 0.137 - ETA: 49s - loss: 3.3711 - accuracy: 0.137 - ETA: 48s - loss: 3.3707 - accuracy: 0.137 - ETA: 47s - loss: 3.3700 - accuracy: 0.138 - ETA: 47s - loss: 3.3698 - accuracy: 0.137 - ETA: 46s - loss: 3.3703 - accuracy: 0.137 - ETA: 45s - loss: 3.3705 - accuracy: 0.137 - ETA: 44s - loss: 3.3703 - accuracy: 0.137 - ETA: 44s - loss: 3.3701 - accuracy: 0.137 - ETA: 43s - loss: 3.3701 - accuracy: 0.137 - ETA: 42s - loss: 3.3698 - accuracy: 0.137 - ETA: 41s - loss: 3.3692 - accuracy: 0.137 - ETA: 41s - loss: 3.3681 - accuracy: 0.137 - ETA: 40s - loss: 3.3681 - accuracy: 0.137 - ETA: 39s - loss: 3.3685 - accuracy: 0.138 - ETA: 38s - loss: 3.3682 - accuracy: 0.138 - ETA: 38s - loss: 3.3683 - accuracy: 0.138 - ETA: 37s - loss: 3.3680 - accuracy: 0.138 - ETA: 36s - loss: 3.3674 - accuracy: 0.138 - ETA: 35s - loss: 3.3670 - accuracy: 0.138 - ETA: 35s - loss: 3.3660 - accuracy: 0.138 - ETA: 34s - loss: 3.3656 - accuracy: 0.138 - ETA: 33s - loss: 3.3654 - accuracy: 0.138 - ETA: 32s - loss: 3.3654 - accuracy: 0.138 - ETA: 32s - loss: 3.3648 - accuracy: 0.138 - ETA: 31s - loss: 3.3649 - accuracy: 0.138 - ETA: 30s - loss: 3.3644 - accuracy: 0.138 - ETA: 29s - loss: 3.3637 - accuracy: 0.138 - ETA: 29s - loss: 3.3635 - accuracy: 0.138 - ETA: 28s - loss: 3.3623 - accuracy: 0.139 - ETA: 27s - loss: 3.3627 - accuracy: 0.138 - ETA: 26s - loss: 3.3625 - accuracy: 0.139 - ETA: 26s - loss: 3.3625 - accuracy: 0.139 - ETA: 25s - loss: 3.3623 - accuracy: 0.138 - ETA: 24s - loss: 3.3632 - accuracy: 0.138 - ETA: 23s - loss: 3.3636 - accuracy: 0.138 - ETA: 23s - loss: 3.3628 - accuracy: 0.138 - ETA: 22s - loss: 3.3628 - accuracy: 0.138 - ETA: 21s - loss: 3.3625 - accuracy: 0.138 - ETA: 20s - loss: 3.3625 - accuracy: 0.138 - ETA: 20s - loss: 3.3617 - accuracy: 0.138 - ETA: 19s - loss: 3.3615 - accuracy: 0.138 - ETA: 18s - loss: 3.3614 - accuracy: 0.138 - ETA: 17s - loss: 3.3615 - accuracy: 0.138 - ETA: 17s - loss: 3.3609 - accuracy: 0.138 - ETA: 16s - loss: 3.3613 - accuracy: 0.138 - ETA: 15s - loss: 3.3604 - accuracy: 0.139 - ETA: 14s - loss: 3.3603 - accuracy: 0.138 - ETA: 14s - loss: 3.3601 - accuracy: 0.139 - ETA: 13s - loss: 3.3601 - accuracy: 0.139 - ETA: 12s - loss: 3.3605 - accuracy: 0.138 - ETA: 11s - loss: 3.3604 - accuracy: 0.139 - ETA: 11s - loss: 3.3609 - accuracy: 0.138 - ETA: 10s - loss: 3.3608 - accuracy: 0.138 - ETA: 9s - loss: 3.3601 - accuracy: 0.139 - ETA: 8s - loss: 3.3604 - accuracy: 0.13 - ETA: 8s - loss: 3.3605 - accuracy: 0.13 - ETA: 7s - loss: 3.3599 - accuracy: 0.13 - ETA: 6s - loss: 3.3598 - accuracy: 0.13 - ETA: 6s - loss: 3.3599 - accuracy: 0.13 - ETA: 5s - loss: 3.3592 - accuracy: 0.13 - ETA: 4s - loss: 3.3593 - accuracy: 0.13 - ETA: 3s - loss: 3.3597 - accuracy: 0.13 - ETA: 3s - loss: 3.3596 - accuracy: 0.13 - ETA: 2s - loss: 3.3593 - accuracy: 0.13 - ETA: 1s - loss: 3.3586 - accuracy: 0.13 - ETA: 0s - loss: 3.3582 - accuracy: 0.13 - ETA: 0s - loss: 3.3578 - accuracy: 0.13 - 265s 6ms/step - loss: 3.3578 - accuracy: 0.1393 - val_loss: 4.0599 - val_accuracy: 0.0232\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23808/42378 [===============>..............] - ETA: 4:04 - loss: 3.1292 - accuracy: 0.17 - ETA: 4:00 - loss: 3.2709 - accuracy: 0.16 - ETA: 4:01 - loss: 3.2171 - accuracy: 0.16 - ETA: 4:00 - loss: 3.2431 - accuracy: 0.15 - ETA: 4:03 - loss: 3.2375 - accuracy: 0.16 - ETA: 4:03 - loss: 3.2475 - accuracy: 0.16 - ETA: 4:03 - loss: 3.2819 - accuracy: 0.15 - ETA: 4:03 - loss: 3.2716 - accuracy: 0.16 - ETA: 4:01 - loss: 3.3022 - accuracy: 0.15 - ETA: 4:00 - loss: 3.3070 - accuracy: 0.15 - ETA: 3:59 - loss: 3.3302 - accuracy: 0.14 - ETA: 3:57 - loss: 3.3292 - accuracy: 0.14 - ETA: 3:55 - loss: 3.3316 - accuracy: 0.14 - ETA: 3:54 - loss: 3.3363 - accuracy: 0.13 - ETA: 3:53 - loss: 3.3421 - accuracy: 0.13 - ETA: 3:52 - loss: 3.3426 - accuracy: 0.13 - ETA: 3:51 - loss: 3.3382 - accuracy: 0.13 - ETA: 3:50 - loss: 3.3389 - accuracy: 0.13 - ETA: 3:49 - loss: 3.3230 - accuracy: 0.14 - ETA: 3:48 - loss: 3.3278 - accuracy: 0.14 - ETA: 3:47 - loss: 3.3223 - accuracy: 0.14 - ETA: 3:46 - loss: 3.3153 - accuracy: 0.14 - ETA: 3:45 - loss: 3.3197 - accuracy: 0.14 - ETA: 3:44 - loss: 3.3114 - accuracy: 0.14 - ETA: 3:43 - loss: 3.3122 - accuracy: 0.14 - ETA: 3:43 - loss: 3.3182 - accuracy: 0.14 - ETA: 3:42 - loss: 3.3209 - accuracy: 0.14 - ETA: 3:42 - loss: 3.3161 - accuracy: 0.14 - ETA: 3:41 - loss: 3.3133 - accuracy: 0.14 - ETA: 3:40 - loss: 3.3092 - accuracy: 0.14 - ETA: 3:39 - loss: 3.3041 - accuracy: 0.14 - ETA: 3:39 - loss: 3.3031 - accuracy: 0.14 - ETA: 3:38 - loss: 3.3013 - accuracy: 0.14 - ETA: 3:38 - loss: 3.2975 - accuracy: 0.14 - ETA: 3:37 - loss: 3.3008 - accuracy: 0.14 - ETA: 3:36 - loss: 3.3017 - accuracy: 0.14 - ETA: 3:35 - loss: 3.3050 - accuracy: 0.14 - ETA: 3:34 - loss: 3.3060 - accuracy: 0.14 - ETA: 3:33 - loss: 3.3065 - accuracy: 0.14 - ETA: 3:33 - loss: 3.3022 - accuracy: 0.14 - ETA: 3:32 - loss: 3.3033 - accuracy: 0.14 - ETA: 3:31 - loss: 3.3037 - accuracy: 0.14 - ETA: 3:30 - loss: 3.3085 - accuracy: 0.14 - ETA: 3:30 - loss: 3.3083 - accuracy: 0.14 - ETA: 3:29 - loss: 3.3109 - accuracy: 0.14 - ETA: 3:28 - loss: 3.3108 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3097 - accuracy: 0.14 - ETA: 3:27 - loss: 3.3152 - accuracy: 0.14 - ETA: 3:26 - loss: 3.3129 - accuracy: 0.14 - ETA: 3:25 - loss: 3.3105 - accuracy: 0.14 - ETA: 3:24 - loss: 3.3122 - accuracy: 0.14 - ETA: 3:23 - loss: 3.3112 - accuracy: 0.14 - ETA: 3:23 - loss: 3.3110 - accuracy: 0.14 - ETA: 3:22 - loss: 3.3123 - accuracy: 0.14 - ETA: 3:21 - loss: 3.3109 - accuracy: 0.14 - ETA: 3:20 - loss: 3.3100 - accuracy: 0.14 - ETA: 3:20 - loss: 3.3137 - accuracy: 0.14 - ETA: 3:19 - loss: 3.3197 - accuracy: 0.14 - ETA: 3:18 - loss: 3.3204 - accuracy: 0.14 - ETA: 3:17 - loss: 3.3187 - accuracy: 0.14 - ETA: 3:16 - loss: 3.3169 - accuracy: 0.14 - ETA: 3:16 - loss: 3.3174 - accuracy: 0.14 - ETA: 3:15 - loss: 3.3145 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3144 - accuracy: 0.14 - ETA: 3:14 - loss: 3.3179 - accuracy: 0.14 - ETA: 3:13 - loss: 3.3160 - accuracy: 0.14 - ETA: 3:12 - loss: 3.3184 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3175 - accuracy: 0.14 - ETA: 3:11 - loss: 3.3158 - accuracy: 0.14 - ETA: 3:10 - loss: 3.3149 - accuracy: 0.14 - ETA: 3:09 - loss: 3.3167 - accuracy: 0.14 - ETA: 3:09 - loss: 3.3154 - accuracy: 0.14 - ETA: 3:08 - loss: 3.3145 - accuracy: 0.14 - ETA: 3:07 - loss: 3.3138 - accuracy: 0.14 - ETA: 3:07 - loss: 3.3139 - accuracy: 0.14 - ETA: 3:06 - loss: 3.3141 - accuracy: 0.14 - ETA: 3:05 - loss: 3.3137 - accuracy: 0.14 - ETA: 3:05 - loss: 3.3144 - accuracy: 0.14 - ETA: 3:04 - loss: 3.3147 - accuracy: 0.14 - ETA: 3:04 - loss: 3.3138 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3175 - accuracy: 0.14 - ETA: 3:03 - loss: 3.3133 - accuracy: 0.14 - ETA: 3:02 - loss: 3.3117 - accuracy: 0.14 - ETA: 3:01 - loss: 3.3114 - accuracy: 0.14 - ETA: 3:00 - loss: 3.3110 - accuracy: 0.14 - ETA: 3:00 - loss: 3.3135 - accuracy: 0.14 - ETA: 2:59 - loss: 3.3148 - accuracy: 0.14 - ETA: 2:58 - loss: 3.3152 - accuracy: 0.14 - ETA: 2:58 - loss: 3.3163 - accuracy: 0.14 - ETA: 2:57 - loss: 3.3147 - accuracy: 0.14 - ETA: 2:56 - loss: 3.3159 - accuracy: 0.14 - ETA: 2:55 - loss: 3.3167 - accuracy: 0.14 - ETA: 2:55 - loss: 3.3179 - accuracy: 0.14 - ETA: 2:54 - loss: 3.3177 - accuracy: 0.14 - ETA: 2:54 - loss: 3.3193 - accuracy: 0.14 - ETA: 2:53 - loss: 3.3186 - accuracy: 0.14 - ETA: 2:52 - loss: 3.3187 - accuracy: 0.14 - ETA: 2:52 - loss: 3.3178 - accuracy: 0.14 - ETA: 2:51 - loss: 3.3192 - accuracy: 0.14 - ETA: 2:50 - loss: 3.3191 - accuracy: 0.14 - ETA: 2:49 - loss: 3.3171 - accuracy: 0.14 - ETA: 2:49 - loss: 3.3174 - accuracy: 0.14 - ETA: 2:48 - loss: 3.3180 - accuracy: 0.14 - ETA: 2:47 - loss: 3.3176 - accuracy: 0.14 - ETA: 2:47 - loss: 3.3182 - accuracy: 0.14 - ETA: 2:46 - loss: 3.3189 - accuracy: 0.14 - ETA: 2:46 - loss: 3.3195 - accuracy: 0.14 - ETA: 2:45 - loss: 3.3199 - accuracy: 0.14 - ETA: 2:44 - loss: 3.3207 - accuracy: 0.14 - ETA: 2:43 - loss: 3.3205 - accuracy: 0.14 - ETA: 2:43 - loss: 3.3211 - accuracy: 0.14 - ETA: 2:42 - loss: 3.3198 - accuracy: 0.14 - ETA: 2:41 - loss: 3.3207 - accuracy: 0.14 - ETA: 2:41 - loss: 3.3189 - accuracy: 0.14 - ETA: 2:40 - loss: 3.3182 - accuracy: 0.14 - ETA: 2:39 - loss: 3.3176 - accuracy: 0.14 - ETA: 2:38 - loss: 3.3186 - accuracy: 0.14 - ETA: 2:38 - loss: 3.3178 - accuracy: 0.14 - ETA: 2:37 - loss: 3.3154 - accuracy: 0.14 - ETA: 2:36 - loss: 3.3167 - accuracy: 0.14 - ETA: 2:36 - loss: 3.3164 - accuracy: 0.14 - ETA: 2:35 - loss: 3.3148 - accuracy: 0.14 - ETA: 2:34 - loss: 3.3148 - accuracy: 0.14 - ETA: 2:33 - loss: 3.3139 - accuracy: 0.14 - ETA: 2:33 - loss: 3.3142 - accuracy: 0.14 - ETA: 2:32 - loss: 3.3144 - accuracy: 0.14 - ETA: 2:31 - loss: 3.3150 - accuracy: 0.14 - ETA: 2:31 - loss: 3.3146 - accuracy: 0.14 - ETA: 2:30 - loss: 3.3140 - accuracy: 0.14 - ETA: 2:29 - loss: 3.3154 - accuracy: 0.14 - ETA: 2:28 - loss: 3.3154 - accuracy: 0.14 - ETA: 2:28 - loss: 3.3157 - accuracy: 0.14 - ETA: 2:27 - loss: 3.3163 - accuracy: 0.14 - ETA: 2:27 - loss: 3.3171 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3166 - accuracy: 0.14 - ETA: 2:26 - loss: 3.3155 - accuracy: 0.14 - ETA: 2:25 - loss: 3.3163 - accuracy: 0.14 - ETA: 2:24 - loss: 3.3166 - accuracy: 0.14 - ETA: 2:23 - loss: 3.3156 - accuracy: 0.14 - ETA: 2:22 - loss: 3.3157 - accuracy: 0.14 - ETA: 2:21 - loss: 3.3161 - accuracy: 0.14 - ETA: 2:20 - loss: 3.3167 - accuracy: 0.14 - ETA: 2:19 - loss: 3.3174 - accuracy: 0.14 - ETA: 2:17 - loss: 3.3165 - accuracy: 0.14 - ETA: 2:16 - loss: 3.3160 - accuracy: 0.14 - ETA: 2:15 - loss: 3.3144 - accuracy: 0.14 - ETA: 2:14 - loss: 3.3141 - accuracy: 0.14 - ETA: 2:13 - loss: 3.3144 - accuracy: 0.14 - ETA: 2:12 - loss: 3.3147 - accuracy: 0.14 - ETA: 2:11 - loss: 3.3141 - accuracy: 0.14 - ETA: 2:10 - loss: 3.3141 - accuracy: 0.14 - ETA: 2:09 - loss: 3.3148 - accuracy: 0.14 - ETA: 2:08 - loss: 3.3168 - accuracy: 0.14 - ETA: 2:07 - loss: 3.3198 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3191 - accuracy: 0.14 - ETA: 2:06 - loss: 3.3228 - accuracy: 0.14 - ETA: 2:05 - loss: 3.3226 - accuracy: 0.14 - ETA: 2:04 - loss: 3.3225 - accuracy: 0.14 - ETA: 2:03 - loss: 3.3230 - accuracy: 0.14 - ETA: 2:02 - loss: 3.3241 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3247 - accuracy: 0.14 - ETA: 2:01 - loss: 3.3240 - accuracy: 0.14 - ETA: 2:00 - loss: 3.3227 - accuracy: 0.14 - ETA: 1:59 - loss: 3.3219 - accuracy: 0.14 - ETA: 1:58 - loss: 3.3214 - accuracy: 0.14 - ETA: 1:57 - loss: 3.3214 - accuracy: 0.14 - ETA: 1:56 - loss: 3.3217 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3226 - accuracy: 0.14 - ETA: 1:55 - loss: 3.3216 - accuracy: 0.14 - ETA: 1:54 - loss: 3.3204 - accuracy: 0.14 - ETA: 1:53 - loss: 3.3201 - accuracy: 0.14 - ETA: 1:52 - loss: 3.3202 - accuracy: 0.14 - ETA: 1:51 - loss: 3.3201 - accuracy: 0.14 - ETA: 1:50 - loss: 3.3194 - accuracy: 0.14 - ETA: 1:49 - loss: 3.3200 - accuracy: 0.14 - ETA: 1:48 - loss: 3.3200 - accuracy: 0.14 - ETA: 1:47 - loss: 3.3205 - accuracy: 0.14 - ETA: 1:46 - loss: 3.3203 - accuracy: 0.14 - ETA: 1:45 - loss: 3.3196 - accuracy: 0.14 - ETA: 1:44 - loss: 3.3182 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3182 - accuracy: 0.14 - ETA: 1:43 - loss: 3.3183 - accuracy: 0.14 - ETA: 1:42 - loss: 3.3175 - accuracy: 0.14 - ETA: 1:41 - loss: 3.3177 - accuracy: 0.14 - ETA: 1:40 - loss: 3.3174 - accuracy: 0.14 - ETA: 1:39 - loss: 3.3173 - accuracy: 0.1437"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42378/42378 [==============================] - ETA: 1:38 - loss: 3.3170 - accuracy: 0.14 - ETA: 1:38 - loss: 3.3161 - accuracy: 0.14 - ETA: 1:37 - loss: 3.3163 - accuracy: 0.14 - ETA: 1:36 - loss: 3.3156 - accuracy: 0.14 - ETA: 1:35 - loss: 3.3148 - accuracy: 0.14 - ETA: 1:34 - loss: 3.3157 - accuracy: 0.14 - ETA: 1:33 - loss: 3.3152 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3148 - accuracy: 0.14 - ETA: 1:32 - loss: 3.3144 - accuracy: 0.14 - ETA: 1:31 - loss: 3.3137 - accuracy: 0.14 - ETA: 1:30 - loss: 3.3130 - accuracy: 0.14 - ETA: 1:29 - loss: 3.3124 - accuracy: 0.14 - ETA: 1:28 - loss: 3.3129 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3127 - accuracy: 0.14 - ETA: 1:27 - loss: 3.3130 - accuracy: 0.14 - ETA: 1:26 - loss: 3.3125 - accuracy: 0.14 - ETA: 1:25 - loss: 3.3127 - accuracy: 0.14 - ETA: 1:24 - loss: 3.3135 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3140 - accuracy: 0.14 - ETA: 1:23 - loss: 3.3153 - accuracy: 0.14 - ETA: 1:22 - loss: 3.3157 - accuracy: 0.14 - ETA: 1:21 - loss: 3.3150 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3155 - accuracy: 0.14 - ETA: 1:20 - loss: 3.3159 - accuracy: 0.14 - ETA: 1:19 - loss: 3.3149 - accuracy: 0.14 - ETA: 1:18 - loss: 3.3144 - accuracy: 0.14 - ETA: 1:17 - loss: 3.3145 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3153 - accuracy: 0.14 - ETA: 1:16 - loss: 3.3153 - accuracy: 0.14 - ETA: 1:15 - loss: 3.3149 - accuracy: 0.14 - ETA: 1:14 - loss: 3.3138 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3130 - accuracy: 0.14 - ETA: 1:13 - loss: 3.3127 - accuracy: 0.14 - ETA: 1:12 - loss: 3.3130 - accuracy: 0.14 - ETA: 1:11 - loss: 3.3127 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3126 - accuracy: 0.14 - ETA: 1:10 - loss: 3.3110 - accuracy: 0.14 - ETA: 1:09 - loss: 3.3116 - accuracy: 0.14 - ETA: 1:08 - loss: 3.3100 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3090 - accuracy: 0.14 - ETA: 1:07 - loss: 3.3090 - accuracy: 0.14 - ETA: 1:06 - loss: 3.3084 - accuracy: 0.14 - ETA: 1:05 - loss: 3.3079 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3081 - accuracy: 0.14 - ETA: 1:04 - loss: 3.3079 - accuracy: 0.14 - ETA: 1:03 - loss: 3.3082 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3072 - accuracy: 0.14 - ETA: 1:02 - loss: 3.3069 - accuracy: 0.14 - ETA: 1:01 - loss: 3.3070 - accuracy: 0.14 - ETA: 1:00 - loss: 3.3065 - accuracy: 0.14 - ETA: 59s - loss: 3.3056 - accuracy: 0.1457 - ETA: 59s - loss: 3.3054 - accuracy: 0.145 - ETA: 58s - loss: 3.3052 - accuracy: 0.145 - ETA: 57s - loss: 3.3043 - accuracy: 0.146 - ETA: 57s - loss: 3.3036 - accuracy: 0.146 - ETA: 56s - loss: 3.3039 - accuracy: 0.146 - ETA: 55s - loss: 3.3041 - accuracy: 0.146 - ETA: 54s - loss: 3.3039 - accuracy: 0.146 - ETA: 54s - loss: 3.3042 - accuracy: 0.146 - ETA: 53s - loss: 3.3034 - accuracy: 0.146 - ETA: 52s - loss: 3.3028 - accuracy: 0.146 - ETA: 52s - loss: 3.3035 - accuracy: 0.146 - ETA: 51s - loss: 3.3038 - accuracy: 0.146 - ETA: 50s - loss: 3.3028 - accuracy: 0.146 - ETA: 50s - loss: 3.3023 - accuracy: 0.146 - ETA: 49s - loss: 3.3027 - accuracy: 0.146 - ETA: 48s - loss: 3.3019 - accuracy: 0.146 - ETA: 48s - loss: 3.3013 - accuracy: 0.146 - ETA: 47s - loss: 3.3007 - accuracy: 0.146 - ETA: 46s - loss: 3.3005 - accuracy: 0.146 - ETA: 46s - loss: 3.3006 - accuracy: 0.146 - ETA: 45s - loss: 3.3002 - accuracy: 0.146 - ETA: 44s - loss: 3.2994 - accuracy: 0.146 - ETA: 44s - loss: 3.2992 - accuracy: 0.147 - ETA: 43s - loss: 3.2989 - accuracy: 0.147 - ETA: 42s - loss: 3.2990 - accuracy: 0.147 - ETA: 42s - loss: 3.2991 - accuracy: 0.147 - ETA: 41s - loss: 3.2982 - accuracy: 0.147 - ETA: 40s - loss: 3.2983 - accuracy: 0.147 - ETA: 40s - loss: 3.2975 - accuracy: 0.147 - ETA: 39s - loss: 3.2979 - accuracy: 0.147 - ETA: 38s - loss: 3.2979 - accuracy: 0.147 - ETA: 38s - loss: 3.2974 - accuracy: 0.147 - ETA: 37s - loss: 3.2969 - accuracy: 0.147 - ETA: 36s - loss: 3.2963 - accuracy: 0.147 - ETA: 36s - loss: 3.2962 - accuracy: 0.147 - ETA: 35s - loss: 3.2959 - accuracy: 0.147 - ETA: 34s - loss: 3.2949 - accuracy: 0.148 - ETA: 34s - loss: 3.2945 - accuracy: 0.148 - ETA: 33s - loss: 3.2945 - accuracy: 0.148 - ETA: 32s - loss: 3.2944 - accuracy: 0.148 - ETA: 32s - loss: 3.2948 - accuracy: 0.148 - ETA: 31s - loss: 3.2944 - accuracy: 0.148 - ETA: 31s - loss: 3.2937 - accuracy: 0.148 - ETA: 30s - loss: 3.2936 - accuracy: 0.148 - ETA: 29s - loss: 3.2930 - accuracy: 0.148 - ETA: 29s - loss: 3.2930 - accuracy: 0.148 - ETA: 28s - loss: 3.2926 - accuracy: 0.148 - ETA: 27s - loss: 3.2920 - accuracy: 0.148 - ETA: 27s - loss: 3.2916 - accuracy: 0.148 - ETA: 26s - loss: 3.2909 - accuracy: 0.148 - ETA: 25s - loss: 3.2909 - accuracy: 0.148 - ETA: 25s - loss: 3.2906 - accuracy: 0.148 - ETA: 24s - loss: 3.2909 - accuracy: 0.148 - ETA: 24s - loss: 3.2906 - accuracy: 0.148 - ETA: 23s - loss: 3.2910 - accuracy: 0.148 - ETA: 22s - loss: 3.2913 - accuracy: 0.148 - ETA: 22s - loss: 3.2906 - accuracy: 0.148 - ETA: 21s - loss: 3.2908 - accuracy: 0.148 - ETA: 21s - loss: 3.2908 - accuracy: 0.148 - ETA: 20s - loss: 3.2908 - accuracy: 0.148 - ETA: 19s - loss: 3.2900 - accuracy: 0.148 - ETA: 19s - loss: 3.2903 - accuracy: 0.148 - ETA: 18s - loss: 3.2891 - accuracy: 0.148 - ETA: 18s - loss: 3.2894 - accuracy: 0.148 - ETA: 17s - loss: 3.2896 - accuracy: 0.148 - ETA: 16s - loss: 3.2889 - accuracy: 0.148 - ETA: 16s - loss: 3.2888 - accuracy: 0.149 - ETA: 15s - loss: 3.2886 - accuracy: 0.149 - ETA: 14s - loss: 3.2898 - accuracy: 0.149 - ETA: 14s - loss: 3.2896 - accuracy: 0.149 - ETA: 13s - loss: 3.2890 - accuracy: 0.149 - ETA: 13s - loss: 3.2895 - accuracy: 0.149 - ETA: 12s - loss: 3.2897 - accuracy: 0.149 - ETA: 11s - loss: 3.2892 - accuracy: 0.149 - ETA: 11s - loss: 3.2892 - accuracy: 0.149 - ETA: 10s - loss: 3.2898 - accuracy: 0.149 - ETA: 10s - loss: 3.2901 - accuracy: 0.149 - ETA: 9s - loss: 3.2897 - accuracy: 0.149 - ETA: 8s - loss: 3.2898 - accuracy: 0.14 - ETA: 8s - loss: 3.2896 - accuracy: 0.14 - ETA: 7s - loss: 3.2889 - accuracy: 0.14 - ETA: 7s - loss: 3.2885 - accuracy: 0.14 - ETA: 6s - loss: 3.2880 - accuracy: 0.14 - ETA: 5s - loss: 3.2877 - accuracy: 0.14 - ETA: 5s - loss: 3.2878 - accuracy: 0.14 - ETA: 4s - loss: 3.2878 - accuracy: 0.14 - ETA: 4s - loss: 3.2878 - accuracy: 0.14 - ETA: 3s - loss: 3.2872 - accuracy: 0.14 - ETA: 3s - loss: 3.2868 - accuracy: 0.14 - ETA: 2s - loss: 3.2870 - accuracy: 0.14 - ETA: 1s - loss: 3.2868 - accuracy: 0.14 - ETA: 1s - loss: 3.2866 - accuracy: 0.14 - ETA: 0s - loss: 3.2861 - accuracy: 0.14 - ETA: 0s - loss: 3.2860 - accuracy: 0.14 - 206s 5ms/step - loss: 3.2861 - accuracy: 0.1497 - val_loss: 4.0619 - val_accuracy: 0.0241\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "history=model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[mcp_save], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Pickle/InceptionResNetV2_history_OF.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model as a pickle in a file \n",
    "joblib.dump(history, '../Pickle/InceptionResNetV2_history_OF.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file \n",
    "history = joblib.load('../Pickle/InceptionResNetV2_history_OF.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [3.930078220139928, 3.946676676400369, 3.954065845538903, 3.939247364186975, 3.9399430535671915, 3.915038529328564, 3.916283665306796, 3.920334899857493, 3.9314908472689716, 3.9015208427355965, 3.899913162152144, 3.9092192231396794, 3.9139391874354303, 3.8620622031298413, 3.912578944142645, 3.9274674272260977, 3.9238948804146183, 4.095032733502096, 3.9514060543705547, 4.041578525550579, 3.991216848644901, 3.9622055364849484, 3.8905721993171722, 3.98052576275342, 3.893250358493358, 3.9288979869369323, 4.123523266413232, 4.01311885477751, 3.967449525317238, 4.00157850389655, 3.9737809621704097, 4.095248677033695, 3.9512078640989916, 4.156800900157793, 3.9741341986467145, 3.9543320278437264, 3.9629896275092293, 4.0728053372786235, 4.060070911727276, 4.014286739858973, 4.089308473016543, 3.9433023241126075, 3.96678978925833, 4.061845429929755, 3.9865814318890385, 4.0436073559110275, 4.140215562633881, 4.129294980781587, 4.167229552817699, 3.8417120894207057, 3.9002238161981233, 3.9843740569425283, 3.915032407390547, 4.161946764049727, 4.1293646126569135, 4.104036468193637, 3.9928847488917696, 4.001577601139754, 3.9974891176877017, 4.004296586660584, 3.9342162279219814, 4.157374680699049, 4.029349049857756, 3.878376703268829, 3.9809388834333523, 4.00343594824091, 4.067412386749815, 3.9784770009464316, 4.016520568244663, 4.052356914080207, 4.003523456851258, 4.3036944576383895, 4.03626837449278, 3.991186580400121, 3.984234659587553, 4.1204282502674126, 3.998095139229716, 4.266906036702825, 4.422698291177087, 4.34139750010044, 4.232366536620475, 4.345116345811727, 4.197449835595605, 4.38104953996623, 4.496246324646653, 4.600362760024248, 4.047588401960403, 4.244094191688469, 4.1395396999672, 4.300537561376325, 4.4060278318015795, 4.091970105909508, 4.024746012121632, 4.003309838706548, 4.002370887434809, 4.069417493229331, 4.045482956413842, 4.103168845515244, 4.059855716899892, 4.061935429002674], 'val_accuracy': [0.01624446175992489, 0.03339770436286926, 0.029648983851075172, 0.03407929092645645, 0.04112234339118004, 0.02862660400569439, 0.030216971412301064, 0.03373849764466286, 0.0372600257396698, 0.03237532824277878, 0.02726343274116516, 0.030898557975888252, 0.03305691108107567, 0.027036238461732864, 0.02669544517993927, 0.03169374167919159, 0.027717823162674904, 0.035669658333063126, 0.03544246405363083, 0.035101670771837234, 0.03555605933070183, 0.028967397287487984, 0.027490628883242607, 0.03453368321061134, 0.028285810723900795, 0.027717823162674904, 0.032716117799282074, 0.029648983851075172, 0.0372600257396698, 0.03623764589428902, 0.03112575225532055, 0.03112575225532055, 0.03339770436286926, 0.035669658333063126, 0.02430989407002926, 0.033284109085798264, 0.030216971412301064, 0.03112575225532055, 0.035101670771837234, 0.034420084208250046, 0.031466543674468994, 0.031239349395036697, 0.02544587105512619, 0.030784958973526955, 0.029308190569281578, 0.031239349395036697, 0.03158014267683029, 0.03169374167919159, 0.028967397287487984, 0.021356355398893356, 0.025900261476635933, 0.03430648520588875, 0.02555946819484234, 0.01749403588473797, 0.03237532824277878, 0.028285810723900795, 0.02351471036672592, 0.01738043874502182, 0.026354651898145676, 0.018402816727757454, 0.018402816727757454, 0.03294331580400467, 0.024991480633616447, 0.019765987992286682, 0.02794501930475235, 0.02112916111946106, 0.02976258099079132, 0.014767692424356937, 0.02555946819484234, 0.020788367837667465, 0.029648983851075172, 0.027490628883242607, 0.019765987992286682, 0.031466543674468994, 0.027717823162674904, 0.027149835601449013, 0.018970806151628494, 0.029080994427204132, 0.019538793712854385, 0.01749403588473797, 0.02430989407002926, 0.025105077773332596, 0.023287516087293625, 0.030671361833810806, 0.03226172924041748, 0.02237873524427414, 0.027604226022958755, 0.02555946819484234, 0.019993184134364128, 0.019652390852570534, 0.020674770697951317, 0.03987276926636696, 0.026581846177577972, 0.019993184134364128, 0.02987617813050747, 0.029421787708997726, 0.02669544517993927, 0.02419629693031311, 0.023173917084932327, 0.024082699790596962], 'loss': [4.197777905073278, 3.7720773316941956, 3.720972585601587, 3.6821673699213835, 3.6449543681183645, 3.6122645870117642, 3.5860287514283806, 3.5527855153970234, 3.5336169688730545, 3.5210711602823963, 3.4967182489242385, 3.4879980046097963, 3.4796471671377587, 3.446389402414461, 3.426373064920234, 3.4152465586766034, 3.4271804935007584, 3.407573408614448, 3.410036631700333, 3.3974864213731855, 3.387257641542908, 3.373302959823086, 3.3812428974622577, 3.3918956441940935, 3.3630334955709014, 3.359310186422925, 3.3846328058566404, 3.3916479103899064, 3.365659732715985, 3.3575343563600666, 3.3606803476278797, 3.3536969644564603, 3.3638717840828325, 3.3248458183291634, 3.353608985412812, 3.3444150251318066, 3.3222004395556533, 3.329664921928214, 3.3204158049508767, 3.327646197241572, 3.3269851010485634, 3.3294494285630303, 3.3076497551020885, 3.302092520263141, 3.299856637365737, 3.3045762181535214, 3.2978943264148532, 3.3238296692196623, 3.2904641895387807, 3.287354778076637, 3.262973217917139, 3.2788399439759544, 3.2868066929652424, 3.298818379070947, 3.2851859264575416, 3.260692653449419, 3.263831410695396, 3.302533932441954, 3.2805556076943123, 3.2620829069415813, 3.283223935175912, 3.279775628047102, 3.2503854006457122, 3.270662384882954, 3.2940332046894776, 3.3144530396423457, 3.3181849167702406, 3.3728440796829924, 3.4093158606950915, 3.2851928083818662, 3.3367368261745143, 3.2790046855656105, 3.298912119339949, 3.3402476834172914, 3.2810220284956837, 3.2404041370397425, 3.2705295626227913, 3.306162518695221, 3.27896288296518, 3.3040492157920567, 3.3654330943033477, 3.4045165994088675, 3.2971065054979953, 3.255299937376818, 3.2717389366176546, 3.2377940163334893, 3.2510483750221755, 3.335443271101548, 3.3155152895591353, 3.357673329446004, 3.3279646348185863, 3.35492194861777, 3.330907532602705, 3.3482792654496105, 3.2912413376320235, 3.3277821859601615, 3.326230222639105, 3.3434479232777106, 3.357802464268995, 3.2860734970990775], 'accuracy': [0.04188494, 0.065222524, 0.08157535, 0.08853651, 0.09224126, 0.09705508, 0.103072345, 0.10939638, 0.111732505, 0.114469774, 0.11727783, 0.11723064, 0.11817452, 0.123885036, 0.12633914, 0.12556043, 0.12254, 0.12518288, 0.12397943, 0.12525366, 0.12704705, 0.12900561, 0.1273774, 0.1274954, 0.13051583, 0.13134174, 0.12792015, 0.12636274, 0.13315871, 0.13134174, 0.12910001, 0.13108216, 0.1308934, 0.13606116, 0.13275756, 0.13646232, 0.13516447, 0.1341026, 0.13757138, 0.13773656, 0.13629714, 0.13627353, 0.14059182, 0.1399311, 0.14115815, 0.14132333, 0.14243239, 0.1391052, 0.14474492, 0.14507528, 0.14639671, 0.14875643, 0.14611354, 0.14328189, 0.14698665, 0.14965312, 0.149016, 0.14807211, 0.14972392, 0.1514465, 0.147435, 0.14861485, 0.15279154, 0.15033744, 0.14748219, 0.1433055, 0.14531125, 0.13440937, 0.13035065, 0.14726982, 0.1398839, 0.14809571, 0.14679787, 0.14337628, 0.14611354, 0.15578838, 0.15326348, 0.14686866, 0.15111615, 0.14679787, 0.13407901, 0.12527727, 0.15189485, 0.15437256, 0.15154089, 0.15675586, 0.15578838, 0.14007267, 0.14512247, 0.13922319, 0.14417858, 0.14373024, 0.14188966, 0.14188966, 0.14665629, 0.14120534, 0.14493369, 0.13971873, 0.13929397, 0.14970031]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot history: MAE\n",
    "plt.plot(history.history['loss'], label='MAE (testing data)')\n",
    "plt.plot(history.history['val_loss'], label='MAE (validation data)')\n",
    "plt.title('MAE')\n",
    "plt.ylabel('MAE value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = np.arange(0, len(history.history['loss']))\n",
    "\n",
    "# You can chose the style of your preference\n",
    "# print(plt.style.available) to see the available options\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "# Plot train loss, train acc, val loss and val acc against epochs passed\n",
    "plt.figure()\n",
    "plt.plot(N, history.history['loss'], label = \"train_loss\")\n",
    "plt.plot(N, history.history['accuracy'], label = \"train_acc\")\n",
    "plt.plot(N, history.history['val_loss'], label = \"val_loss\")\n",
    "plt.plot(N, history.history['val_accuracy'], label = \"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "# Make sure there exists a folder called output in the current directory\n",
    "# or replace 'output' with whatever direcory you want to put in the plots\n",
    "plt.show()\n",
    "plt.savefig('../Output/EpochInceptionResNetV2_OF.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
