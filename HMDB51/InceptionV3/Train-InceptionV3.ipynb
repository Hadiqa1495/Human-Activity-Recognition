{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\HH TRADERS\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13017</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_frame0.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13018</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_frame1.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13019</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_frame2.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13020</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_frame3.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13021</th>\n",
       "      <td>winKen_wave_u_cm_np1_ri_bad_1_frame4.jpg</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image class\n",
       "13017  winKen_wave_u_cm_np1_ri_bad_1_frame0.jpg  wave\n",
       "13018  winKen_wave_u_cm_np1_ri_bad_1_frame1.jpg  wave\n",
       "13019  winKen_wave_u_cm_np1_ri_bad_1_frame2.jpg  wave\n",
       "13020  winKen_wave_u_cm_np1_ri_bad_1_frame3.jpg  wave\n",
       "13021  winKen_wave_u_cm_np1_ri_bad_1_frame4.jpg  wave"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "train.sort_values(by=['class', 'image'])\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13022/13022 [01:33<00:00, 139.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating an empty list\n",
    "train_image = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    # loading the image and keeping the target size as (224,224,3)\n",
    "    img = image.load_img('../data/train_frame/'+train['image'][i], target_size=(224,224,3))\n",
    "    # converting it to array\n",
    "    img = image.img_to_array(img)\n",
    "    # normalizing the pixel value\n",
    "    img = img/255\n",
    "    # appending the image to the train_image list\n",
    "    train_image.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13022, 224, 224, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the list to numpy array\n",
    "X_train = np.array(train_image,np.float16)\n",
    "\n",
    "# shape of the array\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5504</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5505</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5506</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5507</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5508</th>\n",
       "      <td>prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  image class\n",
       "5504  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "5505  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "5506  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "5507  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave\n",
       "5508  prelinger_LetsBeGo1953_wave_u_cm_np10_ba_med_3...  wave"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = pd.read_csv('../data/val.csv')\n",
    "val.sort_values(by=['class', 'image'])\n",
    "val.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5509/5509 [00:51<00:00, 106.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating an empty list\n",
    "val_image = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(val.shape[0])):\n",
    "    # loading the image and keeping the target size as (224,224,3)\n",
    "    img = image.load_img('../data/val_frame/'+val['image'][i], target_size=(224,224,3))\n",
    "    # converting it to array\n",
    "    img = image.img_to_array(img)\n",
    "    # normalizing the pixel value\n",
    "    img = img/255\n",
    "    # appending the image to the train_image list\n",
    "    val_image.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5509, 224, 224, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the list to numpy array\n",
    "X_test = np.array(val_image,np.float16)\n",
    "\n",
    "# shape of the array\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image    13022\n",
      "class       51\n",
      "dtype: int64\n",
      "image    5509\n",
      "class      51\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# separating the target\n",
    "y_train = train['class']\n",
    "y_test = val['class']\n",
    "print(train.nunique())\n",
    "print(val.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13022, 51)\n",
      "(5509, 51)\n"
     ]
    }
   ],
   "source": [
    "# creating dummies of target variable for train and validation set\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, None, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(base_model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inception_v3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 3 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 3 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None, 3 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 3 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 3 96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, None, 3 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 6 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 6 192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, None, 6 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 6 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 8 5120        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 8 240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, None, 8 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 1 138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 1 576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, None, 1 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 1 0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 6 192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, None, 6 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 4 9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 9 55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 4 144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 9 288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, None, 4 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, None, 9 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, None, None, 1 0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 6 76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 9 82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 3 6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 6 192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 6 192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 9 288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, 3 96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, None, 6 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, None, 6 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, None, 9 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, None, 3 0           batch_normalization_12[0][0]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, None, None, 2 0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, None, 6 192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, None, 6 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 4 12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 9 55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, None, 4 144         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, None, 9 288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, None, 4 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, None, 9 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, None, None, 2 0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 6 76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 9 82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 6 16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, None, 6 192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, None, 6 192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, None, 9 288         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, None, 6 192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, None, 6 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, None, 6 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, None, None, 9 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, None, None, 6 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, None, None, 2 0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, None, 6 192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, None, 6 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, None, None, 4 13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, None, None, 9 55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, None, 4 144         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, None, 9 288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, None, 4 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, None, None, 9 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, None, None, 2 0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, None, None, 6 76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, None, None, 9 82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, None, None, 6 18432       average_pooling2d_3[0][0]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, None, 6 192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, None, 6 192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, None, 9 288         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, None, 6 192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, None, None, 6 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, None, 6 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, None, None, 9 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, None, None, 6 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, None, None, 2 0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, None, None, 6 18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, None, None, 6 192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, None, None, 6 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, None, None, 9 55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, None, None, 9 288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, None, None, 9 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, None, None, 3 995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, None, None, 9 82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, None, None, 3 1152        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, None, None, 9 288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, None, None, 3 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, None, None, 9 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, None, None, 2 0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, None, None, 7 0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, None, 1 384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, None, None, 1 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, None, None, 1 114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, None, 1 384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, None, None, 1 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, None, None, 1 114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, None, 1 384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, None, None, 1 384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, None, None, 1 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, None, None, 1 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, None, None, 1 114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, None, None, 1 114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, None, 1 384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_38 (BatchNo (None, None, None, 1 384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, None, None, 1 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, None, None, 1 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, None, None, 7 0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, None, None, 1 147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, None, None, 1 172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, None, None, 1 172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, None, 1 576         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, None, 1 576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, None, None, 1 576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, None, None, 1 576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, None, None, 1 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, None, None, 1 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, None, None, 1 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, None, None, 1 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, None, None, 7 0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, None, None, 1 480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, None, None, 1 0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, None, None, 1 179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, None, None, 1 480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, None, None, 1 0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, None, None, 1 179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, None, None, 1 480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, None, None, 1 480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, None, None, 1 0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, None, None, 1 0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, None, None, 1 179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, None, None, 1 179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, None, None, 1 480         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, None, None, 1 480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, None, None, 1 0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, None, None, 1 0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, None, None, 7 0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, None, None, 1 147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, None, None, 1 215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, None, None, 1 215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_5[0][0]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, None, None, 1 576         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, None, None, 1 576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, None, None, 1 576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, None, None, 1 576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, None, None, 1 0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, None, None, 1 0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, None, None, 1 0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, None, None, 1 0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, None, None, 7 0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, None, None, 1 480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, None, None, 1 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, None, None, 1 179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, None, None, 1 480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, None, None, 1 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, None, None, 1 179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, None, None, 1 480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, None, None, 1 480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, None, None, 1 0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, None, None, 1 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, None, None, 1 179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, None, None, 1 179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, None, None, 1 480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, None, None, 1 480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, None, None, 1 0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, None, None, 1 0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, None, None, 7 0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, None, None, 1 147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, None, None, 1 215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, None, None, 1 215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, None, None, 1 576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, None, None, 1 576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, None, None, 1 576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, None, None, 1 576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, None, None, 1 0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, None, None, 1 0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, None, None, 1 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_60 (Activation)      (None, None, None, 1 0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, None, None, 7 0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, None, None, 1 576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, None, None, 1 0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, None, None, 1 258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, None, None, 1 576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, None, None, 1 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, None, None, 1 258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, None, None, 1 576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, None, None, 1 576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, None, None, 1 0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, None, None, 1 0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, None, None, 1 258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, None, None, 1 258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, None, None, 1 576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, None, None, 1 576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, None, None, 1 0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, None, None, 1 0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, None, None, 7 0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, None, None, 1 258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, None, None, 1 258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, None, None, 1 576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, None, None, 1 576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, None, None, 1 576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, None, None, 1 576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, None, None, 1 0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, None, None, 1 0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, None, None, 1 0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, None, None, 1 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, None, None, 7 0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, None, None, 1 576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, None, None, 1 0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, None, None, 1 258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_74 (BatchNo (None, None, None, 1 576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, None, None, 1 0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, None, None, 1 258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, None, None, 1 576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, None, None, 1 576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, None, None, 1 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, None, None, 1 0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, None, None, 3 552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, None, None, 1 331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, None, None, 3 960         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, None, None, 1 576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, None, None, 3 0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, None, None, 1 0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, None, None, 7 0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, None, None, 1 0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, None, None, 4 573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, None, None, 4 1344        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, None, None, 4 0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, None, None, 3 491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, None, None, 3 1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, None, None, 3 1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, None, None, 3 1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, None, None, 3 0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, None, None, 3 0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, None, None, 3 442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, None, None, 3 442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, None, None, 3 442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, None, None, 3 442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, None, None, 1 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, None, None, 3 409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, None, None, 3 1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, None, None, 3 1152        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, None, None, 3 1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, None, None, 3 1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, None, None, 1 245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, None, None, 3 960         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, None, None, 3 0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, None, None, 3 0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, None, None, 3 0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_84 (Activation)      (None, None, None, 3 0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, None, None, 1 576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, None, None, 3 0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, None, None, 7 0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 7 0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, None, None, 1 0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, None, None, 2 0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, None, None, 4 917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, None, None, 4 1344        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, None, None, 4 0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, None, None, 3 786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, None, None, 3 1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, None, None, 3 1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, None, None, 3 1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, None, None, 3 0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, None, None, 3 0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, None, None, 3 442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, None, None, 3 442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, None, None, 3 442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, None, None, 3 442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, None, None, 2 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, None, None, 3 655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, None, None, 3 1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, None, None, 3 1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, None, None, 3 1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, None, None, 3 1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, None, None, 1 393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, None, None, 3 960         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, None, None, 3 0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, None, None, 3 0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, None, None, 3 0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, None, None, 3 0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, None, None, 1 576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, None, None, 3 0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, None, None, 7 0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 7 0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, None, None, 1 0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, None, None, 2 0           activation_86[0][0]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'inception_v3',\n",
       " 'layers': [{'name': 'input_1',\n",
       "   'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, None, None, 3),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'name': 'input_1'},\n",
       "   'inbound_nodes': []},\n",
       "  {'name': 'conv2d_1',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['input_1', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_1',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_1', 0, 0, {}]]]},\n",
       "  {'name': 'activation_1',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_1', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_2',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_1', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_2',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_2', 0, 0, {}]]]},\n",
       "  {'name': 'activation_2',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_2', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_3',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_2', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_3',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_3', 0, 0, {}]]]},\n",
       "  {'name': 'activation_3',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_3', 0, 0, {}]]]},\n",
       "  {'name': 'max_pooling2d_1',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'max_pooling2d_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_3', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_4',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 80,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['max_pooling2d_1', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_4',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_4', 0, 0, {}]]]},\n",
       "  {'name': 'activation_4',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_4', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_5',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_4', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_5',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_5', 0, 0, {}]]]},\n",
       "  {'name': 'activation_5',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_5', 0, 0, {}]]]},\n",
       "  {'name': 'max_pooling2d_2',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'max_pooling2d_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['activation_5', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_9',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['max_pooling2d_2', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_9',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_9', 0, 0, {}]]]},\n",
       "  {'name': 'activation_9',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_9', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_7',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 48,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['max_pooling2d_2', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_10',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 96,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_9', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_7',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_7', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_10',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_10', 0, 0, {}]]]},\n",
       "  {'name': 'activation_7',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_7', 0, 0, {}]]]},\n",
       "  {'name': 'activation_10',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_10', 0, 0, {}]]]},\n",
       "  {'name': 'average_pooling2d_1',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'average_pooling2d_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['max_pooling2d_2', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_6',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['max_pooling2d_2', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_8',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_7', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_11',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 96,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_10', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_12',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['average_pooling2d_1', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_6',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_6', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_8',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_8', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_11',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_11', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_12',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_12', 0, 0, {}]]]},\n",
       "  {'name': 'activation_6',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_6', 0, 0, {}]]]},\n",
       "  {'name': 'activation_8',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_8', 0, 0, {}]]]},\n",
       "  {'name': 'activation_11',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_11', 0, 0, {}]]]},\n",
       "  {'name': 'activation_12',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_12', 0, 0, {}]]]},\n",
       "  {'name': 'mixed0',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_6', 0, 0, {}],\n",
       "     ['activation_8', 0, 0, {}],\n",
       "     ['activation_11', 0, 0, {}],\n",
       "     ['activation_12', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_16',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_16',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed0', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_16',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_16',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_16', 0, 0, {}]]]},\n",
       "  {'name': 'activation_16',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_16',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_16', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_14',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_14',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 48,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed0', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_17',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_17',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 96,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_16', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_14',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_14',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_14', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_17',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_17',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_17', 0, 0, {}]]]},\n",
       "  {'name': 'activation_14',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_14',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_14', 0, 0, {}]]]},\n",
       "  {'name': 'activation_17',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_17',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_17', 0, 0, {}]]]},\n",
       "  {'name': 'average_pooling2d_2',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'average_pooling2d_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['mixed0', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_13',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_13',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed0', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_15',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_15',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_14', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_18',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_18',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 96,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_17', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_19',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_19',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['average_pooling2d_2', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_13',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_13',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_13', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_15',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_15',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_15', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_18',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_18',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_18', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_19',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_19',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_19', 0, 0, {}]]]},\n",
       "  {'name': 'activation_13',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_13',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_13', 0, 0, {}]]]},\n",
       "  {'name': 'activation_15',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_15',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_15', 0, 0, {}]]]},\n",
       "  {'name': 'activation_18',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_18',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_18', 0, 0, {}]]]},\n",
       "  {'name': 'activation_19',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_19',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_19', 0, 0, {}]]]},\n",
       "  {'name': 'mixed1',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_13', 0, 0, {}],\n",
       "     ['activation_15', 0, 0, {}],\n",
       "     ['activation_18', 0, 0, {}],\n",
       "     ['activation_19', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_23',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_23',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed1', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_23',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_23',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_23', 0, 0, {}]]]},\n",
       "  {'name': 'activation_23',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_23',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_23', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_21',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_21',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 48,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed1', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_24',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_24',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 96,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_23', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_21',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_21',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_21', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_24',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_24',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_24', 0, 0, {}]]]},\n",
       "  {'name': 'activation_21',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_21',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_21', 0, 0, {}]]]},\n",
       "  {'name': 'activation_24',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_24',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_24', 0, 0, {}]]]},\n",
       "  {'name': 'average_pooling2d_3',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'average_pooling2d_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['mixed1', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_20',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_20',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed1', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_22',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_22',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_21', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_25',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_25',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 96,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_24', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_26',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_26',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['average_pooling2d_3', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_20',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_20',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_20', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_22',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_22',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_22', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_25',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_25',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_25', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_26',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_26',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_26', 0, 0, {}]]]},\n",
       "  {'name': 'activation_20',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_20',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_20', 0, 0, {}]]]},\n",
       "  {'name': 'activation_22',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_22',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_22', 0, 0, {}]]]},\n",
       "  {'name': 'activation_25',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_25',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_25', 0, 0, {}]]]},\n",
       "  {'name': 'activation_26',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_26',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_26', 0, 0, {}]]]},\n",
       "  {'name': 'mixed2',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_20', 0, 0, {}],\n",
       "     ['activation_22', 0, 0, {}],\n",
       "     ['activation_25', 0, 0, {}],\n",
       "     ['activation_26', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_28',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_28',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 64,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed2', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_28',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_28',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_28', 0, 0, {}]]]},\n",
       "  {'name': 'activation_28',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_28',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_28', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_29',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_29',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 96,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_28', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_29',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_29',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_29', 0, 0, {}]]]},\n",
       "  {'name': 'activation_29',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_29',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_29', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_27',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_27',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed2', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_30',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_30',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 96,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_29', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_27',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_27',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_27', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_30',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_30',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_30', 0, 0, {}]]]},\n",
       "  {'name': 'activation_27',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_27',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_27', 0, 0, {}]]]},\n",
       "  {'name': 'activation_30',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_30',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_30', 0, 0, {}]]]},\n",
       "  {'name': 'max_pooling2d_3',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'max_pooling2d_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['mixed2', 0, 0, {}]]]},\n",
       "  {'name': 'mixed3',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_27', 0, 0, {}],\n",
       "     ['activation_30', 0, 0, {}],\n",
       "     ['max_pooling2d_3', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_35',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_35',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed3', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_35',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_35',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_35', 0, 0, {}]]]},\n",
       "  {'name': 'activation_35',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_35',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_35', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_36',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_36',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_35', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_36',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_36',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_36', 0, 0, {}]]]},\n",
       "  {'name': 'activation_36',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_36',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_36', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_32',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_32',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed3', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_37',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_37',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_36', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_32',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_32',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_32', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_37',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_37',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_37', 0, 0, {}]]]},\n",
       "  {'name': 'activation_32',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_32',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_32', 0, 0, {}]]]},\n",
       "  {'name': 'activation_37',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_37',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_37', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_33',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_33',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_32', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_38',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_38',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 128,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_37', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_33',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_33',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_33', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_38',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_38',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_38', 0, 0, {}]]]},\n",
       "  {'name': 'activation_33',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_33',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_33', 0, 0, {}]]]},\n",
       "  {'name': 'activation_38',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_38',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_38', 0, 0, {}]]]},\n",
       "  {'name': 'average_pooling2d_4',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'average_pooling2d_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['mixed3', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_31',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_31',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed3', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_34',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_34',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_33', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_39',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_39',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_38', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_40',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_40',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['average_pooling2d_4', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_31',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_31',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_31', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_34',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_34',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_34', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_39',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_39',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_39', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_40',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_40',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_40', 0, 0, {}]]]},\n",
       "  {'name': 'activation_31',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_31',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_31', 0, 0, {}]]]},\n",
       "  {'name': 'activation_34',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_34',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_34', 0, 0, {}]]]},\n",
       "  {'name': 'activation_39',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_39',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_39', 0, 0, {}]]]},\n",
       "  {'name': 'activation_40',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_40',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_40', 0, 0, {}]]]},\n",
       "  {'name': 'mixed4',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_31', 0, 0, {}],\n",
       "     ['activation_34', 0, 0, {}],\n",
       "     ['activation_39', 0, 0, {}],\n",
       "     ['activation_40', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_45',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_45',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed4', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_45',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_45',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_45', 0, 0, {}]]]},\n",
       "  {'name': 'activation_45',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_45',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_45', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_46',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_46',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_45', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_46',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_46',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_46', 0, 0, {}]]]},\n",
       "  {'name': 'activation_46',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_46',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_46', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_42',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_42',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed4', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_47',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_47',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_46', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_42',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_42',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_42', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_47',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_47',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_47', 0, 0, {}]]]},\n",
       "  {'name': 'activation_42',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_42',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_42', 0, 0, {}]]]},\n",
       "  {'name': 'activation_47',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_47',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_47', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_43',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_43',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_42', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_48',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_48',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_47', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_43',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_43',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_43', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_48',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_48',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_48', 0, 0, {}]]]},\n",
       "  {'name': 'activation_43',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_43',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_43', 0, 0, {}]]]},\n",
       "  {'name': 'activation_48',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_48',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_48', 0, 0, {}]]]},\n",
       "  {'name': 'average_pooling2d_5',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'average_pooling2d_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['mixed4', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_41',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_41',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed4', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_44',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_44',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_43', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_49',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_49',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_48', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_50',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_50',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['average_pooling2d_5', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_41',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_41',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_41', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_44',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_44',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_44', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_49',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_49',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_49', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_50',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_50',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_50', 0, 0, {}]]]},\n",
       "  {'name': 'activation_41',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_41',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_41', 0, 0, {}]]]},\n",
       "  {'name': 'activation_44',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_44',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_44', 0, 0, {}]]]},\n",
       "  {'name': 'activation_49',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_49',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_49', 0, 0, {}]]]},\n",
       "  {'name': 'activation_50',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_50',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_50', 0, 0, {}]]]},\n",
       "  {'name': 'mixed5',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_41', 0, 0, {}],\n",
       "     ['activation_44', 0, 0, {}],\n",
       "     ['activation_49', 0, 0, {}],\n",
       "     ['activation_50', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_55',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_55',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed5', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_55',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_55',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_55', 0, 0, {}]]]},\n",
       "  {'name': 'activation_55',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_55',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_55', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_56',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_56',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_55', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_56',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_56',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_56', 0, 0, {}]]]},\n",
       "  {'name': 'activation_56',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_56',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_56', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_52',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_52',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed5', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_57',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_57',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_56', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_52',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_52',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_52', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_57',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_57',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_57', 0, 0, {}]]]},\n",
       "  {'name': 'activation_52',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_52',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_52', 0, 0, {}]]]},\n",
       "  {'name': 'activation_57',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_57',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_57', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_53',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_53',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_52', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_58',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_58',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 160,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_57', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_53',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_53',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_53', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_58',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_58',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_58', 0, 0, {}]]]},\n",
       "  {'name': 'activation_53',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_53',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_53', 0, 0, {}]]]},\n",
       "  {'name': 'activation_58',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_58',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_58', 0, 0, {}]]]},\n",
       "  {'name': 'average_pooling2d_6',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'average_pooling2d_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['mixed5', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_51',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_51',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed5', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_54',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_54',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_53', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_59',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_59',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_58', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_60',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_60',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['average_pooling2d_6', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_51',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_51',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_51', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_54',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_54',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_54', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_59',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_59',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_59', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_60',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_60',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_60', 0, 0, {}]]]},\n",
       "  {'name': 'activation_51',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_51',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_51', 0, 0, {}]]]},\n",
       "  {'name': 'activation_54',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_54',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_54', 0, 0, {}]]]},\n",
       "  {'name': 'activation_59',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_59',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_59', 0, 0, {}]]]},\n",
       "  {'name': 'activation_60',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_60',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_60', 0, 0, {}]]]},\n",
       "  {'name': 'mixed6',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_51', 0, 0, {}],\n",
       "     ['activation_54', 0, 0, {}],\n",
       "     ['activation_59', 0, 0, {}],\n",
       "     ['activation_60', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_65',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_65',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed6', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_65',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_65',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_65', 0, 0, {}]]]},\n",
       "  {'name': 'activation_65',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_65',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_65', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_66',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_66',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_65', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_66',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_66',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_66', 0, 0, {}]]]},\n",
       "  {'name': 'activation_66',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_66',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_66', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_62',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_62',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed6', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_67',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_67',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_66', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_62',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_62',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_62', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_67',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_67',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_67', 0, 0, {}]]]},\n",
       "  {'name': 'activation_62',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_62',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_62', 0, 0, {}]]]},\n",
       "  {'name': 'activation_67',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_67',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_67', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_63',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_63',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_62', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_68',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_68',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_67', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_63',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_63',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_63', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_68',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_68',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_68', 0, 0, {}]]]},\n",
       "  {'name': 'activation_63',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_63',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_63', 0, 0, {}]]]},\n",
       "  {'name': 'activation_68',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_68',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_68', 0, 0, {}]]]},\n",
       "  {'name': 'average_pooling2d_7',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'average_pooling2d_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['mixed6', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_61',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_61',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed6', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_64',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_64',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_63', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_69',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_69',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_68', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_70',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_70',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['average_pooling2d_7', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_61',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_61',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_61', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_64',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_64',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_64', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_69',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_69',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_69', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_70',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_70',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_70', 0, 0, {}]]]},\n",
       "  {'name': 'activation_61',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_61',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_61', 0, 0, {}]]]},\n",
       "  {'name': 'activation_64',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_64',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_64', 0, 0, {}]]]},\n",
       "  {'name': 'activation_69',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_69',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_69', 0, 0, {}]]]},\n",
       "  {'name': 'activation_70',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_70',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_70', 0, 0, {}]]]},\n",
       "  {'name': 'mixed7',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_61', 0, 0, {}],\n",
       "     ['activation_64', 0, 0, {}],\n",
       "     ['activation_69', 0, 0, {}],\n",
       "     ['activation_70', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_73',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_73',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed7', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_73',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_73',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_73', 0, 0, {}]]]},\n",
       "  {'name': 'activation_73',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_73',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_73', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_74',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_74',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 7),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_73', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_74',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_74',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_74', 0, 0, {}]]]},\n",
       "  {'name': 'activation_74',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_74',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_74', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_71',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_71',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed7', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_75',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_75',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (7, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_74', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_71',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_71',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_71', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_75',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_75',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_75', 0, 0, {}]]]},\n",
       "  {'name': 'activation_71',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_71',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_71', 0, 0, {}]]]},\n",
       "  {'name': 'activation_75',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_75',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_75', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_72',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_72',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 320,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_71', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_76',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_76',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_75', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_72',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_72',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_72', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_76',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_76',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_76', 0, 0, {}]]]},\n",
       "  {'name': 'activation_72',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_72',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_72', 0, 0, {}]]]},\n",
       "  {'name': 'activation_76',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_76',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_76', 0, 0, {}]]]},\n",
       "  {'name': 'max_pooling2d_4',\n",
       "   'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'max_pooling2d_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['mixed7', 0, 0, {}]]]},\n",
       "  {'name': 'mixed8',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_72', 0, 0, {}],\n",
       "     ['activation_76', 0, 0, {}],\n",
       "     ['max_pooling2d_4', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_81',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_81',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 448,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed8', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_81',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_81',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_81', 0, 0, {}]]]},\n",
       "  {'name': 'activation_81',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_81',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_81', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_78',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_78',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed8', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_82',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_82',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_81', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_78',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_78',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_78', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_82',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_82',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_82', 0, 0, {}]]]},\n",
       "  {'name': 'activation_78',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_78',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_78', 0, 0, {}]]]},\n",
       "  {'name': 'activation_82',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_82',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_82', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_79',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_79',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (1, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_78', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_80',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_80',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (3, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_78', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_83',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_83',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (1, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_82', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_84',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_84',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (3, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_82', 0, 0, {}]]]},\n",
       "  {'name': 'average_pooling2d_8',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'average_pooling2d_8',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['mixed8', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_77',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_77',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 320,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed8', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_79',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_79',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_79', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_80',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_80',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_80', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_83',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_83',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_83', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_84',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_84',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_84', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_85',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_85',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['average_pooling2d_8', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_77',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_77',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_77', 0, 0, {}]]]},\n",
       "  {'name': 'activation_79',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_79',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_79', 0, 0, {}]]]},\n",
       "  {'name': 'activation_80',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_80',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_80', 0, 0, {}]]]},\n",
       "  {'name': 'activation_83',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_83',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_83', 0, 0, {}]]]},\n",
       "  {'name': 'activation_84',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_84',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_84', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_85',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_85',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_85', 0, 0, {}]]]},\n",
       "  {'name': 'activation_77',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_77',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_77', 0, 0, {}]]]},\n",
       "  {'name': 'mixed9_0',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed9_0',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_79', 0, 0, {}],\n",
       "     ['activation_80', 0, 0, {}]]]},\n",
       "  {'name': 'concatenate_1',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'concatenate_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_83', 0, 0, {}],\n",
       "     ['activation_84', 0, 0, {}]]]},\n",
       "  {'name': 'activation_85',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_85',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_85', 0, 0, {}]]]},\n",
       "  {'name': 'mixed9',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_77', 0, 0, {}],\n",
       "     ['mixed9_0', 0, 0, {}],\n",
       "     ['concatenate_1', 0, 0, {}],\n",
       "     ['activation_85', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_90',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_90',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 448,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed9', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_90',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_90',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_90', 0, 0, {}]]]},\n",
       "  {'name': 'activation_90',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_90',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_90', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_87',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_87',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed9', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_91',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_91',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (3, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_90', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_87',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_87',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_87', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_91',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_91',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_91', 0, 0, {}]]]},\n",
       "  {'name': 'activation_87',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_87',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_87', 0, 0, {}]]]},\n",
       "  {'name': 'activation_91',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_91',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_91', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_88',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_88',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (1, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_87', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_89',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_89',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (3, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_87', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_92',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_92',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (1, 3),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_91', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_93',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_93',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 384,\n",
       "    'kernel_size': (3, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['activation_91', 0, 0, {}]]]},\n",
       "  {'name': 'average_pooling2d_9',\n",
       "   'class_name': 'AveragePooling2D',\n",
       "   'config': {'name': 'average_pooling2d_9',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (3, 3),\n",
       "    'padding': 'same',\n",
       "    'strides': (1, 1),\n",
       "    'data_format': 'channels_last'},\n",
       "   'inbound_nodes': [[['mixed9', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_86',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_86',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 320,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['mixed9', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_88',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_88',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_88', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_89',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_89',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_89', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_92',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_92',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_92', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_93',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_93',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_93', 0, 0, {}]]]},\n",
       "  {'name': 'conv2d_94',\n",
       "   'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_94',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'filters': 192,\n",
       "    'kernel_size': (1, 1),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'same',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'linear',\n",
       "    'use_bias': False,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'inbound_nodes': [[['average_pooling2d_9', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_86',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_86',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_86', 0, 0, {}]]]},\n",
       "  {'name': 'activation_88',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_88',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_88', 0, 0, {}]]]},\n",
       "  {'name': 'activation_89',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_89',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_89', 0, 0, {}]]]},\n",
       "  {'name': 'activation_92',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_92',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_92', 0, 0, {}]]]},\n",
       "  {'name': 'activation_93',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_93',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_93', 0, 0, {}]]]},\n",
       "  {'name': 'batch_normalization_94',\n",
       "   'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_94',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3,\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': False,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None},\n",
       "   'inbound_nodes': [[['conv2d_94', 0, 0, {}]]]},\n",
       "  {'name': 'activation_86',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_86',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_86', 0, 0, {}]]]},\n",
       "  {'name': 'mixed9_1',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed9_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_88', 0, 0, {}],\n",
       "     ['activation_89', 0, 0, {}]]]},\n",
       "  {'name': 'concatenate_2',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'concatenate_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_92', 0, 0, {}],\n",
       "     ['activation_93', 0, 0, {}]]]},\n",
       "  {'name': 'activation_94',\n",
       "   'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_94',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'relu'},\n",
       "   'inbound_nodes': [[['batch_normalization_94', 0, 0, {}]]]},\n",
       "  {'name': 'mixed10',\n",
       "   'class_name': 'Concatenate',\n",
       "   'config': {'name': 'mixed10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': 3},\n",
       "   'inbound_nodes': [[['activation_86', 0, 0, {}],\n",
       "     ['mixed9_1', 0, 0, {}],\n",
       "     ['concatenate_2', 0, 0, {}],\n",
       "     ['activation_94', 0, 0, {}]]]}],\n",
       " 'input_layers': [['input_1', 0, 0]],\n",
       " 'output_layers': [['mixed10', 0, 0]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-19 01:33:52.296285\n",
      "(13022, 5, 5, 2048)\n",
      "2020-03-19 02:09:43.712907\n"
     ]
    }
   ],
   "source": [
    "t1=datetime.datetime.now()\n",
    "print(t1)\n",
    "# extracting features for training frames\n",
    "X_train = base_model.predict(X_train)\n",
    "print(X_train.shape)\n",
    "t2=datetime.datetime.now()\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-19 02:09:43.737841\n",
      "(5509, 5, 5, 2048)\n",
      "2020-03-19 02:22:42.079422\n"
     ]
    }
   ],
   "source": [
    "t3=datetime.datetime.now()\n",
    "print(t3)\n",
    "# extracting features for validation frames\n",
    "X_test = base_model.predict(X_test)\n",
    "print(X_test.shape)\n",
    "t4=datetime.datetime.now()\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the training as well as validation frames in single dimension\n",
    "X_train = X_train.reshape(13022, 5*5*2048)\n",
    "X_test = X_test.reshape(5509, 5*5*2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the pixel values\n",
    "max = X_train.max()\n",
    "X_train = X_train/max\n",
    "X_test = X_test/max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Pickle/InceptionV3_X_test.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model as a pickle in a file \n",
    "joblib.dump(X_train, '../Pickle/InceptionV3_X_train.pkl') \n",
    "joblib.dump(X_test, '../Pickle/InceptionV3_X_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file \n",
    "X_train = joblib.load('../Pickle/InceptionV3_X_train.pkl') \n",
    "X_test = joblib.load('../Pickle/InceptionV3_X_test.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13022, 51200)\n",
      "(5509, 51200)\n",
      "(13022, 51)\n",
      "(5509, 51)\n"
     ]
    }
   ],
   "source": [
    "# shape of images\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(51200,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(51, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 51)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              52429824  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 51)                6579      \n",
      "=================================================================\n",
      "Total params: 53,125,427\n",
      "Trainable params: 53,125,427\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential_1',\n",
       " 'layers': [{'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_1',\n",
       "    'trainable': True,\n",
       "    'batch_input_shape': (None, 51200),\n",
       "    'dtype': 'float32',\n",
       "    'units': 1024,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 512,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 256,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 128,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.5,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 51,\n",
       "    'activation': 'softmax',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_save = ModelCheckpoint('../Models/weightInceptionV3.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-19 02:23:00.041278\n",
      "Train on 13022 samples, validate on 5509 samples\n",
      "Epoch 1/100\n",
      "13022/13022 [==============================] - ETA: 2:43 - loss: 3.9231 - accuracy: 0.00 - ETA: 1:48 - loss: 4.0695 - accuracy: 0.01 - ETA: 1:29 - loss: 4.1136 - accuracy: 0.01 - ETA: 1:21 - loss: 4.1179 - accuracy: 0.01 - ETA: 1:15 - loss: 4.1306 - accuracy: 0.01 - ETA: 1:10 - loss: 4.1067 - accuracy: 0.02 - ETA: 1:07 - loss: 4.1111 - accuracy: 0.02 - ETA: 1:05 - loss: 4.1074 - accuracy: 0.02 - ETA: 1:03 - loss: 4.1130 - accuracy: 0.01 - ETA: 1:01 - loss: 4.1125 - accuracy: 0.01 - ETA: 1:00 - loss: 4.1114 - accuracy: 0.01 - ETA: 59s - loss: 4.1048 - accuracy: 0.0182 - ETA: 58s - loss: 4.1032 - accuracy: 0.018 - ETA: 56s - loss: 4.1012 - accuracy: 0.017 - ETA: 55s - loss: 4.0928 - accuracy: 0.017 - ETA: 54s - loss: 4.0859 - accuracy: 0.018 - ETA: 53s - loss: 4.0773 - accuracy: 0.017 - ETA: 52s - loss: 4.0739 - accuracy: 0.018 - ETA: 52s - loss: 4.0689 - accuracy: 0.017 - ETA: 51s - loss: 4.0626 - accuracy: 0.019 - ETA: 50s - loss: 4.0580 - accuracy: 0.019 - ETA: 49s - loss: 4.0532 - accuracy: 0.019 - ETA: 48s - loss: 4.0488 - accuracy: 0.022 - ETA: 47s - loss: 4.0445 - accuracy: 0.022 - ETA: 47s - loss: 4.0400 - accuracy: 0.024 - ETA: 46s - loss: 4.0356 - accuracy: 0.025 - ETA: 45s - loss: 4.0328 - accuracy: 0.025 - ETA: 44s - loss: 4.0310 - accuracy: 0.025 - ETA: 44s - loss: 4.0276 - accuracy: 0.026 - ETA: 43s - loss: 4.0214 - accuracy: 0.026 - ETA: 42s - loss: 4.0192 - accuracy: 0.026 - ETA: 41s - loss: 4.0158 - accuracy: 0.025 - ETA: 41s - loss: 4.0133 - accuracy: 0.025 - ETA: 40s - loss: 4.0096 - accuracy: 0.026 - ETA: 39s - loss: 4.0065 - accuracy: 0.026 - ETA: 39s - loss: 4.0058 - accuracy: 0.026 - ETA: 38s - loss: 4.0030 - accuracy: 0.027 - ETA: 37s - loss: 4.0012 - accuracy: 0.027 - ETA: 37s - loss: 3.9990 - accuracy: 0.028 - ETA: 36s - loss: 3.9972 - accuracy: 0.028 - ETA: 36s - loss: 3.9944 - accuracy: 0.028 - ETA: 35s - loss: 3.9912 - accuracy: 0.028 - ETA: 34s - loss: 3.9880 - accuracy: 0.029 - ETA: 34s - loss: 3.9861 - accuracy: 0.029 - ETA: 33s - loss: 3.9856 - accuracy: 0.029 - ETA: 33s - loss: 3.9840 - accuracy: 0.029 - ETA: 32s - loss: 3.9814 - accuracy: 0.029 - ETA: 31s - loss: 3.9793 - accuracy: 0.029 - ETA: 31s - loss: 3.9775 - accuracy: 0.030 - ETA: 30s - loss: 3.9753 - accuracy: 0.030 - ETA: 29s - loss: 3.9731 - accuracy: 0.031 - ETA: 29s - loss: 3.9713 - accuracy: 0.032 - ETA: 28s - loss: 3.9692 - accuracy: 0.032 - ETA: 28s - loss: 3.9668 - accuracy: 0.033 - ETA: 27s - loss: 3.9663 - accuracy: 0.033 - ETA: 26s - loss: 3.9660 - accuracy: 0.033 - ETA: 26s - loss: 3.9639 - accuracy: 0.033 - ETA: 25s - loss: 3.9635 - accuracy: 0.033 - ETA: 25s - loss: 3.9616 - accuracy: 0.033 - ETA: 24s - loss: 3.9594 - accuracy: 0.033 - ETA: 23s - loss: 3.9575 - accuracy: 0.033 - ETA: 23s - loss: 3.9559 - accuracy: 0.034 - ETA: 22s - loss: 3.9542 - accuracy: 0.034 - ETA: 22s - loss: 3.9539 - accuracy: 0.034 - ETA: 21s - loss: 3.9521 - accuracy: 0.034 - ETA: 20s - loss: 3.9497 - accuracy: 0.035 - ETA: 20s - loss: 3.9477 - accuracy: 0.035 - ETA: 19s - loss: 3.9461 - accuracy: 0.035 - ETA: 19s - loss: 3.9448 - accuracy: 0.036 - ETA: 18s - loss: 3.9446 - accuracy: 0.035 - ETA: 18s - loss: 3.9442 - accuracy: 0.036 - ETA: 17s - loss: 3.9438 - accuracy: 0.036 - ETA: 16s - loss: 3.9425 - accuracy: 0.036 - ETA: 16s - loss: 3.9410 - accuracy: 0.036 - ETA: 15s - loss: 3.9393 - accuracy: 0.037 - ETA: 15s - loss: 3.9385 - accuracy: 0.037 - ETA: 14s - loss: 3.9374 - accuracy: 0.037 - ETA: 13s - loss: 3.9359 - accuracy: 0.037 - ETA: 13s - loss: 3.9344 - accuracy: 0.037 - ETA: 12s - loss: 3.9338 - accuracy: 0.037 - ETA: 12s - loss: 3.9323 - accuracy: 0.038 - ETA: 11s - loss: 3.9306 - accuracy: 0.038 - ETA: 10s - loss: 3.9293 - accuracy: 0.038 - ETA: 10s - loss: 3.9279 - accuracy: 0.038 - ETA: 9s - loss: 3.9267 - accuracy: 0.038 - ETA: 9s - loss: 3.9259 - accuracy: 0.03 - ETA: 8s - loss: 3.9247 - accuracy: 0.03 - ETA: 8s - loss: 3.9234 - accuracy: 0.03 - ETA: 7s - loss: 3.9214 - accuracy: 0.03 - ETA: 6s - loss: 3.9188 - accuracy: 0.03 - ETA: 6s - loss: 3.9170 - accuracy: 0.04 - ETA: 5s - loss: 3.9159 - accuracy: 0.04 - ETA: 5s - loss: 3.9144 - accuracy: 0.04 - ETA: 4s - loss: 3.9136 - accuracy: 0.04 - ETA: 3s - loss: 3.9116 - accuracy: 0.04 - ETA: 3s - loss: 3.9097 - accuracy: 0.04 - ETA: 2s - loss: 3.9090 - accuracy: 0.04 - ETA: 2s - loss: 3.9079 - accuracy: 0.04 - ETA: 1s - loss: 3.9066 - accuracy: 0.04 - ETA: 1s - loss: 3.9045 - accuracy: 0.04 - ETA: 0s - loss: 3.9034 - accuracy: 0.04 - 67s 5ms/step - loss: 3.9028 - accuracy: 0.0425 - val_loss: 3.8793 - val_accuracy: 0.0274\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:02 - loss: 3.7618 - accuracy: 0.06 - ETA: 1:00 - loss: 3.7664 - accuracy: 0.07 - ETA: 59s - loss: 3.7472 - accuracy: 0.0677 - ETA: 58s - loss: 3.7472 - accuracy: 0.066 - ETA: 57s - loss: 3.7255 - accuracy: 0.068 - ETA: 56s - loss: 3.7157 - accuracy: 0.070 - ETA: 55s - loss: 3.7054 - accuracy: 0.072 - ETA: 54s - loss: 3.7274 - accuracy: 0.066 - ETA: 53s - loss: 3.7260 - accuracy: 0.063 - ETA: 52s - loss: 3.7110 - accuracy: 0.061 - ETA: 52s - loss: 3.7055 - accuracy: 0.063 - ETA: 51s - loss: 3.7117 - accuracy: 0.062 - ETA: 51s - loss: 3.7127 - accuracy: 0.063 - ETA: 50s - loss: 3.7107 - accuracy: 0.063 - ETA: 49s - loss: 3.7059 - accuracy: 0.063 - ETA: 49s - loss: 3.7069 - accuracy: 0.063 - ETA: 48s - loss: 3.7124 - accuracy: 0.062 - ETA: 48s - loss: 3.7090 - accuracy: 0.063 - ETA: 47s - loss: 3.7123 - accuracy: 0.064 - ETA: 47s - loss: 3.7091 - accuracy: 0.066 - ETA: 46s - loss: 3.7074 - accuracy: 0.067 - ETA: 46s - loss: 3.7082 - accuracy: 0.066 - ETA: 45s - loss: 3.7073 - accuracy: 0.066 - ETA: 45s - loss: 3.7001 - accuracy: 0.069 - ETA: 44s - loss: 3.6970 - accuracy: 0.070 - ETA: 43s - loss: 3.6934 - accuracy: 0.070 - ETA: 43s - loss: 3.6874 - accuracy: 0.071 - ETA: 42s - loss: 3.6853 - accuracy: 0.071 - ETA: 42s - loss: 3.6823 - accuracy: 0.072 - ETA: 41s - loss: 3.6808 - accuracy: 0.072 - ETA: 41s - loss: 3.6784 - accuracy: 0.074 - ETA: 40s - loss: 3.6775 - accuracy: 0.075 - ETA: 40s - loss: 3.6711 - accuracy: 0.076 - ETA: 39s - loss: 3.6683 - accuracy: 0.077 - ETA: 39s - loss: 3.6641 - accuracy: 0.078 - ETA: 38s - loss: 3.6616 - accuracy: 0.079 - ETA: 37s - loss: 3.6610 - accuracy: 0.078 - ETA: 37s - loss: 3.6587 - accuracy: 0.080 - ETA: 36s - loss: 3.6575 - accuracy: 0.080 - ETA: 36s - loss: 3.6540 - accuracy: 0.081 - ETA: 35s - loss: 3.6556 - accuracy: 0.080 - ETA: 34s - loss: 3.6512 - accuracy: 0.082 - ETA: 34s - loss: 3.6509 - accuracy: 0.082 - ETA: 33s - loss: 3.6441 - accuracy: 0.083 - ETA: 33s - loss: 3.6423 - accuracy: 0.083 - ETA: 32s - loss: 3.6416 - accuracy: 0.084 - ETA: 32s - loss: 3.6407 - accuracy: 0.085 - ETA: 31s - loss: 3.6387 - accuracy: 0.085 - ETA: 30s - loss: 3.6346 - accuracy: 0.085 - ETA: 30s - loss: 3.6312 - accuracy: 0.086 - ETA: 29s - loss: 3.6269 - accuracy: 0.086 - ETA: 29s - loss: 3.6240 - accuracy: 0.087 - ETA: 28s - loss: 3.6191 - accuracy: 0.088 - ETA: 27s - loss: 3.6194 - accuracy: 0.089 - ETA: 27s - loss: 3.6151 - accuracy: 0.089 - ETA: 26s - loss: 3.6136 - accuracy: 0.090 - ETA: 26s - loss: 3.6077 - accuracy: 0.091 - ETA: 25s - loss: 3.6061 - accuracy: 0.091 - ETA: 25s - loss: 3.6042 - accuracy: 0.092 - ETA: 24s - loss: 3.6022 - accuracy: 0.093 - ETA: 23s - loss: 3.6009 - accuracy: 0.093 - ETA: 23s - loss: 3.5992 - accuracy: 0.093 - ETA: 22s - loss: 3.5988 - accuracy: 0.094 - ETA: 22s - loss: 3.5959 - accuracy: 0.095 - ETA: 21s - loss: 3.5959 - accuracy: 0.095 - ETA: 20s - loss: 3.5934 - accuracy: 0.095 - ETA: 20s - loss: 3.5933 - accuracy: 0.095 - ETA: 19s - loss: 3.5908 - accuracy: 0.096 - ETA: 19s - loss: 3.5885 - accuracy: 0.096 - ETA: 18s - loss: 3.5870 - accuracy: 0.097 - ETA: 17s - loss: 3.5826 - accuracy: 0.098 - ETA: 17s - loss: 3.5805 - accuracy: 0.099 - ETA: 16s - loss: 3.5803 - accuracy: 0.099 - ETA: 16s - loss: 3.5794 - accuracy: 0.099 - ETA: 15s - loss: 3.5799 - accuracy: 0.099 - ETA: 15s - loss: 3.5758 - accuracy: 0.101 - ETA: 14s - loss: 3.5732 - accuracy: 0.101 - ETA: 13s - loss: 3.5710 - accuracy: 0.102 - ETA: 13s - loss: 3.5672 - accuracy: 0.103 - ETA: 12s - loss: 3.5651 - accuracy: 0.103 - ETA: 12s - loss: 3.5619 - accuracy: 0.104 - ETA: 11s - loss: 3.5580 - accuracy: 0.105 - ETA: 10s - loss: 3.5547 - accuracy: 0.106 - ETA: 10s - loss: 3.5525 - accuracy: 0.107 - ETA: 9s - loss: 3.5497 - accuracy: 0.108 - ETA: 9s - loss: 3.5478 - accuracy: 0.10 - ETA: 8s - loss: 3.5470 - accuracy: 0.10 - ETA: 8s - loss: 3.5431 - accuracy: 0.11 - ETA: 7s - loss: 3.5392 - accuracy: 0.11 - ETA: 6s - loss: 3.5381 - accuracy: 0.11 - ETA: 6s - loss: 3.5359 - accuracy: 0.11 - ETA: 5s - loss: 3.5330 - accuracy: 0.11 - ETA: 5s - loss: 3.5324 - accuracy: 0.11 - ETA: 4s - loss: 3.5290 - accuracy: 0.11 - ETA: 3s - loss: 3.5242 - accuracy: 0.11 - ETA: 3s - loss: 3.5221 - accuracy: 0.11 - ETA: 2s - loss: 3.5202 - accuracy: 0.11 - ETA: 2s - loss: 3.5193 - accuracy: 0.11 - ETA: 1s - loss: 3.5162 - accuracy: 0.11 - ETA: 1s - loss: 3.5133 - accuracy: 0.11 - ETA: 0s - loss: 3.5132 - accuracy: 0.11 - 67s 5ms/step - loss: 3.5121 - accuracy: 0.1183 - val_loss: 3.6525 - val_accuracy: 0.0931\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 58s - loss: 3.0263 - accuracy: 0.234 - ETA: 56s - loss: 3.1812 - accuracy: 0.187 - ETA: 55s - loss: 3.1571 - accuracy: 0.200 - ETA: 55s - loss: 3.1734 - accuracy: 0.195 - ETA: 56s - loss: 3.1532 - accuracy: 0.200 - ETA: 56s - loss: 3.1314 - accuracy: 0.209 - ETA: 55s - loss: 3.1363 - accuracy: 0.205 - ETA: 55s - loss: 3.1399 - accuracy: 0.204 - ETA: 54s - loss: 3.1364 - accuracy: 0.201 - ETA: 53s - loss: 3.1582 - accuracy: 0.193 - ETA: 52s - loss: 3.1813 - accuracy: 0.187 - ETA: 52s - loss: 3.1920 - accuracy: 0.182 - ETA: 51s - loss: 3.1923 - accuracy: 0.183 - ETA: 51s - loss: 3.1870 - accuracy: 0.184 - ETA: 50s - loss: 3.1774 - accuracy: 0.185 - ETA: 49s - loss: 3.1735 - accuracy: 0.186 - ETA: 49s - loss: 3.1666 - accuracy: 0.188 - ETA: 48s - loss: 3.1805 - accuracy: 0.185 - ETA: 48s - loss: 3.1711 - accuracy: 0.188 - ETA: 47s - loss: 3.1660 - accuracy: 0.187 - ETA: 47s - loss: 3.1584 - accuracy: 0.188 - ETA: 46s - loss: 3.1532 - accuracy: 0.188 - ETA: 46s - loss: 3.1439 - accuracy: 0.191 - ETA: 45s - loss: 3.1498 - accuracy: 0.189 - ETA: 45s - loss: 3.1561 - accuracy: 0.188 - ETA: 44s - loss: 3.1526 - accuracy: 0.188 - ETA: 43s - loss: 3.1441 - accuracy: 0.190 - ETA: 43s - loss: 3.1332 - accuracy: 0.192 - ETA: 42s - loss: 3.1316 - accuracy: 0.192 - ETA: 42s - loss: 3.1295 - accuracy: 0.194 - ETA: 41s - loss: 3.1272 - accuracy: 0.194 - ETA: 40s - loss: 3.1244 - accuracy: 0.195 - ETA: 40s - loss: 3.1202 - accuracy: 0.196 - ETA: 39s - loss: 3.1219 - accuracy: 0.197 - ETA: 39s - loss: 3.1184 - accuracy: 0.197 - ETA: 38s - loss: 3.1224 - accuracy: 0.196 - ETA: 37s - loss: 3.1239 - accuracy: 0.197 - ETA: 37s - loss: 3.1248 - accuracy: 0.197 - ETA: 36s - loss: 3.1282 - accuracy: 0.196 - ETA: 36s - loss: 3.1264 - accuracy: 0.197 - ETA: 35s - loss: 3.1288 - accuracy: 0.197 - ETA: 34s - loss: 3.1268 - accuracy: 0.198 - ETA: 34s - loss: 3.1259 - accuracy: 0.197 - ETA: 33s - loss: 3.1244 - accuracy: 0.198 - ETA: 33s - loss: 3.1276 - accuracy: 0.197 - ETA: 32s - loss: 3.1289 - accuracy: 0.196 - ETA: 31s - loss: 3.1232 - accuracy: 0.197 - ETA: 31s - loss: 3.1284 - accuracy: 0.195 - ETA: 30s - loss: 3.1298 - accuracy: 0.195 - ETA: 30s - loss: 3.1322 - accuracy: 0.194 - ETA: 29s - loss: 3.1263 - accuracy: 0.195 - ETA: 29s - loss: 3.1252 - accuracy: 0.195 - ETA: 28s - loss: 3.1218 - accuracy: 0.195 - ETA: 27s - loss: 3.1226 - accuracy: 0.195 - ETA: 27s - loss: 3.1207 - accuracy: 0.195 - ETA: 26s - loss: 3.1209 - accuracy: 0.195 - ETA: 26s - loss: 3.1187 - accuracy: 0.195 - ETA: 25s - loss: 3.1170 - accuracy: 0.196 - ETA: 24s - loss: 3.1140 - accuracy: 0.196 - ETA: 24s - loss: 3.1141 - accuracy: 0.196 - ETA: 23s - loss: 3.1091 - accuracy: 0.198 - ETA: 23s - loss: 3.1089 - accuracy: 0.197 - ETA: 22s - loss: 3.1064 - accuracy: 0.197 - ETA: 22s - loss: 3.1072 - accuracy: 0.197 - ETA: 21s - loss: 3.1074 - accuracy: 0.197 - ETA: 20s - loss: 3.1097 - accuracy: 0.196 - ETA: 20s - loss: 3.1074 - accuracy: 0.197 - ETA: 19s - loss: 3.1069 - accuracy: 0.197 - ETA: 19s - loss: 3.1033 - accuracy: 0.198 - ETA: 18s - loss: 3.1022 - accuracy: 0.199 - ETA: 17s - loss: 3.0972 - accuracy: 0.199 - ETA: 17s - loss: 3.0933 - accuracy: 0.200 - ETA: 16s - loss: 3.0937 - accuracy: 0.200 - ETA: 16s - loss: 3.0942 - accuracy: 0.199 - ETA: 15s - loss: 3.0917 - accuracy: 0.200 - ETA: 15s - loss: 3.0919 - accuracy: 0.199 - ETA: 14s - loss: 3.0923 - accuracy: 0.199 - ETA: 13s - loss: 3.0871 - accuracy: 0.201 - ETA: 13s - loss: 3.0865 - accuracy: 0.201 - ETA: 12s - loss: 3.0879 - accuracy: 0.201 - ETA: 12s - loss: 3.0870 - accuracy: 0.201 - ETA: 11s - loss: 3.0850 - accuracy: 0.202 - ETA: 10s - loss: 3.0838 - accuracy: 0.202 - ETA: 10s - loss: 3.0796 - accuracy: 0.203 - ETA: 9s - loss: 3.0752 - accuracy: 0.204 - ETA: 9s - loss: 3.0734 - accuracy: 0.20 - ETA: 8s - loss: 3.0700 - accuracy: 0.20 - ETA: 8s - loss: 3.0672 - accuracy: 0.20 - ETA: 7s - loss: 3.0669 - accuracy: 0.20 - ETA: 6s - loss: 3.0632 - accuracy: 0.20 - ETA: 6s - loss: 3.0596 - accuracy: 0.20 - ETA: 5s - loss: 3.0548 - accuracy: 0.21 - ETA: 5s - loss: 3.0500 - accuracy: 0.21 - ETA: 4s - loss: 3.0493 - accuracy: 0.21 - ETA: 3s - loss: 3.0474 - accuracy: 0.21 - ETA: 3s - loss: 3.0451 - accuracy: 0.21 - ETA: 2s - loss: 3.0454 - accuracy: 0.21 - ETA: 2s - loss: 3.0419 - accuracy: 0.21 - ETA: 1s - loss: 3.0422 - accuracy: 0.21 - ETA: 1s - loss: 3.0405 - accuracy: 0.21 - ETA: 0s - loss: 3.0385 - accuracy: 0.21 - 67s 5ms/step - loss: 3.0367 - accuracy: 0.2136 - val_loss: 3.4753 - val_accuracy: 0.1027\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 55s - loss: 2.6057 - accuracy: 0.265 - ETA: 55s - loss: 2.7098 - accuracy: 0.253 - ETA: 56s - loss: 2.7304 - accuracy: 0.257 - ETA: 56s - loss: 2.8001 - accuracy: 0.248 - ETA: 55s - loss: 2.7928 - accuracy: 0.259 - ETA: 55s - loss: 2.8362 - accuracy: 0.251 - ETA: 54s - loss: 2.8394 - accuracy: 0.246 - ETA: 53s - loss: 2.8160 - accuracy: 0.252 - ETA: 53s - loss: 2.8263 - accuracy: 0.251 - ETA: 52s - loss: 2.7854 - accuracy: 0.266 - ETA: 52s - loss: 2.7959 - accuracy: 0.265 - ETA: 51s - loss: 2.8023 - accuracy: 0.264 - ETA: 51s - loss: 2.8276 - accuracy: 0.259 - ETA: 50s - loss: 2.8307 - accuracy: 0.260 - ETA: 50s - loss: 2.8292 - accuracy: 0.257 - ETA: 50s - loss: 2.8362 - accuracy: 0.253 - ETA: 49s - loss: 2.8258 - accuracy: 0.256 - ETA: 49s - loss: 2.8152 - accuracy: 0.259 - ETA: 49s - loss: 2.8100 - accuracy: 0.261 - ETA: 48s - loss: 2.8058 - accuracy: 0.262 - ETA: 48s - loss: 2.8087 - accuracy: 0.262 - ETA: 48s - loss: 2.8095 - accuracy: 0.260 - ETA: 47s - loss: 2.8060 - accuracy: 0.261 - ETA: 47s - loss: 2.7937 - accuracy: 0.263 - ETA: 47s - loss: 2.7833 - accuracy: 0.267 - ETA: 46s - loss: 2.7799 - accuracy: 0.269 - ETA: 46s - loss: 2.7750 - accuracy: 0.270 - ETA: 45s - loss: 2.7760 - accuracy: 0.269 - ETA: 45s - loss: 2.7747 - accuracy: 0.269 - ETA: 44s - loss: 2.7770 - accuracy: 0.267 - ETA: 44s - loss: 2.7792 - accuracy: 0.267 - ETA: 44s - loss: 2.7829 - accuracy: 0.265 - ETA: 43s - loss: 2.7830 - accuracy: 0.265 - ETA: 43s - loss: 2.7795 - accuracy: 0.267 - ETA: 42s - loss: 2.7716 - accuracy: 0.269 - ETA: 42s - loss: 2.7715 - accuracy: 0.269 - ETA: 42s - loss: 2.7748 - accuracy: 0.269 - ETA: 41s - loss: 2.7785 - accuracy: 0.269 - ETA: 41s - loss: 2.7722 - accuracy: 0.270 - ETA: 40s - loss: 2.7725 - accuracy: 0.269 - ETA: 40s - loss: 2.7730 - accuracy: 0.267 - ETA: 39s - loss: 2.7702 - accuracy: 0.269 - ETA: 39s - loss: 2.7685 - accuracy: 0.268 - ETA: 38s - loss: 2.7677 - accuracy: 0.269 - ETA: 38s - loss: 2.7653 - accuracy: 0.270 - ETA: 37s - loss: 2.7637 - accuracy: 0.272 - ETA: 36s - loss: 2.7621 - accuracy: 0.272 - ETA: 36s - loss: 2.7603 - accuracy: 0.272 - ETA: 35s - loss: 2.7569 - accuracy: 0.273 - ETA: 35s - loss: 2.7536 - accuracy: 0.274 - ETA: 34s - loss: 2.7485 - accuracy: 0.275 - ETA: 33s - loss: 2.7445 - accuracy: 0.275 - ETA: 32s - loss: 2.7415 - accuracy: 0.276 - ETA: 32s - loss: 2.7422 - accuracy: 0.276 - ETA: 31s - loss: 2.7429 - accuracy: 0.275 - ETA: 30s - loss: 2.7371 - accuracy: 0.276 - ETA: 30s - loss: 2.7354 - accuracy: 0.277 - ETA: 29s - loss: 2.7319 - accuracy: 0.278 - ETA: 28s - loss: 2.7340 - accuracy: 0.277 - ETA: 28s - loss: 2.7288 - accuracy: 0.279 - ETA: 27s - loss: 2.7274 - accuracy: 0.279 - ETA: 26s - loss: 2.7244 - accuracy: 0.280 - ETA: 26s - loss: 2.7234 - accuracy: 0.280 - ETA: 25s - loss: 2.7219 - accuracy: 0.280 - ETA: 25s - loss: 2.7229 - accuracy: 0.281 - ETA: 24s - loss: 2.7200 - accuracy: 0.281 - ETA: 23s - loss: 2.7172 - accuracy: 0.282 - ETA: 23s - loss: 2.7194 - accuracy: 0.281 - ETA: 22s - loss: 2.7154 - accuracy: 0.281 - ETA: 21s - loss: 2.7175 - accuracy: 0.281 - ETA: 20s - loss: 2.7153 - accuracy: 0.282 - ETA: 20s - loss: 2.7101 - accuracy: 0.283 - ETA: 19s - loss: 2.7083 - accuracy: 0.283 - ETA: 18s - loss: 2.7097 - accuracy: 0.283 - ETA: 18s - loss: 2.7061 - accuracy: 0.284 - ETA: 17s - loss: 2.7040 - accuracy: 0.284 - ETA: 16s - loss: 2.7037 - accuracy: 0.285 - ETA: 16s - loss: 2.7025 - accuracy: 0.285 - ETA: 15s - loss: 2.7002 - accuracy: 0.286 - ETA: 14s - loss: 2.6982 - accuracy: 0.286 - ETA: 14s - loss: 2.6995 - accuracy: 0.285 - ETA: 13s - loss: 2.6968 - accuracy: 0.287 - ETA: 12s - loss: 2.6954 - accuracy: 0.287 - ETA: 12s - loss: 2.6963 - accuracy: 0.286 - ETA: 11s - loss: 2.6952 - accuracy: 0.286 - ETA: 10s - loss: 2.6933 - accuracy: 0.287 - ETA: 10s - loss: 2.6923 - accuracy: 0.286 - ETA: 9s - loss: 2.6899 - accuracy: 0.287 - ETA: 8s - loss: 2.6868 - accuracy: 0.28 - ETA: 8s - loss: 2.6856 - accuracy: 0.28 - ETA: 7s - loss: 2.6831 - accuracy: 0.28 - ETA: 6s - loss: 2.6816 - accuracy: 0.28 - ETA: 5s - loss: 2.6785 - accuracy: 0.28 - ETA: 5s - loss: 2.6800 - accuracy: 0.28 - ETA: 4s - loss: 2.6788 - accuracy: 0.28 - ETA: 3s - loss: 2.6768 - accuracy: 0.29 - ETA: 3s - loss: 2.6742 - accuracy: 0.29 - ETA: 2s - loss: 2.6737 - accuracy: 0.29 - ETA: 1s - loss: 2.6706 - accuracy: 0.29 - ETA: 1s - loss: 2.6685 - accuracy: 0.29 - ETA: 0s - loss: 2.6663 - accuracy: 0.29 - 84s 6ms/step - loss: 2.6651 - accuracy: 0.2925 - val_loss: 3.3361 - val_accuracy: 0.1419\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:42 - loss: 2.5574 - accuracy: 0.26 - ETA: 1:39 - loss: 2.5705 - accuracy: 0.27 - ETA: 1:36 - loss: 2.5853 - accuracy: 0.28 - ETA: 1:33 - loss: 2.5836 - accuracy: 0.29 - ETA: 1:31 - loss: 2.5875 - accuracy: 0.30 - ETA: 1:29 - loss: 2.5053 - accuracy: 0.32 - ETA: 1:28 - loss: 2.5145 - accuracy: 0.32 - ETA: 1:27 - loss: 2.5411 - accuracy: 0.31 - ETA: 1:26 - loss: 2.5150 - accuracy: 0.32 - ETA: 1:25 - loss: 2.5068 - accuracy: 0.32 - ETA: 1:23 - loss: 2.5096 - accuracy: 0.32 - ETA: 1:22 - loss: 2.4935 - accuracy: 0.32 - ETA: 1:21 - loss: 2.4908 - accuracy: 0.32 - ETA: 1:20 - loss: 2.4984 - accuracy: 0.32 - ETA: 1:19 - loss: 2.5116 - accuracy: 0.32 - ETA: 1:18 - loss: 2.5160 - accuracy: 0.31 - ETA: 1:17 - loss: 2.5118 - accuracy: 0.32 - ETA: 1:16 - loss: 2.5170 - accuracy: 0.32 - ETA: 1:15 - loss: 2.5198 - accuracy: 0.32 - ETA: 1:14 - loss: 2.5042 - accuracy: 0.32 - ETA: 1:13 - loss: 2.4992 - accuracy: 0.33 - ETA: 1:12 - loss: 2.4982 - accuracy: 0.32 - ETA: 1:11 - loss: 2.4914 - accuracy: 0.33 - ETA: 1:11 - loss: 2.5016 - accuracy: 0.32 - ETA: 1:10 - loss: 2.5007 - accuracy: 0.32 - ETA: 1:09 - loss: 2.5048 - accuracy: 0.32 - ETA: 1:08 - loss: 2.4975 - accuracy: 0.32 - ETA: 1:07 - loss: 2.4842 - accuracy: 0.33 - ETA: 1:06 - loss: 2.4786 - accuracy: 0.33 - ETA: 1:05 - loss: 2.4767 - accuracy: 0.33 - ETA: 1:04 - loss: 2.4750 - accuracy: 0.33 - ETA: 1:03 - loss: 2.4737 - accuracy: 0.33 - ETA: 1:02 - loss: 2.4739 - accuracy: 0.33 - ETA: 1:01 - loss: 2.4704 - accuracy: 0.33 - ETA: 1:01 - loss: 2.4658 - accuracy: 0.33 - ETA: 1:00 - loss: 2.4682 - accuracy: 0.33 - ETA: 59s - loss: 2.4616 - accuracy: 0.3378 - ETA: 58s - loss: 2.4561 - accuracy: 0.339 - ETA: 57s - loss: 2.4526 - accuracy: 0.340 - ETA: 56s - loss: 2.4469 - accuracy: 0.342 - ETA: 55s - loss: 2.4509 - accuracy: 0.340 - ETA: 54s - loss: 2.4515 - accuracy: 0.341 - ETA: 54s - loss: 2.4511 - accuracy: 0.341 - ETA: 53s - loss: 2.4570 - accuracy: 0.339 - ETA: 52s - loss: 2.4535 - accuracy: 0.339 - ETA: 51s - loss: 2.4470 - accuracy: 0.341 - ETA: 50s - loss: 2.4425 - accuracy: 0.343 - ETA: 50s - loss: 2.4487 - accuracy: 0.342 - ETA: 49s - loss: 2.4482 - accuracy: 0.342 - ETA: 48s - loss: 2.4456 - accuracy: 0.344 - ETA: 47s - loss: 2.4420 - accuracy: 0.344 - ETA: 46s - loss: 2.4404 - accuracy: 0.345 - ETA: 45s - loss: 2.4415 - accuracy: 0.344 - ETA: 44s - loss: 2.4383 - accuracy: 0.345 - ETA: 44s - loss: 2.4375 - accuracy: 0.346 - ETA: 43s - loss: 2.4330 - accuracy: 0.347 - ETA: 42s - loss: 2.4320 - accuracy: 0.348 - ETA: 41s - loss: 2.4318 - accuracy: 0.348 - ETA: 40s - loss: 2.4257 - accuracy: 0.350 - ETA: 39s - loss: 2.4236 - accuracy: 0.350 - ETA: 38s - loss: 2.4192 - accuracy: 0.352 - ETA: 37s - loss: 2.4167 - accuracy: 0.352 - ETA: 36s - loss: 2.4105 - accuracy: 0.353 - ETA: 35s - loss: 2.4103 - accuracy: 0.354 - ETA: 34s - loss: 2.4097 - accuracy: 0.354 - ETA: 33s - loss: 2.4076 - accuracy: 0.355 - ETA: 32s - loss: 2.4079 - accuracy: 0.355 - ETA: 31s - loss: 2.4061 - accuracy: 0.355 - ETA: 30s - loss: 2.4046 - accuracy: 0.356 - ETA: 29s - loss: 2.4028 - accuracy: 0.356 - ETA: 28s - loss: 2.4023 - accuracy: 0.356 - ETA: 27s - loss: 2.3998 - accuracy: 0.357 - ETA: 26s - loss: 2.3997 - accuracy: 0.357 - ETA: 25s - loss: 2.3978 - accuracy: 0.356 - ETA: 24s - loss: 2.3992 - accuracy: 0.356 - ETA: 24s - loss: 2.4003 - accuracy: 0.357 - ETA: 23s - loss: 2.3951 - accuracy: 0.357 - ETA: 22s - loss: 2.3951 - accuracy: 0.357 - ETA: 21s - loss: 2.3926 - accuracy: 0.358 - ETA: 20s - loss: 2.3901 - accuracy: 0.358 - ETA: 19s - loss: 2.3900 - accuracy: 0.358 - ETA: 18s - loss: 2.3890 - accuracy: 0.358 - ETA: 17s - loss: 2.3891 - accuracy: 0.358 - ETA: 16s - loss: 2.3869 - accuracy: 0.358 - ETA: 15s - loss: 2.3859 - accuracy: 0.359 - ETA: 14s - loss: 2.3861 - accuracy: 0.358 - ETA: 13s - loss: 2.3861 - accuracy: 0.358 - ETA: 12s - loss: 2.3867 - accuracy: 0.358 - ETA: 11s - loss: 2.3854 - accuracy: 0.359 - ETA: 10s - loss: 2.3883 - accuracy: 0.358 - ETA: 9s - loss: 2.3904 - accuracy: 0.358 - ETA: 9s - loss: 2.3863 - accuracy: 0.35 - ETA: 8s - loss: 2.3850 - accuracy: 0.36 - ETA: 7s - loss: 2.3840 - accuracy: 0.36 - ETA: 6s - loss: 2.3845 - accuracy: 0.36 - ETA: 5s - loss: 2.3835 - accuracy: 0.36 - ETA: 4s - loss: 2.3827 - accuracy: 0.36 - ETA: 3s - loss: 2.3834 - accuracy: 0.35 - ETA: 2s - loss: 2.3835 - accuracy: 0.35 - ETA: 1s - loss: 2.3821 - accuracy: 0.36 - ETA: 0s - loss: 2.3813 - accuracy: 0.36 - 108s 8ms/step - loss: 2.3807 - accuracy: 0.3605 - val_loss: 3.1267 - val_accuracy: 0.1857\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:37 - loss: 2.1918 - accuracy: 0.39 - ETA: 1:36 - loss: 2.2289 - accuracy: 0.39 - ETA: 1:35 - loss: 2.1860 - accuracy: 0.41 - ETA: 1:34 - loss: 2.1187 - accuracy: 0.43 - ETA: 1:32 - loss: 2.1400 - accuracy: 0.42 - ETA: 1:30 - loss: 2.1899 - accuracy: 0.40 - ETA: 1:29 - loss: 2.1844 - accuracy: 0.41 - ETA: 1:27 - loss: 2.1858 - accuracy: 0.41 - ETA: 1:26 - loss: 2.1708 - accuracy: 0.41 - ETA: 1:24 - loss: 2.1930 - accuracy: 0.40 - ETA: 1:24 - loss: 2.2140 - accuracy: 0.39 - ETA: 1:23 - loss: 2.1933 - accuracy: 0.40 - ETA: 1:22 - loss: 2.2230 - accuracy: 0.40 - ETA: 1:21 - loss: 2.2302 - accuracy: 0.40 - ETA: 1:20 - loss: 2.2459 - accuracy: 0.39 - ETA: 1:19 - loss: 2.2433 - accuracy: 0.39 - ETA: 1:18 - loss: 2.2367 - accuracy: 0.39 - ETA: 1:17 - loss: 2.2241 - accuracy: 0.40 - ETA: 1:16 - loss: 2.2323 - accuracy: 0.40 - ETA: 1:16 - loss: 2.2264 - accuracy: 0.40 - ETA: 1:15 - loss: 2.2297 - accuracy: 0.39 - ETA: 1:14 - loss: 2.2275 - accuracy: 0.39 - ETA: 1:13 - loss: 2.2228 - accuracy: 0.40 - ETA: 1:12 - loss: 2.2076 - accuracy: 0.40 - ETA: 1:11 - loss: 2.2159 - accuracy: 0.39 - ETA: 1:10 - loss: 2.2208 - accuracy: 0.39 - ETA: 1:09 - loss: 2.2223 - accuracy: 0.39 - ETA: 1:08 - loss: 2.2233 - accuracy: 0.39 - ETA: 1:07 - loss: 2.2184 - accuracy: 0.39 - ETA: 1:06 - loss: 2.2188 - accuracy: 0.39 - ETA: 1:05 - loss: 2.2304 - accuracy: 0.39 - ETA: 1:04 - loss: 2.2270 - accuracy: 0.39 - ETA: 1:03 - loss: 2.2270 - accuracy: 0.39 - ETA: 1:02 - loss: 2.2269 - accuracy: 0.39 - ETA: 1:01 - loss: 2.2246 - accuracy: 0.39 - ETA: 1:00 - loss: 2.2231 - accuracy: 0.39 - ETA: 59s - loss: 2.2176 - accuracy: 0.3970 - ETA: 58s - loss: 2.2185 - accuracy: 0.396 - ETA: 57s - loss: 2.2136 - accuracy: 0.396 - ETA: 57s - loss: 2.2203 - accuracy: 0.395 - ETA: 56s - loss: 2.2131 - accuracy: 0.397 - ETA: 55s - loss: 2.2134 - accuracy: 0.398 - ETA: 54s - loss: 2.2083 - accuracy: 0.399 - ETA: 53s - loss: 2.2070 - accuracy: 0.399 - ETA: 52s - loss: 2.2013 - accuracy: 0.399 - ETA: 51s - loss: 2.2070 - accuracy: 0.397 - ETA: 50s - loss: 2.2025 - accuracy: 0.398 - ETA: 49s - loss: 2.1998 - accuracy: 0.399 - ETA: 48s - loss: 2.1993 - accuracy: 0.399 - ETA: 47s - loss: 2.1942 - accuracy: 0.400 - ETA: 46s - loss: 2.1976 - accuracy: 0.400 - ETA: 45s - loss: 2.1989 - accuracy: 0.399 - ETA: 44s - loss: 2.2029 - accuracy: 0.397 - ETA: 43s - loss: 2.1999 - accuracy: 0.398 - ETA: 43s - loss: 2.2007 - accuracy: 0.398 - ETA: 42s - loss: 2.2031 - accuracy: 0.397 - ETA: 41s - loss: 2.2018 - accuracy: 0.398 - ETA: 40s - loss: 2.2010 - accuracy: 0.398 - ETA: 39s - loss: 2.1976 - accuracy: 0.398 - ETA: 38s - loss: 2.1984 - accuracy: 0.399 - ETA: 37s - loss: 2.1984 - accuracy: 0.399 - ETA: 36s - loss: 2.1979 - accuracy: 0.399 - ETA: 35s - loss: 2.1986 - accuracy: 0.399 - ETA: 34s - loss: 2.1952 - accuracy: 0.400 - ETA: 33s - loss: 2.1926 - accuracy: 0.400 - ETA: 32s - loss: 2.1901 - accuracy: 0.401 - ETA: 31s - loss: 2.1933 - accuracy: 0.400 - ETA: 30s - loss: 2.1904 - accuracy: 0.400 - ETA: 29s - loss: 2.1887 - accuracy: 0.401 - ETA: 29s - loss: 2.1863 - accuracy: 0.401 - ETA: 28s - loss: 2.1876 - accuracy: 0.401 - ETA: 27s - loss: 2.1935 - accuracy: 0.400 - ETA: 26s - loss: 2.1921 - accuracy: 0.400 - ETA: 25s - loss: 2.1917 - accuracy: 0.400 - ETA: 24s - loss: 2.1911 - accuracy: 0.399 - ETA: 23s - loss: 2.1869 - accuracy: 0.401 - ETA: 22s - loss: 2.1865 - accuracy: 0.401 - ETA: 21s - loss: 2.1851 - accuracy: 0.400 - ETA: 20s - loss: 2.1829 - accuracy: 0.401 - ETA: 19s - loss: 2.1789 - accuracy: 0.402 - ETA: 19s - loss: 2.1791 - accuracy: 0.401 - ETA: 18s - loss: 2.1784 - accuracy: 0.401 - ETA: 17s - loss: 2.1800 - accuracy: 0.401 - ETA: 16s - loss: 2.1803 - accuracy: 0.400 - ETA: 15s - loss: 2.1838 - accuracy: 0.399 - ETA: 14s - loss: 2.1831 - accuracy: 0.400 - ETA: 13s - loss: 2.1828 - accuracy: 0.400 - ETA: 12s - loss: 2.1846 - accuracy: 0.399 - ETA: 11s - loss: 2.1841 - accuracy: 0.399 - ETA: 10s - loss: 2.1827 - accuracy: 0.399 - ETA: 9s - loss: 2.1784 - accuracy: 0.400 - ETA: 8s - loss: 2.1766 - accuracy: 0.40 - ETA: 8s - loss: 2.1764 - accuracy: 0.40 - ETA: 7s - loss: 2.1769 - accuracy: 0.40 - ETA: 6s - loss: 2.1751 - accuracy: 0.40 - ETA: 5s - loss: 2.1730 - accuracy: 0.40 - ETA: 4s - loss: 2.1708 - accuracy: 0.40 - ETA: 3s - loss: 2.1718 - accuracy: 0.40 - ETA: 2s - loss: 2.1704 - accuracy: 0.40 - ETA: 1s - loss: 2.1693 - accuracy: 0.40 - ETA: 0s - loss: 2.1685 - accuracy: 0.40 - 107s 8ms/step - loss: 2.1678 - accuracy: 0.4050 - val_loss: 2.9718 - val_accuracy: 0.2022\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:34 - loss: 1.9878 - accuracy: 0.47 - ETA: 1:31 - loss: 2.0720 - accuracy: 0.46 - ETA: 1:32 - loss: 1.9699 - accuracy: 0.45 - ETA: 1:31 - loss: 2.0357 - accuracy: 0.43 - ETA: 1:30 - loss: 2.0317 - accuracy: 0.44 - ETA: 1:28 - loss: 2.0586 - accuracy: 0.43 - ETA: 1:27 - loss: 2.0694 - accuracy: 0.43 - ETA: 1:26 - loss: 2.0654 - accuracy: 0.43 - ETA: 1:25 - loss: 2.0331 - accuracy: 0.44 - ETA: 1:24 - loss: 2.0353 - accuracy: 0.43 - ETA: 1:23 - loss: 2.0387 - accuracy: 0.43 - ETA: 1:22 - loss: 2.0435 - accuracy: 0.43 - ETA: 1:22 - loss: 2.0447 - accuracy: 0.43 - ETA: 1:21 - loss: 2.0508 - accuracy: 0.43 - ETA: 1:20 - loss: 2.0446 - accuracy: 0.43 - ETA: 1:19 - loss: 2.0491 - accuracy: 0.43 - ETA: 1:18 - loss: 2.0489 - accuracy: 0.43 - ETA: 1:17 - loss: 2.0393 - accuracy: 0.43 - ETA: 1:16 - loss: 2.0396 - accuracy: 0.43 - ETA: 1:15 - loss: 2.0389 - accuracy: 0.43 - ETA: 1:14 - loss: 2.0381 - accuracy: 0.43 - ETA: 1:13 - loss: 2.0391 - accuracy: 0.43 - ETA: 1:13 - loss: 2.0375 - accuracy: 0.42 - ETA: 1:12 - loss: 2.0241 - accuracy: 0.43 - ETA: 1:11 - loss: 2.0247 - accuracy: 0.43 - ETA: 1:10 - loss: 2.0268 - accuracy: 0.43 - ETA: 1:09 - loss: 2.0366 - accuracy: 0.43 - ETA: 1:08 - loss: 2.0283 - accuracy: 0.43 - ETA: 1:07 - loss: 2.0247 - accuracy: 0.43 - ETA: 1:06 - loss: 2.0182 - accuracy: 0.43 - ETA: 1:05 - loss: 2.0225 - accuracy: 0.43 - ETA: 1:04 - loss: 2.0295 - accuracy: 0.43 - ETA: 1:03 - loss: 2.0290 - accuracy: 0.43 - ETA: 1:02 - loss: 2.0278 - accuracy: 0.43 - ETA: 1:01 - loss: 2.0262 - accuracy: 0.43 - ETA: 1:00 - loss: 2.0279 - accuracy: 0.43 - ETA: 59s - loss: 2.0251 - accuracy: 0.4398 - ETA: 59s - loss: 2.0290 - accuracy: 0.439 - ETA: 58s - loss: 2.0258 - accuracy: 0.439 - ETA: 57s - loss: 2.0225 - accuracy: 0.439 - ETA: 56s - loss: 2.0172 - accuracy: 0.440 - ETA: 55s - loss: 2.0150 - accuracy: 0.439 - ETA: 54s - loss: 2.0116 - accuracy: 0.440 - ETA: 53s - loss: 2.0129 - accuracy: 0.440 - ETA: 52s - loss: 2.0172 - accuracy: 0.439 - ETA: 51s - loss: 2.0149 - accuracy: 0.439 - ETA: 50s - loss: 2.0173 - accuracy: 0.439 - ETA: 49s - loss: 2.0161 - accuracy: 0.439 - ETA: 48s - loss: 2.0179 - accuracy: 0.438 - ETA: 47s - loss: 2.0135 - accuracy: 0.438 - ETA: 46s - loss: 2.0063 - accuracy: 0.440 - ETA: 45s - loss: 2.0053 - accuracy: 0.441 - ETA: 44s - loss: 2.0058 - accuracy: 0.441 - ETA: 43s - loss: 2.0057 - accuracy: 0.441 - ETA: 42s - loss: 2.0032 - accuracy: 0.441 - ETA: 42s - loss: 2.0037 - accuracy: 0.441 - ETA: 41s - loss: 2.0031 - accuracy: 0.441 - ETA: 40s - loss: 2.0052 - accuracy: 0.441 - ETA: 39s - loss: 2.0044 - accuracy: 0.442 - ETA: 38s - loss: 2.0069 - accuracy: 0.441 - ETA: 37s - loss: 2.0060 - accuracy: 0.441 - ETA: 36s - loss: 2.0040 - accuracy: 0.442 - ETA: 35s - loss: 2.0050 - accuracy: 0.442 - ETA: 34s - loss: 2.0029 - accuracy: 0.442 - ETA: 33s - loss: 2.0034 - accuracy: 0.442 - ETA: 32s - loss: 2.0034 - accuracy: 0.442 - ETA: 31s - loss: 2.0035 - accuracy: 0.442 - ETA: 30s - loss: 2.0026 - accuracy: 0.443 - ETA: 30s - loss: 2.0031 - accuracy: 0.443 - ETA: 29s - loss: 2.0047 - accuracy: 0.442 - ETA: 28s - loss: 2.0045 - accuracy: 0.443 - ETA: 27s - loss: 2.0031 - accuracy: 0.443 - ETA: 26s - loss: 2.0002 - accuracy: 0.444 - ETA: 25s - loss: 1.9985 - accuracy: 0.444 - ETA: 24s - loss: 1.9987 - accuracy: 0.444 - ETA: 23s - loss: 1.9962 - accuracy: 0.444 - ETA: 22s - loss: 1.9978 - accuracy: 0.444 - ETA: 21s - loss: 1.9980 - accuracy: 0.444 - ETA: 20s - loss: 1.9995 - accuracy: 0.444 - ETA: 20s - loss: 2.0001 - accuracy: 0.443 - ETA: 19s - loss: 2.0020 - accuracy: 0.443 - ETA: 18s - loss: 2.0028 - accuracy: 0.443 - ETA: 17s - loss: 2.0043 - accuracy: 0.443 - ETA: 16s - loss: 2.0037 - accuracy: 0.443 - ETA: 15s - loss: 2.0014 - accuracy: 0.443 - ETA: 14s - loss: 2.0005 - accuracy: 0.444 - ETA: 13s - loss: 1.9975 - accuracy: 0.444 - ETA: 12s - loss: 1.9949 - accuracy: 0.444 - ETA: 11s - loss: 1.9986 - accuracy: 0.443 - ETA: 10s - loss: 2.0004 - accuracy: 0.443 - ETA: 9s - loss: 2.0002 - accuracy: 0.443 - ETA: 9s - loss: 1.9984 - accuracy: 0.44 - ETA: 8s - loss: 1.9987 - accuracy: 0.44 - ETA: 7s - loss: 1.9983 - accuracy: 0.44 - ETA: 6s - loss: 1.9979 - accuracy: 0.44 - ETA: 5s - loss: 1.9978 - accuracy: 0.44 - ETA: 4s - loss: 1.9959 - accuracy: 0.44 - ETA: 3s - loss: 1.9953 - accuracy: 0.44 - ETA: 2s - loss: 1.9960 - accuracy: 0.44 - ETA: 1s - loss: 1.9946 - accuracy: 0.44 - ETA: 0s - loss: 1.9939 - accuracy: 0.44 - 110s 8ms/step - loss: 1.9924 - accuracy: 0.4457 - val_loss: 2.8512 - val_accuracy: 0.3276\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:35 - loss: 1.9167 - accuracy: 0.46 - ETA: 1:32 - loss: 2.0046 - accuracy: 0.44 - ETA: 1:33 - loss: 1.9900 - accuracy: 0.44 - ETA: 1:32 - loss: 1.9888 - accuracy: 0.45 - ETA: 1:32 - loss: 1.9729 - accuracy: 0.45 - ETA: 1:30 - loss: 1.9919 - accuracy: 0.44 - ETA: 1:28 - loss: 1.9735 - accuracy: 0.44 - ETA: 1:28 - loss: 1.9962 - accuracy: 0.43 - ETA: 1:26 - loss: 1.9976 - accuracy: 0.43 - ETA: 1:25 - loss: 1.9942 - accuracy: 0.43 - ETA: 1:24 - loss: 1.9800 - accuracy: 0.44 - ETA: 1:24 - loss: 1.9754 - accuracy: 0.44 - ETA: 1:23 - loss: 1.9752 - accuracy: 0.44 - ETA: 1:22 - loss: 1.9491 - accuracy: 0.45 - ETA: 1:20 - loss: 1.9610 - accuracy: 0.44 - ETA: 1:19 - loss: 1.9466 - accuracy: 0.44 - ETA: 1:18 - loss: 1.9461 - accuracy: 0.44 - ETA: 1:17 - loss: 1.9577 - accuracy: 0.44 - ETA: 1:16 - loss: 1.9407 - accuracy: 0.44 - ETA: 1:15 - loss: 1.9341 - accuracy: 0.44 - ETA: 1:14 - loss: 1.9271 - accuracy: 0.45 - ETA: 1:13 - loss: 1.9208 - accuracy: 0.45 - ETA: 1:12 - loss: 1.9210 - accuracy: 0.45 - ETA: 1:11 - loss: 1.9148 - accuracy: 0.45 - ETA: 1:10 - loss: 1.9185 - accuracy: 0.45 - ETA: 1:09 - loss: 1.9139 - accuracy: 0.45 - ETA: 1:08 - loss: 1.9190 - accuracy: 0.45 - ETA: 1:07 - loss: 1.9299 - accuracy: 0.45 - ETA: 1:06 - loss: 1.9283 - accuracy: 0.45 - ETA: 1:05 - loss: 1.9290 - accuracy: 0.45 - ETA: 1:04 - loss: 1.9236 - accuracy: 0.45 - ETA: 1:03 - loss: 1.9245 - accuracy: 0.45 - ETA: 1:02 - loss: 1.9197 - accuracy: 0.45 - ETA: 1:01 - loss: 1.9214 - accuracy: 0.45 - ETA: 1:00 - loss: 1.9145 - accuracy: 0.45 - ETA: 59s - loss: 1.9130 - accuracy: 0.4583 - ETA: 58s - loss: 1.9104 - accuracy: 0.459 - ETA: 58s - loss: 1.9070 - accuracy: 0.459 - ETA: 57s - loss: 1.9084 - accuracy: 0.459 - ETA: 56s - loss: 1.9090 - accuracy: 0.459 - ETA: 55s - loss: 1.9108 - accuracy: 0.459 - ETA: 54s - loss: 1.9057 - accuracy: 0.461 - ETA: 53s - loss: 1.9101 - accuracy: 0.460 - ETA: 52s - loss: 1.9086 - accuracy: 0.461 - ETA: 51s - loss: 1.9076 - accuracy: 0.462 - ETA: 50s - loss: 1.9109 - accuracy: 0.462 - ETA: 50s - loss: 1.9129 - accuracy: 0.461 - ETA: 49s - loss: 1.9077 - accuracy: 0.461 - ETA: 48s - loss: 1.9112 - accuracy: 0.461 - ETA: 47s - loss: 1.9152 - accuracy: 0.460 - ETA: 46s - loss: 1.9140 - accuracy: 0.461 - ETA: 45s - loss: 1.9134 - accuracy: 0.461 - ETA: 44s - loss: 1.9118 - accuracy: 0.462 - ETA: 43s - loss: 1.9081 - accuracy: 0.463 - ETA: 42s - loss: 1.9057 - accuracy: 0.463 - ETA: 41s - loss: 1.9019 - accuracy: 0.464 - ETA: 40s - loss: 1.9024 - accuracy: 0.465 - ETA: 39s - loss: 1.9020 - accuracy: 0.464 - ETA: 38s - loss: 1.9035 - accuracy: 0.465 - ETA: 38s - loss: 1.9038 - accuracy: 0.465 - ETA: 37s - loss: 1.9032 - accuracy: 0.465 - ETA: 36s - loss: 1.9036 - accuracy: 0.464 - ETA: 35s - loss: 1.9039 - accuracy: 0.465 - ETA: 34s - loss: 1.8999 - accuracy: 0.466 - ETA: 33s - loss: 1.8981 - accuracy: 0.467 - ETA: 32s - loss: 1.8964 - accuracy: 0.466 - ETA: 31s - loss: 1.8931 - accuracy: 0.468 - ETA: 30s - loss: 1.8943 - accuracy: 0.468 - ETA: 29s - loss: 1.8937 - accuracy: 0.469 - ETA: 28s - loss: 1.8899 - accuracy: 0.470 - ETA: 27s - loss: 1.8874 - accuracy: 0.471 - ETA: 26s - loss: 1.8898 - accuracy: 0.470 - ETA: 26s - loss: 1.8935 - accuracy: 0.469 - ETA: 25s - loss: 1.8922 - accuracy: 0.469 - ETA: 24s - loss: 1.8919 - accuracy: 0.470 - ETA: 23s - loss: 1.8953 - accuracy: 0.469 - ETA: 22s - loss: 1.8952 - accuracy: 0.469 - ETA: 21s - loss: 1.8947 - accuracy: 0.469 - ETA: 20s - loss: 1.8959 - accuracy: 0.469 - ETA: 19s - loss: 1.8953 - accuracy: 0.469 - ETA: 18s - loss: 1.8957 - accuracy: 0.469 - ETA: 17s - loss: 1.8974 - accuracy: 0.469 - ETA: 16s - loss: 1.8955 - accuracy: 0.469 - ETA: 16s - loss: 1.8967 - accuracy: 0.469 - ETA: 15s - loss: 1.8949 - accuracy: 0.470 - ETA: 14s - loss: 1.8960 - accuracy: 0.469 - ETA: 13s - loss: 1.8941 - accuracy: 0.470 - ETA: 12s - loss: 1.8940 - accuracy: 0.470 - ETA: 11s - loss: 1.8946 - accuracy: 0.470 - ETA: 10s - loss: 1.8929 - accuracy: 0.470 - ETA: 9s - loss: 1.8910 - accuracy: 0.471 - ETA: 8s - loss: 1.8913 - accuracy: 0.47 - ETA: 7s - loss: 1.8928 - accuracy: 0.47 - ETA: 7s - loss: 1.8920 - accuracy: 0.47 - ETA: 6s - loss: 1.8891 - accuracy: 0.47 - ETA: 5s - loss: 1.8872 - accuracy: 0.47 - ETA: 4s - loss: 1.8890 - accuracy: 0.47 - ETA: 3s - loss: 1.8910 - accuracy: 0.47 - ETA: 2s - loss: 1.8908 - accuracy: 0.47 - ETA: 1s - loss: 1.8906 - accuracy: 0.47 - ETA: 0s - loss: 1.8910 - accuracy: 0.47 - 106s 8ms/step - loss: 1.8901 - accuracy: 0.4715 - val_loss: 2.8171 - val_accuracy: 0.2790\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:34 - loss: 1.6548 - accuracy: 0.54 - ETA: 1:30 - loss: 1.6747 - accuracy: 0.51 - ETA: 1:28 - loss: 1.6546 - accuracy: 0.52 - ETA: 1:27 - loss: 1.6626 - accuracy: 0.52 - ETA: 1:27 - loss: 1.6693 - accuracy: 0.51 - ETA: 1:27 - loss: 1.6957 - accuracy: 0.50 - ETA: 1:27 - loss: 1.6758 - accuracy: 0.51 - ETA: 1:26 - loss: 1.6552 - accuracy: 0.51 - ETA: 1:24 - loss: 1.6809 - accuracy: 0.51 - ETA: 1:23 - loss: 1.6965 - accuracy: 0.50 - ETA: 1:22 - loss: 1.6965 - accuracy: 0.50 - ETA: 1:21 - loss: 1.7060 - accuracy: 0.50 - ETA: 1:20 - loss: 1.7007 - accuracy: 0.50 - ETA: 1:19 - loss: 1.6962 - accuracy: 0.50 - ETA: 1:18 - loss: 1.6914 - accuracy: 0.50 - ETA: 1:17 - loss: 1.7024 - accuracy: 0.50 - ETA: 1:16 - loss: 1.7105 - accuracy: 0.50 - ETA: 1:15 - loss: 1.7214 - accuracy: 0.50 - ETA: 1:14 - loss: 1.7268 - accuracy: 0.50 - ETA: 1:13 - loss: 1.7250 - accuracy: 0.49 - ETA: 1:12 - loss: 1.7215 - accuracy: 0.49 - ETA: 1:11 - loss: 1.7367 - accuracy: 0.49 - ETA: 1:10 - loss: 1.7404 - accuracy: 0.49 - ETA: 1:10 - loss: 1.7393 - accuracy: 0.49 - ETA: 1:09 - loss: 1.7267 - accuracy: 0.50 - ETA: 1:08 - loss: 1.7180 - accuracy: 0.50 - ETA: 1:07 - loss: 1.7217 - accuracy: 0.50 - ETA: 1:06 - loss: 1.7205 - accuracy: 0.50 - ETA: 1:05 - loss: 1.7235 - accuracy: 0.50 - ETA: 1:04 - loss: 1.7171 - accuracy: 0.50 - ETA: 1:03 - loss: 1.7150 - accuracy: 0.50 - ETA: 1:02 - loss: 1.7211 - accuracy: 0.50 - ETA: 1:01 - loss: 1.7256 - accuracy: 0.50 - ETA: 1:00 - loss: 1.7253 - accuracy: 0.49 - ETA: 59s - loss: 1.7260 - accuracy: 0.4996 - ETA: 59s - loss: 1.7236 - accuracy: 0.501 - ETA: 58s - loss: 1.7226 - accuracy: 0.502 - ETA: 57s - loss: 1.7210 - accuracy: 0.502 - ETA: 56s - loss: 1.7169 - accuracy: 0.504 - ETA: 55s - loss: 1.7130 - accuracy: 0.506 - ETA: 54s - loss: 1.7158 - accuracy: 0.505 - ETA: 53s - loss: 1.7230 - accuracy: 0.504 - ETA: 52s - loss: 1.7210 - accuracy: 0.504 - ETA: 51s - loss: 1.7219 - accuracy: 0.503 - ETA: 51s - loss: 1.7248 - accuracy: 0.503 - ETA: 50s - loss: 1.7234 - accuracy: 0.504 - ETA: 49s - loss: 1.7290 - accuracy: 0.502 - ETA: 48s - loss: 1.7304 - accuracy: 0.502 - ETA: 47s - loss: 1.7284 - accuracy: 0.501 - ETA: 46s - loss: 1.7240 - accuracy: 0.503 - ETA: 45s - loss: 1.7286 - accuracy: 0.501 - ETA: 44s - loss: 1.7298 - accuracy: 0.501 - ETA: 43s - loss: 1.7316 - accuracy: 0.501 - ETA: 42s - loss: 1.7305 - accuracy: 0.502 - ETA: 41s - loss: 1.7333 - accuracy: 0.501 - ETA: 41s - loss: 1.7329 - accuracy: 0.500 - ETA: 40s - loss: 1.7330 - accuracy: 0.500 - ETA: 39s - loss: 1.7355 - accuracy: 0.500 - ETA: 38s - loss: 1.7353 - accuracy: 0.500 - ETA: 37s - loss: 1.7340 - accuracy: 0.500 - ETA: 36s - loss: 1.7320 - accuracy: 0.501 - ETA: 35s - loss: 1.7355 - accuracy: 0.500 - ETA: 34s - loss: 1.7380 - accuracy: 0.500 - ETA: 33s - loss: 1.7385 - accuracy: 0.500 - ETA: 33s - loss: 1.7378 - accuracy: 0.500 - ETA: 32s - loss: 1.7396 - accuracy: 0.500 - ETA: 31s - loss: 1.7449 - accuracy: 0.499 - ETA: 30s - loss: 1.7453 - accuracy: 0.498 - ETA: 29s - loss: 1.7442 - accuracy: 0.499 - ETA: 28s - loss: 1.7417 - accuracy: 0.499 - ETA: 27s - loss: 1.7401 - accuracy: 0.500 - ETA: 26s - loss: 1.7393 - accuracy: 0.500 - ETA: 25s - loss: 1.7401 - accuracy: 0.499 - ETA: 25s - loss: 1.7393 - accuracy: 0.500 - ETA: 24s - loss: 1.7366 - accuracy: 0.500 - ETA: 23s - loss: 1.7389 - accuracy: 0.499 - ETA: 22s - loss: 1.7372 - accuracy: 0.500 - ETA: 21s - loss: 1.7377 - accuracy: 0.500 - ETA: 20s - loss: 1.7389 - accuracy: 0.500 - ETA: 19s - loss: 1.7391 - accuracy: 0.500 - ETA: 18s - loss: 1.7384 - accuracy: 0.501 - ETA: 17s - loss: 1.7400 - accuracy: 0.500 - ETA: 16s - loss: 1.7388 - accuracy: 0.501 - ETA: 16s - loss: 1.7384 - accuracy: 0.500 - ETA: 15s - loss: 1.7380 - accuracy: 0.501 - ETA: 14s - loss: 1.7409 - accuracy: 0.500 - ETA: 13s - loss: 1.7460 - accuracy: 0.499 - ETA: 12s - loss: 1.7462 - accuracy: 0.498 - ETA: 11s - loss: 1.7461 - accuracy: 0.498 - ETA: 10s - loss: 1.7474 - accuracy: 0.498 - ETA: 9s - loss: 1.7456 - accuracy: 0.498 - ETA: 8s - loss: 1.7443 - accuracy: 0.49 - ETA: 7s - loss: 1.7433 - accuracy: 0.49 - ETA: 7s - loss: 1.7416 - accuracy: 0.49 - ETA: 6s - loss: 1.7399 - accuracy: 0.49 - ETA: 5s - loss: 1.7404 - accuracy: 0.49 - ETA: 4s - loss: 1.7412 - accuracy: 0.49 - ETA: 3s - loss: 1.7427 - accuracy: 0.49 - ETA: 2s - loss: 1.7408 - accuracy: 0.50 - ETA: 1s - loss: 1.7399 - accuracy: 0.50 - ETA: 0s - loss: 1.7383 - accuracy: 0.50 - 105s 8ms/step - loss: 1.7398 - accuracy: 0.5020 - val_loss: 2.7817 - val_accuracy: 0.3137\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:33 - loss: 1.5941 - accuracy: 0.51 - ETA: 1:30 - loss: 1.7506 - accuracy: 0.50 - ETA: 1:29 - loss: 1.7180 - accuracy: 0.49 - ETA: 1:28 - loss: 1.7245 - accuracy: 0.48 - ETA: 1:28 - loss: 1.7317 - accuracy: 0.49 - ETA: 1:26 - loss: 1.6993 - accuracy: 0.49 - ETA: 1:25 - loss: 1.7187 - accuracy: 0.48 - ETA: 1:24 - loss: 1.6956 - accuracy: 0.49 - ETA: 1:23 - loss: 1.6964 - accuracy: 0.49 - ETA: 1:23 - loss: 1.6888 - accuracy: 0.49 - ETA: 1:22 - loss: 1.7006 - accuracy: 0.49 - ETA: 1:21 - loss: 1.7015 - accuracy: 0.49 - ETA: 1:20 - loss: 1.6925 - accuracy: 0.50 - ETA: 1:19 - loss: 1.6796 - accuracy: 0.50 - ETA: 1:18 - loss: 1.6903 - accuracy: 0.50 - ETA: 1:17 - loss: 1.6928 - accuracy: 0.50 - ETA: 1:16 - loss: 1.6743 - accuracy: 0.50 - ETA: 1:16 - loss: 1.6735 - accuracy: 0.50 - ETA: 1:15 - loss: 1.6678 - accuracy: 0.51 - ETA: 1:14 - loss: 1.6773 - accuracy: 0.50 - ETA: 1:13 - loss: 1.6801 - accuracy: 0.50 - ETA: 1:12 - loss: 1.6817 - accuracy: 0.50 - ETA: 1:11 - loss: 1.6689 - accuracy: 0.51 - ETA: 1:10 - loss: 1.6728 - accuracy: 0.51 - ETA: 1:10 - loss: 1.6714 - accuracy: 0.51 - ETA: 1:09 - loss: 1.6695 - accuracy: 0.51 - ETA: 1:08 - loss: 1.6697 - accuracy: 0.51 - ETA: 1:07 - loss: 1.6704 - accuracy: 0.51 - ETA: 1:06 - loss: 1.6782 - accuracy: 0.51 - ETA: 1:05 - loss: 1.6732 - accuracy: 0.51 - ETA: 1:04 - loss: 1.6755 - accuracy: 0.51 - ETA: 1:03 - loss: 1.6728 - accuracy: 0.51 - ETA: 1:02 - loss: 1.6690 - accuracy: 0.51 - ETA: 1:01 - loss: 1.6678 - accuracy: 0.51 - ETA: 1:00 - loss: 1.6682 - accuracy: 0.52 - ETA: 59s - loss: 1.6666 - accuracy: 0.5211 - ETA: 58s - loss: 1.6662 - accuracy: 0.522 - ETA: 57s - loss: 1.6610 - accuracy: 0.523 - ETA: 56s - loss: 1.6664 - accuracy: 0.522 - ETA: 55s - loss: 1.6654 - accuracy: 0.521 - ETA: 55s - loss: 1.6645 - accuracy: 0.521 - ETA: 54s - loss: 1.6637 - accuracy: 0.521 - ETA: 53s - loss: 1.6607 - accuracy: 0.522 - ETA: 52s - loss: 1.6583 - accuracy: 0.523 - ETA: 51s - loss: 1.6600 - accuracy: 0.522 - ETA: 50s - loss: 1.6589 - accuracy: 0.522 - ETA: 49s - loss: 1.6590 - accuracy: 0.522 - ETA: 48s - loss: 1.6563 - accuracy: 0.522 - ETA: 47s - loss: 1.6557 - accuracy: 0.522 - ETA: 46s - loss: 1.6600 - accuracy: 0.521 - ETA: 45s - loss: 1.6616 - accuracy: 0.520 - ETA: 45s - loss: 1.6620 - accuracy: 0.520 - ETA: 44s - loss: 1.6575 - accuracy: 0.521 - ETA: 43s - loss: 1.6572 - accuracy: 0.521 - ETA: 42s - loss: 1.6571 - accuracy: 0.521 - ETA: 41s - loss: 1.6592 - accuracy: 0.521 - ETA: 40s - loss: 1.6609 - accuracy: 0.521 - ETA: 39s - loss: 1.6630 - accuracy: 0.520 - ETA: 38s - loss: 1.6622 - accuracy: 0.520 - ETA: 37s - loss: 1.6627 - accuracy: 0.519 - ETA: 36s - loss: 1.6630 - accuracy: 0.519 - ETA: 35s - loss: 1.6638 - accuracy: 0.520 - ETA: 34s - loss: 1.6645 - accuracy: 0.519 - ETA: 34s - loss: 1.6660 - accuracy: 0.518 - ETA: 33s - loss: 1.6644 - accuracy: 0.518 - ETA: 32s - loss: 1.6620 - accuracy: 0.520 - ETA: 31s - loss: 1.6622 - accuracy: 0.519 - ETA: 30s - loss: 1.6628 - accuracy: 0.520 - ETA: 29s - loss: 1.6625 - accuracy: 0.519 - ETA: 28s - loss: 1.6634 - accuracy: 0.519 - ETA: 27s - loss: 1.6596 - accuracy: 0.520 - ETA: 26s - loss: 1.6625 - accuracy: 0.519 - ETA: 25s - loss: 1.6595 - accuracy: 0.520 - ETA: 24s - loss: 1.6580 - accuracy: 0.520 - ETA: 24s - loss: 1.6607 - accuracy: 0.520 - ETA: 23s - loss: 1.6587 - accuracy: 0.520 - ETA: 22s - loss: 1.6558 - accuracy: 0.520 - ETA: 21s - loss: 1.6574 - accuracy: 0.519 - ETA: 20s - loss: 1.6560 - accuracy: 0.520 - ETA: 19s - loss: 1.6545 - accuracy: 0.520 - ETA: 18s - loss: 1.6539 - accuracy: 0.521 - ETA: 17s - loss: 1.6534 - accuracy: 0.521 - ETA: 16s - loss: 1.6565 - accuracy: 0.520 - ETA: 15s - loss: 1.6591 - accuracy: 0.519 - ETA: 15s - loss: 1.6575 - accuracy: 0.519 - ETA: 14s - loss: 1.6551 - accuracy: 0.520 - ETA: 13s - loss: 1.6527 - accuracy: 0.520 - ETA: 12s - loss: 1.6534 - accuracy: 0.520 - ETA: 11s - loss: 1.6536 - accuracy: 0.520 - ETA: 10s - loss: 1.6535 - accuracy: 0.520 - ETA: 9s - loss: 1.6556 - accuracy: 0.520 - ETA: 8s - loss: 1.6546 - accuracy: 0.52 - ETA: 7s - loss: 1.6557 - accuracy: 0.51 - ETA: 6s - loss: 1.6590 - accuracy: 0.51 - ETA: 6s - loss: 1.6582 - accuracy: 0.51 - ETA: 5s - loss: 1.6578 - accuracy: 0.51 - ETA: 4s - loss: 1.6555 - accuracy: 0.52 - ETA: 3s - loss: 1.6544 - accuracy: 0.52 - ETA: 2s - loss: 1.6568 - accuracy: 0.52 - ETA: 1s - loss: 1.6557 - accuracy: 0.52 - ETA: 0s - loss: 1.6545 - accuracy: 0.52 - 105s 8ms/step - loss: 1.6570 - accuracy: 0.5214 - val_loss: 2.7031 - val_accuracy: 0.3627\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:28 - loss: 1.7833 - accuracy: 0.48 - ETA: 1:26 - loss: 1.6966 - accuracy: 0.48 - ETA: 1:26 - loss: 1.6571 - accuracy: 0.51 - ETA: 1:26 - loss: 1.6586 - accuracy: 0.50 - ETA: 1:25 - loss: 1.6341 - accuracy: 0.51 - ETA: 1:25 - loss: 1.5981 - accuracy: 0.52 - ETA: 1:24 - loss: 1.5743 - accuracy: 0.53 - ETA: 1:23 - loss: 1.5912 - accuracy: 0.53 - ETA: 1:22 - loss: 1.5822 - accuracy: 0.54 - ETA: 1:22 - loss: 1.6076 - accuracy: 0.52 - ETA: 1:21 - loss: 1.5888 - accuracy: 0.53 - ETA: 1:20 - loss: 1.6064 - accuracy: 0.52 - ETA: 1:20 - loss: 1.6051 - accuracy: 0.52 - ETA: 1:19 - loss: 1.5928 - accuracy: 0.53 - ETA: 1:18 - loss: 1.5935 - accuracy: 0.53 - ETA: 1:17 - loss: 1.5925 - accuracy: 0.53 - ETA: 1:16 - loss: 1.5906 - accuracy: 0.53 - ETA: 1:15 - loss: 1.6094 - accuracy: 0.52 - ETA: 1:14 - loss: 1.6032 - accuracy: 0.53 - ETA: 1:13 - loss: 1.5963 - accuracy: 0.53 - ETA: 1:12 - loss: 1.5908 - accuracy: 0.53 - ETA: 1:11 - loss: 1.5869 - accuracy: 0.53 - ETA: 1:10 - loss: 1.5938 - accuracy: 0.53 - ETA: 1:09 - loss: 1.5975 - accuracy: 0.53 - ETA: 1:08 - loss: 1.5973 - accuracy: 0.53 - ETA: 1:08 - loss: 1.6026 - accuracy: 0.52 - ETA: 1:07 - loss: 1.6049 - accuracy: 0.52 - ETA: 1:06 - loss: 1.6043 - accuracy: 0.52 - ETA: 1:05 - loss: 1.6016 - accuracy: 0.52 - ETA: 1:04 - loss: 1.5924 - accuracy: 0.53 - ETA: 1:03 - loss: 1.5924 - accuracy: 0.53 - ETA: 1:02 - loss: 1.5886 - accuracy: 0.53 - ETA: 1:01 - loss: 1.5987 - accuracy: 0.53 - ETA: 1:01 - loss: 1.5929 - accuracy: 0.53 - ETA: 1:00 - loss: 1.5863 - accuracy: 0.53 - ETA: 59s - loss: 1.5860 - accuracy: 0.5354 - ETA: 58s - loss: 1.5847 - accuracy: 0.536 - ETA: 57s - loss: 1.5788 - accuracy: 0.536 - ETA: 56s - loss: 1.5784 - accuracy: 0.536 - ETA: 55s - loss: 1.5761 - accuracy: 0.536 - ETA: 54s - loss: 1.5765 - accuracy: 0.536 - ETA: 53s - loss: 1.5718 - accuracy: 0.537 - ETA: 52s - loss: 1.5759 - accuracy: 0.536 - ETA: 51s - loss: 1.5756 - accuracy: 0.536 - ETA: 50s - loss: 1.5776 - accuracy: 0.535 - ETA: 50s - loss: 1.5724 - accuracy: 0.536 - ETA: 49s - loss: 1.5699 - accuracy: 0.537 - ETA: 48s - loss: 1.5736 - accuracy: 0.536 - ETA: 47s - loss: 1.5769 - accuracy: 0.535 - ETA: 46s - loss: 1.5761 - accuracy: 0.536 - ETA: 45s - loss: 1.5752 - accuracy: 0.536 - ETA: 44s - loss: 1.5701 - accuracy: 0.537 - ETA: 44s - loss: 1.5723 - accuracy: 0.536 - ETA: 43s - loss: 1.5724 - accuracy: 0.536 - ETA: 42s - loss: 1.5725 - accuracy: 0.536 - ETA: 41s - loss: 1.5688 - accuracy: 0.538 - ETA: 40s - loss: 1.5685 - accuracy: 0.539 - ETA: 39s - loss: 1.5641 - accuracy: 0.539 - ETA: 38s - loss: 1.5609 - accuracy: 0.541 - ETA: 37s - loss: 1.5613 - accuracy: 0.540 - ETA: 36s - loss: 1.5615 - accuracy: 0.540 - ETA: 35s - loss: 1.5620 - accuracy: 0.539 - ETA: 34s - loss: 1.5608 - accuracy: 0.540 - ETA: 34s - loss: 1.5613 - accuracy: 0.540 - ETA: 33s - loss: 1.5606 - accuracy: 0.540 - ETA: 32s - loss: 1.5619 - accuracy: 0.540 - ETA: 31s - loss: 1.5599 - accuracy: 0.540 - ETA: 30s - loss: 1.5610 - accuracy: 0.540 - ETA: 29s - loss: 1.5616 - accuracy: 0.540 - ETA: 28s - loss: 1.5638 - accuracy: 0.539 - ETA: 27s - loss: 1.5637 - accuracy: 0.540 - ETA: 26s - loss: 1.5676 - accuracy: 0.539 - ETA: 25s - loss: 1.5650 - accuracy: 0.539 - ETA: 25s - loss: 1.5651 - accuracy: 0.539 - ETA: 24s - loss: 1.5626 - accuracy: 0.539 - ETA: 23s - loss: 1.5648 - accuracy: 0.539 - ETA: 22s - loss: 1.5631 - accuracy: 0.539 - ETA: 21s - loss: 1.5617 - accuracy: 0.540 - ETA: 20s - loss: 1.5602 - accuracy: 0.540 - ETA: 19s - loss: 1.5605 - accuracy: 0.540 - ETA: 18s - loss: 1.5606 - accuracy: 0.540 - ETA: 17s - loss: 1.5618 - accuracy: 0.539 - ETA: 16s - loss: 1.5641 - accuracy: 0.539 - ETA: 15s - loss: 1.5652 - accuracy: 0.538 - ETA: 15s - loss: 1.5680 - accuracy: 0.538 - ETA: 14s - loss: 1.5695 - accuracy: 0.537 - ETA: 13s - loss: 1.5710 - accuracy: 0.536 - ETA: 12s - loss: 1.5696 - accuracy: 0.537 - ETA: 11s - loss: 1.5692 - accuracy: 0.537 - ETA: 10s - loss: 1.5711 - accuracy: 0.537 - ETA: 9s - loss: 1.5712 - accuracy: 0.538 - ETA: 8s - loss: 1.5710 - accuracy: 0.53 - ETA: 7s - loss: 1.5716 - accuracy: 0.53 - ETA: 6s - loss: 1.5709 - accuracy: 0.53 - ETA: 6s - loss: 1.5717 - accuracy: 0.53 - ETA: 5s - loss: 1.5689 - accuracy: 0.53 - ETA: 4s - loss: 1.5673 - accuracy: 0.53 - ETA: 3s - loss: 1.5712 - accuracy: 0.53 - ETA: 2s - loss: 1.5698 - accuracy: 0.53 - ETA: 1s - loss: 1.5710 - accuracy: 0.53 - ETA: 0s - loss: 1.5696 - accuracy: 0.53 - 105s 8ms/step - loss: 1.5687 - accuracy: 0.5392 - val_loss: 2.7151 - val_accuracy: 0.3220\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 1.5924 - accuracy: 0.50 - ETA: 1:29 - loss: 1.5704 - accuracy: 0.51 - ETA: 1:29 - loss: 1.5296 - accuracy: 0.55 - ETA: 1:31 - loss: 1.4868 - accuracy: 0.56 - ETA: 1:31 - loss: 1.5225 - accuracy: 0.55 - ETA: 1:30 - loss: 1.4846 - accuracy: 0.55 - ETA: 1:29 - loss: 1.4988 - accuracy: 0.55 - ETA: 1:27 - loss: 1.4958 - accuracy: 0.55 - ETA: 1:26 - loss: 1.4808 - accuracy: 0.56 - ETA: 1:25 - loss: 1.4978 - accuracy: 0.56 - ETA: 1:23 - loss: 1.5161 - accuracy: 0.55 - ETA: 1:22 - loss: 1.5200 - accuracy: 0.55 - ETA: 1:21 - loss: 1.5156 - accuracy: 0.55 - ETA: 1:20 - loss: 1.5080 - accuracy: 0.55 - ETA: 1:19 - loss: 1.5136 - accuracy: 0.54 - ETA: 1:18 - loss: 1.5032 - accuracy: 0.55 - ETA: 1:17 - loss: 1.5173 - accuracy: 0.54 - ETA: 1:16 - loss: 1.5147 - accuracy: 0.55 - ETA: 1:15 - loss: 1.5164 - accuracy: 0.54 - ETA: 1:14 - loss: 1.5408 - accuracy: 0.54 - ETA: 1:13 - loss: 1.5369 - accuracy: 0.54 - ETA: 1:12 - loss: 1.5393 - accuracy: 0.54 - ETA: 1:12 - loss: 1.5398 - accuracy: 0.54 - ETA: 1:11 - loss: 1.5308 - accuracy: 0.54 - ETA: 1:10 - loss: 1.5229 - accuracy: 0.55 - ETA: 1:09 - loss: 1.5153 - accuracy: 0.55 - ETA: 1:08 - loss: 1.5202 - accuracy: 0.54 - ETA: 1:07 - loss: 1.5251 - accuracy: 0.54 - ETA: 1:06 - loss: 1.5286 - accuracy: 0.54 - ETA: 1:05 - loss: 1.5280 - accuracy: 0.54 - ETA: 1:04 - loss: 1.5236 - accuracy: 0.55 - ETA: 1:03 - loss: 1.5183 - accuracy: 0.55 - ETA: 1:02 - loss: 1.5189 - accuracy: 0.55 - ETA: 1:01 - loss: 1.5194 - accuracy: 0.55 - ETA: 1:00 - loss: 1.5084 - accuracy: 0.55 - ETA: 59s - loss: 1.5041 - accuracy: 0.5536 - ETA: 58s - loss: 1.5064 - accuracy: 0.553 - ETA: 57s - loss: 1.5007 - accuracy: 0.554 - ETA: 56s - loss: 1.5031 - accuracy: 0.554 - ETA: 56s - loss: 1.5018 - accuracy: 0.553 - ETA: 55s - loss: 1.5013 - accuracy: 0.553 - ETA: 54s - loss: 1.4999 - accuracy: 0.554 - ETA: 53s - loss: 1.5013 - accuracy: 0.552 - ETA: 52s - loss: 1.4999 - accuracy: 0.553 - ETA: 51s - loss: 1.4965 - accuracy: 0.554 - ETA: 50s - loss: 1.4959 - accuracy: 0.554 - ETA: 49s - loss: 1.5011 - accuracy: 0.554 - ETA: 48s - loss: 1.4979 - accuracy: 0.554 - ETA: 47s - loss: 1.5001 - accuracy: 0.554 - ETA: 46s - loss: 1.4946 - accuracy: 0.555 - ETA: 46s - loss: 1.4938 - accuracy: 0.555 - ETA: 45s - loss: 1.4987 - accuracy: 0.553 - ETA: 44s - loss: 1.4963 - accuracy: 0.553 - ETA: 43s - loss: 1.4920 - accuracy: 0.554 - ETA: 42s - loss: 1.4893 - accuracy: 0.555 - ETA: 41s - loss: 1.4871 - accuracy: 0.555 - ETA: 40s - loss: 1.4854 - accuracy: 0.557 - ETA: 39s - loss: 1.4867 - accuracy: 0.556 - ETA: 38s - loss: 1.4871 - accuracy: 0.556 - ETA: 37s - loss: 1.4820 - accuracy: 0.557 - ETA: 36s - loss: 1.4826 - accuracy: 0.557 - ETA: 35s - loss: 1.4842 - accuracy: 0.556 - ETA: 35s - loss: 1.4855 - accuracy: 0.556 - ETA: 34s - loss: 1.4920 - accuracy: 0.554 - ETA: 33s - loss: 1.4890 - accuracy: 0.555 - ETA: 32s - loss: 1.4897 - accuracy: 0.555 - ETA: 31s - loss: 1.4868 - accuracy: 0.555 - ETA: 30s - loss: 1.4839 - accuracy: 0.556 - ETA: 29s - loss: 1.4862 - accuracy: 0.556 - ETA: 28s - loss: 1.4851 - accuracy: 0.556 - ETA: 27s - loss: 1.4877 - accuracy: 0.556 - ETA: 26s - loss: 1.4908 - accuracy: 0.555 - ETA: 25s - loss: 1.4915 - accuracy: 0.555 - ETA: 25s - loss: 1.4903 - accuracy: 0.555 - ETA: 24s - loss: 1.4900 - accuracy: 0.555 - ETA: 23s - loss: 1.4900 - accuracy: 0.555 - ETA: 22s - loss: 1.4887 - accuracy: 0.555 - ETA: 21s - loss: 1.4897 - accuracy: 0.555 - ETA: 20s - loss: 1.4894 - accuracy: 0.556 - ETA: 19s - loss: 1.4934 - accuracy: 0.555 - ETA: 18s - loss: 1.4948 - accuracy: 0.554 - ETA: 17s - loss: 1.4933 - accuracy: 0.555 - ETA: 16s - loss: 1.4925 - accuracy: 0.555 - ETA: 16s - loss: 1.4900 - accuracy: 0.556 - ETA: 15s - loss: 1.4923 - accuracy: 0.555 - ETA: 14s - loss: 1.4895 - accuracy: 0.556 - ETA: 13s - loss: 1.4889 - accuracy: 0.556 - ETA: 12s - loss: 1.4942 - accuracy: 0.554 - ETA: 11s - loss: 1.4948 - accuracy: 0.554 - ETA: 10s - loss: 1.4965 - accuracy: 0.554 - ETA: 9s - loss: 1.4963 - accuracy: 0.554 - ETA: 8s - loss: 1.4963 - accuracy: 0.55 - ETA: 7s - loss: 1.4954 - accuracy: 0.55 - ETA: 6s - loss: 1.4968 - accuracy: 0.55 - ETA: 6s - loss: 1.4992 - accuracy: 0.55 - ETA: 5s - loss: 1.4971 - accuracy: 0.55 - ETA: 4s - loss: 1.4968 - accuracy: 0.55 - ETA: 3s - loss: 1.4966 - accuracy: 0.55 - ETA: 2s - loss: 1.4964 - accuracy: 0.55 - ETA: 1s - loss: 1.4991 - accuracy: 0.55 - ETA: 0s - loss: 1.4995 - accuracy: 0.55 - 105s 8ms/step - loss: 1.5002 - accuracy: 0.5541 - val_loss: 2.6474 - val_accuracy: 0.3247\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:27 - loss: 1.5203 - accuracy: 0.53 - ETA: 1:29 - loss: 1.5236 - accuracy: 0.52 - ETA: 1:27 - loss: 1.5027 - accuracy: 0.55 - ETA: 1:26 - loss: 1.5153 - accuracy: 0.54 - ETA: 1:26 - loss: 1.5105 - accuracy: 0.55 - ETA: 1:26 - loss: 1.5344 - accuracy: 0.54 - ETA: 1:25 - loss: 1.5015 - accuracy: 0.54 - ETA: 1:25 - loss: 1.5104 - accuracy: 0.54 - ETA: 1:24 - loss: 1.4956 - accuracy: 0.55 - ETA: 1:23 - loss: 1.4824 - accuracy: 0.55 - ETA: 1:22 - loss: 1.4689 - accuracy: 0.55 - ETA: 1:21 - loss: 1.4691 - accuracy: 0.55 - ETA: 1:20 - loss: 1.4630 - accuracy: 0.56 - ETA: 1:19 - loss: 1.4522 - accuracy: 0.56 - ETA: 1:19 - loss: 1.4617 - accuracy: 0.56 - ETA: 1:18 - loss: 1.4570 - accuracy: 0.56 - ETA: 1:17 - loss: 1.4536 - accuracy: 0.56 - ETA: 1:16 - loss: 1.4523 - accuracy: 0.56 - ETA: 1:15 - loss: 1.4503 - accuracy: 0.56 - ETA: 1:14 - loss: 1.4456 - accuracy: 0.56 - ETA: 1:13 - loss: 1.4589 - accuracy: 0.56 - ETA: 1:12 - loss: 1.4670 - accuracy: 0.56 - ETA: 1:11 - loss: 1.4615 - accuracy: 0.56 - ETA: 1:10 - loss: 1.4564 - accuracy: 0.56 - ETA: 1:09 - loss: 1.4566 - accuracy: 0.56 - ETA: 1:08 - loss: 1.4656 - accuracy: 0.56 - ETA: 1:07 - loss: 1.4659 - accuracy: 0.56 - ETA: 1:06 - loss: 1.4627 - accuracy: 0.56 - ETA: 1:05 - loss: 1.4642 - accuracy: 0.56 - ETA: 1:05 - loss: 1.4620 - accuracy: 0.56 - ETA: 1:04 - loss: 1.4643 - accuracy: 0.56 - ETA: 1:03 - loss: 1.4712 - accuracy: 0.56 - ETA: 1:02 - loss: 1.4722 - accuracy: 0.56 - ETA: 1:01 - loss: 1.4661 - accuracy: 0.56 - ETA: 1:00 - loss: 1.4626 - accuracy: 0.56 - ETA: 59s - loss: 1.4702 - accuracy: 0.5640 - ETA: 58s - loss: 1.4678 - accuracy: 0.564 - ETA: 57s - loss: 1.4653 - accuracy: 0.564 - ETA: 56s - loss: 1.4628 - accuracy: 0.566 - ETA: 55s - loss: 1.4573 - accuracy: 0.566 - ETA: 54s - loss: 1.4525 - accuracy: 0.567 - ETA: 54s - loss: 1.4605 - accuracy: 0.564 - ETA: 53s - loss: 1.4559 - accuracy: 0.565 - ETA: 52s - loss: 1.4566 - accuracy: 0.565 - ETA: 51s - loss: 1.4610 - accuracy: 0.564 - ETA: 50s - loss: 1.4572 - accuracy: 0.566 - ETA: 49s - loss: 1.4595 - accuracy: 0.565 - ETA: 48s - loss: 1.4588 - accuracy: 0.566 - ETA: 47s - loss: 1.4574 - accuracy: 0.567 - ETA: 46s - loss: 1.4518 - accuracy: 0.568 - ETA: 45s - loss: 1.4535 - accuracy: 0.568 - ETA: 45s - loss: 1.4537 - accuracy: 0.568 - ETA: 44s - loss: 1.4528 - accuracy: 0.568 - ETA: 43s - loss: 1.4498 - accuracy: 0.568 - ETA: 42s - loss: 1.4503 - accuracy: 0.568 - ETA: 41s - loss: 1.4499 - accuracy: 0.569 - ETA: 40s - loss: 1.4575 - accuracy: 0.567 - ETA: 39s - loss: 1.4570 - accuracy: 0.567 - ETA: 38s - loss: 1.4585 - accuracy: 0.567 - ETA: 37s - loss: 1.4615 - accuracy: 0.565 - ETA: 36s - loss: 1.4611 - accuracy: 0.566 - ETA: 35s - loss: 1.4589 - accuracy: 0.567 - ETA: 34s - loss: 1.4567 - accuracy: 0.567 - ETA: 34s - loss: 1.4587 - accuracy: 0.565 - ETA: 33s - loss: 1.4605 - accuracy: 0.564 - ETA: 32s - loss: 1.4595 - accuracy: 0.564 - ETA: 31s - loss: 1.4622 - accuracy: 0.564 - ETA: 30s - loss: 1.4609 - accuracy: 0.564 - ETA: 29s - loss: 1.4595 - accuracy: 0.565 - ETA: 28s - loss: 1.4587 - accuracy: 0.565 - ETA: 27s - loss: 1.4578 - accuracy: 0.565 - ETA: 26s - loss: 1.4555 - accuracy: 0.565 - ETA: 25s - loss: 1.4566 - accuracy: 0.566 - ETA: 24s - loss: 1.4541 - accuracy: 0.567 - ETA: 24s - loss: 1.4531 - accuracy: 0.567 - ETA: 23s - loss: 1.4515 - accuracy: 0.567 - ETA: 22s - loss: 1.4530 - accuracy: 0.567 - ETA: 21s - loss: 1.4486 - accuracy: 0.568 - ETA: 20s - loss: 1.4475 - accuracy: 0.568 - ETA: 19s - loss: 1.4484 - accuracy: 0.568 - ETA: 18s - loss: 1.4492 - accuracy: 0.568 - ETA: 17s - loss: 1.4471 - accuracy: 0.568 - ETA: 16s - loss: 1.4476 - accuracy: 0.569 - ETA: 16s - loss: 1.4455 - accuracy: 0.569 - ETA: 15s - loss: 1.4448 - accuracy: 0.569 - ETA: 14s - loss: 1.4442 - accuracy: 0.569 - ETA: 13s - loss: 1.4461 - accuracy: 0.569 - ETA: 12s - loss: 1.4446 - accuracy: 0.570 - ETA: 11s - loss: 1.4442 - accuracy: 0.570 - ETA: 10s - loss: 1.4434 - accuracy: 0.570 - ETA: 9s - loss: 1.4432 - accuracy: 0.569 - ETA: 8s - loss: 1.4412 - accuracy: 0.56 - ETA: 7s - loss: 1.4392 - accuracy: 0.57 - ETA: 6s - loss: 1.4421 - accuracy: 0.57 - ETA: 6s - loss: 1.4429 - accuracy: 0.56 - ETA: 5s - loss: 1.4405 - accuracy: 0.57 - ETA: 4s - loss: 1.4404 - accuracy: 0.57 - ETA: 3s - loss: 1.4401 - accuracy: 0.57 - ETA: 2s - loss: 1.4425 - accuracy: 0.56 - ETA: 1s - loss: 1.4416 - accuracy: 0.56 - ETA: 0s - loss: 1.4424 - accuracy: 0.56 - 106s 8ms/step - loss: 1.4414 - accuracy: 0.5696 - val_loss: 2.6671 - val_accuracy: 0.3560\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:37 - loss: 1.3385 - accuracy: 0.60 - ETA: 1:33 - loss: 1.5108 - accuracy: 0.55 - ETA: 1:32 - loss: 1.4001 - accuracy: 0.59 - ETA: 1:30 - loss: 1.4171 - accuracy: 0.57 - ETA: 1:28 - loss: 1.4172 - accuracy: 0.56 - ETA: 1:27 - loss: 1.3746 - accuracy: 0.58 - ETA: 1:26 - loss: 1.3760 - accuracy: 0.58 - ETA: 1:25 - loss: 1.3736 - accuracy: 0.59 - ETA: 1:24 - loss: 1.3759 - accuracy: 0.59 - ETA: 1:23 - loss: 1.3798 - accuracy: 0.59 - ETA: 1:21 - loss: 1.3564 - accuracy: 0.60 - ETA: 1:20 - loss: 1.3524 - accuracy: 0.60 - ETA: 1:19 - loss: 1.3583 - accuracy: 0.60 - ETA: 1:18 - loss: 1.3846 - accuracy: 0.59 - ETA: 1:17 - loss: 1.3882 - accuracy: 0.59 - ETA: 1:17 - loss: 1.3815 - accuracy: 0.59 - ETA: 1:16 - loss: 1.3789 - accuracy: 0.59 - ETA: 1:15 - loss: 1.3864 - accuracy: 0.59 - ETA: 1:14 - loss: 1.3951 - accuracy: 0.59 - ETA: 1:13 - loss: 1.3958 - accuracy: 0.59 - ETA: 1:12 - loss: 1.4038 - accuracy: 0.59 - ETA: 1:11 - loss: 1.3931 - accuracy: 0.59 - ETA: 1:10 - loss: 1.3802 - accuracy: 0.59 - ETA: 1:09 - loss: 1.3824 - accuracy: 0.59 - ETA: 1:08 - loss: 1.3800 - accuracy: 0.59 - ETA: 1:07 - loss: 1.3732 - accuracy: 0.60 - ETA: 1:07 - loss: 1.3797 - accuracy: 0.59 - ETA: 1:06 - loss: 1.3754 - accuracy: 0.59 - ETA: 1:05 - loss: 1.3765 - accuracy: 0.59 - ETA: 1:04 - loss: 1.3773 - accuracy: 0.59 - ETA: 1:03 - loss: 1.3763 - accuracy: 0.59 - ETA: 1:02 - loss: 1.3753 - accuracy: 0.59 - ETA: 1:01 - loss: 1.3717 - accuracy: 0.59 - ETA: 1:01 - loss: 1.3754 - accuracy: 0.59 - ETA: 1:00 - loss: 1.3744 - accuracy: 0.59 - ETA: 59s - loss: 1.3738 - accuracy: 0.5951 - ETA: 58s - loss: 1.3733 - accuracy: 0.595 - ETA: 57s - loss: 1.3712 - accuracy: 0.595 - ETA: 56s - loss: 1.3725 - accuracy: 0.595 - ETA: 55s - loss: 1.3808 - accuracy: 0.593 - ETA: 54s - loss: 1.3813 - accuracy: 0.592 - ETA: 53s - loss: 1.3847 - accuracy: 0.591 - ETA: 52s - loss: 1.3808 - accuracy: 0.593 - ETA: 52s - loss: 1.3834 - accuracy: 0.593 - ETA: 51s - loss: 1.3843 - accuracy: 0.592 - ETA: 50s - loss: 1.3929 - accuracy: 0.590 - ETA: 49s - loss: 1.3958 - accuracy: 0.588 - ETA: 48s - loss: 1.4002 - accuracy: 0.587 - ETA: 47s - loss: 1.3995 - accuracy: 0.587 - ETA: 46s - loss: 1.4003 - accuracy: 0.586 - ETA: 45s - loss: 1.3959 - accuracy: 0.588 - ETA: 44s - loss: 1.3967 - accuracy: 0.588 - ETA: 43s - loss: 1.3977 - accuracy: 0.587 - ETA: 42s - loss: 1.4022 - accuracy: 0.586 - ETA: 42s - loss: 1.4018 - accuracy: 0.586 - ETA: 41s - loss: 1.4026 - accuracy: 0.586 - ETA: 40s - loss: 1.4060 - accuracy: 0.586 - ETA: 39s - loss: 1.4045 - accuracy: 0.586 - ETA: 38s - loss: 1.4033 - accuracy: 0.586 - ETA: 37s - loss: 1.4007 - accuracy: 0.587 - ETA: 36s - loss: 1.4003 - accuracy: 0.586 - ETA: 35s - loss: 1.3996 - accuracy: 0.586 - ETA: 34s - loss: 1.4000 - accuracy: 0.585 - ETA: 34s - loss: 1.4018 - accuracy: 0.585 - ETA: 33s - loss: 1.4010 - accuracy: 0.584 - ETA: 32s - loss: 1.4017 - accuracy: 0.584 - ETA: 31s - loss: 1.4014 - accuracy: 0.584 - ETA: 30s - loss: 1.3975 - accuracy: 0.585 - ETA: 29s - loss: 1.3988 - accuracy: 0.585 - ETA: 28s - loss: 1.3969 - accuracy: 0.586 - ETA: 27s - loss: 1.3964 - accuracy: 0.585 - ETA: 26s - loss: 1.3988 - accuracy: 0.585 - ETA: 25s - loss: 1.4019 - accuracy: 0.584 - ETA: 25s - loss: 1.4013 - accuracy: 0.585 - ETA: 24s - loss: 1.4028 - accuracy: 0.585 - ETA: 23s - loss: 1.4005 - accuracy: 0.586 - ETA: 22s - loss: 1.3969 - accuracy: 0.586 - ETA: 21s - loss: 1.3972 - accuracy: 0.586 - ETA: 20s - loss: 1.3966 - accuracy: 0.586 - ETA: 19s - loss: 1.3966 - accuracy: 0.586 - ETA: 18s - loss: 1.3973 - accuracy: 0.586 - ETA: 17s - loss: 1.3973 - accuracy: 0.586 - ETA: 16s - loss: 1.3974 - accuracy: 0.586 - ETA: 15s - loss: 1.3981 - accuracy: 0.585 - ETA: 15s - loss: 1.3967 - accuracy: 0.586 - ETA: 14s - loss: 1.3967 - accuracy: 0.586 - ETA: 13s - loss: 1.3968 - accuracy: 0.586 - ETA: 12s - loss: 1.3961 - accuracy: 0.585 - ETA: 11s - loss: 1.3946 - accuracy: 0.586 - ETA: 10s - loss: 1.3929 - accuracy: 0.586 - ETA: 9s - loss: 1.3913 - accuracy: 0.585 - ETA: 8s - loss: 1.3902 - accuracy: 0.58 - ETA: 7s - loss: 1.3899 - accuracy: 0.58 - ETA: 6s - loss: 1.3915 - accuracy: 0.58 - ETA: 6s - loss: 1.3940 - accuracy: 0.58 - ETA: 5s - loss: 1.3931 - accuracy: 0.58 - ETA: 4s - loss: 1.3886 - accuracy: 0.58 - ETA: 3s - loss: 1.3897 - accuracy: 0.58 - ETA: 2s - loss: 1.3900 - accuracy: 0.58 - ETA: 1s - loss: 1.3902 - accuracy: 0.58 - ETA: 0s - loss: 1.3912 - accuracy: 0.58 - 105s 8ms/step - loss: 1.3887 - accuracy: 0.5865 - val_loss: 2.6819 - val_accuracy: 0.3267\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:33 - loss: 1.3899 - accuracy: 0.60 - ETA: 1:31 - loss: 1.3022 - accuracy: 0.61 - ETA: 1:31 - loss: 1.3591 - accuracy: 0.60 - ETA: 1:30 - loss: 1.3652 - accuracy: 0.59 - ETA: 1:28 - loss: 1.3890 - accuracy: 0.58 - ETA: 1:27 - loss: 1.3958 - accuracy: 0.57 - ETA: 1:27 - loss: 1.4192 - accuracy: 0.57 - ETA: 1:25 - loss: 1.4258 - accuracy: 0.57 - ETA: 1:24 - loss: 1.4140 - accuracy: 0.57 - ETA: 1:23 - loss: 1.3925 - accuracy: 0.57 - ETA: 1:22 - loss: 1.4114 - accuracy: 0.57 - ETA: 1:21 - loss: 1.4072 - accuracy: 0.57 - ETA: 1:21 - loss: 1.3915 - accuracy: 0.58 - ETA: 1:20 - loss: 1.3907 - accuracy: 0.58 - ETA: 1:19 - loss: 1.3877 - accuracy: 0.58 - ETA: 1:19 - loss: 1.3719 - accuracy: 0.58 - ETA: 1:18 - loss: 1.3715 - accuracy: 0.59 - ETA: 1:17 - loss: 1.3579 - accuracy: 0.59 - ETA: 1:16 - loss: 1.3588 - accuracy: 0.59 - ETA: 1:15 - loss: 1.3802 - accuracy: 0.59 - ETA: 1:14 - loss: 1.3696 - accuracy: 0.59 - ETA: 1:13 - loss: 1.3768 - accuracy: 0.59 - ETA: 1:12 - loss: 1.3693 - accuracy: 0.59 - ETA: 1:11 - loss: 1.3658 - accuracy: 0.59 - ETA: 1:10 - loss: 1.3638 - accuracy: 0.59 - ETA: 1:09 - loss: 1.3621 - accuracy: 0.59 - ETA: 1:08 - loss: 1.3690 - accuracy: 0.59 - ETA: 1:07 - loss: 1.3679 - accuracy: 0.59 - ETA: 1:06 - loss: 1.3638 - accuracy: 0.59 - ETA: 1:05 - loss: 1.3639 - accuracy: 0.59 - ETA: 1:04 - loss: 1.3732 - accuracy: 0.59 - ETA: 1:03 - loss: 1.3730 - accuracy: 0.59 - ETA: 1:02 - loss: 1.3770 - accuracy: 0.59 - ETA: 1:02 - loss: 1.3755 - accuracy: 0.59 - ETA: 1:01 - loss: 1.3733 - accuracy: 0.59 - ETA: 1:00 - loss: 1.3723 - accuracy: 0.59 - ETA: 59s - loss: 1.3714 - accuracy: 0.5904 - ETA: 58s - loss: 1.3685 - accuracy: 0.591 - ETA: 57s - loss: 1.3629 - accuracy: 0.593 - ETA: 56s - loss: 1.3634 - accuracy: 0.593 - ETA: 55s - loss: 1.3649 - accuracy: 0.593 - ETA: 54s - loss: 1.3635 - accuracy: 0.594 - ETA: 53s - loss: 1.3675 - accuracy: 0.594 - ETA: 52s - loss: 1.3682 - accuracy: 0.594 - ETA: 51s - loss: 1.3680 - accuracy: 0.595 - ETA: 50s - loss: 1.3648 - accuracy: 0.595 - ETA: 49s - loss: 1.3631 - accuracy: 0.595 - ETA: 48s - loss: 1.3609 - accuracy: 0.596 - ETA: 47s - loss: 1.3582 - accuracy: 0.597 - ETA: 47s - loss: 1.3588 - accuracy: 0.597 - ETA: 46s - loss: 1.3579 - accuracy: 0.597 - ETA: 45s - loss: 1.3592 - accuracy: 0.598 - ETA: 44s - loss: 1.3661 - accuracy: 0.595 - ETA: 43s - loss: 1.3637 - accuracy: 0.595 - ETA: 42s - loss: 1.3611 - accuracy: 0.596 - ETA: 41s - loss: 1.3599 - accuracy: 0.596 - ETA: 40s - loss: 1.3619 - accuracy: 0.595 - ETA: 39s - loss: 1.3643 - accuracy: 0.594 - ETA: 38s - loss: 1.3676 - accuracy: 0.594 - ETA: 37s - loss: 1.3656 - accuracy: 0.594 - ETA: 36s - loss: 1.3654 - accuracy: 0.594 - ETA: 36s - loss: 1.3656 - accuracy: 0.594 - ETA: 35s - loss: 1.3631 - accuracy: 0.595 - ETA: 34s - loss: 1.3643 - accuracy: 0.595 - ETA: 33s - loss: 1.3695 - accuracy: 0.593 - ETA: 32s - loss: 1.3724 - accuracy: 0.592 - ETA: 31s - loss: 1.3733 - accuracy: 0.591 - ETA: 30s - loss: 1.3738 - accuracy: 0.591 - ETA: 29s - loss: 1.3731 - accuracy: 0.591 - ETA: 28s - loss: 1.3730 - accuracy: 0.592 - ETA: 27s - loss: 1.3721 - accuracy: 0.592 - ETA: 26s - loss: 1.3727 - accuracy: 0.592 - ETA: 26s - loss: 1.3751 - accuracy: 0.592 - ETA: 25s - loss: 1.3728 - accuracy: 0.592 - ETA: 24s - loss: 1.3767 - accuracy: 0.592 - ETA: 23s - loss: 1.3810 - accuracy: 0.591 - ETA: 22s - loss: 1.3833 - accuracy: 0.591 - ETA: 21s - loss: 1.3810 - accuracy: 0.591 - ETA: 20s - loss: 1.3830 - accuracy: 0.591 - ETA: 19s - loss: 1.3841 - accuracy: 0.591 - ETA: 18s - loss: 1.3842 - accuracy: 0.590 - ETA: 17s - loss: 1.3827 - accuracy: 0.590 - ETA: 16s - loss: 1.3812 - accuracy: 0.590 - ETA: 16s - loss: 1.3828 - accuracy: 0.589 - ETA: 15s - loss: 1.3830 - accuracy: 0.590 - ETA: 14s - loss: 1.3791 - accuracy: 0.591 - ETA: 13s - loss: 1.3797 - accuracy: 0.590 - ETA: 12s - loss: 1.3770 - accuracy: 0.591 - ETA: 11s - loss: 1.3771 - accuracy: 0.591 - ETA: 10s - loss: 1.3784 - accuracy: 0.591 - ETA: 9s - loss: 1.3785 - accuracy: 0.591 - ETA: 8s - loss: 1.3777 - accuracy: 0.59 - ETA: 7s - loss: 1.3799 - accuracy: 0.59 - ETA: 7s - loss: 1.3808 - accuracy: 0.59 - ETA: 6s - loss: 1.3765 - accuracy: 0.59 - ETA: 5s - loss: 1.3784 - accuracy: 0.59 - ETA: 4s - loss: 1.3781 - accuracy: 0.59 - ETA: 3s - loss: 1.3795 - accuracy: 0.59 - ETA: 2s - loss: 1.3805 - accuracy: 0.59 - ETA: 1s - loss: 1.3797 - accuracy: 0.59 - ETA: 0s - loss: 1.3763 - accuracy: 0.59 - 106s 8ms/step - loss: 1.3761 - accuracy: 0.5916 - val_loss: 2.7406 - val_accuracy: 0.3046\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 1.2961 - accuracy: 0.61 - ETA: 1:29 - loss: 1.3517 - accuracy: 0.61 - ETA: 1:27 - loss: 1.2752 - accuracy: 0.63 - ETA: 1:26 - loss: 1.2497 - accuracy: 0.62 - ETA: 1:26 - loss: 1.3007 - accuracy: 0.61 - ETA: 1:25 - loss: 1.2957 - accuracy: 0.61 - ETA: 1:24 - loss: 1.2642 - accuracy: 0.62 - ETA: 1:23 - loss: 1.2986 - accuracy: 0.61 - ETA: 1:22 - loss: 1.2981 - accuracy: 0.61 - ETA: 1:21 - loss: 1.2970 - accuracy: 0.61 - ETA: 1:21 - loss: 1.2896 - accuracy: 0.61 - ETA: 1:20 - loss: 1.3085 - accuracy: 0.61 - ETA: 1:20 - loss: 1.2930 - accuracy: 0.61 - ETA: 1:19 - loss: 1.2949 - accuracy: 0.61 - ETA: 1:18 - loss: 1.3030 - accuracy: 0.61 - ETA: 1:17 - loss: 1.3072 - accuracy: 0.61 - ETA: 1:16 - loss: 1.2949 - accuracy: 0.61 - ETA: 1:15 - loss: 1.2992 - accuracy: 0.61 - ETA: 1:14 - loss: 1.2990 - accuracy: 0.61 - ETA: 1:13 - loss: 1.3108 - accuracy: 0.61 - ETA: 1:12 - loss: 1.3209 - accuracy: 0.60 - ETA: 1:11 - loss: 1.3193 - accuracy: 0.60 - ETA: 1:10 - loss: 1.3274 - accuracy: 0.60 - ETA: 1:09 - loss: 1.3266 - accuracy: 0.60 - ETA: 1:08 - loss: 1.3315 - accuracy: 0.60 - ETA: 1:07 - loss: 1.3285 - accuracy: 0.60 - ETA: 1:06 - loss: 1.3198 - accuracy: 0.60 - ETA: 1:06 - loss: 1.3184 - accuracy: 0.60 - ETA: 1:05 - loss: 1.3183 - accuracy: 0.60 - ETA: 1:04 - loss: 1.3201 - accuracy: 0.60 - ETA: 1:03 - loss: 1.3170 - accuracy: 0.60 - ETA: 1:02 - loss: 1.3151 - accuracy: 0.60 - ETA: 1:02 - loss: 1.3136 - accuracy: 0.60 - ETA: 1:01 - loss: 1.3135 - accuracy: 0.60 - ETA: 1:00 - loss: 1.3151 - accuracy: 0.60 - ETA: 1:00 - loss: 1.3276 - accuracy: 0.60 - ETA: 59s - loss: 1.3249 - accuracy: 0.6035 - ETA: 58s - loss: 1.3227 - accuracy: 0.604 - ETA: 57s - loss: 1.3209 - accuracy: 0.605 - ETA: 56s - loss: 1.3209 - accuracy: 0.606 - ETA: 55s - loss: 1.3201 - accuracy: 0.607 - ETA: 55s - loss: 1.3238 - accuracy: 0.605 - ETA: 54s - loss: 1.3214 - accuracy: 0.607 - ETA: 53s - loss: 1.3208 - accuracy: 0.607 - ETA: 52s - loss: 1.3193 - accuracy: 0.608 - ETA: 51s - loss: 1.3189 - accuracy: 0.608 - ETA: 50s - loss: 1.3201 - accuracy: 0.606 - ETA: 49s - loss: 1.3211 - accuracy: 0.606 - ETA: 48s - loss: 1.3186 - accuracy: 0.606 - ETA: 47s - loss: 1.3161 - accuracy: 0.607 - ETA: 46s - loss: 1.3178 - accuracy: 0.607 - ETA: 45s - loss: 1.3176 - accuracy: 0.606 - ETA: 44s - loss: 1.3150 - accuracy: 0.606 - ETA: 43s - loss: 1.3182 - accuracy: 0.605 - ETA: 42s - loss: 1.3136 - accuracy: 0.607 - ETA: 41s - loss: 1.3115 - accuracy: 0.607 - ETA: 41s - loss: 1.3116 - accuracy: 0.607 - ETA: 40s - loss: 1.3124 - accuracy: 0.607 - ETA: 39s - loss: 1.3125 - accuracy: 0.606 - ETA: 38s - loss: 1.3121 - accuracy: 0.607 - ETA: 37s - loss: 1.3172 - accuracy: 0.605 - ETA: 36s - loss: 1.3165 - accuracy: 0.606 - ETA: 35s - loss: 1.3145 - accuracy: 0.607 - ETA: 34s - loss: 1.3127 - accuracy: 0.606 - ETA: 33s - loss: 1.3126 - accuracy: 0.607 - ETA: 32s - loss: 1.3119 - accuracy: 0.608 - ETA: 31s - loss: 1.3089 - accuracy: 0.609 - ETA: 30s - loss: 1.3080 - accuracy: 0.609 - ETA: 29s - loss: 1.3111 - accuracy: 0.608 - ETA: 29s - loss: 1.3112 - accuracy: 0.608 - ETA: 28s - loss: 1.3166 - accuracy: 0.607 - ETA: 27s - loss: 1.3168 - accuracy: 0.606 - ETA: 26s - loss: 1.3142 - accuracy: 0.607 - ETA: 25s - loss: 1.3117 - accuracy: 0.609 - ETA: 24s - loss: 1.3092 - accuracy: 0.610 - ETA: 23s - loss: 1.3078 - accuracy: 0.609 - ETA: 22s - loss: 1.3105 - accuracy: 0.609 - ETA: 21s - loss: 1.3084 - accuracy: 0.610 - ETA: 20s - loss: 1.3060 - accuracy: 0.610 - ETA: 19s - loss: 1.3056 - accuracy: 0.610 - ETA: 18s - loss: 1.3034 - accuracy: 0.610 - ETA: 18s - loss: 1.3062 - accuracy: 0.610 - ETA: 17s - loss: 1.3072 - accuracy: 0.609 - ETA: 16s - loss: 1.3067 - accuracy: 0.610 - ETA: 15s - loss: 1.3077 - accuracy: 0.609 - ETA: 14s - loss: 1.3092 - accuracy: 0.608 - ETA: 13s - loss: 1.3106 - accuracy: 0.608 - ETA: 12s - loss: 1.3090 - accuracy: 0.608 - ETA: 11s - loss: 1.3071 - accuracy: 0.608 - ETA: 10s - loss: 1.3052 - accuracy: 0.609 - ETA: 9s - loss: 1.3076 - accuracy: 0.609 - ETA: 8s - loss: 1.3060 - accuracy: 0.60 - ETA: 7s - loss: 1.3070 - accuracy: 0.60 - ETA: 7s - loss: 1.3098 - accuracy: 0.60 - ETA: 6s - loss: 1.3094 - accuracy: 0.60 - ETA: 5s - loss: 1.3096 - accuracy: 0.60 - ETA: 4s - loss: 1.3103 - accuracy: 0.60 - ETA: 3s - loss: 1.3106 - accuracy: 0.60 - ETA: 2s - loss: 1.3106 - accuracy: 0.60 - ETA: 1s - loss: 1.3078 - accuracy: 0.60 - ETA: 0s - loss: 1.3056 - accuracy: 0.60 - 107s 8ms/step - loss: 1.3052 - accuracy: 0.6085 - val_loss: 2.6581 - val_accuracy: 0.3442\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:33 - loss: 1.2223 - accuracy: 0.61 - ETA: 1:30 - loss: 1.2370 - accuracy: 0.59 - ETA: 1:29 - loss: 1.2222 - accuracy: 0.60 - ETA: 1:28 - loss: 1.2259 - accuracy: 0.61 - ETA: 1:27 - loss: 1.2825 - accuracy: 0.58 - ETA: 1:26 - loss: 1.2468 - accuracy: 0.60 - ETA: 1:25 - loss: 1.2495 - accuracy: 0.60 - ETA: 1:23 - loss: 1.2344 - accuracy: 0.61 - ETA: 1:22 - loss: 1.2333 - accuracy: 0.62 - ETA: 1:21 - loss: 1.2370 - accuracy: 0.61 - ETA: 1:20 - loss: 1.2320 - accuracy: 0.62 - ETA: 1:19 - loss: 1.2558 - accuracy: 0.61 - ETA: 1:18 - loss: 1.2695 - accuracy: 0.61 - ETA: 1:18 - loss: 1.2784 - accuracy: 0.61 - ETA: 1:17 - loss: 1.2731 - accuracy: 0.61 - ETA: 1:16 - loss: 1.2668 - accuracy: 0.62 - ETA: 1:16 - loss: 1.2609 - accuracy: 0.62 - ETA: 1:15 - loss: 1.2553 - accuracy: 0.62 - ETA: 1:14 - loss: 1.2521 - accuracy: 0.62 - ETA: 1:13 - loss: 1.2543 - accuracy: 0.62 - ETA: 1:12 - loss: 1.2538 - accuracy: 0.62 - ETA: 1:11 - loss: 1.2578 - accuracy: 0.62 - ETA: 1:11 - loss: 1.2569 - accuracy: 0.62 - ETA: 1:10 - loss: 1.2663 - accuracy: 0.61 - ETA: 1:09 - loss: 1.2712 - accuracy: 0.62 - ETA: 1:08 - loss: 1.2786 - accuracy: 0.61 - ETA: 1:07 - loss: 1.2773 - accuracy: 0.61 - ETA: 1:06 - loss: 1.2771 - accuracy: 0.61 - ETA: 1:05 - loss: 1.2751 - accuracy: 0.61 - ETA: 1:04 - loss: 1.2733 - accuracy: 0.61 - ETA: 1:03 - loss: 1.2740 - accuracy: 0.61 - ETA: 1:02 - loss: 1.2702 - accuracy: 0.61 - ETA: 1:01 - loss: 1.2696 - accuracy: 0.61 - ETA: 1:01 - loss: 1.2685 - accuracy: 0.61 - ETA: 1:00 - loss: 1.2706 - accuracy: 0.61 - ETA: 59s - loss: 1.2704 - accuracy: 0.6161 - ETA: 58s - loss: 1.2683 - accuracy: 0.616 - ETA: 57s - loss: 1.2736 - accuracy: 0.615 - ETA: 56s - loss: 1.2752 - accuracy: 0.613 - ETA: 55s - loss: 1.2789 - accuracy: 0.611 - ETA: 54s - loss: 1.2771 - accuracy: 0.611 - ETA: 54s - loss: 1.2747 - accuracy: 0.612 - ETA: 53s - loss: 1.2729 - accuracy: 0.612 - ETA: 52s - loss: 1.2641 - accuracy: 0.616 - ETA: 51s - loss: 1.2673 - accuracy: 0.615 - ETA: 50s - loss: 1.2689 - accuracy: 0.614 - ETA: 49s - loss: 1.2769 - accuracy: 0.612 - ETA: 48s - loss: 1.2770 - accuracy: 0.611 - ETA: 47s - loss: 1.2773 - accuracy: 0.610 - ETA: 46s - loss: 1.2739 - accuracy: 0.611 - ETA: 45s - loss: 1.2751 - accuracy: 0.611 - ETA: 45s - loss: 1.2757 - accuracy: 0.611 - ETA: 44s - loss: 1.2767 - accuracy: 0.611 - ETA: 43s - loss: 1.2757 - accuracy: 0.611 - ETA: 42s - loss: 1.2820 - accuracy: 0.609 - ETA: 41s - loss: 1.2794 - accuracy: 0.610 - ETA: 40s - loss: 1.2797 - accuracy: 0.610 - ETA: 39s - loss: 1.2787 - accuracy: 0.610 - ETA: 38s - loss: 1.2790 - accuracy: 0.610 - ETA: 37s - loss: 1.2764 - accuracy: 0.611 - ETA: 36s - loss: 1.2766 - accuracy: 0.611 - ETA: 35s - loss: 1.2754 - accuracy: 0.611 - ETA: 35s - loss: 1.2773 - accuracy: 0.610 - ETA: 34s - loss: 1.2764 - accuracy: 0.610 - ETA: 33s - loss: 1.2763 - accuracy: 0.611 - ETA: 32s - loss: 1.2765 - accuracy: 0.611 - ETA: 31s - loss: 1.2771 - accuracy: 0.611 - ETA: 30s - loss: 1.2783 - accuracy: 0.610 - ETA: 29s - loss: 1.2760 - accuracy: 0.611 - ETA: 28s - loss: 1.2747 - accuracy: 0.612 - ETA: 27s - loss: 1.2753 - accuracy: 0.611 - ETA: 26s - loss: 1.2755 - accuracy: 0.611 - ETA: 26s - loss: 1.2762 - accuracy: 0.610 - ETA: 25s - loss: 1.2719 - accuracy: 0.611 - ETA: 24s - loss: 1.2708 - accuracy: 0.611 - ETA: 23s - loss: 1.2722 - accuracy: 0.611 - ETA: 22s - loss: 1.2697 - accuracy: 0.611 - ETA: 21s - loss: 1.2708 - accuracy: 0.611 - ETA: 20s - loss: 1.2706 - accuracy: 0.611 - ETA: 19s - loss: 1.2712 - accuracy: 0.611 - ETA: 18s - loss: 1.2721 - accuracy: 0.610 - ETA: 17s - loss: 1.2706 - accuracy: 0.610 - ETA: 16s - loss: 1.2701 - accuracy: 0.610 - ETA: 16s - loss: 1.2672 - accuracy: 0.611 - ETA: 15s - loss: 1.2652 - accuracy: 0.612 - ETA: 14s - loss: 1.2639 - accuracy: 0.612 - ETA: 13s - loss: 1.2666 - accuracy: 0.612 - ETA: 12s - loss: 1.2672 - accuracy: 0.612 - ETA: 11s - loss: 1.2697 - accuracy: 0.612 - ETA: 10s - loss: 1.2720 - accuracy: 0.612 - ETA: 9s - loss: 1.2737 - accuracy: 0.611 - ETA: 8s - loss: 1.2736 - accuracy: 0.61 - ETA: 7s - loss: 1.2737 - accuracy: 0.61 - ETA: 7s - loss: 1.2731 - accuracy: 0.61 - ETA: 6s - loss: 1.2709 - accuracy: 0.61 - ETA: 5s - loss: 1.2701 - accuracy: 0.61 - ETA: 4s - loss: 1.2690 - accuracy: 0.61 - ETA: 3s - loss: 1.2694 - accuracy: 0.61 - ETA: 2s - loss: 1.2712 - accuracy: 0.61 - ETA: 1s - loss: 1.2731 - accuracy: 0.61 - ETA: 0s - loss: 1.2738 - accuracy: 0.61 - 106s 8ms/step - loss: 1.2739 - accuracy: 0.6120 - val_loss: 2.6571 - val_accuracy: 0.3335\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 1.1503 - accuracy: 0.67 - ETA: 1:29 - loss: 1.1553 - accuracy: 0.62 - ETA: 1:28 - loss: 1.1842 - accuracy: 0.62 - ETA: 1:27 - loss: 1.2354 - accuracy: 0.61 - ETA: 1:27 - loss: 1.2540 - accuracy: 0.60 - ETA: 1:27 - loss: 1.2223 - accuracy: 0.61 - ETA: 1:27 - loss: 1.1949 - accuracy: 0.62 - ETA: 1:25 - loss: 1.2175 - accuracy: 0.62 - ETA: 1:24 - loss: 1.2262 - accuracy: 0.62 - ETA: 1:23 - loss: 1.2372 - accuracy: 0.62 - ETA: 1:22 - loss: 1.2276 - accuracy: 0.62 - ETA: 1:21 - loss: 1.2332 - accuracy: 0.61 - ETA: 1:20 - loss: 1.2347 - accuracy: 0.61 - ETA: 1:19 - loss: 1.2263 - accuracy: 0.62 - ETA: 1:18 - loss: 1.2277 - accuracy: 0.62 - ETA: 1:17 - loss: 1.2463 - accuracy: 0.61 - ETA: 1:16 - loss: 1.2378 - accuracy: 0.61 - ETA: 1:15 - loss: 1.2286 - accuracy: 0.61 - ETA: 1:14 - loss: 1.2255 - accuracy: 0.62 - ETA: 1:14 - loss: 1.2333 - accuracy: 0.61 - ETA: 1:13 - loss: 1.2408 - accuracy: 0.61 - ETA: 1:12 - loss: 1.2494 - accuracy: 0.61 - ETA: 1:11 - loss: 1.2422 - accuracy: 0.62 - ETA: 1:10 - loss: 1.2445 - accuracy: 0.61 - ETA: 1:09 - loss: 1.2485 - accuracy: 0.61 - ETA: 1:08 - loss: 1.2508 - accuracy: 0.61 - ETA: 1:08 - loss: 1.2503 - accuracy: 0.61 - ETA: 1:07 - loss: 1.2468 - accuracy: 0.61 - ETA: 1:06 - loss: 1.2420 - accuracy: 0.61 - ETA: 1:05 - loss: 1.2419 - accuracy: 0.61 - ETA: 1:04 - loss: 1.2405 - accuracy: 0.61 - ETA: 1:03 - loss: 1.2420 - accuracy: 0.61 - ETA: 1:02 - loss: 1.2373 - accuracy: 0.61 - ETA: 1:01 - loss: 1.2393 - accuracy: 0.61 - ETA: 1:00 - loss: 1.2420 - accuracy: 0.61 - ETA: 59s - loss: 1.2358 - accuracy: 0.6198 - ETA: 58s - loss: 1.2350 - accuracy: 0.619 - ETA: 58s - loss: 1.2329 - accuracy: 0.621 - ETA: 57s - loss: 1.2330 - accuracy: 0.621 - ETA: 56s - loss: 1.2278 - accuracy: 0.623 - ETA: 55s - loss: 1.2387 - accuracy: 0.621 - ETA: 54s - loss: 1.2409 - accuracy: 0.619 - ETA: 53s - loss: 1.2375 - accuracy: 0.620 - ETA: 52s - loss: 1.2335 - accuracy: 0.621 - ETA: 51s - loss: 1.2354 - accuracy: 0.620 - ETA: 50s - loss: 1.2348 - accuracy: 0.620 - ETA: 49s - loss: 1.2335 - accuracy: 0.620 - ETA: 48s - loss: 1.2340 - accuracy: 0.619 - ETA: 47s - loss: 1.2301 - accuracy: 0.621 - ETA: 46s - loss: 1.2283 - accuracy: 0.621 - ETA: 45s - loss: 1.2276 - accuracy: 0.621 - ETA: 45s - loss: 1.2271 - accuracy: 0.621 - ETA: 44s - loss: 1.2297 - accuracy: 0.620 - ETA: 43s - loss: 1.2298 - accuracy: 0.619 - ETA: 42s - loss: 1.2331 - accuracy: 0.618 - ETA: 41s - loss: 1.2334 - accuracy: 0.618 - ETA: 40s - loss: 1.2350 - accuracy: 0.618 - ETA: 39s - loss: 1.2310 - accuracy: 0.619 - ETA: 38s - loss: 1.2336 - accuracy: 0.619 - ETA: 37s - loss: 1.2334 - accuracy: 0.619 - ETA: 36s - loss: 1.2310 - accuracy: 0.620 - ETA: 35s - loss: 1.2316 - accuracy: 0.619 - ETA: 35s - loss: 1.2328 - accuracy: 0.618 - ETA: 34s - loss: 1.2321 - accuracy: 0.618 - ETA: 33s - loss: 1.2346 - accuracy: 0.618 - ETA: 32s - loss: 1.2346 - accuracy: 0.618 - ETA: 31s - loss: 1.2411 - accuracy: 0.617 - ETA: 30s - loss: 1.2391 - accuracy: 0.618 - ETA: 29s - loss: 1.2388 - accuracy: 0.617 - ETA: 28s - loss: 1.2370 - accuracy: 0.618 - ETA: 27s - loss: 1.2343 - accuracy: 0.618 - ETA: 26s - loss: 1.2325 - accuracy: 0.619 - ETA: 25s - loss: 1.2332 - accuracy: 0.619 - ETA: 25s - loss: 1.2349 - accuracy: 0.619 - ETA: 24s - loss: 1.2336 - accuracy: 0.620 - ETA: 23s - loss: 1.2339 - accuracy: 0.620 - ETA: 22s - loss: 1.2340 - accuracy: 0.620 - ETA: 21s - loss: 1.2359 - accuracy: 0.620 - ETA: 20s - loss: 1.2367 - accuracy: 0.620 - ETA: 19s - loss: 1.2359 - accuracy: 0.620 - ETA: 18s - loss: 1.2370 - accuracy: 0.620 - ETA: 17s - loss: 1.2387 - accuracy: 0.619 - ETA: 16s - loss: 1.2367 - accuracy: 0.619 - ETA: 16s - loss: 1.2361 - accuracy: 0.619 - ETA: 15s - loss: 1.2348 - accuracy: 0.620 - ETA: 14s - loss: 1.2354 - accuracy: 0.620 - ETA: 13s - loss: 1.2347 - accuracy: 0.620 - ETA: 12s - loss: 1.2348 - accuracy: 0.620 - ETA: 11s - loss: 1.2333 - accuracy: 0.620 - ETA: 10s - loss: 1.2328 - accuracy: 0.620 - ETA: 9s - loss: 1.2320 - accuracy: 0.621 - ETA: 8s - loss: 1.2328 - accuracy: 0.62 - ETA: 7s - loss: 1.2322 - accuracy: 0.62 - ETA: 6s - loss: 1.2343 - accuracy: 0.62 - ETA: 6s - loss: 1.2328 - accuracy: 0.62 - ETA: 5s - loss: 1.2361 - accuracy: 0.62 - ETA: 4s - loss: 1.2366 - accuracy: 0.62 - ETA: 3s - loss: 1.2376 - accuracy: 0.61 - ETA: 2s - loss: 1.2383 - accuracy: 0.61 - ETA: 1s - loss: 1.2357 - accuracy: 0.61 - ETA: 0s - loss: 1.2354 - accuracy: 0.61 - 106s 8ms/step - loss: 1.2344 - accuracy: 0.6194 - val_loss: 2.7077 - val_accuracy: 0.3267\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 1.3269 - accuracy: 0.60 - ETA: 1:28 - loss: 1.2439 - accuracy: 0.62 - ETA: 1:27 - loss: 1.2280 - accuracy: 0.62 - ETA: 1:26 - loss: 1.2257 - accuracy: 0.62 - ETA: 1:25 - loss: 1.2441 - accuracy: 0.62 - ETA: 1:24 - loss: 1.1937 - accuracy: 0.63 - ETA: 1:23 - loss: 1.1947 - accuracy: 0.64 - ETA: 1:22 - loss: 1.2025 - accuracy: 0.63 - ETA: 1:22 - loss: 1.2332 - accuracy: 0.62 - ETA: 1:21 - loss: 1.2147 - accuracy: 0.62 - ETA: 1:20 - loss: 1.2078 - accuracy: 0.62 - ETA: 1:20 - loss: 1.2077 - accuracy: 0.62 - ETA: 1:19 - loss: 1.2133 - accuracy: 0.62 - ETA: 1:18 - loss: 1.2192 - accuracy: 0.62 - ETA: 1:17 - loss: 1.2136 - accuracy: 0.62 - ETA: 1:17 - loss: 1.2198 - accuracy: 0.62 - ETA: 1:16 - loss: 1.2258 - accuracy: 0.62 - ETA: 1:15 - loss: 1.2311 - accuracy: 0.62 - ETA: 1:14 - loss: 1.2212 - accuracy: 0.62 - ETA: 1:13 - loss: 1.2194 - accuracy: 0.62 - ETA: 1:12 - loss: 1.2300 - accuracy: 0.62 - ETA: 1:11 - loss: 1.2385 - accuracy: 0.62 - ETA: 1:10 - loss: 1.2352 - accuracy: 0.62 - ETA: 1:09 - loss: 1.2337 - accuracy: 0.62 - ETA: 1:08 - loss: 1.2320 - accuracy: 0.62 - ETA: 1:07 - loss: 1.2316 - accuracy: 0.62 - ETA: 1:07 - loss: 1.2291 - accuracy: 0.62 - ETA: 1:06 - loss: 1.2276 - accuracy: 0.62 - ETA: 1:05 - loss: 1.2287 - accuracy: 0.62 - ETA: 1:04 - loss: 1.2293 - accuracy: 0.62 - ETA: 1:03 - loss: 1.2266 - accuracy: 0.62 - ETA: 1:02 - loss: 1.2248 - accuracy: 0.62 - ETA: 1:01 - loss: 1.2274 - accuracy: 0.62 - ETA: 1:00 - loss: 1.2260 - accuracy: 0.62 - ETA: 1:00 - loss: 1.2213 - accuracy: 0.62 - ETA: 59s - loss: 1.2192 - accuracy: 0.6278 - ETA: 58s - loss: 1.2171 - accuracy: 0.629 - ETA: 57s - loss: 1.2124 - accuracy: 0.630 - ETA: 56s - loss: 1.2100 - accuracy: 0.630 - ETA: 55s - loss: 1.2084 - accuracy: 0.631 - ETA: 54s - loss: 1.2072 - accuracy: 0.630 - ETA: 53s - loss: 1.2013 - accuracy: 0.632 - ETA: 52s - loss: 1.2006 - accuracy: 0.633 - ETA: 51s - loss: 1.1992 - accuracy: 0.633 - ETA: 50s - loss: 1.1975 - accuracy: 0.633 - ETA: 50s - loss: 1.1997 - accuracy: 0.632 - ETA: 49s - loss: 1.1975 - accuracy: 0.633 - ETA: 48s - loss: 1.1986 - accuracy: 0.633 - ETA: 47s - loss: 1.1944 - accuracy: 0.634 - ETA: 46s - loss: 1.1989 - accuracy: 0.632 - ETA: 45s - loss: 1.1972 - accuracy: 0.633 - ETA: 44s - loss: 1.1960 - accuracy: 0.633 - ETA: 43s - loss: 1.1943 - accuracy: 0.634 - ETA: 43s - loss: 1.1986 - accuracy: 0.632 - ETA: 42s - loss: 1.1970 - accuracy: 0.632 - ETA: 41s - loss: 1.1989 - accuracy: 0.632 - ETA: 40s - loss: 1.1995 - accuracy: 0.632 - ETA: 39s - loss: 1.2001 - accuracy: 0.632 - ETA: 38s - loss: 1.2024 - accuracy: 0.632 - ETA: 37s - loss: 1.2001 - accuracy: 0.633 - ETA: 37s - loss: 1.1989 - accuracy: 0.633 - ETA: 36s - loss: 1.2006 - accuracy: 0.633 - ETA: 35s - loss: 1.1991 - accuracy: 0.633 - ETA: 34s - loss: 1.1985 - accuracy: 0.633 - ETA: 33s - loss: 1.1973 - accuracy: 0.634 - ETA: 32s - loss: 1.1985 - accuracy: 0.633 - ETA: 31s - loss: 1.2012 - accuracy: 0.632 - ETA: 30s - loss: 1.2007 - accuracy: 0.633 - ETA: 29s - loss: 1.2035 - accuracy: 0.632 - ETA: 28s - loss: 1.2008 - accuracy: 0.632 - ETA: 28s - loss: 1.2002 - accuracy: 0.632 - ETA: 27s - loss: 1.2018 - accuracy: 0.632 - ETA: 26s - loss: 1.2011 - accuracy: 0.632 - ETA: 25s - loss: 1.2012 - accuracy: 0.632 - ETA: 24s - loss: 1.2026 - accuracy: 0.632 - ETA: 23s - loss: 1.2021 - accuracy: 0.632 - ETA: 22s - loss: 1.1994 - accuracy: 0.632 - ETA: 21s - loss: 1.1981 - accuracy: 0.633 - ETA: 20s - loss: 1.1983 - accuracy: 0.633 - ETA: 19s - loss: 1.1973 - accuracy: 0.634 - ETA: 18s - loss: 1.1999 - accuracy: 0.633 - ETA: 17s - loss: 1.2046 - accuracy: 0.632 - ETA: 17s - loss: 1.2058 - accuracy: 0.632 - ETA: 16s - loss: 1.2072 - accuracy: 0.632 - ETA: 15s - loss: 1.2064 - accuracy: 0.633 - ETA: 14s - loss: 1.2065 - accuracy: 0.633 - ETA: 13s - loss: 1.2056 - accuracy: 0.633 - ETA: 12s - loss: 1.2050 - accuracy: 0.633 - ETA: 11s - loss: 1.2027 - accuracy: 0.634 - ETA: 10s - loss: 1.2038 - accuracy: 0.634 - ETA: 9s - loss: 1.2045 - accuracy: 0.633 - ETA: 8s - loss: 1.2052 - accuracy: 0.63 - ETA: 7s - loss: 1.2062 - accuracy: 0.63 - ETA: 7s - loss: 1.2049 - accuracy: 0.63 - ETA: 6s - loss: 1.2024 - accuracy: 0.63 - ETA: 5s - loss: 1.2033 - accuracy: 0.63 - ETA: 4s - loss: 1.2016 - accuracy: 0.63 - ETA: 3s - loss: 1.2022 - accuracy: 0.63 - ETA: 2s - loss: 1.2046 - accuracy: 0.63 - ETA: 1s - loss: 1.2030 - accuracy: 0.63 - ETA: 0s - loss: 1.2028 - accuracy: 0.63 - 106s 8ms/step - loss: 1.2037 - accuracy: 0.6335 - val_loss: 2.6594 - val_accuracy: 0.3707\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:34 - loss: 1.2794 - accuracy: 0.61 - ETA: 1:33 - loss: 1.1938 - accuracy: 0.62 - ETA: 1:31 - loss: 1.1389 - accuracy: 0.64 - ETA: 1:29 - loss: 1.1553 - accuracy: 0.64 - ETA: 1:28 - loss: 1.1631 - accuracy: 0.63 - ETA: 1:27 - loss: 1.1611 - accuracy: 0.64 - ETA: 1:26 - loss: 1.1421 - accuracy: 0.64 - ETA: 1:24 - loss: 1.1308 - accuracy: 0.64 - ETA: 1:23 - loss: 1.1571 - accuracy: 0.63 - ETA: 1:23 - loss: 1.1301 - accuracy: 0.64 - ETA: 1:22 - loss: 1.1288 - accuracy: 0.64 - ETA: 1:21 - loss: 1.1264 - accuracy: 0.64 - ETA: 1:21 - loss: 1.1358 - accuracy: 0.64 - ETA: 1:20 - loss: 1.1457 - accuracy: 0.64 - ETA: 1:18 - loss: 1.1539 - accuracy: 0.63 - ETA: 1:18 - loss: 1.1590 - accuracy: 0.63 - ETA: 1:17 - loss: 1.1616 - accuracy: 0.63 - ETA: 1:16 - loss: 1.1568 - accuracy: 0.63 - ETA: 1:15 - loss: 1.1505 - accuracy: 0.64 - ETA: 1:14 - loss: 1.1518 - accuracy: 0.64 - ETA: 1:13 - loss: 1.1577 - accuracy: 0.63 - ETA: 1:13 - loss: 1.1620 - accuracy: 0.63 - ETA: 1:12 - loss: 1.1626 - accuracy: 0.63 - ETA: 1:11 - loss: 1.1507 - accuracy: 0.64 - ETA: 1:10 - loss: 1.1533 - accuracy: 0.64 - ETA: 1:09 - loss: 1.1590 - accuracy: 0.64 - ETA: 1:08 - loss: 1.1642 - accuracy: 0.64 - ETA: 1:07 - loss: 1.1644 - accuracy: 0.63 - ETA: 1:06 - loss: 1.1641 - accuracy: 0.63 - ETA: 1:05 - loss: 1.1649 - accuracy: 0.63 - ETA: 1:04 - loss: 1.1603 - accuracy: 0.63 - ETA: 1:03 - loss: 1.1502 - accuracy: 0.64 - ETA: 1:02 - loss: 1.1505 - accuracy: 0.64 - ETA: 1:01 - loss: 1.1540 - accuracy: 0.64 - ETA: 1:00 - loss: 1.1598 - accuracy: 0.63 - ETA: 59s - loss: 1.1618 - accuracy: 0.6389 - ETA: 58s - loss: 1.1617 - accuracy: 0.638 - ETA: 58s - loss: 1.1564 - accuracy: 0.639 - ETA: 57s - loss: 1.1522 - accuracy: 0.641 - ETA: 56s - loss: 1.1528 - accuracy: 0.640 - ETA: 55s - loss: 1.1591 - accuracy: 0.639 - ETA: 54s - loss: 1.1619 - accuracy: 0.639 - ETA: 53s - loss: 1.1622 - accuracy: 0.639 - ETA: 52s - loss: 1.1651 - accuracy: 0.638 - ETA: 51s - loss: 1.1613 - accuracy: 0.639 - ETA: 50s - loss: 1.1648 - accuracy: 0.639 - ETA: 49s - loss: 1.1635 - accuracy: 0.640 - ETA: 48s - loss: 1.1628 - accuracy: 0.641 - ETA: 47s - loss: 1.1631 - accuracy: 0.641 - ETA: 46s - loss: 1.1610 - accuracy: 0.641 - ETA: 45s - loss: 1.1621 - accuracy: 0.639 - ETA: 45s - loss: 1.1634 - accuracy: 0.639 - ETA: 44s - loss: 1.1643 - accuracy: 0.639 - ETA: 43s - loss: 1.1627 - accuracy: 0.639 - ETA: 42s - loss: 1.1626 - accuracy: 0.638 - ETA: 41s - loss: 1.1626 - accuracy: 0.638 - ETA: 40s - loss: 1.1645 - accuracy: 0.638 - ETA: 39s - loss: 1.1659 - accuracy: 0.638 - ETA: 38s - loss: 1.1655 - accuracy: 0.638 - ETA: 37s - loss: 1.1690 - accuracy: 0.636 - ETA: 36s - loss: 1.1653 - accuracy: 0.638 - ETA: 35s - loss: 1.1664 - accuracy: 0.639 - ETA: 35s - loss: 1.1651 - accuracy: 0.639 - ETA: 34s - loss: 1.1666 - accuracy: 0.638 - ETA: 33s - loss: 1.1665 - accuracy: 0.638 - ETA: 32s - loss: 1.1659 - accuracy: 0.639 - ETA: 31s - loss: 1.1627 - accuracy: 0.640 - ETA: 30s - loss: 1.1639 - accuracy: 0.639 - ETA: 29s - loss: 1.1659 - accuracy: 0.639 - ETA: 28s - loss: 1.1700 - accuracy: 0.637 - ETA: 27s - loss: 1.1703 - accuracy: 0.637 - ETA: 26s - loss: 1.1694 - accuracy: 0.637 - ETA: 25s - loss: 1.1658 - accuracy: 0.639 - ETA: 25s - loss: 1.1623 - accuracy: 0.640 - ETA: 24s - loss: 1.1614 - accuracy: 0.640 - ETA: 23s - loss: 1.1618 - accuracy: 0.640 - ETA: 22s - loss: 1.1609 - accuracy: 0.640 - ETA: 21s - loss: 1.1601 - accuracy: 0.640 - ETA: 20s - loss: 1.1626 - accuracy: 0.639 - ETA: 19s - loss: 1.1631 - accuracy: 0.639 - ETA: 18s - loss: 1.1594 - accuracy: 0.640 - ETA: 17s - loss: 1.1621 - accuracy: 0.640 - ETA: 16s - loss: 1.1604 - accuracy: 0.640 - ETA: 16s - loss: 1.1609 - accuracy: 0.640 - ETA: 15s - loss: 1.1623 - accuracy: 0.639 - ETA: 14s - loss: 1.1612 - accuracy: 0.639 - ETA: 13s - loss: 1.1618 - accuracy: 0.639 - ETA: 12s - loss: 1.1625 - accuracy: 0.639 - ETA: 11s - loss: 1.1626 - accuracy: 0.638 - ETA: 10s - loss: 1.1632 - accuracy: 0.638 - ETA: 9s - loss: 1.1637 - accuracy: 0.638 - ETA: 8s - loss: 1.1675 - accuracy: 0.63 - ETA: 7s - loss: 1.1672 - accuracy: 0.63 - ETA: 7s - loss: 1.1674 - accuracy: 0.63 - ETA: 6s - loss: 1.1687 - accuracy: 0.63 - ETA: 5s - loss: 1.1690 - accuracy: 0.63 - ETA: 4s - loss: 1.1677 - accuracy: 0.63 - ETA: 3s - loss: 1.1688 - accuracy: 0.63 - ETA: 2s - loss: 1.1703 - accuracy: 0.63 - ETA: 1s - loss: 1.1678 - accuracy: 0.63 - ETA: 0s - loss: 1.1699 - accuracy: 0.63 - 106s 8ms/step - loss: 1.1689 - accuracy: 0.6379 - val_loss: 2.6174 - val_accuracy: 0.3338\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:29 - loss: 1.0400 - accuracy: 0.62 - ETA: 1:28 - loss: 1.1542 - accuracy: 0.60 - ETA: 1:28 - loss: 1.1311 - accuracy: 0.62 - ETA: 1:29 - loss: 1.1196 - accuracy: 0.63 - ETA: 1:28 - loss: 1.1427 - accuracy: 0.63 - ETA: 1:28 - loss: 1.1214 - accuracy: 0.64 - ETA: 1:27 - loss: 1.1365 - accuracy: 0.63 - ETA: 1:25 - loss: 1.1414 - accuracy: 0.63 - ETA: 1:24 - loss: 1.1479 - accuracy: 0.63 - ETA: 1:23 - loss: 1.1535 - accuracy: 0.62 - ETA: 1:22 - loss: 1.1726 - accuracy: 0.62 - ETA: 1:20 - loss: 1.1519 - accuracy: 0.63 - ETA: 1:20 - loss: 1.1512 - accuracy: 0.63 - ETA: 1:19 - loss: 1.1416 - accuracy: 0.64 - ETA: 1:18 - loss: 1.1455 - accuracy: 0.64 - ETA: 1:17 - loss: 1.1614 - accuracy: 0.64 - ETA: 1:15 - loss: 1.1573 - accuracy: 0.64 - ETA: 1:14 - loss: 1.1473 - accuracy: 0.64 - ETA: 1:13 - loss: 1.1411 - accuracy: 0.64 - ETA: 1:13 - loss: 1.1539 - accuracy: 0.64 - ETA: 1:12 - loss: 1.1627 - accuracy: 0.64 - ETA: 1:11 - loss: 1.1602 - accuracy: 0.64 - ETA: 1:10 - loss: 1.1544 - accuracy: 0.64 - ETA: 1:09 - loss: 1.1583 - accuracy: 0.64 - ETA: 1:08 - loss: 1.1589 - accuracy: 0.64 - ETA: 1:07 - loss: 1.1567 - accuracy: 0.64 - ETA: 1:07 - loss: 1.1567 - accuracy: 0.64 - ETA: 1:06 - loss: 1.1552 - accuracy: 0.64 - ETA: 1:05 - loss: 1.1545 - accuracy: 0.64 - ETA: 1:04 - loss: 1.1504 - accuracy: 0.64 - ETA: 1:03 - loss: 1.1460 - accuracy: 0.64 - ETA: 1:02 - loss: 1.1479 - accuracy: 0.64 - ETA: 1:01 - loss: 1.1481 - accuracy: 0.64 - ETA: 1:00 - loss: 1.1432 - accuracy: 0.64 - ETA: 59s - loss: 1.1522 - accuracy: 0.6458 - ETA: 58s - loss: 1.1549 - accuracy: 0.646 - ETA: 58s - loss: 1.1560 - accuracy: 0.645 - ETA: 57s - loss: 1.1516 - accuracy: 0.647 - ETA: 56s - loss: 1.1565 - accuracy: 0.646 - ETA: 55s - loss: 1.1541 - accuracy: 0.646 - ETA: 54s - loss: 1.1524 - accuracy: 0.646 - ETA: 53s - loss: 1.1559 - accuracy: 0.647 - ETA: 52s - loss: 1.1589 - accuracy: 0.646 - ETA: 52s - loss: 1.1663 - accuracy: 0.643 - ETA: 51s - loss: 1.1679 - accuracy: 0.643 - ETA: 50s - loss: 1.1654 - accuracy: 0.644 - ETA: 49s - loss: 1.1659 - accuracy: 0.644 - ETA: 48s - loss: 1.1666 - accuracy: 0.645 - ETA: 47s - loss: 1.1645 - accuracy: 0.645 - ETA: 46s - loss: 1.1663 - accuracy: 0.645 - ETA: 45s - loss: 1.1638 - accuracy: 0.646 - ETA: 44s - loss: 1.1669 - accuracy: 0.644 - ETA: 43s - loss: 1.1707 - accuracy: 0.643 - ETA: 42s - loss: 1.1696 - accuracy: 0.643 - ETA: 42s - loss: 1.1650 - accuracy: 0.643 - ETA: 41s - loss: 1.1612 - accuracy: 0.644 - ETA: 40s - loss: 1.1604 - accuracy: 0.644 - ETA: 39s - loss: 1.1624 - accuracy: 0.644 - ETA: 38s - loss: 1.1631 - accuracy: 0.643 - ETA: 37s - loss: 1.1606 - accuracy: 0.644 - ETA: 36s - loss: 1.1605 - accuracy: 0.644 - ETA: 35s - loss: 1.1600 - accuracy: 0.644 - ETA: 34s - loss: 1.1642 - accuracy: 0.643 - ETA: 33s - loss: 1.1633 - accuracy: 0.643 - ETA: 33s - loss: 1.1651 - accuracy: 0.642 - ETA: 32s - loss: 1.1650 - accuracy: 0.643 - ETA: 31s - loss: 1.1661 - accuracy: 0.643 - ETA: 30s - loss: 1.1639 - accuracy: 0.644 - ETA: 29s - loss: 1.1638 - accuracy: 0.643 - ETA: 28s - loss: 1.1629 - accuracy: 0.644 - ETA: 27s - loss: 1.1614 - accuracy: 0.644 - ETA: 26s - loss: 1.1607 - accuracy: 0.644 - ETA: 25s - loss: 1.1618 - accuracy: 0.643 - ETA: 24s - loss: 1.1625 - accuracy: 0.644 - ETA: 23s - loss: 1.1611 - accuracy: 0.644 - ETA: 23s - loss: 1.1632 - accuracy: 0.644 - ETA: 22s - loss: 1.1639 - accuracy: 0.644 - ETA: 21s - loss: 1.1637 - accuracy: 0.645 - ETA: 20s - loss: 1.1619 - accuracy: 0.645 - ETA: 19s - loss: 1.1639 - accuracy: 0.644 - ETA: 18s - loss: 1.1651 - accuracy: 0.644 - ETA: 17s - loss: 1.1645 - accuracy: 0.644 - ETA: 16s - loss: 1.1638 - accuracy: 0.644 - ETA: 15s - loss: 1.1607 - accuracy: 0.645 - ETA: 15s - loss: 1.1609 - accuracy: 0.645 - ETA: 14s - loss: 1.1598 - accuracy: 0.645 - ETA: 13s - loss: 1.1602 - accuracy: 0.645 - ETA: 12s - loss: 1.1580 - accuracy: 0.645 - ETA: 11s - loss: 1.1569 - accuracy: 0.645 - ETA: 10s - loss: 1.1565 - accuracy: 0.645 - ETA: 9s - loss: 1.1566 - accuracy: 0.645 - ETA: 8s - loss: 1.1561 - accuracy: 0.64 - ETA: 7s - loss: 1.1566 - accuracy: 0.64 - ETA: 6s - loss: 1.1553 - accuracy: 0.64 - ETA: 6s - loss: 1.1557 - accuracy: 0.64 - ETA: 5s - loss: 1.1590 - accuracy: 0.64 - ETA: 4s - loss: 1.1574 - accuracy: 0.64 - ETA: 3s - loss: 1.1575 - accuracy: 0.64 - ETA: 2s - loss: 1.1563 - accuracy: 0.64 - ETA: 1s - loss: 1.1571 - accuracy: 0.64 - ETA: 0s - loss: 1.1534 - accuracy: 0.64 - 105s 8ms/step - loss: 1.1517 - accuracy: 0.6464 - val_loss: 2.7110 - val_accuracy: 0.3453\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 1.4259 - accuracy: 0.57 - ETA: 1:30 - loss: 1.2366 - accuracy: 0.62 - ETA: 1:29 - loss: 1.2612 - accuracy: 0.61 - ETA: 1:27 - loss: 1.2020 - accuracy: 0.63 - ETA: 1:26 - loss: 1.1901 - accuracy: 0.64 - ETA: 1:25 - loss: 1.2082 - accuracy: 0.63 - ETA: 1:24 - loss: 1.2166 - accuracy: 0.63 - ETA: 1:23 - loss: 1.1848 - accuracy: 0.64 - ETA: 1:23 - loss: 1.1837 - accuracy: 0.64 - ETA: 1:22 - loss: 1.1948 - accuracy: 0.64 - ETA: 1:21 - loss: 1.1997 - accuracy: 0.64 - ETA: 1:20 - loss: 1.1912 - accuracy: 0.64 - ETA: 1:19 - loss: 1.1771 - accuracy: 0.64 - ETA: 1:18 - loss: 1.1892 - accuracy: 0.63 - ETA: 1:17 - loss: 1.1852 - accuracy: 0.63 - ETA: 1:17 - loss: 1.1769 - accuracy: 0.63 - ETA: 1:16 - loss: 1.1640 - accuracy: 0.64 - ETA: 1:15 - loss: 1.1600 - accuracy: 0.64 - ETA: 1:14 - loss: 1.1671 - accuracy: 0.64 - ETA: 1:13 - loss: 1.1622 - accuracy: 0.64 - ETA: 1:12 - loss: 1.1645 - accuracy: 0.64 - ETA: 1:11 - loss: 1.1582 - accuracy: 0.64 - ETA: 1:10 - loss: 1.1532 - accuracy: 0.64 - ETA: 1:09 - loss: 1.1505 - accuracy: 0.64 - ETA: 1:08 - loss: 1.1419 - accuracy: 0.64 - ETA: 1:08 - loss: 1.1378 - accuracy: 0.64 - ETA: 1:07 - loss: 1.1337 - accuracy: 0.65 - ETA: 1:06 - loss: 1.1256 - accuracy: 0.65 - ETA: 1:05 - loss: 1.1201 - accuracy: 0.65 - ETA: 1:04 - loss: 1.1218 - accuracy: 0.65 - ETA: 1:03 - loss: 1.1280 - accuracy: 0.64 - ETA: 1:03 - loss: 1.1264 - accuracy: 0.64 - ETA: 1:02 - loss: 1.1205 - accuracy: 0.65 - ETA: 1:01 - loss: 1.1183 - accuracy: 0.65 - ETA: 1:00 - loss: 1.1172 - accuracy: 0.65 - ETA: 59s - loss: 1.1150 - accuracy: 0.6532 - ETA: 58s - loss: 1.1177 - accuracy: 0.652 - ETA: 57s - loss: 1.1187 - accuracy: 0.652 - ETA: 56s - loss: 1.1240 - accuracy: 0.651 - ETA: 55s - loss: 1.1183 - accuracy: 0.651 - ETA: 54s - loss: 1.1158 - accuracy: 0.652 - ETA: 53s - loss: 1.1144 - accuracy: 0.653 - ETA: 52s - loss: 1.1133 - accuracy: 0.653 - ETA: 52s - loss: 1.1080 - accuracy: 0.654 - ETA: 51s - loss: 1.1089 - accuracy: 0.654 - ETA: 50s - loss: 1.1118 - accuracy: 0.654 - ETA: 49s - loss: 1.1093 - accuracy: 0.654 - ETA: 48s - loss: 1.1074 - accuracy: 0.655 - ETA: 47s - loss: 1.1114 - accuracy: 0.655 - ETA: 46s - loss: 1.1133 - accuracy: 0.653 - ETA: 45s - loss: 1.1182 - accuracy: 0.653 - ETA: 44s - loss: 1.1193 - accuracy: 0.653 - ETA: 44s - loss: 1.1193 - accuracy: 0.653 - ETA: 43s - loss: 1.1190 - accuracy: 0.653 - ETA: 42s - loss: 1.1215 - accuracy: 0.653 - ETA: 41s - loss: 1.1234 - accuracy: 0.653 - ETA: 40s - loss: 1.1253 - accuracy: 0.652 - ETA: 39s - loss: 1.1247 - accuracy: 0.651 - ETA: 38s - loss: 1.1220 - accuracy: 0.652 - ETA: 37s - loss: 1.1227 - accuracy: 0.653 - ETA: 36s - loss: 1.1211 - accuracy: 0.654 - ETA: 35s - loss: 1.1229 - accuracy: 0.653 - ETA: 34s - loss: 1.1195 - accuracy: 0.654 - ETA: 34s - loss: 1.1188 - accuracy: 0.654 - ETA: 33s - loss: 1.1173 - accuracy: 0.654 - ETA: 32s - loss: 1.1200 - accuracy: 0.654 - ETA: 31s - loss: 1.1216 - accuracy: 0.653 - ETA: 30s - loss: 1.1222 - accuracy: 0.653 - ETA: 29s - loss: 1.1197 - accuracy: 0.654 - ETA: 28s - loss: 1.1201 - accuracy: 0.654 - ETA: 27s - loss: 1.1225 - accuracy: 0.653 - ETA: 26s - loss: 1.1230 - accuracy: 0.653 - ETA: 25s - loss: 1.1240 - accuracy: 0.653 - ETA: 25s - loss: 1.1244 - accuracy: 0.652 - ETA: 24s - loss: 1.1257 - accuracy: 0.652 - ETA: 23s - loss: 1.1267 - accuracy: 0.651 - ETA: 22s - loss: 1.1263 - accuracy: 0.651 - ETA: 21s - loss: 1.1294 - accuracy: 0.650 - ETA: 20s - loss: 1.1303 - accuracy: 0.649 - ETA: 19s - loss: 1.1299 - accuracy: 0.650 - ETA: 18s - loss: 1.1288 - accuracy: 0.650 - ETA: 17s - loss: 1.1278 - accuracy: 0.650 - ETA: 16s - loss: 1.1290 - accuracy: 0.649 - ETA: 16s - loss: 1.1295 - accuracy: 0.649 - ETA: 15s - loss: 1.1304 - accuracy: 0.649 - ETA: 14s - loss: 1.1295 - accuracy: 0.649 - ETA: 13s - loss: 1.1290 - accuracy: 0.650 - ETA: 12s - loss: 1.1301 - accuracy: 0.649 - ETA: 11s - loss: 1.1295 - accuracy: 0.649 - ETA: 10s - loss: 1.1320 - accuracy: 0.649 - ETA: 9s - loss: 1.1311 - accuracy: 0.649 - ETA: 8s - loss: 1.1304 - accuracy: 0.64 - ETA: 7s - loss: 1.1306 - accuracy: 0.64 - ETA: 6s - loss: 1.1304 - accuracy: 0.64 - ETA: 6s - loss: 1.1293 - accuracy: 0.64 - ETA: 5s - loss: 1.1306 - accuracy: 0.64 - ETA: 4s - loss: 1.1300 - accuracy: 0.64 - ETA: 3s - loss: 1.1311 - accuracy: 0.64 - ETA: 2s - loss: 1.1295 - accuracy: 0.64 - ETA: 1s - loss: 1.1297 - accuracy: 0.64 - ETA: 0s - loss: 1.1307 - accuracy: 0.64 - 106s 8ms/step - loss: 1.1333 - accuracy: 0.6491 - val_loss: 2.7733 - val_accuracy: 0.3146\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:38 - loss: 1.0247 - accuracy: 0.66 - ETA: 1:33 - loss: 0.9779 - accuracy: 0.66 - ETA: 1:31 - loss: 1.0505 - accuracy: 0.66 - ETA: 1:30 - loss: 1.0986 - accuracy: 0.66 - ETA: 1:28 - loss: 1.1162 - accuracy: 0.66 - ETA: 1:26 - loss: 1.1125 - accuracy: 0.66 - ETA: 1:25 - loss: 1.1104 - accuracy: 0.66 - ETA: 1:24 - loss: 1.1105 - accuracy: 0.66 - ETA: 1:23 - loss: 1.1363 - accuracy: 0.66 - ETA: 1:22 - loss: 1.1292 - accuracy: 0.66 - ETA: 1:21 - loss: 1.1361 - accuracy: 0.66 - ETA: 1:21 - loss: 1.1312 - accuracy: 0.66 - ETA: 1:20 - loss: 1.1363 - accuracy: 0.66 - ETA: 1:19 - loss: 1.1407 - accuracy: 0.66 - ETA: 1:18 - loss: 1.1268 - accuracy: 0.66 - ETA: 1:17 - loss: 1.1459 - accuracy: 0.65 - ETA: 1:16 - loss: 1.1483 - accuracy: 0.65 - ETA: 1:15 - loss: 1.1437 - accuracy: 0.65 - ETA: 1:14 - loss: 1.1402 - accuracy: 0.65 - ETA: 1:13 - loss: 1.1330 - accuracy: 0.65 - ETA: 1:13 - loss: 1.1294 - accuracy: 0.65 - ETA: 1:12 - loss: 1.1258 - accuracy: 0.65 - ETA: 1:11 - loss: 1.1299 - accuracy: 0.65 - ETA: 1:10 - loss: 1.1249 - accuracy: 0.65 - ETA: 1:09 - loss: 1.1227 - accuracy: 0.65 - ETA: 1:08 - loss: 1.1305 - accuracy: 0.65 - ETA: 1:07 - loss: 1.1280 - accuracy: 0.65 - ETA: 1:06 - loss: 1.1253 - accuracy: 0.65 - ETA: 1:05 - loss: 1.1226 - accuracy: 0.65 - ETA: 1:05 - loss: 1.1236 - accuracy: 0.65 - ETA: 1:04 - loss: 1.1261 - accuracy: 0.65 - ETA: 1:03 - loss: 1.1202 - accuracy: 0.65 - ETA: 1:02 - loss: 1.1190 - accuracy: 0.65 - ETA: 1:01 - loss: 1.1159 - accuracy: 0.65 - ETA: 1:00 - loss: 1.1234 - accuracy: 0.65 - ETA: 59s - loss: 1.1272 - accuracy: 0.6513 - ETA: 58s - loss: 1.1207 - accuracy: 0.653 - ETA: 57s - loss: 1.1209 - accuracy: 0.652 - ETA: 56s - loss: 1.1204 - accuracy: 0.652 - ETA: 55s - loss: 1.1246 - accuracy: 0.651 - ETA: 54s - loss: 1.1309 - accuracy: 0.651 - ETA: 53s - loss: 1.1356 - accuracy: 0.648 - ETA: 52s - loss: 1.1380 - accuracy: 0.647 - ETA: 52s - loss: 1.1405 - accuracy: 0.647 - ETA: 51s - loss: 1.1435 - accuracy: 0.646 - ETA: 50s - loss: 1.1423 - accuracy: 0.645 - ETA: 49s - loss: 1.1397 - accuracy: 0.646 - ETA: 48s - loss: 1.1413 - accuracy: 0.647 - ETA: 47s - loss: 1.1392 - accuracy: 0.648 - ETA: 46s - loss: 1.1378 - accuracy: 0.648 - ETA: 45s - loss: 1.1378 - accuracy: 0.648 - ETA: 44s - loss: 1.1343 - accuracy: 0.648 - ETA: 43s - loss: 1.1353 - accuracy: 0.647 - ETA: 42s - loss: 1.1322 - accuracy: 0.649 - ETA: 41s - loss: 1.1293 - accuracy: 0.650 - ETA: 41s - loss: 1.1270 - accuracy: 0.651 - ETA: 40s - loss: 1.1225 - accuracy: 0.653 - ETA: 39s - loss: 1.1205 - accuracy: 0.653 - ETA: 38s - loss: 1.1194 - accuracy: 0.653 - ETA: 37s - loss: 1.1175 - accuracy: 0.654 - ETA: 36s - loss: 1.1180 - accuracy: 0.654 - ETA: 35s - loss: 1.1203 - accuracy: 0.654 - ETA: 34s - loss: 1.1182 - accuracy: 0.655 - ETA: 33s - loss: 1.1171 - accuracy: 0.656 - ETA: 32s - loss: 1.1193 - accuracy: 0.655 - ETA: 32s - loss: 1.1197 - accuracy: 0.655 - ETA: 31s - loss: 1.1195 - accuracy: 0.655 - ETA: 30s - loss: 1.1175 - accuracy: 0.655 - ETA: 29s - loss: 1.1161 - accuracy: 0.655 - ETA: 28s - loss: 1.1145 - accuracy: 0.656 - ETA: 27s - loss: 1.1157 - accuracy: 0.656 - ETA: 26s - loss: 1.1165 - accuracy: 0.655 - ETA: 25s - loss: 1.1136 - accuracy: 0.656 - ETA: 24s - loss: 1.1139 - accuracy: 0.655 - ETA: 24s - loss: 1.1141 - accuracy: 0.655 - ETA: 23s - loss: 1.1126 - accuracy: 0.655 - ETA: 22s - loss: 1.1116 - accuracy: 0.655 - ETA: 21s - loss: 1.1121 - accuracy: 0.655 - ETA: 20s - loss: 1.1118 - accuracy: 0.655 - ETA: 19s - loss: 1.1137 - accuracy: 0.655 - ETA: 18s - loss: 1.1130 - accuracy: 0.655 - ETA: 17s - loss: 1.1141 - accuracy: 0.655 - ETA: 16s - loss: 1.1119 - accuracy: 0.655 - ETA: 15s - loss: 1.1138 - accuracy: 0.655 - ETA: 15s - loss: 1.1124 - accuracy: 0.655 - ETA: 14s - loss: 1.1112 - accuracy: 0.656 - ETA: 13s - loss: 1.1117 - accuracy: 0.656 - ETA: 12s - loss: 1.1123 - accuracy: 0.655 - ETA: 11s - loss: 1.1123 - accuracy: 0.655 - ETA: 10s - loss: 1.1134 - accuracy: 0.654 - ETA: 9s - loss: 1.1131 - accuracy: 0.655 - ETA: 8s - loss: 1.1135 - accuracy: 0.65 - ETA: 7s - loss: 1.1113 - accuracy: 0.65 - ETA: 6s - loss: 1.1082 - accuracy: 0.65 - ETA: 6s - loss: 1.1061 - accuracy: 0.65 - ETA: 5s - loss: 1.1075 - accuracy: 0.65 - ETA: 4s - loss: 1.1070 - accuracy: 0.65 - ETA: 3s - loss: 1.1039 - accuracy: 0.65 - ETA: 2s - loss: 1.1025 - accuracy: 0.65 - ETA: 1s - loss: 1.1010 - accuracy: 0.65 - ETA: 0s - loss: 1.1007 - accuracy: 0.65 - 105s 8ms/step - loss: 1.1015 - accuracy: 0.6579 - val_loss: 2.7136 - val_accuracy: 0.3580\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 0.9820 - accuracy: 0.67 - ETA: 1:28 - loss: 1.0446 - accuracy: 0.66 - ETA: 1:27 - loss: 1.1038 - accuracy: 0.66 - ETA: 1:27 - loss: 1.1271 - accuracy: 0.66 - ETA: 1:26 - loss: 1.1509 - accuracy: 0.65 - ETA: 1:25 - loss: 1.1174 - accuracy: 0.66 - ETA: 1:25 - loss: 1.0983 - accuracy: 0.66 - ETA: 1:24 - loss: 1.0781 - accuracy: 0.66 - ETA: 1:23 - loss: 1.0825 - accuracy: 0.66 - ETA: 1:22 - loss: 1.0886 - accuracy: 0.66 - ETA: 1:22 - loss: 1.0720 - accuracy: 0.66 - ETA: 1:21 - loss: 1.0745 - accuracy: 0.66 - ETA: 1:20 - loss: 1.0884 - accuracy: 0.66 - ETA: 1:19 - loss: 1.0827 - accuracy: 0.66 - ETA: 1:18 - loss: 1.0758 - accuracy: 0.66 - ETA: 1:17 - loss: 1.0700 - accuracy: 0.66 - ETA: 1:16 - loss: 1.0637 - accuracy: 0.66 - ETA: 1:15 - loss: 1.0636 - accuracy: 0.66 - ETA: 1:14 - loss: 1.0570 - accuracy: 0.67 - ETA: 1:13 - loss: 1.0468 - accuracy: 0.67 - ETA: 1:12 - loss: 1.0646 - accuracy: 0.67 - ETA: 1:11 - loss: 1.0714 - accuracy: 0.66 - ETA: 1:10 - loss: 1.0749 - accuracy: 0.66 - ETA: 1:09 - loss: 1.0801 - accuracy: 0.66 - ETA: 1:08 - loss: 1.0842 - accuracy: 0.66 - ETA: 1:08 - loss: 1.0783 - accuracy: 0.66 - ETA: 1:07 - loss: 1.0739 - accuracy: 0.66 - ETA: 1:06 - loss: 1.0757 - accuracy: 0.66 - ETA: 1:05 - loss: 1.0759 - accuracy: 0.66 - ETA: 1:04 - loss: 1.0760 - accuracy: 0.66 - ETA: 1:03 - loss: 1.0816 - accuracy: 0.66 - ETA: 1:02 - loss: 1.0829 - accuracy: 0.66 - ETA: 1:01 - loss: 1.0851 - accuracy: 0.66 - ETA: 1:00 - loss: 1.0837 - accuracy: 0.66 - ETA: 59s - loss: 1.0862 - accuracy: 0.6643 - ETA: 59s - loss: 1.0820 - accuracy: 0.663 - ETA: 58s - loss: 1.0862 - accuracy: 0.662 - ETA: 57s - loss: 1.0936 - accuracy: 0.660 - ETA: 56s - loss: 1.0968 - accuracy: 0.659 - ETA: 55s - loss: 1.1025 - accuracy: 0.658 - ETA: 54s - loss: 1.1053 - accuracy: 0.657 - ETA: 53s - loss: 1.1040 - accuracy: 0.657 - ETA: 52s - loss: 1.1022 - accuracy: 0.658 - ETA: 51s - loss: 1.1009 - accuracy: 0.658 - ETA: 50s - loss: 1.1014 - accuracy: 0.656 - ETA: 50s - loss: 1.1003 - accuracy: 0.658 - ETA: 49s - loss: 1.0978 - accuracy: 0.658 - ETA: 48s - loss: 1.0984 - accuracy: 0.658 - ETA: 47s - loss: 1.0924 - accuracy: 0.660 - ETA: 46s - loss: 1.0949 - accuracy: 0.660 - ETA: 45s - loss: 1.0972 - accuracy: 0.659 - ETA: 45s - loss: 1.0997 - accuracy: 0.659 - ETA: 44s - loss: 1.1000 - accuracy: 0.659 - ETA: 43s - loss: 1.0996 - accuracy: 0.659 - ETA: 42s - loss: 1.0961 - accuracy: 0.660 - ETA: 41s - loss: 1.0991 - accuracy: 0.659 - ETA: 40s - loss: 1.1005 - accuracy: 0.658 - ETA: 39s - loss: 1.1063 - accuracy: 0.657 - ETA: 38s - loss: 1.1055 - accuracy: 0.656 - ETA: 37s - loss: 1.1035 - accuracy: 0.657 - ETA: 36s - loss: 1.1023 - accuracy: 0.657 - ETA: 35s - loss: 1.1071 - accuracy: 0.657 - ETA: 35s - loss: 1.1092 - accuracy: 0.656 - ETA: 34s - loss: 1.1071 - accuracy: 0.657 - ETA: 33s - loss: 1.1063 - accuracy: 0.657 - ETA: 32s - loss: 1.1051 - accuracy: 0.657 - ETA: 31s - loss: 1.0999 - accuracy: 0.659 - ETA: 30s - loss: 1.1008 - accuracy: 0.659 - ETA: 29s - loss: 1.1013 - accuracy: 0.659 - ETA: 28s - loss: 1.0991 - accuracy: 0.659 - ETA: 27s - loss: 1.0968 - accuracy: 0.661 - ETA: 26s - loss: 1.0970 - accuracy: 0.660 - ETA: 25s - loss: 1.0965 - accuracy: 0.661 - ETA: 25s - loss: 1.0958 - accuracy: 0.661 - ETA: 24s - loss: 1.0949 - accuracy: 0.661 - ETA: 23s - loss: 1.0926 - accuracy: 0.662 - ETA: 22s - loss: 1.0906 - accuracy: 0.662 - ETA: 21s - loss: 1.0910 - accuracy: 0.662 - ETA: 20s - loss: 1.0915 - accuracy: 0.662 - ETA: 19s - loss: 1.0908 - accuracy: 0.662 - ETA: 18s - loss: 1.0932 - accuracy: 0.661 - ETA: 17s - loss: 1.0938 - accuracy: 0.661 - ETA: 16s - loss: 1.0935 - accuracy: 0.662 - ETA: 15s - loss: 1.0946 - accuracy: 0.661 - ETA: 15s - loss: 1.0939 - accuracy: 0.661 - ETA: 14s - loss: 1.0940 - accuracy: 0.661 - ETA: 13s - loss: 1.0948 - accuracy: 0.661 - ETA: 12s - loss: 1.0961 - accuracy: 0.661 - ETA: 11s - loss: 1.0992 - accuracy: 0.659 - ETA: 10s - loss: 1.1006 - accuracy: 0.660 - ETA: 9s - loss: 1.0999 - accuracy: 0.659 - ETA: 8s - loss: 1.0999 - accuracy: 0.65 - ETA: 7s - loss: 1.1008 - accuracy: 0.65 - ETA: 6s - loss: 1.1004 - accuracy: 0.65 - ETA: 6s - loss: 1.0988 - accuracy: 0.66 - ETA: 5s - loss: 1.0987 - accuracy: 0.66 - ETA: 4s - loss: 1.0990 - accuracy: 0.66 - ETA: 3s - loss: 1.0999 - accuracy: 0.66 - ETA: 2s - loss: 1.0965 - accuracy: 0.66 - ETA: 1s - loss: 1.0975 - accuracy: 0.66 - ETA: 0s - loss: 1.0982 - accuracy: 0.66 - 106s 8ms/step - loss: 1.0968 - accuracy: 0.6610 - val_loss: 2.7434 - val_accuracy: 0.3269\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 1.0729 - accuracy: 0.64 - ETA: 1:29 - loss: 1.0665 - accuracy: 0.64 - ETA: 1:28 - loss: 0.9783 - accuracy: 0.66 - ETA: 1:29 - loss: 1.0198 - accuracy: 0.67 - ETA: 1:28 - loss: 1.0357 - accuracy: 0.67 - ETA: 1:28 - loss: 1.0153 - accuracy: 0.67 - ETA: 1:28 - loss: 1.0440 - accuracy: 0.67 - ETA: 1:27 - loss: 1.0689 - accuracy: 0.66 - ETA: 1:25 - loss: 1.0814 - accuracy: 0.66 - ETA: 1:24 - loss: 1.0716 - accuracy: 0.66 - ETA: 1:23 - loss: 1.0660 - accuracy: 0.66 - ETA: 1:22 - loss: 1.0661 - accuracy: 0.66 - ETA: 1:22 - loss: 1.0568 - accuracy: 0.66 - ETA: 1:20 - loss: 1.0577 - accuracy: 0.66 - ETA: 1:19 - loss: 1.0484 - accuracy: 0.67 - ETA: 1:18 - loss: 1.0570 - accuracy: 0.66 - ETA: 1:18 - loss: 1.0480 - accuracy: 0.67 - ETA: 1:16 - loss: 1.0435 - accuracy: 0.67 - ETA: 1:16 - loss: 1.0310 - accuracy: 0.67 - ETA: 1:14 - loss: 1.0361 - accuracy: 0.67 - ETA: 1:13 - loss: 1.0373 - accuracy: 0.67 - ETA: 1:13 - loss: 1.0364 - accuracy: 0.67 - ETA: 1:11 - loss: 1.0364 - accuracy: 0.67 - ETA: 1:10 - loss: 1.0364 - accuracy: 0.67 - ETA: 1:09 - loss: 1.0348 - accuracy: 0.67 - ETA: 1:08 - loss: 1.0336 - accuracy: 0.67 - ETA: 1:07 - loss: 1.0320 - accuracy: 0.67 - ETA: 1:06 - loss: 1.0312 - accuracy: 0.67 - ETA: 1:06 - loss: 1.0368 - accuracy: 0.67 - ETA: 1:05 - loss: 1.0399 - accuracy: 0.67 - ETA: 1:04 - loss: 1.0342 - accuracy: 0.67 - ETA: 1:03 - loss: 1.0354 - accuracy: 0.67 - ETA: 1:02 - loss: 1.0316 - accuracy: 0.67 - ETA: 1:01 - loss: 1.0356 - accuracy: 0.67 - ETA: 1:01 - loss: 1.0402 - accuracy: 0.67 - ETA: 1:00 - loss: 1.0396 - accuracy: 0.67 - ETA: 59s - loss: 1.0386 - accuracy: 0.6721 - ETA: 58s - loss: 1.0401 - accuracy: 0.671 - ETA: 57s - loss: 1.0381 - accuracy: 0.672 - ETA: 56s - loss: 1.0412 - accuracy: 0.671 - ETA: 55s - loss: 1.0407 - accuracy: 0.670 - ETA: 54s - loss: 1.0359 - accuracy: 0.673 - ETA: 53s - loss: 1.0386 - accuracy: 0.672 - ETA: 52s - loss: 1.0384 - accuracy: 0.672 - ETA: 51s - loss: 1.0399 - accuracy: 0.672 - ETA: 50s - loss: 1.0432 - accuracy: 0.671 - ETA: 49s - loss: 1.0473 - accuracy: 0.670 - ETA: 49s - loss: 1.0451 - accuracy: 0.671 - ETA: 48s - loss: 1.0434 - accuracy: 0.671 - ETA: 47s - loss: 1.0447 - accuracy: 0.671 - ETA: 46s - loss: 1.0484 - accuracy: 0.670 - ETA: 45s - loss: 1.0468 - accuracy: 0.669 - ETA: 44s - loss: 1.0501 - accuracy: 0.669 - ETA: 43s - loss: 1.0498 - accuracy: 0.669 - ETA: 42s - loss: 1.0491 - accuracy: 0.669 - ETA: 41s - loss: 1.0528 - accuracy: 0.669 - ETA: 40s - loss: 1.0543 - accuracy: 0.669 - ETA: 39s - loss: 1.0568 - accuracy: 0.668 - ETA: 38s - loss: 1.0547 - accuracy: 0.669 - ETA: 37s - loss: 1.0549 - accuracy: 0.669 - ETA: 37s - loss: 1.0573 - accuracy: 0.669 - ETA: 36s - loss: 1.0587 - accuracy: 0.668 - ETA: 35s - loss: 1.0573 - accuracy: 0.668 - ETA: 34s - loss: 1.0558 - accuracy: 0.669 - ETA: 33s - loss: 1.0595 - accuracy: 0.668 - ETA: 32s - loss: 1.0570 - accuracy: 0.669 - ETA: 31s - loss: 1.0562 - accuracy: 0.670 - ETA: 30s - loss: 1.0569 - accuracy: 0.669 - ETA: 29s - loss: 1.0562 - accuracy: 0.670 - ETA: 28s - loss: 1.0555 - accuracy: 0.670 - ETA: 27s - loss: 1.0521 - accuracy: 0.671 - ETA: 27s - loss: 1.0516 - accuracy: 0.671 - ETA: 26s - loss: 1.0501 - accuracy: 0.672 - ETA: 25s - loss: 1.0500 - accuracy: 0.672 - ETA: 24s - loss: 1.0495 - accuracy: 0.672 - ETA: 23s - loss: 1.0519 - accuracy: 0.672 - ETA: 22s - loss: 1.0508 - accuracy: 0.672 - ETA: 21s - loss: 1.0500 - accuracy: 0.672 - ETA: 20s - loss: 1.0503 - accuracy: 0.672 - ETA: 19s - loss: 1.0500 - accuracy: 0.672 - ETA: 18s - loss: 1.0489 - accuracy: 0.673 - ETA: 17s - loss: 1.0475 - accuracy: 0.673 - ETA: 17s - loss: 1.0482 - accuracy: 0.672 - ETA: 16s - loss: 1.0493 - accuracy: 0.673 - ETA: 15s - loss: 1.0524 - accuracy: 0.672 - ETA: 14s - loss: 1.0524 - accuracy: 0.672 - ETA: 13s - loss: 1.0559 - accuracy: 0.671 - ETA: 12s - loss: 1.0532 - accuracy: 0.672 - ETA: 11s - loss: 1.0547 - accuracy: 0.672 - ETA: 10s - loss: 1.0538 - accuracy: 0.672 - ETA: 9s - loss: 1.0509 - accuracy: 0.672 - ETA: 8s - loss: 1.0478 - accuracy: 0.67 - ETA: 7s - loss: 1.0482 - accuracy: 0.67 - ETA: 7s - loss: 1.0499 - accuracy: 0.67 - ETA: 6s - loss: 1.0495 - accuracy: 0.67 - ETA: 5s - loss: 1.0510 - accuracy: 0.67 - ETA: 4s - loss: 1.0508 - accuracy: 0.67 - ETA: 3s - loss: 1.0506 - accuracy: 0.67 - ETA: 2s - loss: 1.0502 - accuracy: 0.67 - ETA: 1s - loss: 1.0512 - accuracy: 0.67 - ETA: 0s - loss: 1.0516 - accuracy: 0.67 - 106s 8ms/step - loss: 1.0518 - accuracy: 0.6718 - val_loss: 2.7399 - val_accuracy: 0.3512\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:33 - loss: 1.1720 - accuracy: 0.64 - ETA: 1:30 - loss: 0.9919 - accuracy: 0.68 - ETA: 1:28 - loss: 0.9885 - accuracy: 0.67 - ETA: 1:28 - loss: 0.9825 - accuracy: 0.67 - ETA: 1:28 - loss: 0.9824 - accuracy: 0.67 - ETA: 1:27 - loss: 0.9390 - accuracy: 0.69 - ETA: 1:27 - loss: 0.9611 - accuracy: 0.69 - ETA: 1:26 - loss: 0.9846 - accuracy: 0.68 - ETA: 1:24 - loss: 1.0175 - accuracy: 0.68 - ETA: 1:23 - loss: 1.0278 - accuracy: 0.68 - ETA: 1:22 - loss: 1.0271 - accuracy: 0.68 - ETA: 1:21 - loss: 1.0389 - accuracy: 0.67 - ETA: 1:20 - loss: 1.0353 - accuracy: 0.68 - ETA: 1:19 - loss: 1.0336 - accuracy: 0.68 - ETA: 1:18 - loss: 1.0256 - accuracy: 0.68 - ETA: 1:17 - loss: 1.0220 - accuracy: 0.68 - ETA: 1:16 - loss: 1.0192 - accuracy: 0.68 - ETA: 1:15 - loss: 1.0153 - accuracy: 0.68 - ETA: 1:14 - loss: 1.0136 - accuracy: 0.68 - ETA: 1:13 - loss: 1.0177 - accuracy: 0.68 - ETA: 1:12 - loss: 1.0202 - accuracy: 0.68 - ETA: 1:11 - loss: 1.0161 - accuracy: 0.68 - ETA: 1:11 - loss: 1.0188 - accuracy: 0.68 - ETA: 1:10 - loss: 1.0247 - accuracy: 0.68 - ETA: 1:09 - loss: 1.0330 - accuracy: 0.67 - ETA: 1:08 - loss: 1.0323 - accuracy: 0.67 - ETA: 1:07 - loss: 1.0304 - accuracy: 0.67 - ETA: 1:06 - loss: 1.0296 - accuracy: 0.67 - ETA: 1:05 - loss: 1.0261 - accuracy: 0.67 - ETA: 1:04 - loss: 1.0261 - accuracy: 0.67 - ETA: 1:03 - loss: 1.0329 - accuracy: 0.67 - ETA: 1:03 - loss: 1.0371 - accuracy: 0.67 - ETA: 1:02 - loss: 1.0342 - accuracy: 0.67 - ETA: 1:01 - loss: 1.0383 - accuracy: 0.67 - ETA: 1:00 - loss: 1.0416 - accuracy: 0.67 - ETA: 1:00 - loss: 1.0395 - accuracy: 0.67 - ETA: 59s - loss: 1.0393 - accuracy: 0.6727 - ETA: 58s - loss: 1.0387 - accuracy: 0.672 - ETA: 57s - loss: 1.0404 - accuracy: 0.672 - ETA: 56s - loss: 1.0408 - accuracy: 0.672 - ETA: 55s - loss: 1.0433 - accuracy: 0.671 - ETA: 54s - loss: 1.0481 - accuracy: 0.669 - ETA: 53s - loss: 1.0471 - accuracy: 0.670 - ETA: 52s - loss: 1.0491 - accuracy: 0.670 - ETA: 51s - loss: 1.0463 - accuracy: 0.671 - ETA: 50s - loss: 1.0425 - accuracy: 0.673 - ETA: 49s - loss: 1.0499 - accuracy: 0.672 - ETA: 48s - loss: 1.0498 - accuracy: 0.671 - ETA: 47s - loss: 1.0484 - accuracy: 0.671 - ETA: 47s - loss: 1.0520 - accuracy: 0.670 - ETA: 46s - loss: 1.0507 - accuracy: 0.671 - ETA: 45s - loss: 1.0483 - accuracy: 0.671 - ETA: 44s - loss: 1.0517 - accuracy: 0.671 - ETA: 43s - loss: 1.0560 - accuracy: 0.670 - ETA: 42s - loss: 1.0590 - accuracy: 0.669 - ETA: 41s - loss: 1.0586 - accuracy: 0.669 - ETA: 40s - loss: 1.0576 - accuracy: 0.669 - ETA: 39s - loss: 1.0557 - accuracy: 0.669 - ETA: 38s - loss: 1.0582 - accuracy: 0.668 - ETA: 37s - loss: 1.0582 - accuracy: 0.668 - ETA: 37s - loss: 1.0605 - accuracy: 0.667 - ETA: 36s - loss: 1.0578 - accuracy: 0.668 - ETA: 35s - loss: 1.0553 - accuracy: 0.668 - ETA: 34s - loss: 1.0548 - accuracy: 0.668 - ETA: 33s - loss: 1.0547 - accuracy: 0.668 - ETA: 32s - loss: 1.0536 - accuracy: 0.668 - ETA: 31s - loss: 1.0523 - accuracy: 0.668 - ETA: 30s - loss: 1.0493 - accuracy: 0.669 - ETA: 29s - loss: 1.0504 - accuracy: 0.669 - ETA: 28s - loss: 1.0513 - accuracy: 0.668 - ETA: 27s - loss: 1.0504 - accuracy: 0.669 - ETA: 27s - loss: 1.0493 - accuracy: 0.669 - ETA: 26s - loss: 1.0502 - accuracy: 0.669 - ETA: 25s - loss: 1.0478 - accuracy: 0.670 - ETA: 24s - loss: 1.0452 - accuracy: 0.671 - ETA: 23s - loss: 1.0460 - accuracy: 0.671 - ETA: 22s - loss: 1.0448 - accuracy: 0.671 - ETA: 21s - loss: 1.0452 - accuracy: 0.672 - ETA: 20s - loss: 1.0473 - accuracy: 0.671 - ETA: 19s - loss: 1.0509 - accuracy: 0.670 - ETA: 18s - loss: 1.0492 - accuracy: 0.670 - ETA: 17s - loss: 1.0478 - accuracy: 0.671 - ETA: 16s - loss: 1.0484 - accuracy: 0.671 - ETA: 16s - loss: 1.0464 - accuracy: 0.672 - ETA: 15s - loss: 1.0457 - accuracy: 0.672 - ETA: 14s - loss: 1.0455 - accuracy: 0.672 - ETA: 13s - loss: 1.0436 - accuracy: 0.672 - ETA: 12s - loss: 1.0425 - accuracy: 0.673 - ETA: 11s - loss: 1.0456 - accuracy: 0.671 - ETA: 10s - loss: 1.0448 - accuracy: 0.672 - ETA: 9s - loss: 1.0454 - accuracy: 0.672 - ETA: 8s - loss: 1.0476 - accuracy: 0.67 - ETA: 7s - loss: 1.0479 - accuracy: 0.67 - ETA: 6s - loss: 1.0477 - accuracy: 0.67 - ETA: 6s - loss: 1.0452 - accuracy: 0.67 - ETA: 5s - loss: 1.0458 - accuracy: 0.67 - ETA: 4s - loss: 1.0494 - accuracy: 0.67 - ETA: 3s - loss: 1.0505 - accuracy: 0.67 - ETA: 2s - loss: 1.0490 - accuracy: 0.67 - ETA: 1s - loss: 1.0483 - accuracy: 0.67 - ETA: 0s - loss: 1.0455 - accuracy: 0.67 - 106s 8ms/step - loss: 1.0455 - accuracy: 0.6736 - val_loss: 2.8087 - val_accuracy: 0.3256\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:29 - loss: 1.0806 - accuracy: 0.69 - ETA: 1:28 - loss: 1.1285 - accuracy: 0.65 - ETA: 1:28 - loss: 1.0945 - accuracy: 0.65 - ETA: 1:27 - loss: 1.1085 - accuracy: 0.64 - ETA: 1:25 - loss: 1.0973 - accuracy: 0.65 - ETA: 1:24 - loss: 1.1098 - accuracy: 0.65 - ETA: 1:23 - loss: 1.0909 - accuracy: 0.65 - ETA: 1:22 - loss: 1.0678 - accuracy: 0.65 - ETA: 1:21 - loss: 1.0550 - accuracy: 0.66 - ETA: 1:20 - loss: 1.0576 - accuracy: 0.66 - ETA: 1:20 - loss: 1.0504 - accuracy: 0.66 - ETA: 1:19 - loss: 1.0449 - accuracy: 0.66 - ETA: 1:19 - loss: 1.0585 - accuracy: 0.66 - ETA: 1:18 - loss: 1.0543 - accuracy: 0.66 - ETA: 1:17 - loss: 1.0439 - accuracy: 0.66 - ETA: 1:16 - loss: 1.0377 - accuracy: 0.66 - ETA: 1:15 - loss: 1.0293 - accuracy: 0.67 - ETA: 1:14 - loss: 1.0301 - accuracy: 0.66 - ETA: 1:13 - loss: 1.0293 - accuracy: 0.67 - ETA: 1:12 - loss: 1.0322 - accuracy: 0.67 - ETA: 1:11 - loss: 1.0277 - accuracy: 0.67 - ETA: 1:10 - loss: 1.0264 - accuracy: 0.67 - ETA: 1:09 - loss: 1.0163 - accuracy: 0.67 - ETA: 1:09 - loss: 1.0118 - accuracy: 0.68 - ETA: 1:08 - loss: 1.0196 - accuracy: 0.67 - ETA: 1:07 - loss: 1.0193 - accuracy: 0.67 - ETA: 1:06 - loss: 1.0259 - accuracy: 0.67 - ETA: 1:05 - loss: 1.0237 - accuracy: 0.67 - ETA: 1:04 - loss: 1.0291 - accuracy: 0.67 - ETA: 1:03 - loss: 1.0339 - accuracy: 0.67 - ETA: 1:03 - loss: 1.0276 - accuracy: 0.67 - ETA: 1:02 - loss: 1.0344 - accuracy: 0.67 - ETA: 1:01 - loss: 1.0384 - accuracy: 0.67 - ETA: 1:00 - loss: 1.0404 - accuracy: 0.67 - ETA: 59s - loss: 1.0442 - accuracy: 0.6696 - ETA: 58s - loss: 1.0428 - accuracy: 0.670 - ETA: 57s - loss: 1.0438 - accuracy: 0.670 - ETA: 56s - loss: 1.0397 - accuracy: 0.672 - ETA: 55s - loss: 1.0376 - accuracy: 0.673 - ETA: 54s - loss: 1.0361 - accuracy: 0.673 - ETA: 54s - loss: 1.0314 - accuracy: 0.674 - ETA: 53s - loss: 1.0303 - accuracy: 0.675 - ETA: 52s - loss: 1.0283 - accuracy: 0.675 - ETA: 51s - loss: 1.0290 - accuracy: 0.675 - ETA: 50s - loss: 1.0289 - accuracy: 0.675 - ETA: 49s - loss: 1.0322 - accuracy: 0.674 - ETA: 48s - loss: 1.0322 - accuracy: 0.674 - ETA: 47s - loss: 1.0321 - accuracy: 0.674 - ETA: 46s - loss: 1.0315 - accuracy: 0.675 - ETA: 46s - loss: 1.0349 - accuracy: 0.675 - ETA: 45s - loss: 1.0314 - accuracy: 0.676 - ETA: 44s - loss: 1.0345 - accuracy: 0.675 - ETA: 43s - loss: 1.0324 - accuracy: 0.676 - ETA: 42s - loss: 1.0329 - accuracy: 0.675 - ETA: 41s - loss: 1.0292 - accuracy: 0.677 - ETA: 40s - loss: 1.0294 - accuracy: 0.678 - ETA: 39s - loss: 1.0295 - accuracy: 0.678 - ETA: 38s - loss: 1.0268 - accuracy: 0.679 - ETA: 38s - loss: 1.0247 - accuracy: 0.680 - ETA: 37s - loss: 1.0219 - accuracy: 0.680 - ETA: 36s - loss: 1.0247 - accuracy: 0.680 - ETA: 35s - loss: 1.0249 - accuracy: 0.679 - ETA: 34s - loss: 1.0266 - accuracy: 0.678 - ETA: 33s - loss: 1.0266 - accuracy: 0.678 - ETA: 32s - loss: 1.0272 - accuracy: 0.678 - ETA: 31s - loss: 1.0287 - accuracy: 0.678 - ETA: 31s - loss: 1.0282 - accuracy: 0.678 - ETA: 30s - loss: 1.0265 - accuracy: 0.679 - ETA: 29s - loss: 1.0218 - accuracy: 0.679 - ETA: 28s - loss: 1.0207 - accuracy: 0.680 - ETA: 27s - loss: 1.0212 - accuracy: 0.680 - ETA: 26s - loss: 1.0208 - accuracy: 0.680 - ETA: 25s - loss: 1.0217 - accuracy: 0.680 - ETA: 24s - loss: 1.0222 - accuracy: 0.680 - ETA: 23s - loss: 1.0231 - accuracy: 0.680 - ETA: 22s - loss: 1.0219 - accuracy: 0.681 - ETA: 22s - loss: 1.0213 - accuracy: 0.681 - ETA: 21s - loss: 1.0211 - accuracy: 0.681 - ETA: 20s - loss: 1.0233 - accuracy: 0.680 - ETA: 19s - loss: 1.0265 - accuracy: 0.679 - ETA: 18s - loss: 1.0223 - accuracy: 0.681 - ETA: 17s - loss: 1.0211 - accuracy: 0.681 - ETA: 16s - loss: 1.0222 - accuracy: 0.680 - ETA: 15s - loss: 1.0256 - accuracy: 0.680 - ETA: 14s - loss: 1.0274 - accuracy: 0.680 - ETA: 14s - loss: 1.0263 - accuracy: 0.680 - ETA: 13s - loss: 1.0288 - accuracy: 0.680 - ETA: 12s - loss: 1.0292 - accuracy: 0.680 - ETA: 11s - loss: 1.0286 - accuracy: 0.680 - ETA: 10s - loss: 1.0275 - accuracy: 0.680 - ETA: 9s - loss: 1.0267 - accuracy: 0.680 - ETA: 8s - loss: 1.0252 - accuracy: 0.68 - ETA: 7s - loss: 1.0264 - accuracy: 0.68 - ETA: 6s - loss: 1.0271 - accuracy: 0.68 - ETA: 6s - loss: 1.0293 - accuracy: 0.68 - ETA: 5s - loss: 1.0292 - accuracy: 0.68 - ETA: 4s - loss: 1.0293 - accuracy: 0.68 - ETA: 3s - loss: 1.0296 - accuracy: 0.68 - ETA: 2s - loss: 1.0314 - accuracy: 0.67 - ETA: 1s - loss: 1.0282 - accuracy: 0.68 - ETA: 0s - loss: 1.0288 - accuracy: 0.68 - 104s 8ms/step - loss: 1.0287 - accuracy: 0.6808 - val_loss: 2.7975 - val_accuracy: 0.3394\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:34 - loss: 0.9696 - accuracy: 0.67 - ETA: 1:33 - loss: 1.0725 - accuracy: 0.69 - ETA: 1:32 - loss: 1.1054 - accuracy: 0.67 - ETA: 1:32 - loss: 1.0257 - accuracy: 0.69 - ETA: 1:30 - loss: 0.9914 - accuracy: 0.70 - ETA: 1:28 - loss: 0.9869 - accuracy: 0.70 - ETA: 1:27 - loss: 0.9827 - accuracy: 0.70 - ETA: 1:25 - loss: 1.0125 - accuracy: 0.69 - ETA: 1:24 - loss: 1.0186 - accuracy: 0.69 - ETA: 1:23 - loss: 1.0225 - accuracy: 0.68 - ETA: 1:22 - loss: 1.0076 - accuracy: 0.68 - ETA: 1:21 - loss: 1.0142 - accuracy: 0.68 - ETA: 1:20 - loss: 0.9948 - accuracy: 0.69 - ETA: 1:19 - loss: 0.9989 - accuracy: 0.69 - ETA: 1:18 - loss: 1.0102 - accuracy: 0.68 - ETA: 1:17 - loss: 1.0023 - accuracy: 0.68 - ETA: 1:16 - loss: 0.9899 - accuracy: 0.69 - ETA: 1:15 - loss: 1.0044 - accuracy: 0.68 - ETA: 1:15 - loss: 1.0087 - accuracy: 0.68 - ETA: 1:14 - loss: 1.0057 - accuracy: 0.68 - ETA: 1:13 - loss: 1.0037 - accuracy: 0.69 - ETA: 1:13 - loss: 1.0031 - accuracy: 0.69 - ETA: 1:12 - loss: 1.0049 - accuracy: 0.68 - ETA: 1:11 - loss: 1.0052 - accuracy: 0.68 - ETA: 1:10 - loss: 1.0014 - accuracy: 0.69 - ETA: 1:09 - loss: 1.0003 - accuracy: 0.69 - ETA: 1:08 - loss: 0.9985 - accuracy: 0.69 - ETA: 1:07 - loss: 0.9971 - accuracy: 0.69 - ETA: 1:06 - loss: 0.9986 - accuracy: 0.69 - ETA: 1:05 - loss: 1.0063 - accuracy: 0.69 - ETA: 1:04 - loss: 1.0052 - accuracy: 0.69 - ETA: 1:03 - loss: 0.9942 - accuracy: 0.69 - ETA: 1:02 - loss: 0.9987 - accuracy: 0.69 - ETA: 1:02 - loss: 0.9955 - accuracy: 0.69 - ETA: 1:01 - loss: 0.9944 - accuracy: 0.69 - ETA: 1:00 - loss: 0.9975 - accuracy: 0.69 - ETA: 59s - loss: 0.9968 - accuracy: 0.6951 - ETA: 58s - loss: 1.0002 - accuracy: 0.694 - ETA: 57s - loss: 1.0079 - accuracy: 0.692 - ETA: 56s - loss: 1.0039 - accuracy: 0.693 - ETA: 55s - loss: 1.0035 - accuracy: 0.692 - ETA: 54s - loss: 1.0049 - accuracy: 0.691 - ETA: 53s - loss: 1.0034 - accuracy: 0.692 - ETA: 52s - loss: 1.0021 - accuracy: 0.691 - ETA: 51s - loss: 0.9971 - accuracy: 0.692 - ETA: 51s - loss: 0.9976 - accuracy: 0.692 - ETA: 50s - loss: 1.0055 - accuracy: 0.690 - ETA: 49s - loss: 1.0056 - accuracy: 0.689 - ETA: 48s - loss: 1.0084 - accuracy: 0.689 - ETA: 47s - loss: 1.0046 - accuracy: 0.690 - ETA: 46s - loss: 1.0026 - accuracy: 0.690 - ETA: 45s - loss: 1.0080 - accuracy: 0.688 - ETA: 44s - loss: 1.0122 - accuracy: 0.687 - ETA: 43s - loss: 1.0141 - accuracy: 0.687 - ETA: 42s - loss: 1.0143 - accuracy: 0.687 - ETA: 41s - loss: 1.0157 - accuracy: 0.687 - ETA: 40s - loss: 1.0206 - accuracy: 0.685 - ETA: 39s - loss: 1.0235 - accuracy: 0.684 - ETA: 39s - loss: 1.0200 - accuracy: 0.685 - ETA: 38s - loss: 1.0174 - accuracy: 0.685 - ETA: 37s - loss: 1.0191 - accuracy: 0.685 - ETA: 36s - loss: 1.0202 - accuracy: 0.684 - ETA: 35s - loss: 1.0202 - accuracy: 0.684 - ETA: 34s - loss: 1.0181 - accuracy: 0.685 - ETA: 33s - loss: 1.0182 - accuracy: 0.685 - ETA: 32s - loss: 1.0163 - accuracy: 0.686 - ETA: 31s - loss: 1.0169 - accuracy: 0.685 - ETA: 30s - loss: 1.0147 - accuracy: 0.686 - ETA: 29s - loss: 1.0142 - accuracy: 0.686 - ETA: 28s - loss: 1.0138 - accuracy: 0.685 - ETA: 27s - loss: 1.0140 - accuracy: 0.685 - ETA: 27s - loss: 1.0146 - accuracy: 0.684 - ETA: 26s - loss: 1.0160 - accuracy: 0.684 - ETA: 25s - loss: 1.0148 - accuracy: 0.684 - ETA: 24s - loss: 1.0167 - accuracy: 0.684 - ETA: 23s - loss: 1.0149 - accuracy: 0.684 - ETA: 22s - loss: 1.0137 - accuracy: 0.684 - ETA: 21s - loss: 1.0134 - accuracy: 0.684 - ETA: 20s - loss: 1.0189 - accuracy: 0.683 - ETA: 19s - loss: 1.0218 - accuracy: 0.682 - ETA: 18s - loss: 1.0216 - accuracy: 0.681 - ETA: 17s - loss: 1.0247 - accuracy: 0.680 - ETA: 17s - loss: 1.0267 - accuracy: 0.680 - ETA: 16s - loss: 1.0290 - accuracy: 0.679 - ETA: 15s - loss: 1.0304 - accuracy: 0.679 - ETA: 14s - loss: 1.0314 - accuracy: 0.679 - ETA: 13s - loss: 1.0324 - accuracy: 0.678 - ETA: 12s - loss: 1.0328 - accuracy: 0.678 - ETA: 11s - loss: 1.0340 - accuracy: 0.678 - ETA: 10s - loss: 1.0353 - accuracy: 0.677 - ETA: 9s - loss: 1.0374 - accuracy: 0.676 - ETA: 8s - loss: 1.0360 - accuracy: 0.67 - ETA: 7s - loss: 1.0362 - accuracy: 0.67 - ETA: 7s - loss: 1.0354 - accuracy: 0.67 - ETA: 6s - loss: 1.0339 - accuracy: 0.67 - ETA: 5s - loss: 1.0337 - accuracy: 0.67 - ETA: 4s - loss: 1.0350 - accuracy: 0.67 - ETA: 3s - loss: 1.0344 - accuracy: 0.67 - ETA: 2s - loss: 1.0363 - accuracy: 0.67 - ETA: 1s - loss: 1.0366 - accuracy: 0.67 - ETA: 0s - loss: 1.0360 - accuracy: 0.67 - 106s 8ms/step - loss: 1.0368 - accuracy: 0.6772 - val_loss: 2.7147 - val_accuracy: 0.3373\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:31 - loss: 1.0792 - accuracy: 0.65 - ETA: 1:27 - loss: 1.0837 - accuracy: 0.64 - ETA: 1:27 - loss: 1.1099 - accuracy: 0.64 - ETA: 1:26 - loss: 1.0775 - accuracy: 0.65 - ETA: 1:25 - loss: 0.9942 - accuracy: 0.68 - ETA: 1:25 - loss: 0.9710 - accuracy: 0.68 - ETA: 1:24 - loss: 0.9721 - accuracy: 0.68 - ETA: 1:25 - loss: 0.9676 - accuracy: 0.69 - ETA: 1:24 - loss: 0.9771 - accuracy: 0.68 - ETA: 1:23 - loss: 0.9633 - accuracy: 0.69 - ETA: 1:22 - loss: 0.9616 - accuracy: 0.69 - ETA: 1:21 - loss: 0.9576 - accuracy: 0.69 - ETA: 1:20 - loss: 0.9718 - accuracy: 0.69 - ETA: 1:19 - loss: 0.9752 - accuracy: 0.69 - ETA: 1:18 - loss: 0.9722 - accuracy: 0.69 - ETA: 1:17 - loss: 0.9762 - accuracy: 0.69 - ETA: 1:16 - loss: 0.9880 - accuracy: 0.69 - ETA: 1:15 - loss: 0.9935 - accuracy: 0.68 - ETA: 1:15 - loss: 0.9933 - accuracy: 0.68 - ETA: 1:14 - loss: 0.9952 - accuracy: 0.68 - ETA: 1:13 - loss: 1.0001 - accuracy: 0.68 - ETA: 1:12 - loss: 1.0017 - accuracy: 0.68 - ETA: 1:11 - loss: 1.0034 - accuracy: 0.68 - ETA: 1:10 - loss: 1.0079 - accuracy: 0.68 - ETA: 1:09 - loss: 1.0187 - accuracy: 0.68 - ETA: 1:08 - loss: 1.0124 - accuracy: 0.68 - ETA: 1:07 - loss: 1.0070 - accuracy: 0.68 - ETA: 1:06 - loss: 1.0062 - accuracy: 0.68 - ETA: 1:05 - loss: 1.0067 - accuracy: 0.68 - ETA: 1:04 - loss: 1.0177 - accuracy: 0.68 - ETA: 1:04 - loss: 1.0148 - accuracy: 0.68 - ETA: 1:03 - loss: 1.0162 - accuracy: 0.68 - ETA: 1:02 - loss: 1.0166 - accuracy: 0.68 - ETA: 1:01 - loss: 1.0132 - accuracy: 0.68 - ETA: 1:00 - loss: 1.0083 - accuracy: 0.68 - ETA: 59s - loss: 1.0049 - accuracy: 0.6827 - ETA: 58s - loss: 1.0028 - accuracy: 0.683 - ETA: 57s - loss: 1.0050 - accuracy: 0.684 - ETA: 56s - loss: 1.0066 - accuracy: 0.683 - ETA: 55s - loss: 1.0023 - accuracy: 0.684 - ETA: 54s - loss: 0.9989 - accuracy: 0.684 - ETA: 53s - loss: 0.9985 - accuracy: 0.685 - ETA: 52s - loss: 0.9999 - accuracy: 0.685 - ETA: 52s - loss: 0.9996 - accuracy: 0.686 - ETA: 51s - loss: 1.0012 - accuracy: 0.686 - ETA: 50s - loss: 0.9996 - accuracy: 0.687 - ETA: 49s - loss: 1.0032 - accuracy: 0.685 - ETA: 48s - loss: 1.0005 - accuracy: 0.686 - ETA: 47s - loss: 1.0039 - accuracy: 0.685 - ETA: 47s - loss: 0.9988 - accuracy: 0.686 - ETA: 46s - loss: 1.0032 - accuracy: 0.685 - ETA: 45s - loss: 1.0085 - accuracy: 0.685 - ETA: 44s - loss: 1.0043 - accuracy: 0.686 - ETA: 43s - loss: 1.0039 - accuracy: 0.686 - ETA: 42s - loss: 0.9997 - accuracy: 0.688 - ETA: 41s - loss: 0.9991 - accuracy: 0.688 - ETA: 40s - loss: 1.0011 - accuracy: 0.688 - ETA: 39s - loss: 0.9995 - accuracy: 0.687 - ETA: 38s - loss: 0.9972 - accuracy: 0.688 - ETA: 37s - loss: 0.9988 - accuracy: 0.688 - ETA: 36s - loss: 0.9990 - accuracy: 0.688 - ETA: 35s - loss: 0.9959 - accuracy: 0.688 - ETA: 35s - loss: 0.9955 - accuracy: 0.688 - ETA: 34s - loss: 0.9950 - accuracy: 0.688 - ETA: 33s - loss: 0.9941 - accuracy: 0.688 - ETA: 32s - loss: 0.9936 - accuracy: 0.688 - ETA: 31s - loss: 0.9945 - accuracy: 0.688 - ETA: 30s - loss: 0.9903 - accuracy: 0.689 - ETA: 29s - loss: 0.9876 - accuracy: 0.690 - ETA: 28s - loss: 0.9866 - accuracy: 0.691 - ETA: 27s - loss: 0.9874 - accuracy: 0.691 - ETA: 26s - loss: 0.9901 - accuracy: 0.690 - ETA: 25s - loss: 0.9876 - accuracy: 0.690 - ETA: 25s - loss: 0.9872 - accuracy: 0.691 - ETA: 24s - loss: 0.9850 - accuracy: 0.691 - ETA: 23s - loss: 0.9884 - accuracy: 0.691 - ETA: 22s - loss: 0.9890 - accuracy: 0.690 - ETA: 21s - loss: 0.9886 - accuracy: 0.690 - ETA: 20s - loss: 0.9881 - accuracy: 0.691 - ETA: 19s - loss: 0.9869 - accuracy: 0.691 - ETA: 18s - loss: 0.9900 - accuracy: 0.690 - ETA: 17s - loss: 0.9893 - accuracy: 0.690 - ETA: 16s - loss: 0.9883 - accuracy: 0.691 - ETA: 16s - loss: 0.9889 - accuracy: 0.690 - ETA: 15s - loss: 0.9896 - accuracy: 0.690 - ETA: 14s - loss: 0.9915 - accuracy: 0.690 - ETA: 13s - loss: 0.9914 - accuracy: 0.690 - ETA: 12s - loss: 0.9928 - accuracy: 0.690 - ETA: 11s - loss: 0.9919 - accuracy: 0.690 - ETA: 10s - loss: 0.9911 - accuracy: 0.690 - ETA: 9s - loss: 0.9928 - accuracy: 0.689 - ETA: 8s - loss: 0.9947 - accuracy: 0.68 - ETA: 7s - loss: 0.9944 - accuracy: 0.68 - ETA: 6s - loss: 0.9951 - accuracy: 0.68 - ETA: 6s - loss: 0.9935 - accuracy: 0.69 - ETA: 5s - loss: 0.9952 - accuracy: 0.68 - ETA: 4s - loss: 0.9948 - accuracy: 0.68 - ETA: 3s - loss: 0.9955 - accuracy: 0.68 - ETA: 2s - loss: 0.9948 - accuracy: 0.68 - ETA: 1s - loss: 0.9937 - accuracy: 0.69 - ETA: 0s - loss: 0.9950 - accuracy: 0.68 - 106s 8ms/step - loss: 0.9941 - accuracy: 0.6906 - val_loss: 2.8490 - val_accuracy: 0.3355\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 0.8298 - accuracy: 0.72 - ETA: 1:28 - loss: 0.8960 - accuracy: 0.71 - ETA: 1:27 - loss: 0.8836 - accuracy: 0.71 - ETA: 1:29 - loss: 0.9133 - accuracy: 0.72 - ETA: 1:29 - loss: 0.9474 - accuracy: 0.71 - ETA: 1:28 - loss: 0.9518 - accuracy: 0.70 - ETA: 1:27 - loss: 0.9779 - accuracy: 0.70 - ETA: 1:26 - loss: 0.9759 - accuracy: 0.70 - ETA: 1:25 - loss: 0.9608 - accuracy: 0.70 - ETA: 1:24 - loss: 0.9705 - accuracy: 0.69 - ETA: 1:23 - loss: 0.9693 - accuracy: 0.70 - ETA: 1:22 - loss: 0.9686 - accuracy: 0.70 - ETA: 1:20 - loss: 0.9746 - accuracy: 0.70 - ETA: 1:20 - loss: 0.9678 - accuracy: 0.70 - ETA: 1:17 - loss: 0.9776 - accuracy: 0.70 - ETA: 1:15 - loss: 0.9719 - accuracy: 0.70 - ETA: 1:13 - loss: 0.9647 - accuracy: 0.70 - ETA: 1:11 - loss: 0.9491 - accuracy: 0.70 - ETA: 1:10 - loss: 0.9505 - accuracy: 0.70 - ETA: 1:08 - loss: 0.9570 - accuracy: 0.70 - ETA: 1:06 - loss: 0.9586 - accuracy: 0.70 - ETA: 1:05 - loss: 0.9608 - accuracy: 0.70 - ETA: 1:03 - loss: 0.9639 - accuracy: 0.69 - ETA: 1:02 - loss: 0.9754 - accuracy: 0.69 - ETA: 1:01 - loss: 0.9787 - accuracy: 0.69 - ETA: 59s - loss: 0.9748 - accuracy: 0.6956 - ETA: 58s - loss: 0.9776 - accuracy: 0.696 - ETA: 57s - loss: 0.9850 - accuracy: 0.695 - ETA: 57s - loss: 0.9759 - accuracy: 0.698 - ETA: 56s - loss: 0.9711 - accuracy: 0.699 - ETA: 56s - loss: 0.9775 - accuracy: 0.698 - ETA: 55s - loss: 0.9807 - accuracy: 0.696 - ETA: 54s - loss: 0.9794 - accuracy: 0.697 - ETA: 54s - loss: 0.9808 - accuracy: 0.695 - ETA: 53s - loss: 0.9837 - accuracy: 0.694 - ETA: 52s - loss: 0.9827 - accuracy: 0.695 - ETA: 52s - loss: 0.9799 - accuracy: 0.696 - ETA: 51s - loss: 0.9837 - accuracy: 0.694 - ETA: 51s - loss: 0.9885 - accuracy: 0.693 - ETA: 50s - loss: 0.9872 - accuracy: 0.694 - ETA: 49s - loss: 0.9826 - accuracy: 0.695 - ETA: 49s - loss: 0.9831 - accuracy: 0.694 - ETA: 48s - loss: 0.9864 - accuracy: 0.695 - ETA: 47s - loss: 0.9854 - accuracy: 0.695 - ETA: 46s - loss: 0.9833 - accuracy: 0.695 - ETA: 46s - loss: 0.9832 - accuracy: 0.695 - ETA: 45s - loss: 0.9808 - accuracy: 0.695 - ETA: 44s - loss: 0.9817 - accuracy: 0.694 - ETA: 43s - loss: 0.9773 - accuracy: 0.696 - ETA: 43s - loss: 0.9768 - accuracy: 0.696 - ETA: 42s - loss: 0.9734 - accuracy: 0.697 - ETA: 41s - loss: 0.9746 - accuracy: 0.698 - ETA: 40s - loss: 0.9685 - accuracy: 0.699 - ETA: 39s - loss: 0.9645 - accuracy: 0.701 - ETA: 39s - loss: 0.9665 - accuracy: 0.700 - ETA: 38s - loss: 0.9655 - accuracy: 0.701 - ETA: 37s - loss: 0.9634 - accuracy: 0.701 - ETA: 36s - loss: 0.9633 - accuracy: 0.701 - ETA: 36s - loss: 0.9644 - accuracy: 0.701 - ETA: 35s - loss: 0.9695 - accuracy: 0.700 - ETA: 34s - loss: 0.9711 - accuracy: 0.700 - ETA: 33s - loss: 0.9705 - accuracy: 0.700 - ETA: 32s - loss: 0.9671 - accuracy: 0.701 - ETA: 31s - loss: 0.9671 - accuracy: 0.701 - ETA: 31s - loss: 0.9694 - accuracy: 0.700 - ETA: 30s - loss: 0.9700 - accuracy: 0.699 - ETA: 29s - loss: 0.9721 - accuracy: 0.699 - ETA: 28s - loss: 0.9709 - accuracy: 0.700 - ETA: 27s - loss: 0.9697 - accuracy: 0.700 - ETA: 27s - loss: 0.9700 - accuracy: 0.700 - ETA: 26s - loss: 0.9705 - accuracy: 0.700 - ETA: 25s - loss: 0.9702 - accuracy: 0.700 - ETA: 24s - loss: 0.9711 - accuracy: 0.699 - ETA: 23s - loss: 0.9712 - accuracy: 0.699 - ETA: 22s - loss: 0.9737 - accuracy: 0.699 - ETA: 22s - loss: 0.9749 - accuracy: 0.698 - ETA: 21s - loss: 0.9746 - accuracy: 0.698 - ETA: 20s - loss: 0.9727 - accuracy: 0.698 - ETA: 19s - loss: 0.9709 - accuracy: 0.699 - ETA: 18s - loss: 0.9701 - accuracy: 0.699 - ETA: 17s - loss: 0.9718 - accuracy: 0.698 - ETA: 17s - loss: 0.9730 - accuracy: 0.697 - ETA: 16s - loss: 0.9729 - accuracy: 0.697 - ETA: 15s - loss: 0.9733 - accuracy: 0.697 - ETA: 14s - loss: 0.9711 - accuracy: 0.698 - ETA: 13s - loss: 0.9705 - accuracy: 0.698 - ETA: 12s - loss: 0.9700 - accuracy: 0.698 - ETA: 11s - loss: 0.9704 - accuracy: 0.698 - ETA: 11s - loss: 0.9678 - accuracy: 0.698 - ETA: 10s - loss: 0.9668 - accuracy: 0.699 - ETA: 9s - loss: 0.9674 - accuracy: 0.699 - ETA: 8s - loss: 0.9681 - accuracy: 0.69 - ETA: 7s - loss: 0.9674 - accuracy: 0.69 - ETA: 6s - loss: 0.9697 - accuracy: 0.69 - ETA: 5s - loss: 0.9689 - accuracy: 0.69 - ETA: 4s - loss: 0.9718 - accuracy: 0.69 - ETA: 4s - loss: 0.9699 - accuracy: 0.69 - ETA: 3s - loss: 0.9704 - accuracy: 0.69 - ETA: 2s - loss: 0.9709 - accuracy: 0.69 - ETA: 1s - loss: 0.9710 - accuracy: 0.69 - ETA: 0s - loss: 0.9707 - accuracy: 0.69 - 102s 8ms/step - loss: 0.9703 - accuracy: 0.6990 - val_loss: 2.7950 - val_accuracy: 0.3133\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:38 - loss: 1.0679 - accuracy: 0.65 - ETA: 1:34 - loss: 1.1003 - accuracy: 0.66 - ETA: 1:32 - loss: 1.0160 - accuracy: 0.68 - ETA: 1:30 - loss: 1.0263 - accuracy: 0.67 - ETA: 1:29 - loss: 0.9415 - accuracy: 0.70 - ETA: 1:27 - loss: 0.9224 - accuracy: 0.71 - ETA: 1:26 - loss: 0.9728 - accuracy: 0.69 - ETA: 1:26 - loss: 0.9688 - accuracy: 0.69 - ETA: 1:25 - loss: 0.9628 - accuracy: 0.69 - ETA: 1:24 - loss: 0.9489 - accuracy: 0.70 - ETA: 1:23 - loss: 0.9386 - accuracy: 0.70 - ETA: 1:22 - loss: 0.9448 - accuracy: 0.70 - ETA: 1:21 - loss: 0.9662 - accuracy: 0.69 - ETA: 1:20 - loss: 0.9669 - accuracy: 0.69 - ETA: 1:19 - loss: 0.9769 - accuracy: 0.68 - ETA: 1:18 - loss: 0.9951 - accuracy: 0.68 - ETA: 1:17 - loss: 0.9911 - accuracy: 0.68 - ETA: 1:16 - loss: 0.9992 - accuracy: 0.68 - ETA: 1:15 - loss: 0.9978 - accuracy: 0.68 - ETA: 1:14 - loss: 0.9949 - accuracy: 0.68 - ETA: 1:13 - loss: 1.0013 - accuracy: 0.68 - ETA: 1:12 - loss: 0.9964 - accuracy: 0.68 - ETA: 1:11 - loss: 0.9962 - accuracy: 0.68 - ETA: 1:10 - loss: 0.9972 - accuracy: 0.68 - ETA: 1:09 - loss: 0.9967 - accuracy: 0.68 - ETA: 1:08 - loss: 0.9971 - accuracy: 0.68 - ETA: 1:07 - loss: 0.9918 - accuracy: 0.68 - ETA: 1:06 - loss: 0.9829 - accuracy: 0.69 - ETA: 1:05 - loss: 0.9758 - accuracy: 0.69 - ETA: 1:04 - loss: 0.9780 - accuracy: 0.69 - ETA: 1:03 - loss: 0.9806 - accuracy: 0.69 - ETA: 1:03 - loss: 0.9754 - accuracy: 0.69 - ETA: 1:02 - loss: 0.9779 - accuracy: 0.69 - ETA: 1:01 - loss: 0.9746 - accuracy: 0.69 - ETA: 1:00 - loss: 0.9761 - accuracy: 0.69 - ETA: 59s - loss: 0.9776 - accuracy: 0.6914 - ETA: 58s - loss: 0.9716 - accuracy: 0.693 - ETA: 57s - loss: 0.9691 - accuracy: 0.695 - ETA: 57s - loss: 0.9755 - accuracy: 0.693 - ETA: 56s - loss: 0.9752 - accuracy: 0.692 - ETA: 55s - loss: 0.9720 - accuracy: 0.694 - ETA: 54s - loss: 0.9754 - accuracy: 0.694 - ETA: 53s - loss: 0.9762 - accuracy: 0.694 - ETA: 52s - loss: 0.9785 - accuracy: 0.693 - ETA: 51s - loss: 0.9818 - accuracy: 0.692 - ETA: 50s - loss: 0.9834 - accuracy: 0.693 - ETA: 49s - loss: 0.9863 - accuracy: 0.691 - ETA: 48s - loss: 0.9859 - accuracy: 0.691 - ETA: 48s - loss: 0.9846 - accuracy: 0.693 - ETA: 47s - loss: 0.9815 - accuracy: 0.693 - ETA: 46s - loss: 0.9791 - accuracy: 0.694 - ETA: 45s - loss: 0.9794 - accuracy: 0.694 - ETA: 44s - loss: 0.9793 - accuracy: 0.694 - ETA: 43s - loss: 0.9777 - accuracy: 0.694 - ETA: 42s - loss: 0.9789 - accuracy: 0.695 - ETA: 41s - loss: 0.9763 - accuracy: 0.695 - ETA: 40s - loss: 0.9749 - accuracy: 0.696 - ETA: 39s - loss: 0.9761 - accuracy: 0.695 - ETA: 38s - loss: 0.9757 - accuracy: 0.696 - ETA: 37s - loss: 0.9776 - accuracy: 0.695 - ETA: 37s - loss: 0.9784 - accuracy: 0.694 - ETA: 36s - loss: 0.9779 - accuracy: 0.693 - ETA: 35s - loss: 0.9802 - accuracy: 0.693 - ETA: 34s - loss: 0.9839 - accuracy: 0.693 - ETA: 33s - loss: 0.9843 - accuracy: 0.692 - ETA: 32s - loss: 0.9840 - accuracy: 0.692 - ETA: 31s - loss: 0.9855 - accuracy: 0.692 - ETA: 30s - loss: 0.9850 - accuracy: 0.692 - ETA: 29s - loss: 0.9850 - accuracy: 0.692 - ETA: 28s - loss: 0.9865 - accuracy: 0.692 - ETA: 27s - loss: 0.9854 - accuracy: 0.693 - ETA: 27s - loss: 0.9850 - accuracy: 0.692 - ETA: 26s - loss: 0.9848 - accuracy: 0.693 - ETA: 25s - loss: 0.9867 - accuracy: 0.692 - ETA: 24s - loss: 0.9861 - accuracy: 0.692 - ETA: 23s - loss: 0.9836 - accuracy: 0.693 - ETA: 22s - loss: 0.9859 - accuracy: 0.693 - ETA: 21s - loss: 0.9844 - accuracy: 0.693 - ETA: 20s - loss: 0.9878 - accuracy: 0.692 - ETA: 19s - loss: 0.9836 - accuracy: 0.693 - ETA: 18s - loss: 0.9821 - accuracy: 0.693 - ETA: 17s - loss: 0.9816 - accuracy: 0.693 - ETA: 16s - loss: 0.9830 - accuracy: 0.693 - ETA: 16s - loss: 0.9861 - accuracy: 0.692 - ETA: 15s - loss: 0.9861 - accuracy: 0.692 - ETA: 14s - loss: 0.9858 - accuracy: 0.692 - ETA: 13s - loss: 0.9903 - accuracy: 0.691 - ETA: 12s - loss: 0.9888 - accuracy: 0.691 - ETA: 11s - loss: 0.9885 - accuracy: 0.691 - ETA: 10s - loss: 0.9878 - accuracy: 0.691 - ETA: 9s - loss: 0.9887 - accuracy: 0.691 - ETA: 8s - loss: 0.9908 - accuracy: 0.69 - ETA: 7s - loss: 0.9915 - accuracy: 0.69 - ETA: 7s - loss: 0.9901 - accuracy: 0.69 - ETA: 6s - loss: 0.9907 - accuracy: 0.69 - ETA: 5s - loss: 0.9894 - accuracy: 0.69 - ETA: 4s - loss: 0.9887 - accuracy: 0.69 - ETA: 3s - loss: 0.9888 - accuracy: 0.69 - ETA: 2s - loss: 0.9894 - accuracy: 0.69 - ETA: 1s - loss: 0.9886 - accuracy: 0.69 - ETA: 0s - loss: 0.9911 - accuracy: 0.68 - 106s 8ms/step - loss: 0.9923 - accuracy: 0.6894 - val_loss: 2.7239 - val_accuracy: 0.3460\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 1.0581 - accuracy: 0.67 - ETA: 1:30 - loss: 0.9672 - accuracy: 0.67 - ETA: 1:28 - loss: 1.0268 - accuracy: 0.66 - ETA: 1:26 - loss: 0.9569 - accuracy: 0.68 - ETA: 1:26 - loss: 0.9338 - accuracy: 0.69 - ETA: 1:25 - loss: 0.9387 - accuracy: 0.69 - ETA: 1:24 - loss: 0.9424 - accuracy: 0.69 - ETA: 1:23 - loss: 0.9341 - accuracy: 0.70 - ETA: 1:22 - loss: 0.9313 - accuracy: 0.70 - ETA: 1:21 - loss: 0.9406 - accuracy: 0.69 - ETA: 1:21 - loss: 0.9508 - accuracy: 0.69 - ETA: 1:19 - loss: 0.9264 - accuracy: 0.69 - ETA: 1:19 - loss: 0.9378 - accuracy: 0.69 - ETA: 1:18 - loss: 0.9503 - accuracy: 0.69 - ETA: 1:17 - loss: 0.9544 - accuracy: 0.68 - ETA: 1:16 - loss: 0.9519 - accuracy: 0.68 - ETA: 1:16 - loss: 0.9582 - accuracy: 0.68 - ETA: 1:15 - loss: 0.9701 - accuracy: 0.68 - ETA: 1:14 - loss: 0.9774 - accuracy: 0.68 - ETA: 1:13 - loss: 0.9759 - accuracy: 0.68 - ETA: 1:12 - loss: 0.9757 - accuracy: 0.68 - ETA: 1:11 - loss: 0.9689 - accuracy: 0.69 - ETA: 1:10 - loss: 0.9647 - accuracy: 0.69 - ETA: 1:09 - loss: 0.9606 - accuracy: 0.69 - ETA: 1:08 - loss: 0.9505 - accuracy: 0.69 - ETA: 1:08 - loss: 0.9443 - accuracy: 0.69 - ETA: 1:07 - loss: 0.9489 - accuracy: 0.69 - ETA: 1:06 - loss: 0.9484 - accuracy: 0.69 - ETA: 1:05 - loss: 0.9453 - accuracy: 0.69 - ETA: 1:04 - loss: 0.9481 - accuracy: 0.69 - ETA: 1:03 - loss: 0.9486 - accuracy: 0.69 - ETA: 1:02 - loss: 0.9463 - accuracy: 0.69 - ETA: 1:01 - loss: 0.9447 - accuracy: 0.69 - ETA: 1:00 - loss: 0.9481 - accuracy: 0.69 - ETA: 1:00 - loss: 0.9455 - accuracy: 0.69 - ETA: 59s - loss: 0.9458 - accuracy: 0.6981 - ETA: 58s - loss: 0.9430 - accuracy: 0.699 - ETA: 57s - loss: 0.9448 - accuracy: 0.699 - ETA: 56s - loss: 0.9468 - accuracy: 0.699 - ETA: 55s - loss: 0.9487 - accuracy: 0.698 - ETA: 54s - loss: 0.9481 - accuracy: 0.698 - ETA: 53s - loss: 0.9434 - accuracy: 0.700 - ETA: 52s - loss: 0.9389 - accuracy: 0.701 - ETA: 51s - loss: 0.9391 - accuracy: 0.700 - ETA: 50s - loss: 0.9405 - accuracy: 0.700 - ETA: 50s - loss: 0.9424 - accuracy: 0.699 - ETA: 49s - loss: 0.9437 - accuracy: 0.700 - ETA: 48s - loss: 0.9404 - accuracy: 0.700 - ETA: 47s - loss: 0.9412 - accuracy: 0.700 - ETA: 46s - loss: 0.9374 - accuracy: 0.701 - ETA: 45s - loss: 0.9378 - accuracy: 0.701 - ETA: 44s - loss: 0.9375 - accuracy: 0.701 - ETA: 43s - loss: 0.9340 - accuracy: 0.703 - ETA: 42s - loss: 0.9360 - accuracy: 0.703 - ETA: 42s - loss: 0.9366 - accuracy: 0.703 - ETA: 41s - loss: 0.9326 - accuracy: 0.703 - ETA: 40s - loss: 0.9332 - accuracy: 0.702 - ETA: 39s - loss: 0.9320 - accuracy: 0.702 - ETA: 38s - loss: 0.9327 - accuracy: 0.702 - ETA: 37s - loss: 0.9308 - accuracy: 0.703 - ETA: 36s - loss: 0.9317 - accuracy: 0.703 - ETA: 35s - loss: 0.9300 - accuracy: 0.703 - ETA: 34s - loss: 0.9295 - accuracy: 0.704 - ETA: 34s - loss: 0.9291 - accuracy: 0.704 - ETA: 33s - loss: 0.9283 - accuracy: 0.704 - ETA: 32s - loss: 0.9268 - accuracy: 0.705 - ETA: 31s - loss: 0.9291 - accuracy: 0.705 - ETA: 30s - loss: 0.9287 - accuracy: 0.705 - ETA: 29s - loss: 0.9294 - accuracy: 0.704 - ETA: 28s - loss: 0.9269 - accuracy: 0.705 - ETA: 27s - loss: 0.9279 - accuracy: 0.705 - ETA: 26s - loss: 0.9282 - accuracy: 0.705 - ETA: 26s - loss: 0.9315 - accuracy: 0.704 - ETA: 25s - loss: 0.9312 - accuracy: 0.704 - ETA: 24s - loss: 0.9319 - accuracy: 0.704 - ETA: 23s - loss: 0.9306 - accuracy: 0.704 - ETA: 22s - loss: 0.9315 - accuracy: 0.704 - ETA: 21s - loss: 0.9339 - accuracy: 0.703 - ETA: 20s - loss: 0.9346 - accuracy: 0.704 - ETA: 19s - loss: 0.9358 - accuracy: 0.704 - ETA: 18s - loss: 0.9363 - accuracy: 0.704 - ETA: 17s - loss: 0.9350 - accuracy: 0.704 - ETA: 16s - loss: 0.9350 - accuracy: 0.704 - ETA: 16s - loss: 0.9357 - accuracy: 0.704 - ETA: 15s - loss: 0.9342 - accuracy: 0.705 - ETA: 14s - loss: 0.9346 - accuracy: 0.705 - ETA: 13s - loss: 0.9355 - accuracy: 0.705 - ETA: 12s - loss: 0.9351 - accuracy: 0.705 - ETA: 11s - loss: 0.9355 - accuracy: 0.705 - ETA: 10s - loss: 0.9354 - accuracy: 0.705 - ETA: 9s - loss: 0.9358 - accuracy: 0.705 - ETA: 8s - loss: 0.9351 - accuracy: 0.70 - ETA: 7s - loss: 0.9322 - accuracy: 0.70 - ETA: 6s - loss: 0.9315 - accuracy: 0.70 - ETA: 6s - loss: 0.9335 - accuracy: 0.70 - ETA: 5s - loss: 0.9337 - accuracy: 0.70 - ETA: 4s - loss: 0.9345 - accuracy: 0.70 - ETA: 3s - loss: 0.9357 - accuracy: 0.70 - ETA: 2s - loss: 0.9365 - accuracy: 0.70 - ETA: 1s - loss: 0.9356 - accuracy: 0.70 - ETA: 0s - loss: 0.9373 - accuracy: 0.70 - 105s 8ms/step - loss: 0.9394 - accuracy: 0.7037 - val_loss: 2.7206 - val_accuracy: 0.3520\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.9845 - accuracy: 0.73 - ETA: 1:30 - loss: 0.8869 - accuracy: 0.73 - ETA: 1:27 - loss: 0.9189 - accuracy: 0.70 - ETA: 1:27 - loss: 0.9876 - accuracy: 0.68 - ETA: 1:28 - loss: 0.9551 - accuracy: 0.70 - ETA: 1:27 - loss: 0.9459 - accuracy: 0.70 - ETA: 1:26 - loss: 0.9637 - accuracy: 0.69 - ETA: 1:25 - loss: 0.9582 - accuracy: 0.69 - ETA: 1:24 - loss: 0.9462 - accuracy: 0.70 - ETA: 1:23 - loss: 0.9372 - accuracy: 0.70 - ETA: 1:22 - loss: 0.9288 - accuracy: 0.70 - ETA: 1:21 - loss: 0.9342 - accuracy: 0.70 - ETA: 1:20 - loss: 0.9243 - accuracy: 0.70 - ETA: 1:19 - loss: 0.9163 - accuracy: 0.70 - ETA: 1:18 - loss: 0.9061 - accuracy: 0.70 - ETA: 1:17 - loss: 0.9085 - accuracy: 0.70 - ETA: 1:16 - loss: 0.9129 - accuracy: 0.70 - ETA: 1:15 - loss: 0.9090 - accuracy: 0.71 - ETA: 1:14 - loss: 0.9114 - accuracy: 0.70 - ETA: 1:13 - loss: 0.9054 - accuracy: 0.71 - ETA: 1:13 - loss: 0.9136 - accuracy: 0.70 - ETA: 1:12 - loss: 0.9164 - accuracy: 0.70 - ETA: 1:11 - loss: 0.9123 - accuracy: 0.70 - ETA: 1:10 - loss: 0.9072 - accuracy: 0.70 - ETA: 1:10 - loss: 0.9056 - accuracy: 0.70 - ETA: 1:09 - loss: 0.9082 - accuracy: 0.70 - ETA: 1:08 - loss: 0.9055 - accuracy: 0.71 - ETA: 1:07 - loss: 0.9065 - accuracy: 0.71 - ETA: 1:06 - loss: 0.9113 - accuracy: 0.71 - ETA: 1:05 - loss: 0.9082 - accuracy: 0.71 - ETA: 1:04 - loss: 0.9049 - accuracy: 0.71 - ETA: 1:03 - loss: 0.8992 - accuracy: 0.71 - ETA: 1:02 - loss: 0.8990 - accuracy: 0.71 - ETA: 1:01 - loss: 0.8995 - accuracy: 0.71 - ETA: 1:00 - loss: 0.9000 - accuracy: 0.71 - ETA: 59s - loss: 0.9011 - accuracy: 0.7138 - ETA: 58s - loss: 0.8986 - accuracy: 0.714 - ETA: 57s - loss: 0.8996 - accuracy: 0.715 - ETA: 56s - loss: 0.9015 - accuracy: 0.715 - ETA: 56s - loss: 0.9025 - accuracy: 0.715 - ETA: 55s - loss: 0.9035 - accuracy: 0.715 - ETA: 54s - loss: 0.9035 - accuracy: 0.715 - ETA: 53s - loss: 0.9061 - accuracy: 0.714 - ETA: 52s - loss: 0.9089 - accuracy: 0.714 - ETA: 51s - loss: 0.9081 - accuracy: 0.713 - ETA: 50s - loss: 0.9083 - accuracy: 0.714 - ETA: 49s - loss: 0.9088 - accuracy: 0.715 - ETA: 48s - loss: 0.9084 - accuracy: 0.715 - ETA: 47s - loss: 0.9115 - accuracy: 0.715 - ETA: 46s - loss: 0.9056 - accuracy: 0.716 - ETA: 45s - loss: 0.9060 - accuracy: 0.716 - ETA: 44s - loss: 0.9036 - accuracy: 0.716 - ETA: 44s - loss: 0.9040 - accuracy: 0.716 - ETA: 43s - loss: 0.9083 - accuracy: 0.716 - ETA: 42s - loss: 0.9096 - accuracy: 0.716 - ETA: 41s - loss: 0.9102 - accuracy: 0.715 - ETA: 40s - loss: 0.9103 - accuracy: 0.715 - ETA: 39s - loss: 0.9120 - accuracy: 0.715 - ETA: 38s - loss: 0.9109 - accuracy: 0.716 - ETA: 38s - loss: 0.9101 - accuracy: 0.716 - ETA: 37s - loss: 0.9109 - accuracy: 0.716 - ETA: 36s - loss: 0.9107 - accuracy: 0.717 - ETA: 35s - loss: 0.9152 - accuracy: 0.716 - ETA: 34s - loss: 0.9141 - accuracy: 0.716 - ETA: 33s - loss: 0.9173 - accuracy: 0.716 - ETA: 32s - loss: 0.9189 - accuracy: 0.715 - ETA: 31s - loss: 0.9207 - accuracy: 0.715 - ETA: 30s - loss: 0.9200 - accuracy: 0.715 - ETA: 29s - loss: 0.9193 - accuracy: 0.716 - ETA: 29s - loss: 0.9236 - accuracy: 0.714 - ETA: 28s - loss: 0.9233 - accuracy: 0.714 - ETA: 27s - loss: 0.9264 - accuracy: 0.713 - ETA: 26s - loss: 0.9265 - accuracy: 0.713 - ETA: 25s - loss: 0.9240 - accuracy: 0.714 - ETA: 24s - loss: 0.9241 - accuracy: 0.714 - ETA: 23s - loss: 0.9253 - accuracy: 0.714 - ETA: 22s - loss: 0.9266 - accuracy: 0.714 - ETA: 21s - loss: 0.9282 - accuracy: 0.714 - ETA: 20s - loss: 0.9279 - accuracy: 0.715 - ETA: 19s - loss: 0.9303 - accuracy: 0.714 - ETA: 18s - loss: 0.9311 - accuracy: 0.714 - ETA: 18s - loss: 0.9301 - accuracy: 0.714 - ETA: 17s - loss: 0.9295 - accuracy: 0.714 - ETA: 16s - loss: 0.9315 - accuracy: 0.713 - ETA: 15s - loss: 0.9334 - accuracy: 0.713 - ETA: 14s - loss: 0.9325 - accuracy: 0.713 - ETA: 13s - loss: 0.9311 - accuracy: 0.714 - ETA: 12s - loss: 0.9306 - accuracy: 0.714 - ETA: 11s - loss: 0.9318 - accuracy: 0.713 - ETA: 10s - loss: 0.9317 - accuracy: 0.714 - ETA: 9s - loss: 0.9325 - accuracy: 0.713 - ETA: 8s - loss: 0.9342 - accuracy: 0.71 - ETA: 7s - loss: 0.9372 - accuracy: 0.71 - ETA: 7s - loss: 0.9352 - accuracy: 0.71 - ETA: 6s - loss: 0.9383 - accuracy: 0.71 - ETA: 5s - loss: 0.9410 - accuracy: 0.71 - ETA: 4s - loss: 0.9392 - accuracy: 0.71 - ETA: 3s - loss: 0.9382 - accuracy: 0.71 - ETA: 2s - loss: 0.9382 - accuracy: 0.71 - ETA: 1s - loss: 0.9370 - accuracy: 0.71 - ETA: 0s - loss: 0.9367 - accuracy: 0.71 - 106s 8ms/step - loss: 0.9371 - accuracy: 0.7106 - val_loss: 2.8300 - val_accuracy: 0.3195\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.9688 - accuracy: 0.71 - ETA: 1:29 - loss: 1.0770 - accuracy: 0.68 - ETA: 1:27 - loss: 1.0149 - accuracy: 0.69 - ETA: 1:26 - loss: 1.0615 - accuracy: 0.68 - ETA: 1:26 - loss: 1.0373 - accuracy: 0.69 - ETA: 1:25 - loss: 1.0224 - accuracy: 0.69 - ETA: 1:24 - loss: 1.0158 - accuracy: 0.69 - ETA: 1:23 - loss: 1.0040 - accuracy: 0.70 - ETA: 1:22 - loss: 0.9938 - accuracy: 0.70 - ETA: 1:21 - loss: 0.9760 - accuracy: 0.71 - ETA: 1:20 - loss: 0.9514 - accuracy: 0.71 - ETA: 1:19 - loss: 0.9773 - accuracy: 0.71 - ETA: 1:19 - loss: 0.9694 - accuracy: 0.71 - ETA: 1:18 - loss: 0.9678 - accuracy: 0.71 - ETA: 1:17 - loss: 0.9549 - accuracy: 0.71 - ETA: 1:16 - loss: 0.9423 - accuracy: 0.72 - ETA: 1:15 - loss: 0.9493 - accuracy: 0.72 - ETA: 1:14 - loss: 0.9492 - accuracy: 0.71 - ETA: 1:13 - loss: 0.9554 - accuracy: 0.71 - ETA: 1:12 - loss: 0.9470 - accuracy: 0.71 - ETA: 1:11 - loss: 0.9503 - accuracy: 0.71 - ETA: 1:10 - loss: 0.9436 - accuracy: 0.71 - ETA: 1:09 - loss: 0.9444 - accuracy: 0.71 - ETA: 1:08 - loss: 0.9475 - accuracy: 0.71 - ETA: 1:08 - loss: 0.9436 - accuracy: 0.71 - ETA: 1:07 - loss: 0.9418 - accuracy: 0.71 - ETA: 1:06 - loss: 0.9427 - accuracy: 0.71 - ETA: 1:05 - loss: 0.9357 - accuracy: 0.72 - ETA: 1:04 - loss: 0.9400 - accuracy: 0.71 - ETA: 1:03 - loss: 0.9404 - accuracy: 0.71 - ETA: 1:02 - loss: 0.9463 - accuracy: 0.71 - ETA: 1:02 - loss: 0.9490 - accuracy: 0.71 - ETA: 1:01 - loss: 0.9477 - accuracy: 0.71 - ETA: 1:00 - loss: 0.9465 - accuracy: 0.71 - ETA: 59s - loss: 0.9444 - accuracy: 0.7172 - ETA: 58s - loss: 0.9493 - accuracy: 0.715 - ETA: 57s - loss: 0.9488 - accuracy: 0.716 - ETA: 56s - loss: 0.9536 - accuracy: 0.715 - ETA: 55s - loss: 0.9548 - accuracy: 0.714 - ETA: 54s - loss: 0.9571 - accuracy: 0.712 - ETA: 53s - loss: 0.9551 - accuracy: 0.712 - ETA: 53s - loss: 0.9535 - accuracy: 0.712 - ETA: 52s - loss: 0.9544 - accuracy: 0.711 - ETA: 51s - loss: 0.9521 - accuracy: 0.712 - ETA: 50s - loss: 0.9471 - accuracy: 0.713 - ETA: 49s - loss: 0.9416 - accuracy: 0.714 - ETA: 48s - loss: 0.9394 - accuracy: 0.714 - ETA: 47s - loss: 0.9427 - accuracy: 0.714 - ETA: 47s - loss: 0.9384 - accuracy: 0.715 - ETA: 46s - loss: 0.9384 - accuracy: 0.716 - ETA: 45s - loss: 0.9391 - accuracy: 0.716 - ETA: 44s - loss: 0.9409 - accuracy: 0.716 - ETA: 43s - loss: 0.9364 - accuracy: 0.718 - ETA: 42s - loss: 0.9321 - accuracy: 0.719 - ETA: 41s - loss: 0.9326 - accuracy: 0.718 - ETA: 41s - loss: 0.9329 - accuracy: 0.717 - ETA: 40s - loss: 0.9297 - accuracy: 0.719 - ETA: 39s - loss: 0.9302 - accuracy: 0.718 - ETA: 38s - loss: 0.9288 - accuracy: 0.719 - ETA: 37s - loss: 0.9287 - accuracy: 0.718 - ETA: 36s - loss: 0.9303 - accuracy: 0.717 - ETA: 35s - loss: 0.9307 - accuracy: 0.717 - ETA: 34s - loss: 0.9329 - accuracy: 0.717 - ETA: 33s - loss: 0.9324 - accuracy: 0.716 - ETA: 32s - loss: 0.9288 - accuracy: 0.717 - ETA: 32s - loss: 0.9279 - accuracy: 0.717 - ETA: 31s - loss: 0.9272 - accuracy: 0.717 - ETA: 30s - loss: 0.9273 - accuracy: 0.717 - ETA: 29s - loss: 0.9293 - accuracy: 0.716 - ETA: 28s - loss: 0.9311 - accuracy: 0.715 - ETA: 27s - loss: 0.9357 - accuracy: 0.714 - ETA: 26s - loss: 0.9331 - accuracy: 0.715 - ETA: 25s - loss: 0.9318 - accuracy: 0.715 - ETA: 24s - loss: 0.9322 - accuracy: 0.715 - ETA: 23s - loss: 0.9295 - accuracy: 0.715 - ETA: 23s - loss: 0.9311 - accuracy: 0.715 - ETA: 22s - loss: 0.9317 - accuracy: 0.715 - ETA: 21s - loss: 0.9333 - accuracy: 0.715 - ETA: 20s - loss: 0.9320 - accuracy: 0.715 - ETA: 19s - loss: 0.9320 - accuracy: 0.715 - ETA: 18s - loss: 0.9318 - accuracy: 0.714 - ETA: 17s - loss: 0.9313 - accuracy: 0.714 - ETA: 16s - loss: 0.9313 - accuracy: 0.714 - ETA: 15s - loss: 0.9307 - accuracy: 0.714 - ETA: 15s - loss: 0.9284 - accuracy: 0.715 - ETA: 14s - loss: 0.9278 - accuracy: 0.715 - ETA: 13s - loss: 0.9282 - accuracy: 0.715 - ETA: 12s - loss: 0.9309 - accuracy: 0.714 - ETA: 11s - loss: 0.9310 - accuracy: 0.714 - ETA: 10s - loss: 0.9321 - accuracy: 0.714 - ETA: 9s - loss: 0.9352 - accuracy: 0.713 - ETA: 8s - loss: 0.9330 - accuracy: 0.71 - ETA: 7s - loss: 0.9317 - accuracy: 0.71 - ETA: 6s - loss: 0.9297 - accuracy: 0.71 - ETA: 6s - loss: 0.9310 - accuracy: 0.71 - ETA: 5s - loss: 0.9337 - accuracy: 0.71 - ETA: 4s - loss: 0.9360 - accuracy: 0.71 - ETA: 3s - loss: 0.9349 - accuracy: 0.71 - ETA: 2s - loss: 0.9347 - accuracy: 0.71 - ETA: 1s - loss: 0.9353 - accuracy: 0.71 - ETA: 0s - loss: 0.9351 - accuracy: 0.71 - 105s 8ms/step - loss: 0.9339 - accuracy: 0.7140 - val_loss: 2.6914 - val_accuracy: 0.3485\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:36 - loss: 0.9630 - accuracy: 0.70 - ETA: 1:34 - loss: 0.9736 - accuracy: 0.70 - ETA: 1:32 - loss: 0.8865 - accuracy: 0.73 - ETA: 1:32 - loss: 0.8969 - accuracy: 0.73 - ETA: 1:31 - loss: 0.8688 - accuracy: 0.72 - ETA: 1:29 - loss: 0.8707 - accuracy: 0.72 - ETA: 1:28 - loss: 0.8815 - accuracy: 0.72 - ETA: 1:27 - loss: 0.9034 - accuracy: 0.71 - ETA: 1:26 - loss: 0.8885 - accuracy: 0.72 - ETA: 1:25 - loss: 0.8954 - accuracy: 0.72 - ETA: 1:24 - loss: 0.8962 - accuracy: 0.71 - ETA: 1:22 - loss: 0.8850 - accuracy: 0.72 - ETA: 1:21 - loss: 0.8704 - accuracy: 0.72 - ETA: 1:20 - loss: 0.8819 - accuracy: 0.72 - ETA: 1:19 - loss: 0.8723 - accuracy: 0.72 - ETA: 1:18 - loss: 0.8839 - accuracy: 0.71 - ETA: 1:17 - loss: 0.8845 - accuracy: 0.71 - ETA: 1:16 - loss: 0.8901 - accuracy: 0.71 - ETA: 1:15 - loss: 0.8875 - accuracy: 0.71 - ETA: 1:14 - loss: 0.8987 - accuracy: 0.71 - ETA: 1:13 - loss: 0.8996 - accuracy: 0.71 - ETA: 1:12 - loss: 0.9003 - accuracy: 0.71 - ETA: 1:12 - loss: 0.8910 - accuracy: 0.71 - ETA: 1:11 - loss: 0.8924 - accuracy: 0.71 - ETA: 1:10 - loss: 0.8879 - accuracy: 0.71 - ETA: 1:08 - loss: 0.8873 - accuracy: 0.71 - ETA: 1:07 - loss: 0.8929 - accuracy: 0.71 - ETA: 1:07 - loss: 0.8940 - accuracy: 0.72 - ETA: 1:06 - loss: 0.8941 - accuracy: 0.72 - ETA: 1:05 - loss: 0.8919 - accuracy: 0.72 - ETA: 1:04 - loss: 0.8990 - accuracy: 0.72 - ETA: 1:03 - loss: 0.9032 - accuracy: 0.72 - ETA: 1:02 - loss: 0.9028 - accuracy: 0.72 - ETA: 1:01 - loss: 0.9132 - accuracy: 0.71 - ETA: 1:00 - loss: 0.9052 - accuracy: 0.72 - ETA: 59s - loss: 0.9056 - accuracy: 0.7203 - ETA: 58s - loss: 0.9064 - accuracy: 0.720 - ETA: 58s - loss: 0.9060 - accuracy: 0.720 - ETA: 57s - loss: 0.9097 - accuracy: 0.720 - ETA: 56s - loss: 0.9132 - accuracy: 0.719 - ETA: 55s - loss: 0.9079 - accuracy: 0.721 - ETA: 54s - loss: 0.9079 - accuracy: 0.720 - ETA: 53s - loss: 0.9082 - accuracy: 0.720 - ETA: 52s - loss: 0.9073 - accuracy: 0.720 - ETA: 51s - loss: 0.9030 - accuracy: 0.721 - ETA: 50s - loss: 0.9011 - accuracy: 0.723 - ETA: 49s - loss: 0.9025 - accuracy: 0.722 - ETA: 48s - loss: 0.9000 - accuracy: 0.723 - ETA: 47s - loss: 0.8985 - accuracy: 0.723 - ETA: 46s - loss: 0.8985 - accuracy: 0.723 - ETA: 45s - loss: 0.9054 - accuracy: 0.721 - ETA: 44s - loss: 0.9093 - accuracy: 0.720 - ETA: 44s - loss: 0.9105 - accuracy: 0.720 - ETA: 43s - loss: 0.9123 - accuracy: 0.718 - ETA: 42s - loss: 0.9092 - accuracy: 0.719 - ETA: 41s - loss: 0.9144 - accuracy: 0.718 - ETA: 40s - loss: 0.9083 - accuracy: 0.721 - ETA: 39s - loss: 0.9055 - accuracy: 0.722 - ETA: 38s - loss: 0.9083 - accuracy: 0.721 - ETA: 37s - loss: 0.9086 - accuracy: 0.720 - ETA: 36s - loss: 0.9092 - accuracy: 0.719 - ETA: 35s - loss: 0.9069 - accuracy: 0.719 - ETA: 34s - loss: 0.9085 - accuracy: 0.719 - ETA: 34s - loss: 0.9093 - accuracy: 0.718 - ETA: 33s - loss: 0.9082 - accuracy: 0.718 - ETA: 32s - loss: 0.9087 - accuracy: 0.717 - ETA: 31s - loss: 0.9087 - accuracy: 0.717 - ETA: 30s - loss: 0.9121 - accuracy: 0.716 - ETA: 29s - loss: 0.9114 - accuracy: 0.717 - ETA: 28s - loss: 0.9099 - accuracy: 0.717 - ETA: 27s - loss: 0.9124 - accuracy: 0.716 - ETA: 26s - loss: 0.9132 - accuracy: 0.716 - ETA: 25s - loss: 0.9150 - accuracy: 0.716 - ETA: 25s - loss: 0.9169 - accuracy: 0.717 - ETA: 24s - loss: 0.9189 - accuracy: 0.716 - ETA: 23s - loss: 0.9196 - accuracy: 0.716 - ETA: 22s - loss: 0.9188 - accuracy: 0.717 - ETA: 21s - loss: 0.9201 - accuracy: 0.716 - ETA: 20s - loss: 0.9203 - accuracy: 0.716 - ETA: 19s - loss: 0.9215 - accuracy: 0.716 - ETA: 18s - loss: 0.9218 - accuracy: 0.716 - ETA: 17s - loss: 0.9214 - accuracy: 0.716 - ETA: 16s - loss: 0.9238 - accuracy: 0.715 - ETA: 16s - loss: 0.9231 - accuracy: 0.715 - ETA: 15s - loss: 0.9238 - accuracy: 0.714 - ETA: 14s - loss: 0.9237 - accuracy: 0.714 - ETA: 13s - loss: 0.9251 - accuracy: 0.714 - ETA: 12s - loss: 0.9253 - accuracy: 0.714 - ETA: 11s - loss: 0.9293 - accuracy: 0.713 - ETA: 10s - loss: 0.9317 - accuracy: 0.712 - ETA: 9s - loss: 0.9322 - accuracy: 0.711 - ETA: 8s - loss: 0.9327 - accuracy: 0.71 - ETA: 7s - loss: 0.9314 - accuracy: 0.71 - ETA: 7s - loss: 0.9312 - accuracy: 0.71 - ETA: 6s - loss: 0.9307 - accuracy: 0.71 - ETA: 5s - loss: 0.9292 - accuracy: 0.71 - ETA: 4s - loss: 0.9263 - accuracy: 0.71 - ETA: 3s - loss: 0.9255 - accuracy: 0.71 - ETA: 2s - loss: 0.9269 - accuracy: 0.71 - ETA: 1s - loss: 0.9264 - accuracy: 0.71 - ETA: 0s - loss: 0.9243 - accuracy: 0.71 - 106s 8ms/step - loss: 0.9253 - accuracy: 0.7144 - val_loss: 2.9305 - val_accuracy: 0.3157\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 0.9999 - accuracy: 0.67 - ETA: 1:29 - loss: 0.9460 - accuracy: 0.68 - ETA: 1:28 - loss: 0.8715 - accuracy: 0.71 - ETA: 1:27 - loss: 0.8555 - accuracy: 0.72 - ETA: 1:26 - loss: 0.8499 - accuracy: 0.72 - ETA: 1:25 - loss: 0.8535 - accuracy: 0.73 - ETA: 1:24 - loss: 0.8545 - accuracy: 0.73 - ETA: 1:24 - loss: 0.8482 - accuracy: 0.72 - ETA: 1:24 - loss: 0.8754 - accuracy: 0.72 - ETA: 1:23 - loss: 0.9049 - accuracy: 0.71 - ETA: 1:22 - loss: 0.9151 - accuracy: 0.71 - ETA: 1:21 - loss: 0.9242 - accuracy: 0.70 - ETA: 1:20 - loss: 0.9226 - accuracy: 0.70 - ETA: 1:19 - loss: 0.9169 - accuracy: 0.70 - ETA: 1:18 - loss: 0.9239 - accuracy: 0.70 - ETA: 1:17 - loss: 0.9146 - accuracy: 0.71 - ETA: 1:16 - loss: 0.9174 - accuracy: 0.71 - ETA: 1:15 - loss: 0.9163 - accuracy: 0.71 - ETA: 1:14 - loss: 0.9033 - accuracy: 0.72 - ETA: 1:13 - loss: 0.8963 - accuracy: 0.72 - ETA: 1:12 - loss: 0.8961 - accuracy: 0.72 - ETA: 1:11 - loss: 0.8868 - accuracy: 0.72 - ETA: 1:10 - loss: 0.8950 - accuracy: 0.72 - ETA: 1:09 - loss: 0.8856 - accuracy: 0.72 - ETA: 1:08 - loss: 0.8823 - accuracy: 0.72 - ETA: 1:07 - loss: 0.8807 - accuracy: 0.72 - ETA: 1:07 - loss: 0.8802 - accuracy: 0.72 - ETA: 1:06 - loss: 0.8792 - accuracy: 0.72 - ETA: 1:05 - loss: 0.8773 - accuracy: 0.72 - ETA: 1:04 - loss: 0.8774 - accuracy: 0.72 - ETA: 1:03 - loss: 0.8764 - accuracy: 0.72 - ETA: 1:02 - loss: 0.8820 - accuracy: 0.72 - ETA: 1:01 - loss: 0.8874 - accuracy: 0.71 - ETA: 1:01 - loss: 0.8922 - accuracy: 0.71 - ETA: 1:00 - loss: 0.8911 - accuracy: 0.71 - ETA: 59s - loss: 0.8928 - accuracy: 0.7192 - ETA: 58s - loss: 0.8963 - accuracy: 0.718 - ETA: 57s - loss: 0.8946 - accuracy: 0.718 - ETA: 56s - loss: 0.8962 - accuracy: 0.719 - ETA: 55s - loss: 0.8939 - accuracy: 0.719 - ETA: 55s - loss: 0.8928 - accuracy: 0.719 - ETA: 54s - loss: 0.8966 - accuracy: 0.718 - ETA: 53s - loss: 0.9003 - accuracy: 0.716 - ETA: 52s - loss: 0.9008 - accuracy: 0.717 - ETA: 51s - loss: 0.8999 - accuracy: 0.716 - ETA: 50s - loss: 0.8975 - accuracy: 0.717 - ETA: 49s - loss: 0.8967 - accuracy: 0.717 - ETA: 48s - loss: 0.8980 - accuracy: 0.717 - ETA: 47s - loss: 0.8983 - accuracy: 0.717 - ETA: 46s - loss: 0.9005 - accuracy: 0.716 - ETA: 45s - loss: 0.9031 - accuracy: 0.715 - ETA: 44s - loss: 0.9061 - accuracy: 0.714 - ETA: 44s - loss: 0.9106 - accuracy: 0.714 - ETA: 43s - loss: 0.9081 - accuracy: 0.714 - ETA: 42s - loss: 0.9151 - accuracy: 0.712 - ETA: 41s - loss: 0.9170 - accuracy: 0.712 - ETA: 40s - loss: 0.9183 - accuracy: 0.712 - ETA: 39s - loss: 0.9208 - accuracy: 0.712 - ETA: 38s - loss: 0.9224 - accuracy: 0.711 - ETA: 37s - loss: 0.9247 - accuracy: 0.711 - ETA: 36s - loss: 0.9261 - accuracy: 0.712 - ETA: 35s - loss: 0.9254 - accuracy: 0.711 - ETA: 34s - loss: 0.9320 - accuracy: 0.709 - ETA: 34s - loss: 0.9302 - accuracy: 0.710 - ETA: 33s - loss: 0.9296 - accuracy: 0.710 - ETA: 32s - loss: 0.9280 - accuracy: 0.710 - ETA: 31s - loss: 0.9251 - accuracy: 0.710 - ETA: 30s - loss: 0.9254 - accuracy: 0.709 - ETA: 29s - loss: 0.9265 - accuracy: 0.710 - ETA: 28s - loss: 0.9271 - accuracy: 0.709 - ETA: 27s - loss: 0.9260 - accuracy: 0.710 - ETA: 26s - loss: 0.9234 - accuracy: 0.711 - ETA: 25s - loss: 0.9197 - accuracy: 0.712 - ETA: 24s - loss: 0.9194 - accuracy: 0.712 - ETA: 24s - loss: 0.9197 - accuracy: 0.712 - ETA: 23s - loss: 0.9167 - accuracy: 0.713 - ETA: 22s - loss: 0.9182 - accuracy: 0.712 - ETA: 21s - loss: 0.9153 - accuracy: 0.713 - ETA: 20s - loss: 0.9158 - accuracy: 0.713 - ETA: 19s - loss: 0.9147 - accuracy: 0.713 - ETA: 18s - loss: 0.9136 - accuracy: 0.713 - ETA: 17s - loss: 0.9134 - accuracy: 0.713 - ETA: 16s - loss: 0.9115 - accuracy: 0.713 - ETA: 15s - loss: 0.9122 - accuracy: 0.713 - ETA: 15s - loss: 0.9111 - accuracy: 0.713 - ETA: 14s - loss: 0.9133 - accuracy: 0.712 - ETA: 13s - loss: 0.9126 - accuracy: 0.712 - ETA: 12s - loss: 0.9127 - accuracy: 0.713 - ETA: 11s - loss: 0.9108 - accuracy: 0.713 - ETA: 10s - loss: 0.9114 - accuracy: 0.713 - ETA: 9s - loss: 0.9099 - accuracy: 0.714 - ETA: 8s - loss: 0.9099 - accuracy: 0.71 - ETA: 7s - loss: 0.9091 - accuracy: 0.71 - ETA: 6s - loss: 0.9097 - accuracy: 0.71 - ETA: 6s - loss: 0.9088 - accuracy: 0.71 - ETA: 5s - loss: 0.9069 - accuracy: 0.71 - ETA: 4s - loss: 0.9074 - accuracy: 0.71 - ETA: 3s - loss: 0.9092 - accuracy: 0.71 - ETA: 2s - loss: 0.9110 - accuracy: 0.71 - ETA: 1s - loss: 0.9091 - accuracy: 0.71 - ETA: 0s - loss: 0.9072 - accuracy: 0.71 - 106s 8ms/step - loss: 0.9080 - accuracy: 0.7151 - val_loss: 2.8737 - val_accuracy: 0.3378\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:36 - loss: 0.8305 - accuracy: 0.70 - ETA: 1:32 - loss: 0.7939 - accuracy: 0.72 - ETA: 1:30 - loss: 0.7740 - accuracy: 0.73 - ETA: 1:29 - loss: 0.8428 - accuracy: 0.72 - ETA: 1:27 - loss: 0.8459 - accuracy: 0.72 - ETA: 1:26 - loss: 0.8772 - accuracy: 0.71 - ETA: 1:25 - loss: 0.8920 - accuracy: 0.70 - ETA: 1:24 - loss: 0.8902 - accuracy: 0.71 - ETA: 1:23 - loss: 0.8860 - accuracy: 0.71 - ETA: 1:22 - loss: 0.8725 - accuracy: 0.72 - ETA: 1:21 - loss: 0.8658 - accuracy: 0.72 - ETA: 1:20 - loss: 0.8832 - accuracy: 0.72 - ETA: 1:19 - loss: 0.9020 - accuracy: 0.71 - ETA: 1:19 - loss: 0.9050 - accuracy: 0.71 - ETA: 1:18 - loss: 0.8973 - accuracy: 0.71 - ETA: 1:17 - loss: 0.8993 - accuracy: 0.71 - ETA: 1:17 - loss: 0.9037 - accuracy: 0.71 - ETA: 1:15 - loss: 0.9088 - accuracy: 0.71 - ETA: 1:15 - loss: 0.9103 - accuracy: 0.71 - ETA: 1:14 - loss: 0.9084 - accuracy: 0.71 - ETA: 1:13 - loss: 0.9094 - accuracy: 0.71 - ETA: 1:12 - loss: 0.9095 - accuracy: 0.71 - ETA: 1:11 - loss: 0.9049 - accuracy: 0.71 - ETA: 1:10 - loss: 0.8967 - accuracy: 0.72 - ETA: 1:09 - loss: 0.8865 - accuracy: 0.72 - ETA: 1:08 - loss: 0.8859 - accuracy: 0.72 - ETA: 1:07 - loss: 0.8819 - accuracy: 0.72 - ETA: 1:06 - loss: 0.8877 - accuracy: 0.72 - ETA: 1:05 - loss: 0.8811 - accuracy: 0.72 - ETA: 1:04 - loss: 0.8818 - accuracy: 0.72 - ETA: 1:03 - loss: 0.8866 - accuracy: 0.72 - ETA: 1:02 - loss: 0.8881 - accuracy: 0.72 - ETA: 1:02 - loss: 0.8894 - accuracy: 0.72 - ETA: 1:01 - loss: 0.8937 - accuracy: 0.72 - ETA: 1:00 - loss: 0.8940 - accuracy: 0.72 - ETA: 59s - loss: 0.8971 - accuracy: 0.7166 - ETA: 58s - loss: 0.8938 - accuracy: 0.716 - ETA: 57s - loss: 0.8934 - accuracy: 0.717 - ETA: 56s - loss: 0.8912 - accuracy: 0.718 - ETA: 55s - loss: 0.8894 - accuracy: 0.718 - ETA: 54s - loss: 0.8910 - accuracy: 0.717 - ETA: 54s - loss: 0.8933 - accuracy: 0.717 - ETA: 53s - loss: 0.8946 - accuracy: 0.717 - ETA: 52s - loss: 0.8910 - accuracy: 0.718 - ETA: 51s - loss: 0.8868 - accuracy: 0.719 - ETA: 50s - loss: 0.8886 - accuracy: 0.719 - ETA: 49s - loss: 0.8916 - accuracy: 0.718 - ETA: 48s - loss: 0.8930 - accuracy: 0.718 - ETA: 47s - loss: 0.8971 - accuracy: 0.717 - ETA: 46s - loss: 0.9004 - accuracy: 0.716 - ETA: 45s - loss: 0.9016 - accuracy: 0.716 - ETA: 45s - loss: 0.9003 - accuracy: 0.716 - ETA: 44s - loss: 0.8979 - accuracy: 0.716 - ETA: 43s - loss: 0.8972 - accuracy: 0.716 - ETA: 42s - loss: 0.8971 - accuracy: 0.716 - ETA: 41s - loss: 0.8933 - accuracy: 0.717 - ETA: 40s - loss: 0.8949 - accuracy: 0.717 - ETA: 39s - loss: 0.8954 - accuracy: 0.717 - ETA: 38s - loss: 0.8953 - accuracy: 0.718 - ETA: 37s - loss: 0.8938 - accuracy: 0.719 - ETA: 36s - loss: 0.8904 - accuracy: 0.720 - ETA: 36s - loss: 0.8902 - accuracy: 0.720 - ETA: 35s - loss: 0.8878 - accuracy: 0.721 - ETA: 34s - loss: 0.8890 - accuracy: 0.720 - ETA: 33s - loss: 0.8886 - accuracy: 0.720 - ETA: 32s - loss: 0.8893 - accuracy: 0.720 - ETA: 31s - loss: 0.8914 - accuracy: 0.719 - ETA: 30s - loss: 0.8894 - accuracy: 0.720 - ETA: 29s - loss: 0.8899 - accuracy: 0.719 - ETA: 28s - loss: 0.8887 - accuracy: 0.719 - ETA: 28s - loss: 0.8887 - accuracy: 0.719 - ETA: 27s - loss: 0.8881 - accuracy: 0.719 - ETA: 26s - loss: 0.8878 - accuracy: 0.720 - ETA: 25s - loss: 0.8874 - accuracy: 0.720 - ETA: 24s - loss: 0.8841 - accuracy: 0.721 - ETA: 23s - loss: 0.8817 - accuracy: 0.721 - ETA: 22s - loss: 0.8794 - accuracy: 0.723 - ETA: 21s - loss: 0.8819 - accuracy: 0.722 - ETA: 20s - loss: 0.8826 - accuracy: 0.722 - ETA: 19s - loss: 0.8827 - accuracy: 0.722 - ETA: 18s - loss: 0.8825 - accuracy: 0.722 - ETA: 17s - loss: 0.8835 - accuracy: 0.722 - ETA: 17s - loss: 0.8855 - accuracy: 0.721 - ETA: 16s - loss: 0.8852 - accuracy: 0.721 - ETA: 15s - loss: 0.8863 - accuracy: 0.721 - ETA: 14s - loss: 0.8867 - accuracy: 0.721 - ETA: 13s - loss: 0.8856 - accuracy: 0.721 - ETA: 12s - loss: 0.8870 - accuracy: 0.720 - ETA: 11s - loss: 0.8873 - accuracy: 0.721 - ETA: 10s - loss: 0.8882 - accuracy: 0.721 - ETA: 9s - loss: 0.8895 - accuracy: 0.720 - ETA: 8s - loss: 0.8912 - accuracy: 0.72 - ETA: 7s - loss: 0.8883 - accuracy: 0.72 - ETA: 7s - loss: 0.8867 - accuracy: 0.72 - ETA: 6s - loss: 0.8881 - accuracy: 0.72 - ETA: 5s - loss: 0.8898 - accuracy: 0.72 - ETA: 4s - loss: 0.8895 - accuracy: 0.72 - ETA: 3s - loss: 0.8897 - accuracy: 0.72 - ETA: 2s - loss: 0.8883 - accuracy: 0.72 - ETA: 1s - loss: 0.8886 - accuracy: 0.72 - ETA: 0s - loss: 0.8902 - accuracy: 0.71 - 106s 8ms/step - loss: 0.8897 - accuracy: 0.7198 - val_loss: 2.8066 - val_accuracy: 0.3258\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:31 - loss: 0.8526 - accuracy: 0.76 - ETA: 1:29 - loss: 0.9037 - accuracy: 0.73 - ETA: 1:27 - loss: 0.8315 - accuracy: 0.75 - ETA: 1:28 - loss: 0.8915 - accuracy: 0.73 - ETA: 1:26 - loss: 0.8430 - accuracy: 0.73 - ETA: 1:26 - loss: 0.8002 - accuracy: 0.75 - ETA: 1:25 - loss: 0.8045 - accuracy: 0.75 - ETA: 1:24 - loss: 0.7904 - accuracy: 0.75 - ETA: 1:22 - loss: 0.7862 - accuracy: 0.75 - ETA: 1:21 - loss: 0.7932 - accuracy: 0.75 - ETA: 1:20 - loss: 0.8014 - accuracy: 0.74 - ETA: 1:19 - loss: 0.8156 - accuracy: 0.74 - ETA: 1:18 - loss: 0.8181 - accuracy: 0.74 - ETA: 1:18 - loss: 0.8323 - accuracy: 0.73 - ETA: 1:17 - loss: 0.8361 - accuracy: 0.73 - ETA: 1:16 - loss: 0.8530 - accuracy: 0.73 - ETA: 1:15 - loss: 0.8494 - accuracy: 0.73 - ETA: 1:15 - loss: 0.8471 - accuracy: 0.73 - ETA: 1:14 - loss: 0.8531 - accuracy: 0.73 - ETA: 1:13 - loss: 0.8531 - accuracy: 0.73 - ETA: 1:12 - loss: 0.8670 - accuracy: 0.72 - ETA: 1:12 - loss: 0.8717 - accuracy: 0.72 - ETA: 1:11 - loss: 0.8766 - accuracy: 0.72 - ETA: 1:10 - loss: 0.8842 - accuracy: 0.72 - ETA: 1:09 - loss: 0.8841 - accuracy: 0.72 - ETA: 1:08 - loss: 0.8941 - accuracy: 0.72 - ETA: 1:07 - loss: 0.8966 - accuracy: 0.72 - ETA: 1:06 - loss: 0.8901 - accuracy: 0.72 - ETA: 1:05 - loss: 0.8923 - accuracy: 0.72 - ETA: 1:05 - loss: 0.8877 - accuracy: 0.72 - ETA: 1:04 - loss: 0.8939 - accuracy: 0.72 - ETA: 1:03 - loss: 0.8956 - accuracy: 0.72 - ETA: 1:02 - loss: 0.9029 - accuracy: 0.72 - ETA: 1:01 - loss: 0.8973 - accuracy: 0.72 - ETA: 1:00 - loss: 0.8954 - accuracy: 0.72 - ETA: 59s - loss: 0.8995 - accuracy: 0.7222 - ETA: 58s - loss: 0.9012 - accuracy: 0.721 - ETA: 57s - loss: 0.9008 - accuracy: 0.721 - ETA: 56s - loss: 0.8966 - accuracy: 0.721 - ETA: 55s - loss: 0.8977 - accuracy: 0.720 - ETA: 54s - loss: 0.8953 - accuracy: 0.721 - ETA: 54s - loss: 0.8940 - accuracy: 0.722 - ETA: 53s - loss: 0.8953 - accuracy: 0.722 - ETA: 52s - loss: 0.8892 - accuracy: 0.724 - ETA: 51s - loss: 0.8877 - accuracy: 0.725 - ETA: 50s - loss: 0.8875 - accuracy: 0.725 - ETA: 49s - loss: 0.8875 - accuracy: 0.725 - ETA: 48s - loss: 0.8846 - accuracy: 0.725 - ETA: 47s - loss: 0.8883 - accuracy: 0.724 - ETA: 46s - loss: 0.8858 - accuracy: 0.725 - ETA: 45s - loss: 0.8867 - accuracy: 0.725 - ETA: 44s - loss: 0.8878 - accuracy: 0.724 - ETA: 43s - loss: 0.8877 - accuracy: 0.724 - ETA: 43s - loss: 0.8854 - accuracy: 0.725 - ETA: 42s - loss: 0.8821 - accuracy: 0.725 - ETA: 41s - loss: 0.8799 - accuracy: 0.726 - ETA: 40s - loss: 0.8790 - accuracy: 0.727 - ETA: 39s - loss: 0.8843 - accuracy: 0.726 - ETA: 38s - loss: 0.8887 - accuracy: 0.725 - ETA: 37s - loss: 0.8856 - accuracy: 0.726 - ETA: 36s - loss: 0.8883 - accuracy: 0.725 - ETA: 35s - loss: 0.8862 - accuracy: 0.725 - ETA: 35s - loss: 0.8867 - accuracy: 0.725 - ETA: 34s - loss: 0.8904 - accuracy: 0.724 - ETA: 33s - loss: 0.8895 - accuracy: 0.725 - ETA: 32s - loss: 0.8910 - accuracy: 0.724 - ETA: 31s - loss: 0.8879 - accuracy: 0.725 - ETA: 30s - loss: 0.8860 - accuracy: 0.725 - ETA: 29s - loss: 0.8839 - accuracy: 0.725 - ETA: 28s - loss: 0.8840 - accuracy: 0.725 - ETA: 27s - loss: 0.8851 - accuracy: 0.725 - ETA: 26s - loss: 0.8878 - accuracy: 0.724 - ETA: 25s - loss: 0.8867 - accuracy: 0.725 - ETA: 25s - loss: 0.8875 - accuracy: 0.725 - ETA: 24s - loss: 0.8885 - accuracy: 0.725 - ETA: 23s - loss: 0.8894 - accuracy: 0.724 - ETA: 22s - loss: 0.8885 - accuracy: 0.724 - ETA: 21s - loss: 0.8863 - accuracy: 0.725 - ETA: 20s - loss: 0.8872 - accuracy: 0.725 - ETA: 19s - loss: 0.8860 - accuracy: 0.726 - ETA: 18s - loss: 0.8836 - accuracy: 0.726 - ETA: 17s - loss: 0.8858 - accuracy: 0.725 - ETA: 16s - loss: 0.8885 - accuracy: 0.724 - ETA: 16s - loss: 0.8869 - accuracy: 0.725 - ETA: 15s - loss: 0.8886 - accuracy: 0.724 - ETA: 14s - loss: 0.8878 - accuracy: 0.725 - ETA: 13s - loss: 0.8892 - accuracy: 0.724 - ETA: 12s - loss: 0.8893 - accuracy: 0.724 - ETA: 11s - loss: 0.8899 - accuracy: 0.723 - ETA: 10s - loss: 0.8907 - accuracy: 0.723 - ETA: 9s - loss: 0.8914 - accuracy: 0.723 - ETA: 8s - loss: 0.8930 - accuracy: 0.72 - ETA: 7s - loss: 0.8915 - accuracy: 0.72 - ETA: 7s - loss: 0.8910 - accuracy: 0.72 - ETA: 6s - loss: 0.8912 - accuracy: 0.72 - ETA: 5s - loss: 0.8894 - accuracy: 0.72 - ETA: 4s - loss: 0.8899 - accuracy: 0.72 - ETA: 3s - loss: 0.8890 - accuracy: 0.72 - ETA: 2s - loss: 0.8895 - accuracy: 0.72 - ETA: 1s - loss: 0.8880 - accuracy: 0.72 - ETA: 0s - loss: 0.8901 - accuracy: 0.72 - 106s 8ms/step - loss: 0.8903 - accuracy: 0.7240 - val_loss: 2.7256 - val_accuracy: 0.3360\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 0.8667 - accuracy: 0.75 - ETA: 1:29 - loss: 0.8660 - accuracy: 0.73 - ETA: 1:31 - loss: 0.8511 - accuracy: 0.72 - ETA: 1:29 - loss: 0.8433 - accuracy: 0.73 - ETA: 1:27 - loss: 0.8018 - accuracy: 0.74 - ETA: 1:25 - loss: 0.8611 - accuracy: 0.72 - ETA: 1:25 - loss: 0.8390 - accuracy: 0.72 - ETA: 1:23 - loss: 0.8073 - accuracy: 0.73 - ETA: 1:22 - loss: 0.8086 - accuracy: 0.73 - ETA: 1:22 - loss: 0.8052 - accuracy: 0.74 - ETA: 1:21 - loss: 0.8067 - accuracy: 0.74 - ETA: 1:20 - loss: 0.8162 - accuracy: 0.74 - ETA: 1:20 - loss: 0.8285 - accuracy: 0.73 - ETA: 1:19 - loss: 0.8381 - accuracy: 0.73 - ETA: 1:18 - loss: 0.8472 - accuracy: 0.73 - ETA: 1:17 - loss: 0.8392 - accuracy: 0.73 - ETA: 1:16 - loss: 0.8449 - accuracy: 0.73 - ETA: 1:15 - loss: 0.8551 - accuracy: 0.73 - ETA: 1:14 - loss: 0.8528 - accuracy: 0.73 - ETA: 1:13 - loss: 0.8599 - accuracy: 0.73 - ETA: 1:12 - loss: 0.8591 - accuracy: 0.73 - ETA: 1:12 - loss: 0.8697 - accuracy: 0.73 - ETA: 1:11 - loss: 0.8755 - accuracy: 0.72 - ETA: 1:10 - loss: 0.8750 - accuracy: 0.72 - ETA: 1:09 - loss: 0.8745 - accuracy: 0.72 - ETA: 1:08 - loss: 0.8824 - accuracy: 0.72 - ETA: 1:07 - loss: 0.8862 - accuracy: 0.72 - ETA: 1:06 - loss: 0.8905 - accuracy: 0.72 - ETA: 1:05 - loss: 0.8852 - accuracy: 0.72 - ETA: 1:04 - loss: 0.8795 - accuracy: 0.72 - ETA: 1:04 - loss: 0.8820 - accuracy: 0.72 - ETA: 1:03 - loss: 0.8805 - accuracy: 0.72 - ETA: 1:02 - loss: 0.8849 - accuracy: 0.72 - ETA: 1:01 - loss: 0.8860 - accuracy: 0.72 - ETA: 1:00 - loss: 0.8857 - accuracy: 0.72 - ETA: 59s - loss: 0.8839 - accuracy: 0.7259 - ETA: 58s - loss: 0.8754 - accuracy: 0.729 - ETA: 57s - loss: 0.8718 - accuracy: 0.731 - ETA: 56s - loss: 0.8693 - accuracy: 0.731 - ETA: 55s - loss: 0.8730 - accuracy: 0.730 - ETA: 54s - loss: 0.8767 - accuracy: 0.730 - ETA: 54s - loss: 0.8742 - accuracy: 0.731 - ETA: 53s - loss: 0.8737 - accuracy: 0.730 - ETA: 52s - loss: 0.8741 - accuracy: 0.730 - ETA: 51s - loss: 0.8717 - accuracy: 0.731 - ETA: 50s - loss: 0.8707 - accuracy: 0.730 - ETA: 49s - loss: 0.8670 - accuracy: 0.731 - ETA: 48s - loss: 0.8629 - accuracy: 0.733 - ETA: 47s - loss: 0.8657 - accuracy: 0.732 - ETA: 47s - loss: 0.8688 - accuracy: 0.731 - ETA: 46s - loss: 0.8706 - accuracy: 0.730 - ETA: 45s - loss: 0.8702 - accuracy: 0.729 - ETA: 44s - loss: 0.8705 - accuracy: 0.729 - ETA: 43s - loss: 0.8735 - accuracy: 0.728 - ETA: 42s - loss: 0.8743 - accuracy: 0.728 - ETA: 41s - loss: 0.8755 - accuracy: 0.728 - ETA: 40s - loss: 0.8717 - accuracy: 0.729 - ETA: 39s - loss: 0.8681 - accuracy: 0.730 - ETA: 38s - loss: 0.8687 - accuracy: 0.730 - ETA: 37s - loss: 0.8670 - accuracy: 0.730 - ETA: 36s - loss: 0.8663 - accuracy: 0.730 - ETA: 35s - loss: 0.8641 - accuracy: 0.731 - ETA: 35s - loss: 0.8654 - accuracy: 0.731 - ETA: 34s - loss: 0.8672 - accuracy: 0.730 - ETA: 33s - loss: 0.8661 - accuracy: 0.730 - ETA: 32s - loss: 0.8663 - accuracy: 0.730 - ETA: 31s - loss: 0.8681 - accuracy: 0.730 - ETA: 30s - loss: 0.8679 - accuracy: 0.730 - ETA: 29s - loss: 0.8700 - accuracy: 0.730 - ETA: 28s - loss: 0.8712 - accuracy: 0.729 - ETA: 27s - loss: 0.8710 - accuracy: 0.729 - ETA: 26s - loss: 0.8724 - accuracy: 0.729 - ETA: 26s - loss: 0.8694 - accuracy: 0.730 - ETA: 25s - loss: 0.8675 - accuracy: 0.730 - ETA: 24s - loss: 0.8651 - accuracy: 0.730 - ETA: 23s - loss: 0.8655 - accuracy: 0.730 - ETA: 22s - loss: 0.8673 - accuracy: 0.729 - ETA: 21s - loss: 0.8660 - accuracy: 0.730 - ETA: 20s - loss: 0.8644 - accuracy: 0.731 - ETA: 19s - loss: 0.8634 - accuracy: 0.731 - ETA: 18s - loss: 0.8624 - accuracy: 0.731 - ETA: 17s - loss: 0.8602 - accuracy: 0.732 - ETA: 16s - loss: 0.8592 - accuracy: 0.732 - ETA: 16s - loss: 0.8595 - accuracy: 0.732 - ETA: 15s - loss: 0.8598 - accuracy: 0.732 - ETA: 14s - loss: 0.8602 - accuracy: 0.732 - ETA: 13s - loss: 0.8610 - accuracy: 0.732 - ETA: 12s - loss: 0.8616 - accuracy: 0.732 - ETA: 11s - loss: 0.8601 - accuracy: 0.732 - ETA: 10s - loss: 0.8600 - accuracy: 0.733 - ETA: 9s - loss: 0.8584 - accuracy: 0.733 - ETA: 8s - loss: 0.8576 - accuracy: 0.73 - ETA: 7s - loss: 0.8588 - accuracy: 0.73 - ETA: 6s - loss: 0.8594 - accuracy: 0.73 - ETA: 6s - loss: 0.8615 - accuracy: 0.73 - ETA: 5s - loss: 0.8616 - accuracy: 0.73 - ETA: 4s - loss: 0.8625 - accuracy: 0.73 - ETA: 3s - loss: 0.8614 - accuracy: 0.73 - ETA: 2s - loss: 0.8614 - accuracy: 0.73 - ETA: 1s - loss: 0.8624 - accuracy: 0.73 - ETA: 0s - loss: 0.8619 - accuracy: 0.73 - 106s 8ms/step - loss: 0.8629 - accuracy: 0.7315 - val_loss: 2.8614 - val_accuracy: 0.3222\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 1.0715 - accuracy: 0.69 - ETA: 1:30 - loss: 0.9856 - accuracy: 0.70 - ETA: 1:31 - loss: 0.9032 - accuracy: 0.72 - ETA: 1:31 - loss: 0.8727 - accuracy: 0.72 - ETA: 1:29 - loss: 0.8562 - accuracy: 0.73 - ETA: 1:28 - loss: 0.8835 - accuracy: 0.71 - ETA: 1:26 - loss: 0.8738 - accuracy: 0.72 - ETA: 1:25 - loss: 0.9057 - accuracy: 0.72 - ETA: 1:24 - loss: 0.8993 - accuracy: 0.72 - ETA: 1:23 - loss: 0.9004 - accuracy: 0.72 - ETA: 1:22 - loss: 0.8838 - accuracy: 0.72 - ETA: 1:21 - loss: 0.8874 - accuracy: 0.72 - ETA: 1:20 - loss: 0.8942 - accuracy: 0.72 - ETA: 1:19 - loss: 0.8856 - accuracy: 0.72 - ETA: 1:18 - loss: 0.8756 - accuracy: 0.72 - ETA: 1:17 - loss: 0.8667 - accuracy: 0.73 - ETA: 1:16 - loss: 0.8517 - accuracy: 0.73 - ETA: 1:15 - loss: 0.8532 - accuracy: 0.73 - ETA: 1:14 - loss: 0.8523 - accuracy: 0.73 - ETA: 1:14 - loss: 0.8639 - accuracy: 0.73 - ETA: 1:13 - loss: 0.8547 - accuracy: 0.73 - ETA: 1:12 - loss: 0.8499 - accuracy: 0.74 - ETA: 1:11 - loss: 0.8511 - accuracy: 0.74 - ETA: 1:10 - loss: 0.8495 - accuracy: 0.74 - ETA: 1:09 - loss: 0.8556 - accuracy: 0.73 - ETA: 1:08 - loss: 0.8585 - accuracy: 0.73 - ETA: 1:07 - loss: 0.8500 - accuracy: 0.74 - ETA: 1:06 - loss: 0.8494 - accuracy: 0.74 - ETA: 1:05 - loss: 0.8569 - accuracy: 0.73 - ETA: 1:04 - loss: 0.8539 - accuracy: 0.73 - ETA: 1:04 - loss: 0.8471 - accuracy: 0.74 - ETA: 1:03 - loss: 0.8465 - accuracy: 0.74 - ETA: 1:02 - loss: 0.8564 - accuracy: 0.73 - ETA: 1:01 - loss: 0.8562 - accuracy: 0.73 - ETA: 1:00 - loss: 0.8513 - accuracy: 0.74 - ETA: 59s - loss: 0.8434 - accuracy: 0.7435 - ETA: 58s - loss: 0.8453 - accuracy: 0.743 - ETA: 57s - loss: 0.8457 - accuracy: 0.743 - ETA: 56s - loss: 0.8467 - accuracy: 0.743 - ETA: 55s - loss: 0.8500 - accuracy: 0.740 - ETA: 54s - loss: 0.8420 - accuracy: 0.742 - ETA: 54s - loss: 0.8405 - accuracy: 0.744 - ETA: 53s - loss: 0.8423 - accuracy: 0.744 - ETA: 52s - loss: 0.8414 - accuracy: 0.744 - ETA: 51s - loss: 0.8406 - accuracy: 0.744 - ETA: 50s - loss: 0.8403 - accuracy: 0.744 - ETA: 49s - loss: 0.8405 - accuracy: 0.744 - ETA: 48s - loss: 0.8384 - accuracy: 0.744 - ETA: 47s - loss: 0.8420 - accuracy: 0.743 - ETA: 46s - loss: 0.8454 - accuracy: 0.742 - ETA: 45s - loss: 0.8508 - accuracy: 0.741 - ETA: 44s - loss: 0.8523 - accuracy: 0.740 - ETA: 43s - loss: 0.8539 - accuracy: 0.739 - ETA: 43s - loss: 0.8529 - accuracy: 0.739 - ETA: 42s - loss: 0.8550 - accuracy: 0.738 - ETA: 41s - loss: 0.8569 - accuracy: 0.738 - ETA: 40s - loss: 0.8562 - accuracy: 0.738 - ETA: 39s - loss: 0.8591 - accuracy: 0.738 - ETA: 38s - loss: 0.8627 - accuracy: 0.737 - ETA: 37s - loss: 0.8606 - accuracy: 0.737 - ETA: 36s - loss: 0.8579 - accuracy: 0.738 - ETA: 35s - loss: 0.8592 - accuracy: 0.738 - ETA: 34s - loss: 0.8566 - accuracy: 0.739 - ETA: 34s - loss: 0.8550 - accuracy: 0.739 - ETA: 33s - loss: 0.8536 - accuracy: 0.739 - ETA: 32s - loss: 0.8519 - accuracy: 0.740 - ETA: 31s - loss: 0.8512 - accuracy: 0.740 - ETA: 30s - loss: 0.8538 - accuracy: 0.740 - ETA: 29s - loss: 0.8582 - accuracy: 0.739 - ETA: 28s - loss: 0.8572 - accuracy: 0.739 - ETA: 27s - loss: 0.8576 - accuracy: 0.740 - ETA: 26s - loss: 0.8595 - accuracy: 0.739 - ETA: 25s - loss: 0.8589 - accuracy: 0.739 - ETA: 25s - loss: 0.8615 - accuracy: 0.738 - ETA: 24s - loss: 0.8589 - accuracy: 0.739 - ETA: 23s - loss: 0.8614 - accuracy: 0.738 - ETA: 22s - loss: 0.8627 - accuracy: 0.737 - ETA: 21s - loss: 0.8607 - accuracy: 0.738 - ETA: 20s - loss: 0.8597 - accuracy: 0.738 - ETA: 19s - loss: 0.8599 - accuracy: 0.738 - ETA: 18s - loss: 0.8615 - accuracy: 0.737 - ETA: 17s - loss: 0.8623 - accuracy: 0.737 - ETA: 16s - loss: 0.8600 - accuracy: 0.737 - ETA: 16s - loss: 0.8586 - accuracy: 0.738 - ETA: 15s - loss: 0.8589 - accuracy: 0.738 - ETA: 14s - loss: 0.8600 - accuracy: 0.738 - ETA: 13s - loss: 0.8602 - accuracy: 0.738 - ETA: 12s - loss: 0.8602 - accuracy: 0.737 - ETA: 11s - loss: 0.8605 - accuracy: 0.737 - ETA: 10s - loss: 0.8610 - accuracy: 0.737 - ETA: 9s - loss: 0.8624 - accuracy: 0.737 - ETA: 8s - loss: 0.8632 - accuracy: 0.73 - ETA: 7s - loss: 0.8657 - accuracy: 0.73 - ETA: 7s - loss: 0.8644 - accuracy: 0.73 - ETA: 6s - loss: 0.8632 - accuracy: 0.73 - ETA: 5s - loss: 0.8625 - accuracy: 0.73 - ETA: 4s - loss: 0.8632 - accuracy: 0.73 - ETA: 3s - loss: 0.8623 - accuracy: 0.73 - ETA: 2s - loss: 0.8631 - accuracy: 0.73 - ETA: 1s - loss: 0.8634 - accuracy: 0.73 - ETA: 0s - loss: 0.8629 - accuracy: 0.73 - 106s 8ms/step - loss: 0.8632 - accuracy: 0.7355 - val_loss: 2.9429 - val_accuracy: 0.3157\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 0.8603 - accuracy: 0.76 - ETA: 1:31 - loss: 0.8117 - accuracy: 0.75 - ETA: 1:28 - loss: 0.8763 - accuracy: 0.74 - ETA: 1:27 - loss: 0.8570 - accuracy: 0.74 - ETA: 1:26 - loss: 0.8899 - accuracy: 0.73 - ETA: 1:25 - loss: 0.8662 - accuracy: 0.74 - ETA: 1:24 - loss: 0.8569 - accuracy: 0.74 - ETA: 1:23 - loss: 0.8519 - accuracy: 0.74 - ETA: 1:23 - loss: 0.8468 - accuracy: 0.74 - ETA: 1:22 - loss: 0.8315 - accuracy: 0.74 - ETA: 1:21 - loss: 0.8354 - accuracy: 0.73 - ETA: 1:20 - loss: 0.8293 - accuracy: 0.74 - ETA: 1:19 - loss: 0.8271 - accuracy: 0.74 - ETA: 1:18 - loss: 0.8449 - accuracy: 0.73 - ETA: 1:17 - loss: 0.8414 - accuracy: 0.73 - ETA: 1:16 - loss: 0.8426 - accuracy: 0.73 - ETA: 1:16 - loss: 0.8453 - accuracy: 0.73 - ETA: 1:15 - loss: 0.8454 - accuracy: 0.73 - ETA: 1:14 - loss: 0.8433 - accuracy: 0.73 - ETA: 1:13 - loss: 0.8492 - accuracy: 0.73 - ETA: 1:12 - loss: 0.8488 - accuracy: 0.73 - ETA: 1:11 - loss: 0.8486 - accuracy: 0.73 - ETA: 1:10 - loss: 0.8464 - accuracy: 0.73 - ETA: 1:09 - loss: 0.8423 - accuracy: 0.73 - ETA: 1:08 - loss: 0.8417 - accuracy: 0.73 - ETA: 1:08 - loss: 0.8380 - accuracy: 0.73 - ETA: 1:07 - loss: 0.8408 - accuracy: 0.73 - ETA: 1:06 - loss: 0.8378 - accuracy: 0.73 - ETA: 1:05 - loss: 0.8411 - accuracy: 0.73 - ETA: 1:04 - loss: 0.8481 - accuracy: 0.73 - ETA: 1:03 - loss: 0.8502 - accuracy: 0.73 - ETA: 1:03 - loss: 0.8500 - accuracy: 0.73 - ETA: 1:02 - loss: 0.8454 - accuracy: 0.73 - ETA: 1:01 - loss: 0.8409 - accuracy: 0.73 - ETA: 1:00 - loss: 0.8344 - accuracy: 0.73 - ETA: 59s - loss: 0.8309 - accuracy: 0.7391 - ETA: 58s - loss: 0.8246 - accuracy: 0.741 - ETA: 57s - loss: 0.8261 - accuracy: 0.740 - ETA: 57s - loss: 0.8256 - accuracy: 0.740 - ETA: 56s - loss: 0.8242 - accuracy: 0.741 - ETA: 55s - loss: 0.8225 - accuracy: 0.742 - ETA: 54s - loss: 0.8166 - accuracy: 0.743 - ETA: 53s - loss: 0.8143 - accuracy: 0.744 - ETA: 52s - loss: 0.8097 - accuracy: 0.745 - ETA: 51s - loss: 0.8121 - accuracy: 0.743 - ETA: 50s - loss: 0.8149 - accuracy: 0.743 - ETA: 49s - loss: 0.8136 - accuracy: 0.744 - ETA: 48s - loss: 0.8129 - accuracy: 0.743 - ETA: 47s - loss: 0.8133 - accuracy: 0.742 - ETA: 46s - loss: 0.8131 - accuracy: 0.743 - ETA: 46s - loss: 0.8171 - accuracy: 0.741 - ETA: 45s - loss: 0.8151 - accuracy: 0.742 - ETA: 44s - loss: 0.8212 - accuracy: 0.739 - ETA: 43s - loss: 0.8209 - accuracy: 0.740 - ETA: 42s - loss: 0.8237 - accuracy: 0.739 - ETA: 41s - loss: 0.8292 - accuracy: 0.738 - ETA: 40s - loss: 0.8298 - accuracy: 0.738 - ETA: 39s - loss: 0.8278 - accuracy: 0.738 - ETA: 38s - loss: 0.8299 - accuracy: 0.737 - ETA: 37s - loss: 0.8310 - accuracy: 0.736 - ETA: 37s - loss: 0.8323 - accuracy: 0.735 - ETA: 36s - loss: 0.8352 - accuracy: 0.736 - ETA: 35s - loss: 0.8349 - accuracy: 0.736 - ETA: 34s - loss: 0.8350 - accuracy: 0.736 - ETA: 33s - loss: 0.8356 - accuracy: 0.737 - ETA: 32s - loss: 0.8355 - accuracy: 0.737 - ETA: 31s - loss: 0.8374 - accuracy: 0.736 - ETA: 30s - loss: 0.8371 - accuracy: 0.736 - ETA: 29s - loss: 0.8405 - accuracy: 0.736 - ETA: 28s - loss: 0.8418 - accuracy: 0.736 - ETA: 27s - loss: 0.8411 - accuracy: 0.736 - ETA: 27s - loss: 0.8412 - accuracy: 0.736 - ETA: 26s - loss: 0.8417 - accuracy: 0.736 - ETA: 25s - loss: 0.8421 - accuracy: 0.736 - ETA: 24s - loss: 0.8444 - accuracy: 0.735 - ETA: 23s - loss: 0.8416 - accuracy: 0.736 - ETA: 22s - loss: 0.8428 - accuracy: 0.735 - ETA: 21s - loss: 0.8430 - accuracy: 0.736 - ETA: 20s - loss: 0.8442 - accuracy: 0.735 - ETA: 19s - loss: 0.8437 - accuracy: 0.735 - ETA: 18s - loss: 0.8455 - accuracy: 0.735 - ETA: 17s - loss: 0.8478 - accuracy: 0.734 - ETA: 17s - loss: 0.8495 - accuracy: 0.733 - ETA: 16s - loss: 0.8477 - accuracy: 0.734 - ETA: 15s - loss: 0.8493 - accuracy: 0.734 - ETA: 14s - loss: 0.8495 - accuracy: 0.733 - ETA: 13s - loss: 0.8500 - accuracy: 0.734 - ETA: 12s - loss: 0.8494 - accuracy: 0.734 - ETA: 11s - loss: 0.8519 - accuracy: 0.733 - ETA: 10s - loss: 0.8532 - accuracy: 0.733 - ETA: 9s - loss: 0.8528 - accuracy: 0.733 - ETA: 8s - loss: 0.8492 - accuracy: 0.73 - ETA: 7s - loss: 0.8491 - accuracy: 0.73 - ETA: 7s - loss: 0.8513 - accuracy: 0.73 - ETA: 6s - loss: 0.8526 - accuracy: 0.73 - ETA: 5s - loss: 0.8539 - accuracy: 0.73 - ETA: 4s - loss: 0.8547 - accuracy: 0.73 - ETA: 3s - loss: 0.8530 - accuracy: 0.73 - ETA: 2s - loss: 0.8515 - accuracy: 0.73 - ETA: 1s - loss: 0.8540 - accuracy: 0.73 - ETA: 0s - loss: 0.8539 - accuracy: 0.73 - 106s 8ms/step - loss: 0.8551 - accuracy: 0.7329 - val_loss: 2.8278 - val_accuracy: 0.3431\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:28 - loss: 1.0265 - accuracy: 0.67 - ETA: 1:26 - loss: 0.9235 - accuracy: 0.70 - ETA: 1:26 - loss: 0.7864 - accuracy: 0.75 - ETA: 1:25 - loss: 0.7888 - accuracy: 0.75 - ETA: 1:24 - loss: 0.7851 - accuracy: 0.75 - ETA: 1:23 - loss: 0.8161 - accuracy: 0.75 - ETA: 1:22 - loss: 0.8220 - accuracy: 0.74 - ETA: 1:22 - loss: 0.8501 - accuracy: 0.74 - ETA: 1:23 - loss: 0.8468 - accuracy: 0.74 - ETA: 1:22 - loss: 0.8350 - accuracy: 0.74 - ETA: 1:22 - loss: 0.8503 - accuracy: 0.73 - ETA: 1:22 - loss: 0.8478 - accuracy: 0.73 - ETA: 1:21 - loss: 0.8686 - accuracy: 0.73 - ETA: 1:21 - loss: 0.8751 - accuracy: 0.73 - ETA: 1:20 - loss: 0.8701 - accuracy: 0.73 - ETA: 1:19 - loss: 0.8759 - accuracy: 0.73 - ETA: 1:18 - loss: 0.8700 - accuracy: 0.73 - ETA: 1:18 - loss: 0.8636 - accuracy: 0.73 - ETA: 1:17 - loss: 0.8640 - accuracy: 0.73 - ETA: 1:16 - loss: 0.8687 - accuracy: 0.73 - ETA: 1:15 - loss: 0.8642 - accuracy: 0.73 - ETA: 1:14 - loss: 0.8628 - accuracy: 0.73 - ETA: 1:13 - loss: 0.8601 - accuracy: 0.73 - ETA: 1:12 - loss: 0.8569 - accuracy: 0.73 - ETA: 1:11 - loss: 0.8522 - accuracy: 0.73 - ETA: 1:10 - loss: 0.8579 - accuracy: 0.73 - ETA: 1:08 - loss: 0.8546 - accuracy: 0.73 - ETA: 1:07 - loss: 0.8510 - accuracy: 0.73 - ETA: 1:06 - loss: 0.8448 - accuracy: 0.73 - ETA: 1:05 - loss: 0.8480 - accuracy: 0.73 - ETA: 1:05 - loss: 0.8508 - accuracy: 0.73 - ETA: 1:04 - loss: 0.8533 - accuracy: 0.73 - ETA: 1:03 - loss: 0.8562 - accuracy: 0.73 - ETA: 1:02 - loss: 0.8536 - accuracy: 0.73 - ETA: 1:01 - loss: 0.8548 - accuracy: 0.73 - ETA: 1:00 - loss: 0.8590 - accuracy: 0.73 - ETA: 59s - loss: 0.8605 - accuracy: 0.7361 - ETA: 58s - loss: 0.8600 - accuracy: 0.736 - ETA: 57s - loss: 0.8637 - accuracy: 0.734 - ETA: 56s - loss: 0.8644 - accuracy: 0.734 - ETA: 55s - loss: 0.8642 - accuracy: 0.733 - ETA: 54s - loss: 0.8637 - accuracy: 0.734 - ETA: 53s - loss: 0.8669 - accuracy: 0.733 - ETA: 52s - loss: 0.8664 - accuracy: 0.732 - ETA: 51s - loss: 0.8649 - accuracy: 0.732 - ETA: 50s - loss: 0.8710 - accuracy: 0.731 - ETA: 50s - loss: 0.8693 - accuracy: 0.731 - ETA: 49s - loss: 0.8666 - accuracy: 0.733 - ETA: 48s - loss: 0.8647 - accuracy: 0.733 - ETA: 47s - loss: 0.8650 - accuracy: 0.733 - ETA: 46s - loss: 0.8669 - accuracy: 0.732 - ETA: 45s - loss: 0.8650 - accuracy: 0.733 - ETA: 44s - loss: 0.8662 - accuracy: 0.733 - ETA: 43s - loss: 0.8636 - accuracy: 0.734 - ETA: 42s - loss: 0.8614 - accuracy: 0.734 - ETA: 41s - loss: 0.8616 - accuracy: 0.735 - ETA: 40s - loss: 0.8612 - accuracy: 0.736 - ETA: 39s - loss: 0.8636 - accuracy: 0.735 - ETA: 38s - loss: 0.8650 - accuracy: 0.735 - ETA: 38s - loss: 0.8630 - accuracy: 0.735 - ETA: 37s - loss: 0.8629 - accuracy: 0.735 - ETA: 36s - loss: 0.8644 - accuracy: 0.735 - ETA: 35s - loss: 0.8617 - accuracy: 0.736 - ETA: 34s - loss: 0.8606 - accuracy: 0.735 - ETA: 33s - loss: 0.8603 - accuracy: 0.735 - ETA: 32s - loss: 0.8613 - accuracy: 0.735 - ETA: 31s - loss: 0.8627 - accuracy: 0.735 - ETA: 30s - loss: 0.8640 - accuracy: 0.735 - ETA: 29s - loss: 0.8636 - accuracy: 0.735 - ETA: 29s - loss: 0.8642 - accuracy: 0.735 - ETA: 28s - loss: 0.8608 - accuracy: 0.736 - ETA: 27s - loss: 0.8589 - accuracy: 0.736 - ETA: 26s - loss: 0.8596 - accuracy: 0.735 - ETA: 25s - loss: 0.8579 - accuracy: 0.736 - ETA: 24s - loss: 0.8581 - accuracy: 0.735 - ETA: 23s - loss: 0.8597 - accuracy: 0.734 - ETA: 22s - loss: 0.8599 - accuracy: 0.734 - ETA: 21s - loss: 0.8585 - accuracy: 0.734 - ETA: 20s - loss: 0.8609 - accuracy: 0.734 - ETA: 19s - loss: 0.8583 - accuracy: 0.735 - ETA: 18s - loss: 0.8588 - accuracy: 0.735 - ETA: 17s - loss: 0.8579 - accuracy: 0.735 - ETA: 17s - loss: 0.8576 - accuracy: 0.735 - ETA: 16s - loss: 0.8566 - accuracy: 0.735 - ETA: 15s - loss: 0.8579 - accuracy: 0.735 - ETA: 14s - loss: 0.8567 - accuracy: 0.736 - ETA: 13s - loss: 0.8578 - accuracy: 0.735 - ETA: 12s - loss: 0.8590 - accuracy: 0.735 - ETA: 11s - loss: 0.8596 - accuracy: 0.734 - ETA: 10s - loss: 0.8592 - accuracy: 0.734 - ETA: 9s - loss: 0.8605 - accuracy: 0.734 - ETA: 8s - loss: 0.8600 - accuracy: 0.73 - ETA: 7s - loss: 0.8597 - accuracy: 0.73 - ETA: 7s - loss: 0.8626 - accuracy: 0.73 - ETA: 6s - loss: 0.8612 - accuracy: 0.73 - ETA: 5s - loss: 0.8610 - accuracy: 0.73 - ETA: 4s - loss: 0.8619 - accuracy: 0.73 - ETA: 3s - loss: 0.8618 - accuracy: 0.73 - ETA: 2s - loss: 0.8615 - accuracy: 0.73 - ETA: 1s - loss: 0.8607 - accuracy: 0.73 - ETA: 0s - loss: 0.8609 - accuracy: 0.73 - 106s 8ms/step - loss: 0.8595 - accuracy: 0.7334 - val_loss: 2.8602 - val_accuracy: 0.3347\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 0.5870 - accuracy: 0.79 - ETA: 1:32 - loss: 0.7267 - accuracy: 0.74 - ETA: 1:32 - loss: 0.7163 - accuracy: 0.76 - ETA: 1:30 - loss: 0.8181 - accuracy: 0.73 - ETA: 1:30 - loss: 0.7871 - accuracy: 0.74 - ETA: 1:28 - loss: 0.7726 - accuracy: 0.75 - ETA: 1:27 - loss: 0.7968 - accuracy: 0.74 - ETA: 1:26 - loss: 0.7907 - accuracy: 0.74 - ETA: 1:25 - loss: 0.7992 - accuracy: 0.74 - ETA: 1:24 - loss: 0.8096 - accuracy: 0.74 - ETA: 1:23 - loss: 0.8066 - accuracy: 0.74 - ETA: 1:22 - loss: 0.8007 - accuracy: 0.75 - ETA: 1:21 - loss: 0.8125 - accuracy: 0.74 - ETA: 1:20 - loss: 0.8179 - accuracy: 0.74 - ETA: 1:19 - loss: 0.8237 - accuracy: 0.74 - ETA: 1:18 - loss: 0.8308 - accuracy: 0.74 - ETA: 1:17 - loss: 0.8345 - accuracy: 0.74 - ETA: 1:17 - loss: 0.8266 - accuracy: 0.74 - ETA: 1:16 - loss: 0.8352 - accuracy: 0.74 - ETA: 1:15 - loss: 0.8362 - accuracy: 0.74 - ETA: 1:14 - loss: 0.8254 - accuracy: 0.74 - ETA: 1:14 - loss: 0.8164 - accuracy: 0.74 - ETA: 1:13 - loss: 0.8213 - accuracy: 0.74 - ETA: 1:12 - loss: 0.8191 - accuracy: 0.74 - ETA: 1:11 - loss: 0.8233 - accuracy: 0.74 - ETA: 1:10 - loss: 0.8175 - accuracy: 0.74 - ETA: 1:09 - loss: 0.8123 - accuracy: 0.74 - ETA: 1:08 - loss: 0.8170 - accuracy: 0.74 - ETA: 1:07 - loss: 0.8226 - accuracy: 0.74 - ETA: 1:06 - loss: 0.8274 - accuracy: 0.73 - ETA: 1:05 - loss: 0.8257 - accuracy: 0.73 - ETA: 1:04 - loss: 0.8205 - accuracy: 0.74 - ETA: 1:03 - loss: 0.8213 - accuracy: 0.74 - ETA: 1:02 - loss: 0.8276 - accuracy: 0.73 - ETA: 1:01 - loss: 0.8327 - accuracy: 0.73 - ETA: 1:00 - loss: 0.8299 - accuracy: 0.73 - ETA: 59s - loss: 0.8336 - accuracy: 0.7367 - ETA: 58s - loss: 0.8334 - accuracy: 0.735 - ETA: 57s - loss: 0.8362 - accuracy: 0.734 - ETA: 56s - loss: 0.8432 - accuracy: 0.732 - ETA: 55s - loss: 0.8395 - accuracy: 0.733 - ETA: 54s - loss: 0.8340 - accuracy: 0.735 - ETA: 53s - loss: 0.8317 - accuracy: 0.736 - ETA: 52s - loss: 0.8324 - accuracy: 0.736 - ETA: 51s - loss: 0.8319 - accuracy: 0.735 - ETA: 50s - loss: 0.8340 - accuracy: 0.735 - ETA: 50s - loss: 0.8342 - accuracy: 0.736 - ETA: 49s - loss: 0.8336 - accuracy: 0.736 - ETA: 48s - loss: 0.8354 - accuracy: 0.735 - ETA: 47s - loss: 0.8401 - accuracy: 0.734 - ETA: 46s - loss: 0.8457 - accuracy: 0.732 - ETA: 45s - loss: 0.8414 - accuracy: 0.734 - ETA: 44s - loss: 0.8403 - accuracy: 0.734 - ETA: 43s - loss: 0.8411 - accuracy: 0.734 - ETA: 42s - loss: 0.8473 - accuracy: 0.733 - ETA: 41s - loss: 0.8452 - accuracy: 0.733 - ETA: 40s - loss: 0.8439 - accuracy: 0.734 - ETA: 39s - loss: 0.8477 - accuracy: 0.733 - ETA: 38s - loss: 0.8463 - accuracy: 0.733 - ETA: 38s - loss: 0.8449 - accuracy: 0.734 - ETA: 37s - loss: 0.8463 - accuracy: 0.734 - ETA: 36s - loss: 0.8463 - accuracy: 0.734 - ETA: 35s - loss: 0.8470 - accuracy: 0.733 - ETA: 34s - loss: 0.8445 - accuracy: 0.734 - ETA: 33s - loss: 0.8416 - accuracy: 0.734 - ETA: 32s - loss: 0.8387 - accuracy: 0.735 - ETA: 31s - loss: 0.8406 - accuracy: 0.735 - ETA: 30s - loss: 0.8413 - accuracy: 0.734 - ETA: 29s - loss: 0.8408 - accuracy: 0.734 - ETA: 28s - loss: 0.8406 - accuracy: 0.734 - ETA: 27s - loss: 0.8415 - accuracy: 0.733 - ETA: 27s - loss: 0.8434 - accuracy: 0.733 - ETA: 26s - loss: 0.8403 - accuracy: 0.734 - ETA: 25s - loss: 0.8403 - accuracy: 0.734 - ETA: 24s - loss: 0.8415 - accuracy: 0.734 - ETA: 23s - loss: 0.8423 - accuracy: 0.734 - ETA: 22s - loss: 0.8437 - accuracy: 0.734 - ETA: 21s - loss: 0.8468 - accuracy: 0.733 - ETA: 20s - loss: 0.8467 - accuracy: 0.733 - ETA: 19s - loss: 0.8447 - accuracy: 0.734 - ETA: 18s - loss: 0.8447 - accuracy: 0.734 - ETA: 17s - loss: 0.8473 - accuracy: 0.733 - ETA: 16s - loss: 0.8486 - accuracy: 0.733 - ETA: 16s - loss: 0.8477 - accuracy: 0.733 - ETA: 15s - loss: 0.8516 - accuracy: 0.732 - ETA: 14s - loss: 0.8504 - accuracy: 0.732 - ETA: 13s - loss: 0.8501 - accuracy: 0.732 - ETA: 12s - loss: 0.8497 - accuracy: 0.733 - ETA: 11s - loss: 0.8470 - accuracy: 0.733 - ETA: 10s - loss: 0.8485 - accuracy: 0.733 - ETA: 9s - loss: 0.8476 - accuracy: 0.733 - ETA: 8s - loss: 0.8479 - accuracy: 0.73 - ETA: 7s - loss: 0.8495 - accuracy: 0.73 - ETA: 7s - loss: 0.8495 - accuracy: 0.73 - ETA: 6s - loss: 0.8503 - accuracy: 0.73 - ETA: 5s - loss: 0.8508 - accuracy: 0.73 - ETA: 4s - loss: 0.8530 - accuracy: 0.73 - ETA: 3s - loss: 0.8560 - accuracy: 0.73 - ETA: 2s - loss: 0.8555 - accuracy: 0.73 - ETA: 1s - loss: 0.8562 - accuracy: 0.73 - ETA: 0s - loss: 0.8566 - accuracy: 0.73 - 106s 8ms/step - loss: 0.8561 - accuracy: 0.7312 - val_loss: 2.8367 - val_accuracy: 0.3247\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.9068 - accuracy: 0.72 - ETA: 1:30 - loss: 0.9253 - accuracy: 0.71 - ETA: 1:28 - loss: 0.8553 - accuracy: 0.72 - ETA: 1:27 - loss: 0.8074 - accuracy: 0.73 - ETA: 1:26 - loss: 0.7568 - accuracy: 0.74 - ETA: 1:25 - loss: 0.7884 - accuracy: 0.74 - ETA: 1:23 - loss: 0.7981 - accuracy: 0.74 - ETA: 1:23 - loss: 0.7930 - accuracy: 0.74 - ETA: 1:22 - loss: 0.8046 - accuracy: 0.74 - ETA: 1:22 - loss: 0.7946 - accuracy: 0.74 - ETA: 1:21 - loss: 0.8301 - accuracy: 0.74 - ETA: 1:20 - loss: 0.8325 - accuracy: 0.74 - ETA: 1:20 - loss: 0.8473 - accuracy: 0.74 - ETA: 1:18 - loss: 0.8503 - accuracy: 0.73 - ETA: 1:17 - loss: 0.8429 - accuracy: 0.74 - ETA: 1:16 - loss: 0.8560 - accuracy: 0.73 - ETA: 1:15 - loss: 0.8415 - accuracy: 0.74 - ETA: 1:14 - loss: 0.8482 - accuracy: 0.74 - ETA: 1:14 - loss: 0.8441 - accuracy: 0.74 - ETA: 1:13 - loss: 0.8505 - accuracy: 0.74 - ETA: 1:12 - loss: 0.8592 - accuracy: 0.73 - ETA: 1:11 - loss: 0.8514 - accuracy: 0.73 - ETA: 1:10 - loss: 0.8425 - accuracy: 0.74 - ETA: 1:09 - loss: 0.8505 - accuracy: 0.73 - ETA: 1:08 - loss: 0.8514 - accuracy: 0.73 - ETA: 1:07 - loss: 0.8474 - accuracy: 0.73 - ETA: 1:06 - loss: 0.8449 - accuracy: 0.73 - ETA: 1:06 - loss: 0.8500 - accuracy: 0.73 - ETA: 1:05 - loss: 0.8428 - accuracy: 0.73 - ETA: 1:04 - loss: 0.8379 - accuracy: 0.74 - ETA: 1:03 - loss: 0.8334 - accuracy: 0.74 - ETA: 1:02 - loss: 0.8331 - accuracy: 0.74 - ETA: 1:01 - loss: 0.8301 - accuracy: 0.74 - ETA: 1:00 - loss: 0.8260 - accuracy: 0.74 - ETA: 59s - loss: 0.8281 - accuracy: 0.7411 - ETA: 59s - loss: 0.8296 - accuracy: 0.741 - ETA: 58s - loss: 0.8260 - accuracy: 0.742 - ETA: 57s - loss: 0.8250 - accuracy: 0.742 - ETA: 56s - loss: 0.8237 - accuracy: 0.743 - ETA: 55s - loss: 0.8207 - accuracy: 0.744 - ETA: 54s - loss: 0.8232 - accuracy: 0.743 - ETA: 53s - loss: 0.8205 - accuracy: 0.744 - ETA: 52s - loss: 0.8214 - accuracy: 0.743 - ETA: 51s - loss: 0.8252 - accuracy: 0.742 - ETA: 51s - loss: 0.8254 - accuracy: 0.742 - ETA: 50s - loss: 0.8259 - accuracy: 0.741 - ETA: 49s - loss: 0.8299 - accuracy: 0.740 - ETA: 48s - loss: 0.8320 - accuracy: 0.740 - ETA: 47s - loss: 0.8304 - accuracy: 0.740 - ETA: 46s - loss: 0.8347 - accuracy: 0.740 - ETA: 45s - loss: 0.8349 - accuracy: 0.739 - ETA: 44s - loss: 0.8364 - accuracy: 0.739 - ETA: 44s - loss: 0.8363 - accuracy: 0.740 - ETA: 43s - loss: 0.8397 - accuracy: 0.739 - ETA: 42s - loss: 0.8462 - accuracy: 0.737 - ETA: 41s - loss: 0.8443 - accuracy: 0.738 - ETA: 40s - loss: 0.8401 - accuracy: 0.738 - ETA: 39s - loss: 0.8396 - accuracy: 0.738 - ETA: 38s - loss: 0.8373 - accuracy: 0.738 - ETA: 37s - loss: 0.8350 - accuracy: 0.739 - ETA: 36s - loss: 0.8396 - accuracy: 0.738 - ETA: 35s - loss: 0.8353 - accuracy: 0.740 - ETA: 34s - loss: 0.8374 - accuracy: 0.739 - ETA: 34s - loss: 0.8374 - accuracy: 0.739 - ETA: 33s - loss: 0.8379 - accuracy: 0.739 - ETA: 32s - loss: 0.8378 - accuracy: 0.739 - ETA: 31s - loss: 0.8374 - accuracy: 0.739 - ETA: 30s - loss: 0.8374 - accuracy: 0.739 - ETA: 29s - loss: 0.8358 - accuracy: 0.740 - ETA: 28s - loss: 0.8349 - accuracy: 0.740 - ETA: 27s - loss: 0.8340 - accuracy: 0.740 - ETA: 26s - loss: 0.8377 - accuracy: 0.739 - ETA: 25s - loss: 0.8388 - accuracy: 0.739 - ETA: 24s - loss: 0.8381 - accuracy: 0.739 - ETA: 24s - loss: 0.8403 - accuracy: 0.739 - ETA: 23s - loss: 0.8433 - accuracy: 0.738 - ETA: 22s - loss: 0.8440 - accuracy: 0.738 - ETA: 21s - loss: 0.8412 - accuracy: 0.739 - ETA: 20s - loss: 0.8419 - accuracy: 0.739 - ETA: 19s - loss: 0.8430 - accuracy: 0.738 - ETA: 18s - loss: 0.8431 - accuracy: 0.739 - ETA: 17s - loss: 0.8420 - accuracy: 0.739 - ETA: 16s - loss: 0.8411 - accuracy: 0.739 - ETA: 16s - loss: 0.8386 - accuracy: 0.740 - ETA: 15s - loss: 0.8374 - accuracy: 0.740 - ETA: 14s - loss: 0.8351 - accuracy: 0.741 - ETA: 13s - loss: 0.8346 - accuracy: 0.741 - ETA: 12s - loss: 0.8348 - accuracy: 0.740 - ETA: 11s - loss: 0.8334 - accuracy: 0.741 - ETA: 10s - loss: 0.8346 - accuracy: 0.740 - ETA: 9s - loss: 0.8344 - accuracy: 0.741 - ETA: 8s - loss: 0.8340 - accuracy: 0.74 - ETA: 7s - loss: 0.8329 - accuracy: 0.74 - ETA: 6s - loss: 0.8347 - accuracy: 0.74 - ETA: 6s - loss: 0.8350 - accuracy: 0.74 - ETA: 5s - loss: 0.8345 - accuracy: 0.73 - ETA: 4s - loss: 0.8333 - accuracy: 0.74 - ETA: 3s - loss: 0.8316 - accuracy: 0.74 - ETA: 2s - loss: 0.8301 - accuracy: 0.74 - ETA: 1s - loss: 0.8313 - accuracy: 0.74 - ETA: 0s - loss: 0.8292 - accuracy: 0.74 - 105s 8ms/step - loss: 0.8307 - accuracy: 0.7412 - val_loss: 2.8579 - val_accuracy: 0.3569\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:41 - loss: 0.7904 - accuracy: 0.75 - ETA: 1:35 - loss: 0.8750 - accuracy: 0.74 - ETA: 1:32 - loss: 0.8832 - accuracy: 0.74 - ETA: 1:31 - loss: 0.8109 - accuracy: 0.75 - ETA: 1:31 - loss: 0.8266 - accuracy: 0.75 - ETA: 1:29 - loss: 0.8038 - accuracy: 0.75 - ETA: 1:27 - loss: 0.7961 - accuracy: 0.76 - ETA: 1:26 - loss: 0.7917 - accuracy: 0.76 - ETA: 1:24 - loss: 0.8191 - accuracy: 0.75 - ETA: 1:23 - loss: 0.8034 - accuracy: 0.75 - ETA: 1:22 - loss: 0.8081 - accuracy: 0.75 - ETA: 1:21 - loss: 0.8185 - accuracy: 0.75 - ETA: 1:20 - loss: 0.8156 - accuracy: 0.75 - ETA: 1:19 - loss: 0.8225 - accuracy: 0.75 - ETA: 1:18 - loss: 0.8330 - accuracy: 0.75 - ETA: 1:17 - loss: 0.8402 - accuracy: 0.75 - ETA: 1:16 - loss: 0.8456 - accuracy: 0.74 - ETA: 1:15 - loss: 0.8379 - accuracy: 0.75 - ETA: 1:14 - loss: 0.8256 - accuracy: 0.75 - ETA: 1:13 - loss: 0.8346 - accuracy: 0.74 - ETA: 1:13 - loss: 0.8346 - accuracy: 0.74 - ETA: 1:12 - loss: 0.8399 - accuracy: 0.74 - ETA: 1:11 - loss: 0.8422 - accuracy: 0.74 - ETA: 1:10 - loss: 0.8490 - accuracy: 0.74 - ETA: 1:09 - loss: 0.8481 - accuracy: 0.74 - ETA: 1:08 - loss: 0.8477 - accuracy: 0.74 - ETA: 1:07 - loss: 0.8434 - accuracy: 0.74 - ETA: 1:06 - loss: 0.8426 - accuracy: 0.74 - ETA: 1:05 - loss: 0.8456 - accuracy: 0.73 - ETA: 1:04 - loss: 0.8444 - accuracy: 0.73 - ETA: 1:03 - loss: 0.8448 - accuracy: 0.73 - ETA: 1:03 - loss: 0.8468 - accuracy: 0.73 - ETA: 1:02 - loss: 0.8483 - accuracy: 0.73 - ETA: 1:01 - loss: 0.8518 - accuracy: 0.73 - ETA: 1:00 - loss: 0.8575 - accuracy: 0.73 - ETA: 59s - loss: 0.8562 - accuracy: 0.7344 - ETA: 58s - loss: 0.8564 - accuracy: 0.735 - ETA: 57s - loss: 0.8551 - accuracy: 0.735 - ETA: 56s - loss: 0.8531 - accuracy: 0.736 - ETA: 55s - loss: 0.8552 - accuracy: 0.736 - ETA: 54s - loss: 0.8556 - accuracy: 0.736 - ETA: 53s - loss: 0.8550 - accuracy: 0.735 - ETA: 52s - loss: 0.8575 - accuracy: 0.734 - ETA: 51s - loss: 0.8528 - accuracy: 0.735 - ETA: 51s - loss: 0.8522 - accuracy: 0.735 - ETA: 50s - loss: 0.8502 - accuracy: 0.734 - ETA: 49s - loss: 0.8479 - accuracy: 0.736 - ETA: 48s - loss: 0.8493 - accuracy: 0.735 - ETA: 47s - loss: 0.8485 - accuracy: 0.735 - ETA: 46s - loss: 0.8461 - accuracy: 0.735 - ETA: 45s - loss: 0.8464 - accuracy: 0.735 - ETA: 44s - loss: 0.8447 - accuracy: 0.736 - ETA: 43s - loss: 0.8429 - accuracy: 0.736 - ETA: 42s - loss: 0.8432 - accuracy: 0.736 - ETA: 42s - loss: 0.8452 - accuracy: 0.735 - ETA: 41s - loss: 0.8474 - accuracy: 0.735 - ETA: 40s - loss: 0.8465 - accuracy: 0.736 - ETA: 39s - loss: 0.8450 - accuracy: 0.737 - ETA: 38s - loss: 0.8472 - accuracy: 0.736 - ETA: 37s - loss: 0.8477 - accuracy: 0.736 - ETA: 36s - loss: 0.8469 - accuracy: 0.736 - ETA: 35s - loss: 0.8449 - accuracy: 0.735 - ETA: 34s - loss: 0.8431 - accuracy: 0.736 - ETA: 33s - loss: 0.8450 - accuracy: 0.736 - ETA: 32s - loss: 0.8449 - accuracy: 0.735 - ETA: 32s - loss: 0.8442 - accuracy: 0.736 - ETA: 31s - loss: 0.8445 - accuracy: 0.736 - ETA: 30s - loss: 0.8436 - accuracy: 0.736 - ETA: 29s - loss: 0.8419 - accuracy: 0.737 - ETA: 28s - loss: 0.8395 - accuracy: 0.738 - ETA: 27s - loss: 0.8400 - accuracy: 0.737 - ETA: 26s - loss: 0.8408 - accuracy: 0.737 - ETA: 25s - loss: 0.8387 - accuracy: 0.738 - ETA: 24s - loss: 0.8368 - accuracy: 0.738 - ETA: 24s - loss: 0.8359 - accuracy: 0.739 - ETA: 23s - loss: 0.8330 - accuracy: 0.740 - ETA: 22s - loss: 0.8330 - accuracy: 0.740 - ETA: 21s - loss: 0.8322 - accuracy: 0.740 - ETA: 20s - loss: 0.8312 - accuracy: 0.740 - ETA: 19s - loss: 0.8295 - accuracy: 0.740 - ETA: 18s - loss: 0.8282 - accuracy: 0.741 - ETA: 17s - loss: 0.8298 - accuracy: 0.741 - ETA: 16s - loss: 0.8315 - accuracy: 0.740 - ETA: 15s - loss: 0.8327 - accuracy: 0.740 - ETA: 15s - loss: 0.8322 - accuracy: 0.740 - ETA: 14s - loss: 0.8337 - accuracy: 0.740 - ETA: 13s - loss: 0.8380 - accuracy: 0.739 - ETA: 12s - loss: 0.8376 - accuracy: 0.739 - ETA: 11s - loss: 0.8351 - accuracy: 0.740 - ETA: 10s - loss: 0.8378 - accuracy: 0.739 - ETA: 9s - loss: 0.8381 - accuracy: 0.739 - ETA: 8s - loss: 0.8388 - accuracy: 0.73 - ETA: 7s - loss: 0.8364 - accuracy: 0.73 - ETA: 6s - loss: 0.8337 - accuracy: 0.73 - ETA: 6s - loss: 0.8335 - accuracy: 0.73 - ETA: 5s - loss: 0.8312 - accuracy: 0.74 - ETA: 4s - loss: 0.8328 - accuracy: 0.73 - ETA: 3s - loss: 0.8335 - accuracy: 0.73 - ETA: 2s - loss: 0.8337 - accuracy: 0.73 - ETA: 1s - loss: 0.8331 - accuracy: 0.73 - ETA: 0s - loss: 0.8312 - accuracy: 0.73 - 105s 8ms/step - loss: 0.8299 - accuracy: 0.7402 - val_loss: 2.8095 - val_accuracy: 0.3581\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:34 - loss: 0.6949 - accuracy: 0.77 - ETA: 1:31 - loss: 0.7662 - accuracy: 0.74 - ETA: 1:28 - loss: 0.8261 - accuracy: 0.74 - ETA: 1:28 - loss: 0.8200 - accuracy: 0.74 - ETA: 1:27 - loss: 0.8240 - accuracy: 0.74 - ETA: 1:26 - loss: 0.8351 - accuracy: 0.74 - ETA: 1:25 - loss: 0.8327 - accuracy: 0.74 - ETA: 1:25 - loss: 0.8317 - accuracy: 0.74 - ETA: 1:25 - loss: 0.8446 - accuracy: 0.74 - ETA: 1:24 - loss: 0.8305 - accuracy: 0.75 - ETA: 1:22 - loss: 0.8241 - accuracy: 0.75 - ETA: 1:21 - loss: 0.8049 - accuracy: 0.75 - ETA: 1:20 - loss: 0.8206 - accuracy: 0.75 - ETA: 1:19 - loss: 0.8313 - accuracy: 0.74 - ETA: 1:18 - loss: 0.8465 - accuracy: 0.74 - ETA: 1:17 - loss: 0.8595 - accuracy: 0.73 - ETA: 1:16 - loss: 0.8482 - accuracy: 0.74 - ETA: 1:15 - loss: 0.8532 - accuracy: 0.74 - ETA: 1:14 - loss: 0.8476 - accuracy: 0.74 - ETA: 1:13 - loss: 0.8405 - accuracy: 0.74 - ETA: 1:12 - loss: 0.8562 - accuracy: 0.73 - ETA: 1:11 - loss: 0.8466 - accuracy: 0.73 - ETA: 1:10 - loss: 0.8495 - accuracy: 0.73 - ETA: 1:10 - loss: 0.8488 - accuracy: 0.73 - ETA: 1:09 - loss: 0.8402 - accuracy: 0.73 - ETA: 1:08 - loss: 0.8460 - accuracy: 0.73 - ETA: 1:07 - loss: 0.8478 - accuracy: 0.73 - ETA: 1:06 - loss: 0.8447 - accuracy: 0.73 - ETA: 1:05 - loss: 0.8442 - accuracy: 0.73 - ETA: 1:05 - loss: 0.8462 - accuracy: 0.73 - ETA: 1:04 - loss: 0.8457 - accuracy: 0.73 - ETA: 1:03 - loss: 0.8468 - accuracy: 0.73 - ETA: 1:02 - loss: 0.8465 - accuracy: 0.73 - ETA: 1:01 - loss: 0.8435 - accuracy: 0.73 - ETA: 1:00 - loss: 0.8447 - accuracy: 0.73 - ETA: 59s - loss: 0.8453 - accuracy: 0.7368 - ETA: 58s - loss: 0.8473 - accuracy: 0.737 - ETA: 58s - loss: 0.8422 - accuracy: 0.738 - ETA: 57s - loss: 0.8421 - accuracy: 0.738 - ETA: 56s - loss: 0.8401 - accuracy: 0.739 - ETA: 55s - loss: 0.8424 - accuracy: 0.738 - ETA: 54s - loss: 0.8416 - accuracy: 0.738 - ETA: 53s - loss: 0.8356 - accuracy: 0.739 - ETA: 52s - loss: 0.8319 - accuracy: 0.741 - ETA: 51s - loss: 0.8368 - accuracy: 0.740 - ETA: 50s - loss: 0.8362 - accuracy: 0.740 - ETA: 49s - loss: 0.8327 - accuracy: 0.741 - ETA: 48s - loss: 0.8303 - accuracy: 0.742 - ETA: 47s - loss: 0.8288 - accuracy: 0.742 - ETA: 46s - loss: 0.8328 - accuracy: 0.741 - ETA: 46s - loss: 0.8344 - accuracy: 0.740 - ETA: 45s - loss: 0.8351 - accuracy: 0.741 - ETA: 44s - loss: 0.8345 - accuracy: 0.740 - ETA: 43s - loss: 0.8399 - accuracy: 0.739 - ETA: 42s - loss: 0.8457 - accuracy: 0.739 - ETA: 41s - loss: 0.8469 - accuracy: 0.739 - ETA: 40s - loss: 0.8522 - accuracy: 0.737 - ETA: 39s - loss: 0.8506 - accuracy: 0.737 - ETA: 38s - loss: 0.8459 - accuracy: 0.739 - ETA: 37s - loss: 0.8458 - accuracy: 0.739 - ETA: 36s - loss: 0.8470 - accuracy: 0.739 - ETA: 36s - loss: 0.8462 - accuracy: 0.739 - ETA: 35s - loss: 0.8494 - accuracy: 0.738 - ETA: 34s - loss: 0.8497 - accuracy: 0.737 - ETA: 33s - loss: 0.8511 - accuracy: 0.738 - ETA: 32s - loss: 0.8532 - accuracy: 0.738 - ETA: 31s - loss: 0.8502 - accuracy: 0.739 - ETA: 30s - loss: 0.8506 - accuracy: 0.739 - ETA: 29s - loss: 0.8521 - accuracy: 0.738 - ETA: 28s - loss: 0.8511 - accuracy: 0.739 - ETA: 27s - loss: 0.8509 - accuracy: 0.739 - ETA: 26s - loss: 0.8528 - accuracy: 0.738 - ETA: 25s - loss: 0.8527 - accuracy: 0.738 - ETA: 25s - loss: 0.8532 - accuracy: 0.738 - ETA: 24s - loss: 0.8536 - accuracy: 0.738 - ETA: 23s - loss: 0.8547 - accuracy: 0.737 - ETA: 22s - loss: 0.8524 - accuracy: 0.738 - ETA: 21s - loss: 0.8527 - accuracy: 0.738 - ETA: 20s - loss: 0.8541 - accuracy: 0.737 - ETA: 19s - loss: 0.8540 - accuracy: 0.737 - ETA: 18s - loss: 0.8523 - accuracy: 0.738 - ETA: 17s - loss: 0.8502 - accuracy: 0.739 - ETA: 16s - loss: 0.8494 - accuracy: 0.739 - ETA: 16s - loss: 0.8484 - accuracy: 0.739 - ETA: 15s - loss: 0.8479 - accuracy: 0.739 - ETA: 14s - loss: 0.8470 - accuracy: 0.740 - ETA: 13s - loss: 0.8451 - accuracy: 0.740 - ETA: 12s - loss: 0.8455 - accuracy: 0.740 - ETA: 11s - loss: 0.8444 - accuracy: 0.740 - ETA: 10s - loss: 0.8458 - accuracy: 0.740 - ETA: 9s - loss: 0.8432 - accuracy: 0.741 - ETA: 8s - loss: 0.8410 - accuracy: 0.74 - ETA: 7s - loss: 0.8433 - accuracy: 0.74 - ETA: 6s - loss: 0.8436 - accuracy: 0.74 - ETA: 6s - loss: 0.8433 - accuracy: 0.74 - ETA: 5s - loss: 0.8435 - accuracy: 0.74 - ETA: 4s - loss: 0.8442 - accuracy: 0.74 - ETA: 3s - loss: 0.8438 - accuracy: 0.74 - ETA: 2s - loss: 0.8412 - accuracy: 0.74 - ETA: 1s - loss: 0.8411 - accuracy: 0.74 - ETA: 0s - loss: 0.8400 - accuracy: 0.74 - 106s 8ms/step - loss: 0.8397 - accuracy: 0.7428 - val_loss: 2.8123 - val_accuracy: 0.3494\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:31 - loss: 0.7618 - accuracy: 0.76 - ETA: 1:27 - loss: 0.8051 - accuracy: 0.75 - ETA: 1:26 - loss: 0.8169 - accuracy: 0.74 - ETA: 1:26 - loss: 0.8324 - accuracy: 0.74 - ETA: 1:25 - loss: 0.8454 - accuracy: 0.74 - ETA: 1:24 - loss: 0.8294 - accuracy: 0.73 - ETA: 1:23 - loss: 0.8094 - accuracy: 0.74 - ETA: 1:23 - loss: 0.7831 - accuracy: 0.75 - ETA: 1:22 - loss: 0.7708 - accuracy: 0.75 - ETA: 1:21 - loss: 0.7928 - accuracy: 0.75 - ETA: 1:20 - loss: 0.7989 - accuracy: 0.75 - ETA: 1:19 - loss: 0.8115 - accuracy: 0.75 - ETA: 1:19 - loss: 0.8169 - accuracy: 0.75 - ETA: 1:18 - loss: 0.8035 - accuracy: 0.75 - ETA: 1:17 - loss: 0.8057 - accuracy: 0.75 - ETA: 1:17 - loss: 0.8019 - accuracy: 0.76 - ETA: 1:16 - loss: 0.7893 - accuracy: 0.76 - ETA: 1:15 - loss: 0.7966 - accuracy: 0.75 - ETA: 1:14 - loss: 0.7964 - accuracy: 0.75 - ETA: 1:13 - loss: 0.7864 - accuracy: 0.76 - ETA: 1:12 - loss: 0.7911 - accuracy: 0.76 - ETA: 1:11 - loss: 0.7946 - accuracy: 0.75 - ETA: 1:10 - loss: 0.7964 - accuracy: 0.75 - ETA: 1:09 - loss: 0.7910 - accuracy: 0.76 - ETA: 1:08 - loss: 0.7835 - accuracy: 0.76 - ETA: 1:07 - loss: 0.7793 - accuracy: 0.76 - ETA: 1:07 - loss: 0.7726 - accuracy: 0.76 - ETA: 1:06 - loss: 0.7717 - accuracy: 0.76 - ETA: 1:05 - loss: 0.7703 - accuracy: 0.76 - ETA: 1:04 - loss: 0.7737 - accuracy: 0.76 - ETA: 1:03 - loss: 0.7712 - accuracy: 0.76 - ETA: 1:02 - loss: 0.7748 - accuracy: 0.76 - ETA: 1:01 - loss: 0.7769 - accuracy: 0.76 - ETA: 1:01 - loss: 0.7784 - accuracy: 0.76 - ETA: 1:00 - loss: 0.7773 - accuracy: 0.76 - ETA: 59s - loss: 0.7797 - accuracy: 0.7632 - ETA: 58s - loss: 0.7807 - accuracy: 0.761 - ETA: 57s - loss: 0.7818 - accuracy: 0.762 - ETA: 56s - loss: 0.7817 - accuracy: 0.761 - ETA: 55s - loss: 0.7834 - accuracy: 0.760 - ETA: 54s - loss: 0.7838 - accuracy: 0.760 - ETA: 53s - loss: 0.7834 - accuracy: 0.760 - ETA: 52s - loss: 0.7800 - accuracy: 0.760 - ETA: 51s - loss: 0.7792 - accuracy: 0.761 - ETA: 51s - loss: 0.7810 - accuracy: 0.762 - ETA: 50s - loss: 0.7811 - accuracy: 0.761 - ETA: 49s - loss: 0.7841 - accuracy: 0.760 - ETA: 48s - loss: 0.7817 - accuracy: 0.761 - ETA: 47s - loss: 0.7833 - accuracy: 0.761 - ETA: 46s - loss: 0.7822 - accuracy: 0.760 - ETA: 45s - loss: 0.7862 - accuracy: 0.759 - ETA: 44s - loss: 0.7857 - accuracy: 0.760 - ETA: 43s - loss: 0.7863 - accuracy: 0.760 - ETA: 43s - loss: 0.7861 - accuracy: 0.760 - ETA: 42s - loss: 0.7908 - accuracy: 0.759 - ETA: 41s - loss: 0.7920 - accuracy: 0.758 - ETA: 40s - loss: 0.7936 - accuracy: 0.757 - ETA: 39s - loss: 0.7912 - accuracy: 0.757 - ETA: 38s - loss: 0.7921 - accuracy: 0.756 - ETA: 37s - loss: 0.7924 - accuracy: 0.757 - ETA: 36s - loss: 0.7916 - accuracy: 0.757 - ETA: 35s - loss: 0.7901 - accuracy: 0.757 - ETA: 35s - loss: 0.7894 - accuracy: 0.758 - ETA: 34s - loss: 0.7888 - accuracy: 0.758 - ETA: 33s - loss: 0.7894 - accuracy: 0.758 - ETA: 32s - loss: 0.7894 - accuracy: 0.758 - ETA: 31s - loss: 0.7888 - accuracy: 0.758 - ETA: 30s - loss: 0.7883 - accuracy: 0.758 - ETA: 29s - loss: 0.7846 - accuracy: 0.759 - ETA: 28s - loss: 0.7847 - accuracy: 0.759 - ETA: 27s - loss: 0.7856 - accuracy: 0.759 - ETA: 26s - loss: 0.7843 - accuracy: 0.758 - ETA: 26s - loss: 0.7844 - accuracy: 0.758 - ETA: 25s - loss: 0.7852 - accuracy: 0.758 - ETA: 24s - loss: 0.7846 - accuracy: 0.757 - ETA: 23s - loss: 0.7850 - accuracy: 0.757 - ETA: 22s - loss: 0.7891 - accuracy: 0.757 - ETA: 21s - loss: 0.7880 - accuracy: 0.757 - ETA: 20s - loss: 0.7877 - accuracy: 0.757 - ETA: 19s - loss: 0.7869 - accuracy: 0.757 - ETA: 18s - loss: 0.7881 - accuracy: 0.756 - ETA: 17s - loss: 0.7870 - accuracy: 0.756 - ETA: 16s - loss: 0.7869 - accuracy: 0.756 - ETA: 16s - loss: 0.7856 - accuracy: 0.757 - ETA: 15s - loss: 0.7863 - accuracy: 0.756 - ETA: 14s - loss: 0.7872 - accuracy: 0.756 - ETA: 13s - loss: 0.7850 - accuracy: 0.756 - ETA: 12s - loss: 0.7854 - accuracy: 0.756 - ETA: 11s - loss: 0.7867 - accuracy: 0.756 - ETA: 10s - loss: 0.7877 - accuracy: 0.755 - ETA: 9s - loss: 0.7899 - accuracy: 0.755 - ETA: 8s - loss: 0.7932 - accuracy: 0.75 - ETA: 7s - loss: 0.7938 - accuracy: 0.75 - ETA: 6s - loss: 0.7922 - accuracy: 0.75 - ETA: 6s - loss: 0.7928 - accuracy: 0.75 - ETA: 5s - loss: 0.7921 - accuracy: 0.75 - ETA: 4s - loss: 0.7909 - accuracy: 0.75 - ETA: 3s - loss: 0.7919 - accuracy: 0.75 - ETA: 2s - loss: 0.7924 - accuracy: 0.75 - ETA: 1s - loss: 0.7933 - accuracy: 0.75 - ETA: 0s - loss: 0.7930 - accuracy: 0.75 - 105s 8ms/step - loss: 0.7947 - accuracy: 0.7541 - val_loss: 2.8410 - val_accuracy: 0.3500\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.8025 - accuracy: 0.73 - ETA: 1:29 - loss: 0.7743 - accuracy: 0.75 - ETA: 1:28 - loss: 0.7782 - accuracy: 0.74 - ETA: 1:28 - loss: 0.7552 - accuracy: 0.75 - ETA: 1:27 - loss: 0.7887 - accuracy: 0.74 - ETA: 1:27 - loss: 0.7739 - accuracy: 0.75 - ETA: 1:26 - loss: 0.7799 - accuracy: 0.75 - ETA: 1:25 - loss: 0.7873 - accuracy: 0.74 - ETA: 1:23 - loss: 0.8089 - accuracy: 0.74 - ETA: 1:22 - loss: 0.8092 - accuracy: 0.74 - ETA: 1:22 - loss: 0.7773 - accuracy: 0.75 - ETA: 1:22 - loss: 0.7891 - accuracy: 0.75 - ETA: 1:21 - loss: 0.7892 - accuracy: 0.75 - ETA: 1:20 - loss: 0.7780 - accuracy: 0.75 - ETA: 1:20 - loss: 0.7672 - accuracy: 0.75 - ETA: 1:19 - loss: 0.7745 - accuracy: 0.75 - ETA: 1:18 - loss: 0.7798 - accuracy: 0.75 - ETA: 1:17 - loss: 0.7745 - accuracy: 0.75 - ETA: 1:16 - loss: 0.7731 - accuracy: 0.75 - ETA: 1:14 - loss: 0.7797 - accuracy: 0.75 - ETA: 1:13 - loss: 0.7761 - accuracy: 0.75 - ETA: 1:12 - loss: 0.7755 - accuracy: 0.75 - ETA: 1:11 - loss: 0.7790 - accuracy: 0.75 - ETA: 1:10 - loss: 0.7771 - accuracy: 0.75 - ETA: 1:10 - loss: 0.7764 - accuracy: 0.75 - ETA: 1:09 - loss: 0.7725 - accuracy: 0.75 - ETA: 1:08 - loss: 0.7714 - accuracy: 0.75 - ETA: 1:07 - loss: 0.7719 - accuracy: 0.75 - ETA: 1:06 - loss: 0.7780 - accuracy: 0.75 - ETA: 1:05 - loss: 0.7785 - accuracy: 0.75 - ETA: 1:04 - loss: 0.7756 - accuracy: 0.75 - ETA: 1:03 - loss: 0.7755 - accuracy: 0.75 - ETA: 1:02 - loss: 0.7687 - accuracy: 0.75 - ETA: 1:01 - loss: 0.7735 - accuracy: 0.75 - ETA: 1:00 - loss: 0.7801 - accuracy: 0.75 - ETA: 59s - loss: 0.7819 - accuracy: 0.7530 - ETA: 58s - loss: 0.7831 - accuracy: 0.752 - ETA: 57s - loss: 0.7846 - accuracy: 0.751 - ETA: 56s - loss: 0.7856 - accuracy: 0.751 - ETA: 55s - loss: 0.7870 - accuracy: 0.751 - ETA: 55s - loss: 0.7847 - accuracy: 0.751 - ETA: 54s - loss: 0.7824 - accuracy: 0.753 - ETA: 53s - loss: 0.7876 - accuracy: 0.753 - ETA: 52s - loss: 0.7889 - accuracy: 0.753 - ETA: 51s - loss: 0.7880 - accuracy: 0.754 - ETA: 50s - loss: 0.7844 - accuracy: 0.755 - ETA: 49s - loss: 0.7827 - accuracy: 0.755 - ETA: 48s - loss: 0.7852 - accuracy: 0.754 - ETA: 47s - loss: 0.7881 - accuracy: 0.754 - ETA: 46s - loss: 0.7838 - accuracy: 0.755 - ETA: 45s - loss: 0.7892 - accuracy: 0.755 - ETA: 44s - loss: 0.7909 - accuracy: 0.755 - ETA: 43s - loss: 0.7908 - accuracy: 0.755 - ETA: 43s - loss: 0.7938 - accuracy: 0.753 - ETA: 42s - loss: 0.7948 - accuracy: 0.753 - ETA: 41s - loss: 0.7953 - accuracy: 0.752 - ETA: 40s - loss: 0.7964 - accuracy: 0.752 - ETA: 39s - loss: 0.7987 - accuracy: 0.750 - ETA: 38s - loss: 0.7959 - accuracy: 0.751 - ETA: 37s - loss: 0.7948 - accuracy: 0.752 - ETA: 36s - loss: 0.7928 - accuracy: 0.752 - ETA: 35s - loss: 0.7938 - accuracy: 0.751 - ETA: 34s - loss: 0.7966 - accuracy: 0.751 - ETA: 34s - loss: 0.7959 - accuracy: 0.751 - ETA: 33s - loss: 0.7951 - accuracy: 0.751 - ETA: 32s - loss: 0.7951 - accuracy: 0.750 - ETA: 31s - loss: 0.7989 - accuracy: 0.750 - ETA: 30s - loss: 0.7952 - accuracy: 0.751 - ETA: 29s - loss: 0.7945 - accuracy: 0.751 - ETA: 28s - loss: 0.7938 - accuracy: 0.751 - ETA: 27s - loss: 0.7947 - accuracy: 0.751 - ETA: 26s - loss: 0.7942 - accuracy: 0.751 - ETA: 25s - loss: 0.7929 - accuracy: 0.752 - ETA: 24s - loss: 0.7937 - accuracy: 0.751 - ETA: 24s - loss: 0.7931 - accuracy: 0.752 - ETA: 23s - loss: 0.7934 - accuracy: 0.751 - ETA: 22s - loss: 0.7942 - accuracy: 0.751 - ETA: 21s - loss: 0.7949 - accuracy: 0.752 - ETA: 20s - loss: 0.7924 - accuracy: 0.752 - ETA: 19s - loss: 0.7904 - accuracy: 0.752 - ETA: 18s - loss: 0.7887 - accuracy: 0.753 - ETA: 17s - loss: 0.7888 - accuracy: 0.753 - ETA: 16s - loss: 0.7909 - accuracy: 0.752 - ETA: 15s - loss: 0.7931 - accuracy: 0.752 - ETA: 15s - loss: 0.7922 - accuracy: 0.752 - ETA: 14s - loss: 0.7939 - accuracy: 0.752 - ETA: 13s - loss: 0.7949 - accuracy: 0.752 - ETA: 12s - loss: 0.7952 - accuracy: 0.752 - ETA: 11s - loss: 0.7941 - accuracy: 0.753 - ETA: 10s - loss: 0.7948 - accuracy: 0.753 - ETA: 9s - loss: 0.7945 - accuracy: 0.753 - ETA: 8s - loss: 0.7944 - accuracy: 0.75 - ETA: 7s - loss: 0.7957 - accuracy: 0.75 - ETA: 6s - loss: 0.7989 - accuracy: 0.75 - ETA: 6s - loss: 0.7999 - accuracy: 0.75 - ETA: 5s - loss: 0.7978 - accuracy: 0.75 - ETA: 4s - loss: 0.7993 - accuracy: 0.75 - ETA: 3s - loss: 0.7986 - accuracy: 0.75 - ETA: 2s - loss: 0.7985 - accuracy: 0.75 - ETA: 1s - loss: 0.7986 - accuracy: 0.75 - ETA: 0s - loss: 0.7986 - accuracy: 0.75 - 105s 8ms/step - loss: 0.7981 - accuracy: 0.7530 - val_loss: 2.9218 - val_accuracy: 0.3300\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:34 - loss: 0.8425 - accuracy: 0.78 - ETA: 1:29 - loss: 0.8842 - accuracy: 0.75 - ETA: 1:30 - loss: 0.7975 - accuracy: 0.76 - ETA: 1:29 - loss: 0.8092 - accuracy: 0.76 - ETA: 1:27 - loss: 0.7842 - accuracy: 0.77 - ETA: 1:26 - loss: 0.7923 - accuracy: 0.76 - ETA: 1:24 - loss: 0.8287 - accuracy: 0.75 - ETA: 1:24 - loss: 0.8101 - accuracy: 0.75 - ETA: 1:23 - loss: 0.8122 - accuracy: 0.76 - ETA: 1:22 - loss: 0.8121 - accuracy: 0.76 - ETA: 1:22 - loss: 0.8096 - accuracy: 0.76 - ETA: 1:21 - loss: 0.8143 - accuracy: 0.76 - ETA: 1:20 - loss: 0.8104 - accuracy: 0.76 - ETA: 1:19 - loss: 0.8098 - accuracy: 0.76 - ETA: 1:19 - loss: 0.8106 - accuracy: 0.75 - ETA: 1:17 - loss: 0.8157 - accuracy: 0.75 - ETA: 1:16 - loss: 0.8155 - accuracy: 0.75 - ETA: 1:15 - loss: 0.8086 - accuracy: 0.76 - ETA: 1:14 - loss: 0.7992 - accuracy: 0.76 - ETA: 1:13 - loss: 0.8011 - accuracy: 0.76 - ETA: 1:12 - loss: 0.7969 - accuracy: 0.75 - ETA: 1:11 - loss: 0.7931 - accuracy: 0.76 - ETA: 1:10 - loss: 0.7918 - accuracy: 0.76 - ETA: 1:09 - loss: 0.7919 - accuracy: 0.76 - ETA: 1:09 - loss: 0.7873 - accuracy: 0.76 - ETA: 1:08 - loss: 0.7868 - accuracy: 0.76 - ETA: 1:07 - loss: 0.7884 - accuracy: 0.76 - ETA: 1:06 - loss: 0.7960 - accuracy: 0.76 - ETA: 1:05 - loss: 0.7992 - accuracy: 0.76 - ETA: 1:04 - loss: 0.7966 - accuracy: 0.76 - ETA: 1:03 - loss: 0.7897 - accuracy: 0.76 - ETA: 1:02 - loss: 0.7887 - accuracy: 0.76 - ETA: 1:01 - loss: 0.7888 - accuracy: 0.76 - ETA: 1:00 - loss: 0.7914 - accuracy: 0.76 - ETA: 1:00 - loss: 0.7893 - accuracy: 0.76 - ETA: 59s - loss: 0.7930 - accuracy: 0.7652 - ETA: 58s - loss: 0.7945 - accuracy: 0.764 - ETA: 57s - loss: 0.7995 - accuracy: 0.764 - ETA: 56s - loss: 0.7978 - accuracy: 0.765 - ETA: 55s - loss: 0.8002 - accuracy: 0.765 - ETA: 54s - loss: 0.7995 - accuracy: 0.765 - ETA: 53s - loss: 0.8027 - accuracy: 0.765 - ETA: 52s - loss: 0.7992 - accuracy: 0.764 - ETA: 52s - loss: 0.7988 - accuracy: 0.764 - ETA: 51s - loss: 0.8024 - accuracy: 0.763 - ETA: 50s - loss: 0.8005 - accuracy: 0.763 - ETA: 49s - loss: 0.8004 - accuracy: 0.763 - ETA: 48s - loss: 0.8000 - accuracy: 0.763 - ETA: 47s - loss: 0.8014 - accuracy: 0.762 - ETA: 46s - loss: 0.8015 - accuracy: 0.762 - ETA: 45s - loss: 0.8071 - accuracy: 0.760 - ETA: 44s - loss: 0.8080 - accuracy: 0.760 - ETA: 43s - loss: 0.8075 - accuracy: 0.759 - ETA: 43s - loss: 0.8071 - accuracy: 0.759 - ETA: 42s - loss: 0.8073 - accuracy: 0.759 - ETA: 41s - loss: 0.8081 - accuracy: 0.759 - ETA: 40s - loss: 0.8065 - accuracy: 0.760 - ETA: 39s - loss: 0.8050 - accuracy: 0.760 - ETA: 38s - loss: 0.8013 - accuracy: 0.761 - ETA: 37s - loss: 0.7992 - accuracy: 0.762 - ETA: 36s - loss: 0.8008 - accuracy: 0.761 - ETA: 35s - loss: 0.8003 - accuracy: 0.761 - ETA: 34s - loss: 0.8036 - accuracy: 0.760 - ETA: 34s - loss: 0.8049 - accuracy: 0.760 - ETA: 33s - loss: 0.8053 - accuracy: 0.760 - ETA: 32s - loss: 0.8051 - accuracy: 0.760 - ETA: 31s - loss: 0.8054 - accuracy: 0.760 - ETA: 30s - loss: 0.8052 - accuracy: 0.760 - ETA: 29s - loss: 0.8051 - accuracy: 0.760 - ETA: 28s - loss: 0.8069 - accuracy: 0.759 - ETA: 27s - loss: 0.8045 - accuracy: 0.760 - ETA: 26s - loss: 0.8041 - accuracy: 0.760 - ETA: 25s - loss: 0.8023 - accuracy: 0.760 - ETA: 25s - loss: 0.8013 - accuracy: 0.761 - ETA: 24s - loss: 0.8000 - accuracy: 0.761 - ETA: 23s - loss: 0.8001 - accuracy: 0.761 - ETA: 22s - loss: 0.8002 - accuracy: 0.761 - ETA: 21s - loss: 0.7980 - accuracy: 0.762 - ETA: 20s - loss: 0.7986 - accuracy: 0.762 - ETA: 19s - loss: 0.7990 - accuracy: 0.762 - ETA: 18s - loss: 0.8012 - accuracy: 0.762 - ETA: 17s - loss: 0.8046 - accuracy: 0.761 - ETA: 16s - loss: 0.8049 - accuracy: 0.761 - ETA: 16s - loss: 0.8054 - accuracy: 0.761 - ETA: 15s - loss: 0.8054 - accuracy: 0.761 - ETA: 14s - loss: 0.8035 - accuracy: 0.762 - ETA: 13s - loss: 0.8030 - accuracy: 0.762 - ETA: 12s - loss: 0.8032 - accuracy: 0.762 - ETA: 11s - loss: 0.8039 - accuracy: 0.762 - ETA: 10s - loss: 0.8033 - accuracy: 0.762 - ETA: 9s - loss: 0.8059 - accuracy: 0.761 - ETA: 8s - loss: 0.8065 - accuracy: 0.76 - ETA: 7s - loss: 0.8079 - accuracy: 0.76 - ETA: 6s - loss: 0.8082 - accuracy: 0.75 - ETA: 6s - loss: 0.8080 - accuracy: 0.76 - ETA: 5s - loss: 0.8090 - accuracy: 0.76 - ETA: 4s - loss: 0.8081 - accuracy: 0.76 - ETA: 3s - loss: 0.8098 - accuracy: 0.75 - ETA: 2s - loss: 0.8089 - accuracy: 0.75 - ETA: 1s - loss: 0.8088 - accuracy: 0.75 - ETA: 0s - loss: 0.8087 - accuracy: 0.75 - 105s 8ms/step - loss: 0.8082 - accuracy: 0.7588 - val_loss: 2.8246 - val_accuracy: 0.3345\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:37 - loss: 0.6499 - accuracy: 0.82 - ETA: 1:35 - loss: 0.6934 - accuracy: 0.77 - ETA: 1:35 - loss: 0.6630 - accuracy: 0.77 - ETA: 1:33 - loss: 0.6982 - accuracy: 0.77 - ETA: 1:31 - loss: 0.7156 - accuracy: 0.77 - ETA: 1:29 - loss: 0.7334 - accuracy: 0.76 - ETA: 1:27 - loss: 0.7468 - accuracy: 0.76 - ETA: 1:26 - loss: 0.7527 - accuracy: 0.76 - ETA: 1:25 - loss: 0.7535 - accuracy: 0.76 - ETA: 1:24 - loss: 0.7460 - accuracy: 0.76 - ETA: 1:23 - loss: 0.7509 - accuracy: 0.76 - ETA: 1:21 - loss: 0.7745 - accuracy: 0.75 - ETA: 1:20 - loss: 0.7759 - accuracy: 0.75 - ETA: 1:20 - loss: 0.7633 - accuracy: 0.76 - ETA: 1:18 - loss: 0.7767 - accuracy: 0.76 - ETA: 1:18 - loss: 0.7798 - accuracy: 0.76 - ETA: 1:17 - loss: 0.7709 - accuracy: 0.75 - ETA: 1:16 - loss: 0.7682 - accuracy: 0.76 - ETA: 1:15 - loss: 0.7753 - accuracy: 0.75 - ETA: 1:14 - loss: 0.7724 - accuracy: 0.75 - ETA: 1:13 - loss: 0.7671 - accuracy: 0.76 - ETA: 1:13 - loss: 0.7602 - accuracy: 0.76 - ETA: 1:12 - loss: 0.7588 - accuracy: 0.76 - ETA: 1:11 - loss: 0.7662 - accuracy: 0.76 - ETA: 1:10 - loss: 0.7749 - accuracy: 0.76 - ETA: 1:09 - loss: 0.7680 - accuracy: 0.76 - ETA: 1:08 - loss: 0.7702 - accuracy: 0.76 - ETA: 1:07 - loss: 0.7711 - accuracy: 0.76 - ETA: 1:06 - loss: 0.7726 - accuracy: 0.75 - ETA: 1:05 - loss: 0.7776 - accuracy: 0.75 - ETA: 1:04 - loss: 0.7729 - accuracy: 0.75 - ETA: 1:03 - loss: 0.7777 - accuracy: 0.75 - ETA: 1:02 - loss: 0.7788 - accuracy: 0.75 - ETA: 1:01 - loss: 0.7759 - accuracy: 0.75 - ETA: 1:00 - loss: 0.7778 - accuracy: 0.75 - ETA: 59s - loss: 0.7809 - accuracy: 0.7552 - ETA: 58s - loss: 0.7829 - accuracy: 0.755 - ETA: 57s - loss: 0.7810 - accuracy: 0.755 - ETA: 56s - loss: 0.7843 - accuracy: 0.755 - ETA: 56s - loss: 0.7864 - accuracy: 0.754 - ETA: 55s - loss: 0.7870 - accuracy: 0.754 - ETA: 54s - loss: 0.7880 - accuracy: 0.754 - ETA: 53s - loss: 0.7906 - accuracy: 0.753 - ETA: 52s - loss: 0.7903 - accuracy: 0.752 - ETA: 51s - loss: 0.7977 - accuracy: 0.751 - ETA: 50s - loss: 0.7970 - accuracy: 0.751 - ETA: 49s - loss: 0.7973 - accuracy: 0.750 - ETA: 48s - loss: 0.7956 - accuracy: 0.750 - ETA: 47s - loss: 0.7970 - accuracy: 0.751 - ETA: 46s - loss: 0.7983 - accuracy: 0.751 - ETA: 45s - loss: 0.7985 - accuracy: 0.751 - ETA: 44s - loss: 0.8046 - accuracy: 0.749 - ETA: 44s - loss: 0.8035 - accuracy: 0.750 - ETA: 43s - loss: 0.8024 - accuracy: 0.750 - ETA: 42s - loss: 0.8015 - accuracy: 0.750 - ETA: 41s - loss: 0.8019 - accuracy: 0.749 - ETA: 40s - loss: 0.8049 - accuracy: 0.748 - ETA: 39s - loss: 0.8017 - accuracy: 0.749 - ETA: 38s - loss: 0.8013 - accuracy: 0.749 - ETA: 37s - loss: 0.8015 - accuracy: 0.749 - ETA: 36s - loss: 0.7991 - accuracy: 0.749 - ETA: 35s - loss: 0.7955 - accuracy: 0.751 - ETA: 34s - loss: 0.7940 - accuracy: 0.751 - ETA: 34s - loss: 0.7899 - accuracy: 0.752 - ETA: 33s - loss: 0.7879 - accuracy: 0.752 - ETA: 32s - loss: 0.7853 - accuracy: 0.753 - ETA: 31s - loss: 0.7845 - accuracy: 0.753 - ETA: 30s - loss: 0.7872 - accuracy: 0.753 - ETA: 29s - loss: 0.7861 - accuracy: 0.753 - ETA: 28s - loss: 0.7848 - accuracy: 0.753 - ETA: 27s - loss: 0.7851 - accuracy: 0.753 - ETA: 26s - loss: 0.7839 - accuracy: 0.754 - ETA: 25s - loss: 0.7853 - accuracy: 0.754 - ETA: 25s - loss: 0.7843 - accuracy: 0.754 - ETA: 24s - loss: 0.7849 - accuracy: 0.754 - ETA: 23s - loss: 0.7857 - accuracy: 0.754 - ETA: 22s - loss: 0.7827 - accuracy: 0.754 - ETA: 21s - loss: 0.7820 - accuracy: 0.755 - ETA: 20s - loss: 0.7806 - accuracy: 0.755 - ETA: 19s - loss: 0.7811 - accuracy: 0.755 - ETA: 18s - loss: 0.7798 - accuracy: 0.755 - ETA: 17s - loss: 0.7811 - accuracy: 0.755 - ETA: 16s - loss: 0.7798 - accuracy: 0.756 - ETA: 16s - loss: 0.7802 - accuracy: 0.756 - ETA: 15s - loss: 0.7798 - accuracy: 0.756 - ETA: 14s - loss: 0.7801 - accuracy: 0.756 - ETA: 13s - loss: 0.7828 - accuracy: 0.756 - ETA: 12s - loss: 0.7829 - accuracy: 0.756 - ETA: 11s - loss: 0.7829 - accuracy: 0.756 - ETA: 10s - loss: 0.7831 - accuracy: 0.756 - ETA: 9s - loss: 0.7839 - accuracy: 0.756 - ETA: 8s - loss: 0.7827 - accuracy: 0.75 - ETA: 7s - loss: 0.7821 - accuracy: 0.75 - ETA: 7s - loss: 0.7827 - accuracy: 0.75 - ETA: 6s - loss: 0.7822 - accuracy: 0.75 - ETA: 5s - loss: 0.7811 - accuracy: 0.75 - ETA: 4s - loss: 0.7806 - accuracy: 0.75 - ETA: 3s - loss: 0.7813 - accuracy: 0.75 - ETA: 2s - loss: 0.7816 - accuracy: 0.75 - ETA: 1s - loss: 0.7827 - accuracy: 0.75 - ETA: 0s - loss: 0.7818 - accuracy: 0.75 - 106s 8ms/step - loss: 0.7821 - accuracy: 0.7573 - val_loss: 2.9311 - val_accuracy: 0.3569\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:34 - loss: 0.7439 - accuracy: 0.77 - ETA: 1:31 - loss: 0.7696 - accuracy: 0.75 - ETA: 1:30 - loss: 0.8434 - accuracy: 0.72 - ETA: 1:29 - loss: 0.8651 - accuracy: 0.72 - ETA: 1:27 - loss: 0.8354 - accuracy: 0.73 - ETA: 1:26 - loss: 0.7817 - accuracy: 0.75 - ETA: 1:25 - loss: 0.7813 - accuracy: 0.75 - ETA: 1:25 - loss: 0.7763 - accuracy: 0.75 - ETA: 1:24 - loss: 0.7678 - accuracy: 0.75 - ETA: 1:23 - loss: 0.7657 - accuracy: 0.76 - ETA: 1:22 - loss: 0.7720 - accuracy: 0.76 - ETA: 1:21 - loss: 0.7607 - accuracy: 0.76 - ETA: 1:20 - loss: 0.7575 - accuracy: 0.76 - ETA: 1:19 - loss: 0.7527 - accuracy: 0.76 - ETA: 1:18 - loss: 0.7530 - accuracy: 0.76 - ETA: 1:17 - loss: 0.7606 - accuracy: 0.76 - ETA: 1:16 - loss: 0.7575 - accuracy: 0.76 - ETA: 1:15 - loss: 0.7529 - accuracy: 0.76 - ETA: 1:14 - loss: 0.7606 - accuracy: 0.76 - ETA: 1:13 - loss: 0.7628 - accuracy: 0.75 - ETA: 1:12 - loss: 0.7639 - accuracy: 0.75 - ETA: 1:11 - loss: 0.7783 - accuracy: 0.75 - ETA: 1:10 - loss: 0.7685 - accuracy: 0.75 - ETA: 1:09 - loss: 0.7739 - accuracy: 0.75 - ETA: 1:08 - loss: 0.7724 - accuracy: 0.75 - ETA: 1:08 - loss: 0.7765 - accuracy: 0.75 - ETA: 1:07 - loss: 0.7816 - accuracy: 0.75 - ETA: 1:06 - loss: 0.7814 - accuracy: 0.75 - ETA: 1:05 - loss: 0.7750 - accuracy: 0.76 - ETA: 1:05 - loss: 0.7739 - accuracy: 0.75 - ETA: 1:04 - loss: 0.7770 - accuracy: 0.75 - ETA: 1:03 - loss: 0.7813 - accuracy: 0.75 - ETA: 1:02 - loss: 0.7780 - accuracy: 0.75 - ETA: 1:01 - loss: 0.7756 - accuracy: 0.75 - ETA: 1:00 - loss: 0.7757 - accuracy: 0.75 - ETA: 59s - loss: 0.7724 - accuracy: 0.7602 - ETA: 58s - loss: 0.7708 - accuracy: 0.761 - ETA: 57s - loss: 0.7759 - accuracy: 0.758 - ETA: 56s - loss: 0.7747 - accuracy: 0.759 - ETA: 55s - loss: 0.7797 - accuracy: 0.757 - ETA: 55s - loss: 0.7790 - accuracy: 0.757 - ETA: 54s - loss: 0.7827 - accuracy: 0.757 - ETA: 53s - loss: 0.7822 - accuracy: 0.756 - ETA: 52s - loss: 0.7757 - accuracy: 0.758 - ETA: 51s - loss: 0.7745 - accuracy: 0.758 - ETA: 50s - loss: 0.7756 - accuracy: 0.757 - ETA: 49s - loss: 0.7733 - accuracy: 0.758 - ETA: 48s - loss: 0.7719 - accuracy: 0.759 - ETA: 47s - loss: 0.7721 - accuracy: 0.760 - ETA: 46s - loss: 0.7754 - accuracy: 0.759 - ETA: 45s - loss: 0.7769 - accuracy: 0.758 - ETA: 45s - loss: 0.7755 - accuracy: 0.758 - ETA: 44s - loss: 0.7736 - accuracy: 0.759 - ETA: 43s - loss: 0.7718 - accuracy: 0.760 - ETA: 42s - loss: 0.7711 - accuracy: 0.760 - ETA: 41s - loss: 0.7790 - accuracy: 0.758 - ETA: 40s - loss: 0.7782 - accuracy: 0.759 - ETA: 39s - loss: 0.7782 - accuracy: 0.759 - ETA: 38s - loss: 0.7758 - accuracy: 0.759 - ETA: 37s - loss: 0.7761 - accuracy: 0.759 - ETA: 36s - loss: 0.7738 - accuracy: 0.760 - ETA: 36s - loss: 0.7739 - accuracy: 0.760 - ETA: 35s - loss: 0.7719 - accuracy: 0.761 - ETA: 34s - loss: 0.7744 - accuracy: 0.761 - ETA: 33s - loss: 0.7738 - accuracy: 0.761 - ETA: 32s - loss: 0.7717 - accuracy: 0.762 - ETA: 31s - loss: 0.7726 - accuracy: 0.762 - ETA: 30s - loss: 0.7752 - accuracy: 0.762 - ETA: 29s - loss: 0.7769 - accuracy: 0.761 - ETA: 28s - loss: 0.7782 - accuracy: 0.761 - ETA: 27s - loss: 0.7787 - accuracy: 0.761 - ETA: 26s - loss: 0.7822 - accuracy: 0.760 - ETA: 25s - loss: 0.7817 - accuracy: 0.760 - ETA: 25s - loss: 0.7825 - accuracy: 0.761 - ETA: 24s - loss: 0.7831 - accuracy: 0.760 - ETA: 23s - loss: 0.7824 - accuracy: 0.761 - ETA: 22s - loss: 0.7789 - accuracy: 0.762 - ETA: 21s - loss: 0.7805 - accuracy: 0.762 - ETA: 20s - loss: 0.7819 - accuracy: 0.762 - ETA: 19s - loss: 0.7813 - accuracy: 0.762 - ETA: 18s - loss: 0.7807 - accuracy: 0.762 - ETA: 17s - loss: 0.7798 - accuracy: 0.763 - ETA: 16s - loss: 0.7786 - accuracy: 0.763 - ETA: 16s - loss: 0.7788 - accuracy: 0.763 - ETA: 15s - loss: 0.7776 - accuracy: 0.763 - ETA: 14s - loss: 0.7784 - accuracy: 0.763 - ETA: 13s - loss: 0.7790 - accuracy: 0.763 - ETA: 12s - loss: 0.7796 - accuracy: 0.763 - ETA: 11s - loss: 0.7802 - accuracy: 0.763 - ETA: 10s - loss: 0.7799 - accuracy: 0.763 - ETA: 9s - loss: 0.7807 - accuracy: 0.763 - ETA: 8s - loss: 0.7805 - accuracy: 0.76 - ETA: 7s - loss: 0.7818 - accuracy: 0.76 - ETA: 6s - loss: 0.7823 - accuracy: 0.76 - ETA: 6s - loss: 0.7828 - accuracy: 0.76 - ETA: 5s - loss: 0.7840 - accuracy: 0.76 - ETA: 4s - loss: 0.7814 - accuracy: 0.76 - ETA: 3s - loss: 0.7804 - accuracy: 0.76 - ETA: 2s - loss: 0.7814 - accuracy: 0.76 - ETA: 1s - loss: 0.7814 - accuracy: 0.76 - ETA: 0s - loss: 0.7802 - accuracy: 0.76 - 106s 8ms/step - loss: 0.7811 - accuracy: 0.7627 - val_loss: 2.8592 - val_accuracy: 0.3306\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 0.6882 - accuracy: 0.78 - ETA: 1:28 - loss: 0.7262 - accuracy: 0.76 - ETA: 1:27 - loss: 0.7218 - accuracy: 0.76 - ETA: 1:26 - loss: 0.7516 - accuracy: 0.75 - ETA: 1:25 - loss: 0.7401 - accuracy: 0.76 - ETA: 1:23 - loss: 0.7107 - accuracy: 0.77 - ETA: 1:23 - loss: 0.7485 - accuracy: 0.76 - ETA: 1:22 - loss: 0.7629 - accuracy: 0.76 - ETA: 1:21 - loss: 0.7822 - accuracy: 0.76 - ETA: 1:20 - loss: 0.7796 - accuracy: 0.76 - ETA: 1:20 - loss: 0.7762 - accuracy: 0.75 - ETA: 1:19 - loss: 0.7718 - accuracy: 0.75 - ETA: 1:18 - loss: 0.7844 - accuracy: 0.75 - ETA: 1:17 - loss: 0.7796 - accuracy: 0.75 - ETA: 1:16 - loss: 0.7752 - accuracy: 0.75 - ETA: 1:16 - loss: 0.7742 - accuracy: 0.75 - ETA: 1:15 - loss: 0.7690 - accuracy: 0.75 - ETA: 1:14 - loss: 0.7693 - accuracy: 0.75 - ETA: 1:14 - loss: 0.7658 - accuracy: 0.75 - ETA: 1:13 - loss: 0.7660 - accuracy: 0.75 - ETA: 1:12 - loss: 0.7681 - accuracy: 0.75 - ETA: 1:11 - loss: 0.7663 - accuracy: 0.76 - ETA: 1:10 - loss: 0.7710 - accuracy: 0.76 - ETA: 1:09 - loss: 0.7701 - accuracy: 0.76 - ETA: 1:08 - loss: 0.7642 - accuracy: 0.76 - ETA: 1:07 - loss: 0.7681 - accuracy: 0.76 - ETA: 1:06 - loss: 0.7648 - accuracy: 0.76 - ETA: 1:05 - loss: 0.7594 - accuracy: 0.76 - ETA: 1:04 - loss: 0.7563 - accuracy: 0.76 - ETA: 1:04 - loss: 0.7556 - accuracy: 0.76 - ETA: 1:03 - loss: 0.7557 - accuracy: 0.76 - ETA: 1:02 - loss: 0.7517 - accuracy: 0.76 - ETA: 1:01 - loss: 0.7600 - accuracy: 0.76 - ETA: 1:00 - loss: 0.7642 - accuracy: 0.76 - ETA: 59s - loss: 0.7643 - accuracy: 0.7641 - ETA: 59s - loss: 0.7626 - accuracy: 0.763 - ETA: 58s - loss: 0.7576 - accuracy: 0.765 - ETA: 57s - loss: 0.7593 - accuracy: 0.766 - ETA: 56s - loss: 0.7634 - accuracy: 0.765 - ETA: 55s - loss: 0.7614 - accuracy: 0.765 - ETA: 54s - loss: 0.7635 - accuracy: 0.764 - ETA: 53s - loss: 0.7694 - accuracy: 0.763 - ETA: 52s - loss: 0.7696 - accuracy: 0.762 - ETA: 51s - loss: 0.7730 - accuracy: 0.762 - ETA: 50s - loss: 0.7704 - accuracy: 0.765 - ETA: 49s - loss: 0.7693 - accuracy: 0.766 - ETA: 48s - loss: 0.7705 - accuracy: 0.766 - ETA: 47s - loss: 0.7690 - accuracy: 0.766 - ETA: 47s - loss: 0.7693 - accuracy: 0.767 - ETA: 46s - loss: 0.7716 - accuracy: 0.765 - ETA: 45s - loss: 0.7744 - accuracy: 0.766 - ETA: 44s - loss: 0.7785 - accuracy: 0.764 - ETA: 43s - loss: 0.7797 - accuracy: 0.764 - ETA: 42s - loss: 0.7804 - accuracy: 0.763 - ETA: 41s - loss: 0.7783 - accuracy: 0.764 - ETA: 40s - loss: 0.7794 - accuracy: 0.764 - ETA: 40s - loss: 0.7796 - accuracy: 0.764 - ETA: 39s - loss: 0.7807 - accuracy: 0.764 - ETA: 38s - loss: 0.7837 - accuracy: 0.763 - ETA: 37s - loss: 0.7813 - accuracy: 0.764 - ETA: 36s - loss: 0.7812 - accuracy: 0.764 - ETA: 35s - loss: 0.7844 - accuracy: 0.763 - ETA: 34s - loss: 0.7844 - accuracy: 0.763 - ETA: 33s - loss: 0.7838 - accuracy: 0.763 - ETA: 32s - loss: 0.7813 - accuracy: 0.763 - ETA: 32s - loss: 0.7840 - accuracy: 0.763 - ETA: 31s - loss: 0.7860 - accuracy: 0.763 - ETA: 30s - loss: 0.7840 - accuracy: 0.764 - ETA: 29s - loss: 0.7866 - accuracy: 0.764 - ETA: 28s - loss: 0.7858 - accuracy: 0.764 - ETA: 27s - loss: 0.7854 - accuracy: 0.764 - ETA: 26s - loss: 0.7869 - accuracy: 0.764 - ETA: 25s - loss: 0.7878 - accuracy: 0.764 - ETA: 24s - loss: 0.7863 - accuracy: 0.764 - ETA: 23s - loss: 0.7861 - accuracy: 0.764 - ETA: 23s - loss: 0.7861 - accuracy: 0.764 - ETA: 22s - loss: 0.7866 - accuracy: 0.764 - ETA: 21s - loss: 0.7838 - accuracy: 0.765 - ETA: 20s - loss: 0.7837 - accuracy: 0.764 - ETA: 19s - loss: 0.7850 - accuracy: 0.764 - ETA: 18s - loss: 0.7849 - accuracy: 0.764 - ETA: 17s - loss: 0.7862 - accuracy: 0.764 - ETA: 16s - loss: 0.7853 - accuracy: 0.764 - ETA: 15s - loss: 0.7867 - accuracy: 0.764 - ETA: 15s - loss: 0.7866 - accuracy: 0.764 - ETA: 14s - loss: 0.7871 - accuracy: 0.764 - ETA: 13s - loss: 0.7879 - accuracy: 0.763 - ETA: 12s - loss: 0.7885 - accuracy: 0.763 - ETA: 11s - loss: 0.7879 - accuracy: 0.763 - ETA: 10s - loss: 0.7880 - accuracy: 0.763 - ETA: 9s - loss: 0.7880 - accuracy: 0.762 - ETA: 8s - loss: 0.7844 - accuracy: 0.76 - ETA: 7s - loss: 0.7827 - accuracy: 0.76 - ETA: 6s - loss: 0.7829 - accuracy: 0.76 - ETA: 6s - loss: 0.7827 - accuracy: 0.76 - ETA: 5s - loss: 0.7839 - accuracy: 0.76 - ETA: 4s - loss: 0.7831 - accuracy: 0.76 - ETA: 3s - loss: 0.7832 - accuracy: 0.76 - ETA: 2s - loss: 0.7824 - accuracy: 0.76 - ETA: 1s - loss: 0.7842 - accuracy: 0.76 - ETA: 0s - loss: 0.7840 - accuracy: 0.76 - 105s 8ms/step - loss: 0.7835 - accuracy: 0.7629 - val_loss: 2.9203 - val_accuracy: 0.3129\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.9502 - accuracy: 0.71 - ETA: 1:31 - loss: 0.8968 - accuracy: 0.73 - ETA: 1:31 - loss: 0.8351 - accuracy: 0.73 - ETA: 1:31 - loss: 0.8322 - accuracy: 0.73 - ETA: 1:29 - loss: 0.8323 - accuracy: 0.73 - ETA: 1:29 - loss: 0.8052 - accuracy: 0.74 - ETA: 1:27 - loss: 0.8002 - accuracy: 0.74 - ETA: 1:26 - loss: 0.7592 - accuracy: 0.75 - ETA: 1:25 - loss: 0.7442 - accuracy: 0.76 - ETA: 1:24 - loss: 0.7600 - accuracy: 0.76 - ETA: 1:23 - loss: 0.7456 - accuracy: 0.76 - ETA: 1:22 - loss: 0.7643 - accuracy: 0.75 - ETA: 1:22 - loss: 0.7732 - accuracy: 0.75 - ETA: 1:21 - loss: 0.7885 - accuracy: 0.75 - ETA: 1:20 - loss: 0.7886 - accuracy: 0.75 - ETA: 1:19 - loss: 0.7839 - accuracy: 0.75 - ETA: 1:18 - loss: 0.7800 - accuracy: 0.75 - ETA: 1:17 - loss: 0.7844 - accuracy: 0.75 - ETA: 1:16 - loss: 0.7776 - accuracy: 0.75 - ETA: 1:15 - loss: 0.7723 - accuracy: 0.75 - ETA: 1:14 - loss: 0.7864 - accuracy: 0.75 - ETA: 1:13 - loss: 0.7860 - accuracy: 0.75 - ETA: 1:12 - loss: 0.7771 - accuracy: 0.75 - ETA: 1:12 - loss: 0.7801 - accuracy: 0.75 - ETA: 1:11 - loss: 0.7756 - accuracy: 0.75 - ETA: 1:10 - loss: 0.7723 - accuracy: 0.75 - ETA: 1:08 - loss: 0.7696 - accuracy: 0.75 - ETA: 1:07 - loss: 0.7670 - accuracy: 0.76 - ETA: 1:07 - loss: 0.7700 - accuracy: 0.76 - ETA: 1:05 - loss: 0.7736 - accuracy: 0.75 - ETA: 1:05 - loss: 0.7777 - accuracy: 0.75 - ETA: 1:03 - loss: 0.7776 - accuracy: 0.75 - ETA: 1:02 - loss: 0.7809 - accuracy: 0.75 - ETA: 1:02 - loss: 0.7802 - accuracy: 0.75 - ETA: 1:01 - loss: 0.7821 - accuracy: 0.75 - ETA: 1:00 - loss: 0.7825 - accuracy: 0.75 - ETA: 59s - loss: 0.7845 - accuracy: 0.7542 - ETA: 58s - loss: 0.7817 - accuracy: 0.754 - ETA: 57s - loss: 0.7788 - accuracy: 0.755 - ETA: 56s - loss: 0.7831 - accuracy: 0.755 - ETA: 55s - loss: 0.7850 - accuracy: 0.755 - ETA: 54s - loss: 0.7807 - accuracy: 0.756 - ETA: 53s - loss: 0.7836 - accuracy: 0.754 - ETA: 52s - loss: 0.7854 - accuracy: 0.755 - ETA: 51s - loss: 0.7902 - accuracy: 0.753 - ETA: 50s - loss: 0.7985 - accuracy: 0.751 - ETA: 49s - loss: 0.7962 - accuracy: 0.753 - ETA: 48s - loss: 0.7922 - accuracy: 0.754 - ETA: 47s - loss: 0.7929 - accuracy: 0.754 - ETA: 47s - loss: 0.7936 - accuracy: 0.755 - ETA: 46s - loss: 0.7923 - accuracy: 0.756 - ETA: 45s - loss: 0.7930 - accuracy: 0.756 - ETA: 44s - loss: 0.7907 - accuracy: 0.757 - ETA: 43s - loss: 0.7904 - accuracy: 0.757 - ETA: 42s - loss: 0.7885 - accuracy: 0.757 - ETA: 41s - loss: 0.7906 - accuracy: 0.757 - ETA: 40s - loss: 0.7895 - accuracy: 0.757 - ETA: 39s - loss: 0.7915 - accuracy: 0.758 - ETA: 38s - loss: 0.7907 - accuracy: 0.758 - ETA: 38s - loss: 0.7905 - accuracy: 0.758 - ETA: 37s - loss: 0.7940 - accuracy: 0.756 - ETA: 36s - loss: 0.7927 - accuracy: 0.756 - ETA: 35s - loss: 0.7908 - accuracy: 0.757 - ETA: 34s - loss: 0.7879 - accuracy: 0.758 - ETA: 33s - loss: 0.7899 - accuracy: 0.758 - ETA: 32s - loss: 0.7881 - accuracy: 0.759 - ETA: 31s - loss: 0.7857 - accuracy: 0.759 - ETA: 30s - loss: 0.7871 - accuracy: 0.759 - ETA: 29s - loss: 0.7870 - accuracy: 0.759 - ETA: 28s - loss: 0.7864 - accuracy: 0.758 - ETA: 27s - loss: 0.7855 - accuracy: 0.759 - ETA: 26s - loss: 0.7847 - accuracy: 0.759 - ETA: 26s - loss: 0.7837 - accuracy: 0.760 - ETA: 25s - loss: 0.7845 - accuracy: 0.759 - ETA: 24s - loss: 0.7848 - accuracy: 0.759 - ETA: 23s - loss: 0.7858 - accuracy: 0.758 - ETA: 22s - loss: 0.7861 - accuracy: 0.758 - ETA: 21s - loss: 0.7865 - accuracy: 0.758 - ETA: 20s - loss: 0.7848 - accuracy: 0.758 - ETA: 19s - loss: 0.7846 - accuracy: 0.758 - ETA: 18s - loss: 0.7828 - accuracy: 0.759 - ETA: 17s - loss: 0.7828 - accuracy: 0.759 - ETA: 17s - loss: 0.7824 - accuracy: 0.759 - ETA: 16s - loss: 0.7819 - accuracy: 0.759 - ETA: 15s - loss: 0.7819 - accuracy: 0.759 - ETA: 14s - loss: 0.7813 - accuracy: 0.760 - ETA: 13s - loss: 0.7812 - accuracy: 0.760 - ETA: 12s - loss: 0.7849 - accuracy: 0.758 - ETA: 11s - loss: 0.7840 - accuracy: 0.759 - ETA: 10s - loss: 0.7833 - accuracy: 0.759 - ETA: 9s - loss: 0.7817 - accuracy: 0.759 - ETA: 8s - loss: 0.7814 - accuracy: 0.75 - ETA: 7s - loss: 0.7798 - accuracy: 0.76 - ETA: 7s - loss: 0.7817 - accuracy: 0.76 - ETA: 6s - loss: 0.7833 - accuracy: 0.75 - ETA: 5s - loss: 0.7834 - accuracy: 0.75 - ETA: 4s - loss: 0.7840 - accuracy: 0.75 - ETA: 3s - loss: 0.7823 - accuracy: 0.76 - ETA: 2s - loss: 0.7822 - accuracy: 0.76 - ETA: 1s - loss: 0.7821 - accuracy: 0.76 - ETA: 0s - loss: 0.7803 - accuracy: 0.76 - 106s 8ms/step - loss: 0.7798 - accuracy: 0.7615 - val_loss: 2.9937 - val_accuracy: 0.3347\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.6043 - accuracy: 0.84 - ETA: 1:31 - loss: 0.6202 - accuracy: 0.82 - ETA: 1:28 - loss: 0.6165 - accuracy: 0.82 - ETA: 1:27 - loss: 0.6502 - accuracy: 0.81 - ETA: 1:25 - loss: 0.6721 - accuracy: 0.80 - ETA: 1:24 - loss: 0.7012 - accuracy: 0.79 - ETA: 1:23 - loss: 0.6897 - accuracy: 0.79 - ETA: 1:23 - loss: 0.6755 - accuracy: 0.79 - ETA: 1:22 - loss: 0.6939 - accuracy: 0.79 - ETA: 1:22 - loss: 0.7232 - accuracy: 0.79 - ETA: 1:21 - loss: 0.7112 - accuracy: 0.79 - ETA: 1:20 - loss: 0.7050 - accuracy: 0.79 - ETA: 1:19 - loss: 0.7124 - accuracy: 0.78 - ETA: 1:18 - loss: 0.7149 - accuracy: 0.78 - ETA: 1:17 - loss: 0.7171 - accuracy: 0.78 - ETA: 1:16 - loss: 0.7148 - accuracy: 0.78 - ETA: 1:15 - loss: 0.7277 - accuracy: 0.78 - ETA: 1:14 - loss: 0.7313 - accuracy: 0.78 - ETA: 1:14 - loss: 0.7334 - accuracy: 0.78 - ETA: 1:12 - loss: 0.7325 - accuracy: 0.78 - ETA: 1:11 - loss: 0.7374 - accuracy: 0.77 - ETA: 1:11 - loss: 0.7326 - accuracy: 0.78 - ETA: 1:10 - loss: 0.7318 - accuracy: 0.78 - ETA: 1:09 - loss: 0.7296 - accuracy: 0.78 - ETA: 1:08 - loss: 0.7319 - accuracy: 0.77 - ETA: 1:07 - loss: 0.7284 - accuracy: 0.78 - ETA: 1:07 - loss: 0.7310 - accuracy: 0.78 - ETA: 1:06 - loss: 0.7329 - accuracy: 0.77 - ETA: 1:05 - loss: 0.7412 - accuracy: 0.77 - ETA: 1:04 - loss: 0.7366 - accuracy: 0.77 - ETA: 1:03 - loss: 0.7385 - accuracy: 0.77 - ETA: 1:02 - loss: 0.7393 - accuracy: 0.77 - ETA: 1:01 - loss: 0.7376 - accuracy: 0.77 - ETA: 1:00 - loss: 0.7392 - accuracy: 0.77 - ETA: 1:00 - loss: 0.7363 - accuracy: 0.77 - ETA: 59s - loss: 0.7360 - accuracy: 0.7754 - ETA: 58s - loss: 0.7438 - accuracy: 0.773 - ETA: 57s - loss: 0.7390 - accuracy: 0.774 - ETA: 56s - loss: 0.7407 - accuracy: 0.774 - ETA: 55s - loss: 0.7467 - accuracy: 0.773 - ETA: 54s - loss: 0.7514 - accuracy: 0.773 - ETA: 53s - loss: 0.7544 - accuracy: 0.772 - ETA: 52s - loss: 0.7545 - accuracy: 0.772 - ETA: 52s - loss: 0.7550 - accuracy: 0.772 - ETA: 51s - loss: 0.7560 - accuracy: 0.772 - ETA: 50s - loss: 0.7539 - accuracy: 0.773 - ETA: 49s - loss: 0.7512 - accuracy: 0.773 - ETA: 48s - loss: 0.7491 - accuracy: 0.772 - ETA: 47s - loss: 0.7484 - accuracy: 0.773 - ETA: 46s - loss: 0.7505 - accuracy: 0.772 - ETA: 45s - loss: 0.7558 - accuracy: 0.772 - ETA: 45s - loss: 0.7568 - accuracy: 0.772 - ETA: 44s - loss: 0.7614 - accuracy: 0.770 - ETA: 43s - loss: 0.7617 - accuracy: 0.770 - ETA: 42s - loss: 0.7626 - accuracy: 0.769 - ETA: 41s - loss: 0.7618 - accuracy: 0.769 - ETA: 40s - loss: 0.7618 - accuracy: 0.769 - ETA: 39s - loss: 0.7622 - accuracy: 0.769 - ETA: 38s - loss: 0.7632 - accuracy: 0.768 - ETA: 37s - loss: 0.7650 - accuracy: 0.767 - ETA: 36s - loss: 0.7671 - accuracy: 0.767 - ETA: 35s - loss: 0.7693 - accuracy: 0.766 - ETA: 35s - loss: 0.7672 - accuracy: 0.766 - ETA: 34s - loss: 0.7650 - accuracy: 0.766 - ETA: 33s - loss: 0.7631 - accuracy: 0.766 - ETA: 32s - loss: 0.7647 - accuracy: 0.766 - ETA: 31s - loss: 0.7668 - accuracy: 0.764 - ETA: 30s - loss: 0.7700 - accuracy: 0.764 - ETA: 29s - loss: 0.7698 - accuracy: 0.764 - ETA: 28s - loss: 0.7693 - accuracy: 0.764 - ETA: 27s - loss: 0.7667 - accuracy: 0.765 - ETA: 26s - loss: 0.7691 - accuracy: 0.764 - ETA: 26s - loss: 0.7672 - accuracy: 0.764 - ETA: 25s - loss: 0.7715 - accuracy: 0.763 - ETA: 24s - loss: 0.7710 - accuracy: 0.764 - ETA: 23s - loss: 0.7703 - accuracy: 0.764 - ETA: 22s - loss: 0.7686 - accuracy: 0.764 - ETA: 21s - loss: 0.7687 - accuracy: 0.765 - ETA: 20s - loss: 0.7683 - accuracy: 0.764 - ETA: 19s - loss: 0.7664 - accuracy: 0.765 - ETA: 18s - loss: 0.7646 - accuracy: 0.765 - ETA: 17s - loss: 0.7648 - accuracy: 0.766 - ETA: 16s - loss: 0.7653 - accuracy: 0.765 - ETA: 16s - loss: 0.7658 - accuracy: 0.765 - ETA: 15s - loss: 0.7666 - accuracy: 0.765 - ETA: 14s - loss: 0.7676 - accuracy: 0.765 - ETA: 13s - loss: 0.7681 - accuracy: 0.764 - ETA: 12s - loss: 0.7661 - accuracy: 0.765 - ETA: 11s - loss: 0.7662 - accuracy: 0.765 - ETA: 10s - loss: 0.7663 - accuracy: 0.765 - ETA: 9s - loss: 0.7672 - accuracy: 0.765 - ETA: 8s - loss: 0.7683 - accuracy: 0.76 - ETA: 7s - loss: 0.7654 - accuracy: 0.76 - ETA: 6s - loss: 0.7638 - accuracy: 0.76 - ETA: 6s - loss: 0.7640 - accuracy: 0.76 - ETA: 5s - loss: 0.7649 - accuracy: 0.76 - ETA: 4s - loss: 0.7624 - accuracy: 0.76 - ETA: 3s - loss: 0.7610 - accuracy: 0.76 - ETA: 2s - loss: 0.7601 - accuracy: 0.76 - ETA: 1s - loss: 0.7596 - accuracy: 0.76 - ETA: 0s - loss: 0.7589 - accuracy: 0.76 - 106s 8ms/step - loss: 0.7584 - accuracy: 0.7682 - val_loss: 2.9593 - val_accuracy: 0.3431\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.9082 - accuracy: 0.71 - ETA: 1:32 - loss: 0.8925 - accuracy: 0.73 - ETA: 1:32 - loss: 0.8758 - accuracy: 0.74 - ETA: 1:30 - loss: 0.8200 - accuracy: 0.75 - ETA: 1:28 - loss: 0.8392 - accuracy: 0.75 - ETA: 1:27 - loss: 0.8282 - accuracy: 0.75 - ETA: 1:26 - loss: 0.8318 - accuracy: 0.75 - ETA: 1:25 - loss: 0.8217 - accuracy: 0.75 - ETA: 1:24 - loss: 0.7969 - accuracy: 0.76 - ETA: 1:23 - loss: 0.7931 - accuracy: 0.76 - ETA: 1:22 - loss: 0.7864 - accuracy: 0.76 - ETA: 1:21 - loss: 0.8004 - accuracy: 0.75 - ETA: 1:20 - loss: 0.8028 - accuracy: 0.76 - ETA: 1:19 - loss: 0.7885 - accuracy: 0.76 - ETA: 1:18 - loss: 0.7986 - accuracy: 0.76 - ETA: 1:17 - loss: 0.7999 - accuracy: 0.76 - ETA: 1:16 - loss: 0.7951 - accuracy: 0.76 - ETA: 1:15 - loss: 0.8090 - accuracy: 0.76 - ETA: 1:14 - loss: 0.8076 - accuracy: 0.75 - ETA: 1:14 - loss: 0.8054 - accuracy: 0.75 - ETA: 1:13 - loss: 0.8194 - accuracy: 0.75 - ETA: 1:12 - loss: 0.8109 - accuracy: 0.75 - ETA: 1:11 - loss: 0.8092 - accuracy: 0.75 - ETA: 1:10 - loss: 0.8037 - accuracy: 0.75 - ETA: 1:09 - loss: 0.8018 - accuracy: 0.75 - ETA: 1:08 - loss: 0.8023 - accuracy: 0.75 - ETA: 1:07 - loss: 0.8033 - accuracy: 0.75 - ETA: 1:06 - loss: 0.7941 - accuracy: 0.75 - ETA: 1:05 - loss: 0.7887 - accuracy: 0.76 - ETA: 1:04 - loss: 0.7931 - accuracy: 0.75 - ETA: 1:03 - loss: 0.7943 - accuracy: 0.75 - ETA: 1:02 - loss: 0.7912 - accuracy: 0.75 - ETA: 1:01 - loss: 0.7889 - accuracy: 0.75 - ETA: 1:00 - loss: 0.7855 - accuracy: 0.76 - ETA: 59s - loss: 0.7846 - accuracy: 0.7627 - ETA: 59s - loss: 0.7795 - accuracy: 0.764 - ETA: 58s - loss: 0.7828 - accuracy: 0.762 - ETA: 57s - loss: 0.7855 - accuracy: 0.761 - ETA: 56s - loss: 0.7879 - accuracy: 0.762 - ETA: 55s - loss: 0.7879 - accuracy: 0.763 - ETA: 54s - loss: 0.7905 - accuracy: 0.763 - ETA: 53s - loss: 0.7926 - accuracy: 0.763 - ETA: 52s - loss: 0.7921 - accuracy: 0.763 - ETA: 51s - loss: 0.7916 - accuracy: 0.763 - ETA: 50s - loss: 0.7904 - accuracy: 0.763 - ETA: 49s - loss: 0.7901 - accuracy: 0.762 - ETA: 49s - loss: 0.7895 - accuracy: 0.762 - ETA: 48s - loss: 0.7859 - accuracy: 0.763 - ETA: 47s - loss: 0.7883 - accuracy: 0.762 - ETA: 46s - loss: 0.7877 - accuracy: 0.762 - ETA: 45s - loss: 0.7850 - accuracy: 0.763 - ETA: 44s - loss: 0.7880 - accuracy: 0.762 - ETA: 43s - loss: 0.7869 - accuracy: 0.763 - ETA: 42s - loss: 0.7856 - accuracy: 0.764 - ETA: 41s - loss: 0.7851 - accuracy: 0.763 - ETA: 41s - loss: 0.7893 - accuracy: 0.763 - ETA: 40s - loss: 0.7868 - accuracy: 0.763 - ETA: 39s - loss: 0.7838 - accuracy: 0.763 - ETA: 38s - loss: 0.7837 - accuracy: 0.763 - ETA: 37s - loss: 0.7813 - accuracy: 0.763 - ETA: 36s - loss: 0.7793 - accuracy: 0.764 - ETA: 35s - loss: 0.7775 - accuracy: 0.764 - ETA: 34s - loss: 0.7766 - accuracy: 0.764 - ETA: 33s - loss: 0.7774 - accuracy: 0.764 - ETA: 32s - loss: 0.7753 - accuracy: 0.765 - ETA: 31s - loss: 0.7749 - accuracy: 0.765 - ETA: 31s - loss: 0.7773 - accuracy: 0.764 - ETA: 30s - loss: 0.7791 - accuracy: 0.764 - ETA: 29s - loss: 0.7770 - accuracy: 0.764 - ETA: 28s - loss: 0.7782 - accuracy: 0.764 - ETA: 27s - loss: 0.7764 - accuracy: 0.764 - ETA: 26s - loss: 0.7765 - accuracy: 0.764 - ETA: 25s - loss: 0.7738 - accuracy: 0.765 - ETA: 24s - loss: 0.7745 - accuracy: 0.764 - ETA: 24s - loss: 0.7719 - accuracy: 0.765 - ETA: 23s - loss: 0.7715 - accuracy: 0.765 - ETA: 22s - loss: 0.7706 - accuracy: 0.765 - ETA: 21s - loss: 0.7733 - accuracy: 0.764 - ETA: 20s - loss: 0.7733 - accuracy: 0.764 - ETA: 19s - loss: 0.7713 - accuracy: 0.764 - ETA: 18s - loss: 0.7714 - accuracy: 0.764 - ETA: 17s - loss: 0.7696 - accuracy: 0.765 - ETA: 16s - loss: 0.7711 - accuracy: 0.765 - ETA: 15s - loss: 0.7702 - accuracy: 0.765 - ETA: 15s - loss: 0.7710 - accuracy: 0.766 - ETA: 14s - loss: 0.7703 - accuracy: 0.766 - ETA: 13s - loss: 0.7681 - accuracy: 0.767 - ETA: 12s - loss: 0.7676 - accuracy: 0.767 - ETA: 11s - loss: 0.7681 - accuracy: 0.767 - ETA: 10s - loss: 0.7664 - accuracy: 0.768 - ETA: 9s - loss: 0.7663 - accuracy: 0.768 - ETA: 8s - loss: 0.7640 - accuracy: 0.76 - ETA: 7s - loss: 0.7647 - accuracy: 0.76 - ETA: 6s - loss: 0.7644 - accuracy: 0.76 - ETA: 6s - loss: 0.7630 - accuracy: 0.76 - ETA: 5s - loss: 0.7613 - accuracy: 0.76 - ETA: 4s - loss: 0.7613 - accuracy: 0.76 - ETA: 3s - loss: 0.7625 - accuracy: 0.76 - ETA: 2s - loss: 0.7607 - accuracy: 0.76 - ETA: 1s - loss: 0.7595 - accuracy: 0.76 - ETA: 0s - loss: 0.7602 - accuracy: 0.76 - 105s 8ms/step - loss: 0.7610 - accuracy: 0.7688 - val_loss: 2.9788 - val_accuracy: 0.3433\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.7679 - accuracy: 0.73 - ETA: 1:31 - loss: 0.6556 - accuracy: 0.79 - ETA: 1:28 - loss: 0.7519 - accuracy: 0.77 - ETA: 1:27 - loss: 0.8129 - accuracy: 0.76 - ETA: 1:26 - loss: 0.8304 - accuracy: 0.75 - ETA: 1:25 - loss: 0.8201 - accuracy: 0.75 - ETA: 1:24 - loss: 0.8447 - accuracy: 0.75 - ETA: 1:24 - loss: 0.8513 - accuracy: 0.74 - ETA: 1:23 - loss: 0.8444 - accuracy: 0.74 - ETA: 1:23 - loss: 0.8504 - accuracy: 0.74 - ETA: 1:22 - loss: 0.8474 - accuracy: 0.74 - ETA: 1:21 - loss: 0.8210 - accuracy: 0.75 - ETA: 1:19 - loss: 0.8299 - accuracy: 0.75 - ETA: 1:18 - loss: 0.8180 - accuracy: 0.75 - ETA: 1:18 - loss: 0.8149 - accuracy: 0.75 - ETA: 1:17 - loss: 0.7973 - accuracy: 0.75 - ETA: 1:16 - loss: 0.7870 - accuracy: 0.75 - ETA: 1:15 - loss: 0.7896 - accuracy: 0.75 - ETA: 1:14 - loss: 0.7854 - accuracy: 0.75 - ETA: 1:13 - loss: 0.7806 - accuracy: 0.75 - ETA: 1:12 - loss: 0.7776 - accuracy: 0.75 - ETA: 1:11 - loss: 0.7762 - accuracy: 0.75 - ETA: 1:10 - loss: 0.7782 - accuracy: 0.75 - ETA: 1:09 - loss: 0.7759 - accuracy: 0.76 - ETA: 1:09 - loss: 0.7804 - accuracy: 0.75 - ETA: 1:08 - loss: 0.7820 - accuracy: 0.75 - ETA: 1:07 - loss: 0.7802 - accuracy: 0.75 - ETA: 1:06 - loss: 0.7804 - accuracy: 0.75 - ETA: 1:06 - loss: 0.7785 - accuracy: 0.76 - ETA: 1:05 - loss: 0.7774 - accuracy: 0.76 - ETA: 1:04 - loss: 0.7792 - accuracy: 0.76 - ETA: 1:03 - loss: 0.7795 - accuracy: 0.76 - ETA: 1:02 - loss: 0.7784 - accuracy: 0.76 - ETA: 1:01 - loss: 0.7751 - accuracy: 0.76 - ETA: 1:00 - loss: 0.7853 - accuracy: 0.76 - ETA: 59s - loss: 0.7850 - accuracy: 0.7622 - ETA: 58s - loss: 0.7809 - accuracy: 0.763 - ETA: 57s - loss: 0.7818 - accuracy: 0.762 - ETA: 56s - loss: 0.7847 - accuracy: 0.760 - ETA: 55s - loss: 0.7869 - accuracy: 0.759 - ETA: 54s - loss: 0.7898 - accuracy: 0.758 - ETA: 53s - loss: 0.7914 - accuracy: 0.758 - ETA: 53s - loss: 0.7928 - accuracy: 0.758 - ETA: 52s - loss: 0.7960 - accuracy: 0.758 - ETA: 51s - loss: 0.7944 - accuracy: 0.758 - ETA: 50s - loss: 0.7926 - accuracy: 0.758 - ETA: 49s - loss: 0.7891 - accuracy: 0.759 - ETA: 48s - loss: 0.7858 - accuracy: 0.760 - ETA: 47s - loss: 0.7883 - accuracy: 0.760 - ETA: 46s - loss: 0.7879 - accuracy: 0.760 - ETA: 45s - loss: 0.7849 - accuracy: 0.760 - ETA: 44s - loss: 0.7849 - accuracy: 0.761 - ETA: 43s - loss: 0.7876 - accuracy: 0.761 - ETA: 43s - loss: 0.7907 - accuracy: 0.760 - ETA: 42s - loss: 0.7913 - accuracy: 0.760 - ETA: 41s - loss: 0.7902 - accuracy: 0.760 - ETA: 40s - loss: 0.7911 - accuracy: 0.760 - ETA: 39s - loss: 0.7872 - accuracy: 0.761 - ETA: 38s - loss: 0.7856 - accuracy: 0.762 - ETA: 37s - loss: 0.7890 - accuracy: 0.761 - ETA: 36s - loss: 0.7893 - accuracy: 0.762 - ETA: 35s - loss: 0.7910 - accuracy: 0.761 - ETA: 34s - loss: 0.7904 - accuracy: 0.761 - ETA: 34s - loss: 0.7892 - accuracy: 0.761 - ETA: 33s - loss: 0.7887 - accuracy: 0.761 - ETA: 32s - loss: 0.7868 - accuracy: 0.761 - ETA: 31s - loss: 0.7846 - accuracy: 0.761 - ETA: 30s - loss: 0.7864 - accuracy: 0.761 - ETA: 29s - loss: 0.7881 - accuracy: 0.760 - ETA: 28s - loss: 0.7857 - accuracy: 0.761 - ETA: 27s - loss: 0.7845 - accuracy: 0.761 - ETA: 26s - loss: 0.7826 - accuracy: 0.762 - ETA: 25s - loss: 0.7810 - accuracy: 0.762 - ETA: 24s - loss: 0.7807 - accuracy: 0.762 - ETA: 24s - loss: 0.7826 - accuracy: 0.762 - ETA: 23s - loss: 0.7822 - accuracy: 0.763 - ETA: 22s - loss: 0.7845 - accuracy: 0.762 - ETA: 21s - loss: 0.7840 - accuracy: 0.762 - ETA: 20s - loss: 0.7824 - accuracy: 0.763 - ETA: 19s - loss: 0.7854 - accuracy: 0.762 - ETA: 18s - loss: 0.7853 - accuracy: 0.762 - ETA: 17s - loss: 0.7845 - accuracy: 0.762 - ETA: 16s - loss: 0.7848 - accuracy: 0.762 - ETA: 15s - loss: 0.7844 - accuracy: 0.762 - ETA: 15s - loss: 0.7848 - accuracy: 0.762 - ETA: 14s - loss: 0.7832 - accuracy: 0.762 - ETA: 13s - loss: 0.7844 - accuracy: 0.761 - ETA: 12s - loss: 0.7824 - accuracy: 0.762 - ETA: 11s - loss: 0.7810 - accuracy: 0.762 - ETA: 10s - loss: 0.7805 - accuracy: 0.762 - ETA: 9s - loss: 0.7808 - accuracy: 0.762 - ETA: 8s - loss: 0.7796 - accuracy: 0.76 - ETA: 7s - loss: 0.7802 - accuracy: 0.76 - ETA: 6s - loss: 0.7811 - accuracy: 0.76 - ETA: 6s - loss: 0.7801 - accuracy: 0.76 - ETA: 5s - loss: 0.7789 - accuracy: 0.76 - ETA: 4s - loss: 0.7762 - accuracy: 0.76 - ETA: 3s - loss: 0.7755 - accuracy: 0.76 - ETA: 2s - loss: 0.7736 - accuracy: 0.76 - ETA: 1s - loss: 0.7738 - accuracy: 0.76 - ETA: 0s - loss: 0.7738 - accuracy: 0.76 - 106s 8ms/step - loss: 0.7754 - accuracy: 0.7625 - val_loss: 2.9161 - val_accuracy: 0.3275\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:31 - loss: 0.6170 - accuracy: 0.81 - ETA: 1:29 - loss: 0.6503 - accuracy: 0.79 - ETA: 1:28 - loss: 0.7325 - accuracy: 0.78 - ETA: 1:27 - loss: 0.7210 - accuracy: 0.78 - ETA: 1:26 - loss: 0.7362 - accuracy: 0.77 - ETA: 1:25 - loss: 0.7502 - accuracy: 0.77 - ETA: 1:24 - loss: 0.7565 - accuracy: 0.76 - ETA: 1:23 - loss: 0.7431 - accuracy: 0.77 - ETA: 1:21 - loss: 0.7569 - accuracy: 0.77 - ETA: 1:20 - loss: 0.7812 - accuracy: 0.77 - ETA: 1:19 - loss: 0.7634 - accuracy: 0.77 - ETA: 1:19 - loss: 0.7524 - accuracy: 0.77 - ETA: 1:17 - loss: 0.7721 - accuracy: 0.77 - ETA: 1:17 - loss: 0.7562 - accuracy: 0.77 - ETA: 1:17 - loss: 0.7568 - accuracy: 0.77 - ETA: 1:16 - loss: 0.7587 - accuracy: 0.77 - ETA: 1:15 - loss: 0.7493 - accuracy: 0.77 - ETA: 1:14 - loss: 0.7591 - accuracy: 0.77 - ETA: 1:13 - loss: 0.7634 - accuracy: 0.77 - ETA: 1:12 - loss: 0.7689 - accuracy: 0.77 - ETA: 1:12 - loss: 0.7697 - accuracy: 0.77 - ETA: 1:11 - loss: 0.7607 - accuracy: 0.77 - ETA: 1:10 - loss: 0.7614 - accuracy: 0.77 - ETA: 1:09 - loss: 0.7656 - accuracy: 0.76 - ETA: 1:08 - loss: 0.7722 - accuracy: 0.76 - ETA: 1:07 - loss: 0.7675 - accuracy: 0.76 - ETA: 1:06 - loss: 0.7686 - accuracy: 0.76 - ETA: 1:05 - loss: 0.7683 - accuracy: 0.76 - ETA: 1:04 - loss: 0.7644 - accuracy: 0.76 - ETA: 1:04 - loss: 0.7653 - accuracy: 0.76 - ETA: 1:03 - loss: 0.7619 - accuracy: 0.76 - ETA: 1:02 - loss: 0.7626 - accuracy: 0.76 - ETA: 1:01 - loss: 0.7658 - accuracy: 0.76 - ETA: 1:00 - loss: 0.7634 - accuracy: 0.76 - ETA: 59s - loss: 0.7639 - accuracy: 0.7681 - ETA: 58s - loss: 0.7626 - accuracy: 0.768 - ETA: 58s - loss: 0.7655 - accuracy: 0.769 - ETA: 57s - loss: 0.7662 - accuracy: 0.769 - ETA: 56s - loss: 0.7633 - accuracy: 0.771 - ETA: 55s - loss: 0.7603 - accuracy: 0.772 - ETA: 54s - loss: 0.7590 - accuracy: 0.772 - ETA: 53s - loss: 0.7601 - accuracy: 0.771 - ETA: 52s - loss: 0.7596 - accuracy: 0.771 - ETA: 51s - loss: 0.7581 - accuracy: 0.770 - ETA: 50s - loss: 0.7570 - accuracy: 0.770 - ETA: 49s - loss: 0.7534 - accuracy: 0.771 - ETA: 48s - loss: 0.7510 - accuracy: 0.771 - ETA: 48s - loss: 0.7536 - accuracy: 0.771 - ETA: 47s - loss: 0.7525 - accuracy: 0.770 - ETA: 46s - loss: 0.7496 - accuracy: 0.771 - ETA: 45s - loss: 0.7508 - accuracy: 0.771 - ETA: 44s - loss: 0.7484 - accuracy: 0.771 - ETA: 43s - loss: 0.7465 - accuracy: 0.771 - ETA: 42s - loss: 0.7460 - accuracy: 0.771 - ETA: 42s - loss: 0.7469 - accuracy: 0.771 - ETA: 41s - loss: 0.7496 - accuracy: 0.770 - ETA: 40s - loss: 0.7493 - accuracy: 0.770 - ETA: 39s - loss: 0.7504 - accuracy: 0.769 - ETA: 38s - loss: 0.7552 - accuracy: 0.768 - ETA: 37s - loss: 0.7526 - accuracy: 0.769 - ETA: 36s - loss: 0.7563 - accuracy: 0.768 - ETA: 35s - loss: 0.7547 - accuracy: 0.768 - ETA: 34s - loss: 0.7549 - accuracy: 0.769 - ETA: 33s - loss: 0.7547 - accuracy: 0.768 - ETA: 33s - loss: 0.7528 - accuracy: 0.768 - ETA: 32s - loss: 0.7531 - accuracy: 0.768 - ETA: 31s - loss: 0.7515 - accuracy: 0.768 - ETA: 30s - loss: 0.7520 - accuracy: 0.769 - ETA: 29s - loss: 0.7522 - accuracy: 0.768 - ETA: 28s - loss: 0.7540 - accuracy: 0.768 - ETA: 27s - loss: 0.7589 - accuracy: 0.767 - ETA: 26s - loss: 0.7590 - accuracy: 0.768 - ETA: 25s - loss: 0.7599 - accuracy: 0.767 - ETA: 24s - loss: 0.7603 - accuracy: 0.767 - ETA: 24s - loss: 0.7606 - accuracy: 0.767 - ETA: 23s - loss: 0.7652 - accuracy: 0.766 - ETA: 22s - loss: 0.7632 - accuracy: 0.766 - ETA: 21s - loss: 0.7617 - accuracy: 0.767 - ETA: 20s - loss: 0.7628 - accuracy: 0.767 - ETA: 19s - loss: 0.7621 - accuracy: 0.767 - ETA: 18s - loss: 0.7606 - accuracy: 0.768 - ETA: 17s - loss: 0.7608 - accuracy: 0.768 - ETA: 16s - loss: 0.7624 - accuracy: 0.768 - ETA: 15s - loss: 0.7620 - accuracy: 0.768 - ETA: 15s - loss: 0.7632 - accuracy: 0.768 - ETA: 14s - loss: 0.7640 - accuracy: 0.767 - ETA: 13s - loss: 0.7635 - accuracy: 0.768 - ETA: 12s - loss: 0.7646 - accuracy: 0.767 - ETA: 11s - loss: 0.7637 - accuracy: 0.767 - ETA: 10s - loss: 0.7647 - accuracy: 0.767 - ETA: 9s - loss: 0.7639 - accuracy: 0.767 - ETA: 8s - loss: 0.7646 - accuracy: 0.76 - ETA: 7s - loss: 0.7638 - accuracy: 0.76 - ETA: 6s - loss: 0.7634 - accuracy: 0.76 - ETA: 6s - loss: 0.7644 - accuracy: 0.76 - ETA: 5s - loss: 0.7641 - accuracy: 0.76 - ETA: 4s - loss: 0.7621 - accuracy: 0.76 - ETA: 3s - loss: 0.7607 - accuracy: 0.76 - ETA: 2s - loss: 0.7615 - accuracy: 0.76 - ETA: 1s - loss: 0.7608 - accuracy: 0.76 - ETA: 0s - loss: 0.7623 - accuracy: 0.76 - 105s 8ms/step - loss: 0.7631 - accuracy: 0.7681 - val_loss: 2.8202 - val_accuracy: 0.3365\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.6533 - accuracy: 0.80 - ETA: 1:28 - loss: 0.6222 - accuracy: 0.80 - ETA: 1:27 - loss: 0.6268 - accuracy: 0.79 - ETA: 1:28 - loss: 0.6465 - accuracy: 0.79 - ETA: 1:28 - loss: 0.6977 - accuracy: 0.78 - ETA: 1:28 - loss: 0.7105 - accuracy: 0.77 - ETA: 1:27 - loss: 0.7047 - accuracy: 0.77 - ETA: 1:25 - loss: 0.7150 - accuracy: 0.77 - ETA: 1:24 - loss: 0.7185 - accuracy: 0.77 - ETA: 1:24 - loss: 0.7224 - accuracy: 0.77 - ETA: 1:23 - loss: 0.7178 - accuracy: 0.77 - ETA: 1:22 - loss: 0.7047 - accuracy: 0.78 - ETA: 1:21 - loss: 0.6957 - accuracy: 0.78 - ETA: 1:21 - loss: 0.6917 - accuracy: 0.78 - ETA: 1:20 - loss: 0.6971 - accuracy: 0.78 - ETA: 1:19 - loss: 0.6902 - accuracy: 0.78 - ETA: 1:18 - loss: 0.6939 - accuracy: 0.78 - ETA: 1:17 - loss: 0.7009 - accuracy: 0.78 - ETA: 1:16 - loss: 0.7025 - accuracy: 0.78 - ETA: 1:15 - loss: 0.7056 - accuracy: 0.78 - ETA: 1:14 - loss: 0.6972 - accuracy: 0.78 - ETA: 1:13 - loss: 0.7025 - accuracy: 0.78 - ETA: 1:12 - loss: 0.7053 - accuracy: 0.78 - ETA: 1:11 - loss: 0.6994 - accuracy: 0.79 - ETA: 1:10 - loss: 0.7083 - accuracy: 0.78 - ETA: 1:09 - loss: 0.7081 - accuracy: 0.78 - ETA: 1:08 - loss: 0.7081 - accuracy: 0.78 - ETA: 1:07 - loss: 0.7076 - accuracy: 0.78 - ETA: 1:06 - loss: 0.7103 - accuracy: 0.78 - ETA: 1:05 - loss: 0.7113 - accuracy: 0.78 - ETA: 1:04 - loss: 0.7160 - accuracy: 0.78 - ETA: 1:03 - loss: 0.7150 - accuracy: 0.78 - ETA: 1:02 - loss: 0.7189 - accuracy: 0.78 - ETA: 1:01 - loss: 0.7188 - accuracy: 0.78 - ETA: 1:00 - loss: 0.7243 - accuracy: 0.78 - ETA: 59s - loss: 0.7241 - accuracy: 0.7804 - ETA: 58s - loss: 0.7291 - accuracy: 0.779 - ETA: 57s - loss: 0.7261 - accuracy: 0.779 - ETA: 57s - loss: 0.7253 - accuracy: 0.780 - ETA: 56s - loss: 0.7253 - accuracy: 0.779 - ETA: 55s - loss: 0.7302 - accuracy: 0.779 - ETA: 54s - loss: 0.7299 - accuracy: 0.779 - ETA: 53s - loss: 0.7325 - accuracy: 0.778 - ETA: 52s - loss: 0.7285 - accuracy: 0.780 - ETA: 51s - loss: 0.7307 - accuracy: 0.780 - ETA: 50s - loss: 0.7345 - accuracy: 0.779 - ETA: 49s - loss: 0.7379 - accuracy: 0.778 - ETA: 48s - loss: 0.7412 - accuracy: 0.777 - ETA: 47s - loss: 0.7434 - accuracy: 0.776 - ETA: 46s - loss: 0.7403 - accuracy: 0.777 - ETA: 45s - loss: 0.7414 - accuracy: 0.777 - ETA: 45s - loss: 0.7446 - accuracy: 0.776 - ETA: 44s - loss: 0.7493 - accuracy: 0.774 - ETA: 43s - loss: 0.7479 - accuracy: 0.775 - ETA: 42s - loss: 0.7461 - accuracy: 0.775 - ETA: 41s - loss: 0.7427 - accuracy: 0.775 - ETA: 40s - loss: 0.7431 - accuracy: 0.775 - ETA: 39s - loss: 0.7424 - accuracy: 0.774 - ETA: 38s - loss: 0.7432 - accuracy: 0.774 - ETA: 37s - loss: 0.7413 - accuracy: 0.774 - ETA: 36s - loss: 0.7401 - accuracy: 0.774 - ETA: 36s - loss: 0.7392 - accuracy: 0.775 - ETA: 35s - loss: 0.7431 - accuracy: 0.774 - ETA: 34s - loss: 0.7436 - accuracy: 0.774 - ETA: 33s - loss: 0.7437 - accuracy: 0.774 - ETA: 32s - loss: 0.7451 - accuracy: 0.773 - ETA: 31s - loss: 0.7428 - accuracy: 0.774 - ETA: 30s - loss: 0.7436 - accuracy: 0.773 - ETA: 29s - loss: 0.7423 - accuracy: 0.774 - ETA: 28s - loss: 0.7432 - accuracy: 0.773 - ETA: 27s - loss: 0.7430 - accuracy: 0.773 - ETA: 26s - loss: 0.7440 - accuracy: 0.772 - ETA: 25s - loss: 0.7496 - accuracy: 0.772 - ETA: 25s - loss: 0.7488 - accuracy: 0.772 - ETA: 24s - loss: 0.7503 - accuracy: 0.771 - ETA: 23s - loss: 0.7528 - accuracy: 0.771 - ETA: 22s - loss: 0.7519 - accuracy: 0.771 - ETA: 21s - loss: 0.7493 - accuracy: 0.771 - ETA: 20s - loss: 0.7511 - accuracy: 0.771 - ETA: 19s - loss: 0.7509 - accuracy: 0.771 - ETA: 18s - loss: 0.7517 - accuracy: 0.771 - ETA: 17s - loss: 0.7504 - accuracy: 0.772 - ETA: 16s - loss: 0.7509 - accuracy: 0.772 - ETA: 16s - loss: 0.7497 - accuracy: 0.773 - ETA: 15s - loss: 0.7471 - accuracy: 0.773 - ETA: 14s - loss: 0.7450 - accuracy: 0.773 - ETA: 13s - loss: 0.7442 - accuracy: 0.773 - ETA: 12s - loss: 0.7441 - accuracy: 0.773 - ETA: 11s - loss: 0.7445 - accuracy: 0.773 - ETA: 10s - loss: 0.7441 - accuracy: 0.773 - ETA: 9s - loss: 0.7448 - accuracy: 0.773 - ETA: 8s - loss: 0.7469 - accuracy: 0.77 - ETA: 7s - loss: 0.7475 - accuracy: 0.77 - ETA: 7s - loss: 0.7463 - accuracy: 0.77 - ETA: 6s - loss: 0.7457 - accuracy: 0.77 - ETA: 5s - loss: 0.7448 - accuracy: 0.77 - ETA: 4s - loss: 0.7429 - accuracy: 0.77 - ETA: 3s - loss: 0.7440 - accuracy: 0.77 - ETA: 2s - loss: 0.7446 - accuracy: 0.77 - ETA: 1s - loss: 0.7460 - accuracy: 0.77 - ETA: 0s - loss: 0.7454 - accuracy: 0.77 - 106s 8ms/step - loss: 0.7452 - accuracy: 0.7721 - val_loss: 3.0777 - val_accuracy: 0.3142\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.8617 - accuracy: 0.74 - ETA: 1:30 - loss: 0.7761 - accuracy: 0.75 - ETA: 1:27 - loss: 0.7641 - accuracy: 0.76 - ETA: 1:27 - loss: 0.7813 - accuracy: 0.75 - ETA: 1:25 - loss: 0.7461 - accuracy: 0.75 - ETA: 1:25 - loss: 0.7786 - accuracy: 0.75 - ETA: 1:24 - loss: 0.8104 - accuracy: 0.75 - ETA: 1:23 - loss: 0.7983 - accuracy: 0.75 - ETA: 1:22 - loss: 0.7756 - accuracy: 0.75 - ETA: 1:21 - loss: 0.7495 - accuracy: 0.76 - ETA: 1:21 - loss: 0.7451 - accuracy: 0.76 - ETA: 1:20 - loss: 0.7633 - accuracy: 0.76 - ETA: 1:20 - loss: 0.7544 - accuracy: 0.76 - ETA: 1:19 - loss: 0.7452 - accuracy: 0.76 - ETA: 1:18 - loss: 0.7384 - accuracy: 0.76 - ETA: 1:17 - loss: 0.7219 - accuracy: 0.77 - ETA: 1:16 - loss: 0.7287 - accuracy: 0.77 - ETA: 1:15 - loss: 0.7245 - accuracy: 0.77 - ETA: 1:14 - loss: 0.7271 - accuracy: 0.77 - ETA: 1:13 - loss: 0.7273 - accuracy: 0.77 - ETA: 1:12 - loss: 0.7281 - accuracy: 0.77 - ETA: 1:11 - loss: 0.7265 - accuracy: 0.77 - ETA: 1:10 - loss: 0.7270 - accuracy: 0.77 - ETA: 1:09 - loss: 0.7228 - accuracy: 0.77 - ETA: 1:08 - loss: 0.7320 - accuracy: 0.77 - ETA: 1:07 - loss: 0.7457 - accuracy: 0.76 - ETA: 1:06 - loss: 0.7417 - accuracy: 0.76 - ETA: 1:05 - loss: 0.7358 - accuracy: 0.77 - ETA: 1:04 - loss: 0.7378 - accuracy: 0.76 - ETA: 1:04 - loss: 0.7322 - accuracy: 0.77 - ETA: 1:03 - loss: 0.7367 - accuracy: 0.77 - ETA: 1:02 - loss: 0.7448 - accuracy: 0.76 - ETA: 1:01 - loss: 0.7432 - accuracy: 0.76 - ETA: 1:00 - loss: 0.7437 - accuracy: 0.76 - ETA: 59s - loss: 0.7443 - accuracy: 0.7694 - ETA: 58s - loss: 0.7386 - accuracy: 0.771 - ETA: 57s - loss: 0.7376 - accuracy: 0.772 - ETA: 56s - loss: 0.7386 - accuracy: 0.772 - ETA: 56s - loss: 0.7368 - accuracy: 0.772 - ETA: 55s - loss: 0.7362 - accuracy: 0.772 - ETA: 54s - loss: 0.7379 - accuracy: 0.771 - ETA: 53s - loss: 0.7337 - accuracy: 0.773 - ETA: 53s - loss: 0.7362 - accuracy: 0.771 - ETA: 52s - loss: 0.7380 - accuracy: 0.771 - ETA: 51s - loss: 0.7370 - accuracy: 0.771 - ETA: 50s - loss: 0.7372 - accuracy: 0.770 - ETA: 49s - loss: 0.7368 - accuracy: 0.769 - ETA: 48s - loss: 0.7358 - accuracy: 0.769 - ETA: 48s - loss: 0.7378 - accuracy: 0.769 - ETA: 47s - loss: 0.7398 - accuracy: 0.769 - ETA: 46s - loss: 0.7416 - accuracy: 0.768 - ETA: 45s - loss: 0.7428 - accuracy: 0.768 - ETA: 44s - loss: 0.7426 - accuracy: 0.768 - ETA: 43s - loss: 0.7426 - accuracy: 0.768 - ETA: 42s - loss: 0.7414 - accuracy: 0.769 - ETA: 41s - loss: 0.7411 - accuracy: 0.769 - ETA: 40s - loss: 0.7398 - accuracy: 0.769 - ETA: 39s - loss: 0.7380 - accuracy: 0.770 - ETA: 39s - loss: 0.7393 - accuracy: 0.770 - ETA: 38s - loss: 0.7370 - accuracy: 0.771 - ETA: 37s - loss: 0.7370 - accuracy: 0.771 - ETA: 36s - loss: 0.7353 - accuracy: 0.771 - ETA: 35s - loss: 0.7352 - accuracy: 0.771 - ETA: 34s - loss: 0.7322 - accuracy: 0.772 - ETA: 33s - loss: 0.7343 - accuracy: 0.772 - ETA: 32s - loss: 0.7340 - accuracy: 0.772 - ETA: 31s - loss: 0.7347 - accuracy: 0.771 - ETA: 30s - loss: 0.7330 - accuracy: 0.772 - ETA: 29s - loss: 0.7293 - accuracy: 0.773 - ETA: 28s - loss: 0.7296 - accuracy: 0.773 - ETA: 28s - loss: 0.7264 - accuracy: 0.774 - ETA: 27s - loss: 0.7258 - accuracy: 0.774 - ETA: 26s - loss: 0.7253 - accuracy: 0.774 - ETA: 25s - loss: 0.7245 - accuracy: 0.775 - ETA: 24s - loss: 0.7240 - accuracy: 0.775 - ETA: 23s - loss: 0.7226 - accuracy: 0.776 - ETA: 22s - loss: 0.7217 - accuracy: 0.776 - ETA: 21s - loss: 0.7206 - accuracy: 0.776 - ETA: 20s - loss: 0.7207 - accuracy: 0.776 - ETA: 19s - loss: 0.7203 - accuracy: 0.775 - ETA: 18s - loss: 0.7205 - accuracy: 0.775 - ETA: 17s - loss: 0.7217 - accuracy: 0.774 - ETA: 17s - loss: 0.7216 - accuracy: 0.774 - ETA: 16s - loss: 0.7188 - accuracy: 0.775 - ETA: 15s - loss: 0.7185 - accuracy: 0.775 - ETA: 14s - loss: 0.7187 - accuracy: 0.775 - ETA: 13s - loss: 0.7198 - accuracy: 0.775 - ETA: 12s - loss: 0.7203 - accuracy: 0.775 - ETA: 11s - loss: 0.7208 - accuracy: 0.775 - ETA: 10s - loss: 0.7212 - accuracy: 0.775 - ETA: 9s - loss: 0.7223 - accuracy: 0.775 - ETA: 8s - loss: 0.7227 - accuracy: 0.77 - ETA: 7s - loss: 0.7241 - accuracy: 0.77 - ETA: 7s - loss: 0.7227 - accuracy: 0.77 - ETA: 6s - loss: 0.7207 - accuracy: 0.77 - ETA: 5s - loss: 0.7187 - accuracy: 0.77 - ETA: 4s - loss: 0.7175 - accuracy: 0.77 - ETA: 3s - loss: 0.7173 - accuracy: 0.77 - ETA: 2s - loss: 0.7173 - accuracy: 0.77 - ETA: 1s - loss: 0.7176 - accuracy: 0.77 - ETA: 0s - loss: 0.7185 - accuracy: 0.77 - 106s 8ms/step - loss: 0.7193 - accuracy: 0.7759 - val_loss: 2.9014 - val_accuracy: 0.3514\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.8147 - accuracy: 0.72 - ETA: 1:33 - loss: 0.7665 - accuracy: 0.75 - ETA: 1:31 - loss: 0.7200 - accuracy: 0.77 - ETA: 1:29 - loss: 0.7106 - accuracy: 0.77 - ETA: 1:27 - loss: 0.7137 - accuracy: 0.77 - ETA: 1:27 - loss: 0.7061 - accuracy: 0.77 - ETA: 1:26 - loss: 0.7255 - accuracy: 0.78 - ETA: 1:24 - loss: 0.7095 - accuracy: 0.78 - ETA: 1:24 - loss: 0.6988 - accuracy: 0.79 - ETA: 1:22 - loss: 0.7017 - accuracy: 0.78 - ETA: 1:22 - loss: 0.7036 - accuracy: 0.78 - ETA: 1:21 - loss: 0.6921 - accuracy: 0.78 - ETA: 1:19 - loss: 0.6819 - accuracy: 0.79 - ETA: 1:19 - loss: 0.6866 - accuracy: 0.79 - ETA: 1:18 - loss: 0.6828 - accuracy: 0.79 - ETA: 1:16 - loss: 0.7013 - accuracy: 0.79 - ETA: 1:16 - loss: 0.7150 - accuracy: 0.78 - ETA: 1:15 - loss: 0.7239 - accuracy: 0.78 - ETA: 1:14 - loss: 0.7216 - accuracy: 0.78 - ETA: 1:13 - loss: 0.7171 - accuracy: 0.78 - ETA: 1:13 - loss: 0.7249 - accuracy: 0.78 - ETA: 1:12 - loss: 0.7269 - accuracy: 0.78 - ETA: 1:11 - loss: 0.7328 - accuracy: 0.77 - ETA: 1:10 - loss: 0.7363 - accuracy: 0.77 - ETA: 1:09 - loss: 0.7429 - accuracy: 0.77 - ETA: 1:08 - loss: 0.7414 - accuracy: 0.77 - ETA: 1:07 - loss: 0.7409 - accuracy: 0.77 - ETA: 1:06 - loss: 0.7383 - accuracy: 0.77 - ETA: 1:05 - loss: 0.7375 - accuracy: 0.77 - ETA: 1:04 - loss: 0.7344 - accuracy: 0.77 - ETA: 1:03 - loss: 0.7327 - accuracy: 0.77 - ETA: 1:02 - loss: 0.7326 - accuracy: 0.77 - ETA: 1:01 - loss: 0.7322 - accuracy: 0.77 - ETA: 1:00 - loss: 0.7349 - accuracy: 0.77 - ETA: 1:00 - loss: 0.7355 - accuracy: 0.77 - ETA: 59s - loss: 0.7294 - accuracy: 0.7780 - ETA: 58s - loss: 0.7340 - accuracy: 0.777 - ETA: 57s - loss: 0.7297 - accuracy: 0.778 - ETA: 56s - loss: 0.7287 - accuracy: 0.778 - ETA: 55s - loss: 0.7262 - accuracy: 0.778 - ETA: 54s - loss: 0.7219 - accuracy: 0.778 - ETA: 53s - loss: 0.7223 - accuracy: 0.778 - ETA: 52s - loss: 0.7201 - accuracy: 0.779 - ETA: 51s - loss: 0.7208 - accuracy: 0.779 - ETA: 51s - loss: 0.7272 - accuracy: 0.777 - ETA: 50s - loss: 0.7297 - accuracy: 0.776 - ETA: 49s - loss: 0.7325 - accuracy: 0.775 - ETA: 48s - loss: 0.7336 - accuracy: 0.776 - ETA: 47s - loss: 0.7353 - accuracy: 0.776 - ETA: 46s - loss: 0.7347 - accuracy: 0.776 - ETA: 45s - loss: 0.7321 - accuracy: 0.777 - ETA: 44s - loss: 0.7345 - accuracy: 0.776 - ETA: 43s - loss: 0.7345 - accuracy: 0.777 - ETA: 43s - loss: 0.7355 - accuracy: 0.776 - ETA: 42s - loss: 0.7337 - accuracy: 0.776 - ETA: 41s - loss: 0.7356 - accuracy: 0.775 - ETA: 40s - loss: 0.7326 - accuracy: 0.777 - ETA: 39s - loss: 0.7308 - accuracy: 0.778 - ETA: 38s - loss: 0.7273 - accuracy: 0.779 - ETA: 37s - loss: 0.7261 - accuracy: 0.779 - ETA: 36s - loss: 0.7262 - accuracy: 0.778 - ETA: 35s - loss: 0.7255 - accuracy: 0.779 - ETA: 34s - loss: 0.7268 - accuracy: 0.778 - ETA: 33s - loss: 0.7283 - accuracy: 0.778 - ETA: 33s - loss: 0.7282 - accuracy: 0.777 - ETA: 32s - loss: 0.7264 - accuracy: 0.777 - ETA: 31s - loss: 0.7274 - accuracy: 0.778 - ETA: 30s - loss: 0.7265 - accuracy: 0.779 - ETA: 29s - loss: 0.7254 - accuracy: 0.779 - ETA: 28s - loss: 0.7250 - accuracy: 0.779 - ETA: 27s - loss: 0.7211 - accuracy: 0.780 - ETA: 26s - loss: 0.7251 - accuracy: 0.779 - ETA: 25s - loss: 0.7263 - accuracy: 0.779 - ETA: 25s - loss: 0.7279 - accuracy: 0.778 - ETA: 24s - loss: 0.7269 - accuracy: 0.778 - ETA: 23s - loss: 0.7303 - accuracy: 0.777 - ETA: 22s - loss: 0.7266 - accuracy: 0.778 - ETA: 21s - loss: 0.7278 - accuracy: 0.777 - ETA: 20s - loss: 0.7268 - accuracy: 0.777 - ETA: 19s - loss: 0.7283 - accuracy: 0.777 - ETA: 18s - loss: 0.7281 - accuracy: 0.777 - ETA: 17s - loss: 0.7281 - accuracy: 0.778 - ETA: 16s - loss: 0.7277 - accuracy: 0.778 - ETA: 15s - loss: 0.7295 - accuracy: 0.777 - ETA: 15s - loss: 0.7286 - accuracy: 0.777 - ETA: 14s - loss: 0.7308 - accuracy: 0.777 - ETA: 13s - loss: 0.7321 - accuracy: 0.777 - ETA: 12s - loss: 0.7330 - accuracy: 0.776 - ETA: 11s - loss: 0.7333 - accuracy: 0.776 - ETA: 10s - loss: 0.7333 - accuracy: 0.776 - ETA: 9s - loss: 0.7337 - accuracy: 0.776 - ETA: 8s - loss: 0.7332 - accuracy: 0.77 - ETA: 7s - loss: 0.7319 - accuracy: 0.77 - ETA: 6s - loss: 0.7319 - accuracy: 0.77 - ETA: 6s - loss: 0.7314 - accuracy: 0.77 - ETA: 5s - loss: 0.7317 - accuracy: 0.77 - ETA: 4s - loss: 0.7307 - accuracy: 0.77 - ETA: 3s - loss: 0.7301 - accuracy: 0.77 - ETA: 2s - loss: 0.7292 - accuracy: 0.77 - ETA: 1s - loss: 0.7296 - accuracy: 0.77 - ETA: 0s - loss: 0.7303 - accuracy: 0.77 - 105s 8ms/step - loss: 0.7292 - accuracy: 0.7772 - val_loss: 3.0435 - val_accuracy: 0.3309\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:28 - loss: 0.7959 - accuracy: 0.72 - ETA: 1:27 - loss: 0.7884 - accuracy: 0.74 - ETA: 1:28 - loss: 0.7725 - accuracy: 0.75 - ETA: 1:27 - loss: 0.7749 - accuracy: 0.76 - ETA: 1:26 - loss: 0.7864 - accuracy: 0.76 - ETA: 1:25 - loss: 0.7268 - accuracy: 0.77 - ETA: 1:24 - loss: 0.7016 - accuracy: 0.78 - ETA: 1:24 - loss: 0.6932 - accuracy: 0.78 - ETA: 1:23 - loss: 0.7017 - accuracy: 0.78 - ETA: 1:22 - loss: 0.6956 - accuracy: 0.78 - ETA: 1:21 - loss: 0.7178 - accuracy: 0.78 - ETA: 1:20 - loss: 0.7186 - accuracy: 0.78 - ETA: 1:19 - loss: 0.7125 - accuracy: 0.78 - ETA: 1:18 - loss: 0.7104 - accuracy: 0.78 - ETA: 1:17 - loss: 0.7013 - accuracy: 0.78 - ETA: 1:16 - loss: 0.6978 - accuracy: 0.78 - ETA: 1:15 - loss: 0.6923 - accuracy: 0.78 - ETA: 1:14 - loss: 0.6987 - accuracy: 0.78 - ETA: 1:13 - loss: 0.7038 - accuracy: 0.78 - ETA: 1:13 - loss: 0.7110 - accuracy: 0.78 - ETA: 1:12 - loss: 0.7070 - accuracy: 0.78 - ETA: 1:11 - loss: 0.7069 - accuracy: 0.78 - ETA: 1:10 - loss: 0.7095 - accuracy: 0.78 - ETA: 1:09 - loss: 0.7159 - accuracy: 0.78 - ETA: 1:08 - loss: 0.7071 - accuracy: 0.78 - ETA: 1:08 - loss: 0.7076 - accuracy: 0.78 - ETA: 1:07 - loss: 0.7021 - accuracy: 0.78 - ETA: 1:06 - loss: 0.7004 - accuracy: 0.78 - ETA: 1:05 - loss: 0.7005 - accuracy: 0.78 - ETA: 1:04 - loss: 0.6991 - accuracy: 0.78 - ETA: 1:03 - loss: 0.7021 - accuracy: 0.78 - ETA: 1:02 - loss: 0.7028 - accuracy: 0.78 - ETA: 1:01 - loss: 0.7060 - accuracy: 0.78 - ETA: 1:00 - loss: 0.7043 - accuracy: 0.78 - ETA: 1:00 - loss: 0.7042 - accuracy: 0.78 - ETA: 59s - loss: 0.7047 - accuracy: 0.7841 - ETA: 58s - loss: 0.7087 - accuracy: 0.783 - ETA: 57s - loss: 0.7082 - accuracy: 0.783 - ETA: 56s - loss: 0.7066 - accuracy: 0.784 - ETA: 55s - loss: 0.7074 - accuracy: 0.785 - ETA: 54s - loss: 0.7092 - accuracy: 0.784 - ETA: 53s - loss: 0.7103 - accuracy: 0.783 - ETA: 52s - loss: 0.7131 - accuracy: 0.782 - ETA: 51s - loss: 0.7090 - accuracy: 0.784 - ETA: 50s - loss: 0.7083 - accuracy: 0.784 - ETA: 50s - loss: 0.7065 - accuracy: 0.786 - ETA: 49s - loss: 0.7113 - accuracy: 0.784 - ETA: 48s - loss: 0.7131 - accuracy: 0.783 - ETA: 47s - loss: 0.7082 - accuracy: 0.785 - ETA: 46s - loss: 0.7061 - accuracy: 0.785 - ETA: 45s - loss: 0.7080 - accuracy: 0.784 - ETA: 44s - loss: 0.7099 - accuracy: 0.784 - ETA: 43s - loss: 0.7079 - accuracy: 0.785 - ETA: 42s - loss: 0.7100 - accuracy: 0.784 - ETA: 41s - loss: 0.7116 - accuracy: 0.783 - ETA: 40s - loss: 0.7126 - accuracy: 0.783 - ETA: 40s - loss: 0.7153 - accuracy: 0.783 - ETA: 39s - loss: 0.7153 - accuracy: 0.783 - ETA: 38s - loss: 0.7135 - accuracy: 0.783 - ETA: 37s - loss: 0.7159 - accuracy: 0.783 - ETA: 36s - loss: 0.7185 - accuracy: 0.782 - ETA: 35s - loss: 0.7151 - accuracy: 0.783 - ETA: 34s - loss: 0.7142 - accuracy: 0.783 - ETA: 33s - loss: 0.7159 - accuracy: 0.783 - ETA: 32s - loss: 0.7132 - accuracy: 0.784 - ETA: 32s - loss: 0.7148 - accuracy: 0.784 - ETA: 31s - loss: 0.7172 - accuracy: 0.783 - ETA: 30s - loss: 0.7195 - accuracy: 0.782 - ETA: 29s - loss: 0.7198 - accuracy: 0.782 - ETA: 28s - loss: 0.7215 - accuracy: 0.781 - ETA: 27s - loss: 0.7224 - accuracy: 0.781 - ETA: 26s - loss: 0.7224 - accuracy: 0.781 - ETA: 25s - loss: 0.7229 - accuracy: 0.781 - ETA: 24s - loss: 0.7225 - accuracy: 0.781 - ETA: 23s - loss: 0.7213 - accuracy: 0.781 - ETA: 23s - loss: 0.7211 - accuracy: 0.781 - ETA: 22s - loss: 0.7216 - accuracy: 0.780 - ETA: 21s - loss: 0.7243 - accuracy: 0.780 - ETA: 20s - loss: 0.7227 - accuracy: 0.780 - ETA: 19s - loss: 0.7207 - accuracy: 0.781 - ETA: 18s - loss: 0.7220 - accuracy: 0.781 - ETA: 17s - loss: 0.7213 - accuracy: 0.781 - ETA: 16s - loss: 0.7226 - accuracy: 0.781 - ETA: 15s - loss: 0.7221 - accuracy: 0.781 - ETA: 14s - loss: 0.7208 - accuracy: 0.781 - ETA: 14s - loss: 0.7200 - accuracy: 0.782 - ETA: 13s - loss: 0.7187 - accuracy: 0.782 - ETA: 12s - loss: 0.7185 - accuracy: 0.782 - ETA: 11s - loss: 0.7168 - accuracy: 0.782 - ETA: 10s - loss: 0.7167 - accuracy: 0.782 - ETA: 9s - loss: 0.7169 - accuracy: 0.782 - ETA: 8s - loss: 0.7156 - accuracy: 0.78 - ETA: 7s - loss: 0.7168 - accuracy: 0.78 - ETA: 6s - loss: 0.7185 - accuracy: 0.78 - ETA: 6s - loss: 0.7178 - accuracy: 0.78 - ETA: 5s - loss: 0.7167 - accuracy: 0.78 - ETA: 4s - loss: 0.7151 - accuracy: 0.78 - ETA: 3s - loss: 0.7139 - accuracy: 0.78 - ETA: 2s - loss: 0.7151 - accuracy: 0.78 - ETA: 1s - loss: 0.7147 - accuracy: 0.78 - ETA: 0s - loss: 0.7166 - accuracy: 0.78 - 105s 8ms/step - loss: 0.7172 - accuracy: 0.7822 - val_loss: 2.9899 - val_accuracy: 0.3267\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:34 - loss: 0.6931 - accuracy: 0.77 - ETA: 1:30 - loss: 0.7630 - accuracy: 0.76 - ETA: 1:29 - loss: 0.6885 - accuracy: 0.78 - ETA: 1:28 - loss: 0.7085 - accuracy: 0.78 - ETA: 1:27 - loss: 0.7560 - accuracy: 0.77 - ETA: 1:27 - loss: 0.7470 - accuracy: 0.77 - ETA: 1:25 - loss: 0.7553 - accuracy: 0.77 - ETA: 1:24 - loss: 0.7584 - accuracy: 0.77 - ETA: 1:23 - loss: 0.7607 - accuracy: 0.77 - ETA: 1:22 - loss: 0.7621 - accuracy: 0.77 - ETA: 1:21 - loss: 0.7584 - accuracy: 0.77 - ETA: 1:20 - loss: 0.7728 - accuracy: 0.77 - ETA: 1:19 - loss: 0.7650 - accuracy: 0.77 - ETA: 1:18 - loss: 0.7542 - accuracy: 0.77 - ETA: 1:17 - loss: 0.7458 - accuracy: 0.77 - ETA: 1:17 - loss: 0.7475 - accuracy: 0.77 - ETA: 1:16 - loss: 0.7495 - accuracy: 0.77 - ETA: 1:15 - loss: 0.7566 - accuracy: 0.77 - ETA: 1:14 - loss: 0.7561 - accuracy: 0.77 - ETA: 1:13 - loss: 0.7615 - accuracy: 0.77 - ETA: 1:12 - loss: 0.7545 - accuracy: 0.77 - ETA: 1:11 - loss: 0.7486 - accuracy: 0.77 - ETA: 1:10 - loss: 0.7484 - accuracy: 0.77 - ETA: 1:10 - loss: 0.7401 - accuracy: 0.77 - ETA: 1:09 - loss: 0.7348 - accuracy: 0.77 - ETA: 1:08 - loss: 0.7476 - accuracy: 0.77 - ETA: 1:07 - loss: 0.7544 - accuracy: 0.76 - ETA: 1:06 - loss: 0.7556 - accuracy: 0.76 - ETA: 1:05 - loss: 0.7534 - accuracy: 0.76 - ETA: 1:04 - loss: 0.7488 - accuracy: 0.76 - ETA: 1:03 - loss: 0.7509 - accuracy: 0.76 - ETA: 1:02 - loss: 0.7496 - accuracy: 0.76 - ETA: 1:02 - loss: 0.7473 - accuracy: 0.76 - ETA: 1:01 - loss: 0.7493 - accuracy: 0.76 - ETA: 1:00 - loss: 0.7433 - accuracy: 0.77 - ETA: 59s - loss: 0.7361 - accuracy: 0.7724 - ETA: 58s - loss: 0.7350 - accuracy: 0.773 - ETA: 57s - loss: 0.7317 - accuracy: 0.774 - ETA: 56s - loss: 0.7280 - accuracy: 0.775 - ETA: 55s - loss: 0.7246 - accuracy: 0.775 - ETA: 54s - loss: 0.7219 - accuracy: 0.777 - ETA: 54s - loss: 0.7256 - accuracy: 0.775 - ETA: 53s - loss: 0.7209 - accuracy: 0.776 - ETA: 52s - loss: 0.7197 - accuracy: 0.777 - ETA: 51s - loss: 0.7204 - accuracy: 0.778 - ETA: 50s - loss: 0.7221 - accuracy: 0.778 - ETA: 49s - loss: 0.7242 - accuracy: 0.778 - ETA: 48s - loss: 0.7222 - accuracy: 0.779 - ETA: 47s - loss: 0.7207 - accuracy: 0.779 - ETA: 46s - loss: 0.7194 - accuracy: 0.779 - ETA: 45s - loss: 0.7176 - accuracy: 0.779 - ETA: 44s - loss: 0.7151 - accuracy: 0.781 - ETA: 43s - loss: 0.7138 - accuracy: 0.782 - ETA: 43s - loss: 0.7182 - accuracy: 0.781 - ETA: 42s - loss: 0.7188 - accuracy: 0.780 - ETA: 41s - loss: 0.7184 - accuracy: 0.781 - ETA: 40s - loss: 0.7191 - accuracy: 0.780 - ETA: 39s - loss: 0.7221 - accuracy: 0.780 - ETA: 38s - loss: 0.7227 - accuracy: 0.779 - ETA: 37s - loss: 0.7237 - accuracy: 0.779 - ETA: 36s - loss: 0.7244 - accuracy: 0.778 - ETA: 35s - loss: 0.7252 - accuracy: 0.778 - ETA: 35s - loss: 0.7286 - accuracy: 0.777 - ETA: 34s - loss: 0.7272 - accuracy: 0.777 - ETA: 33s - loss: 0.7260 - accuracy: 0.777 - ETA: 32s - loss: 0.7258 - accuracy: 0.778 - ETA: 31s - loss: 0.7258 - accuracy: 0.779 - ETA: 30s - loss: 0.7277 - accuracy: 0.778 - ETA: 29s - loss: 0.7287 - accuracy: 0.778 - ETA: 28s - loss: 0.7302 - accuracy: 0.778 - ETA: 27s - loss: 0.7300 - accuracy: 0.778 - ETA: 26s - loss: 0.7308 - accuracy: 0.777 - ETA: 25s - loss: 0.7301 - accuracy: 0.778 - ETA: 25s - loss: 0.7326 - accuracy: 0.778 - ETA: 24s - loss: 0.7333 - accuracy: 0.778 - ETA: 23s - loss: 0.7351 - accuracy: 0.777 - ETA: 22s - loss: 0.7366 - accuracy: 0.777 - ETA: 21s - loss: 0.7386 - accuracy: 0.776 - ETA: 20s - loss: 0.7399 - accuracy: 0.776 - ETA: 19s - loss: 0.7380 - accuracy: 0.777 - ETA: 18s - loss: 0.7397 - accuracy: 0.776 - ETA: 17s - loss: 0.7361 - accuracy: 0.777 - ETA: 16s - loss: 0.7357 - accuracy: 0.777 - ETA: 16s - loss: 0.7368 - accuracy: 0.777 - ETA: 15s - loss: 0.7379 - accuracy: 0.776 - ETA: 14s - loss: 0.7370 - accuracy: 0.777 - ETA: 13s - loss: 0.7353 - accuracy: 0.777 - ETA: 12s - loss: 0.7356 - accuracy: 0.777 - ETA: 11s - loss: 0.7377 - accuracy: 0.777 - ETA: 10s - loss: 0.7372 - accuracy: 0.777 - ETA: 9s - loss: 0.7367 - accuracy: 0.777 - ETA: 8s - loss: 0.7359 - accuracy: 0.77 - ETA: 7s - loss: 0.7349 - accuracy: 0.77 - ETA: 6s - loss: 0.7359 - accuracy: 0.77 - ETA: 6s - loss: 0.7381 - accuracy: 0.77 - ETA: 5s - loss: 0.7389 - accuracy: 0.77 - ETA: 4s - loss: 0.7399 - accuracy: 0.77 - ETA: 3s - loss: 0.7388 - accuracy: 0.77 - ETA: 2s - loss: 0.7398 - accuracy: 0.77 - ETA: 1s - loss: 0.7388 - accuracy: 0.77 - ETA: 0s - loss: 0.7388 - accuracy: 0.77 - 105s 8ms/step - loss: 0.7384 - accuracy: 0.7764 - val_loss: 2.9630 - val_accuracy: 0.3153\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.7053 - accuracy: 0.75 - ETA: 1:27 - loss: 0.7147 - accuracy: 0.77 - ETA: 1:26 - loss: 0.7359 - accuracy: 0.78 - ETA: 1:26 - loss: 0.7582 - accuracy: 0.78 - ETA: 1:25 - loss: 0.7407 - accuracy: 0.79 - ETA: 1:25 - loss: 0.7336 - accuracy: 0.79 - ETA: 1:24 - loss: 0.7102 - accuracy: 0.80 - ETA: 1:24 - loss: 0.7024 - accuracy: 0.80 - ETA: 1:22 - loss: 0.7145 - accuracy: 0.79 - ETA: 1:22 - loss: 0.6993 - accuracy: 0.79 - ETA: 1:21 - loss: 0.6980 - accuracy: 0.79 - ETA: 1:21 - loss: 0.7026 - accuracy: 0.79 - ETA: 1:20 - loss: 0.7031 - accuracy: 0.79 - ETA: 1:20 - loss: 0.7027 - accuracy: 0.79 - ETA: 1:19 - loss: 0.7080 - accuracy: 0.79 - ETA: 1:18 - loss: 0.7059 - accuracy: 0.79 - ETA: 1:17 - loss: 0.7151 - accuracy: 0.78 - ETA: 1:16 - loss: 0.7309 - accuracy: 0.78 - ETA: 1:15 - loss: 0.7247 - accuracy: 0.78 - ETA: 1:14 - loss: 0.7194 - accuracy: 0.78 - ETA: 1:13 - loss: 0.7242 - accuracy: 0.78 - ETA: 1:12 - loss: 0.7334 - accuracy: 0.77 - ETA: 1:11 - loss: 0.7280 - accuracy: 0.78 - ETA: 1:10 - loss: 0.7287 - accuracy: 0.78 - ETA: 1:09 - loss: 0.7255 - accuracy: 0.78 - ETA: 1:08 - loss: 0.7345 - accuracy: 0.78 - ETA: 1:07 - loss: 0.7372 - accuracy: 0.78 - ETA: 1:06 - loss: 0.7346 - accuracy: 0.78 - ETA: 1:05 - loss: 0.7342 - accuracy: 0.78 - ETA: 1:05 - loss: 0.7380 - accuracy: 0.77 - ETA: 1:04 - loss: 0.7338 - accuracy: 0.77 - ETA: 1:03 - loss: 0.7302 - accuracy: 0.78 - ETA: 1:02 - loss: 0.7313 - accuracy: 0.77 - ETA: 1:01 - loss: 0.7321 - accuracy: 0.77 - ETA: 1:00 - loss: 0.7304 - accuracy: 0.77 - ETA: 59s - loss: 0.7286 - accuracy: 0.7778 - ETA: 58s - loss: 0.7390 - accuracy: 0.775 - ETA: 57s - loss: 0.7399 - accuracy: 0.774 - ETA: 56s - loss: 0.7433 - accuracy: 0.773 - ETA: 55s - loss: 0.7438 - accuracy: 0.772 - ETA: 54s - loss: 0.7505 - accuracy: 0.770 - ETA: 54s - loss: 0.7479 - accuracy: 0.771 - ETA: 53s - loss: 0.7467 - accuracy: 0.771 - ETA: 52s - loss: 0.7416 - accuracy: 0.772 - ETA: 51s - loss: 0.7392 - accuracy: 0.773 - ETA: 50s - loss: 0.7400 - accuracy: 0.772 - ETA: 49s - loss: 0.7382 - accuracy: 0.773 - ETA: 48s - loss: 0.7432 - accuracy: 0.772 - ETA: 47s - loss: 0.7458 - accuracy: 0.771 - ETA: 46s - loss: 0.7451 - accuracy: 0.771 - ETA: 45s - loss: 0.7467 - accuracy: 0.772 - ETA: 44s - loss: 0.7459 - accuracy: 0.772 - ETA: 43s - loss: 0.7435 - accuracy: 0.773 - ETA: 42s - loss: 0.7418 - accuracy: 0.773 - ETA: 42s - loss: 0.7408 - accuracy: 0.774 - ETA: 41s - loss: 0.7388 - accuracy: 0.774 - ETA: 40s - loss: 0.7392 - accuracy: 0.774 - ETA: 39s - loss: 0.7416 - accuracy: 0.773 - ETA: 38s - loss: 0.7404 - accuracy: 0.773 - ETA: 37s - loss: 0.7397 - accuracy: 0.774 - ETA: 36s - loss: 0.7410 - accuracy: 0.773 - ETA: 35s - loss: 0.7408 - accuracy: 0.773 - ETA: 34s - loss: 0.7400 - accuracy: 0.774 - ETA: 34s - loss: 0.7442 - accuracy: 0.773 - ETA: 33s - loss: 0.7434 - accuracy: 0.773 - ETA: 32s - loss: 0.7423 - accuracy: 0.774 - ETA: 31s - loss: 0.7421 - accuracy: 0.774 - ETA: 30s - loss: 0.7411 - accuracy: 0.774 - ETA: 29s - loss: 0.7426 - accuracy: 0.774 - ETA: 28s - loss: 0.7408 - accuracy: 0.774 - ETA: 27s - loss: 0.7411 - accuracy: 0.774 - ETA: 26s - loss: 0.7402 - accuracy: 0.775 - ETA: 25s - loss: 0.7396 - accuracy: 0.775 - ETA: 24s - loss: 0.7365 - accuracy: 0.776 - ETA: 24s - loss: 0.7372 - accuracy: 0.775 - ETA: 23s - loss: 0.7372 - accuracy: 0.775 - ETA: 22s - loss: 0.7410 - accuracy: 0.774 - ETA: 21s - loss: 0.7409 - accuracy: 0.775 - ETA: 20s - loss: 0.7408 - accuracy: 0.775 - ETA: 19s - loss: 0.7419 - accuracy: 0.775 - ETA: 18s - loss: 0.7416 - accuracy: 0.775 - ETA: 17s - loss: 0.7409 - accuracy: 0.775 - ETA: 16s - loss: 0.7427 - accuracy: 0.774 - ETA: 16s - loss: 0.7443 - accuracy: 0.774 - ETA: 15s - loss: 0.7427 - accuracy: 0.774 - ETA: 14s - loss: 0.7449 - accuracy: 0.774 - ETA: 13s - loss: 0.7427 - accuracy: 0.775 - ETA: 12s - loss: 0.7406 - accuracy: 0.775 - ETA: 11s - loss: 0.7422 - accuracy: 0.775 - ETA: 10s - loss: 0.7394 - accuracy: 0.776 - ETA: 9s - loss: 0.7393 - accuracy: 0.776 - ETA: 8s - loss: 0.7379 - accuracy: 0.77 - ETA: 7s - loss: 0.7375 - accuracy: 0.77 - ETA: 7s - loss: 0.7364 - accuracy: 0.77 - ETA: 6s - loss: 0.7349 - accuracy: 0.77 - ETA: 5s - loss: 0.7333 - accuracy: 0.77 - ETA: 4s - loss: 0.7312 - accuracy: 0.77 - ETA: 3s - loss: 0.7318 - accuracy: 0.77 - ETA: 2s - loss: 0.7309 - accuracy: 0.77 - ETA: 1s - loss: 0.7293 - accuracy: 0.78 - ETA: 0s - loss: 0.7279 - accuracy: 0.78 - 106s 8ms/step - loss: 0.7308 - accuracy: 0.7792 - val_loss: 3.1476 - val_accuracy: 0.3191\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.7267 - accuracy: 0.78 - ETA: 1:31 - loss: 0.8677 - accuracy: 0.75 - ETA: 1:30 - loss: 0.7847 - accuracy: 0.76 - ETA: 1:29 - loss: 0.7819 - accuracy: 0.77 - ETA: 1:28 - loss: 0.7771 - accuracy: 0.77 - ETA: 1:26 - loss: 0.7845 - accuracy: 0.77 - ETA: 1:25 - loss: 0.7713 - accuracy: 0.77 - ETA: 1:24 - loss: 0.7898 - accuracy: 0.76 - ETA: 1:23 - loss: 0.7717 - accuracy: 0.77 - ETA: 1:22 - loss: 0.7557 - accuracy: 0.77 - ETA: 1:22 - loss: 0.7564 - accuracy: 0.77 - ETA: 1:21 - loss: 0.7452 - accuracy: 0.78 - ETA: 1:20 - loss: 0.7441 - accuracy: 0.77 - ETA: 1:20 - loss: 0.7402 - accuracy: 0.77 - ETA: 1:19 - loss: 0.7269 - accuracy: 0.78 - ETA: 1:18 - loss: 0.7143 - accuracy: 0.78 - ETA: 1:17 - loss: 0.7140 - accuracy: 0.78 - ETA: 1:16 - loss: 0.7117 - accuracy: 0.78 - ETA: 1:15 - loss: 0.7153 - accuracy: 0.78 - ETA: 1:14 - loss: 0.7249 - accuracy: 0.78 - ETA: 1:12 - loss: 0.7230 - accuracy: 0.78 - ETA: 1:12 - loss: 0.7246 - accuracy: 0.78 - ETA: 1:11 - loss: 0.7254 - accuracy: 0.78 - ETA: 1:10 - loss: 0.7249 - accuracy: 0.78 - ETA: 1:09 - loss: 0.7331 - accuracy: 0.78 - ETA: 1:08 - loss: 0.7330 - accuracy: 0.78 - ETA: 1:07 - loss: 0.7252 - accuracy: 0.78 - ETA: 1:06 - loss: 0.7269 - accuracy: 0.78 - ETA: 1:04 - loss: 0.7309 - accuracy: 0.78 - ETA: 1:03 - loss: 0.7265 - accuracy: 0.78 - ETA: 1:02 - loss: 0.7263 - accuracy: 0.78 - ETA: 1:00 - loss: 0.7218 - accuracy: 0.78 - ETA: 59s - loss: 0.7211 - accuracy: 0.7850 - ETA: 58s - loss: 0.7191 - accuracy: 0.785 - ETA: 56s - loss: 0.7201 - accuracy: 0.785 - ETA: 55s - loss: 0.7164 - accuracy: 0.786 - ETA: 54s - loss: 0.7106 - accuracy: 0.788 - ETA: 53s - loss: 0.7126 - accuracy: 0.787 - ETA: 53s - loss: 0.7162 - accuracy: 0.786 - ETA: 52s - loss: 0.7143 - accuracy: 0.786 - ETA: 51s - loss: 0.7174 - accuracy: 0.786 - ETA: 51s - loss: 0.7179 - accuracy: 0.784 - ETA: 50s - loss: 0.7168 - accuracy: 0.784 - ETA: 49s - loss: 0.7166 - accuracy: 0.785 - ETA: 48s - loss: 0.7166 - accuracy: 0.784 - ETA: 47s - loss: 0.7168 - accuracy: 0.785 - ETA: 46s - loss: 0.7155 - accuracy: 0.785 - ETA: 45s - loss: 0.7152 - accuracy: 0.785 - ETA: 43s - loss: 0.7128 - accuracy: 0.785 - ETA: 42s - loss: 0.7094 - accuracy: 0.786 - ETA: 41s - loss: 0.7091 - accuracy: 0.786 - ETA: 40s - loss: 0.7080 - accuracy: 0.786 - ETA: 40s - loss: 0.7075 - accuracy: 0.786 - ETA: 39s - loss: 0.7073 - accuracy: 0.786 - ETA: 38s - loss: 0.7088 - accuracy: 0.786 - ETA: 38s - loss: 0.7100 - accuracy: 0.787 - ETA: 37s - loss: 0.7129 - accuracy: 0.786 - ETA: 36s - loss: 0.7122 - accuracy: 0.786 - ETA: 35s - loss: 0.7119 - accuracy: 0.785 - ETA: 34s - loss: 0.7098 - accuracy: 0.785 - ETA: 33s - loss: 0.7092 - accuracy: 0.786 - ETA: 33s - loss: 0.7083 - accuracy: 0.785 - ETA: 32s - loss: 0.7069 - accuracy: 0.786 - ETA: 31s - loss: 0.7047 - accuracy: 0.786 - ETA: 30s - loss: 0.7045 - accuracy: 0.786 - ETA: 29s - loss: 0.7040 - accuracy: 0.786 - ETA: 29s - loss: 0.7056 - accuracy: 0.786 - ETA: 28s - loss: 0.7060 - accuracy: 0.786 - ETA: 27s - loss: 0.7064 - accuracy: 0.785 - ETA: 26s - loss: 0.7079 - accuracy: 0.785 - ETA: 25s - loss: 0.7092 - accuracy: 0.785 - ETA: 25s - loss: 0.7075 - accuracy: 0.785 - ETA: 24s - loss: 0.7075 - accuracy: 0.785 - ETA: 23s - loss: 0.7080 - accuracy: 0.785 - ETA: 22s - loss: 0.7068 - accuracy: 0.785 - ETA: 21s - loss: 0.7084 - accuracy: 0.785 - ETA: 20s - loss: 0.7072 - accuracy: 0.785 - ETA: 20s - loss: 0.7082 - accuracy: 0.784 - ETA: 19s - loss: 0.7073 - accuracy: 0.785 - ETA: 18s - loss: 0.7047 - accuracy: 0.786 - ETA: 17s - loss: 0.7040 - accuracy: 0.786 - ETA: 16s - loss: 0.7065 - accuracy: 0.786 - ETA: 15s - loss: 0.7062 - accuracy: 0.786 - ETA: 15s - loss: 0.7049 - accuracy: 0.786 - ETA: 14s - loss: 0.7064 - accuracy: 0.785 - ETA: 13s - loss: 0.7053 - accuracy: 0.786 - ETA: 12s - loss: 0.7043 - accuracy: 0.785 - ETA: 11s - loss: 0.7029 - accuracy: 0.786 - ETA: 10s - loss: 0.7012 - accuracy: 0.787 - ETA: 10s - loss: 0.7052 - accuracy: 0.786 - ETA: 9s - loss: 0.7041 - accuracy: 0.786 - ETA: 8s - loss: 0.7053 - accuracy: 0.78 - ETA: 7s - loss: 0.7068 - accuracy: 0.78 - ETA: 6s - loss: 0.7111 - accuracy: 0.78 - ETA: 5s - loss: 0.7120 - accuracy: 0.78 - ETA: 4s - loss: 0.7112 - accuracy: 0.78 - ETA: 4s - loss: 0.7108 - accuracy: 0.78 - ETA: 3s - loss: 0.7092 - accuracy: 0.78 - ETA: 2s - loss: 0.7089 - accuracy: 0.78 - ETA: 1s - loss: 0.7080 - accuracy: 0.78 - ETA: 0s - loss: 0.7075 - accuracy: 0.78 - 101s 8ms/step - loss: 0.7075 - accuracy: 0.7853 - val_loss: 3.0470 - val_accuracy: 0.3369\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:37 - loss: 0.7446 - accuracy: 0.76 - ETA: 1:35 - loss: 0.6798 - accuracy: 0.78 - ETA: 1:34 - loss: 0.7095 - accuracy: 0.76 - ETA: 1:34 - loss: 0.7180 - accuracy: 0.76 - ETA: 1:31 - loss: 0.7204 - accuracy: 0.76 - ETA: 1:31 - loss: 0.6965 - accuracy: 0.77 - ETA: 1:30 - loss: 0.6783 - accuracy: 0.77 - ETA: 1:29 - loss: 0.6967 - accuracy: 0.77 - ETA: 1:27 - loss: 0.7096 - accuracy: 0.77 - ETA: 1:26 - loss: 0.7127 - accuracy: 0.77 - ETA: 1:25 - loss: 0.7187 - accuracy: 0.76 - ETA: 1:24 - loss: 0.7151 - accuracy: 0.76 - ETA: 1:22 - loss: 0.7124 - accuracy: 0.77 - ETA: 1:21 - loss: 0.6953 - accuracy: 0.78 - ETA: 1:20 - loss: 0.6893 - accuracy: 0.78 - ETA: 1:19 - loss: 0.6935 - accuracy: 0.78 - ETA: 1:18 - loss: 0.6814 - accuracy: 0.78 - ETA: 1:16 - loss: 0.6730 - accuracy: 0.79 - ETA: 1:15 - loss: 0.6703 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6736 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6794 - accuracy: 0.79 - ETA: 1:13 - loss: 0.6786 - accuracy: 0.79 - ETA: 1:12 - loss: 0.6802 - accuracy: 0.78 - ETA: 1:11 - loss: 0.6844 - accuracy: 0.78 - ETA: 1:10 - loss: 0.6785 - accuracy: 0.79 - ETA: 1:09 - loss: 0.6769 - accuracy: 0.79 - ETA: 1:08 - loss: 0.6867 - accuracy: 0.78 - ETA: 1:07 - loss: 0.6892 - accuracy: 0.78 - ETA: 1:06 - loss: 0.6940 - accuracy: 0.78 - ETA: 1:05 - loss: 0.6979 - accuracy: 0.78 - ETA: 1:04 - loss: 0.6933 - accuracy: 0.78 - ETA: 1:03 - loss: 0.6946 - accuracy: 0.78 - ETA: 1:02 - loss: 0.6924 - accuracy: 0.78 - ETA: 1:01 - loss: 0.6899 - accuracy: 0.78 - ETA: 1:01 - loss: 0.6917 - accuracy: 0.78 - ETA: 1:00 - loss: 0.6866 - accuracy: 0.79 - ETA: 59s - loss: 0.6819 - accuracy: 0.7920 - ETA: 58s - loss: 0.6885 - accuracy: 0.790 - ETA: 57s - loss: 0.6901 - accuracy: 0.789 - ETA: 56s - loss: 0.6908 - accuracy: 0.790 - ETA: 55s - loss: 0.6899 - accuracy: 0.790 - ETA: 54s - loss: 0.6876 - accuracy: 0.789 - ETA: 53s - loss: 0.6876 - accuracy: 0.789 - ETA: 52s - loss: 0.6953 - accuracy: 0.786 - ETA: 51s - loss: 0.6943 - accuracy: 0.786 - ETA: 50s - loss: 0.6949 - accuracy: 0.785 - ETA: 50s - loss: 0.6982 - accuracy: 0.785 - ETA: 48s - loss: 0.6986 - accuracy: 0.785 - ETA: 47s - loss: 0.7013 - accuracy: 0.784 - ETA: 46s - loss: 0.7034 - accuracy: 0.784 - ETA: 46s - loss: 0.7035 - accuracy: 0.784 - ETA: 45s - loss: 0.7043 - accuracy: 0.783 - ETA: 44s - loss: 0.7078 - accuracy: 0.781 - ETA: 43s - loss: 0.7083 - accuracy: 0.780 - ETA: 42s - loss: 0.7096 - accuracy: 0.780 - ETA: 41s - loss: 0.7080 - accuracy: 0.780 - ETA: 40s - loss: 0.7067 - accuracy: 0.781 - ETA: 39s - loss: 0.7059 - accuracy: 0.781 - ETA: 38s - loss: 0.7040 - accuracy: 0.782 - ETA: 37s - loss: 0.7038 - accuracy: 0.781 - ETA: 36s - loss: 0.7045 - accuracy: 0.782 - ETA: 36s - loss: 0.7024 - accuracy: 0.782 - ETA: 35s - loss: 0.7022 - accuracy: 0.782 - ETA: 34s - loss: 0.7027 - accuracy: 0.781 - ETA: 33s - loss: 0.7045 - accuracy: 0.781 - ETA: 32s - loss: 0.7064 - accuracy: 0.781 - ETA: 31s - loss: 0.7067 - accuracy: 0.781 - ETA: 30s - loss: 0.7059 - accuracy: 0.781 - ETA: 29s - loss: 0.7066 - accuracy: 0.781 - ETA: 28s - loss: 0.7048 - accuracy: 0.782 - ETA: 27s - loss: 0.7036 - accuracy: 0.782 - ETA: 26s - loss: 0.7039 - accuracy: 0.782 - ETA: 25s - loss: 0.7037 - accuracy: 0.782 - ETA: 25s - loss: 0.7010 - accuracy: 0.782 - ETA: 24s - loss: 0.6996 - accuracy: 0.783 - ETA: 23s - loss: 0.6997 - accuracy: 0.783 - ETA: 22s - loss: 0.7014 - accuracy: 0.782 - ETA: 21s - loss: 0.6994 - accuracy: 0.783 - ETA: 20s - loss: 0.6982 - accuracy: 0.783 - ETA: 19s - loss: 0.6994 - accuracy: 0.784 - ETA: 18s - loss: 0.6990 - accuracy: 0.784 - ETA: 17s - loss: 0.6988 - accuracy: 0.784 - ETA: 16s - loss: 0.7001 - accuracy: 0.784 - ETA: 16s - loss: 0.7004 - accuracy: 0.784 - ETA: 15s - loss: 0.7039 - accuracy: 0.783 - ETA: 14s - loss: 0.7046 - accuracy: 0.783 - ETA: 13s - loss: 0.7045 - accuracy: 0.783 - ETA: 12s - loss: 0.7044 - accuracy: 0.783 - ETA: 11s - loss: 0.7035 - accuracy: 0.784 - ETA: 10s - loss: 0.7034 - accuracy: 0.783 - ETA: 9s - loss: 0.7039 - accuracy: 0.783 - ETA: 8s - loss: 0.7040 - accuracy: 0.78 - ETA: 7s - loss: 0.7040 - accuracy: 0.78 - ETA: 7s - loss: 0.7027 - accuracy: 0.78 - ETA: 6s - loss: 0.7030 - accuracy: 0.78 - ETA: 5s - loss: 0.7028 - accuracy: 0.78 - ETA: 4s - loss: 0.7014 - accuracy: 0.78 - ETA: 3s - loss: 0.7048 - accuracy: 0.78 - ETA: 2s - loss: 0.7053 - accuracy: 0.78 - ETA: 1s - loss: 0.7053 - accuracy: 0.78 - ETA: 0s - loss: 0.7041 - accuracy: 0.78 - 106s 8ms/step - loss: 0.7036 - accuracy: 0.7844 - val_loss: 2.9210 - val_accuracy: 0.3229\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.7298 - accuracy: 0.80 - ETA: 1:29 - loss: 0.6710 - accuracy: 0.81 - ETA: 1:28 - loss: 0.7044 - accuracy: 0.80 - ETA: 1:28 - loss: 0.7178 - accuracy: 0.79 - ETA: 1:27 - loss: 0.6863 - accuracy: 0.80 - ETA: 1:26 - loss: 0.7067 - accuracy: 0.79 - ETA: 1:25 - loss: 0.6993 - accuracy: 0.79 - ETA: 1:23 - loss: 0.7003 - accuracy: 0.79 - ETA: 1:23 - loss: 0.7029 - accuracy: 0.79 - ETA: 1:22 - loss: 0.6936 - accuracy: 0.80 - ETA: 1:21 - loss: 0.6872 - accuracy: 0.80 - ETA: 1:20 - loss: 0.6867 - accuracy: 0.80 - ETA: 1:19 - loss: 0.6844 - accuracy: 0.80 - ETA: 1:18 - loss: 0.6731 - accuracy: 0.80 - ETA: 1:18 - loss: 0.6683 - accuracy: 0.80 - ETA: 1:17 - loss: 0.6710 - accuracy: 0.80 - ETA: 1:16 - loss: 0.6874 - accuracy: 0.80 - ETA: 1:15 - loss: 0.6979 - accuracy: 0.79 - ETA: 1:14 - loss: 0.7064 - accuracy: 0.79 - ETA: 1:13 - loss: 0.7038 - accuracy: 0.79 - ETA: 1:12 - loss: 0.7103 - accuracy: 0.79 - ETA: 1:11 - loss: 0.7078 - accuracy: 0.79 - ETA: 1:10 - loss: 0.7083 - accuracy: 0.79 - ETA: 1:09 - loss: 0.7098 - accuracy: 0.79 - ETA: 1:08 - loss: 0.7145 - accuracy: 0.79 - ETA: 1:07 - loss: 0.7128 - accuracy: 0.79 - ETA: 1:06 - loss: 0.7150 - accuracy: 0.79 - ETA: 1:06 - loss: 0.7155 - accuracy: 0.79 - ETA: 1:05 - loss: 0.7194 - accuracy: 0.79 - ETA: 1:04 - loss: 0.7176 - accuracy: 0.78 - ETA: 1:03 - loss: 0.7120 - accuracy: 0.79 - ETA: 1:03 - loss: 0.7160 - accuracy: 0.78 - ETA: 1:02 - loss: 0.7160 - accuracy: 0.78 - ETA: 1:01 - loss: 0.7141 - accuracy: 0.78 - ETA: 1:00 - loss: 0.7115 - accuracy: 0.78 - ETA: 59s - loss: 0.7110 - accuracy: 0.7895 - ETA: 58s - loss: 0.7092 - accuracy: 0.789 - ETA: 57s - loss: 0.7154 - accuracy: 0.787 - ETA: 56s - loss: 0.7195 - accuracy: 0.786 - ETA: 55s - loss: 0.7149 - accuracy: 0.786 - ETA: 54s - loss: 0.7153 - accuracy: 0.786 - ETA: 53s - loss: 0.7139 - accuracy: 0.786 - ETA: 52s - loss: 0.7151 - accuracy: 0.786 - ETA: 51s - loss: 0.7153 - accuracy: 0.785 - ETA: 51s - loss: 0.7153 - accuracy: 0.785 - ETA: 50s - loss: 0.7146 - accuracy: 0.785 - ETA: 49s - loss: 0.7171 - accuracy: 0.784 - ETA: 48s - loss: 0.7195 - accuracy: 0.785 - ETA: 47s - loss: 0.7204 - accuracy: 0.783 - ETA: 46s - loss: 0.7135 - accuracy: 0.785 - ETA: 45s - loss: 0.7115 - accuracy: 0.786 - ETA: 44s - loss: 0.7127 - accuracy: 0.786 - ETA: 43s - loss: 0.7132 - accuracy: 0.785 - ETA: 43s - loss: 0.7143 - accuracy: 0.784 - ETA: 42s - loss: 0.7169 - accuracy: 0.783 - ETA: 41s - loss: 0.7182 - accuracy: 0.783 - ETA: 40s - loss: 0.7212 - accuracy: 0.782 - ETA: 39s - loss: 0.7197 - accuracy: 0.782 - ETA: 38s - loss: 0.7168 - accuracy: 0.783 - ETA: 37s - loss: 0.7147 - accuracy: 0.784 - ETA: 36s - loss: 0.7153 - accuracy: 0.783 - ETA: 35s - loss: 0.7145 - accuracy: 0.783 - ETA: 34s - loss: 0.7163 - accuracy: 0.783 - ETA: 33s - loss: 0.7144 - accuracy: 0.783 - ETA: 33s - loss: 0.7150 - accuracy: 0.783 - ETA: 32s - loss: 0.7135 - accuracy: 0.783 - ETA: 31s - loss: 0.7129 - accuracy: 0.782 - ETA: 30s - loss: 0.7110 - accuracy: 0.783 - ETA: 29s - loss: 0.7106 - accuracy: 0.783 - ETA: 28s - loss: 0.7112 - accuracy: 0.783 - ETA: 27s - loss: 0.7083 - accuracy: 0.784 - ETA: 26s - loss: 0.7081 - accuracy: 0.785 - ETA: 25s - loss: 0.7073 - accuracy: 0.785 - ETA: 24s - loss: 0.7046 - accuracy: 0.785 - ETA: 24s - loss: 0.7039 - accuracy: 0.785 - ETA: 23s - loss: 0.7037 - accuracy: 0.785 - ETA: 22s - loss: 0.7042 - accuracy: 0.785 - ETA: 21s - loss: 0.7033 - accuracy: 0.785 - ETA: 20s - loss: 0.7023 - accuracy: 0.785 - ETA: 19s - loss: 0.7032 - accuracy: 0.785 - ETA: 18s - loss: 0.7041 - accuracy: 0.785 - ETA: 17s - loss: 0.7007 - accuracy: 0.785 - ETA: 16s - loss: 0.7016 - accuracy: 0.785 - ETA: 15s - loss: 0.7021 - accuracy: 0.785 - ETA: 15s - loss: 0.7029 - accuracy: 0.784 - ETA: 14s - loss: 0.7009 - accuracy: 0.785 - ETA: 13s - loss: 0.7010 - accuracy: 0.785 - ETA: 12s - loss: 0.7024 - accuracy: 0.785 - ETA: 11s - loss: 0.7019 - accuracy: 0.785 - ETA: 10s - loss: 0.7008 - accuracy: 0.785 - ETA: 9s - loss: 0.7015 - accuracy: 0.785 - ETA: 8s - loss: 0.7028 - accuracy: 0.78 - ETA: 7s - loss: 0.7020 - accuracy: 0.78 - ETA: 6s - loss: 0.7005 - accuracy: 0.78 - ETA: 6s - loss: 0.7007 - accuracy: 0.78 - ETA: 5s - loss: 0.7001 - accuracy: 0.78 - ETA: 4s - loss: 0.6993 - accuracy: 0.78 - ETA: 3s - loss: 0.7004 - accuracy: 0.78 - ETA: 2s - loss: 0.7007 - accuracy: 0.78 - ETA: 1s - loss: 0.6999 - accuracy: 0.78 - ETA: 0s - loss: 0.6997 - accuracy: 0.78 - 105s 8ms/step - loss: 0.7013 - accuracy: 0.7865 - val_loss: 3.0207 - val_accuracy: 0.3347\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:31 - loss: 0.5848 - accuracy: 0.82 - ETA: 1:27 - loss: 0.6376 - accuracy: 0.80 - ETA: 1:27 - loss: 0.6588 - accuracy: 0.79 - ETA: 1:28 - loss: 0.6571 - accuracy: 0.79 - ETA: 1:26 - loss: 0.6317 - accuracy: 0.79 - ETA: 1:26 - loss: 0.6536 - accuracy: 0.79 - ETA: 1:25 - loss: 0.6586 - accuracy: 0.78 - ETA: 1:24 - loss: 0.6784 - accuracy: 0.78 - ETA: 1:23 - loss: 0.6798 - accuracy: 0.78 - ETA: 1:21 - loss: 0.6807 - accuracy: 0.78 - ETA: 1:21 - loss: 0.6768 - accuracy: 0.78 - ETA: 1:20 - loss: 0.6778 - accuracy: 0.78 - ETA: 1:19 - loss: 0.6742 - accuracy: 0.78 - ETA: 1:18 - loss: 0.6884 - accuracy: 0.78 - ETA: 1:17 - loss: 0.6759 - accuracy: 0.78 - ETA: 1:16 - loss: 0.6666 - accuracy: 0.79 - ETA: 1:15 - loss: 0.6651 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6651 - accuracy: 0.79 - ETA: 1:13 - loss: 0.6704 - accuracy: 0.78 - ETA: 1:13 - loss: 0.6748 - accuracy: 0.78 - ETA: 1:12 - loss: 0.6744 - accuracy: 0.79 - ETA: 1:11 - loss: 0.6744 - accuracy: 0.78 - ETA: 1:10 - loss: 0.6784 - accuracy: 0.78 - ETA: 1:09 - loss: 0.6851 - accuracy: 0.78 - ETA: 1:08 - loss: 0.6909 - accuracy: 0.78 - ETA: 1:07 - loss: 0.6905 - accuracy: 0.78 - ETA: 1:06 - loss: 0.6890 - accuracy: 0.78 - ETA: 1:05 - loss: 0.6896 - accuracy: 0.78 - ETA: 1:04 - loss: 0.7061 - accuracy: 0.78 - ETA: 1:04 - loss: 0.7041 - accuracy: 0.78 - ETA: 1:03 - loss: 0.7100 - accuracy: 0.78 - ETA: 1:02 - loss: 0.7092 - accuracy: 0.78 - ETA: 1:01 - loss: 0.7122 - accuracy: 0.78 - ETA: 1:00 - loss: 0.7086 - accuracy: 0.78 - ETA: 59s - loss: 0.7104 - accuracy: 0.7830 - ETA: 58s - loss: 0.7121 - accuracy: 0.784 - ETA: 57s - loss: 0.7138 - accuracy: 0.784 - ETA: 56s - loss: 0.7102 - accuracy: 0.785 - ETA: 56s - loss: 0.7082 - accuracy: 0.786 - ETA: 55s - loss: 0.7043 - accuracy: 0.787 - ETA: 54s - loss: 0.7036 - accuracy: 0.787 - ETA: 53s - loss: 0.6994 - accuracy: 0.788 - ETA: 52s - loss: 0.6998 - accuracy: 0.788 - ETA: 51s - loss: 0.6963 - accuracy: 0.788 - ETA: 50s - loss: 0.6969 - accuracy: 0.788 - ETA: 49s - loss: 0.6931 - accuracy: 0.789 - ETA: 49s - loss: 0.6925 - accuracy: 0.790 - ETA: 48s - loss: 0.6921 - accuracy: 0.790 - ETA: 47s - loss: 0.6932 - accuracy: 0.790 - ETA: 46s - loss: 0.6913 - accuracy: 0.790 - ETA: 45s - loss: 0.6948 - accuracy: 0.790 - ETA: 44s - loss: 0.6972 - accuracy: 0.790 - ETA: 43s - loss: 0.6963 - accuracy: 0.789 - ETA: 42s - loss: 0.6944 - accuracy: 0.790 - ETA: 41s - loss: 0.6973 - accuracy: 0.789 - ETA: 40s - loss: 0.6989 - accuracy: 0.789 - ETA: 40s - loss: 0.7005 - accuracy: 0.789 - ETA: 39s - loss: 0.7000 - accuracy: 0.789 - ETA: 38s - loss: 0.7037 - accuracy: 0.788 - ETA: 37s - loss: 0.7009 - accuracy: 0.789 - ETA: 36s - loss: 0.7027 - accuracy: 0.788 - ETA: 35s - loss: 0.7011 - accuracy: 0.788 - ETA: 34s - loss: 0.7035 - accuracy: 0.787 - ETA: 33s - loss: 0.7046 - accuracy: 0.786 - ETA: 32s - loss: 0.7057 - accuracy: 0.786 - ETA: 32s - loss: 0.7077 - accuracy: 0.785 - ETA: 31s - loss: 0.7075 - accuracy: 0.785 - ETA: 30s - loss: 0.7074 - accuracy: 0.786 - ETA: 29s - loss: 0.7041 - accuracy: 0.787 - ETA: 28s - loss: 0.7017 - accuracy: 0.787 - ETA: 27s - loss: 0.7050 - accuracy: 0.788 - ETA: 26s - loss: 0.7063 - accuracy: 0.788 - ETA: 25s - loss: 0.7052 - accuracy: 0.788 - ETA: 24s - loss: 0.7055 - accuracy: 0.788 - ETA: 23s - loss: 0.7030 - accuracy: 0.789 - ETA: 23s - loss: 0.7052 - accuracy: 0.788 - ETA: 22s - loss: 0.7070 - accuracy: 0.788 - ETA: 21s - loss: 0.7068 - accuracy: 0.789 - ETA: 20s - loss: 0.7093 - accuracy: 0.788 - ETA: 19s - loss: 0.7082 - accuracy: 0.788 - ETA: 18s - loss: 0.7097 - accuracy: 0.787 - ETA: 17s - loss: 0.7090 - accuracy: 0.787 - ETA: 16s - loss: 0.7108 - accuracy: 0.786 - ETA: 15s - loss: 0.7112 - accuracy: 0.786 - ETA: 15s - loss: 0.7125 - accuracy: 0.785 - ETA: 14s - loss: 0.7107 - accuracy: 0.786 - ETA: 13s - loss: 0.7104 - accuracy: 0.786 - ETA: 12s - loss: 0.7099 - accuracy: 0.786 - ETA: 11s - loss: 0.7103 - accuracy: 0.786 - ETA: 10s - loss: 0.7102 - accuracy: 0.787 - ETA: 9s - loss: 0.7106 - accuracy: 0.787 - ETA: 8s - loss: 0.7101 - accuracy: 0.78 - ETA: 7s - loss: 0.7100 - accuracy: 0.78 - ETA: 6s - loss: 0.7092 - accuracy: 0.78 - ETA: 6s - loss: 0.7082 - accuracy: 0.78 - ETA: 5s - loss: 0.7066 - accuracy: 0.78 - ETA: 4s - loss: 0.7059 - accuracy: 0.78 - ETA: 3s - loss: 0.7070 - accuracy: 0.78 - ETA: 2s - loss: 0.7077 - accuracy: 0.78 - ETA: 1s - loss: 0.7074 - accuracy: 0.78 - ETA: 0s - loss: 0.7077 - accuracy: 0.78 - 105s 8ms/step - loss: 0.7089 - accuracy: 0.7858 - val_loss: 2.8907 - val_accuracy: 0.3443\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.7162 - accuracy: 0.72 - ETA: 1:28 - loss: 0.7550 - accuracy: 0.75 - ETA: 1:27 - loss: 0.7378 - accuracy: 0.76 - ETA: 1:26 - loss: 0.7449 - accuracy: 0.76 - ETA: 1:28 - loss: 0.7156 - accuracy: 0.77 - ETA: 1:28 - loss: 0.7115 - accuracy: 0.77 - ETA: 1:28 - loss: 0.6953 - accuracy: 0.78 - ETA: 1:28 - loss: 0.7217 - accuracy: 0.77 - ETA: 1:27 - loss: 0.7166 - accuracy: 0.77 - ETA: 1:27 - loss: 0.7033 - accuracy: 0.77 - ETA: 1:26 - loss: 0.7311 - accuracy: 0.77 - ETA: 1:25 - loss: 0.7253 - accuracy: 0.77 - ETA: 1:25 - loss: 0.7499 - accuracy: 0.77 - ETA: 1:24 - loss: 0.7460 - accuracy: 0.77 - ETA: 1:23 - loss: 0.7410 - accuracy: 0.77 - ETA: 1:23 - loss: 0.7406 - accuracy: 0.77 - ETA: 1:22 - loss: 0.7337 - accuracy: 0.77 - ETA: 1:21 - loss: 0.7362 - accuracy: 0.77 - ETA: 1:20 - loss: 0.7309 - accuracy: 0.78 - ETA: 1:18 - loss: 0.7305 - accuracy: 0.78 - ETA: 1:17 - loss: 0.7370 - accuracy: 0.77 - ETA: 1:16 - loss: 0.7373 - accuracy: 0.77 - ETA: 1:15 - loss: 0.7338 - accuracy: 0.77 - ETA: 1:14 - loss: 0.7340 - accuracy: 0.77 - ETA: 1:13 - loss: 0.7314 - accuracy: 0.77 - ETA: 1:11 - loss: 0.7261 - accuracy: 0.77 - ETA: 1:10 - loss: 0.7200 - accuracy: 0.78 - ETA: 1:09 - loss: 0.7222 - accuracy: 0.78 - ETA: 1:08 - loss: 0.7279 - accuracy: 0.77 - ETA: 1:07 - loss: 0.7228 - accuracy: 0.78 - ETA: 1:06 - loss: 0.7223 - accuracy: 0.78 - ETA: 1:05 - loss: 0.7281 - accuracy: 0.77 - ETA: 1:04 - loss: 0.7299 - accuracy: 0.77 - ETA: 1:03 - loss: 0.7301 - accuracy: 0.77 - ETA: 1:02 - loss: 0.7357 - accuracy: 0.77 - ETA: 1:01 - loss: 0.7328 - accuracy: 0.77 - ETA: 1:00 - loss: 0.7329 - accuracy: 0.77 - ETA: 59s - loss: 0.7397 - accuracy: 0.7794 - ETA: 58s - loss: 0.7434 - accuracy: 0.778 - ETA: 57s - loss: 0.7421 - accuracy: 0.779 - ETA: 56s - loss: 0.7452 - accuracy: 0.777 - ETA: 55s - loss: 0.7442 - accuracy: 0.777 - ETA: 54s - loss: 0.7407 - accuracy: 0.779 - ETA: 53s - loss: 0.7387 - accuracy: 0.779 - ETA: 52s - loss: 0.7362 - accuracy: 0.780 - ETA: 51s - loss: 0.7350 - accuracy: 0.779 - ETA: 50s - loss: 0.7339 - accuracy: 0.780 - ETA: 49s - loss: 0.7321 - accuracy: 0.780 - ETA: 48s - loss: 0.7298 - accuracy: 0.780 - ETA: 47s - loss: 0.7258 - accuracy: 0.780 - ETA: 46s - loss: 0.7251 - accuracy: 0.781 - ETA: 45s - loss: 0.7217 - accuracy: 0.782 - ETA: 44s - loss: 0.7204 - accuracy: 0.783 - ETA: 44s - loss: 0.7179 - accuracy: 0.783 - ETA: 43s - loss: 0.7185 - accuracy: 0.783 - ETA: 42s - loss: 0.7178 - accuracy: 0.784 - ETA: 41s - loss: 0.7152 - accuracy: 0.785 - ETA: 40s - loss: 0.7137 - accuracy: 0.785 - ETA: 39s - loss: 0.7147 - accuracy: 0.785 - ETA: 38s - loss: 0.7124 - accuracy: 0.785 - ETA: 37s - loss: 0.7106 - accuracy: 0.786 - ETA: 36s - loss: 0.7127 - accuracy: 0.785 - ETA: 35s - loss: 0.7142 - accuracy: 0.785 - ETA: 34s - loss: 0.7126 - accuracy: 0.785 - ETA: 33s - loss: 0.7117 - accuracy: 0.785 - ETA: 32s - loss: 0.7094 - accuracy: 0.785 - ETA: 31s - loss: 0.7112 - accuracy: 0.784 - ETA: 30s - loss: 0.7094 - accuracy: 0.785 - ETA: 30s - loss: 0.7087 - accuracy: 0.785 - ETA: 29s - loss: 0.7084 - accuracy: 0.785 - ETA: 28s - loss: 0.7094 - accuracy: 0.785 - ETA: 27s - loss: 0.7092 - accuracy: 0.786 - ETA: 26s - loss: 0.7078 - accuracy: 0.786 - ETA: 25s - loss: 0.7076 - accuracy: 0.786 - ETA: 24s - loss: 0.7084 - accuracy: 0.786 - ETA: 23s - loss: 0.7089 - accuracy: 0.787 - ETA: 22s - loss: 0.7103 - accuracy: 0.786 - ETA: 21s - loss: 0.7086 - accuracy: 0.787 - ETA: 20s - loss: 0.7087 - accuracy: 0.787 - ETA: 19s - loss: 0.7068 - accuracy: 0.787 - ETA: 18s - loss: 0.7045 - accuracy: 0.788 - ETA: 18s - loss: 0.7060 - accuracy: 0.787 - ETA: 17s - loss: 0.7062 - accuracy: 0.787 - ETA: 16s - loss: 0.7066 - accuracy: 0.787 - ETA: 15s - loss: 0.7090 - accuracy: 0.787 - ETA: 14s - loss: 0.7102 - accuracy: 0.787 - ETA: 13s - loss: 0.7073 - accuracy: 0.788 - ETA: 12s - loss: 0.7057 - accuracy: 0.788 - ETA: 11s - loss: 0.7050 - accuracy: 0.788 - ETA: 10s - loss: 0.7033 - accuracy: 0.789 - ETA: 9s - loss: 0.7036 - accuracy: 0.788 - ETA: 8s - loss: 0.7020 - accuracy: 0.78 - ETA: 7s - loss: 0.7043 - accuracy: 0.78 - ETA: 7s - loss: 0.7036 - accuracy: 0.78 - ETA: 6s - loss: 0.7038 - accuracy: 0.78 - ETA: 5s - loss: 0.7051 - accuracy: 0.78 - ETA: 4s - loss: 0.7050 - accuracy: 0.78 - ETA: 3s - loss: 0.7049 - accuracy: 0.78 - ETA: 2s - loss: 0.7030 - accuracy: 0.78 - ETA: 1s - loss: 0.7018 - accuracy: 0.78 - ETA: 0s - loss: 0.7015 - accuracy: 0.78 - 106s 8ms/step - loss: 0.7016 - accuracy: 0.7875 - val_loss: 3.0312 - val_accuracy: 0.3226\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:34 - loss: 0.7548 - accuracy: 0.78 - ETA: 1:30 - loss: 0.7439 - accuracy: 0.76 - ETA: 1:29 - loss: 0.7588 - accuracy: 0.76 - ETA: 1:28 - loss: 0.6901 - accuracy: 0.79 - ETA: 1:27 - loss: 0.6731 - accuracy: 0.79 - ETA: 1:26 - loss: 0.6546 - accuracy: 0.80 - ETA: 1:25 - loss: 0.6960 - accuracy: 0.79 - ETA: 1:24 - loss: 0.6830 - accuracy: 0.79 - ETA: 1:23 - loss: 0.6611 - accuracy: 0.80 - ETA: 1:22 - loss: 0.6424 - accuracy: 0.80 - ETA: 1:21 - loss: 0.6525 - accuracy: 0.80 - ETA: 1:20 - loss: 0.6531 - accuracy: 0.80 - ETA: 1:19 - loss: 0.6613 - accuracy: 0.80 - ETA: 1:18 - loss: 0.6622 - accuracy: 0.80 - ETA: 1:17 - loss: 0.6537 - accuracy: 0.80 - ETA: 1:16 - loss: 0.6474 - accuracy: 0.81 - ETA: 1:15 - loss: 0.6407 - accuracy: 0.81 - ETA: 1:15 - loss: 0.6411 - accuracy: 0.81 - ETA: 1:14 - loss: 0.6372 - accuracy: 0.81 - ETA: 1:13 - loss: 0.6409 - accuracy: 0.80 - ETA: 1:12 - loss: 0.6402 - accuracy: 0.80 - ETA: 1:11 - loss: 0.6381 - accuracy: 0.80 - ETA: 1:10 - loss: 0.6419 - accuracy: 0.80 - ETA: 1:09 - loss: 0.6528 - accuracy: 0.80 - ETA: 1:08 - loss: 0.6582 - accuracy: 0.80 - ETA: 1:07 - loss: 0.6610 - accuracy: 0.80 - ETA: 1:07 - loss: 0.6599 - accuracy: 0.80 - ETA: 1:06 - loss: 0.6618 - accuracy: 0.80 - ETA: 1:05 - loss: 0.6602 - accuracy: 0.80 - ETA: 1:04 - loss: 0.6570 - accuracy: 0.80 - ETA: 1:03 - loss: 0.6646 - accuracy: 0.80 - ETA: 1:02 - loss: 0.6639 - accuracy: 0.80 - ETA: 1:01 - loss: 0.6683 - accuracy: 0.79 - ETA: 1:00 - loss: 0.6731 - accuracy: 0.79 - ETA: 59s - loss: 0.6756 - accuracy: 0.7940 - ETA: 59s - loss: 0.6708 - accuracy: 0.795 - ETA: 58s - loss: 0.6705 - accuracy: 0.794 - ETA: 57s - loss: 0.6703 - accuracy: 0.794 - ETA: 56s - loss: 0.6715 - accuracy: 0.794 - ETA: 55s - loss: 0.6765 - accuracy: 0.792 - ETA: 54s - loss: 0.6761 - accuracy: 0.793 - ETA: 53s - loss: 0.6816 - accuracy: 0.791 - ETA: 52s - loss: 0.6808 - accuracy: 0.791 - ETA: 52s - loss: 0.6811 - accuracy: 0.791 - ETA: 51s - loss: 0.6821 - accuracy: 0.790 - ETA: 50s - loss: 0.6773 - accuracy: 0.792 - ETA: 49s - loss: 0.6797 - accuracy: 0.791 - ETA: 48s - loss: 0.6808 - accuracy: 0.790 - ETA: 47s - loss: 0.6843 - accuracy: 0.789 - ETA: 46s - loss: 0.6852 - accuracy: 0.789 - ETA: 45s - loss: 0.6824 - accuracy: 0.790 - ETA: 44s - loss: 0.6825 - accuracy: 0.790 - ETA: 43s - loss: 0.6810 - accuracy: 0.791 - ETA: 43s - loss: 0.6827 - accuracy: 0.790 - ETA: 42s - loss: 0.6807 - accuracy: 0.791 - ETA: 41s - loss: 0.6798 - accuracy: 0.790 - ETA: 40s - loss: 0.6815 - accuracy: 0.790 - ETA: 39s - loss: 0.6812 - accuracy: 0.790 - ETA: 38s - loss: 0.6805 - accuracy: 0.790 - ETA: 37s - loss: 0.6805 - accuracy: 0.790 - ETA: 36s - loss: 0.6801 - accuracy: 0.789 - ETA: 35s - loss: 0.6860 - accuracy: 0.788 - ETA: 34s - loss: 0.6899 - accuracy: 0.787 - ETA: 33s - loss: 0.6907 - accuracy: 0.787 - ETA: 33s - loss: 0.6902 - accuracy: 0.787 - ETA: 32s - loss: 0.6901 - accuracy: 0.786 - ETA: 31s - loss: 0.6893 - accuracy: 0.786 - ETA: 30s - loss: 0.6898 - accuracy: 0.787 - ETA: 29s - loss: 0.6892 - accuracy: 0.787 - ETA: 28s - loss: 0.6866 - accuracy: 0.788 - ETA: 27s - loss: 0.6850 - accuracy: 0.789 - ETA: 26s - loss: 0.6865 - accuracy: 0.789 - ETA: 25s - loss: 0.6870 - accuracy: 0.789 - ETA: 24s - loss: 0.6851 - accuracy: 0.790 - ETA: 24s - loss: 0.6844 - accuracy: 0.790 - ETA: 23s - loss: 0.6838 - accuracy: 0.789 - ETA: 22s - loss: 0.6808 - accuracy: 0.790 - ETA: 21s - loss: 0.6832 - accuracy: 0.790 - ETA: 20s - loss: 0.6824 - accuracy: 0.790 - ETA: 19s - loss: 0.6837 - accuracy: 0.789 - ETA: 18s - loss: 0.6827 - accuracy: 0.790 - ETA: 17s - loss: 0.6849 - accuracy: 0.789 - ETA: 16s - loss: 0.6848 - accuracy: 0.790 - ETA: 15s - loss: 0.6845 - accuracy: 0.789 - ETA: 15s - loss: 0.6834 - accuracy: 0.790 - ETA: 14s - loss: 0.6853 - accuracy: 0.790 - ETA: 13s - loss: 0.6855 - accuracy: 0.790 - ETA: 12s - loss: 0.6845 - accuracy: 0.790 - ETA: 11s - loss: 0.6846 - accuracy: 0.790 - ETA: 10s - loss: 0.6845 - accuracy: 0.791 - ETA: 9s - loss: 0.6842 - accuracy: 0.790 - ETA: 8s - loss: 0.6850 - accuracy: 0.79 - ETA: 7s - loss: 0.6840 - accuracy: 0.79 - ETA: 6s - loss: 0.6838 - accuracy: 0.79 - ETA: 6s - loss: 0.6834 - accuracy: 0.79 - ETA: 5s - loss: 0.6843 - accuracy: 0.79 - ETA: 4s - loss: 0.6848 - accuracy: 0.79 - ETA: 3s - loss: 0.6836 - accuracy: 0.79 - ETA: 2s - loss: 0.6844 - accuracy: 0.79 - ETA: 1s - loss: 0.6877 - accuracy: 0.78 - ETA: 0s - loss: 0.6882 - accuracy: 0.78 - 105s 8ms/step - loss: 0.6875 - accuracy: 0.7892 - val_loss: 3.1131 - val_accuracy: 0.3266\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:35 - loss: 0.6614 - accuracy: 0.78 - ETA: 1:35 - loss: 0.6547 - accuracy: 0.78 - ETA: 1:33 - loss: 0.6106 - accuracy: 0.80 - ETA: 1:30 - loss: 0.6834 - accuracy: 0.79 - ETA: 1:29 - loss: 0.6656 - accuracy: 0.79 - ETA: 1:27 - loss: 0.6550 - accuracy: 0.79 - ETA: 1:26 - loss: 0.6751 - accuracy: 0.79 - ETA: 1:26 - loss: 0.6772 - accuracy: 0.79 - ETA: 1:25 - loss: 0.6904 - accuracy: 0.78 - ETA: 1:24 - loss: 0.7008 - accuracy: 0.78 - ETA: 1:23 - loss: 0.7022 - accuracy: 0.78 - ETA: 1:21 - loss: 0.6997 - accuracy: 0.78 - ETA: 1:20 - loss: 0.7103 - accuracy: 0.78 - ETA: 1:19 - loss: 0.6952 - accuracy: 0.78 - ETA: 1:18 - loss: 0.6833 - accuracy: 0.79 - ETA: 1:17 - loss: 0.6776 - accuracy: 0.79 - ETA: 1:16 - loss: 0.6787 - accuracy: 0.79 - ETA: 1:15 - loss: 0.6773 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6746 - accuracy: 0.79 - ETA: 1:13 - loss: 0.6714 - accuracy: 0.79 - ETA: 1:13 - loss: 0.6725 - accuracy: 0.79 - ETA: 1:12 - loss: 0.6709 - accuracy: 0.79 - ETA: 1:11 - loss: 0.6647 - accuracy: 0.79 - ETA: 1:10 - loss: 0.6587 - accuracy: 0.79 - ETA: 1:09 - loss: 0.6639 - accuracy: 0.79 - ETA: 1:08 - loss: 0.6611 - accuracy: 0.79 - ETA: 1:07 - loss: 0.6655 - accuracy: 0.79 - ETA: 1:06 - loss: 0.6650 - accuracy: 0.79 - ETA: 1:05 - loss: 0.6689 - accuracy: 0.79 - ETA: 1:04 - loss: 0.6781 - accuracy: 0.79 - ETA: 1:03 - loss: 0.6766 - accuracy: 0.79 - ETA: 1:03 - loss: 0.6727 - accuracy: 0.79 - ETA: 1:02 - loss: 0.6693 - accuracy: 0.79 - ETA: 1:01 - loss: 0.6683 - accuracy: 0.79 - ETA: 1:00 - loss: 0.6697 - accuracy: 0.79 - ETA: 59s - loss: 0.6654 - accuracy: 0.7958 - ETA: 58s - loss: 0.6672 - accuracy: 0.795 - ETA: 57s - loss: 0.6690 - accuracy: 0.794 - ETA: 56s - loss: 0.6720 - accuracy: 0.793 - ETA: 55s - loss: 0.6693 - accuracy: 0.795 - ETA: 54s - loss: 0.6721 - accuracy: 0.793 - ETA: 53s - loss: 0.6702 - accuracy: 0.794 - ETA: 52s - loss: 0.6675 - accuracy: 0.794 - ETA: 52s - loss: 0.6681 - accuracy: 0.794 - ETA: 51s - loss: 0.6667 - accuracy: 0.794 - ETA: 50s - loss: 0.6731 - accuracy: 0.793 - ETA: 49s - loss: 0.6724 - accuracy: 0.793 - ETA: 48s - loss: 0.6732 - accuracy: 0.792 - ETA: 47s - loss: 0.6726 - accuracy: 0.792 - ETA: 46s - loss: 0.6715 - accuracy: 0.793 - ETA: 45s - loss: 0.6768 - accuracy: 0.791 - ETA: 44s - loss: 0.6766 - accuracy: 0.791 - ETA: 43s - loss: 0.6778 - accuracy: 0.790 - ETA: 43s - loss: 0.6759 - accuracy: 0.791 - ETA: 42s - loss: 0.6755 - accuracy: 0.791 - ETA: 41s - loss: 0.6744 - accuracy: 0.793 - ETA: 40s - loss: 0.6767 - accuracy: 0.792 - ETA: 39s - loss: 0.6777 - accuracy: 0.792 - ETA: 38s - loss: 0.6807 - accuracy: 0.791 - ETA: 37s - loss: 0.6815 - accuracy: 0.791 - ETA: 36s - loss: 0.6808 - accuracy: 0.791 - ETA: 35s - loss: 0.6821 - accuracy: 0.790 - ETA: 34s - loss: 0.6806 - accuracy: 0.790 - ETA: 34s - loss: 0.6806 - accuracy: 0.791 - ETA: 33s - loss: 0.6812 - accuracy: 0.791 - ETA: 32s - loss: 0.6792 - accuracy: 0.792 - ETA: 31s - loss: 0.6787 - accuracy: 0.792 - ETA: 30s - loss: 0.6788 - accuracy: 0.792 - ETA: 29s - loss: 0.6794 - accuracy: 0.792 - ETA: 28s - loss: 0.6797 - accuracy: 0.792 - ETA: 27s - loss: 0.6767 - accuracy: 0.793 - ETA: 26s - loss: 0.6782 - accuracy: 0.792 - ETA: 25s - loss: 0.6786 - accuracy: 0.792 - ETA: 25s - loss: 0.6780 - accuracy: 0.792 - ETA: 24s - loss: 0.6795 - accuracy: 0.791 - ETA: 23s - loss: 0.6802 - accuracy: 0.791 - ETA: 22s - loss: 0.6790 - accuracy: 0.791 - ETA: 21s - loss: 0.6796 - accuracy: 0.792 - ETA: 20s - loss: 0.6793 - accuracy: 0.792 - ETA: 19s - loss: 0.6797 - accuracy: 0.792 - ETA: 18s - loss: 0.6786 - accuracy: 0.792 - ETA: 17s - loss: 0.6771 - accuracy: 0.792 - ETA: 16s - loss: 0.6783 - accuracy: 0.792 - ETA: 16s - loss: 0.6785 - accuracy: 0.791 - ETA: 15s - loss: 0.6787 - accuracy: 0.791 - ETA: 14s - loss: 0.6788 - accuracy: 0.792 - ETA: 13s - loss: 0.6772 - accuracy: 0.792 - ETA: 12s - loss: 0.6795 - accuracy: 0.791 - ETA: 11s - loss: 0.6791 - accuracy: 0.792 - ETA: 10s - loss: 0.6772 - accuracy: 0.792 - ETA: 9s - loss: 0.6780 - accuracy: 0.792 - ETA: 8s - loss: 0.6774 - accuracy: 0.79 - ETA: 7s - loss: 0.6766 - accuracy: 0.79 - ETA: 6s - loss: 0.6758 - accuracy: 0.79 - ETA: 6s - loss: 0.6750 - accuracy: 0.79 - ETA: 5s - loss: 0.6743 - accuracy: 0.79 - ETA: 4s - loss: 0.6736 - accuracy: 0.79 - ETA: 3s - loss: 0.6730 - accuracy: 0.79 - ETA: 2s - loss: 0.6736 - accuracy: 0.79 - ETA: 1s - loss: 0.6748 - accuracy: 0.79 - ETA: 0s - loss: 0.6736 - accuracy: 0.79 - 105s 8ms/step - loss: 0.6747 - accuracy: 0.7933 - val_loss: 3.1280 - val_accuracy: 0.3273\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:28 - loss: 0.7377 - accuracy: 0.79 - ETA: 1:28 - loss: 0.7976 - accuracy: 0.76 - ETA: 1:26 - loss: 0.7349 - accuracy: 0.77 - ETA: 1:26 - loss: 0.7013 - accuracy: 0.78 - ETA: 1:25 - loss: 0.7061 - accuracy: 0.78 - ETA: 1:24 - loss: 0.7080 - accuracy: 0.78 - ETA: 1:23 - loss: 0.7357 - accuracy: 0.78 - ETA: 1:23 - loss: 0.7490 - accuracy: 0.78 - ETA: 1:21 - loss: 0.7334 - accuracy: 0.78 - ETA: 1:20 - loss: 0.7286 - accuracy: 0.78 - ETA: 1:19 - loss: 0.7199 - accuracy: 0.78 - ETA: 1:19 - loss: 0.7136 - accuracy: 0.78 - ETA: 1:18 - loss: 0.7199 - accuracy: 0.78 - ETA: 1:17 - loss: 0.7219 - accuracy: 0.78 - ETA: 1:17 - loss: 0.7253 - accuracy: 0.78 - ETA: 1:16 - loss: 0.7255 - accuracy: 0.78 - ETA: 1:15 - loss: 0.7157 - accuracy: 0.78 - ETA: 1:15 - loss: 0.7071 - accuracy: 0.78 - ETA: 1:14 - loss: 0.6994 - accuracy: 0.78 - ETA: 1:13 - loss: 0.6855 - accuracy: 0.78 - ETA: 1:12 - loss: 0.6905 - accuracy: 0.78 - ETA: 1:11 - loss: 0.6902 - accuracy: 0.78 - ETA: 1:10 - loss: 0.6939 - accuracy: 0.78 - ETA: 1:09 - loss: 0.6902 - accuracy: 0.78 - ETA: 1:08 - loss: 0.6906 - accuracy: 0.78 - ETA: 1:07 - loss: 0.6887 - accuracy: 0.78 - ETA: 1:06 - loss: 0.6847 - accuracy: 0.78 - ETA: 1:05 - loss: 0.6850 - accuracy: 0.78 - ETA: 1:05 - loss: 0.6807 - accuracy: 0.79 - ETA: 1:04 - loss: 0.6833 - accuracy: 0.79 - ETA: 1:03 - loss: 0.6861 - accuracy: 0.79 - ETA: 1:03 - loss: 0.6887 - accuracy: 0.79 - ETA: 1:02 - loss: 0.6869 - accuracy: 0.79 - ETA: 1:01 - loss: 0.6874 - accuracy: 0.79 - ETA: 1:00 - loss: 0.6887 - accuracy: 0.79 - ETA: 59s - loss: 0.6881 - accuracy: 0.7917 - ETA: 58s - loss: 0.6814 - accuracy: 0.793 - ETA: 57s - loss: 0.6790 - accuracy: 0.794 - ETA: 56s - loss: 0.6784 - accuracy: 0.794 - ETA: 55s - loss: 0.6767 - accuracy: 0.793 - ETA: 54s - loss: 0.6787 - accuracy: 0.793 - ETA: 54s - loss: 0.6822 - accuracy: 0.792 - ETA: 53s - loss: 0.6857 - accuracy: 0.792 - ETA: 52s - loss: 0.6861 - accuracy: 0.792 - ETA: 51s - loss: 0.6886 - accuracy: 0.791 - ETA: 50s - loss: 0.6854 - accuracy: 0.793 - ETA: 49s - loss: 0.6892 - accuracy: 0.792 - ETA: 48s - loss: 0.6873 - accuracy: 0.793 - ETA: 47s - loss: 0.6862 - accuracy: 0.793 - ETA: 46s - loss: 0.6886 - accuracy: 0.793 - ETA: 45s - loss: 0.6919 - accuracy: 0.792 - ETA: 44s - loss: 0.6923 - accuracy: 0.791 - ETA: 44s - loss: 0.6884 - accuracy: 0.792 - ETA: 43s - loss: 0.6881 - accuracy: 0.792 - ETA: 42s - loss: 0.6873 - accuracy: 0.792 - ETA: 41s - loss: 0.6850 - accuracy: 0.793 - ETA: 40s - loss: 0.6844 - accuracy: 0.794 - ETA: 39s - loss: 0.6861 - accuracy: 0.793 - ETA: 38s - loss: 0.6861 - accuracy: 0.793 - ETA: 37s - loss: 0.6824 - accuracy: 0.794 - ETA: 36s - loss: 0.6795 - accuracy: 0.795 - ETA: 35s - loss: 0.6763 - accuracy: 0.795 - ETA: 34s - loss: 0.6791 - accuracy: 0.795 - ETA: 33s - loss: 0.6788 - accuracy: 0.794 - ETA: 33s - loss: 0.6763 - accuracy: 0.795 - ETA: 32s - loss: 0.6753 - accuracy: 0.796 - ETA: 31s - loss: 0.6735 - accuracy: 0.796 - ETA: 30s - loss: 0.6741 - accuracy: 0.796 - ETA: 29s - loss: 0.6734 - accuracy: 0.796 - ETA: 28s - loss: 0.6717 - accuracy: 0.797 - ETA: 27s - loss: 0.6714 - accuracy: 0.797 - ETA: 26s - loss: 0.6717 - accuracy: 0.797 - ETA: 25s - loss: 0.6719 - accuracy: 0.797 - ETA: 25s - loss: 0.6720 - accuracy: 0.796 - ETA: 24s - loss: 0.6695 - accuracy: 0.797 - ETA: 23s - loss: 0.6684 - accuracy: 0.796 - ETA: 22s - loss: 0.6671 - accuracy: 0.796 - ETA: 21s - loss: 0.6663 - accuracy: 0.797 - ETA: 20s - loss: 0.6647 - accuracy: 0.797 - ETA: 19s - loss: 0.6645 - accuracy: 0.797 - ETA: 18s - loss: 0.6630 - accuracy: 0.797 - ETA: 17s - loss: 0.6627 - accuracy: 0.797 - ETA: 16s - loss: 0.6624 - accuracy: 0.797 - ETA: 15s - loss: 0.6638 - accuracy: 0.797 - ETA: 15s - loss: 0.6611 - accuracy: 0.798 - ETA: 14s - loss: 0.6602 - accuracy: 0.798 - ETA: 13s - loss: 0.6630 - accuracy: 0.798 - ETA: 12s - loss: 0.6630 - accuracy: 0.797 - ETA: 11s - loss: 0.6628 - accuracy: 0.798 - ETA: 10s - loss: 0.6634 - accuracy: 0.798 - ETA: 9s - loss: 0.6648 - accuracy: 0.797 - ETA: 8s - loss: 0.6632 - accuracy: 0.79 - ETA: 7s - loss: 0.6623 - accuracy: 0.79 - ETA: 6s - loss: 0.6625 - accuracy: 0.79 - ETA: 6s - loss: 0.6613 - accuracy: 0.79 - ETA: 5s - loss: 0.6596 - accuracy: 0.79 - ETA: 4s - loss: 0.6582 - accuracy: 0.79 - ETA: 3s - loss: 0.6592 - accuracy: 0.79 - ETA: 2s - loss: 0.6597 - accuracy: 0.79 - ETA: 1s - loss: 0.6622 - accuracy: 0.79 - ETA: 0s - loss: 0.6630 - accuracy: 0.79 - 105s 8ms/step - loss: 0.6641 - accuracy: 0.7981 - val_loss: 3.1595 - val_accuracy: 0.3389\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:28 - loss: 0.5859 - accuracy: 0.81 - ETA: 1:33 - loss: 0.7082 - accuracy: 0.79 - ETA: 1:30 - loss: 0.7344 - accuracy: 0.78 - ETA: 1:30 - loss: 0.7561 - accuracy: 0.76 - ETA: 1:30 - loss: 0.7446 - accuracy: 0.76 - ETA: 1:29 - loss: 0.7192 - accuracy: 0.77 - ETA: 1:28 - loss: 0.7233 - accuracy: 0.77 - ETA: 1:26 - loss: 0.7249 - accuracy: 0.76 - ETA: 1:25 - loss: 0.7178 - accuracy: 0.77 - ETA: 1:24 - loss: 0.7551 - accuracy: 0.77 - ETA: 1:22 - loss: 0.7367 - accuracy: 0.77 - ETA: 1:21 - loss: 0.7329 - accuracy: 0.77 - ETA: 1:20 - loss: 0.7319 - accuracy: 0.77 - ETA: 1:19 - loss: 0.7143 - accuracy: 0.78 - ETA: 1:18 - loss: 0.7246 - accuracy: 0.78 - ETA: 1:17 - loss: 0.7235 - accuracy: 0.78 - ETA: 1:16 - loss: 0.7386 - accuracy: 0.78 - ETA: 1:15 - loss: 0.7384 - accuracy: 0.77 - ETA: 1:14 - loss: 0.7370 - accuracy: 0.78 - ETA: 1:14 - loss: 0.7375 - accuracy: 0.78 - ETA: 1:13 - loss: 0.7381 - accuracy: 0.78 - ETA: 1:12 - loss: 0.7381 - accuracy: 0.78 - ETA: 1:11 - loss: 0.7458 - accuracy: 0.78 - ETA: 1:10 - loss: 0.7489 - accuracy: 0.77 - ETA: 1:09 - loss: 0.7553 - accuracy: 0.77 - ETA: 1:08 - loss: 0.7552 - accuracy: 0.78 - ETA: 1:07 - loss: 0.7530 - accuracy: 0.77 - ETA: 1:06 - loss: 0.7488 - accuracy: 0.77 - ETA: 1:06 - loss: 0.7509 - accuracy: 0.77 - ETA: 1:05 - loss: 0.7538 - accuracy: 0.77 - ETA: 1:04 - loss: 0.7484 - accuracy: 0.77 - ETA: 1:03 - loss: 0.7445 - accuracy: 0.77 - ETA: 1:02 - loss: 0.7386 - accuracy: 0.78 - ETA: 1:01 - loss: 0.7375 - accuracy: 0.78 - ETA: 1:00 - loss: 0.7331 - accuracy: 0.78 - ETA: 59s - loss: 0.7256 - accuracy: 0.7849 - ETA: 58s - loss: 0.7214 - accuracy: 0.785 - ETA: 57s - loss: 0.7240 - accuracy: 0.784 - ETA: 56s - loss: 0.7236 - accuracy: 0.784 - ETA: 55s - loss: 0.7174 - accuracy: 0.785 - ETA: 54s - loss: 0.7144 - accuracy: 0.785 - ETA: 54s - loss: 0.7144 - accuracy: 0.785 - ETA: 53s - loss: 0.7133 - accuracy: 0.784 - ETA: 52s - loss: 0.7102 - accuracy: 0.785 - ETA: 51s - loss: 0.7080 - accuracy: 0.786 - ETA: 50s - loss: 0.7061 - accuracy: 0.787 - ETA: 49s - loss: 0.7027 - accuracy: 0.787 - ETA: 48s - loss: 0.7033 - accuracy: 0.787 - ETA: 47s - loss: 0.7039 - accuracy: 0.788 - ETA: 46s - loss: 0.7014 - accuracy: 0.789 - ETA: 45s - loss: 0.7071 - accuracy: 0.787 - ETA: 44s - loss: 0.7048 - accuracy: 0.788 - ETA: 44s - loss: 0.7021 - accuracy: 0.789 - ETA: 43s - loss: 0.7007 - accuracy: 0.789 - ETA: 42s - loss: 0.7011 - accuracy: 0.789 - ETA: 41s - loss: 0.7004 - accuracy: 0.789 - ETA: 40s - loss: 0.6971 - accuracy: 0.790 - ETA: 39s - loss: 0.6937 - accuracy: 0.791 - ETA: 38s - loss: 0.6918 - accuracy: 0.792 - ETA: 37s - loss: 0.6905 - accuracy: 0.791 - ETA: 37s - loss: 0.6921 - accuracy: 0.791 - ETA: 36s - loss: 0.6906 - accuracy: 0.792 - ETA: 35s - loss: 0.6928 - accuracy: 0.791 - ETA: 34s - loss: 0.6905 - accuracy: 0.792 - ETA: 33s - loss: 0.6881 - accuracy: 0.792 - ETA: 32s - loss: 0.6879 - accuracy: 0.792 - ETA: 31s - loss: 0.6875 - accuracy: 0.792 - ETA: 30s - loss: 0.6858 - accuracy: 0.792 - ETA: 29s - loss: 0.6886 - accuracy: 0.791 - ETA: 28s - loss: 0.6859 - accuracy: 0.792 - ETA: 27s - loss: 0.6844 - accuracy: 0.792 - ETA: 26s - loss: 0.6854 - accuracy: 0.792 - ETA: 25s - loss: 0.6841 - accuracy: 0.793 - ETA: 25s - loss: 0.6840 - accuracy: 0.792 - ETA: 24s - loss: 0.6847 - accuracy: 0.792 - ETA: 23s - loss: 0.6834 - accuracy: 0.792 - ETA: 22s - loss: 0.6804 - accuracy: 0.793 - ETA: 21s - loss: 0.6821 - accuracy: 0.792 - ETA: 20s - loss: 0.6820 - accuracy: 0.793 - ETA: 19s - loss: 0.6827 - accuracy: 0.791 - ETA: 18s - loss: 0.6808 - accuracy: 0.792 - ETA: 17s - loss: 0.6784 - accuracy: 0.792 - ETA: 16s - loss: 0.6792 - accuracy: 0.792 - ETA: 16s - loss: 0.6812 - accuracy: 0.792 - ETA: 15s - loss: 0.6806 - accuracy: 0.792 - ETA: 14s - loss: 0.6806 - accuracy: 0.792 - ETA: 13s - loss: 0.6798 - accuracy: 0.793 - ETA: 12s - loss: 0.6791 - accuracy: 0.793 - ETA: 11s - loss: 0.6782 - accuracy: 0.793 - ETA: 10s - loss: 0.6782 - accuracy: 0.793 - ETA: 9s - loss: 0.6777 - accuracy: 0.793 - ETA: 8s - loss: 0.6761 - accuracy: 0.79 - ETA: 7s - loss: 0.6758 - accuracy: 0.79 - ETA: 6s - loss: 0.6758 - accuracy: 0.79 - ETA: 6s - loss: 0.6737 - accuracy: 0.79 - ETA: 5s - loss: 0.6748 - accuracy: 0.79 - ETA: 4s - loss: 0.6749 - accuracy: 0.79 - ETA: 3s - loss: 0.6734 - accuracy: 0.79 - ETA: 2s - loss: 0.6739 - accuracy: 0.79 - ETA: 1s - loss: 0.6743 - accuracy: 0.79 - ETA: 0s - loss: 0.6737 - accuracy: 0.79 - 105s 8ms/step - loss: 0.6728 - accuracy: 0.7942 - val_loss: 3.0276 - val_accuracy: 0.3491\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:37 - loss: 0.7562 - accuracy: 0.77 - ETA: 1:32 - loss: 0.7110 - accuracy: 0.80 - ETA: 1:30 - loss: 0.6650 - accuracy: 0.80 - ETA: 1:28 - loss: 0.6629 - accuracy: 0.80 - ETA: 1:27 - loss: 0.6863 - accuracy: 0.80 - ETA: 1:26 - loss: 0.6691 - accuracy: 0.80 - ETA: 1:25 - loss: 0.6541 - accuracy: 0.81 - ETA: 1:24 - loss: 0.6670 - accuracy: 0.80 - ETA: 1:23 - loss: 0.6757 - accuracy: 0.79 - ETA: 1:22 - loss: 0.6861 - accuracy: 0.79 - ETA: 1:21 - loss: 0.6691 - accuracy: 0.80 - ETA: 1:21 - loss: 0.6684 - accuracy: 0.80 - ETA: 1:20 - loss: 0.6695 - accuracy: 0.80 - ETA: 1:20 - loss: 0.6717 - accuracy: 0.80 - ETA: 1:19 - loss: 0.6568 - accuracy: 0.80 - ETA: 1:18 - loss: 0.6529 - accuracy: 0.80 - ETA: 1:17 - loss: 0.6484 - accuracy: 0.80 - ETA: 1:16 - loss: 0.6560 - accuracy: 0.80 - ETA: 1:15 - loss: 0.6529 - accuracy: 0.80 - ETA: 1:14 - loss: 0.6419 - accuracy: 0.80 - ETA: 1:13 - loss: 0.6446 - accuracy: 0.80 - ETA: 1:12 - loss: 0.6502 - accuracy: 0.80 - ETA: 1:11 - loss: 0.6612 - accuracy: 0.79 - ETA: 1:10 - loss: 0.6588 - accuracy: 0.79 - ETA: 1:09 - loss: 0.6572 - accuracy: 0.79 - ETA: 1:08 - loss: 0.6602 - accuracy: 0.79 - ETA: 1:07 - loss: 0.6625 - accuracy: 0.79 - ETA: 1:06 - loss: 0.6625 - accuracy: 0.79 - ETA: 1:05 - loss: 0.6635 - accuracy: 0.79 - ETA: 1:04 - loss: 0.6673 - accuracy: 0.79 - ETA: 1:04 - loss: 0.6662 - accuracy: 0.79 - ETA: 1:03 - loss: 0.6636 - accuracy: 0.79 - ETA: 1:02 - loss: 0.6650 - accuracy: 0.79 - ETA: 1:01 - loss: 0.6621 - accuracy: 0.79 - ETA: 1:00 - loss: 0.6574 - accuracy: 0.79 - ETA: 59s - loss: 0.6570 - accuracy: 0.7971 - ETA: 58s - loss: 0.6596 - accuracy: 0.796 - ETA: 57s - loss: 0.6569 - accuracy: 0.796 - ETA: 56s - loss: 0.6594 - accuracy: 0.796 - ETA: 55s - loss: 0.6639 - accuracy: 0.794 - ETA: 54s - loss: 0.6652 - accuracy: 0.794 - ETA: 54s - loss: 0.6641 - accuracy: 0.793 - ETA: 53s - loss: 0.6640 - accuracy: 0.794 - ETA: 52s - loss: 0.6603 - accuracy: 0.795 - ETA: 51s - loss: 0.6581 - accuracy: 0.796 - ETA: 50s - loss: 0.6562 - accuracy: 0.797 - ETA: 49s - loss: 0.6548 - accuracy: 0.797 - ETA: 48s - loss: 0.6570 - accuracy: 0.797 - ETA: 47s - loss: 0.6593 - accuracy: 0.796 - ETA: 46s - loss: 0.6573 - accuracy: 0.797 - ETA: 45s - loss: 0.6538 - accuracy: 0.798 - ETA: 44s - loss: 0.6569 - accuracy: 0.797 - ETA: 44s - loss: 0.6529 - accuracy: 0.798 - ETA: 43s - loss: 0.6506 - accuracy: 0.799 - ETA: 42s - loss: 0.6536 - accuracy: 0.799 - ETA: 41s - loss: 0.6534 - accuracy: 0.798 - ETA: 40s - loss: 0.6539 - accuracy: 0.797 - ETA: 39s - loss: 0.6541 - accuracy: 0.797 - ETA: 38s - loss: 0.6543 - accuracy: 0.797 - ETA: 37s - loss: 0.6559 - accuracy: 0.798 - ETA: 36s - loss: 0.6560 - accuracy: 0.798 - ETA: 35s - loss: 0.6558 - accuracy: 0.798 - ETA: 34s - loss: 0.6561 - accuracy: 0.798 - ETA: 33s - loss: 0.6570 - accuracy: 0.798 - ETA: 33s - loss: 0.6551 - accuracy: 0.798 - ETA: 32s - loss: 0.6544 - accuracy: 0.798 - ETA: 31s - loss: 0.6528 - accuracy: 0.798 - ETA: 30s - loss: 0.6518 - accuracy: 0.799 - ETA: 29s - loss: 0.6509 - accuracy: 0.799 - ETA: 28s - loss: 0.6511 - accuracy: 0.799 - ETA: 27s - loss: 0.6513 - accuracy: 0.799 - ETA: 26s - loss: 0.6526 - accuracy: 0.798 - ETA: 25s - loss: 0.6557 - accuracy: 0.798 - ETA: 25s - loss: 0.6539 - accuracy: 0.798 - ETA: 24s - loss: 0.6531 - accuracy: 0.798 - ETA: 23s - loss: 0.6526 - accuracy: 0.799 - ETA: 22s - loss: 0.6492 - accuracy: 0.800 - ETA: 21s - loss: 0.6489 - accuracy: 0.800 - ETA: 20s - loss: 0.6490 - accuracy: 0.800 - ETA: 19s - loss: 0.6496 - accuracy: 0.800 - ETA: 18s - loss: 0.6500 - accuracy: 0.800 - ETA: 17s - loss: 0.6499 - accuracy: 0.800 - ETA: 16s - loss: 0.6500 - accuracy: 0.800 - ETA: 15s - loss: 0.6497 - accuracy: 0.800 - ETA: 15s - loss: 0.6504 - accuracy: 0.801 - ETA: 14s - loss: 0.6498 - accuracy: 0.801 - ETA: 13s - loss: 0.6483 - accuracy: 0.801 - ETA: 12s - loss: 0.6480 - accuracy: 0.802 - ETA: 11s - loss: 0.6506 - accuracy: 0.801 - ETA: 10s - loss: 0.6495 - accuracy: 0.802 - ETA: 9s - loss: 0.6482 - accuracy: 0.802 - ETA: 8s - loss: 0.6465 - accuracy: 0.80 - ETA: 7s - loss: 0.6492 - accuracy: 0.80 - ETA: 6s - loss: 0.6512 - accuracy: 0.80 - ETA: 6s - loss: 0.6509 - accuracy: 0.80 - ETA: 5s - loss: 0.6509 - accuracy: 0.80 - ETA: 4s - loss: 0.6504 - accuracy: 0.80 - ETA: 3s - loss: 0.6502 - accuracy: 0.80 - ETA: 2s - loss: 0.6526 - accuracy: 0.80 - ETA: 1s - loss: 0.6546 - accuracy: 0.80 - ETA: 0s - loss: 0.6539 - accuracy: 0.80 - 105s 8ms/step - loss: 0.6537 - accuracy: 0.8009 - val_loss: 3.0923 - val_accuracy: 0.3458\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:35 - loss: 0.7693 - accuracy: 0.76 - ETA: 1:34 - loss: 0.7246 - accuracy: 0.77 - ETA: 1:32 - loss: 0.7082 - accuracy: 0.78 - ETA: 1:31 - loss: 0.7005 - accuracy: 0.78 - ETA: 1:30 - loss: 0.6726 - accuracy: 0.79 - ETA: 1:28 - loss: 0.6649 - accuracy: 0.79 - ETA: 1:27 - loss: 0.6605 - accuracy: 0.79 - ETA: 1:25 - loss: 0.6682 - accuracy: 0.79 - ETA: 1:24 - loss: 0.6734 - accuracy: 0.78 - ETA: 1:23 - loss: 0.6530 - accuracy: 0.78 - ETA: 1:22 - loss: 0.6453 - accuracy: 0.79 - ETA: 1:21 - loss: 0.6509 - accuracy: 0.79 - ETA: 1:20 - loss: 0.6714 - accuracy: 0.79 - ETA: 1:19 - loss: 0.6751 - accuracy: 0.79 - ETA: 1:18 - loss: 0.6797 - accuracy: 0.79 - ETA: 1:17 - loss: 0.6681 - accuracy: 0.79 - ETA: 1:16 - loss: 0.6677 - accuracy: 0.79 - ETA: 1:15 - loss: 0.6587 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6545 - accuracy: 0.79 - ETA: 1:13 - loss: 0.6505 - accuracy: 0.79 - ETA: 1:12 - loss: 0.6491 - accuracy: 0.79 - ETA: 1:12 - loss: 0.6532 - accuracy: 0.79 - ETA: 1:11 - loss: 0.6567 - accuracy: 0.79 - ETA: 1:10 - loss: 0.6524 - accuracy: 0.79 - ETA: 1:09 - loss: 0.6565 - accuracy: 0.79 - ETA: 1:08 - loss: 0.6532 - accuracy: 0.79 - ETA: 1:07 - loss: 0.6514 - accuracy: 0.79 - ETA: 1:06 - loss: 0.6513 - accuracy: 0.79 - ETA: 1:05 - loss: 0.6524 - accuracy: 0.79 - ETA: 1:04 - loss: 0.6533 - accuracy: 0.79 - ETA: 1:03 - loss: 0.6495 - accuracy: 0.79 - ETA: 1:02 - loss: 0.6531 - accuracy: 0.80 - ETA: 1:01 - loss: 0.6585 - accuracy: 0.79 - ETA: 1:00 - loss: 0.6594 - accuracy: 0.79 - ETA: 1:00 - loss: 0.6572 - accuracy: 0.79 - ETA: 59s - loss: 0.6599 - accuracy: 0.7967 - ETA: 58s - loss: 0.6605 - accuracy: 0.795 - ETA: 57s - loss: 0.6554 - accuracy: 0.797 - ETA: 56s - loss: 0.6579 - accuracy: 0.797 - ETA: 55s - loss: 0.6581 - accuracy: 0.797 - ETA: 54s - loss: 0.6584 - accuracy: 0.798 - ETA: 53s - loss: 0.6528 - accuracy: 0.798 - ETA: 53s - loss: 0.6542 - accuracy: 0.798 - ETA: 52s - loss: 0.6553 - accuracy: 0.797 - ETA: 51s - loss: 0.6534 - accuracy: 0.797 - ETA: 50s - loss: 0.6569 - accuracy: 0.796 - ETA: 49s - loss: 0.6561 - accuracy: 0.796 - ETA: 48s - loss: 0.6575 - accuracy: 0.796 - ETA: 47s - loss: 0.6602 - accuracy: 0.795 - ETA: 46s - loss: 0.6565 - accuracy: 0.797 - ETA: 45s - loss: 0.6587 - accuracy: 0.795 - ETA: 44s - loss: 0.6617 - accuracy: 0.795 - ETA: 43s - loss: 0.6620 - accuracy: 0.795 - ETA: 43s - loss: 0.6621 - accuracy: 0.795 - ETA: 42s - loss: 0.6618 - accuracy: 0.795 - ETA: 41s - loss: 0.6615 - accuracy: 0.796 - ETA: 40s - loss: 0.6601 - accuracy: 0.796 - ETA: 39s - loss: 0.6637 - accuracy: 0.796 - ETA: 38s - loss: 0.6654 - accuracy: 0.795 - ETA: 37s - loss: 0.6657 - accuracy: 0.795 - ETA: 36s - loss: 0.6650 - accuracy: 0.795 - ETA: 35s - loss: 0.6649 - accuracy: 0.795 - ETA: 34s - loss: 0.6662 - accuracy: 0.794 - ETA: 34s - loss: 0.6655 - accuracy: 0.795 - ETA: 33s - loss: 0.6699 - accuracy: 0.794 - ETA: 32s - loss: 0.6668 - accuracy: 0.796 - ETA: 31s - loss: 0.6702 - accuracy: 0.796 - ETA: 30s - loss: 0.6683 - accuracy: 0.796 - ETA: 29s - loss: 0.6664 - accuracy: 0.797 - ETA: 28s - loss: 0.6638 - accuracy: 0.797 - ETA: 27s - loss: 0.6637 - accuracy: 0.797 - ETA: 26s - loss: 0.6640 - accuracy: 0.798 - ETA: 25s - loss: 0.6649 - accuracy: 0.797 - ETA: 25s - loss: 0.6672 - accuracy: 0.796 - ETA: 24s - loss: 0.6683 - accuracy: 0.796 - ETA: 23s - loss: 0.6665 - accuracy: 0.796 - ETA: 22s - loss: 0.6650 - accuracy: 0.796 - ETA: 21s - loss: 0.6638 - accuracy: 0.797 - ETA: 20s - loss: 0.6646 - accuracy: 0.797 - ETA: 19s - loss: 0.6635 - accuracy: 0.797 - ETA: 18s - loss: 0.6645 - accuracy: 0.797 - ETA: 17s - loss: 0.6645 - accuracy: 0.797 - ETA: 16s - loss: 0.6616 - accuracy: 0.798 - ETA: 15s - loss: 0.6636 - accuracy: 0.797 - ETA: 15s - loss: 0.6653 - accuracy: 0.797 - ETA: 14s - loss: 0.6652 - accuracy: 0.797 - ETA: 13s - loss: 0.6635 - accuracy: 0.797 - ETA: 12s - loss: 0.6645 - accuracy: 0.797 - ETA: 11s - loss: 0.6667 - accuracy: 0.797 - ETA: 10s - loss: 0.6675 - accuracy: 0.797 - ETA: 9s - loss: 0.6689 - accuracy: 0.797 - ETA: 8s - loss: 0.6698 - accuracy: 0.79 - ETA: 7s - loss: 0.6708 - accuracy: 0.79 - ETA: 6s - loss: 0.6720 - accuracy: 0.79 - ETA: 6s - loss: 0.6711 - accuracy: 0.79 - ETA: 5s - loss: 0.6710 - accuracy: 0.79 - ETA: 4s - loss: 0.6709 - accuracy: 0.79 - ETA: 3s - loss: 0.6715 - accuracy: 0.79 - ETA: 2s - loss: 0.6732 - accuracy: 0.79 - ETA: 1s - loss: 0.6729 - accuracy: 0.79 - ETA: 0s - loss: 0.6752 - accuracy: 0.79 - 105s 8ms/step - loss: 0.6756 - accuracy: 0.7949 - val_loss: 2.9776 - val_accuracy: 0.3554\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:29 - loss: 0.8225 - accuracy: 0.75 - ETA: 1:30 - loss: 0.7406 - accuracy: 0.78 - ETA: 1:28 - loss: 0.7531 - accuracy: 0.77 - ETA: 1:27 - loss: 0.7480 - accuracy: 0.76 - ETA: 1:25 - loss: 0.7162 - accuracy: 0.77 - ETA: 1:24 - loss: 0.6965 - accuracy: 0.77 - ETA: 1:23 - loss: 0.6776 - accuracy: 0.78 - ETA: 1:22 - loss: 0.6790 - accuracy: 0.78 - ETA: 1:22 - loss: 0.6693 - accuracy: 0.78 - ETA: 1:21 - loss: 0.6484 - accuracy: 0.79 - ETA: 1:21 - loss: 0.6452 - accuracy: 0.79 - ETA: 1:20 - loss: 0.6580 - accuracy: 0.79 - ETA: 1:19 - loss: 0.6724 - accuracy: 0.79 - ETA: 1:18 - loss: 0.6730 - accuracy: 0.79 - ETA: 1:17 - loss: 0.6782 - accuracy: 0.79 - ETA: 1:17 - loss: 0.6796 - accuracy: 0.79 - ETA: 1:16 - loss: 0.6832 - accuracy: 0.79 - ETA: 1:15 - loss: 0.6845 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6872 - accuracy: 0.79 - ETA: 1:13 - loss: 0.7052 - accuracy: 0.78 - ETA: 1:12 - loss: 0.6998 - accuracy: 0.78 - ETA: 1:11 - loss: 0.6915 - accuracy: 0.79 - ETA: 1:10 - loss: 0.6831 - accuracy: 0.79 - ETA: 1:09 - loss: 0.6827 - accuracy: 0.79 - ETA: 1:08 - loss: 0.6751 - accuracy: 0.79 - ETA: 1:07 - loss: 0.6679 - accuracy: 0.79 - ETA: 1:07 - loss: 0.6679 - accuracy: 0.79 - ETA: 1:06 - loss: 0.6668 - accuracy: 0.79 - ETA: 1:05 - loss: 0.6693 - accuracy: 0.79 - ETA: 1:04 - loss: 0.6689 - accuracy: 0.79 - ETA: 1:03 - loss: 0.6673 - accuracy: 0.79 - ETA: 1:02 - loss: 0.6675 - accuracy: 0.79 - ETA: 1:01 - loss: 0.6672 - accuracy: 0.79 - ETA: 1:01 - loss: 0.6657 - accuracy: 0.79 - ETA: 1:00 - loss: 0.6673 - accuracy: 0.79 - ETA: 59s - loss: 0.6656 - accuracy: 0.8001 - ETA: 58s - loss: 0.6607 - accuracy: 0.801 - ETA: 57s - loss: 0.6619 - accuracy: 0.802 - ETA: 56s - loss: 0.6631 - accuracy: 0.801 - ETA: 55s - loss: 0.6649 - accuracy: 0.801 - ETA: 54s - loss: 0.6632 - accuracy: 0.802 - ETA: 53s - loss: 0.6636 - accuracy: 0.802 - ETA: 52s - loss: 0.6637 - accuracy: 0.801 - ETA: 51s - loss: 0.6651 - accuracy: 0.801 - ETA: 51s - loss: 0.6666 - accuracy: 0.801 - ETA: 50s - loss: 0.6653 - accuracy: 0.801 - ETA: 49s - loss: 0.6651 - accuracy: 0.801 - ETA: 48s - loss: 0.6623 - accuracy: 0.802 - ETA: 47s - loss: 0.6636 - accuracy: 0.801 - ETA: 46s - loss: 0.6677 - accuracy: 0.801 - ETA: 45s - loss: 0.6730 - accuracy: 0.800 - ETA: 44s - loss: 0.6719 - accuracy: 0.801 - ETA: 43s - loss: 0.6725 - accuracy: 0.800 - ETA: 43s - loss: 0.6719 - accuracy: 0.800 - ETA: 42s - loss: 0.6708 - accuracy: 0.801 - ETA: 41s - loss: 0.6706 - accuracy: 0.801 - ETA: 40s - loss: 0.6706 - accuracy: 0.801 - ETA: 39s - loss: 0.6721 - accuracy: 0.801 - ETA: 38s - loss: 0.6723 - accuracy: 0.801 - ETA: 37s - loss: 0.6736 - accuracy: 0.801 - ETA: 36s - loss: 0.6740 - accuracy: 0.801 - ETA: 35s - loss: 0.6735 - accuracy: 0.801 - ETA: 34s - loss: 0.6721 - accuracy: 0.802 - ETA: 33s - loss: 0.6705 - accuracy: 0.802 - ETA: 33s - loss: 0.6711 - accuracy: 0.802 - ETA: 32s - loss: 0.6723 - accuracy: 0.802 - ETA: 31s - loss: 0.6734 - accuracy: 0.801 - ETA: 30s - loss: 0.6736 - accuracy: 0.801 - ETA: 29s - loss: 0.6717 - accuracy: 0.801 - ETA: 28s - loss: 0.6722 - accuracy: 0.801 - ETA: 27s - loss: 0.6745 - accuracy: 0.800 - ETA: 26s - loss: 0.6730 - accuracy: 0.801 - ETA: 25s - loss: 0.6727 - accuracy: 0.801 - ETA: 25s - loss: 0.6721 - accuracy: 0.801 - ETA: 24s - loss: 0.6768 - accuracy: 0.800 - ETA: 23s - loss: 0.6757 - accuracy: 0.800 - ETA: 22s - loss: 0.6738 - accuracy: 0.800 - ETA: 21s - loss: 0.6760 - accuracy: 0.800 - ETA: 20s - loss: 0.6781 - accuracy: 0.799 - ETA: 19s - loss: 0.6799 - accuracy: 0.799 - ETA: 18s - loss: 0.6784 - accuracy: 0.799 - ETA: 17s - loss: 0.6772 - accuracy: 0.799 - ETA: 16s - loss: 0.6788 - accuracy: 0.799 - ETA: 16s - loss: 0.6803 - accuracy: 0.799 - ETA: 15s - loss: 0.6818 - accuracy: 0.798 - ETA: 14s - loss: 0.6823 - accuracy: 0.798 - ETA: 13s - loss: 0.6806 - accuracy: 0.799 - ETA: 12s - loss: 0.6804 - accuracy: 0.799 - ETA: 11s - loss: 0.6824 - accuracy: 0.798 - ETA: 10s - loss: 0.6835 - accuracy: 0.798 - ETA: 9s - loss: 0.6846 - accuracy: 0.798 - ETA: 8s - loss: 0.6862 - accuracy: 0.79 - ETA: 7s - loss: 0.6841 - accuracy: 0.79 - ETA: 6s - loss: 0.6825 - accuracy: 0.79 - ETA: 6s - loss: 0.6805 - accuracy: 0.79 - ETA: 5s - loss: 0.6799 - accuracy: 0.79 - ETA: 4s - loss: 0.6814 - accuracy: 0.79 - ETA: 3s - loss: 0.6823 - accuracy: 0.79 - ETA: 2s - loss: 0.6832 - accuracy: 0.79 - ETA: 1s - loss: 0.6829 - accuracy: 0.79 - ETA: 0s - loss: 0.6801 - accuracy: 0.79 - 105s 8ms/step - loss: 0.6807 - accuracy: 0.7977 - val_loss: 3.1080 - val_accuracy: 0.3168\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:31 - loss: 0.6671 - accuracy: 0.79 - ETA: 1:30 - loss: 0.6360 - accuracy: 0.78 - ETA: 1:29 - loss: 0.7172 - accuracy: 0.76 - ETA: 1:28 - loss: 0.7159 - accuracy: 0.77 - ETA: 1:27 - loss: 0.6878 - accuracy: 0.78 - ETA: 1:26 - loss: 0.6934 - accuracy: 0.78 - ETA: 1:25 - loss: 0.6742 - accuracy: 0.79 - ETA: 1:24 - loss: 0.6773 - accuracy: 0.78 - ETA: 1:23 - loss: 0.6824 - accuracy: 0.79 - ETA: 1:22 - loss: 0.6885 - accuracy: 0.79 - ETA: 1:21 - loss: 0.6991 - accuracy: 0.78 - ETA: 1:20 - loss: 0.6924 - accuracy: 0.78 - ETA: 1:19 - loss: 0.6823 - accuracy: 0.79 - ETA: 1:18 - loss: 0.6859 - accuracy: 0.78 - ETA: 1:17 - loss: 0.6778 - accuracy: 0.79 - ETA: 1:16 - loss: 0.6733 - accuracy: 0.79 - ETA: 1:16 - loss: 0.6749 - accuracy: 0.79 - ETA: 1:15 - loss: 0.6702 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6543 - accuracy: 0.80 - ETA: 1:13 - loss: 0.6512 - accuracy: 0.79 - ETA: 1:12 - loss: 0.6468 - accuracy: 0.80 - ETA: 1:11 - loss: 0.6534 - accuracy: 0.80 - ETA: 1:10 - loss: 0.6488 - accuracy: 0.80 - ETA: 1:09 - loss: 0.6473 - accuracy: 0.80 - ETA: 1:09 - loss: 0.6559 - accuracy: 0.80 - ETA: 1:08 - loss: 0.6584 - accuracy: 0.80 - ETA: 1:07 - loss: 0.6650 - accuracy: 0.79 - ETA: 1:06 - loss: 0.6735 - accuracy: 0.79 - ETA: 1:06 - loss: 0.6785 - accuracy: 0.79 - ETA: 1:05 - loss: 0.6802 - accuracy: 0.79 - ETA: 1:04 - loss: 0.6768 - accuracy: 0.79 - ETA: 1:03 - loss: 0.6728 - accuracy: 0.79 - ETA: 1:02 - loss: 0.6681 - accuracy: 0.79 - ETA: 1:01 - loss: 0.6681 - accuracy: 0.79 - ETA: 1:00 - loss: 0.6683 - accuracy: 0.79 - ETA: 59s - loss: 0.6662 - accuracy: 0.7993 - ETA: 58s - loss: 0.6707 - accuracy: 0.797 - ETA: 57s - loss: 0.6724 - accuracy: 0.796 - ETA: 57s - loss: 0.6706 - accuracy: 0.797 - ETA: 56s - loss: 0.6692 - accuracy: 0.798 - ETA: 55s - loss: 0.6681 - accuracy: 0.799 - ETA: 54s - loss: 0.6659 - accuracy: 0.799 - ETA: 53s - loss: 0.6686 - accuracy: 0.798 - ETA: 52s - loss: 0.6689 - accuracy: 0.798 - ETA: 51s - loss: 0.6700 - accuracy: 0.797 - ETA: 50s - loss: 0.6670 - accuracy: 0.799 - ETA: 49s - loss: 0.6634 - accuracy: 0.800 - ETA: 48s - loss: 0.6635 - accuracy: 0.800 - ETA: 47s - loss: 0.6631 - accuracy: 0.800 - ETA: 46s - loss: 0.6651 - accuracy: 0.799 - ETA: 45s - loss: 0.6627 - accuracy: 0.800 - ETA: 45s - loss: 0.6649 - accuracy: 0.799 - ETA: 44s - loss: 0.6673 - accuracy: 0.798 - ETA: 43s - loss: 0.6681 - accuracy: 0.798 - ETA: 42s - loss: 0.6682 - accuracy: 0.798 - ETA: 41s - loss: 0.6671 - accuracy: 0.799 - ETA: 40s - loss: 0.6649 - accuracy: 0.799 - ETA: 39s - loss: 0.6649 - accuracy: 0.799 - ETA: 38s - loss: 0.6659 - accuracy: 0.799 - ETA: 37s - loss: 0.6651 - accuracy: 0.799 - ETA: 36s - loss: 0.6643 - accuracy: 0.799 - ETA: 35s - loss: 0.6636 - accuracy: 0.800 - ETA: 34s - loss: 0.6628 - accuracy: 0.800 - ETA: 34s - loss: 0.6644 - accuracy: 0.800 - ETA: 33s - loss: 0.6634 - accuracy: 0.800 - ETA: 32s - loss: 0.6626 - accuracy: 0.801 - ETA: 31s - loss: 0.6640 - accuracy: 0.800 - ETA: 30s - loss: 0.6644 - accuracy: 0.800 - ETA: 29s - loss: 0.6636 - accuracy: 0.800 - ETA: 28s - loss: 0.6617 - accuracy: 0.800 - ETA: 27s - loss: 0.6611 - accuracy: 0.800 - ETA: 26s - loss: 0.6628 - accuracy: 0.800 - ETA: 25s - loss: 0.6626 - accuracy: 0.800 - ETA: 25s - loss: 0.6623 - accuracy: 0.800 - ETA: 24s - loss: 0.6617 - accuracy: 0.800 - ETA: 23s - loss: 0.6607 - accuracy: 0.800 - ETA: 22s - loss: 0.6609 - accuracy: 0.800 - ETA: 21s - loss: 0.6596 - accuracy: 0.800 - ETA: 20s - loss: 0.6609 - accuracy: 0.800 - ETA: 19s - loss: 0.6658 - accuracy: 0.799 - ETA: 18s - loss: 0.6671 - accuracy: 0.799 - ETA: 17s - loss: 0.6700 - accuracy: 0.798 - ETA: 16s - loss: 0.6711 - accuracy: 0.797 - ETA: 16s - loss: 0.6705 - accuracy: 0.797 - ETA: 15s - loss: 0.6710 - accuracy: 0.797 - ETA: 14s - loss: 0.6711 - accuracy: 0.798 - ETA: 13s - loss: 0.6706 - accuracy: 0.798 - ETA: 12s - loss: 0.6698 - accuracy: 0.798 - ETA: 11s - loss: 0.6688 - accuracy: 0.798 - ETA: 10s - loss: 0.6699 - accuracy: 0.798 - ETA: 9s - loss: 0.6690 - accuracy: 0.798 - ETA: 8s - loss: 0.6688 - accuracy: 0.79 - ETA: 7s - loss: 0.6700 - accuracy: 0.79 - ETA: 7s - loss: 0.6704 - accuracy: 0.79 - ETA: 6s - loss: 0.6717 - accuracy: 0.79 - ETA: 5s - loss: 0.6716 - accuracy: 0.79 - ETA: 4s - loss: 0.6711 - accuracy: 0.79 - ETA: 3s - loss: 0.6727 - accuracy: 0.79 - ETA: 2s - loss: 0.6715 - accuracy: 0.79 - ETA: 1s - loss: 0.6711 - accuracy: 0.79 - ETA: 0s - loss: 0.6712 - accuracy: 0.79 - 106s 8ms/step - loss: 0.6696 - accuracy: 0.7982 - val_loss: 3.1072 - val_accuracy: 0.3335\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:36 - loss: 0.6749 - accuracy: 0.79 - ETA: 1:31 - loss: 0.5710 - accuracy: 0.83 - ETA: 1:29 - loss: 0.6234 - accuracy: 0.82 - ETA: 1:28 - loss: 0.6708 - accuracy: 0.82 - ETA: 1:27 - loss: 0.6670 - accuracy: 0.82 - ETA: 1:26 - loss: 0.6543 - accuracy: 0.82 - ETA: 1:26 - loss: 0.6457 - accuracy: 0.82 - ETA: 1:25 - loss: 0.6448 - accuracy: 0.81 - ETA: 1:24 - loss: 0.6566 - accuracy: 0.81 - ETA: 1:23 - loss: 0.6669 - accuracy: 0.80 - ETA: 1:22 - loss: 0.6736 - accuracy: 0.80 - ETA: 1:20 - loss: 0.6718 - accuracy: 0.80 - ETA: 1:20 - loss: 0.6633 - accuracy: 0.80 - ETA: 1:19 - loss: 0.6641 - accuracy: 0.79 - ETA: 1:18 - loss: 0.6779 - accuracy: 0.79 - ETA: 1:17 - loss: 0.6778 - accuracy: 0.79 - ETA: 1:16 - loss: 0.6758 - accuracy: 0.79 - ETA: 1:15 - loss: 0.6792 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6736 - accuracy: 0.79 - ETA: 1:13 - loss: 0.6705 - accuracy: 0.79 - ETA: 1:12 - loss: 0.6695 - accuracy: 0.79 - ETA: 1:11 - loss: 0.6740 - accuracy: 0.79 - ETA: 1:10 - loss: 0.6779 - accuracy: 0.79 - ETA: 1:09 - loss: 0.6775 - accuracy: 0.79 - ETA: 1:09 - loss: 0.6727 - accuracy: 0.79 - ETA: 1:08 - loss: 0.6808 - accuracy: 0.79 - ETA: 1:07 - loss: 0.6857 - accuracy: 0.79 - ETA: 1:06 - loss: 0.6806 - accuracy: 0.79 - ETA: 1:05 - loss: 0.6835 - accuracy: 0.79 - ETA: 1:04 - loss: 0.6810 - accuracy: 0.79 - ETA: 1:03 - loss: 0.6899 - accuracy: 0.79 - ETA: 1:02 - loss: 0.6895 - accuracy: 0.79 - ETA: 1:01 - loss: 0.6863 - accuracy: 0.79 - ETA: 1:00 - loss: 0.6855 - accuracy: 0.79 - ETA: 59s - loss: 0.6813 - accuracy: 0.7937 - ETA: 58s - loss: 0.6827 - accuracy: 0.792 - ETA: 58s - loss: 0.6827 - accuracy: 0.792 - ETA: 57s - loss: 0.6841 - accuracy: 0.792 - ETA: 56s - loss: 0.6850 - accuracy: 0.792 - ETA: 55s - loss: 0.6820 - accuracy: 0.793 - ETA: 54s - loss: 0.6857 - accuracy: 0.792 - ETA: 53s - loss: 0.6888 - accuracy: 0.790 - ETA: 52s - loss: 0.6927 - accuracy: 0.790 - ETA: 51s - loss: 0.6918 - accuracy: 0.790 - ETA: 50s - loss: 0.6889 - accuracy: 0.791 - ETA: 49s - loss: 0.6887 - accuracy: 0.792 - ETA: 49s - loss: 0.6882 - accuracy: 0.792 - ETA: 48s - loss: 0.6906 - accuracy: 0.791 - ETA: 47s - loss: 0.6910 - accuracy: 0.791 - ETA: 46s - loss: 0.6891 - accuracy: 0.791 - ETA: 45s - loss: 0.6923 - accuracy: 0.790 - ETA: 44s - loss: 0.6912 - accuracy: 0.790 - ETA: 43s - loss: 0.6908 - accuracy: 0.789 - ETA: 42s - loss: 0.6878 - accuracy: 0.790 - ETA: 41s - loss: 0.6859 - accuracy: 0.791 - ETA: 41s - loss: 0.6889 - accuracy: 0.790 - ETA: 40s - loss: 0.6884 - accuracy: 0.789 - ETA: 39s - loss: 0.6866 - accuracy: 0.790 - ETA: 38s - loss: 0.6851 - accuracy: 0.790 - ETA: 37s - loss: 0.6861 - accuracy: 0.790 - ETA: 36s - loss: 0.6868 - accuracy: 0.790 - ETA: 35s - loss: 0.6856 - accuracy: 0.790 - ETA: 34s - loss: 0.6848 - accuracy: 0.790 - ETA: 33s - loss: 0.6840 - accuracy: 0.791 - ETA: 33s - loss: 0.6834 - accuracy: 0.791 - ETA: 32s - loss: 0.6828 - accuracy: 0.791 - ETA: 31s - loss: 0.6819 - accuracy: 0.791 - ETA: 30s - loss: 0.6790 - accuracy: 0.792 - ETA: 29s - loss: 0.6772 - accuracy: 0.792 - ETA: 28s - loss: 0.6740 - accuracy: 0.793 - ETA: 27s - loss: 0.6731 - accuracy: 0.794 - ETA: 26s - loss: 0.6729 - accuracy: 0.794 - ETA: 25s - loss: 0.6720 - accuracy: 0.794 - ETA: 24s - loss: 0.6704 - accuracy: 0.795 - ETA: 24s - loss: 0.6709 - accuracy: 0.795 - ETA: 23s - loss: 0.6685 - accuracy: 0.795 - ETA: 22s - loss: 0.6671 - accuracy: 0.796 - ETA: 21s - loss: 0.6717 - accuracy: 0.796 - ETA: 20s - loss: 0.6732 - accuracy: 0.795 - ETA: 19s - loss: 0.6743 - accuracy: 0.795 - ETA: 18s - loss: 0.6746 - accuracy: 0.795 - ETA: 17s - loss: 0.6706 - accuracy: 0.796 - ETA: 16s - loss: 0.6679 - accuracy: 0.798 - ETA: 15s - loss: 0.6702 - accuracy: 0.797 - ETA: 15s - loss: 0.6685 - accuracy: 0.797 - ETA: 14s - loss: 0.6689 - accuracy: 0.797 - ETA: 13s - loss: 0.6683 - accuracy: 0.797 - ETA: 12s - loss: 0.6682 - accuracy: 0.797 - ETA: 11s - loss: 0.6689 - accuracy: 0.797 - ETA: 10s - loss: 0.6710 - accuracy: 0.796 - ETA: 9s - loss: 0.6694 - accuracy: 0.796 - ETA: 8s - loss: 0.6679 - accuracy: 0.79 - ETA: 7s - loss: 0.6695 - accuracy: 0.79 - ETA: 6s - loss: 0.6697 - accuracy: 0.79 - ETA: 6s - loss: 0.6710 - accuracy: 0.79 - ETA: 5s - loss: 0.6726 - accuracy: 0.79 - ETA: 4s - loss: 0.6732 - accuracy: 0.79 - ETA: 3s - loss: 0.6736 - accuracy: 0.79 - ETA: 2s - loss: 0.6724 - accuracy: 0.79 - ETA: 1s - loss: 0.6732 - accuracy: 0.79 - ETA: 0s - loss: 0.6729 - accuracy: 0.79 - 105s 8ms/step - loss: 0.6734 - accuracy: 0.7959 - val_loss: 3.3069 - val_accuracy: 0.3030\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.6626 - accuracy: 0.79 - ETA: 1:29 - loss: 0.7342 - accuracy: 0.78 - ETA: 1:29 - loss: 0.6594 - accuracy: 0.79 - ETA: 1:27 - loss: 0.6482 - accuracy: 0.78 - ETA: 1:27 - loss: 0.6494 - accuracy: 0.77 - ETA: 1:26 - loss: 0.6470 - accuracy: 0.77 - ETA: 1:25 - loss: 0.6361 - accuracy: 0.78 - ETA: 1:24 - loss: 0.6341 - accuracy: 0.78 - ETA: 1:22 - loss: 0.6446 - accuracy: 0.77 - ETA: 1:22 - loss: 0.6511 - accuracy: 0.78 - ETA: 1:22 - loss: 0.6616 - accuracy: 0.77 - ETA: 1:21 - loss: 0.6685 - accuracy: 0.77 - ETA: 1:20 - loss: 0.6683 - accuracy: 0.77 - ETA: 1:20 - loss: 0.6641 - accuracy: 0.77 - ETA: 1:19 - loss: 0.6625 - accuracy: 0.78 - ETA: 1:18 - loss: 0.6694 - accuracy: 0.78 - ETA: 1:17 - loss: 0.6849 - accuracy: 0.78 - ETA: 1:16 - loss: 0.6903 - accuracy: 0.78 - ETA: 1:15 - loss: 0.6827 - accuracy: 0.78 - ETA: 1:14 - loss: 0.6830 - accuracy: 0.78 - ETA: 1:13 - loss: 0.6933 - accuracy: 0.78 - ETA: 1:12 - loss: 0.7010 - accuracy: 0.77 - ETA: 1:11 - loss: 0.6924 - accuracy: 0.78 - ETA: 1:10 - loss: 0.6857 - accuracy: 0.78 - ETA: 1:09 - loss: 0.6810 - accuracy: 0.78 - ETA: 1:08 - loss: 0.6870 - accuracy: 0.78 - ETA: 1:07 - loss: 0.6898 - accuracy: 0.78 - ETA: 1:06 - loss: 0.6883 - accuracy: 0.78 - ETA: 1:05 - loss: 0.6821 - accuracy: 0.78 - ETA: 1:04 - loss: 0.6843 - accuracy: 0.78 - ETA: 1:04 - loss: 0.6905 - accuracy: 0.78 - ETA: 1:03 - loss: 0.6868 - accuracy: 0.78 - ETA: 1:02 - loss: 0.6894 - accuracy: 0.78 - ETA: 1:01 - loss: 0.6896 - accuracy: 0.78 - ETA: 1:00 - loss: 0.6902 - accuracy: 0.78 - ETA: 59s - loss: 0.6879 - accuracy: 0.7823 - ETA: 58s - loss: 0.6844 - accuracy: 0.782 - ETA: 57s - loss: 0.6828 - accuracy: 0.782 - ETA: 56s - loss: 0.6875 - accuracy: 0.782 - ETA: 55s - loss: 0.6871 - accuracy: 0.783 - ETA: 54s - loss: 0.6832 - accuracy: 0.784 - ETA: 53s - loss: 0.6879 - accuracy: 0.784 - ETA: 52s - loss: 0.6896 - accuracy: 0.783 - ETA: 52s - loss: 0.6866 - accuracy: 0.785 - ETA: 51s - loss: 0.6925 - accuracy: 0.783 - ETA: 50s - loss: 0.6880 - accuracy: 0.785 - ETA: 49s - loss: 0.6883 - accuracy: 0.785 - ETA: 48s - loss: 0.6893 - accuracy: 0.785 - ETA: 47s - loss: 0.6891 - accuracy: 0.785 - ETA: 46s - loss: 0.6886 - accuracy: 0.784 - ETA: 45s - loss: 0.6900 - accuracy: 0.784 - ETA: 44s - loss: 0.6865 - accuracy: 0.785 - ETA: 43s - loss: 0.6878 - accuracy: 0.785 - ETA: 42s - loss: 0.6905 - accuracy: 0.785 - ETA: 42s - loss: 0.6888 - accuracy: 0.785 - ETA: 41s - loss: 0.6899 - accuracy: 0.785 - ETA: 40s - loss: 0.6893 - accuracy: 0.786 - ETA: 39s - loss: 0.6891 - accuracy: 0.786 - ETA: 38s - loss: 0.6866 - accuracy: 0.786 - ETA: 37s - loss: 0.6853 - accuracy: 0.787 - ETA: 36s - loss: 0.6828 - accuracy: 0.788 - ETA: 35s - loss: 0.6860 - accuracy: 0.788 - ETA: 34s - loss: 0.6893 - accuracy: 0.787 - ETA: 33s - loss: 0.6896 - accuracy: 0.788 - ETA: 33s - loss: 0.6903 - accuracy: 0.787 - ETA: 32s - loss: 0.6904 - accuracy: 0.788 - ETA: 31s - loss: 0.6918 - accuracy: 0.787 - ETA: 30s - loss: 0.6888 - accuracy: 0.788 - ETA: 29s - loss: 0.6882 - accuracy: 0.788 - ETA: 28s - loss: 0.6856 - accuracy: 0.789 - ETA: 27s - loss: 0.6850 - accuracy: 0.789 - ETA: 26s - loss: 0.6859 - accuracy: 0.789 - ETA: 25s - loss: 0.6847 - accuracy: 0.789 - ETA: 24s - loss: 0.6844 - accuracy: 0.789 - ETA: 24s - loss: 0.6866 - accuracy: 0.789 - ETA: 23s - loss: 0.6878 - accuracy: 0.789 - ETA: 22s - loss: 0.6877 - accuracy: 0.789 - ETA: 21s - loss: 0.6858 - accuracy: 0.789 - ETA: 20s - loss: 0.6853 - accuracy: 0.789 - ETA: 19s - loss: 0.6833 - accuracy: 0.790 - ETA: 18s - loss: 0.6862 - accuracy: 0.789 - ETA: 17s - loss: 0.6839 - accuracy: 0.789 - ETA: 16s - loss: 0.6881 - accuracy: 0.788 - ETA: 15s - loss: 0.6874 - accuracy: 0.789 - ETA: 15s - loss: 0.6864 - accuracy: 0.790 - ETA: 14s - loss: 0.6875 - accuracy: 0.790 - ETA: 13s - loss: 0.6864 - accuracy: 0.790 - ETA: 12s - loss: 0.6839 - accuracy: 0.791 - ETA: 11s - loss: 0.6844 - accuracy: 0.791 - ETA: 10s - loss: 0.6845 - accuracy: 0.791 - ETA: 9s - loss: 0.6844 - accuracy: 0.790 - ETA: 8s - loss: 0.6831 - accuracy: 0.79 - ETA: 7s - loss: 0.6818 - accuracy: 0.79 - ETA: 6s - loss: 0.6799 - accuracy: 0.79 - ETA: 6s - loss: 0.6817 - accuracy: 0.79 - ETA: 5s - loss: 0.6836 - accuracy: 0.79 - ETA: 4s - loss: 0.6819 - accuracy: 0.79 - ETA: 3s - loss: 0.6813 - accuracy: 0.79 - ETA: 2s - loss: 0.6820 - accuracy: 0.79 - ETA: 1s - loss: 0.6804 - accuracy: 0.79 - ETA: 0s - loss: 0.6797 - accuracy: 0.79 - 105s 8ms/step - loss: 0.6800 - accuracy: 0.7927 - val_loss: 3.1461 - val_accuracy: 0.3180\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:31 - loss: 0.6875 - accuracy: 0.80 - ETA: 1:27 - loss: 0.6824 - accuracy: 0.76 - ETA: 1:29 - loss: 0.7408 - accuracy: 0.76 - ETA: 1:30 - loss: 0.7278 - accuracy: 0.76 - ETA: 1:29 - loss: 0.7315 - accuracy: 0.76 - ETA: 1:28 - loss: 0.7043 - accuracy: 0.77 - ETA: 1:27 - loss: 0.7108 - accuracy: 0.78 - ETA: 1:26 - loss: 0.6972 - accuracy: 0.78 - ETA: 1:24 - loss: 0.6940 - accuracy: 0.78 - ETA: 1:23 - loss: 0.6846 - accuracy: 0.78 - ETA: 1:22 - loss: 0.6877 - accuracy: 0.78 - ETA: 1:21 - loss: 0.6826 - accuracy: 0.78 - ETA: 1:20 - loss: 0.6655 - accuracy: 0.79 - ETA: 1:19 - loss: 0.6587 - accuracy: 0.79 - ETA: 1:18 - loss: 0.6594 - accuracy: 0.79 - ETA: 1:17 - loss: 0.6576 - accuracy: 0.80 - ETA: 1:16 - loss: 0.6475 - accuracy: 0.80 - ETA: 1:15 - loss: 0.6460 - accuracy: 0.80 - ETA: 1:14 - loss: 0.6506 - accuracy: 0.80 - ETA: 1:13 - loss: 0.6487 - accuracy: 0.80 - ETA: 1:12 - loss: 0.6562 - accuracy: 0.80 - ETA: 1:12 - loss: 0.6648 - accuracy: 0.80 - ETA: 1:11 - loss: 0.6606 - accuracy: 0.79 - ETA: 1:10 - loss: 0.6628 - accuracy: 0.80 - ETA: 1:09 - loss: 0.6715 - accuracy: 0.80 - ETA: 1:08 - loss: 0.6735 - accuracy: 0.79 - ETA: 1:07 - loss: 0.6701 - accuracy: 0.80 - ETA: 1:06 - loss: 0.6692 - accuracy: 0.80 - ETA: 1:05 - loss: 0.6687 - accuracy: 0.80 - ETA: 1:04 - loss: 0.6764 - accuracy: 0.80 - ETA: 1:03 - loss: 0.6751 - accuracy: 0.80 - ETA: 1:02 - loss: 0.6772 - accuracy: 0.80 - ETA: 1:01 - loss: 0.6765 - accuracy: 0.80 - ETA: 1:00 - loss: 0.6768 - accuracy: 0.80 - ETA: 1:00 - loss: 0.6721 - accuracy: 0.80 - ETA: 59s - loss: 0.6737 - accuracy: 0.8012 - ETA: 58s - loss: 0.6741 - accuracy: 0.801 - ETA: 57s - loss: 0.6762 - accuracy: 0.802 - ETA: 56s - loss: 0.6809 - accuracy: 0.800 - ETA: 55s - loss: 0.6799 - accuracy: 0.800 - ETA: 54s - loss: 0.6815 - accuracy: 0.799 - ETA: 54s - loss: 0.6826 - accuracy: 0.798 - ETA: 53s - loss: 0.6816 - accuracy: 0.798 - ETA: 52s - loss: 0.6812 - accuracy: 0.799 - ETA: 51s - loss: 0.6833 - accuracy: 0.798 - ETA: 50s - loss: 0.6805 - accuracy: 0.799 - ETA: 49s - loss: 0.6817 - accuracy: 0.799 - ETA: 48s - loss: 0.6813 - accuracy: 0.799 - ETA: 47s - loss: 0.6840 - accuracy: 0.797 - ETA: 46s - loss: 0.6814 - accuracy: 0.798 - ETA: 45s - loss: 0.6850 - accuracy: 0.797 - ETA: 44s - loss: 0.6825 - accuracy: 0.797 - ETA: 44s - loss: 0.6820 - accuracy: 0.797 - ETA: 43s - loss: 0.6803 - accuracy: 0.798 - ETA: 42s - loss: 0.6764 - accuracy: 0.800 - ETA: 41s - loss: 0.6757 - accuracy: 0.800 - ETA: 40s - loss: 0.6746 - accuracy: 0.800 - ETA: 39s - loss: 0.6737 - accuracy: 0.800 - ETA: 38s - loss: 0.6775 - accuracy: 0.800 - ETA: 37s - loss: 0.6762 - accuracy: 0.801 - ETA: 36s - loss: 0.6766 - accuracy: 0.801 - ETA: 35s - loss: 0.6784 - accuracy: 0.801 - ETA: 34s - loss: 0.6800 - accuracy: 0.801 - ETA: 34s - loss: 0.6776 - accuracy: 0.801 - ETA: 33s - loss: 0.6784 - accuracy: 0.801 - ETA: 32s - loss: 0.6759 - accuracy: 0.801 - ETA: 31s - loss: 0.6777 - accuracy: 0.801 - ETA: 30s - loss: 0.6790 - accuracy: 0.800 - ETA: 29s - loss: 0.6774 - accuracy: 0.800 - ETA: 28s - loss: 0.6773 - accuracy: 0.801 - ETA: 27s - loss: 0.6769 - accuracy: 0.801 - ETA: 26s - loss: 0.6785 - accuracy: 0.800 - ETA: 25s - loss: 0.6797 - accuracy: 0.800 - ETA: 25s - loss: 0.6791 - accuracy: 0.799 - ETA: 24s - loss: 0.6799 - accuracy: 0.798 - ETA: 23s - loss: 0.6794 - accuracy: 0.799 - ETA: 22s - loss: 0.6795 - accuracy: 0.799 - ETA: 21s - loss: 0.6777 - accuracy: 0.800 - ETA: 20s - loss: 0.6773 - accuracy: 0.799 - ETA: 19s - loss: 0.6765 - accuracy: 0.799 - ETA: 18s - loss: 0.6758 - accuracy: 0.799 - ETA: 17s - loss: 0.6747 - accuracy: 0.799 - ETA: 16s - loss: 0.6735 - accuracy: 0.799 - ETA: 16s - loss: 0.6726 - accuracy: 0.799 - ETA: 15s - loss: 0.6690 - accuracy: 0.800 - ETA: 14s - loss: 0.6667 - accuracy: 0.800 - ETA: 13s - loss: 0.6672 - accuracy: 0.800 - ETA: 12s - loss: 0.6679 - accuracy: 0.800 - ETA: 11s - loss: 0.6671 - accuracy: 0.800 - ETA: 10s - loss: 0.6659 - accuracy: 0.801 - ETA: 9s - loss: 0.6663 - accuracy: 0.801 - ETA: 8s - loss: 0.6664 - accuracy: 0.80 - ETA: 7s - loss: 0.6650 - accuracy: 0.80 - ETA: 6s - loss: 0.6645 - accuracy: 0.80 - ETA: 6s - loss: 0.6616 - accuracy: 0.80 - ETA: 5s - loss: 0.6629 - accuracy: 0.80 - ETA: 4s - loss: 0.6637 - accuracy: 0.80 - ETA: 3s - loss: 0.6639 - accuracy: 0.80 - ETA: 2s - loss: 0.6632 - accuracy: 0.80 - ETA: 1s - loss: 0.6637 - accuracy: 0.80 - ETA: 0s - loss: 0.6628 - accuracy: 0.80 - 106s 8ms/step - loss: 0.6621 - accuracy: 0.8013 - val_loss: 3.1226 - val_accuracy: 0.3311\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:31 - loss: 0.7417 - accuracy: 0.81 - ETA: 1:29 - loss: 0.7074 - accuracy: 0.82 - ETA: 1:28 - loss: 0.7229 - accuracy: 0.81 - ETA: 1:26 - loss: 0.7059 - accuracy: 0.82 - ETA: 1:25 - loss: 0.6817 - accuracy: 0.82 - ETA: 1:24 - loss: 0.6991 - accuracy: 0.81 - ETA: 1:23 - loss: 0.7154 - accuracy: 0.81 - ETA: 1:22 - loss: 0.6870 - accuracy: 0.81 - ETA: 1:22 - loss: 0.6892 - accuracy: 0.81 - ETA: 1:21 - loss: 0.6620 - accuracy: 0.81 - ETA: 1:21 - loss: 0.6634 - accuracy: 0.81 - ETA: 1:20 - loss: 0.6638 - accuracy: 0.81 - ETA: 1:19 - loss: 0.6586 - accuracy: 0.81 - ETA: 1:18 - loss: 0.6620 - accuracy: 0.81 - ETA: 1:17 - loss: 0.6605 - accuracy: 0.81 - ETA: 1:16 - loss: 0.6604 - accuracy: 0.81 - ETA: 1:15 - loss: 0.6584 - accuracy: 0.81 - ETA: 1:14 - loss: 0.6523 - accuracy: 0.81 - ETA: 1:13 - loss: 0.6654 - accuracy: 0.81 - ETA: 1:13 - loss: 0.6588 - accuracy: 0.81 - ETA: 1:12 - loss: 0.6588 - accuracy: 0.80 - ETA: 1:11 - loss: 0.6657 - accuracy: 0.80 - ETA: 1:10 - loss: 0.6695 - accuracy: 0.80 - ETA: 1:09 - loss: 0.6674 - accuracy: 0.80 - ETA: 1:08 - loss: 0.6642 - accuracy: 0.80 - ETA: 1:07 - loss: 0.6692 - accuracy: 0.80 - ETA: 1:06 - loss: 0.6630 - accuracy: 0.80 - ETA: 1:06 - loss: 0.6562 - accuracy: 0.80 - ETA: 1:05 - loss: 0.6527 - accuracy: 0.80 - ETA: 1:04 - loss: 0.6509 - accuracy: 0.80 - ETA: 1:03 - loss: 0.6578 - accuracy: 0.80 - ETA: 1:02 - loss: 0.6582 - accuracy: 0.80 - ETA: 1:01 - loss: 0.6626 - accuracy: 0.80 - ETA: 1:00 - loss: 0.6669 - accuracy: 0.80 - ETA: 59s - loss: 0.6628 - accuracy: 0.8040 - ETA: 59s - loss: 0.6695 - accuracy: 0.804 - ETA: 58s - loss: 0.6706 - accuracy: 0.804 - ETA: 57s - loss: 0.6720 - accuracy: 0.804 - ETA: 56s - loss: 0.6687 - accuracy: 0.804 - ETA: 55s - loss: 0.6674 - accuracy: 0.804 - ETA: 54s - loss: 0.6674 - accuracy: 0.804 - ETA: 53s - loss: 0.6685 - accuracy: 0.802 - ETA: 52s - loss: 0.6731 - accuracy: 0.801 - ETA: 51s - loss: 0.6709 - accuracy: 0.801 - ETA: 51s - loss: 0.6745 - accuracy: 0.799 - ETA: 50s - loss: 0.6746 - accuracy: 0.798 - ETA: 49s - loss: 0.6725 - accuracy: 0.798 - ETA: 48s - loss: 0.6723 - accuracy: 0.799 - ETA: 47s - loss: 0.6693 - accuracy: 0.801 - ETA: 46s - loss: 0.6727 - accuracy: 0.800 - ETA: 45s - loss: 0.6738 - accuracy: 0.800 - ETA: 44s - loss: 0.6730 - accuracy: 0.800 - ETA: 43s - loss: 0.6707 - accuracy: 0.800 - ETA: 42s - loss: 0.6691 - accuracy: 0.800 - ETA: 42s - loss: 0.6677 - accuracy: 0.802 - ETA: 41s - loss: 0.6654 - accuracy: 0.802 - ETA: 40s - loss: 0.6631 - accuracy: 0.802 - ETA: 39s - loss: 0.6650 - accuracy: 0.802 - ETA: 38s - loss: 0.6643 - accuracy: 0.802 - ETA: 37s - loss: 0.6624 - accuracy: 0.803 - ETA: 36s - loss: 0.6605 - accuracy: 0.803 - ETA: 35s - loss: 0.6610 - accuracy: 0.803 - ETA: 34s - loss: 0.6617 - accuracy: 0.803 - ETA: 34s - loss: 0.6611 - accuracy: 0.803 - ETA: 33s - loss: 0.6618 - accuracy: 0.803 - ETA: 32s - loss: 0.6613 - accuracy: 0.803 - ETA: 31s - loss: 0.6630 - accuracy: 0.803 - ETA: 30s - loss: 0.6608 - accuracy: 0.804 - ETA: 29s - loss: 0.6596 - accuracy: 0.804 - ETA: 28s - loss: 0.6607 - accuracy: 0.804 - ETA: 27s - loss: 0.6638 - accuracy: 0.803 - ETA: 26s - loss: 0.6634 - accuracy: 0.804 - ETA: 25s - loss: 0.6608 - accuracy: 0.805 - ETA: 25s - loss: 0.6594 - accuracy: 0.805 - ETA: 24s - loss: 0.6606 - accuracy: 0.804 - ETA: 23s - loss: 0.6603 - accuracy: 0.804 - ETA: 22s - loss: 0.6568 - accuracy: 0.805 - ETA: 21s - loss: 0.6552 - accuracy: 0.805 - ETA: 20s - loss: 0.6550 - accuracy: 0.805 - ETA: 19s - loss: 0.6560 - accuracy: 0.805 - ETA: 18s - loss: 0.6561 - accuracy: 0.805 - ETA: 17s - loss: 0.6568 - accuracy: 0.804 - ETA: 16s - loss: 0.6570 - accuracy: 0.804 - ETA: 16s - loss: 0.6566 - accuracy: 0.804 - ETA: 15s - loss: 0.6579 - accuracy: 0.804 - ETA: 14s - loss: 0.6567 - accuracy: 0.805 - ETA: 13s - loss: 0.6574 - accuracy: 0.804 - ETA: 12s - loss: 0.6563 - accuracy: 0.805 - ETA: 11s - loss: 0.6574 - accuracy: 0.804 - ETA: 10s - loss: 0.6578 - accuracy: 0.804 - ETA: 9s - loss: 0.6568 - accuracy: 0.804 - ETA: 8s - loss: 0.6561 - accuracy: 0.80 - ETA: 7s - loss: 0.6572 - accuracy: 0.80 - ETA: 6s - loss: 0.6571 - accuracy: 0.80 - ETA: 6s - loss: 0.6593 - accuracy: 0.80 - ETA: 5s - loss: 0.6589 - accuracy: 0.80 - ETA: 4s - loss: 0.6580 - accuracy: 0.80 - ETA: 3s - loss: 0.6596 - accuracy: 0.80 - ETA: 2s - loss: 0.6578 - accuracy: 0.80 - ETA: 1s - loss: 0.6556 - accuracy: 0.80 - ETA: 0s - loss: 0.6555 - accuracy: 0.80 - 105s 8ms/step - loss: 0.6578 - accuracy: 0.8040 - val_loss: 3.1124 - val_accuracy: 0.3057\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:34 - loss: 0.7311 - accuracy: 0.79 - ETA: 1:29 - loss: 0.6799 - accuracy: 0.80 - ETA: 1:28 - loss: 0.7229 - accuracy: 0.78 - ETA: 1:26 - loss: 0.6863 - accuracy: 0.78 - ETA: 1:25 - loss: 0.6816 - accuracy: 0.78 - ETA: 1:24 - loss: 0.6412 - accuracy: 0.79 - ETA: 1:23 - loss: 0.6241 - accuracy: 0.79 - ETA: 1:22 - loss: 0.6248 - accuracy: 0.79 - ETA: 1:22 - loss: 0.6100 - accuracy: 0.80 - ETA: 1:21 - loss: 0.6093 - accuracy: 0.80 - ETA: 1:20 - loss: 0.6010 - accuracy: 0.80 - ETA: 1:19 - loss: 0.5852 - accuracy: 0.81 - ETA: 1:18 - loss: 0.5817 - accuracy: 0.80 - ETA: 1:17 - loss: 0.5909 - accuracy: 0.80 - ETA: 1:16 - loss: 0.5827 - accuracy: 0.80 - ETA: 1:15 - loss: 0.5779 - accuracy: 0.81 - ETA: 1:14 - loss: 0.5923 - accuracy: 0.80 - ETA: 1:14 - loss: 0.5948 - accuracy: 0.80 - ETA: 1:13 - loss: 0.6031 - accuracy: 0.80 - ETA: 1:12 - loss: 0.6015 - accuracy: 0.80 - ETA: 1:11 - loss: 0.6027 - accuracy: 0.80 - ETA: 1:10 - loss: 0.6056 - accuracy: 0.80 - ETA: 1:10 - loss: 0.6160 - accuracy: 0.80 - ETA: 1:09 - loss: 0.6236 - accuracy: 0.80 - ETA: 1:08 - loss: 0.6367 - accuracy: 0.80 - ETA: 1:07 - loss: 0.6369 - accuracy: 0.80 - ETA: 1:07 - loss: 0.6368 - accuracy: 0.80 - ETA: 1:06 - loss: 0.6394 - accuracy: 0.80 - ETA: 1:05 - loss: 0.6371 - accuracy: 0.80 - ETA: 1:04 - loss: 0.6336 - accuracy: 0.80 - ETA: 1:03 - loss: 0.6319 - accuracy: 0.80 - ETA: 1:02 - loss: 0.6356 - accuracy: 0.80 - ETA: 1:01 - loss: 0.6334 - accuracy: 0.80 - ETA: 1:00 - loss: 0.6401 - accuracy: 0.80 - ETA: 1:00 - loss: 0.6442 - accuracy: 0.80 - ETA: 59s - loss: 0.6461 - accuracy: 0.8030 - ETA: 58s - loss: 0.6457 - accuracy: 0.803 - ETA: 57s - loss: 0.6459 - accuracy: 0.803 - ETA: 56s - loss: 0.6466 - accuracy: 0.803 - ETA: 55s - loss: 0.6498 - accuracy: 0.802 - ETA: 54s - loss: 0.6502 - accuracy: 0.802 - ETA: 53s - loss: 0.6531 - accuracy: 0.801 - ETA: 52s - loss: 0.6547 - accuracy: 0.800 - ETA: 51s - loss: 0.6548 - accuracy: 0.800 - ETA: 50s - loss: 0.6564 - accuracy: 0.799 - ETA: 49s - loss: 0.6563 - accuracy: 0.799 - ETA: 49s - loss: 0.6528 - accuracy: 0.800 - ETA: 48s - loss: 0.6566 - accuracy: 0.798 - ETA: 47s - loss: 0.6606 - accuracy: 0.797 - ETA: 46s - loss: 0.6651 - accuracy: 0.797 - ETA: 45s - loss: 0.6652 - accuracy: 0.797 - ETA: 44s - loss: 0.6627 - accuracy: 0.798 - ETA: 43s - loss: 0.6617 - accuracy: 0.798 - ETA: 42s - loss: 0.6567 - accuracy: 0.799 - ETA: 42s - loss: 0.6588 - accuracy: 0.798 - ETA: 41s - loss: 0.6582 - accuracy: 0.798 - ETA: 40s - loss: 0.6601 - accuracy: 0.798 - ETA: 39s - loss: 0.6578 - accuracy: 0.798 - ETA: 38s - loss: 0.6583 - accuracy: 0.798 - ETA: 37s - loss: 0.6595 - accuracy: 0.798 - ETA: 36s - loss: 0.6590 - accuracy: 0.799 - ETA: 35s - loss: 0.6588 - accuracy: 0.799 - ETA: 34s - loss: 0.6592 - accuracy: 0.799 - ETA: 33s - loss: 0.6608 - accuracy: 0.798 - ETA: 32s - loss: 0.6630 - accuracy: 0.797 - ETA: 32s - loss: 0.6641 - accuracy: 0.797 - ETA: 31s - loss: 0.6620 - accuracy: 0.797 - ETA: 30s - loss: 0.6605 - accuracy: 0.797 - ETA: 29s - loss: 0.6617 - accuracy: 0.797 - ETA: 28s - loss: 0.6590 - accuracy: 0.798 - ETA: 27s - loss: 0.6584 - accuracy: 0.798 - ETA: 26s - loss: 0.6556 - accuracy: 0.799 - ETA: 25s - loss: 0.6565 - accuracy: 0.799 - ETA: 24s - loss: 0.6558 - accuracy: 0.799 - ETA: 23s - loss: 0.6561 - accuracy: 0.799 - ETA: 23s - loss: 0.6545 - accuracy: 0.800 - ETA: 22s - loss: 0.6539 - accuracy: 0.800 - ETA: 21s - loss: 0.6540 - accuracy: 0.800 - ETA: 20s - loss: 0.6540 - accuracy: 0.800 - ETA: 19s - loss: 0.6526 - accuracy: 0.800 - ETA: 18s - loss: 0.6548 - accuracy: 0.800 - ETA: 17s - loss: 0.6567 - accuracy: 0.800 - ETA: 16s - loss: 0.6564 - accuracy: 0.800 - ETA: 15s - loss: 0.6558 - accuracy: 0.800 - ETA: 15s - loss: 0.6558 - accuracy: 0.800 - ETA: 14s - loss: 0.6570 - accuracy: 0.800 - ETA: 13s - loss: 0.6545 - accuracy: 0.801 - ETA: 12s - loss: 0.6551 - accuracy: 0.801 - ETA: 11s - loss: 0.6549 - accuracy: 0.801 - ETA: 10s - loss: 0.6566 - accuracy: 0.801 - ETA: 9s - loss: 0.6559 - accuracy: 0.801 - ETA: 8s - loss: 0.6553 - accuracy: 0.80 - ETA: 7s - loss: 0.6535 - accuracy: 0.80 - ETA: 6s - loss: 0.6552 - accuracy: 0.80 - ETA: 6s - loss: 0.6556 - accuracy: 0.80 - ETA: 5s - loss: 0.6553 - accuracy: 0.80 - ETA: 4s - loss: 0.6523 - accuracy: 0.80 - ETA: 3s - loss: 0.6529 - accuracy: 0.80 - ETA: 2s - loss: 0.6507 - accuracy: 0.80 - ETA: 1s - loss: 0.6499 - accuracy: 0.80 - ETA: 0s - loss: 0.6508 - accuracy: 0.80 - 105s 8ms/step - loss: 0.6506 - accuracy: 0.8031 - val_loss: 3.0956 - val_accuracy: 0.3235\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:28 - loss: 0.5858 - accuracy: 0.76 - ETA: 1:27 - loss: 0.6175 - accuracy: 0.79 - ETA: 1:28 - loss: 0.6088 - accuracy: 0.81 - ETA: 1:27 - loss: 0.6109 - accuracy: 0.81 - ETA: 1:26 - loss: 0.6508 - accuracy: 0.80 - ETA: 1:26 - loss: 0.6655 - accuracy: 0.80 - ETA: 1:26 - loss: 0.6367 - accuracy: 0.81 - ETA: 1:25 - loss: 0.6400 - accuracy: 0.80 - ETA: 1:25 - loss: 0.6208 - accuracy: 0.80 - ETA: 1:24 - loss: 0.6274 - accuracy: 0.80 - ETA: 1:23 - loss: 0.6289 - accuracy: 0.80 - ETA: 1:21 - loss: 0.6364 - accuracy: 0.80 - ETA: 1:20 - loss: 0.6361 - accuracy: 0.80 - ETA: 1:19 - loss: 0.6331 - accuracy: 0.80 - ETA: 1:18 - loss: 0.6324 - accuracy: 0.80 - ETA: 1:17 - loss: 0.6329 - accuracy: 0.80 - ETA: 1:16 - loss: 0.6350 - accuracy: 0.80 - ETA: 1:15 - loss: 0.6332 - accuracy: 0.80 - ETA: 1:14 - loss: 0.6381 - accuracy: 0.80 - ETA: 1:13 - loss: 0.6334 - accuracy: 0.80 - ETA: 1:12 - loss: 0.6374 - accuracy: 0.80 - ETA: 1:12 - loss: 0.6319 - accuracy: 0.80 - ETA: 1:11 - loss: 0.6280 - accuracy: 0.81 - ETA: 1:10 - loss: 0.6301 - accuracy: 0.80 - ETA: 1:09 - loss: 0.6321 - accuracy: 0.80 - ETA: 1:08 - loss: 0.6305 - accuracy: 0.80 - ETA: 1:07 - loss: 0.6391 - accuracy: 0.80 - ETA: 1:06 - loss: 0.6362 - accuracy: 0.80 - ETA: 1:05 - loss: 0.6342 - accuracy: 0.80 - ETA: 1:04 - loss: 0.6407 - accuracy: 0.80 - ETA: 1:03 - loss: 0.6485 - accuracy: 0.80 - ETA: 1:02 - loss: 0.6465 - accuracy: 0.80 - ETA: 1:02 - loss: 0.6451 - accuracy: 0.80 - ETA: 1:01 - loss: 0.6419 - accuracy: 0.80 - ETA: 1:00 - loss: 0.6431 - accuracy: 0.80 - ETA: 59s - loss: 0.6424 - accuracy: 0.8053 - ETA: 58s - loss: 0.6405 - accuracy: 0.806 - ETA: 57s - loss: 0.6402 - accuracy: 0.806 - ETA: 56s - loss: 0.6367 - accuracy: 0.806 - ETA: 55s - loss: 0.6379 - accuracy: 0.806 - ETA: 54s - loss: 0.6395 - accuracy: 0.804 - ETA: 53s - loss: 0.6390 - accuracy: 0.804 - ETA: 53s - loss: 0.6418 - accuracy: 0.803 - ETA: 52s - loss: 0.6489 - accuracy: 0.802 - ETA: 51s - loss: 0.6482 - accuracy: 0.803 - ETA: 50s - loss: 0.6501 - accuracy: 0.802 - ETA: 49s - loss: 0.6530 - accuracy: 0.802 - ETA: 48s - loss: 0.6527 - accuracy: 0.802 - ETA: 47s - loss: 0.6576 - accuracy: 0.801 - ETA: 46s - loss: 0.6571 - accuracy: 0.801 - ETA: 45s - loss: 0.6568 - accuracy: 0.801 - ETA: 44s - loss: 0.6562 - accuracy: 0.801 - ETA: 43s - loss: 0.6558 - accuracy: 0.802 - ETA: 43s - loss: 0.6553 - accuracy: 0.802 - ETA: 42s - loss: 0.6563 - accuracy: 0.802 - ETA: 41s - loss: 0.6570 - accuracy: 0.802 - ETA: 40s - loss: 0.6560 - accuracy: 0.802 - ETA: 39s - loss: 0.6563 - accuracy: 0.802 - ETA: 38s - loss: 0.6560 - accuracy: 0.802 - ETA: 37s - loss: 0.6517 - accuracy: 0.803 - ETA: 36s - loss: 0.6547 - accuracy: 0.802 - ETA: 35s - loss: 0.6537 - accuracy: 0.802 - ETA: 34s - loss: 0.6504 - accuracy: 0.803 - ETA: 34s - loss: 0.6509 - accuracy: 0.803 - ETA: 33s - loss: 0.6524 - accuracy: 0.803 - ETA: 32s - loss: 0.6526 - accuracy: 0.803 - ETA: 31s - loss: 0.6525 - accuracy: 0.803 - ETA: 30s - loss: 0.6551 - accuracy: 0.803 - ETA: 29s - loss: 0.6541 - accuracy: 0.803 - ETA: 28s - loss: 0.6570 - accuracy: 0.802 - ETA: 27s - loss: 0.6582 - accuracy: 0.802 - ETA: 26s - loss: 0.6564 - accuracy: 0.803 - ETA: 25s - loss: 0.6525 - accuracy: 0.804 - ETA: 24s - loss: 0.6534 - accuracy: 0.803 - ETA: 24s - loss: 0.6549 - accuracy: 0.803 - ETA: 23s - loss: 0.6543 - accuracy: 0.803 - ETA: 22s - loss: 0.6557 - accuracy: 0.803 - ETA: 21s - loss: 0.6552 - accuracy: 0.803 - ETA: 20s - loss: 0.6556 - accuracy: 0.803 - ETA: 19s - loss: 0.6566 - accuracy: 0.803 - ETA: 18s - loss: 0.6554 - accuracy: 0.803 - ETA: 17s - loss: 0.6548 - accuracy: 0.803 - ETA: 16s - loss: 0.6571 - accuracy: 0.804 - ETA: 15s - loss: 0.6555 - accuracy: 0.804 - ETA: 15s - loss: 0.6549 - accuracy: 0.804 - ETA: 14s - loss: 0.6548 - accuracy: 0.804 - ETA: 13s - loss: 0.6564 - accuracy: 0.805 - ETA: 12s - loss: 0.6562 - accuracy: 0.805 - ETA: 11s - loss: 0.6573 - accuracy: 0.805 - ETA: 10s - loss: 0.6568 - accuracy: 0.805 - ETA: 9s - loss: 0.6570 - accuracy: 0.805 - ETA: 8s - loss: 0.6584 - accuracy: 0.80 - ETA: 7s - loss: 0.6572 - accuracy: 0.80 - ETA: 6s - loss: 0.6586 - accuracy: 0.80 - ETA: 6s - loss: 0.6583 - accuracy: 0.80 - ETA: 5s - loss: 0.6567 - accuracy: 0.80 - ETA: 4s - loss: 0.6554 - accuracy: 0.80 - ETA: 3s - loss: 0.6555 - accuracy: 0.80 - ETA: 2s - loss: 0.6542 - accuracy: 0.80 - ETA: 1s - loss: 0.6537 - accuracy: 0.80 - ETA: 0s - loss: 0.6529 - accuracy: 0.80 - 105s 8ms/step - loss: 0.6541 - accuracy: 0.8064 - val_loss: 3.2054 - val_accuracy: 0.3313\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:29 - loss: 0.5054 - accuracy: 0.83 - ETA: 1:28 - loss: 0.5773 - accuracy: 0.80 - ETA: 1:27 - loss: 0.5682 - accuracy: 0.82 - ETA: 1:26 - loss: 0.5535 - accuracy: 0.81 - ETA: 1:26 - loss: 0.5831 - accuracy: 0.80 - ETA: 1:25 - loss: 0.6026 - accuracy: 0.80 - ETA: 1:24 - loss: 0.6118 - accuracy: 0.80 - ETA: 1:24 - loss: 0.6078 - accuracy: 0.81 - ETA: 1:23 - loss: 0.6089 - accuracy: 0.81 - ETA: 1:23 - loss: 0.6093 - accuracy: 0.81 - ETA: 1:22 - loss: 0.6130 - accuracy: 0.81 - ETA: 1:22 - loss: 0.6189 - accuracy: 0.80 - ETA: 1:21 - loss: 0.6156 - accuracy: 0.80 - ETA: 1:20 - loss: 0.6078 - accuracy: 0.80 - ETA: 1:20 - loss: 0.6133 - accuracy: 0.80 - ETA: 1:19 - loss: 0.6140 - accuracy: 0.80 - ETA: 1:18 - loss: 0.6216 - accuracy: 0.80 - ETA: 1:17 - loss: 0.6148 - accuracy: 0.80 - ETA: 1:16 - loss: 0.6153 - accuracy: 0.80 - ETA: 1:15 - loss: 0.6064 - accuracy: 0.80 - ETA: 1:14 - loss: 0.6083 - accuracy: 0.80 - ETA: 1:13 - loss: 0.6083 - accuracy: 0.80 - ETA: 1:12 - loss: 0.6060 - accuracy: 0.80 - ETA: 1:11 - loss: 0.6073 - accuracy: 0.80 - ETA: 1:10 - loss: 0.6030 - accuracy: 0.80 - ETA: 1:09 - loss: 0.6099 - accuracy: 0.80 - ETA: 1:08 - loss: 0.6109 - accuracy: 0.80 - ETA: 1:07 - loss: 0.6094 - accuracy: 0.80 - ETA: 1:06 - loss: 0.6127 - accuracy: 0.80 - ETA: 1:05 - loss: 0.6165 - accuracy: 0.80 - ETA: 1:04 - loss: 0.6151 - accuracy: 0.80 - ETA: 1:03 - loss: 0.6163 - accuracy: 0.80 - ETA: 1:02 - loss: 0.6202 - accuracy: 0.80 - ETA: 1:01 - loss: 0.6183 - accuracy: 0.80 - ETA: 1:01 - loss: 0.6240 - accuracy: 0.80 - ETA: 1:00 - loss: 0.6265 - accuracy: 0.80 - ETA: 59s - loss: 0.6286 - accuracy: 0.8013 - ETA: 58s - loss: 0.6280 - accuracy: 0.802 - ETA: 57s - loss: 0.6284 - accuracy: 0.803 - ETA: 56s - loss: 0.6272 - accuracy: 0.803 - ETA: 55s - loss: 0.6250 - accuracy: 0.804 - ETA: 54s - loss: 0.6276 - accuracy: 0.802 - ETA: 53s - loss: 0.6315 - accuracy: 0.802 - ETA: 52s - loss: 0.6315 - accuracy: 0.802 - ETA: 51s - loss: 0.6363 - accuracy: 0.801 - ETA: 50s - loss: 0.6385 - accuracy: 0.801 - ETA: 49s - loss: 0.6353 - accuracy: 0.802 - ETA: 48s - loss: 0.6344 - accuracy: 0.803 - ETA: 47s - loss: 0.6328 - accuracy: 0.803 - ETA: 46s - loss: 0.6336 - accuracy: 0.803 - ETA: 46s - loss: 0.6313 - accuracy: 0.804 - ETA: 45s - loss: 0.6333 - accuracy: 0.803 - ETA: 44s - loss: 0.6337 - accuracy: 0.803 - ETA: 43s - loss: 0.6349 - accuracy: 0.803 - ETA: 42s - loss: 0.6323 - accuracy: 0.804 - ETA: 41s - loss: 0.6301 - accuracy: 0.804 - ETA: 40s - loss: 0.6317 - accuracy: 0.805 - ETA: 39s - loss: 0.6288 - accuracy: 0.806 - ETA: 38s - loss: 0.6289 - accuracy: 0.806 - ETA: 37s - loss: 0.6307 - accuracy: 0.806 - ETA: 36s - loss: 0.6315 - accuracy: 0.806 - ETA: 35s - loss: 0.6342 - accuracy: 0.805 - ETA: 35s - loss: 0.6313 - accuracy: 0.806 - ETA: 34s - loss: 0.6328 - accuracy: 0.805 - ETA: 33s - loss: 0.6338 - accuracy: 0.806 - ETA: 32s - loss: 0.6345 - accuracy: 0.806 - ETA: 31s - loss: 0.6356 - accuracy: 0.806 - ETA: 30s - loss: 0.6371 - accuracy: 0.806 - ETA: 29s - loss: 0.6359 - accuracy: 0.807 - ETA: 28s - loss: 0.6369 - accuracy: 0.806 - ETA: 27s - loss: 0.6398 - accuracy: 0.806 - ETA: 26s - loss: 0.6405 - accuracy: 0.806 - ETA: 26s - loss: 0.6387 - accuracy: 0.807 - ETA: 25s - loss: 0.6396 - accuracy: 0.806 - ETA: 24s - loss: 0.6402 - accuracy: 0.806 - ETA: 23s - loss: 0.6410 - accuracy: 0.806 - ETA: 22s - loss: 0.6418 - accuracy: 0.805 - ETA: 21s - loss: 0.6439 - accuracy: 0.805 - ETA: 20s - loss: 0.6442 - accuracy: 0.805 - ETA: 19s - loss: 0.6421 - accuracy: 0.806 - ETA: 18s - loss: 0.6413 - accuracy: 0.806 - ETA: 17s - loss: 0.6397 - accuracy: 0.807 - ETA: 16s - loss: 0.6389 - accuracy: 0.807 - ETA: 16s - loss: 0.6403 - accuracy: 0.807 - ETA: 15s - loss: 0.6398 - accuracy: 0.807 - ETA: 14s - loss: 0.6421 - accuracy: 0.806 - ETA: 13s - loss: 0.6434 - accuracy: 0.806 - ETA: 12s - loss: 0.6447 - accuracy: 0.806 - ETA: 11s - loss: 0.6460 - accuracy: 0.806 - ETA: 10s - loss: 0.6462 - accuracy: 0.806 - ETA: 9s - loss: 0.6461 - accuracy: 0.806 - ETA: 8s - loss: 0.6464 - accuracy: 0.80 - ETA: 7s - loss: 0.6482 - accuracy: 0.80 - ETA: 7s - loss: 0.6465 - accuracy: 0.80 - ETA: 6s - loss: 0.6464 - accuracy: 0.80 - ETA: 5s - loss: 0.6453 - accuracy: 0.80 - ETA: 4s - loss: 0.6472 - accuracy: 0.80 - ETA: 3s - loss: 0.6481 - accuracy: 0.80 - ETA: 2s - loss: 0.6482 - accuracy: 0.80 - ETA: 1s - loss: 0.6489 - accuracy: 0.80 - ETA: 0s - loss: 0.6491 - accuracy: 0.80 - 105s 8ms/step - loss: 0.6485 - accuracy: 0.8063 - val_loss: 3.1479 - val_accuracy: 0.3149\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:31 - loss: 0.6528 - accuracy: 0.79 - ETA: 1:32 - loss: 0.5852 - accuracy: 0.82 - ETA: 1:31 - loss: 0.5981 - accuracy: 0.81 - ETA: 1:29 - loss: 0.5994 - accuracy: 0.81 - ETA: 1:28 - loss: 0.5913 - accuracy: 0.81 - ETA: 1:27 - loss: 0.6003 - accuracy: 0.80 - ETA: 1:26 - loss: 0.5941 - accuracy: 0.81 - ETA: 1:25 - loss: 0.6074 - accuracy: 0.81 - ETA: 1:25 - loss: 0.6152 - accuracy: 0.80 - ETA: 1:23 - loss: 0.6223 - accuracy: 0.80 - ETA: 1:22 - loss: 0.6140 - accuracy: 0.80 - ETA: 1:21 - loss: 0.6077 - accuracy: 0.80 - ETA: 1:20 - loss: 0.6118 - accuracy: 0.80 - ETA: 1:19 - loss: 0.5983 - accuracy: 0.81 - ETA: 1:18 - loss: 0.6033 - accuracy: 0.81 - ETA: 1:17 - loss: 0.6081 - accuracy: 0.81 - ETA: 1:16 - loss: 0.6142 - accuracy: 0.80 - ETA: 1:15 - loss: 0.6189 - accuracy: 0.80 - ETA: 1:14 - loss: 0.6174 - accuracy: 0.80 - ETA: 1:13 - loss: 0.6321 - accuracy: 0.80 - ETA: 1:12 - loss: 0.6342 - accuracy: 0.80 - ETA: 1:11 - loss: 0.6396 - accuracy: 0.80 - ETA: 1:11 - loss: 0.6368 - accuracy: 0.80 - ETA: 1:10 - loss: 0.6515 - accuracy: 0.80 - ETA: 1:09 - loss: 0.6469 - accuracy: 0.80 - ETA: 1:08 - loss: 0.6457 - accuracy: 0.80 - ETA: 1:07 - loss: 0.6478 - accuracy: 0.80 - ETA: 1:06 - loss: 0.6441 - accuracy: 0.80 - ETA: 1:05 - loss: 0.6500 - accuracy: 0.80 - ETA: 1:04 - loss: 0.6438 - accuracy: 0.80 - ETA: 1:03 - loss: 0.6424 - accuracy: 0.80 - ETA: 1:02 - loss: 0.6413 - accuracy: 0.80 - ETA: 1:01 - loss: 0.6384 - accuracy: 0.80 - ETA: 1:01 - loss: 0.6393 - accuracy: 0.80 - ETA: 1:00 - loss: 0.6391 - accuracy: 0.80 - ETA: 59s - loss: 0.6372 - accuracy: 0.8071 - ETA: 58s - loss: 0.6457 - accuracy: 0.804 - ETA: 57s - loss: 0.6426 - accuracy: 0.805 - ETA: 56s - loss: 0.6458 - accuracy: 0.805 - ETA: 55s - loss: 0.6413 - accuracy: 0.806 - ETA: 55s - loss: 0.6411 - accuracy: 0.806 - ETA: 54s - loss: 0.6412 - accuracy: 0.805 - ETA: 53s - loss: 0.6421 - accuracy: 0.806 - ETA: 52s - loss: 0.6448 - accuracy: 0.805 - ETA: 51s - loss: 0.6407 - accuracy: 0.806 - ETA: 50s - loss: 0.6392 - accuracy: 0.807 - ETA: 49s - loss: 0.6374 - accuracy: 0.807 - ETA: 48s - loss: 0.6353 - accuracy: 0.808 - ETA: 47s - loss: 0.6350 - accuracy: 0.809 - ETA: 46s - loss: 0.6356 - accuracy: 0.809 - ETA: 46s - loss: 0.6357 - accuracy: 0.809 - ETA: 45s - loss: 0.6340 - accuracy: 0.810 - ETA: 44s - loss: 0.6339 - accuracy: 0.810 - ETA: 43s - loss: 0.6344 - accuracy: 0.810 - ETA: 42s - loss: 0.6320 - accuracy: 0.810 - ETA: 41s - loss: 0.6336 - accuracy: 0.809 - ETA: 40s - loss: 0.6339 - accuracy: 0.810 - ETA: 39s - loss: 0.6331 - accuracy: 0.811 - ETA: 38s - loss: 0.6367 - accuracy: 0.810 - ETA: 37s - loss: 0.6401 - accuracy: 0.809 - ETA: 36s - loss: 0.6394 - accuracy: 0.809 - ETA: 36s - loss: 0.6439 - accuracy: 0.808 - ETA: 35s - loss: 0.6464 - accuracy: 0.807 - ETA: 34s - loss: 0.6465 - accuracy: 0.807 - ETA: 33s - loss: 0.6457 - accuracy: 0.807 - ETA: 32s - loss: 0.6452 - accuracy: 0.807 - ETA: 31s - loss: 0.6458 - accuracy: 0.807 - ETA: 30s - loss: 0.6491 - accuracy: 0.806 - ETA: 29s - loss: 0.6485 - accuracy: 0.806 - ETA: 28s - loss: 0.6488 - accuracy: 0.805 - ETA: 27s - loss: 0.6482 - accuracy: 0.806 - ETA: 26s - loss: 0.6472 - accuracy: 0.807 - ETA: 26s - loss: 0.6454 - accuracy: 0.808 - ETA: 25s - loss: 0.6465 - accuracy: 0.807 - ETA: 24s - loss: 0.6468 - accuracy: 0.806 - ETA: 23s - loss: 0.6497 - accuracy: 0.805 - ETA: 22s - loss: 0.6523 - accuracy: 0.804 - ETA: 21s - loss: 0.6525 - accuracy: 0.804 - ETA: 20s - loss: 0.6531 - accuracy: 0.804 - ETA: 19s - loss: 0.6542 - accuracy: 0.804 - ETA: 18s - loss: 0.6558 - accuracy: 0.803 - ETA: 17s - loss: 0.6555 - accuracy: 0.803 - ETA: 16s - loss: 0.6544 - accuracy: 0.803 - ETA: 16s - loss: 0.6545 - accuracy: 0.803 - ETA: 15s - loss: 0.6553 - accuracy: 0.802 - ETA: 14s - loss: 0.6556 - accuracy: 0.802 - ETA: 13s - loss: 0.6554 - accuracy: 0.802 - ETA: 12s - loss: 0.6554 - accuracy: 0.802 - ETA: 11s - loss: 0.6567 - accuracy: 0.802 - ETA: 10s - loss: 0.6553 - accuracy: 0.802 - ETA: 9s - loss: 0.6548 - accuracy: 0.802 - ETA: 8s - loss: 0.6555 - accuracy: 0.80 - ETA: 7s - loss: 0.6542 - accuracy: 0.80 - ETA: 6s - loss: 0.6529 - accuracy: 0.80 - ETA: 6s - loss: 0.6538 - accuracy: 0.80 - ETA: 5s - loss: 0.6523 - accuracy: 0.80 - ETA: 4s - loss: 0.6515 - accuracy: 0.80 - ETA: 3s - loss: 0.6526 - accuracy: 0.80 - ETA: 2s - loss: 0.6524 - accuracy: 0.80 - ETA: 1s - loss: 0.6530 - accuracy: 0.80 - ETA: 0s - loss: 0.6520 - accuracy: 0.80 - 106s 8ms/step - loss: 0.6522 - accuracy: 0.8037 - val_loss: 3.2074 - val_accuracy: 0.3264\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 0.8071 - accuracy: 0.76 - ETA: 1:30 - loss: 0.7440 - accuracy: 0.78 - ETA: 1:28 - loss: 0.7069 - accuracy: 0.78 - ETA: 1:26 - loss: 0.6985 - accuracy: 0.79 - ETA: 1:25 - loss: 0.6620 - accuracy: 0.79 - ETA: 1:24 - loss: 0.6748 - accuracy: 0.79 - ETA: 1:24 - loss: 0.6758 - accuracy: 0.79 - ETA: 1:22 - loss: 0.6908 - accuracy: 0.79 - ETA: 1:22 - loss: 0.6830 - accuracy: 0.79 - ETA: 1:21 - loss: 0.6878 - accuracy: 0.79 - ETA: 1:20 - loss: 0.6808 - accuracy: 0.79 - ETA: 1:20 - loss: 0.6874 - accuracy: 0.79 - ETA: 1:20 - loss: 0.6860 - accuracy: 0.78 - ETA: 1:19 - loss: 0.6858 - accuracy: 0.78 - ETA: 1:18 - loss: 0.6830 - accuracy: 0.78 - ETA: 1:17 - loss: 0.6753 - accuracy: 0.78 - ETA: 1:16 - loss: 0.6718 - accuracy: 0.79 - ETA: 1:15 - loss: 0.6751 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6708 - accuracy: 0.79 - ETA: 1:13 - loss: 0.6764 - accuracy: 0.79 - ETA: 1:12 - loss: 0.6778 - accuracy: 0.78 - ETA: 1:11 - loss: 0.6891 - accuracy: 0.78 - ETA: 1:10 - loss: 0.6819 - accuracy: 0.78 - ETA: 1:09 - loss: 0.6833 - accuracy: 0.78 - ETA: 1:08 - loss: 0.6831 - accuracy: 0.79 - ETA: 1:07 - loss: 0.6826 - accuracy: 0.79 - ETA: 1:06 - loss: 0.6787 - accuracy: 0.79 - ETA: 1:06 - loss: 0.6774 - accuracy: 0.79 - ETA: 1:05 - loss: 0.6736 - accuracy: 0.79 - ETA: 1:04 - loss: 0.6752 - accuracy: 0.79 - ETA: 1:03 - loss: 0.6801 - accuracy: 0.79 - ETA: 1:02 - loss: 0.6793 - accuracy: 0.79 - ETA: 1:01 - loss: 0.6780 - accuracy: 0.79 - ETA: 1:01 - loss: 0.6802 - accuracy: 0.79 - ETA: 1:00 - loss: 0.6751 - accuracy: 0.79 - ETA: 59s - loss: 0.6730 - accuracy: 0.7960 - ETA: 58s - loss: 0.6783 - accuracy: 0.794 - ETA: 57s - loss: 0.6774 - accuracy: 0.795 - ETA: 56s - loss: 0.6774 - accuracy: 0.796 - ETA: 55s - loss: 0.6744 - accuracy: 0.798 - ETA: 54s - loss: 0.6703 - accuracy: 0.799 - ETA: 53s - loss: 0.6689 - accuracy: 0.799 - ETA: 53s - loss: 0.6659 - accuracy: 0.800 - ETA: 52s - loss: 0.6616 - accuracy: 0.801 - ETA: 51s - loss: 0.6598 - accuracy: 0.802 - ETA: 50s - loss: 0.6588 - accuracy: 0.803 - ETA: 49s - loss: 0.6587 - accuracy: 0.803 - ETA: 49s - loss: 0.6586 - accuracy: 0.803 - ETA: 48s - loss: 0.6576 - accuracy: 0.804 - ETA: 47s - loss: 0.6564 - accuracy: 0.804 - ETA: 46s - loss: 0.6557 - accuracy: 0.804 - ETA: 45s - loss: 0.6543 - accuracy: 0.804 - ETA: 44s - loss: 0.6577 - accuracy: 0.804 - ETA: 43s - loss: 0.6555 - accuracy: 0.805 - ETA: 42s - loss: 0.6581 - accuracy: 0.804 - ETA: 41s - loss: 0.6555 - accuracy: 0.804 - ETA: 40s - loss: 0.6544 - accuracy: 0.805 - ETA: 39s - loss: 0.6555 - accuracy: 0.804 - ETA: 38s - loss: 0.6551 - accuracy: 0.804 - ETA: 38s - loss: 0.6556 - accuracy: 0.804 - ETA: 37s - loss: 0.6533 - accuracy: 0.805 - ETA: 36s - loss: 0.6522 - accuracy: 0.805 - ETA: 35s - loss: 0.6520 - accuracy: 0.804 - ETA: 34s - loss: 0.6474 - accuracy: 0.805 - ETA: 33s - loss: 0.6445 - accuracy: 0.806 - ETA: 32s - loss: 0.6425 - accuracy: 0.807 - ETA: 31s - loss: 0.6418 - accuracy: 0.807 - ETA: 30s - loss: 0.6414 - accuracy: 0.807 - ETA: 29s - loss: 0.6407 - accuracy: 0.808 - ETA: 28s - loss: 0.6405 - accuracy: 0.808 - ETA: 28s - loss: 0.6408 - accuracy: 0.808 - ETA: 27s - loss: 0.6402 - accuracy: 0.808 - ETA: 26s - loss: 0.6374 - accuracy: 0.809 - ETA: 25s - loss: 0.6362 - accuracy: 0.809 - ETA: 24s - loss: 0.6362 - accuracy: 0.809 - ETA: 23s - loss: 0.6354 - accuracy: 0.810 - ETA: 22s - loss: 0.6355 - accuracy: 0.809 - ETA: 21s - loss: 0.6348 - accuracy: 0.810 - ETA: 20s - loss: 0.6325 - accuracy: 0.811 - ETA: 19s - loss: 0.6288 - accuracy: 0.812 - ETA: 18s - loss: 0.6272 - accuracy: 0.812 - ETA: 17s - loss: 0.6269 - accuracy: 0.812 - ETA: 17s - loss: 0.6280 - accuracy: 0.812 - ETA: 16s - loss: 0.6285 - accuracy: 0.812 - ETA: 15s - loss: 0.6274 - accuracy: 0.812 - ETA: 14s - loss: 0.6286 - accuracy: 0.812 - ETA: 13s - loss: 0.6264 - accuracy: 0.812 - ETA: 12s - loss: 0.6256 - accuracy: 0.812 - ETA: 11s - loss: 0.6252 - accuracy: 0.813 - ETA: 10s - loss: 0.6260 - accuracy: 0.813 - ETA: 9s - loss: 0.6249 - accuracy: 0.813 - ETA: 8s - loss: 0.6245 - accuracy: 0.81 - ETA: 7s - loss: 0.6256 - accuracy: 0.81 - ETA: 7s - loss: 0.6264 - accuracy: 0.81 - ETA: 6s - loss: 0.6271 - accuracy: 0.81 - ETA: 5s - loss: 0.6281 - accuracy: 0.81 - ETA: 4s - loss: 0.6286 - accuracy: 0.81 - ETA: 3s - loss: 0.6278 - accuracy: 0.81 - ETA: 2s - loss: 0.6288 - accuracy: 0.81 - ETA: 1s - loss: 0.6269 - accuracy: 0.81 - ETA: 0s - loss: 0.6287 - accuracy: 0.81 - 106s 8ms/step - loss: 0.6298 - accuracy: 0.8125 - val_loss: 3.1763 - val_accuracy: 0.3200\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 0.8389 - accuracy: 0.74 - ETA: 1:30 - loss: 0.7646 - accuracy: 0.78 - ETA: 1:28 - loss: 0.7255 - accuracy: 0.80 - ETA: 1:27 - loss: 0.6755 - accuracy: 0.82 - ETA: 1:26 - loss: 0.6768 - accuracy: 0.81 - ETA: 1:26 - loss: 0.6751 - accuracy: 0.82 - ETA: 1:26 - loss: 0.6697 - accuracy: 0.81 - ETA: 1:25 - loss: 0.6452 - accuracy: 0.82 - ETA: 1:23 - loss: 0.6543 - accuracy: 0.81 - ETA: 1:23 - loss: 0.6459 - accuracy: 0.81 - ETA: 1:22 - loss: 0.6384 - accuracy: 0.81 - ETA: 1:21 - loss: 0.6389 - accuracy: 0.82 - ETA: 1:20 - loss: 0.6277 - accuracy: 0.82 - ETA: 1:19 - loss: 0.6335 - accuracy: 0.82 - ETA: 1:18 - loss: 0.6419 - accuracy: 0.82 - ETA: 1:17 - loss: 0.6436 - accuracy: 0.81 - ETA: 1:16 - loss: 0.6461 - accuracy: 0.81 - ETA: 1:16 - loss: 0.6455 - accuracy: 0.81 - ETA: 1:15 - loss: 0.6350 - accuracy: 0.81 - ETA: 1:14 - loss: 0.6391 - accuracy: 0.81 - ETA: 1:13 - loss: 0.6430 - accuracy: 0.81 - ETA: 1:12 - loss: 0.6391 - accuracy: 0.81 - ETA: 1:11 - loss: 0.6365 - accuracy: 0.81 - ETA: 1:10 - loss: 0.6349 - accuracy: 0.81 - ETA: 1:09 - loss: 0.6377 - accuracy: 0.81 - ETA: 1:08 - loss: 0.6317 - accuracy: 0.81 - ETA: 1:08 - loss: 0.6273 - accuracy: 0.81 - ETA: 1:07 - loss: 0.6294 - accuracy: 0.81 - ETA: 1:06 - loss: 0.6279 - accuracy: 0.81 - ETA: 1:05 - loss: 0.6288 - accuracy: 0.82 - ETA: 1:04 - loss: 0.6291 - accuracy: 0.82 - ETA: 1:03 - loss: 0.6270 - accuracy: 0.82 - ETA: 1:02 - loss: 0.6262 - accuracy: 0.82 - ETA: 1:01 - loss: 0.6298 - accuracy: 0.81 - ETA: 1:00 - loss: 0.6281 - accuracy: 0.81 - ETA: 59s - loss: 0.6284 - accuracy: 0.8199 - ETA: 58s - loss: 0.6249 - accuracy: 0.820 - ETA: 57s - loss: 0.6248 - accuracy: 0.820 - ETA: 56s - loss: 0.6210 - accuracy: 0.821 - ETA: 56s - loss: 0.6186 - accuracy: 0.821 - ETA: 55s - loss: 0.6230 - accuracy: 0.820 - ETA: 54s - loss: 0.6274 - accuracy: 0.818 - ETA: 53s - loss: 0.6264 - accuracy: 0.818 - ETA: 52s - loss: 0.6243 - accuracy: 0.818 - ETA: 51s - loss: 0.6218 - accuracy: 0.818 - ETA: 50s - loss: 0.6189 - accuracy: 0.820 - ETA: 49s - loss: 0.6189 - accuracy: 0.820 - ETA: 48s - loss: 0.6191 - accuracy: 0.820 - ETA: 47s - loss: 0.6196 - accuracy: 0.819 - ETA: 46s - loss: 0.6191 - accuracy: 0.818 - ETA: 45s - loss: 0.6153 - accuracy: 0.819 - ETA: 44s - loss: 0.6143 - accuracy: 0.819 - ETA: 44s - loss: 0.6175 - accuracy: 0.818 - ETA: 43s - loss: 0.6208 - accuracy: 0.818 - ETA: 42s - loss: 0.6224 - accuracy: 0.818 - ETA: 41s - loss: 0.6219 - accuracy: 0.818 - ETA: 40s - loss: 0.6240 - accuracy: 0.817 - ETA: 39s - loss: 0.6245 - accuracy: 0.817 - ETA: 38s - loss: 0.6214 - accuracy: 0.818 - ETA: 37s - loss: 0.6177 - accuracy: 0.819 - ETA: 36s - loss: 0.6180 - accuracy: 0.818 - ETA: 35s - loss: 0.6181 - accuracy: 0.818 - ETA: 34s - loss: 0.6183 - accuracy: 0.819 - ETA: 34s - loss: 0.6177 - accuracy: 0.819 - ETA: 33s - loss: 0.6167 - accuracy: 0.819 - ETA: 32s - loss: 0.6199 - accuracy: 0.819 - ETA: 31s - loss: 0.6177 - accuracy: 0.819 - ETA: 30s - loss: 0.6207 - accuracy: 0.818 - ETA: 29s - loss: 0.6179 - accuracy: 0.819 - ETA: 28s - loss: 0.6182 - accuracy: 0.819 - ETA: 27s - loss: 0.6206 - accuracy: 0.818 - ETA: 26s - loss: 0.6214 - accuracy: 0.818 - ETA: 25s - loss: 0.6214 - accuracy: 0.818 - ETA: 25s - loss: 0.6199 - accuracy: 0.818 - ETA: 24s - loss: 0.6199 - accuracy: 0.818 - ETA: 23s - loss: 0.6179 - accuracy: 0.819 - ETA: 22s - loss: 0.6186 - accuracy: 0.818 - ETA: 21s - loss: 0.6207 - accuracy: 0.817 - ETA: 20s - loss: 0.6195 - accuracy: 0.817 - ETA: 19s - loss: 0.6201 - accuracy: 0.818 - ETA: 18s - loss: 0.6204 - accuracy: 0.818 - ETA: 17s - loss: 0.6196 - accuracy: 0.818 - ETA: 16s - loss: 0.6219 - accuracy: 0.817 - ETA: 15s - loss: 0.6226 - accuracy: 0.817 - ETA: 15s - loss: 0.6213 - accuracy: 0.817 - ETA: 14s - loss: 0.6202 - accuracy: 0.817 - ETA: 13s - loss: 0.6184 - accuracy: 0.818 - ETA: 12s - loss: 0.6178 - accuracy: 0.817 - ETA: 11s - loss: 0.6173 - accuracy: 0.818 - ETA: 10s - loss: 0.6194 - accuracy: 0.817 - ETA: 9s - loss: 0.6205 - accuracy: 0.817 - ETA: 8s - loss: 0.6226 - accuracy: 0.81 - ETA: 7s - loss: 0.6256 - accuracy: 0.81 - ETA: 6s - loss: 0.6261 - accuracy: 0.81 - ETA: 6s - loss: 0.6277 - accuracy: 0.81 - ETA: 5s - loss: 0.6294 - accuracy: 0.81 - ETA: 4s - loss: 0.6288 - accuracy: 0.81 - ETA: 3s - loss: 0.6273 - accuracy: 0.81 - ETA: 2s - loss: 0.6266 - accuracy: 0.81 - ETA: 1s - loss: 0.6260 - accuracy: 0.81 - ETA: 0s - loss: 0.6266 - accuracy: 0.81 - 106s 8ms/step - loss: 0.6254 - accuracy: 0.8166 - val_loss: 3.2146 - val_accuracy: 0.3242\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:34 - loss: 0.5331 - accuracy: 0.84 - ETA: 1:30 - loss: 0.6029 - accuracy: 0.82 - ETA: 1:27 - loss: 0.6296 - accuracy: 0.80 - ETA: 1:27 - loss: 0.6199 - accuracy: 0.80 - ETA: 1:27 - loss: 0.6275 - accuracy: 0.79 - ETA: 1:26 - loss: 0.6545 - accuracy: 0.79 - ETA: 1:25 - loss: 0.6474 - accuracy: 0.80 - ETA: 1:25 - loss: 0.6581 - accuracy: 0.80 - ETA: 1:24 - loss: 0.6544 - accuracy: 0.81 - ETA: 1:23 - loss: 0.6434 - accuracy: 0.81 - ETA: 1:22 - loss: 0.6384 - accuracy: 0.81 - ETA: 1:21 - loss: 0.6389 - accuracy: 0.81 - ETA: 1:20 - loss: 0.6386 - accuracy: 0.81 - ETA: 1:19 - loss: 0.6396 - accuracy: 0.81 - ETA: 1:18 - loss: 0.6451 - accuracy: 0.80 - ETA: 1:17 - loss: 0.6409 - accuracy: 0.81 - ETA: 1:16 - loss: 0.6311 - accuracy: 0.81 - ETA: 1:15 - loss: 0.6289 - accuracy: 0.81 - ETA: 1:14 - loss: 0.6426 - accuracy: 0.81 - ETA: 1:13 - loss: 0.6411 - accuracy: 0.81 - ETA: 1:12 - loss: 0.6455 - accuracy: 0.81 - ETA: 1:12 - loss: 0.6422 - accuracy: 0.81 - ETA: 1:11 - loss: 0.6378 - accuracy: 0.81 - ETA: 1:10 - loss: 0.6368 - accuracy: 0.81 - ETA: 1:09 - loss: 0.6339 - accuracy: 0.81 - ETA: 1:08 - loss: 0.6297 - accuracy: 0.81 - ETA: 1:07 - loss: 0.6329 - accuracy: 0.81 - ETA: 1:06 - loss: 0.6332 - accuracy: 0.81 - ETA: 1:05 - loss: 0.6326 - accuracy: 0.81 - ETA: 1:05 - loss: 0.6324 - accuracy: 0.81 - ETA: 1:04 - loss: 0.6287 - accuracy: 0.81 - ETA: 1:03 - loss: 0.6321 - accuracy: 0.81 - ETA: 1:02 - loss: 0.6382 - accuracy: 0.81 - ETA: 1:01 - loss: 0.6348 - accuracy: 0.81 - ETA: 1:00 - loss: 0.6381 - accuracy: 0.81 - ETA: 59s - loss: 0.6423 - accuracy: 0.8123 - ETA: 58s - loss: 0.6426 - accuracy: 0.811 - ETA: 57s - loss: 0.6400 - accuracy: 0.811 - ETA: 56s - loss: 0.6420 - accuracy: 0.810 - ETA: 55s - loss: 0.6430 - accuracy: 0.810 - ETA: 54s - loss: 0.6447 - accuracy: 0.807 - ETA: 53s - loss: 0.6437 - accuracy: 0.807 - ETA: 53s - loss: 0.6413 - accuracy: 0.808 - ETA: 52s - loss: 0.6390 - accuracy: 0.807 - ETA: 51s - loss: 0.6406 - accuracy: 0.807 - ETA: 50s - loss: 0.6410 - accuracy: 0.807 - ETA: 49s - loss: 0.6454 - accuracy: 0.806 - ETA: 48s - loss: 0.6449 - accuracy: 0.806 - ETA: 47s - loss: 0.6440 - accuracy: 0.806 - ETA: 46s - loss: 0.6450 - accuracy: 0.806 - ETA: 45s - loss: 0.6438 - accuracy: 0.807 - ETA: 45s - loss: 0.6424 - accuracy: 0.807 - ETA: 44s - loss: 0.6421 - accuracy: 0.807 - ETA: 43s - loss: 0.6451 - accuracy: 0.806 - ETA: 42s - loss: 0.6456 - accuracy: 0.806 - ETA: 41s - loss: 0.6444 - accuracy: 0.805 - ETA: 40s - loss: 0.6422 - accuracy: 0.805 - ETA: 39s - loss: 0.6433 - accuracy: 0.804 - ETA: 38s - loss: 0.6420 - accuracy: 0.805 - ETA: 37s - loss: 0.6431 - accuracy: 0.805 - ETA: 37s - loss: 0.6438 - accuracy: 0.805 - ETA: 36s - loss: 0.6454 - accuracy: 0.805 - ETA: 35s - loss: 0.6470 - accuracy: 0.804 - ETA: 34s - loss: 0.6467 - accuracy: 0.805 - ETA: 33s - loss: 0.6449 - accuracy: 0.806 - ETA: 32s - loss: 0.6454 - accuracy: 0.806 - ETA: 31s - loss: 0.6443 - accuracy: 0.806 - ETA: 30s - loss: 0.6462 - accuracy: 0.806 - ETA: 29s - loss: 0.6522 - accuracy: 0.805 - ETA: 28s - loss: 0.6501 - accuracy: 0.806 - ETA: 27s - loss: 0.6479 - accuracy: 0.806 - ETA: 26s - loss: 0.6471 - accuracy: 0.807 - ETA: 26s - loss: 0.6488 - accuracy: 0.806 - ETA: 25s - loss: 0.6509 - accuracy: 0.806 - ETA: 24s - loss: 0.6485 - accuracy: 0.806 - ETA: 23s - loss: 0.6476 - accuracy: 0.806 - ETA: 22s - loss: 0.6460 - accuracy: 0.806 - ETA: 21s - loss: 0.6470 - accuracy: 0.806 - ETA: 20s - loss: 0.6452 - accuracy: 0.806 - ETA: 19s - loss: 0.6440 - accuracy: 0.806 - ETA: 18s - loss: 0.6443 - accuracy: 0.806 - ETA: 17s - loss: 0.6443 - accuracy: 0.806 - ETA: 16s - loss: 0.6452 - accuracy: 0.806 - ETA: 16s - loss: 0.6442 - accuracy: 0.806 - ETA: 15s - loss: 0.6448 - accuracy: 0.805 - ETA: 14s - loss: 0.6437 - accuracy: 0.806 - ETA: 13s - loss: 0.6421 - accuracy: 0.807 - ETA: 12s - loss: 0.6420 - accuracy: 0.806 - ETA: 11s - loss: 0.6423 - accuracy: 0.806 - ETA: 10s - loss: 0.6417 - accuracy: 0.807 - ETA: 9s - loss: 0.6418 - accuracy: 0.806 - ETA: 8s - loss: 0.6418 - accuracy: 0.80 - ETA: 7s - loss: 0.6435 - accuracy: 0.80 - ETA: 6s - loss: 0.6446 - accuracy: 0.80 - ETA: 6s - loss: 0.6433 - accuracy: 0.80 - ETA: 5s - loss: 0.6438 - accuracy: 0.80 - ETA: 4s - loss: 0.6460 - accuracy: 0.80 - ETA: 3s - loss: 0.6484 - accuracy: 0.80 - ETA: 2s - loss: 0.6480 - accuracy: 0.80 - ETA: 1s - loss: 0.6466 - accuracy: 0.80 - ETA: 0s - loss: 0.6473 - accuracy: 0.80 - 105s 8ms/step - loss: 0.6474 - accuracy: 0.8058 - val_loss: 3.2573 - val_accuracy: 0.2837\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:29 - loss: 0.6460 - accuracy: 0.82 - ETA: 1:29 - loss: 0.6757 - accuracy: 0.80 - ETA: 1:29 - loss: 0.6565 - accuracy: 0.81 - ETA: 1:27 - loss: 0.6570 - accuracy: 0.81 - ETA: 1:26 - loss: 0.6633 - accuracy: 0.81 - ETA: 1:26 - loss: 0.6983 - accuracy: 0.80 - ETA: 1:25 - loss: 0.7135 - accuracy: 0.79 - ETA: 1:24 - loss: 0.6856 - accuracy: 0.79 - ETA: 1:23 - loss: 0.6726 - accuracy: 0.80 - ETA: 1:22 - loss: 0.6805 - accuracy: 0.80 - ETA: 1:21 - loss: 0.6779 - accuracy: 0.80 - ETA: 1:21 - loss: 0.6703 - accuracy: 0.80 - ETA: 1:20 - loss: 0.6751 - accuracy: 0.80 - ETA: 1:20 - loss: 0.6686 - accuracy: 0.80 - ETA: 1:19 - loss: 0.6595 - accuracy: 0.80 - ETA: 1:18 - loss: 0.6503 - accuracy: 0.80 - ETA: 1:17 - loss: 0.6486 - accuracy: 0.81 - ETA: 1:16 - loss: 0.6513 - accuracy: 0.81 - ETA: 1:15 - loss: 0.6444 - accuracy: 0.81 - ETA: 1:14 - loss: 0.6401 - accuracy: 0.81 - ETA: 1:13 - loss: 0.6441 - accuracy: 0.81 - ETA: 1:12 - loss: 0.6377 - accuracy: 0.81 - ETA: 1:11 - loss: 0.6405 - accuracy: 0.81 - ETA: 1:10 - loss: 0.6433 - accuracy: 0.81 - ETA: 1:09 - loss: 0.6446 - accuracy: 0.81 - ETA: 1:08 - loss: 0.6463 - accuracy: 0.80 - ETA: 1:07 - loss: 0.6448 - accuracy: 0.80 - ETA: 1:06 - loss: 0.6503 - accuracy: 0.80 - ETA: 1:05 - loss: 0.6546 - accuracy: 0.80 - ETA: 1:04 - loss: 0.6580 - accuracy: 0.80 - ETA: 1:03 - loss: 0.6533 - accuracy: 0.80 - ETA: 1:03 - loss: 0.6546 - accuracy: 0.80 - ETA: 1:02 - loss: 0.6565 - accuracy: 0.80 - ETA: 1:01 - loss: 0.6515 - accuracy: 0.80 - ETA: 1:00 - loss: 0.6490 - accuracy: 0.81 - ETA: 59s - loss: 0.6485 - accuracy: 0.8118 - ETA: 58s - loss: 0.6435 - accuracy: 0.812 - ETA: 57s - loss: 0.6408 - accuracy: 0.813 - ETA: 56s - loss: 0.6380 - accuracy: 0.813 - ETA: 55s - loss: 0.6343 - accuracy: 0.814 - ETA: 54s - loss: 0.6371 - accuracy: 0.813 - ETA: 53s - loss: 0.6377 - accuracy: 0.813 - ETA: 52s - loss: 0.6341 - accuracy: 0.814 - ETA: 52s - loss: 0.6357 - accuracy: 0.815 - ETA: 51s - loss: 0.6308 - accuracy: 0.816 - ETA: 50s - loss: 0.6337 - accuracy: 0.815 - ETA: 49s - loss: 0.6353 - accuracy: 0.814 - ETA: 48s - loss: 0.6349 - accuracy: 0.814 - ETA: 47s - loss: 0.6331 - accuracy: 0.814 - ETA: 46s - loss: 0.6328 - accuracy: 0.814 - ETA: 45s - loss: 0.6336 - accuracy: 0.814 - ETA: 44s - loss: 0.6339 - accuracy: 0.815 - ETA: 43s - loss: 0.6323 - accuracy: 0.815 - ETA: 43s - loss: 0.6314 - accuracy: 0.814 - ETA: 42s - loss: 0.6309 - accuracy: 0.814 - ETA: 41s - loss: 0.6293 - accuracy: 0.814 - ETA: 40s - loss: 0.6275 - accuracy: 0.814 - ETA: 39s - loss: 0.6287 - accuracy: 0.814 - ETA: 38s - loss: 0.6295 - accuracy: 0.814 - ETA: 37s - loss: 0.6318 - accuracy: 0.813 - ETA: 36s - loss: 0.6303 - accuracy: 0.813 - ETA: 35s - loss: 0.6279 - accuracy: 0.814 - ETA: 34s - loss: 0.6238 - accuracy: 0.815 - ETA: 34s - loss: 0.6214 - accuracy: 0.816 - ETA: 33s - loss: 0.6205 - accuracy: 0.816 - ETA: 32s - loss: 0.6194 - accuracy: 0.816 - ETA: 31s - loss: 0.6189 - accuracy: 0.816 - ETA: 30s - loss: 0.6190 - accuracy: 0.817 - ETA: 29s - loss: 0.6193 - accuracy: 0.817 - ETA: 28s - loss: 0.6195 - accuracy: 0.817 - ETA: 27s - loss: 0.6201 - accuracy: 0.817 - ETA: 26s - loss: 0.6227 - accuracy: 0.817 - ETA: 25s - loss: 0.6251 - accuracy: 0.816 - ETA: 24s - loss: 0.6263 - accuracy: 0.816 - ETA: 24s - loss: 0.6257 - accuracy: 0.816 - ETA: 23s - loss: 0.6254 - accuracy: 0.817 - ETA: 22s - loss: 0.6243 - accuracy: 0.818 - ETA: 21s - loss: 0.6243 - accuracy: 0.817 - ETA: 20s - loss: 0.6235 - accuracy: 0.817 - ETA: 19s - loss: 0.6235 - accuracy: 0.817 - ETA: 18s - loss: 0.6248 - accuracy: 0.816 - ETA: 17s - loss: 0.6245 - accuracy: 0.817 - ETA: 16s - loss: 0.6233 - accuracy: 0.817 - ETA: 15s - loss: 0.6244 - accuracy: 0.817 - ETA: 15s - loss: 0.6245 - accuracy: 0.817 - ETA: 14s - loss: 0.6246 - accuracy: 0.817 - ETA: 13s - loss: 0.6221 - accuracy: 0.818 - ETA: 12s - loss: 0.6249 - accuracy: 0.817 - ETA: 11s - loss: 0.6274 - accuracy: 0.816 - ETA: 10s - loss: 0.6296 - accuracy: 0.816 - ETA: 9s - loss: 0.6305 - accuracy: 0.816 - ETA: 8s - loss: 0.6327 - accuracy: 0.81 - ETA: 7s - loss: 0.6323 - accuracy: 0.81 - ETA: 6s - loss: 0.6328 - accuracy: 0.81 - ETA: 6s - loss: 0.6329 - accuracy: 0.81 - ETA: 5s - loss: 0.6314 - accuracy: 0.81 - ETA: 4s - loss: 0.6317 - accuracy: 0.81 - ETA: 3s - loss: 0.6339 - accuracy: 0.81 - ETA: 2s - loss: 0.6329 - accuracy: 0.81 - ETA: 1s - loss: 0.6337 - accuracy: 0.81 - ETA: 0s - loss: 0.6332 - accuracy: 0.81 - 105s 8ms/step - loss: 0.6341 - accuracy: 0.8135 - val_loss: 3.0186 - val_accuracy: 0.3344\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.6374 - accuracy: 0.81 - ETA: 1:29 - loss: 0.5595 - accuracy: 0.82 - ETA: 1:28 - loss: 0.5598 - accuracy: 0.80 - ETA: 1:28 - loss: 0.5417 - accuracy: 0.81 - ETA: 1:28 - loss: 0.5560 - accuracy: 0.81 - ETA: 1:27 - loss: 0.5657 - accuracy: 0.82 - ETA: 1:27 - loss: 0.5725 - accuracy: 0.82 - ETA: 1:25 - loss: 0.5813 - accuracy: 0.82 - ETA: 1:24 - loss: 0.5868 - accuracy: 0.82 - ETA: 1:23 - loss: 0.5858 - accuracy: 0.82 - ETA: 1:22 - loss: 0.5836 - accuracy: 0.82 - ETA: 1:21 - loss: 0.5845 - accuracy: 0.82 - ETA: 1:20 - loss: 0.5876 - accuracy: 0.82 - ETA: 1:19 - loss: 0.5959 - accuracy: 0.81 - ETA: 1:18 - loss: 0.5912 - accuracy: 0.81 - ETA: 1:17 - loss: 0.5911 - accuracy: 0.82 - ETA: 1:16 - loss: 0.5927 - accuracy: 0.82 - ETA: 1:15 - loss: 0.5987 - accuracy: 0.81 - ETA: 1:14 - loss: 0.5962 - accuracy: 0.81 - ETA: 1:13 - loss: 0.5924 - accuracy: 0.82 - ETA: 1:12 - loss: 0.5904 - accuracy: 0.82 - ETA: 1:11 - loss: 0.5943 - accuracy: 0.82 - ETA: 1:11 - loss: 0.5905 - accuracy: 0.82 - ETA: 1:10 - loss: 0.5929 - accuracy: 0.82 - ETA: 1:09 - loss: 0.5967 - accuracy: 0.82 - ETA: 1:08 - loss: 0.5954 - accuracy: 0.82 - ETA: 1:07 - loss: 0.6031 - accuracy: 0.82 - ETA: 1:06 - loss: 0.6002 - accuracy: 0.82 - ETA: 1:05 - loss: 0.5976 - accuracy: 0.82 - ETA: 1:04 - loss: 0.5971 - accuracy: 0.82 - ETA: 1:03 - loss: 0.6010 - accuracy: 0.82 - ETA: 1:02 - loss: 0.6035 - accuracy: 0.82 - ETA: 1:01 - loss: 0.6046 - accuracy: 0.82 - ETA: 1:00 - loss: 0.6058 - accuracy: 0.82 - ETA: 1:00 - loss: 0.6048 - accuracy: 0.82 - ETA: 59s - loss: 0.6069 - accuracy: 0.8201 - ETA: 58s - loss: 0.6055 - accuracy: 0.820 - ETA: 57s - loss: 0.6084 - accuracy: 0.819 - ETA: 56s - loss: 0.6100 - accuracy: 0.818 - ETA: 55s - loss: 0.6143 - accuracy: 0.817 - ETA: 55s - loss: 0.6140 - accuracy: 0.817 - ETA: 54s - loss: 0.6152 - accuracy: 0.817 - ETA: 53s - loss: 0.6156 - accuracy: 0.817 - ETA: 52s - loss: 0.6149 - accuracy: 0.816 - ETA: 51s - loss: 0.6114 - accuracy: 0.817 - ETA: 50s - loss: 0.6091 - accuracy: 0.817 - ETA: 49s - loss: 0.6111 - accuracy: 0.818 - ETA: 48s - loss: 0.6072 - accuracy: 0.818 - ETA: 47s - loss: 0.6057 - accuracy: 0.819 - ETA: 46s - loss: 0.6030 - accuracy: 0.820 - ETA: 45s - loss: 0.6043 - accuracy: 0.820 - ETA: 44s - loss: 0.6034 - accuracy: 0.820 - ETA: 44s - loss: 0.6039 - accuracy: 0.820 - ETA: 43s - loss: 0.6023 - accuracy: 0.821 - ETA: 42s - loss: 0.6028 - accuracy: 0.821 - ETA: 41s - loss: 0.6039 - accuracy: 0.821 - ETA: 40s - loss: 0.6044 - accuracy: 0.820 - ETA: 39s - loss: 0.6027 - accuracy: 0.821 - ETA: 38s - loss: 0.6010 - accuracy: 0.822 - ETA: 37s - loss: 0.6011 - accuracy: 0.822 - ETA: 36s - loss: 0.6015 - accuracy: 0.822 - ETA: 35s - loss: 0.5986 - accuracy: 0.823 - ETA: 35s - loss: 0.6005 - accuracy: 0.823 - ETA: 34s - loss: 0.5999 - accuracy: 0.823 - ETA: 33s - loss: 0.6023 - accuracy: 0.822 - ETA: 32s - loss: 0.6029 - accuracy: 0.822 - ETA: 31s - loss: 0.6006 - accuracy: 0.823 - ETA: 30s - loss: 0.5983 - accuracy: 0.823 - ETA: 29s - loss: 0.6006 - accuracy: 0.823 - ETA: 28s - loss: 0.6027 - accuracy: 0.822 - ETA: 27s - loss: 0.6018 - accuracy: 0.822 - ETA: 26s - loss: 0.6011 - accuracy: 0.823 - ETA: 25s - loss: 0.6018 - accuracy: 0.821 - ETA: 24s - loss: 0.6018 - accuracy: 0.822 - ETA: 24s - loss: 0.6031 - accuracy: 0.821 - ETA: 23s - loss: 0.6022 - accuracy: 0.821 - ETA: 22s - loss: 0.6026 - accuracy: 0.821 - ETA: 21s - loss: 0.6039 - accuracy: 0.821 - ETA: 20s - loss: 0.6054 - accuracy: 0.820 - ETA: 19s - loss: 0.6061 - accuracy: 0.820 - ETA: 18s - loss: 0.6061 - accuracy: 0.820 - ETA: 17s - loss: 0.6049 - accuracy: 0.820 - ETA: 16s - loss: 0.6063 - accuracy: 0.820 - ETA: 15s - loss: 0.6045 - accuracy: 0.820 - ETA: 15s - loss: 0.6040 - accuracy: 0.820 - ETA: 14s - loss: 0.6032 - accuracy: 0.821 - ETA: 13s - loss: 0.6018 - accuracy: 0.821 - ETA: 12s - loss: 0.6023 - accuracy: 0.821 - ETA: 11s - loss: 0.6029 - accuracy: 0.820 - ETA: 10s - loss: 0.6025 - accuracy: 0.820 - ETA: 9s - loss: 0.6018 - accuracy: 0.820 - ETA: 8s - loss: 0.6012 - accuracy: 0.82 - ETA: 7s - loss: 0.6010 - accuracy: 0.82 - ETA: 6s - loss: 0.6012 - accuracy: 0.82 - ETA: 6s - loss: 0.6022 - accuracy: 0.82 - ETA: 5s - loss: 0.6025 - accuracy: 0.82 - ETA: 4s - loss: 0.6019 - accuracy: 0.82 - ETA: 3s - loss: 0.6009 - accuracy: 0.82 - ETA: 2s - loss: 0.5997 - accuracy: 0.82 - ETA: 1s - loss: 0.6018 - accuracy: 0.81 - ETA: 0s - loss: 0.6024 - accuracy: 0.81 - 106s 8ms/step - loss: 0.6024 - accuracy: 0.8193 - val_loss: 3.2708 - val_accuracy: 0.3169\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.5263 - accuracy: 0.82 - ETA: 1:29 - loss: 0.4531 - accuracy: 0.85 - ETA: 1:28 - loss: 0.4857 - accuracy: 0.85 - ETA: 1:27 - loss: 0.5255 - accuracy: 0.84 - ETA: 1:27 - loss: 0.5496 - accuracy: 0.85 - ETA: 1:26 - loss: 0.5564 - accuracy: 0.84 - ETA: 1:25 - loss: 0.5400 - accuracy: 0.85 - ETA: 1:24 - loss: 0.5547 - accuracy: 0.84 - ETA: 1:23 - loss: 0.5538 - accuracy: 0.84 - ETA: 1:22 - loss: 0.5521 - accuracy: 0.84 - ETA: 1:21 - loss: 0.5610 - accuracy: 0.84 - ETA: 1:21 - loss: 0.5969 - accuracy: 0.83 - ETA: 1:20 - loss: 0.5935 - accuracy: 0.83 - ETA: 1:19 - loss: 0.5902 - accuracy: 0.83 - ETA: 1:18 - loss: 0.5895 - accuracy: 0.83 - ETA: 1:17 - loss: 0.5848 - accuracy: 0.83 - ETA: 1:16 - loss: 0.5852 - accuracy: 0.83 - ETA: 1:15 - loss: 0.5829 - accuracy: 0.82 - ETA: 1:14 - loss: 0.5932 - accuracy: 0.82 - ETA: 1:13 - loss: 0.5890 - accuracy: 0.82 - ETA: 1:12 - loss: 0.6016 - accuracy: 0.82 - ETA: 1:11 - loss: 0.6007 - accuracy: 0.82 - ETA: 1:10 - loss: 0.6008 - accuracy: 0.82 - ETA: 1:09 - loss: 0.5956 - accuracy: 0.82 - ETA: 1:08 - loss: 0.5945 - accuracy: 0.82 - ETA: 1:07 - loss: 0.6028 - accuracy: 0.82 - ETA: 1:07 - loss: 0.6079 - accuracy: 0.82 - ETA: 1:06 - loss: 0.6088 - accuracy: 0.82 - ETA: 1:05 - loss: 0.6170 - accuracy: 0.82 - ETA: 1:04 - loss: 0.6080 - accuracy: 0.82 - ETA: 1:03 - loss: 0.6034 - accuracy: 0.82 - ETA: 1:02 - loss: 0.6068 - accuracy: 0.82 - ETA: 1:02 - loss: 0.6104 - accuracy: 0.82 - ETA: 1:01 - loss: 0.6063 - accuracy: 0.82 - ETA: 1:00 - loss: 0.6099 - accuracy: 0.82 - ETA: 59s - loss: 0.6124 - accuracy: 0.8231 - ETA: 58s - loss: 0.6108 - accuracy: 0.823 - ETA: 57s - loss: 0.6102 - accuracy: 0.824 - ETA: 56s - loss: 0.6105 - accuracy: 0.823 - ETA: 55s - loss: 0.6112 - accuracy: 0.823 - ETA: 54s - loss: 0.6103 - accuracy: 0.823 - ETA: 53s - loss: 0.6096 - accuracy: 0.823 - ETA: 52s - loss: 0.6097 - accuracy: 0.823 - ETA: 51s - loss: 0.6089 - accuracy: 0.822 - ETA: 50s - loss: 0.6076 - accuracy: 0.823 - ETA: 50s - loss: 0.6096 - accuracy: 0.822 - ETA: 49s - loss: 0.6083 - accuracy: 0.823 - ETA: 48s - loss: 0.6090 - accuracy: 0.823 - ETA: 47s - loss: 0.6078 - accuracy: 0.824 - ETA: 46s - loss: 0.6078 - accuracy: 0.823 - ETA: 45s - loss: 0.6041 - accuracy: 0.825 - ETA: 44s - loss: 0.6066 - accuracy: 0.823 - ETA: 43s - loss: 0.6066 - accuracy: 0.823 - ETA: 43s - loss: 0.6056 - accuracy: 0.824 - ETA: 42s - loss: 0.6059 - accuracy: 0.824 - ETA: 41s - loss: 0.6066 - accuracy: 0.824 - ETA: 40s - loss: 0.6090 - accuracy: 0.823 - ETA: 39s - loss: 0.6078 - accuracy: 0.824 - ETA: 38s - loss: 0.6085 - accuracy: 0.823 - ETA: 37s - loss: 0.6093 - accuracy: 0.823 - ETA: 36s - loss: 0.6063 - accuracy: 0.824 - ETA: 35s - loss: 0.6073 - accuracy: 0.824 - ETA: 34s - loss: 0.6076 - accuracy: 0.824 - ETA: 33s - loss: 0.6065 - accuracy: 0.824 - ETA: 33s - loss: 0.6054 - accuracy: 0.824 - ETA: 32s - loss: 0.6080 - accuracy: 0.824 - ETA: 31s - loss: 0.6098 - accuracy: 0.823 - ETA: 30s - loss: 0.6102 - accuracy: 0.824 - ETA: 29s - loss: 0.6089 - accuracy: 0.824 - ETA: 28s - loss: 0.6088 - accuracy: 0.823 - ETA: 27s - loss: 0.6096 - accuracy: 0.823 - ETA: 26s - loss: 0.6088 - accuracy: 0.823 - ETA: 25s - loss: 0.6108 - accuracy: 0.822 - ETA: 24s - loss: 0.6114 - accuracy: 0.821 - ETA: 24s - loss: 0.6115 - accuracy: 0.821 - ETA: 23s - loss: 0.6099 - accuracy: 0.821 - ETA: 22s - loss: 0.6124 - accuracy: 0.820 - ETA: 21s - loss: 0.6140 - accuracy: 0.820 - ETA: 20s - loss: 0.6129 - accuracy: 0.821 - ETA: 19s - loss: 0.6128 - accuracy: 0.820 - ETA: 18s - loss: 0.6124 - accuracy: 0.821 - ETA: 17s - loss: 0.6134 - accuracy: 0.820 - ETA: 16s - loss: 0.6145 - accuracy: 0.819 - ETA: 15s - loss: 0.6124 - accuracy: 0.820 - ETA: 15s - loss: 0.6116 - accuracy: 0.820 - ETA: 14s - loss: 0.6140 - accuracy: 0.820 - ETA: 13s - loss: 0.6139 - accuracy: 0.820 - ETA: 12s - loss: 0.6152 - accuracy: 0.819 - ETA: 11s - loss: 0.6168 - accuracy: 0.819 - ETA: 10s - loss: 0.6175 - accuracy: 0.818 - ETA: 9s - loss: 0.6154 - accuracy: 0.819 - ETA: 8s - loss: 0.6156 - accuracy: 0.81 - ETA: 7s - loss: 0.6152 - accuracy: 0.81 - ETA: 6s - loss: 0.6169 - accuracy: 0.81 - ETA: 6s - loss: 0.6158 - accuracy: 0.81 - ETA: 5s - loss: 0.6145 - accuracy: 0.81 - ETA: 4s - loss: 0.6148 - accuracy: 0.81 - ETA: 3s - loss: 0.6154 - accuracy: 0.81 - ETA: 2s - loss: 0.6147 - accuracy: 0.81 - ETA: 1s - loss: 0.6188 - accuracy: 0.81 - ETA: 0s - loss: 0.6192 - accuracy: 0.81 - 105s 8ms/step - loss: 0.6193 - accuracy: 0.8185 - val_loss: 3.1997 - val_accuracy: 0.3353\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:38 - loss: 0.4863 - accuracy: 0.84 - ETA: 1:35 - loss: 0.6861 - accuracy: 0.81 - ETA: 1:33 - loss: 0.6145 - accuracy: 0.82 - ETA: 1:31 - loss: 0.6145 - accuracy: 0.81 - ETA: 1:29 - loss: 0.5944 - accuracy: 0.82 - ETA: 1:27 - loss: 0.5901 - accuracy: 0.82 - ETA: 1:26 - loss: 0.6107 - accuracy: 0.82 - ETA: 1:24 - loss: 0.6012 - accuracy: 0.82 - ETA: 1:23 - loss: 0.6106 - accuracy: 0.82 - ETA: 1:22 - loss: 0.6131 - accuracy: 0.82 - ETA: 1:21 - loss: 0.6283 - accuracy: 0.81 - ETA: 1:20 - loss: 0.6196 - accuracy: 0.81 - ETA: 1:19 - loss: 0.6180 - accuracy: 0.81 - ETA: 1:18 - loss: 0.6084 - accuracy: 0.82 - ETA: 1:17 - loss: 0.6105 - accuracy: 0.81 - ETA: 1:17 - loss: 0.6198 - accuracy: 0.81 - ETA: 1:16 - loss: 0.6110 - accuracy: 0.82 - ETA: 1:15 - loss: 0.6194 - accuracy: 0.81 - ETA: 1:14 - loss: 0.6239 - accuracy: 0.81 - ETA: 1:13 - loss: 0.6215 - accuracy: 0.81 - ETA: 1:12 - loss: 0.6240 - accuracy: 0.81 - ETA: 1:12 - loss: 0.6182 - accuracy: 0.81 - ETA: 1:11 - loss: 0.6260 - accuracy: 0.81 - ETA: 1:10 - loss: 0.6298 - accuracy: 0.81 - ETA: 1:09 - loss: 0.6317 - accuracy: 0.81 - ETA: 1:08 - loss: 0.6208 - accuracy: 0.81 - ETA: 1:07 - loss: 0.6143 - accuracy: 0.81 - ETA: 1:06 - loss: 0.6157 - accuracy: 0.81 - ETA: 1:05 - loss: 0.6156 - accuracy: 0.81 - ETA: 1:04 - loss: 0.6172 - accuracy: 0.81 - ETA: 1:03 - loss: 0.6208 - accuracy: 0.81 - ETA: 1:02 - loss: 0.6222 - accuracy: 0.81 - ETA: 1:02 - loss: 0.6203 - accuracy: 0.81 - ETA: 1:01 - loss: 0.6190 - accuracy: 0.81 - ETA: 1:00 - loss: 0.6190 - accuracy: 0.81 - ETA: 59s - loss: 0.6240 - accuracy: 0.8138 - ETA: 58s - loss: 0.6323 - accuracy: 0.811 - ETA: 57s - loss: 0.6319 - accuracy: 0.812 - ETA: 56s - loss: 0.6351 - accuracy: 0.811 - ETA: 55s - loss: 0.6395 - accuracy: 0.811 - ETA: 54s - loss: 0.6383 - accuracy: 0.811 - ETA: 53s - loss: 0.6352 - accuracy: 0.813 - ETA: 52s - loss: 0.6343 - accuracy: 0.813 - ETA: 52s - loss: 0.6320 - accuracy: 0.814 - ETA: 51s - loss: 0.6286 - accuracy: 0.815 - ETA: 50s - loss: 0.6267 - accuracy: 0.815 - ETA: 49s - loss: 0.6259 - accuracy: 0.816 - ETA: 48s - loss: 0.6303 - accuracy: 0.814 - ETA: 47s - loss: 0.6279 - accuracy: 0.814 - ETA: 46s - loss: 0.6296 - accuracy: 0.813 - ETA: 45s - loss: 0.6280 - accuracy: 0.813 - ETA: 44s - loss: 0.6282 - accuracy: 0.812 - ETA: 43s - loss: 0.6260 - accuracy: 0.813 - ETA: 42s - loss: 0.6228 - accuracy: 0.814 - ETA: 42s - loss: 0.6217 - accuracy: 0.814 - ETA: 41s - loss: 0.6248 - accuracy: 0.813 - ETA: 40s - loss: 0.6225 - accuracy: 0.815 - ETA: 39s - loss: 0.6222 - accuracy: 0.815 - ETA: 38s - loss: 0.6231 - accuracy: 0.814 - ETA: 37s - loss: 0.6250 - accuracy: 0.813 - ETA: 36s - loss: 0.6242 - accuracy: 0.813 - ETA: 35s - loss: 0.6248 - accuracy: 0.813 - ETA: 34s - loss: 0.6229 - accuracy: 0.814 - ETA: 34s - loss: 0.6230 - accuracy: 0.814 - ETA: 33s - loss: 0.6216 - accuracy: 0.814 - ETA: 32s - loss: 0.6225 - accuracy: 0.813 - ETA: 31s - loss: 0.6224 - accuracy: 0.813 - ETA: 30s - loss: 0.6243 - accuracy: 0.813 - ETA: 29s - loss: 0.6229 - accuracy: 0.813 - ETA: 28s - loss: 0.6244 - accuracy: 0.813 - ETA: 27s - loss: 0.6237 - accuracy: 0.812 - ETA: 26s - loss: 0.6251 - accuracy: 0.812 - ETA: 25s - loss: 0.6225 - accuracy: 0.813 - ETA: 24s - loss: 0.6202 - accuracy: 0.813 - ETA: 24s - loss: 0.6203 - accuracy: 0.813 - ETA: 23s - loss: 0.6201 - accuracy: 0.812 - ETA: 22s - loss: 0.6213 - accuracy: 0.812 - ETA: 21s - loss: 0.6213 - accuracy: 0.812 - ETA: 20s - loss: 0.6216 - accuracy: 0.812 - ETA: 19s - loss: 0.6227 - accuracy: 0.811 - ETA: 18s - loss: 0.6237 - accuracy: 0.811 - ETA: 17s - loss: 0.6218 - accuracy: 0.812 - ETA: 16s - loss: 0.6223 - accuracy: 0.812 - ETA: 15s - loss: 0.6221 - accuracy: 0.812 - ETA: 15s - loss: 0.6203 - accuracy: 0.813 - ETA: 14s - loss: 0.6221 - accuracy: 0.812 - ETA: 13s - loss: 0.6241 - accuracy: 0.812 - ETA: 12s - loss: 0.6257 - accuracy: 0.811 - ETA: 11s - loss: 0.6264 - accuracy: 0.811 - ETA: 10s - loss: 0.6285 - accuracy: 0.810 - ETA: 9s - loss: 0.6278 - accuracy: 0.810 - ETA: 8s - loss: 0.6280 - accuracy: 0.81 - ETA: 7s - loss: 0.6276 - accuracy: 0.81 - ETA: 6s - loss: 0.6266 - accuracy: 0.81 - ETA: 6s - loss: 0.6263 - accuracy: 0.81 - ETA: 5s - loss: 0.6272 - accuracy: 0.81 - ETA: 4s - loss: 0.6253 - accuracy: 0.81 - ETA: 3s - loss: 0.6234 - accuracy: 0.81 - ETA: 2s - loss: 0.6255 - accuracy: 0.81 - ETA: 1s - loss: 0.6261 - accuracy: 0.81 - ETA: 0s - loss: 0.6270 - accuracy: 0.81 - 105s 8ms/step - loss: 0.6268 - accuracy: 0.8116 - val_loss: 3.2120 - val_accuracy: 0.3587\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:27 - loss: 0.9701 - accuracy: 0.75 - ETA: 1:28 - loss: 0.8072 - accuracy: 0.80 - ETA: 1:26 - loss: 0.7420 - accuracy: 0.80 - ETA: 1:26 - loss: 0.7461 - accuracy: 0.80 - ETA: 1:24 - loss: 0.7533 - accuracy: 0.79 - ETA: 1:24 - loss: 0.7521 - accuracy: 0.78 - ETA: 1:22 - loss: 0.7297 - accuracy: 0.79 - ETA: 1:22 - loss: 0.7051 - accuracy: 0.80 - ETA: 1:22 - loss: 0.7311 - accuracy: 0.79 - ETA: 1:22 - loss: 0.7234 - accuracy: 0.79 - ETA: 1:21 - loss: 0.7203 - accuracy: 0.79 - ETA: 1:20 - loss: 0.7153 - accuracy: 0.79 - ETA: 1:19 - loss: 0.7029 - accuracy: 0.79 - ETA: 1:18 - loss: 0.6821 - accuracy: 0.80 - ETA: 1:17 - loss: 0.6761 - accuracy: 0.80 - ETA: 1:16 - loss: 0.6622 - accuracy: 0.80 - ETA: 1:15 - loss: 0.6595 - accuracy: 0.80 - ETA: 1:14 - loss: 0.6501 - accuracy: 0.81 - ETA: 1:13 - loss: 0.6440 - accuracy: 0.81 - ETA: 1:12 - loss: 0.6391 - accuracy: 0.81 - ETA: 1:12 - loss: 0.6315 - accuracy: 0.81 - ETA: 1:10 - loss: 0.6261 - accuracy: 0.81 - ETA: 1:09 - loss: 0.6255 - accuracy: 0.81 - ETA: 1:08 - loss: 0.6258 - accuracy: 0.81 - ETA: 1:08 - loss: 0.6286 - accuracy: 0.81 - ETA: 1:07 - loss: 0.6285 - accuracy: 0.81 - ETA: 1:06 - loss: 0.6228 - accuracy: 0.81 - ETA: 1:05 - loss: 0.6253 - accuracy: 0.81 - ETA: 1:04 - loss: 0.6216 - accuracy: 0.81 - ETA: 1:03 - loss: 0.6225 - accuracy: 0.81 - ETA: 1:03 - loss: 0.6200 - accuracy: 0.81 - ETA: 1:02 - loss: 0.6157 - accuracy: 0.81 - ETA: 1:01 - loss: 0.6119 - accuracy: 0.81 - ETA: 1:00 - loss: 0.6137 - accuracy: 0.81 - ETA: 59s - loss: 0.6076 - accuracy: 0.8192 - ETA: 58s - loss: 0.6139 - accuracy: 0.818 - ETA: 57s - loss: 0.6215 - accuracy: 0.817 - ETA: 56s - loss: 0.6233 - accuracy: 0.817 - ETA: 55s - loss: 0.6257 - accuracy: 0.815 - ETA: 55s - loss: 0.6274 - accuracy: 0.814 - ETA: 54s - loss: 0.6264 - accuracy: 0.814 - ETA: 53s - loss: 0.6308 - accuracy: 0.814 - ETA: 52s - loss: 0.6304 - accuracy: 0.814 - ETA: 51s - loss: 0.6268 - accuracy: 0.815 - ETA: 50s - loss: 0.6300 - accuracy: 0.814 - ETA: 49s - loss: 0.6307 - accuracy: 0.814 - ETA: 49s - loss: 0.6370 - accuracy: 0.812 - ETA: 48s - loss: 0.6324 - accuracy: 0.814 - ETA: 47s - loss: 0.6356 - accuracy: 0.814 - ETA: 46s - loss: 0.6367 - accuracy: 0.814 - ETA: 45s - loss: 0.6341 - accuracy: 0.815 - ETA: 44s - loss: 0.6343 - accuracy: 0.815 - ETA: 43s - loss: 0.6343 - accuracy: 0.815 - ETA: 43s - loss: 0.6331 - accuracy: 0.816 - ETA: 42s - loss: 0.6335 - accuracy: 0.816 - ETA: 41s - loss: 0.6346 - accuracy: 0.815 - ETA: 40s - loss: 0.6318 - accuracy: 0.815 - ETA: 39s - loss: 0.6326 - accuracy: 0.815 - ETA: 38s - loss: 0.6333 - accuracy: 0.814 - ETA: 37s - loss: 0.6291 - accuracy: 0.814 - ETA: 36s - loss: 0.6274 - accuracy: 0.815 - ETA: 35s - loss: 0.6259 - accuracy: 0.815 - ETA: 34s - loss: 0.6276 - accuracy: 0.815 - ETA: 33s - loss: 0.6263 - accuracy: 0.815 - ETA: 33s - loss: 0.6220 - accuracy: 0.816 - ETA: 32s - loss: 0.6211 - accuracy: 0.816 - ETA: 31s - loss: 0.6209 - accuracy: 0.816 - ETA: 30s - loss: 0.6191 - accuracy: 0.817 - ETA: 29s - loss: 0.6212 - accuracy: 0.816 - ETA: 28s - loss: 0.6220 - accuracy: 0.816 - ETA: 27s - loss: 0.6211 - accuracy: 0.816 - ETA: 26s - loss: 0.6218 - accuracy: 0.816 - ETA: 25s - loss: 0.6203 - accuracy: 0.816 - ETA: 24s - loss: 0.6195 - accuracy: 0.816 - ETA: 24s - loss: 0.6183 - accuracy: 0.817 - ETA: 23s - loss: 0.6158 - accuracy: 0.817 - ETA: 22s - loss: 0.6138 - accuracy: 0.818 - ETA: 21s - loss: 0.6116 - accuracy: 0.819 - ETA: 20s - loss: 0.6087 - accuracy: 0.819 - ETA: 19s - loss: 0.6107 - accuracy: 0.819 - ETA: 18s - loss: 0.6111 - accuracy: 0.819 - ETA: 17s - loss: 0.6114 - accuracy: 0.819 - ETA: 16s - loss: 0.6127 - accuracy: 0.819 - ETA: 15s - loss: 0.6113 - accuracy: 0.819 - ETA: 15s - loss: 0.6108 - accuracy: 0.820 - ETA: 14s - loss: 0.6122 - accuracy: 0.819 - ETA: 13s - loss: 0.6117 - accuracy: 0.819 - ETA: 12s - loss: 0.6112 - accuracy: 0.819 - ETA: 11s - loss: 0.6115 - accuracy: 0.819 - ETA: 10s - loss: 0.6120 - accuracy: 0.819 - ETA: 9s - loss: 0.6109 - accuracy: 0.819 - ETA: 8s - loss: 0.6127 - accuracy: 0.81 - ETA: 7s - loss: 0.6101 - accuracy: 0.81 - ETA: 6s - loss: 0.6092 - accuracy: 0.81 - ETA: 6s - loss: 0.6087 - accuracy: 0.81 - ETA: 5s - loss: 0.6085 - accuracy: 0.82 - ETA: 4s - loss: 0.6086 - accuracy: 0.81 - ETA: 3s - loss: 0.6080 - accuracy: 0.82 - ETA: 2s - loss: 0.6089 - accuracy: 0.82 - ETA: 1s - loss: 0.6106 - accuracy: 0.81 - ETA: 0s - loss: 0.6105 - accuracy: 0.81 - 105s 8ms/step - loss: 0.6093 - accuracy: 0.8198 - val_loss: 3.1516 - val_accuracy: 0.3353\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:35 - loss: 0.6535 - accuracy: 0.80 - ETA: 1:32 - loss: 0.6522 - accuracy: 0.80 - ETA: 1:30 - loss: 0.6120 - accuracy: 0.82 - ETA: 1:28 - loss: 0.5987 - accuracy: 0.81 - ETA: 1:27 - loss: 0.6357 - accuracy: 0.80 - ETA: 1:27 - loss: 0.6148 - accuracy: 0.81 - ETA: 1:26 - loss: 0.5840 - accuracy: 0.82 - ETA: 1:26 - loss: 0.5873 - accuracy: 0.82 - ETA: 1:26 - loss: 0.5766 - accuracy: 0.82 - ETA: 1:24 - loss: 0.5885 - accuracy: 0.81 - ETA: 1:23 - loss: 0.6037 - accuracy: 0.81 - ETA: 1:22 - loss: 0.5913 - accuracy: 0.81 - ETA: 1:21 - loss: 0.6019 - accuracy: 0.81 - ETA: 1:20 - loss: 0.6057 - accuracy: 0.81 - ETA: 1:19 - loss: 0.5888 - accuracy: 0.81 - ETA: 1:18 - loss: 0.5885 - accuracy: 0.81 - ETA: 1:17 - loss: 0.5989 - accuracy: 0.80 - ETA: 1:16 - loss: 0.5997 - accuracy: 0.80 - ETA: 1:15 - loss: 0.5978 - accuracy: 0.80 - ETA: 1:14 - loss: 0.6013 - accuracy: 0.80 - ETA: 1:13 - loss: 0.6052 - accuracy: 0.80 - ETA: 1:12 - loss: 0.5996 - accuracy: 0.80 - ETA: 1:11 - loss: 0.6018 - accuracy: 0.80 - ETA: 1:10 - loss: 0.6072 - accuracy: 0.80 - ETA: 1:09 - loss: 0.6117 - accuracy: 0.80 - ETA: 1:08 - loss: 0.6093 - accuracy: 0.80 - ETA: 1:07 - loss: 0.6040 - accuracy: 0.81 - ETA: 1:06 - loss: 0.6008 - accuracy: 0.81 - ETA: 1:05 - loss: 0.5972 - accuracy: 0.81 - ETA: 1:04 - loss: 0.5963 - accuracy: 0.81 - ETA: 1:03 - loss: 0.5930 - accuracy: 0.81 - ETA: 1:02 - loss: 0.5909 - accuracy: 0.81 - ETA: 1:01 - loss: 0.5899 - accuracy: 0.81 - ETA: 1:01 - loss: 0.5875 - accuracy: 0.81 - ETA: 1:00 - loss: 0.5871 - accuracy: 0.81 - ETA: 59s - loss: 0.5898 - accuracy: 0.8153 - ETA: 58s - loss: 0.5881 - accuracy: 0.815 - ETA: 57s - loss: 0.5862 - accuracy: 0.816 - ETA: 56s - loss: 0.5874 - accuracy: 0.815 - ETA: 55s - loss: 0.5898 - accuracy: 0.816 - ETA: 54s - loss: 0.5909 - accuracy: 0.815 - ETA: 53s - loss: 0.5989 - accuracy: 0.813 - ETA: 52s - loss: 0.6011 - accuracy: 0.811 - ETA: 51s - loss: 0.6085 - accuracy: 0.810 - ETA: 50s - loss: 0.6096 - accuracy: 0.809 - ETA: 50s - loss: 0.6072 - accuracy: 0.810 - ETA: 49s - loss: 0.6071 - accuracy: 0.808 - ETA: 48s - loss: 0.6046 - accuracy: 0.809 - ETA: 47s - loss: 0.6034 - accuracy: 0.810 - ETA: 46s - loss: 0.6028 - accuracy: 0.810 - ETA: 45s - loss: 0.6000 - accuracy: 0.811 - ETA: 44s - loss: 0.5992 - accuracy: 0.810 - ETA: 44s - loss: 0.6022 - accuracy: 0.810 - ETA: 43s - loss: 0.6074 - accuracy: 0.809 - ETA: 42s - loss: 0.6117 - accuracy: 0.808 - ETA: 41s - loss: 0.6119 - accuracy: 0.808 - ETA: 40s - loss: 0.6113 - accuracy: 0.809 - ETA: 39s - loss: 0.6111 - accuracy: 0.809 - ETA: 38s - loss: 0.6113 - accuracy: 0.809 - ETA: 37s - loss: 0.6119 - accuracy: 0.809 - ETA: 36s - loss: 0.6128 - accuracy: 0.809 - ETA: 35s - loss: 0.6129 - accuracy: 0.809 - ETA: 34s - loss: 0.6155 - accuracy: 0.808 - ETA: 34s - loss: 0.6130 - accuracy: 0.809 - ETA: 33s - loss: 0.6110 - accuracy: 0.810 - ETA: 32s - loss: 0.6134 - accuracy: 0.809 - ETA: 31s - loss: 0.6128 - accuracy: 0.810 - ETA: 30s - loss: 0.6151 - accuracy: 0.810 - ETA: 29s - loss: 0.6137 - accuracy: 0.810 - ETA: 28s - loss: 0.6142 - accuracy: 0.810 - ETA: 27s - loss: 0.6127 - accuracy: 0.811 - ETA: 26s - loss: 0.6133 - accuracy: 0.811 - ETA: 25s - loss: 0.6159 - accuracy: 0.810 - ETA: 25s - loss: 0.6164 - accuracy: 0.810 - ETA: 24s - loss: 0.6181 - accuracy: 0.811 - ETA: 23s - loss: 0.6184 - accuracy: 0.811 - ETA: 22s - loss: 0.6207 - accuracy: 0.810 - ETA: 21s - loss: 0.6203 - accuracy: 0.811 - ETA: 20s - loss: 0.6189 - accuracy: 0.811 - ETA: 19s - loss: 0.6182 - accuracy: 0.812 - ETA: 18s - loss: 0.6162 - accuracy: 0.812 - ETA: 17s - loss: 0.6130 - accuracy: 0.813 - ETA: 16s - loss: 0.6130 - accuracy: 0.813 - ETA: 16s - loss: 0.6132 - accuracy: 0.813 - ETA: 15s - loss: 0.6143 - accuracy: 0.813 - ETA: 14s - loss: 0.6152 - accuracy: 0.814 - ETA: 13s - loss: 0.6143 - accuracy: 0.814 - ETA: 12s - loss: 0.6127 - accuracy: 0.814 - ETA: 11s - loss: 0.6129 - accuracy: 0.814 - ETA: 10s - loss: 0.6129 - accuracy: 0.814 - ETA: 9s - loss: 0.6121 - accuracy: 0.814 - ETA: 8s - loss: 0.6110 - accuracy: 0.81 - ETA: 7s - loss: 0.6120 - accuracy: 0.81 - ETA: 7s - loss: 0.6118 - accuracy: 0.81 - ETA: 6s - loss: 0.6133 - accuracy: 0.81 - ETA: 5s - loss: 0.6128 - accuracy: 0.81 - ETA: 4s - loss: 0.6136 - accuracy: 0.81 - ETA: 3s - loss: 0.6128 - accuracy: 0.81 - ETA: 2s - loss: 0.6127 - accuracy: 0.81 - ETA: 1s - loss: 0.6113 - accuracy: 0.81 - ETA: 0s - loss: 0.6107 - accuracy: 0.81 - 106s 8ms/step - loss: 0.6105 - accuracy: 0.8140 - val_loss: 3.2418 - val_accuracy: 0.3371\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:40 - loss: 0.6777 - accuracy: 0.79 - ETA: 1:38 - loss: 0.6570 - accuracy: 0.80 - ETA: 1:37 - loss: 0.7051 - accuracy: 0.79 - ETA: 1:36 - loss: 0.6673 - accuracy: 0.81 - ETA: 1:34 - loss: 0.6717 - accuracy: 0.81 - ETA: 1:33 - loss: 0.6319 - accuracy: 0.81 - ETA: 1:31 - loss: 0.6562 - accuracy: 0.81 - ETA: 1:30 - loss: 0.6304 - accuracy: 0.82 - ETA: 1:28 - loss: 0.6143 - accuracy: 0.83 - ETA: 1:26 - loss: 0.6174 - accuracy: 0.83 - ETA: 1:25 - loss: 0.6083 - accuracy: 0.83 - ETA: 1:24 - loss: 0.5894 - accuracy: 0.83 - ETA: 1:23 - loss: 0.5763 - accuracy: 0.84 - ETA: 1:22 - loss: 0.5846 - accuracy: 0.84 - ETA: 1:20 - loss: 0.5979 - accuracy: 0.83 - ETA: 1:19 - loss: 0.6011 - accuracy: 0.83 - ETA: 1:18 - loss: 0.5956 - accuracy: 0.83 - ETA: 1:17 - loss: 0.5937 - accuracy: 0.83 - ETA: 1:16 - loss: 0.5865 - accuracy: 0.83 - ETA: 1:15 - loss: 0.5906 - accuracy: 0.83 - ETA: 1:14 - loss: 0.5846 - accuracy: 0.84 - ETA: 1:13 - loss: 0.5934 - accuracy: 0.83 - ETA: 1:12 - loss: 0.5874 - accuracy: 0.83 - ETA: 1:11 - loss: 0.5862 - accuracy: 0.83 - ETA: 1:10 - loss: 0.5882 - accuracy: 0.83 - ETA: 1:09 - loss: 0.5928 - accuracy: 0.83 - ETA: 1:08 - loss: 0.5941 - accuracy: 0.83 - ETA: 1:07 - loss: 0.5961 - accuracy: 0.83 - ETA: 1:06 - loss: 0.5971 - accuracy: 0.83 - ETA: 1:05 - loss: 0.6087 - accuracy: 0.83 - ETA: 1:04 - loss: 0.6031 - accuracy: 0.83 - ETA: 1:03 - loss: 0.5997 - accuracy: 0.83 - ETA: 1:02 - loss: 0.6050 - accuracy: 0.83 - ETA: 1:01 - loss: 0.6089 - accuracy: 0.83 - ETA: 1:00 - loss: 0.6094 - accuracy: 0.83 - ETA: 59s - loss: 0.6106 - accuracy: 0.8307 - ETA: 59s - loss: 0.6106 - accuracy: 0.831 - ETA: 58s - loss: 0.6103 - accuracy: 0.831 - ETA: 57s - loss: 0.6070 - accuracy: 0.831 - ETA: 56s - loss: 0.6060 - accuracy: 0.831 - ETA: 55s - loss: 0.6009 - accuracy: 0.833 - ETA: 54s - loss: 0.5991 - accuracy: 0.833 - ETA: 53s - loss: 0.6014 - accuracy: 0.833 - ETA: 52s - loss: 0.6040 - accuracy: 0.832 - ETA: 51s - loss: 0.6029 - accuracy: 0.832 - ETA: 50s - loss: 0.6033 - accuracy: 0.831 - ETA: 49s - loss: 0.6055 - accuracy: 0.831 - ETA: 48s - loss: 0.6068 - accuracy: 0.830 - ETA: 47s - loss: 0.6082 - accuracy: 0.829 - ETA: 47s - loss: 0.6087 - accuracy: 0.829 - ETA: 46s - loss: 0.6094 - accuracy: 0.829 - ETA: 45s - loss: 0.6109 - accuracy: 0.828 - ETA: 44s - loss: 0.6169 - accuracy: 0.827 - ETA: 43s - loss: 0.6197 - accuracy: 0.827 - ETA: 42s - loss: 0.6155 - accuracy: 0.828 - ETA: 41s - loss: 0.6146 - accuracy: 0.828 - ETA: 40s - loss: 0.6170 - accuracy: 0.827 - ETA: 39s - loss: 0.6202 - accuracy: 0.826 - ETA: 38s - loss: 0.6185 - accuracy: 0.827 - ETA: 37s - loss: 0.6199 - accuracy: 0.826 - ETA: 36s - loss: 0.6211 - accuracy: 0.825 - ETA: 36s - loss: 0.6193 - accuracy: 0.825 - ETA: 35s - loss: 0.6216 - accuracy: 0.825 - ETA: 34s - loss: 0.6239 - accuracy: 0.824 - ETA: 33s - loss: 0.6242 - accuracy: 0.824 - ETA: 32s - loss: 0.6229 - accuracy: 0.824 - ETA: 31s - loss: 0.6242 - accuracy: 0.824 - ETA: 30s - loss: 0.6231 - accuracy: 0.824 - ETA: 29s - loss: 0.6221 - accuracy: 0.824 - ETA: 28s - loss: 0.6223 - accuracy: 0.824 - ETA: 27s - loss: 0.6234 - accuracy: 0.823 - ETA: 26s - loss: 0.6248 - accuracy: 0.823 - ETA: 26s - loss: 0.6256 - accuracy: 0.823 - ETA: 25s - loss: 0.6259 - accuracy: 0.823 - ETA: 24s - loss: 0.6236 - accuracy: 0.823 - ETA: 23s - loss: 0.6199 - accuracy: 0.824 - ETA: 22s - loss: 0.6193 - accuracy: 0.824 - ETA: 21s - loss: 0.6228 - accuracy: 0.823 - ETA: 20s - loss: 0.6228 - accuracy: 0.823 - ETA: 19s - loss: 0.6212 - accuracy: 0.824 - ETA: 18s - loss: 0.6202 - accuracy: 0.824 - ETA: 17s - loss: 0.6189 - accuracy: 0.824 - ETA: 16s - loss: 0.6176 - accuracy: 0.824 - ETA: 16s - loss: 0.6187 - accuracy: 0.824 - ETA: 15s - loss: 0.6185 - accuracy: 0.824 - ETA: 14s - loss: 0.6169 - accuracy: 0.824 - ETA: 13s - loss: 0.6175 - accuracy: 0.824 - ETA: 12s - loss: 0.6181 - accuracy: 0.824 - ETA: 11s - loss: 0.6169 - accuracy: 0.824 - ETA: 10s - loss: 0.6174 - accuracy: 0.823 - ETA: 9s - loss: 0.6161 - accuracy: 0.823 - ETA: 8s - loss: 0.6181 - accuracy: 0.82 - ETA: 7s - loss: 0.6162 - accuracy: 0.82 - ETA: 7s - loss: 0.6154 - accuracy: 0.82 - ETA: 6s - loss: 0.6147 - accuracy: 0.82 - ETA: 5s - loss: 0.6173 - accuracy: 0.82 - ETA: 4s - loss: 0.6154 - accuracy: 0.82 - ETA: 3s - loss: 0.6180 - accuracy: 0.82 - ETA: 2s - loss: 0.6181 - accuracy: 0.82 - ETA: 1s - loss: 0.6181 - accuracy: 0.82 - ETA: 0s - loss: 0.6184 - accuracy: 0.82 - 106s 8ms/step - loss: 0.6186 - accuracy: 0.8231 - val_loss: 3.1841 - val_accuracy: 0.3500\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.6146 - accuracy: 0.78 - ETA: 1:30 - loss: 0.5405 - accuracy: 0.82 - ETA: 1:28 - loss: 0.5345 - accuracy: 0.83 - ETA: 1:27 - loss: 0.6189 - accuracy: 0.81 - ETA: 1:26 - loss: 0.6069 - accuracy: 0.82 - ETA: 1:25 - loss: 0.5914 - accuracy: 0.82 - ETA: 1:25 - loss: 0.5649 - accuracy: 0.83 - ETA: 1:23 - loss: 0.5941 - accuracy: 0.82 - ETA: 1:23 - loss: 0.5946 - accuracy: 0.82 - ETA: 1:22 - loss: 0.6248 - accuracy: 0.81 - ETA: 1:21 - loss: 0.6178 - accuracy: 0.82 - ETA: 1:20 - loss: 0.6192 - accuracy: 0.82 - ETA: 1:19 - loss: 0.6116 - accuracy: 0.82 - ETA: 1:19 - loss: 0.6134 - accuracy: 0.82 - ETA: 1:18 - loss: 0.6174 - accuracy: 0.81 - ETA: 1:17 - loss: 0.6191 - accuracy: 0.81 - ETA: 1:16 - loss: 0.6268 - accuracy: 0.81 - ETA: 1:15 - loss: 0.6248 - accuracy: 0.81 - ETA: 1:14 - loss: 0.6309 - accuracy: 0.81 - ETA: 1:13 - loss: 0.6475 - accuracy: 0.81 - ETA: 1:12 - loss: 0.6388 - accuracy: 0.81 - ETA: 1:11 - loss: 0.6311 - accuracy: 0.81 - ETA: 1:10 - loss: 0.6324 - accuracy: 0.81 - ETA: 1:09 - loss: 0.6344 - accuracy: 0.81 - ETA: 1:08 - loss: 0.6321 - accuracy: 0.81 - ETA: 1:07 - loss: 0.6358 - accuracy: 0.80 - ETA: 1:06 - loss: 0.6344 - accuracy: 0.81 - ETA: 1:05 - loss: 0.6335 - accuracy: 0.81 - ETA: 1:04 - loss: 0.6296 - accuracy: 0.81 - ETA: 1:04 - loss: 0.6276 - accuracy: 0.81 - ETA: 1:03 - loss: 0.6278 - accuracy: 0.81 - ETA: 1:02 - loss: 0.6267 - accuracy: 0.81 - ETA: 1:01 - loss: 0.6227 - accuracy: 0.81 - ETA: 1:00 - loss: 0.6190 - accuracy: 0.81 - ETA: 59s - loss: 0.6150 - accuracy: 0.8176 - ETA: 58s - loss: 0.6163 - accuracy: 0.817 - ETA: 57s - loss: 0.6119 - accuracy: 0.818 - ETA: 56s - loss: 0.6103 - accuracy: 0.818 - ETA: 56s - loss: 0.6066 - accuracy: 0.819 - ETA: 55s - loss: 0.6066 - accuracy: 0.819 - ETA: 54s - loss: 0.6046 - accuracy: 0.821 - ETA: 53s - loss: 0.6067 - accuracy: 0.821 - ETA: 52s - loss: 0.6086 - accuracy: 0.821 - ETA: 51s - loss: 0.6096 - accuracy: 0.820 - ETA: 50s - loss: 0.6155 - accuracy: 0.819 - ETA: 49s - loss: 0.6145 - accuracy: 0.820 - ETA: 48s - loss: 0.6151 - accuracy: 0.819 - ETA: 48s - loss: 0.6115 - accuracy: 0.820 - ETA: 47s - loss: 0.6116 - accuracy: 0.820 - ETA: 46s - loss: 0.6094 - accuracy: 0.820 - ETA: 45s - loss: 0.6071 - accuracy: 0.821 - ETA: 44s - loss: 0.6091 - accuracy: 0.820 - ETA: 43s - loss: 0.6118 - accuracy: 0.820 - ETA: 42s - loss: 0.6093 - accuracy: 0.820 - ETA: 41s - loss: 0.6084 - accuracy: 0.820 - ETA: 41s - loss: 0.6118 - accuracy: 0.819 - ETA: 40s - loss: 0.6105 - accuracy: 0.819 - ETA: 39s - loss: 0.6132 - accuracy: 0.819 - ETA: 38s - loss: 0.6120 - accuracy: 0.820 - ETA: 37s - loss: 0.6119 - accuracy: 0.820 - ETA: 36s - loss: 0.6113 - accuracy: 0.820 - ETA: 35s - loss: 0.6098 - accuracy: 0.821 - ETA: 34s - loss: 0.6115 - accuracy: 0.820 - ETA: 33s - loss: 0.6122 - accuracy: 0.819 - ETA: 33s - loss: 0.6115 - accuracy: 0.819 - ETA: 32s - loss: 0.6092 - accuracy: 0.820 - ETA: 31s - loss: 0.6100 - accuracy: 0.820 - ETA: 30s - loss: 0.6081 - accuracy: 0.820 - ETA: 29s - loss: 0.6084 - accuracy: 0.821 - ETA: 28s - loss: 0.6090 - accuracy: 0.820 - ETA: 27s - loss: 0.6099 - accuracy: 0.820 - ETA: 26s - loss: 0.6108 - accuracy: 0.820 - ETA: 25s - loss: 0.6084 - accuracy: 0.820 - ETA: 24s - loss: 0.6108 - accuracy: 0.819 - ETA: 24s - loss: 0.6098 - accuracy: 0.820 - ETA: 23s - loss: 0.6081 - accuracy: 0.820 - ETA: 22s - loss: 0.6102 - accuracy: 0.820 - ETA: 21s - loss: 0.6091 - accuracy: 0.820 - ETA: 20s - loss: 0.6102 - accuracy: 0.820 - ETA: 19s - loss: 0.6130 - accuracy: 0.819 - ETA: 18s - loss: 0.6145 - accuracy: 0.819 - ETA: 17s - loss: 0.6152 - accuracy: 0.818 - ETA: 16s - loss: 0.6175 - accuracy: 0.818 - ETA: 15s - loss: 0.6201 - accuracy: 0.817 - ETA: 15s - loss: 0.6213 - accuracy: 0.817 - ETA: 14s - loss: 0.6214 - accuracy: 0.817 - ETA: 13s - loss: 0.6215 - accuracy: 0.817 - ETA: 12s - loss: 0.6209 - accuracy: 0.817 - ETA: 11s - loss: 0.6220 - accuracy: 0.817 - ETA: 10s - loss: 0.6236 - accuracy: 0.817 - ETA: 9s - loss: 0.6229 - accuracy: 0.817 - ETA: 8s - loss: 0.6226 - accuracy: 0.81 - ETA: 7s - loss: 0.6230 - accuracy: 0.81 - ETA: 6s - loss: 0.6247 - accuracy: 0.81 - ETA: 6s - loss: 0.6239 - accuracy: 0.81 - ETA: 5s - loss: 0.6261 - accuracy: 0.81 - ETA: 4s - loss: 0.6246 - accuracy: 0.81 - ETA: 3s - loss: 0.6226 - accuracy: 0.81 - ETA: 2s - loss: 0.6220 - accuracy: 0.81 - ETA: 1s - loss: 0.6208 - accuracy: 0.81 - ETA: 0s - loss: 0.6204 - accuracy: 0.81 - 105s 8ms/step - loss: 0.6201 - accuracy: 0.8188 - val_loss: 3.2666 - val_accuracy: 0.3042\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.6639 - accuracy: 0.78 - ETA: 1:34 - loss: 0.6827 - accuracy: 0.78 - ETA: 1:34 - loss: 0.7058 - accuracy: 0.78 - ETA: 1:32 - loss: 0.6981 - accuracy: 0.79 - ETA: 1:30 - loss: 0.6838 - accuracy: 0.79 - ETA: 1:29 - loss: 0.7038 - accuracy: 0.79 - ETA: 1:27 - loss: 0.6944 - accuracy: 0.78 - ETA: 1:26 - loss: 0.7045 - accuracy: 0.78 - ETA: 1:25 - loss: 0.6989 - accuracy: 0.78 - ETA: 1:23 - loss: 0.6913 - accuracy: 0.78 - ETA: 1:22 - loss: 0.6891 - accuracy: 0.78 - ETA: 1:21 - loss: 0.7025 - accuracy: 0.78 - ETA: 1:20 - loss: 0.6846 - accuracy: 0.78 - ETA: 1:19 - loss: 0.6776 - accuracy: 0.79 - ETA: 1:18 - loss: 0.6922 - accuracy: 0.79 - ETA: 1:17 - loss: 0.6783 - accuracy: 0.79 - ETA: 1:16 - loss: 0.6882 - accuracy: 0.79 - ETA: 1:15 - loss: 0.6828 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6760 - accuracy: 0.79 - ETA: 1:14 - loss: 0.6716 - accuracy: 0.79 - ETA: 1:13 - loss: 0.6646 - accuracy: 0.80 - ETA: 1:12 - loss: 0.6582 - accuracy: 0.80 - ETA: 1:11 - loss: 0.6553 - accuracy: 0.80 - ETA: 1:11 - loss: 0.6482 - accuracy: 0.80 - ETA: 1:10 - loss: 0.6451 - accuracy: 0.80 - ETA: 1:09 - loss: 0.6438 - accuracy: 0.80 - ETA: 1:08 - loss: 0.6509 - accuracy: 0.80 - ETA: 1:07 - loss: 0.6419 - accuracy: 0.81 - ETA: 1:06 - loss: 0.6476 - accuracy: 0.81 - ETA: 1:05 - loss: 0.6479 - accuracy: 0.81 - ETA: 1:04 - loss: 0.6487 - accuracy: 0.81 - ETA: 1:03 - loss: 0.6526 - accuracy: 0.81 - ETA: 1:02 - loss: 0.6522 - accuracy: 0.81 - ETA: 1:01 - loss: 0.6533 - accuracy: 0.81 - ETA: 1:00 - loss: 0.6574 - accuracy: 0.81 - ETA: 59s - loss: 0.6659 - accuracy: 0.8097 - ETA: 58s - loss: 0.6628 - accuracy: 0.811 - ETA: 57s - loss: 0.6655 - accuracy: 0.809 - ETA: 57s - loss: 0.6622 - accuracy: 0.809 - ETA: 56s - loss: 0.6606 - accuracy: 0.810 - ETA: 55s - loss: 0.6625 - accuracy: 0.810 - ETA: 54s - loss: 0.6692 - accuracy: 0.808 - ETA: 53s - loss: 0.6679 - accuracy: 0.808 - ETA: 52s - loss: 0.6695 - accuracy: 0.807 - ETA: 51s - loss: 0.6646 - accuracy: 0.808 - ETA: 50s - loss: 0.6690 - accuracy: 0.809 - ETA: 49s - loss: 0.6708 - accuracy: 0.808 - ETA: 48s - loss: 0.6698 - accuracy: 0.808 - ETA: 47s - loss: 0.6663 - accuracy: 0.808 - ETA: 46s - loss: 0.6644 - accuracy: 0.808 - ETA: 45s - loss: 0.6651 - accuracy: 0.807 - ETA: 45s - loss: 0.6670 - accuracy: 0.807 - ETA: 44s - loss: 0.6669 - accuracy: 0.806 - ETA: 43s - loss: 0.6656 - accuracy: 0.807 - ETA: 42s - loss: 0.6661 - accuracy: 0.807 - ETA: 41s - loss: 0.6651 - accuracy: 0.807 - ETA: 40s - loss: 0.6660 - accuracy: 0.807 - ETA: 39s - loss: 0.6652 - accuracy: 0.807 - ETA: 38s - loss: 0.6602 - accuracy: 0.808 - ETA: 37s - loss: 0.6579 - accuracy: 0.808 - ETA: 36s - loss: 0.6585 - accuracy: 0.808 - ETA: 35s - loss: 0.6551 - accuracy: 0.809 - ETA: 34s - loss: 0.6551 - accuracy: 0.809 - ETA: 34s - loss: 0.6550 - accuracy: 0.808 - ETA: 33s - loss: 0.6536 - accuracy: 0.808 - ETA: 32s - loss: 0.6558 - accuracy: 0.808 - ETA: 31s - loss: 0.6539 - accuracy: 0.808 - ETA: 30s - loss: 0.6522 - accuracy: 0.808 - ETA: 29s - loss: 0.6504 - accuracy: 0.808 - ETA: 28s - loss: 0.6480 - accuracy: 0.808 - ETA: 27s - loss: 0.6494 - accuracy: 0.808 - ETA: 26s - loss: 0.6487 - accuracy: 0.808 - ETA: 25s - loss: 0.6489 - accuracy: 0.808 - ETA: 25s - loss: 0.6474 - accuracy: 0.809 - ETA: 24s - loss: 0.6476 - accuracy: 0.809 - ETA: 23s - loss: 0.6466 - accuracy: 0.809 - ETA: 22s - loss: 0.6444 - accuracy: 0.809 - ETA: 21s - loss: 0.6431 - accuracy: 0.810 - ETA: 20s - loss: 0.6435 - accuracy: 0.810 - ETA: 19s - loss: 0.6455 - accuracy: 0.809 - ETA: 18s - loss: 0.6457 - accuracy: 0.809 - ETA: 17s - loss: 0.6440 - accuracy: 0.810 - ETA: 16s - loss: 0.6446 - accuracy: 0.810 - ETA: 16s - loss: 0.6445 - accuracy: 0.810 - ETA: 15s - loss: 0.6450 - accuracy: 0.809 - ETA: 14s - loss: 0.6443 - accuracy: 0.809 - ETA: 13s - loss: 0.6464 - accuracy: 0.809 - ETA: 12s - loss: 0.6474 - accuracy: 0.809 - ETA: 11s - loss: 0.6468 - accuracy: 0.809 - ETA: 10s - loss: 0.6457 - accuracy: 0.810 - ETA: 9s - loss: 0.6441 - accuracy: 0.810 - ETA: 8s - loss: 0.6423 - accuracy: 0.81 - ETA: 7s - loss: 0.6425 - accuracy: 0.81 - ETA: 6s - loss: 0.6425 - accuracy: 0.81 - ETA: 6s - loss: 0.6423 - accuracy: 0.81 - ETA: 5s - loss: 0.6434 - accuracy: 0.81 - ETA: 4s - loss: 0.6422 - accuracy: 0.81 - ETA: 3s - loss: 0.6414 - accuracy: 0.81 - ETA: 2s - loss: 0.6403 - accuracy: 0.81 - ETA: 1s - loss: 0.6388 - accuracy: 0.81 - ETA: 0s - loss: 0.6395 - accuracy: 0.81 - 106s 8ms/step - loss: 0.6404 - accuracy: 0.8104 - val_loss: 3.1364 - val_accuracy: 0.3131\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:33 - loss: 0.9192 - accuracy: 0.75 - ETA: 1:31 - loss: 0.7942 - accuracy: 0.78 - ETA: 1:30 - loss: 0.7124 - accuracy: 0.80 - ETA: 1:28 - loss: 0.6339 - accuracy: 0.81 - ETA: 1:27 - loss: 0.6420 - accuracy: 0.81 - ETA: 1:25 - loss: 0.6404 - accuracy: 0.82 - ETA: 1:24 - loss: 0.6231 - accuracy: 0.82 - ETA: 1:23 - loss: 0.6153 - accuracy: 0.82 - ETA: 1:22 - loss: 0.6165 - accuracy: 0.82 - ETA: 1:22 - loss: 0.6228 - accuracy: 0.82 - ETA: 1:21 - loss: 0.6172 - accuracy: 0.82 - ETA: 1:20 - loss: 0.6241 - accuracy: 0.82 - ETA: 1:19 - loss: 0.6447 - accuracy: 0.82 - ETA: 1:18 - loss: 0.6441 - accuracy: 0.81 - ETA: 1:18 - loss: 0.6466 - accuracy: 0.81 - ETA: 1:17 - loss: 0.6495 - accuracy: 0.81 - ETA: 1:16 - loss: 0.6522 - accuracy: 0.81 - ETA: 1:15 - loss: 0.6522 - accuracy: 0.81 - ETA: 1:14 - loss: 0.6556 - accuracy: 0.81 - ETA: 1:13 - loss: 0.6675 - accuracy: 0.80 - ETA: 1:12 - loss: 0.6651 - accuracy: 0.81 - ETA: 1:11 - loss: 0.6589 - accuracy: 0.81 - ETA: 1:10 - loss: 0.6549 - accuracy: 0.81 - ETA: 1:09 - loss: 0.6505 - accuracy: 0.81 - ETA: 1:08 - loss: 0.6439 - accuracy: 0.81 - ETA: 1:07 - loss: 0.6395 - accuracy: 0.81 - ETA: 1:06 - loss: 0.6430 - accuracy: 0.81 - ETA: 1:05 - loss: 0.6448 - accuracy: 0.81 - ETA: 1:05 - loss: 0.6439 - accuracy: 0.81 - ETA: 1:04 - loss: 0.6481 - accuracy: 0.81 - ETA: 1:03 - loss: 0.6434 - accuracy: 0.81 - ETA: 1:02 - loss: 0.6461 - accuracy: 0.81 - ETA: 1:01 - loss: 0.6447 - accuracy: 0.81 - ETA: 1:00 - loss: 0.6392 - accuracy: 0.81 - ETA: 59s - loss: 0.6414 - accuracy: 0.8150 - ETA: 58s - loss: 0.6394 - accuracy: 0.815 - ETA: 57s - loss: 0.6378 - accuracy: 0.815 - ETA: 57s - loss: 0.6356 - accuracy: 0.815 - ETA: 56s - loss: 0.6312 - accuracy: 0.816 - ETA: 55s - loss: 0.6357 - accuracy: 0.815 - ETA: 54s - loss: 0.6374 - accuracy: 0.815 - ETA: 53s - loss: 0.6354 - accuracy: 0.815 - ETA: 52s - loss: 0.6349 - accuracy: 0.815 - ETA: 51s - loss: 0.6325 - accuracy: 0.816 - ETA: 50s - loss: 0.6308 - accuracy: 0.816 - ETA: 50s - loss: 0.6265 - accuracy: 0.817 - ETA: 49s - loss: 0.6218 - accuracy: 0.818 - ETA: 48s - loss: 0.6249 - accuracy: 0.816 - ETA: 47s - loss: 0.6255 - accuracy: 0.816 - ETA: 46s - loss: 0.6241 - accuracy: 0.817 - ETA: 45s - loss: 0.6246 - accuracy: 0.817 - ETA: 44s - loss: 0.6218 - accuracy: 0.818 - ETA: 44s - loss: 0.6178 - accuracy: 0.819 - ETA: 43s - loss: 0.6192 - accuracy: 0.818 - ETA: 42s - loss: 0.6195 - accuracy: 0.818 - ETA: 41s - loss: 0.6188 - accuracy: 0.818 - ETA: 40s - loss: 0.6188 - accuracy: 0.818 - ETA: 39s - loss: 0.6185 - accuracy: 0.818 - ETA: 38s - loss: 0.6167 - accuracy: 0.819 - ETA: 37s - loss: 0.6182 - accuracy: 0.818 - ETA: 36s - loss: 0.6166 - accuracy: 0.818 - ETA: 35s - loss: 0.6185 - accuracy: 0.818 - ETA: 35s - loss: 0.6205 - accuracy: 0.817 - ETA: 34s - loss: 0.6219 - accuracy: 0.816 - ETA: 33s - loss: 0.6219 - accuracy: 0.816 - ETA: 32s - loss: 0.6188 - accuracy: 0.817 - ETA: 31s - loss: 0.6192 - accuracy: 0.816 - ETA: 30s - loss: 0.6213 - accuracy: 0.816 - ETA: 29s - loss: 0.6200 - accuracy: 0.817 - ETA: 28s - loss: 0.6184 - accuracy: 0.817 - ETA: 27s - loss: 0.6182 - accuracy: 0.817 - ETA: 26s - loss: 0.6179 - accuracy: 0.816 - ETA: 25s - loss: 0.6157 - accuracy: 0.817 - ETA: 25s - loss: 0.6176 - accuracy: 0.817 - ETA: 24s - loss: 0.6176 - accuracy: 0.817 - ETA: 23s - loss: 0.6183 - accuracy: 0.817 - ETA: 22s - loss: 0.6169 - accuracy: 0.817 - ETA: 21s - loss: 0.6178 - accuracy: 0.816 - ETA: 20s - loss: 0.6158 - accuracy: 0.817 - ETA: 19s - loss: 0.6133 - accuracy: 0.817 - ETA: 18s - loss: 0.6138 - accuracy: 0.817 - ETA: 17s - loss: 0.6139 - accuracy: 0.817 - ETA: 16s - loss: 0.6162 - accuracy: 0.817 - ETA: 15s - loss: 0.6172 - accuracy: 0.816 - ETA: 15s - loss: 0.6175 - accuracy: 0.817 - ETA: 14s - loss: 0.6158 - accuracy: 0.817 - ETA: 13s - loss: 0.6137 - accuracy: 0.818 - ETA: 12s - loss: 0.6145 - accuracy: 0.818 - ETA: 11s - loss: 0.6124 - accuracy: 0.818 - ETA: 10s - loss: 0.6130 - accuracy: 0.818 - ETA: 9s - loss: 0.6126 - accuracy: 0.818 - ETA: 8s - loss: 0.6158 - accuracy: 0.81 - ETA: 7s - loss: 0.6158 - accuracy: 0.81 - ETA: 6s - loss: 0.6149 - accuracy: 0.81 - ETA: 6s - loss: 0.6145 - accuracy: 0.81 - ETA: 5s - loss: 0.6161 - accuracy: 0.81 - ETA: 4s - loss: 0.6164 - accuracy: 0.81 - ETA: 3s - loss: 0.6168 - accuracy: 0.81 - ETA: 2s - loss: 0.6186 - accuracy: 0.81 - ETA: 1s - loss: 0.6175 - accuracy: 0.81 - ETA: 0s - loss: 0.6187 - accuracy: 0.81 - 105s 8ms/step - loss: 0.6207 - accuracy: 0.8161 - val_loss: 3.2498 - val_accuracy: 0.3128\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:30 - loss: 0.5473 - accuracy: 0.82 - ETA: 1:28 - loss: 0.5201 - accuracy: 0.85 - ETA: 1:27 - loss: 0.5153 - accuracy: 0.85 - ETA: 1:26 - loss: 0.5449 - accuracy: 0.84 - ETA: 1:26 - loss: 0.5540 - accuracy: 0.83 - ETA: 1:27 - loss: 0.5467 - accuracy: 0.83 - ETA: 1:25 - loss: 0.5517 - accuracy: 0.83 - ETA: 1:25 - loss: 0.5540 - accuracy: 0.82 - ETA: 1:24 - loss: 0.5400 - accuracy: 0.83 - ETA: 1:24 - loss: 0.5600 - accuracy: 0.82 - ETA: 1:22 - loss: 0.5538 - accuracy: 0.82 - ETA: 1:21 - loss: 0.5661 - accuracy: 0.82 - ETA: 1:20 - loss: 0.5643 - accuracy: 0.82 - ETA: 1:19 - loss: 0.5757 - accuracy: 0.82 - ETA: 1:18 - loss: 0.5877 - accuracy: 0.82 - ETA: 1:17 - loss: 0.5791 - accuracy: 0.82 - ETA: 1:17 - loss: 0.5830 - accuracy: 0.82 - ETA: 1:16 - loss: 0.5747 - accuracy: 0.82 - ETA: 1:15 - loss: 0.5810 - accuracy: 0.82 - ETA: 1:14 - loss: 0.5766 - accuracy: 0.82 - ETA: 1:13 - loss: 0.5758 - accuracy: 0.82 - ETA: 1:12 - loss: 0.5821 - accuracy: 0.82 - ETA: 1:11 - loss: 0.5887 - accuracy: 0.82 - ETA: 1:10 - loss: 0.5891 - accuracy: 0.82 - ETA: 1:09 - loss: 0.5854 - accuracy: 0.82 - ETA: 1:08 - loss: 0.5892 - accuracy: 0.82 - ETA: 1:07 - loss: 0.5855 - accuracy: 0.82 - ETA: 1:06 - loss: 0.5922 - accuracy: 0.82 - ETA: 1:05 - loss: 0.5896 - accuracy: 0.82 - ETA: 1:04 - loss: 0.5924 - accuracy: 0.82 - ETA: 1:03 - loss: 0.5895 - accuracy: 0.82 - ETA: 1:02 - loss: 0.5869 - accuracy: 0.82 - ETA: 1:02 - loss: 0.5878 - accuracy: 0.82 - ETA: 1:01 - loss: 0.5835 - accuracy: 0.82 - ETA: 1:00 - loss: 0.5889 - accuracy: 0.82 - ETA: 59s - loss: 0.5948 - accuracy: 0.8279 - ETA: 58s - loss: 0.5914 - accuracy: 0.827 - ETA: 57s - loss: 0.5877 - accuracy: 0.828 - ETA: 56s - loss: 0.5922 - accuracy: 0.828 - ETA: 55s - loss: 0.5894 - accuracy: 0.828 - ETA: 54s - loss: 0.5893 - accuracy: 0.828 - ETA: 53s - loss: 0.5914 - accuracy: 0.828 - ETA: 52s - loss: 0.5906 - accuracy: 0.829 - ETA: 51s - loss: 0.5917 - accuracy: 0.828 - ETA: 51s - loss: 0.5930 - accuracy: 0.827 - ETA: 50s - loss: 0.6018 - accuracy: 0.826 - ETA: 49s - loss: 0.6048 - accuracy: 0.826 - ETA: 48s - loss: 0.6040 - accuracy: 0.826 - ETA: 47s - loss: 0.6047 - accuracy: 0.826 - ETA: 46s - loss: 0.6071 - accuracy: 0.825 - ETA: 45s - loss: 0.6072 - accuracy: 0.825 - ETA: 44s - loss: 0.6062 - accuracy: 0.824 - ETA: 43s - loss: 0.6063 - accuracy: 0.825 - ETA: 42s - loss: 0.6069 - accuracy: 0.824 - ETA: 41s - loss: 0.6058 - accuracy: 0.825 - ETA: 39s - loss: 0.6036 - accuracy: 0.825 - ETA: 38s - loss: 0.6041 - accuracy: 0.824 - ETA: 37s - loss: 0.6043 - accuracy: 0.824 - ETA: 36s - loss: 0.6024 - accuracy: 0.824 - ETA: 36s - loss: 0.6016 - accuracy: 0.824 - ETA: 35s - loss: 0.6029 - accuracy: 0.824 - ETA: 34s - loss: 0.6050 - accuracy: 0.824 - ETA: 33s - loss: 0.6053 - accuracy: 0.823 - ETA: 32s - loss: 0.6045 - accuracy: 0.823 - ETA: 31s - loss: 0.6039 - accuracy: 0.823 - ETA: 30s - loss: 0.6013 - accuracy: 0.825 - ETA: 30s - loss: 0.6032 - accuracy: 0.824 - ETA: 29s - loss: 0.6029 - accuracy: 0.824 - ETA: 28s - loss: 0.6010 - accuracy: 0.824 - ETA: 27s - loss: 0.6018 - accuracy: 0.824 - ETA: 26s - loss: 0.6022 - accuracy: 0.824 - ETA: 25s - loss: 0.6042 - accuracy: 0.824 - ETA: 24s - loss: 0.6051 - accuracy: 0.824 - ETA: 24s - loss: 0.6026 - accuracy: 0.824 - ETA: 23s - loss: 0.6042 - accuracy: 0.823 - ETA: 22s - loss: 0.6041 - accuracy: 0.823 - ETA: 21s - loss: 0.6037 - accuracy: 0.823 - ETA: 20s - loss: 0.6042 - accuracy: 0.823 - ETA: 19s - loss: 0.6022 - accuracy: 0.824 - ETA: 18s - loss: 0.6060 - accuracy: 0.822 - ETA: 18s - loss: 0.6030 - accuracy: 0.823 - ETA: 17s - loss: 0.6045 - accuracy: 0.823 - ETA: 16s - loss: 0.6058 - accuracy: 0.822 - ETA: 15s - loss: 0.6055 - accuracy: 0.822 - ETA: 14s - loss: 0.6059 - accuracy: 0.822 - ETA: 13s - loss: 0.6080 - accuracy: 0.821 - ETA: 12s - loss: 0.6065 - accuracy: 0.822 - ETA: 12s - loss: 0.6074 - accuracy: 0.821 - ETA: 11s - loss: 0.6073 - accuracy: 0.821 - ETA: 10s - loss: 0.6086 - accuracy: 0.821 - ETA: 9s - loss: 0.6091 - accuracy: 0.821 - ETA: 8s - loss: 0.6083 - accuracy: 0.82 - ETA: 7s - loss: 0.6075 - accuracy: 0.82 - ETA: 6s - loss: 0.6060 - accuracy: 0.82 - ETA: 5s - loss: 0.6059 - accuracy: 0.82 - ETA: 5s - loss: 0.6037 - accuracy: 0.82 - ETA: 4s - loss: 0.6030 - accuracy: 0.82 - ETA: 3s - loss: 0.6032 - accuracy: 0.82 - ETA: 2s - loss: 0.6027 - accuracy: 0.82 - ETA: 1s - loss: 0.6033 - accuracy: 0.82 - ETA: 0s - loss: 0.6023 - accuracy: 0.82 - 102s 8ms/step - loss: 0.6025 - accuracy: 0.8222 - val_loss: 3.3713 - val_accuracy: 0.3284\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:32 - loss: 0.3773 - accuracy: 0.89 - ETA: 1:29 - loss: 0.3997 - accuracy: 0.87 - ETA: 1:28 - loss: 0.4194 - accuracy: 0.86 - ETA: 1:27 - loss: 0.4605 - accuracy: 0.85 - ETA: 1:26 - loss: 0.4946 - accuracy: 0.85 - ETA: 1:25 - loss: 0.5280 - accuracy: 0.84 - ETA: 1:24 - loss: 0.5185 - accuracy: 0.84 - ETA: 1:23 - loss: 0.5276 - accuracy: 0.84 - ETA: 1:23 - loss: 0.5468 - accuracy: 0.83 - ETA: 1:22 - loss: 0.5643 - accuracy: 0.83 - ETA: 1:22 - loss: 0.6014 - accuracy: 0.82 - ETA: 1:21 - loss: 0.6131 - accuracy: 0.82 - ETA: 1:20 - loss: 0.6171 - accuracy: 0.82 - ETA: 1:19 - loss: 0.6184 - accuracy: 0.82 - ETA: 1:18 - loss: 0.6198 - accuracy: 0.81 - ETA: 1:17 - loss: 0.6192 - accuracy: 0.81 - ETA: 1:16 - loss: 0.6256 - accuracy: 0.81 - ETA: 1:15 - loss: 0.6199 - accuracy: 0.81 - ETA: 1:14 - loss: 0.6171 - accuracy: 0.81 - ETA: 1:13 - loss: 0.6077 - accuracy: 0.81 - ETA: 1:12 - loss: 0.6075 - accuracy: 0.81 - ETA: 1:11 - loss: 0.6091 - accuracy: 0.81 - ETA: 1:10 - loss: 0.6124 - accuracy: 0.81 - ETA: 1:09 - loss: 0.6062 - accuracy: 0.81 - ETA: 1:08 - loss: 0.6103 - accuracy: 0.81 - ETA: 1:08 - loss: 0.6103 - accuracy: 0.81 - ETA: 1:07 - loss: 0.6161 - accuracy: 0.81 - ETA: 1:06 - loss: 0.6176 - accuracy: 0.81 - ETA: 1:05 - loss: 0.6205 - accuracy: 0.81 - ETA: 1:04 - loss: 0.6234 - accuracy: 0.81 - ETA: 1:03 - loss: 0.6236 - accuracy: 0.81 - ETA: 1:02 - loss: 0.6235 - accuracy: 0.81 - ETA: 1:01 - loss: 0.6342 - accuracy: 0.81 - ETA: 1:00 - loss: 0.6355 - accuracy: 0.81 - ETA: 59s - loss: 0.6382 - accuracy: 0.8150 - ETA: 59s - loss: 0.6343 - accuracy: 0.816 - ETA: 58s - loss: 0.6323 - accuracy: 0.816 - ETA: 57s - loss: 0.6378 - accuracy: 0.816 - ETA: 56s - loss: 0.6399 - accuracy: 0.816 - ETA: 55s - loss: 0.6427 - accuracy: 0.815 - ETA: 54s - loss: 0.6402 - accuracy: 0.817 - ETA: 53s - loss: 0.6407 - accuracy: 0.816 - ETA: 53s - loss: 0.6387 - accuracy: 0.817 - ETA: 52s - loss: 0.6378 - accuracy: 0.817 - ETA: 51s - loss: 0.6449 - accuracy: 0.815 - ETA: 50s - loss: 0.6439 - accuracy: 0.815 - ETA: 49s - loss: 0.6432 - accuracy: 0.816 - ETA: 48s - loss: 0.6435 - accuracy: 0.816 - ETA: 47s - loss: 0.6463 - accuracy: 0.816 - ETA: 46s - loss: 0.6431 - accuracy: 0.816 - ETA: 45s - loss: 0.6424 - accuracy: 0.816 - ETA: 44s - loss: 0.6498 - accuracy: 0.814 - ETA: 43s - loss: 0.6590 - accuracy: 0.812 - ETA: 43s - loss: 0.6596 - accuracy: 0.812 - ETA: 42s - loss: 0.6574 - accuracy: 0.813 - ETA: 41s - loss: 0.6521 - accuracy: 0.814 - ETA: 40s - loss: 0.6507 - accuracy: 0.814 - ETA: 39s - loss: 0.6497 - accuracy: 0.814 - ETA: 38s - loss: 0.6481 - accuracy: 0.814 - ETA: 37s - loss: 0.6475 - accuracy: 0.814 - ETA: 36s - loss: 0.6465 - accuracy: 0.815 - ETA: 35s - loss: 0.6444 - accuracy: 0.815 - ETA: 35s - loss: 0.6491 - accuracy: 0.814 - ETA: 34s - loss: 0.6490 - accuracy: 0.814 - ETA: 33s - loss: 0.6490 - accuracy: 0.813 - ETA: 32s - loss: 0.6501 - accuracy: 0.812 - ETA: 31s - loss: 0.6521 - accuracy: 0.813 - ETA: 30s - loss: 0.6503 - accuracy: 0.813 - ETA: 29s - loss: 0.6507 - accuracy: 0.814 - ETA: 28s - loss: 0.6485 - accuracy: 0.813 - ETA: 27s - loss: 0.6487 - accuracy: 0.813 - ETA: 26s - loss: 0.6481 - accuracy: 0.814 - ETA: 25s - loss: 0.6460 - accuracy: 0.815 - ETA: 25s - loss: 0.6454 - accuracy: 0.815 - ETA: 24s - loss: 0.6440 - accuracy: 0.815 - ETA: 23s - loss: 0.6428 - accuracy: 0.815 - ETA: 22s - loss: 0.6418 - accuracy: 0.815 - ETA: 21s - loss: 0.6417 - accuracy: 0.814 - ETA: 20s - loss: 0.6409 - accuracy: 0.814 - ETA: 19s - loss: 0.6404 - accuracy: 0.814 - ETA: 18s - loss: 0.6390 - accuracy: 0.814 - ETA: 17s - loss: 0.6408 - accuracy: 0.814 - ETA: 16s - loss: 0.6395 - accuracy: 0.814 - ETA: 16s - loss: 0.6376 - accuracy: 0.814 - ETA: 15s - loss: 0.6349 - accuracy: 0.815 - ETA: 14s - loss: 0.6350 - accuracy: 0.815 - ETA: 13s - loss: 0.6358 - accuracy: 0.815 - ETA: 12s - loss: 0.6380 - accuracy: 0.814 - ETA: 11s - loss: 0.6374 - accuracy: 0.814 - ETA: 10s - loss: 0.6359 - accuracy: 0.815 - ETA: 9s - loss: 0.6363 - accuracy: 0.815 - ETA: 8s - loss: 0.6357 - accuracy: 0.81 - ETA: 7s - loss: 0.6349 - accuracy: 0.81 - ETA: 6s - loss: 0.6354 - accuracy: 0.81 - ETA: 6s - loss: 0.6343 - accuracy: 0.81 - ETA: 5s - loss: 0.6327 - accuracy: 0.81 - ETA: 4s - loss: 0.6310 - accuracy: 0.81 - ETA: 3s - loss: 0.6305 - accuracy: 0.81 - ETA: 2s - loss: 0.6300 - accuracy: 0.81 - ETA: 1s - loss: 0.6304 - accuracy: 0.81 - ETA: 0s - loss: 0.6306 - accuracy: 0.81 - 106s 8ms/step - loss: 0.6298 - accuracy: 0.8170 - val_loss: 3.3505 - val_accuracy: 0.3100\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13022/13022 [==============================] - ETA: 1:29 - loss: 0.4724 - accuracy: 0.84 - ETA: 1:28 - loss: 0.5501 - accuracy: 0.82 - ETA: 1:26 - loss: 0.5837 - accuracy: 0.82 - ETA: 1:26 - loss: 0.5677 - accuracy: 0.82 - ETA: 1:25 - loss: 0.5972 - accuracy: 0.82 - ETA: 1:24 - loss: 0.6287 - accuracy: 0.81 - ETA: 1:23 - loss: 0.6581 - accuracy: 0.81 - ETA: 1:22 - loss: 0.6406 - accuracy: 0.81 - ETA: 1:21 - loss: 0.6458 - accuracy: 0.81 - ETA: 1:20 - loss: 0.6536 - accuracy: 0.80 - ETA: 1:19 - loss: 0.6460 - accuracy: 0.81 - ETA: 1:18 - loss: 0.6465 - accuracy: 0.81 - ETA: 1:18 - loss: 0.6509 - accuracy: 0.81 - ETA: 1:19 - loss: 0.6515 - accuracy: 0.81 - ETA: 1:19 - loss: 0.6599 - accuracy: 0.81 - ETA: 1:18 - loss: 0.6641 - accuracy: 0.81 - ETA: 1:18 - loss: 0.6646 - accuracy: 0.81 - ETA: 1:17 - loss: 0.6572 - accuracy: 0.81 - ETA: 1:17 - loss: 0.6498 - accuracy: 0.81 - ETA: 1:16 - loss: 0.6463 - accuracy: 0.81 - ETA: 1:15 - loss: 0.6373 - accuracy: 0.81 - ETA: 1:15 - loss: 0.6406 - accuracy: 0.81 - ETA: 1:14 - loss: 0.6323 - accuracy: 0.82 - ETA: 1:13 - loss: 0.6374 - accuracy: 0.81 - ETA: 1:12 - loss: 0.6291 - accuracy: 0.81 - ETA: 1:11 - loss: 0.6298 - accuracy: 0.82 - ETA: 1:10 - loss: 0.6315 - accuracy: 0.82 - ETA: 1:10 - loss: 0.6349 - accuracy: 0.81 - ETA: 1:09 - loss: 0.6369 - accuracy: 0.81 - ETA: 1:08 - loss: 0.6338 - accuracy: 0.81 - ETA: 1:07 - loss: 0.6333 - accuracy: 0.81 - ETA: 1:06 - loss: 0.6291 - accuracy: 0.81 - ETA: 1:05 - loss: 0.6268 - accuracy: 0.81 - ETA: 1:05 - loss: 0.6267 - accuracy: 0.82 - ETA: 1:04 - loss: 0.6283 - accuracy: 0.81 - ETA: 1:03 - loss: 0.6283 - accuracy: 0.81 - ETA: 1:02 - loss: 0.6268 - accuracy: 0.81 - ETA: 1:01 - loss: 0.6302 - accuracy: 0.81 - ETA: 1:00 - loss: 0.6312 - accuracy: 0.81 - ETA: 59s - loss: 0.6291 - accuracy: 0.8174 - ETA: 58s - loss: 0.6274 - accuracy: 0.816 - ETA: 57s - loss: 0.6262 - accuracy: 0.817 - ETA: 56s - loss: 0.6280 - accuracy: 0.817 - ETA: 55s - loss: 0.6349 - accuracy: 0.814 - ETA: 54s - loss: 0.6319 - accuracy: 0.815 - ETA: 53s - loss: 0.6306 - accuracy: 0.816 - ETA: 52s - loss: 0.6342 - accuracy: 0.815 - ETA: 52s - loss: 0.6311 - accuracy: 0.815 - ETA: 51s - loss: 0.6315 - accuracy: 0.815 - ETA: 50s - loss: 0.6314 - accuracy: 0.815 - ETA: 49s - loss: 0.6334 - accuracy: 0.814 - ETA: 48s - loss: 0.6318 - accuracy: 0.815 - ETA: 47s - loss: 0.6296 - accuracy: 0.815 - ETA: 46s - loss: 0.6309 - accuracy: 0.815 - ETA: 45s - loss: 0.6306 - accuracy: 0.815 - ETA: 44s - loss: 0.6337 - accuracy: 0.814 - ETA: 43s - loss: 0.6306 - accuracy: 0.815 - ETA: 42s - loss: 0.6310 - accuracy: 0.815 - ETA: 41s - loss: 0.6295 - accuracy: 0.816 - ETA: 40s - loss: 0.6311 - accuracy: 0.815 - ETA: 39s - loss: 0.6277 - accuracy: 0.816 - ETA: 38s - loss: 0.6285 - accuracy: 0.815 - ETA: 37s - loss: 0.6251 - accuracy: 0.816 - ETA: 36s - loss: 0.6246 - accuracy: 0.816 - ETA: 35s - loss: 0.6210 - accuracy: 0.817 - ETA: 34s - loss: 0.6212 - accuracy: 0.817 - ETA: 33s - loss: 0.6189 - accuracy: 0.818 - ETA: 32s - loss: 0.6232 - accuracy: 0.818 - ETA: 31s - loss: 0.6259 - accuracy: 0.817 - ETA: 30s - loss: 0.6252 - accuracy: 0.817 - ETA: 29s - loss: 0.6274 - accuracy: 0.816 - ETA: 29s - loss: 0.6255 - accuracy: 0.816 - ETA: 28s - loss: 0.6250 - accuracy: 0.817 - ETA: 27s - loss: 0.6222 - accuracy: 0.817 - ETA: 26s - loss: 0.6207 - accuracy: 0.818 - ETA: 25s - loss: 0.6224 - accuracy: 0.817 - ETA: 24s - loss: 0.6211 - accuracy: 0.817 - ETA: 23s - loss: 0.6211 - accuracy: 0.817 - ETA: 22s - loss: 0.6207 - accuracy: 0.818 - ETA: 21s - loss: 0.6209 - accuracy: 0.818 - ETA: 20s - loss: 0.6209 - accuracy: 0.818 - ETA: 19s - loss: 0.6211 - accuracy: 0.817 - ETA: 18s - loss: 0.6195 - accuracy: 0.818 - ETA: 17s - loss: 0.6193 - accuracy: 0.818 - ETA: 16s - loss: 0.6205 - accuracy: 0.817 - ETA: 15s - loss: 0.6221 - accuracy: 0.817 - ETA: 14s - loss: 0.6226 - accuracy: 0.817 - ETA: 13s - loss: 0.6250 - accuracy: 0.817 - ETA: 12s - loss: 0.6225 - accuracy: 0.817 - ETA: 11s - loss: 0.6235 - accuracy: 0.817 - ETA: 10s - loss: 0.6219 - accuracy: 0.817 - ETA: 9s - loss: 0.6227 - accuracy: 0.817 - ETA: 8s - loss: 0.6215 - accuracy: 0.81 - ETA: 7s - loss: 0.6212 - accuracy: 0.81 - ETA: 6s - loss: 0.6194 - accuracy: 0.81 - ETA: 5s - loss: 0.6193 - accuracy: 0.81 - ETA: 4s - loss: 0.6166 - accuracy: 0.81 - ETA: 3s - loss: 0.6159 - accuracy: 0.81 - ETA: 2s - loss: 0.6153 - accuracy: 0.81 - ETA: 1s - loss: 0.6147 - accuracy: 0.81 - ETA: 0s - loss: 0.6133 - accuracy: 0.82 - 114s 9ms/step - loss: 0.6130 - accuracy: 0.8198 - val_loss: 3.4105 - val_accuracy: 0.3240\n",
      "2020-03-19 05:17:23.633884\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "t5=datetime.datetime.now()\n",
    "print(t5)\n",
    "history=model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[mcp_save], batch_size=128)\n",
    "t6=datetime.datetime.now()\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Pickle/InceptionV3_history.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model as a pickle in a file \n",
    "joblib.dump(history, '../Pickle/InceptionV3_history.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file \n",
    "history = joblib.load('../Pickle/InceptionV3_history.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [3.8793205008746536, 3.6525240989189265, 3.4753214119782916, 3.3361346054302192, 3.126675417514565, 2.9717637575523113, 2.851164906270319, 2.817053628197208, 2.7817472997389463, 2.7030648023598767, 2.715106727899928, 2.647431322614691, 2.6670836862666913, 2.681934752748198, 2.7406293484190454, 2.658106055011289, 2.6570711774214963, 2.7076809488941525, 2.6593554040701424, 2.6174034964997186, 2.7110390982105934, 2.7733235862947976, 2.713603674034649, 2.743437679131162, 2.73985341426616, 2.808650443505886, 2.79747914142716, 2.7146693876854715, 2.848983775770887, 2.794963199833946, 2.7239143576633262, 2.720556826659871, 2.8299779395352562, 2.6914249166297184, 2.93052032228, 2.873652211116736, 2.8065680218469993, 2.7255828048209567, 2.861443209916456, 2.94286072557127, 2.827847744813089, 2.860187359266477, 2.8367181160982735, 2.8578926249757006, 2.80945519432439, 2.8122611309400507, 2.8409605352994123, 2.9218125127798245, 2.824555649171401, 2.9310628204515354, 2.8592425799235652, 2.920272637980238, 2.993680275748096, 2.9592683588361366, 2.97877287038075, 2.9161406494264575, 2.820200614713675, 3.0776827578147326, 2.901447964160363, 3.0434850483975886, 2.9898764187864653, 2.962964799231465, 3.147607347281015, 3.047022977981491, 2.920961974125459, 3.02072491074976, 2.8906622469738967, 3.0311567480842396, 3.1130811069292563, 3.1279685238265023, 3.159508559032585, 3.027638320027862, 3.092307392407035, 2.9775986890650894, 3.108002137782938, 3.107171957301107, 3.30692357083461, 3.146086205175036, 3.12260585165885, 3.112395767530267, 3.09557863736157, 3.205445440933392, 3.147936290568547, 3.2073993228009403, 3.1762734722216908, 3.214616230079885, 3.2573421713357655, 3.018588660383943, 3.2708023102157107, 3.1996741091886953, 3.211980010273453, 3.1516469939041967, 3.241848783563152, 3.1841269010293503, 3.2666267852944197, 3.1364255762074253, 3.2497577364398427, 3.371293498570007, 3.350459356720423, 3.4104728061894494], 'val_accuracy': [0.027409693226218224, 0.09312035143375397, 0.10274096578359604, 0.14194953441619873, 0.18569613993167877, 0.2022145539522171, 0.3276456594467163, 0.2789980173110962, 0.31366854906082153, 0.36267924308776855, 0.32201850414276123, 0.32474133372306824, 0.35596296191215515, 0.32673805952072144, 0.304592490196228, 0.344164103269577, 0.33345434069633484, 0.32673805952072144, 0.37066617608070374, 0.33381739258766174, 0.34525322914123535, 0.3145761489868164, 0.35795968770980835, 0.3269195854663849, 0.35124340653419495, 0.3256489336490631, 0.3394445478916168, 0.33726629614830017, 0.33545106649398804, 0.31330549716949463, 0.3459793031215668, 0.35196951031684875, 0.31947723031044006, 0.3485206067562103, 0.31566527485847473, 0.33781084418296814, 0.32583045959472656, 0.3359956443309784, 0.3222000300884247, 0.31566527485847473, 0.3430749773979187, 0.3347249925136566, 0.32474133372306824, 0.35687056183815, 0.3581412136554718, 0.3494282066822052, 0.34997278451919556, 0.3300054371356964, 0.33454346656799316, 0.35687056183815, 0.33055001497268677, 0.3129424452781677, 0.3347249925136566, 0.3430749773979187, 0.34325650334358215, 0.32746416330337524, 0.33654019236564636, 0.3142130970954895, 0.3514249324798584, 0.3309130370616913, 0.32673805952072144, 0.3153022229671478, 0.31911417841911316, 0.33690324425697327, 0.3229261338710785, 0.3347249925136566, 0.3443456292152405, 0.3225630819797516, 0.326556533575058, 0.3272826373577118, 0.33889997005462646, 0.3490651547908783, 0.3457977771759033, 0.3554184138774872, 0.31675440073013306, 0.33345434069633484, 0.30295878648757935, 0.31802505254745483, 0.33109456300735474, 0.30568161606788635, 0.32347068190574646, 0.3312760889530182, 0.3149392008781433, 0.3263750374317169, 0.32002177834510803, 0.3241967558860779, 0.283717542886734, 0.3343619406223297, 0.3169359266757965, 0.335269570350647, 0.35868579149246216, 0.335269570350647, 0.3370847702026367, 0.34997278451919556, 0.3042294383049011, 0.3131239712238312, 0.31276094913482666, 0.3283717632293701, 0.31003811955451965, 0.3240152597427368], 'loss': [3.902761937982017, 3.512139224764426, 3.0367196295745105, 2.6650732488900246, 2.3807035842591433, 2.1677757812304312, 1.9924073181582969, 1.8900548835226219, 1.7398171590744416, 1.6570096913615857, 1.5686919296086246, 1.5002318026923929, 1.441401655910898, 1.3886557510418747, 1.3760530751451994, 1.3052432580470965, 1.27394130460276, 1.2344208335275932, 1.2037341878301004, 1.168946239077135, 1.1517066054402034, 1.1332837458378964, 1.1014511744339992, 1.0967905953666102, 1.051826412182326, 1.0454909263201306, 1.0287009350551546, 1.036787369049487, 0.99407065347709, 0.970315521213064, 0.9922850413396417, 0.9394330245671121, 0.9371183115369328, 0.9338899228708698, 0.9253388487639066, 0.908045213428662, 0.8896946017596784, 0.890261785077016, 0.8629178866650208, 0.8631572095027005, 0.8550519172615434, 0.8595342170604453, 0.8561206928890177, 0.8306529689285543, 0.8298753695615406, 0.8397138462951337, 0.7947428773357982, 0.7981256304239828, 0.8081733501739279, 0.7820604237025646, 0.7811226462222999, 0.7834918817293257, 0.7797750819573441, 0.7584097541764555, 0.7609939809640859, 0.7754000303126795, 0.7631417218486461, 0.7451646985902963, 0.7192624899360043, 0.7291588408480846, 0.7171560446708145, 0.7383950450621412, 0.7308425046280405, 0.7074827670483449, 0.7035650553891524, 0.7012530809228018, 0.708867518945034, 0.7015696345816888, 0.6874574794964826, 0.674738060107998, 0.6641106923830122, 0.672835507996711, 0.6536845552758346, 0.6755936459230948, 0.6806614924605001, 0.6696232637077812, 0.6734481737775162, 0.6800177483363848, 0.6621452980813407, 0.6578254141743108, 0.6505689328578445, 0.654109689583703, 0.6484665915157146, 0.6522196923472552, 0.6297512398925433, 0.6254078975847617, 0.6474193656083864, 0.6341357449290757, 0.602443691342497, 0.6193158551306791, 0.6267899865746004, 0.6092892786065401, 0.610507575163111, 0.6185797460707921, 0.6200985204073127, 0.6403801634717767, 0.6206905306566514, 0.6024907142074953, 0.6297654387607056, 0.6130440078230611], 'accuracy': [0.042466596, 0.118261404, 0.21356167, 0.292505, 0.3604669, 0.40500692, 0.44570726, 0.47150975, 0.50199664, 0.5214253, 0.53924125, 0.55406237, 0.5695746, 0.5865458, 0.5916142, 0.6085087, 0.61196434, 0.6194133, 0.63354325, 0.63792044, 0.64636767, 0.6490554, 0.6578866, 0.6610352, 0.6717862, 0.6736292, 0.680771, 0.6772385, 0.6906005, 0.69904774, 0.6893718, 0.70365536, 0.71064353, 0.7140224, 0.7144064, 0.71509755, 0.71978194, 0.7240055, 0.73145443, 0.7355245, 0.73291355, 0.7333743, 0.73122406, 0.7412072, 0.74020886, 0.74281985, 0.7541084, 0.75303334, 0.7587928, 0.75733376, 0.76270926, 0.76286286, 0.76148057, 0.7681616, 0.76877594, 0.7624789, 0.76808476, 0.77207804, 0.77591765, 0.77722317, 0.7822147, 0.77637845, 0.7792198, 0.7852864, 0.78436494, 0.7865151, 0.785824, 0.78751343, 0.78920287, 0.79334974, 0.7981109, 0.79419446, 0.8008754, 0.7948856, 0.79765016, 0.7981877, 0.7958839, 0.79265857, 0.8013362, 0.804024, 0.80310243, 0.80640453, 0.806251, 0.8037168, 0.812548, 0.816618, 0.8057902, 0.8134695, 0.8193058, 0.81846106, 0.8116265, 0.8197665, 0.81400704, 0.82314545, 0.81884503, 0.8103978, 0.81608045, 0.8222239, 0.817002, 0.8197665]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot history: MAE\n",
    "plt.plot(history.history['loss'], label='MAE (testing data)')\n",
    "plt.plot(history.history['val_loss'], label='MAE (validation data)')\n",
    "plt.title('MAE')\n",
    "plt.ylabel('MAE value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = np.arange(0, len(history.history['loss']))\n",
    "\n",
    "# You can chose the style of your preference\n",
    "# print(plt.style.available) to see the available options\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "# Plot train loss, train acc, val loss and val acc against epochs passed\n",
    "plt.figure()\n",
    "plt.plot(N, history.history['loss'], label = \"train_loss\")\n",
    "plt.plot(N, history.history['accuracy'], label = \"train_acc\")\n",
    "plt.plot(N, history.history['val_loss'], label = \"val_loss\")\n",
    "plt.plot(N, history.history['val_accuracy'], label = \"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "# Make sure there exists a folder called output in the current directory\n",
    "# or replace 'output' with whatever direcory you want to put in the plots\n",
    "plt.show()\n",
    "plt.savefig('../Output/EpochInceptionV3.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
